{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef adjust_cells(self):\n        self.resizeColumnsToContents()\n        fm = self.horizontalHeader().fontMetrics()\n        names = [fm.width(s.cmd) for s in self.source_model.servers]\n        if names:\n            self.setColumnWidth(CMD, max(names))\n        self.horizontalHeader().setStretchLastSection(True)", "response": "Adjust column size based on contents."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmove to next row from currently selected row.", "response": "def next_row(self):\n        \"\"\"Move to next row from currently selected row.\"\"\"\n        row = self.currentIndex().row()\n        rows = self.source_model.rowCount()\n        if row + 1 == rows:\n            row = -1\n        self.selectRow(row + 1)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmove to previous row from currently selected row.", "response": "def previous_row(self):\n        \"\"\"Move to previous row from currently selected row.\"\"\"\n        row = self.currentIndex().row()\n        rows = self.source_model.rowCount()\n        if row == 0:\n            row = rows\n        self.selectRow(row - 1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef keyPressEvent(self, event):\n        key = event.key()\n        if key in [Qt.Key_Enter, Qt.Key_Return]:\n            self.show_editor()\n        elif key in [Qt.Key_Backtab]:\n            self.parent().reset_btn.setFocus()\n        elif key in [Qt.Key_Up, Qt.Key_Down, Qt.Key_Left, Qt.Key_Right]:\n            super(LSPServerTable, self).keyPressEvent(event)\n        elif key in [Qt.Key_Escape]:\n            self.finder.keyPressEvent(event)\n        else:\n            super(LSPServerTable, self).keyPressEvent(event)", "response": "Override keyPressEvent to show editor and reset button."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding an external path to the combobox.", "response": "def add_external_path(self, path):\r\n        \"\"\"\r\n        Adds an external path to the combobox if it exists on the file system.\r\n        If the path is already listed in the combobox, it is removed from its\r\n        current position and added back at the end. If the maximum number of\r\n        paths is reached, the oldest external path is removed from the list.\r\n        \"\"\"\r\n        if not osp.exists(path):\r\n            return\r\n        self.removeItem(self.findText(path))\r\n        self.addItem(path)\r\n        self.setItemData(self.count() - 1, path, Qt.ToolTipRole)\r\n        while self.count() > MAX_PATH_HISTORY + EXTERNAL_PATHS:\r\n            self.removeItem(EXTERNAL_PATHS)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of the external paths listed in the combobox.", "response": "def get_external_paths(self):\r\n        \"\"\"Returns a list of the external paths listed in the combobox.\"\"\"\r\n        return [to_text_string(self.itemText(i))\r\n                for i in range(EXTERNAL_PATHS, self.count())]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the path corresponding to the currently selected item in the combobox.", "response": "def get_current_searchpath(self):\r\n        \"\"\"\r\n        Returns the path corresponding to the currently selected item\r\n        in the combobox.\r\n        \"\"\"\r\n        idx = self.currentIndex()\r\n        if idx == CWD:\r\n            return self.path\r\n        elif idx == PROJECT:\r\n            return self.project_path\r\n        elif idx == FILE_PATH:\r\n            return self.file_path\r\n        else:\r\n            return self.external_path"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nhandle when the current index of the combobox changes.", "response": "def path_selection_changed(self):\r\n        \"\"\"Handles when the current index of the combobox changes.\"\"\"\r\n        idx = self.currentIndex()\r\n        if idx == SELECT_OTHER:\r\n            external_path = self.select_directory()\r\n            if len(external_path) > 0:\r\n                self.add_external_path(external_path)\r\n                self.setCurrentIndex(self.count() - 1)\r\n            else:\r\n                self.setCurrentIndex(CWD)\r\n        elif idx == CLEAR_LIST:\r\n            reply = QMessageBox.question(\r\n                    self, _(\"Clear other directories\"),\r\n                    _(\"Do you want to clear the list of other directories?\"),\r\n                    QMessageBox.Yes | QMessageBox.No)\r\n            if reply == QMessageBox.Yes:\r\n                self.clear_external_paths()\r\n            self.setCurrentIndex(CWD)\r\n        elif idx >= EXTERNAL_PATHS:\r\n            self.external_path = to_text_string(self.itemText(idx))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef select_directory(self):\r\n        self.__redirect_stdio_emit(False)\r\n        directory = getexistingdirectory(\r\n                self, _(\"Select directory\"), self.path)\r\n        if directory:\r\n            directory = to_unicode_from_fs(osp.abspath(directory))\r\n        self.__redirect_stdio_emit(True)\r\n        return directory", "response": "Select a directory from the user s directory list."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the project path and disables the project search in the combobox", "response": "def set_project_path(self, path):\r\n        \"\"\"\r\n        Sets the project path and disables the project search in the combobox\r\n        if the value of path is None.\r\n        \"\"\"\r\n        if path is None:\r\n            self.project_path = None\r\n            self.model().item(PROJECT, 0).setEnabled(False)\r\n            if self.currentIndex() == PROJECT:\r\n                self.setCurrentIndex(CWD)\r\n        else:\r\n            path = osp.abspath(path)\r\n            self.project_path = path\r\n            self.model().item(PROJECT, 0).setEnabled(True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nuses to handle key events on the combobox.", "response": "def eventFilter(self, widget, event):\r\n        \"\"\"Used to handle key events on the QListView of the combobox.\"\"\"\r\n        if event.type() == QEvent.KeyPress and event.key() == Qt.Key_Delete:\r\n            index = self.view().currentIndex().row()\r\n            if index >= EXTERNAL_PATHS:\r\n                # Remove item and update the view.\r\n                self.removeItem(index)\r\n                self.showPopup()\r\n                # Set the view selection so that it doesn't bounce around.\r\n                new_index = min(self.count() - 1, index)\r\n                new_index = 0 if new_index < EXTERNAL_PATHS else new_index\r\n                self.view().setCurrentIndex(self.model().index(new_index, 0))\r\n                self.setCurrentIndex(new_index)\r\n            return True\r\n        return QComboBox.eventFilter(self, widget, event)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef __redirect_stdio_emit(self, value):\r\n        parent = self.parent()\r\n        while parent is not None:\r\n            try:\r\n                parent.redirect_stdio.emit(value)\r\n            except AttributeError:\r\n                parent = parent.parent()\r\n            else:\r\n                break", "response": "Emits the redirect_stdio signal."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the options for the current menu item.", "response": "def get_options(self, to_save=False):\r\n        \"\"\"Get options\"\"\"\r\n        text_re = self.edit_regexp.isChecked()\r\n        exclude_re = self.exclude_regexp.isChecked()\r\n        case_sensitive = self.case_button.isChecked()\r\n\r\n        # Return current options for them to be saved when closing\r\n        # Spyder.\r\n        if to_save:\r\n            search_text = [to_text_string(self.search_text.itemText(index))\r\n                           for index in range(self.search_text.count())]\r\n            exclude = [to_text_string(self.exclude_pattern.itemText(index))\r\n                       for index in range(self.exclude_pattern.count())]\r\n            exclude_idx = self.exclude_pattern.currentIndex()\r\n            path_history = self.path_selection_combo.get_external_paths()\r\n            more_options = self.more_options.isChecked()\r\n            return (search_text, text_re,\r\n                    exclude, exclude_idx,\r\n                    exclude_re, more_options,\r\n                    case_sensitive, path_history)\r\n\r\n        # Clear fields\r\n        self.search_text.lineEdit().setStyleSheet(\"\")\r\n        self.exclude_pattern.lineEdit().setStyleSheet(\"\")\r\n        self.search_text.setToolTip(\"\")\r\n        self.exclude_pattern.setToolTip(\"\")\r\n\r\n        utext = to_text_string(self.search_text.currentText())\r\n        if not utext:\r\n            return\r\n\r\n        try:\r\n            texts = [(utext.encode('utf-8'), 'utf-8')]\r\n        except UnicodeEncodeError:\r\n            texts = []\r\n            for enc in self.supported_encodings:\r\n                try:\r\n                    texts.append((utext.encode(enc), enc))\r\n                except UnicodeDecodeError:\r\n                    pass\r\n\r\n        exclude = to_text_string(self.exclude_pattern.currentText())\r\n\r\n        if not case_sensitive:\r\n            texts = [(text[0].lower(), text[1]) for text in texts]\r\n\r\n        file_search = self.path_selection_combo.is_file_search()\r\n        path = self.path_selection_combo.get_current_searchpath()\r\n\r\n        if not exclude_re:\r\n            items = [fnmatch.translate(item.strip())\r\n                     for item in exclude.split(\",\")\r\n                     if item.strip() != '']\r\n            exclude = '|'.join(items)\r\n\r\n        # Validate exclude regular expression\r\n        if exclude:\r\n            error_msg = regexp_error_msg(exclude)\r\n            if error_msg:\r\n                exclude_edit = self.exclude_pattern.lineEdit()\r\n                exclude_edit.setStyleSheet(self.REGEX_INVALID)\r\n                tooltip = self.REGEX_ERROR + u': ' + to_text_string(error_msg)\r\n                self.exclude_pattern.setToolTip(tooltip)\r\n                return None\r\n            else:\r\n                exclude = re.compile(exclude)\r\n\r\n        # Validate text regular expression\r\n        if text_re:\r\n            error_msg = regexp_error_msg(texts[0][0])\r\n            if error_msg:\r\n                self.search_text.lineEdit().setStyleSheet(self.REGEX_INVALID)\r\n                tooltip = self.REGEX_ERROR + u': ' + to_text_string(error_msg)\r\n                self.search_text.setToolTip(tooltip)\r\n                return None\r\n            else:\r\n                texts = [(re.compile(x[0]), x[1]) for x in texts]\r\n\r\n        return (path, file_search, exclude, texts, text_re, case_sensitive)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef keyPressEvent(self, event):\r\n        ctrl = event.modifiers() & Qt.ControlModifier\r\n        shift = event.modifiers() & Qt.ShiftModifier\r\n        if event.key() in (Qt.Key_Enter, Qt.Key_Return):\r\n            self.find.emit()\r\n        elif event.key() == Qt.Key_F and ctrl and shift:\r\n            # Toggle find widgets\r\n            self.parent().toggle_visibility.emit(not self.isVisible())\r\n        else:\r\n            QWidget.keyPressEvent(self, event)", "response": "Reimplemented to handle key events"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndoubles - click event", "response": "def activated(self, item):\r\n        \"\"\"Double-click event\"\"\"\r\n        itemdata = self.data.get(id(self.currentItem()))\r\n        if itemdata is not None:\r\n            filename, lineno, colno = itemdata\r\n            self.sig_edit_goto.emit(filename, lineno, self.search_text)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nenables result sorting after search is complete.", "response": "def set_sorting(self, flag):\r\n        \"\"\"Enable result sorting after search is complete.\"\"\"\r\n        self.sorting['status'] = flag\r\n        self.header().setSectionsClickable(flag == ON)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\noverride showEvent to start waiting spinner.", "response": "def showEvent(self, event):\r\n        \"\"\"Override show event to start waiting spinner.\"\"\"\r\n        QWidget.showEvent(self, event)\r\n        self.spinner.start()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef hideEvent(self, event):\r\n        QWidget.hideEvent(self, event)\r\n        self.spinner.stop()", "response": "Override hide event to stop waiting spinner."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalls the find function", "response": "def find(self):\r\n        \"\"\"Call the find function\"\"\"\r\n        options = self.find_options.get_options()\r\n        if options is None:\r\n            return\r\n        self.stop_and_reset_thread(ignore_results=True)\r\n        self.search_thread = SearchThread(self)\r\n        self.search_thread.sig_finished.connect(self.search_complete)\r\n        self.search_thread.sig_current_file.connect(\r\n            lambda x: self.status_bar.set_label_path(x, folder=False)\r\n        )\r\n        self.search_thread.sig_current_folder.connect(\r\n            lambda x: self.status_bar.set_label_path(x, folder=True)\r\n        )\r\n        self.search_thread.sig_file_match.connect(\r\n            self.result_browser.append_result\r\n        )\r\n        self.search_thread.sig_out_print.connect(\r\n            lambda x: sys.stdout.write(str(x) + \"\\n\")\r\n        )\r\n        self.status_bar.reset()\r\n        self.result_browser.clear_title(\r\n            self.find_options.search_text.currentText())\r\n        self.search_thread.initialize(*options)\r\n        self.search_thread.start()\r\n        self.find_options.ok_button.setEnabled(False)\r\n        self.find_options.stop_button.setEnabled(True)\r\n        self.status_bar.show()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef stop_and_reset_thread(self, ignore_results=False):\r\n        if self.search_thread is not None:\r\n            if self.search_thread.isRunning():\r\n                if ignore_results:\r\n                    self.search_thread.sig_finished.disconnect(\r\n                        self.search_complete)\r\n                self.search_thread.stop()\r\n                self.search_thread.wait()\r\n            self.search_thread.setParent(None)\r\n            self.search_thread = None", "response": "Stop current search thread and clean - up"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncall by the search thread when the search thread is finished", "response": "def search_complete(self, completed):\r\n        \"\"\"Current search thread has finished\"\"\"\r\n        self.result_browser.set_sorting(ON)\r\n        self.find_options.ok_button.setEnabled(True)\r\n        self.find_options.stop_button.setEnabled(False)\r\n        self.status_bar.hide()\r\n        self.result_browser.expandAll()\r\n        if self.search_thread is None:\r\n            return\r\n        self.sig_finished.emit()\r\n        found = self.search_thread.get_results()\r\n        self.stop_and_reset_thread()\r\n        if found is not None:\r\n            results, pathlist, nb, error_flag = found\r\n            self.result_browser.show()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the stored credentials from settings.", "response": "def _get_credentials_from_settings(self):\n        \"\"\"Get the stored credentials if any.\"\"\"\n        remember_me = CONF.get('main', 'report_error/remember_me')\n        remember_token = CONF.get('main', 'report_error/remember_token')\n        username = CONF.get('main', 'report_error/username', '')\n        if not remember_me:\n            username = ''\n\n        return username, remember_me, remember_token"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nstores credentials for future use.", "response": "def _store_credentials(self, username, password, remember=False):\n        \"\"\"Store credentials for future use.\"\"\"\n        if username and password and remember:\n            CONF.set('main', 'report_error/username', username)\n            try:\n                keyring.set_password('github', username, password)\n            except Exception:\n                if self._show_msgbox:\n                    QMessageBox.warning(self.parent_widget,\n                                        _('Failed to store password'),\n                                        _('It was not possible to securely '\n                                          'save your password. You will be '\n                                          'prompted for your Github '\n                                          'credentials next time you want '\n                                          'to report an issue.'))\n                remember = False\n        CONF.set('main', 'report_error/remember_me', remember)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _store_token(self, token, remember=False):\n        if token and remember:\n            try:\n                keyring.set_password('github', 'token', token)\n            except Exception:\n                if self._show_msgbox:\n                    QMessageBox.warning(self.parent_widget,\n                                        _('Failed to store token'),\n                                        _('It was not possible to securely '\n                                          'save your token. You will be '\n                                          'prompted for your Github token '\n                                          'next time you want to report '\n                                          'an issue.'))\n                remember = False\n        CONF.set('main', 'report_error/remember_token', remember)", "response": "Store token for future use."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_user_credentials(self):\n        password = None\n        token = None\n        (username, remember_me,\n         remember_token) = self._get_credentials_from_settings()\n        valid_py_os = not (PY2 and sys.platform.startswith('linux'))\n        if username and remember_me and valid_py_os:\n            # Get password from keyring\n            try:\n                password = keyring.get_password('github', username)\n            except Exception:\n                # No safe keyring backend\n                if self._show_msgbox:\n                    QMessageBox.warning(self.parent_widget,\n                                        _('Failed to retrieve password'),\n                                        _('It was not possible to retrieve '\n                                          'your password. Please introduce '\n                                          'it again.'))\n        if remember_token and valid_py_os:\n            # Get token from keyring\n            try:\n                token = keyring.get_password('github', 'token')\n            except Exception:\n                # No safe keyring backend\n                if self._show_msgbox:\n                    QMessageBox.warning(self.parent_widget,\n                                        _('Failed to retrieve token'),\n                                        _('It was not possible to retrieve '\n                                          'your token. Please introduce it '\n                                          'again.'))\n\n        if not running_under_pytest():\n            credentials = DlgGitHubLogin.login(self.parent_widget, username,\n                                            password, token, remember_me,\n                                            remember_token)\n\n            if (credentials['username'] and credentials['password'] and\n                    valid_py_os):\n                self._store_credentials(credentials['username'],\n                                        credentials['password'],\n                                        credentials['remember'])\n                CONF.set('main', 'report_error/remember_me',\n                         credentials['remember'])\n\n            if credentials['token'] and valid_py_os:\n                self._store_token(credentials['token'],\n                                  credentials['remember_token'])\n                CONF.set('main', 'report_error/remember_token',\n                         credentials['remember_token'])\n        else:\n            return dict(username=username,\n                        password=password,\n                        token='',\n                        remember=remember_me,\n                        remember_token=remember_token)\n\n        return credentials", "response": "Get user credentials with the login dialog."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef show_tip(self, point, tip):\n        # Don't attempt to show it if it's already visible and the text\n        # to be displayed is the same as the one displayed before.\n        if self.isVisible():\n            if self.tip == tip:\n                return True\n            else:\n                self.hide()\n\n        # Set the text and resize the widget accordingly.\n        self.tip = tip\n        self.setText(tip)\n        self.resize(self.sizeHint())\n        self.move(point)\n        self.show()\n        return True", "response": "Attempts to show the specified tip at the current cursor location."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef leaveEvent(self, event):\n        super(ToolTipWidget, self).leaveEvent(event)\n        self.hide()", "response": "Override Qt method to hide the tooltip on leave."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef eventFilter(self, obj, event):\n        if obj == self._text_edit:\n            etype = event.type()\n\n            if etype == QEvent.KeyPress:\n                key = event.key()\n                cursor = self._text_edit.textCursor()\n                prev_char = self._text_edit.get_character(cursor.position(),\n                                                          offset=-1)\n                if key in (Qt.Key_Enter, Qt.Key_Return,\n                           Qt.Key_Down, Qt.Key_Up):\n                    self.hide()\n                elif key == Qt.Key_Escape:\n                    self.hide()\n                    return True\n                elif prev_char == ')':\n                    self.hide()\n\n            elif etype == QEvent.FocusOut:\n                self.hide()\n\n            elif etype == QEvent.Enter:\n                if (self._hide_timer.isActive() and\n                  self.app.topLevelAt(QCursor.pos()) == self):\n                    self._hide_timer.stop()\n\n            elif etype == QEvent.Leave:\n                self._leave_event_hide()\n\n        return super(CallTipWidget, self).eventFilter(obj, event)", "response": "Reimplemented to hide on certain key presses and on text edit focus\n            changes."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef timerEvent(self, event):\n        if event.timerId() == self._hide_timer.timerId():\n            self._hide_timer.stop()\n            self.hide()", "response": "Reimplemented to hide the widget when the hide timer fires."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndisconnects signal handlers and event filter.", "response": "def hideEvent(self, event):\n        \"\"\" Reimplemented to disconnect signal handlers and event filter.\n        \"\"\"\n        super(CallTipWidget, self).hideEvent(event)\n        self._text_edit.cursorPositionChanged.disconnect(\n            self._cursor_position_changed)\n        self._text_edit.removeEventFilter(self)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef showEvent(self, event):\n        super(CallTipWidget, self).showEvent(event)\n        self._text_edit.cursorPositionChanged.connect(\n            self._cursor_position_changed)\n        self._text_edit.installEventFilter(self)", "response": "Connect signal handlers and event filter."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef show_tip(self, point, tip, wrapped_tiplines):\n        # Don't attempt to show it if it's already visible and the text\n        # to be displayed is the same as the one displayed before.\n        if self.isVisible():\n            if self.tip == tip:\n                return True\n            else:\n                self.hide()\n\n        # Attempt to find the cursor position at which to show the call tip.\n        text_edit = self._text_edit\n        cursor = text_edit.textCursor()\n        search_pos = cursor.position() - 1\n        self._start_position, _ = self._find_parenthesis(search_pos,\n                                                         forward=False)\n        if self._start_position == -1:\n            return False\n\n        if self.hide_timer_on:\n            self._hide_timer.stop()\n            # Logic to decide how much time to show the calltip depending\n            # on the amount of text present\n            if len(wrapped_tiplines) == 1:\n                args = wrapped_tiplines[0].split('(')[1]\n                nargs = len(args.split(','))\n                if nargs == 1:\n                    hide_time = 1400\n                elif nargs == 2:\n                    hide_time = 1600\n                else:\n                    hide_time = 1800\n            elif len(wrapped_tiplines) == 2:\n                args1 = wrapped_tiplines[1].strip()\n                nargs1 = len(args1.split(','))\n                if nargs1 == 1:\n                    hide_time = 2500\n                else:\n                    hide_time = 2800\n            else:\n                hide_time = 3500\n            self._hide_timer.start(hide_time, self)\n\n        # Set the text and resize the widget accordingly.\n        self.tip = tip\n        self.setText(tip)\n        self.resize(self.sizeHint())\n\n        # Locate and show the widget. Place the tip below the current line\n        # unless it would be off the screen.  In that case, decide the best\n        # location based trying to minimize the  area that goes off-screen.\n        padding = 3 # Distance in pixels between cursor bounds and tip box.\n        cursor_rect = text_edit.cursorRect(cursor)\n        screen_rect = self.app.desktop().screenGeometry(text_edit)\n        point.setY(point.y() + padding)\n        tip_height = self.size().height()\n        tip_width = self.size().width()\n\n        vertical = 'bottom'\n        horizontal = 'Right'\n        if point.y() + tip_height > screen_rect.height() + screen_rect.y():\n            point_ = text_edit.mapToGlobal(cursor_rect.topRight())\n            # If tip is still off screen, check if point is in top or bottom\n            # half of screen.\n            if point_.y() - tip_height < padding:\n                # If point is in upper half of screen, show tip below it.\n                # otherwise above it.\n                if 2*point.y() < screen_rect.height():\n                    vertical = 'bottom'\n                else:\n                    vertical = 'top'\n            else:\n                vertical = 'top'\n        if point.x() + tip_width > screen_rect.width() + screen_rect.x():\n            point_ = text_edit.mapToGlobal(cursor_rect.topRight())\n            # If tip is still off-screen, check if point is in the right or\n            # left half of the screen.\n            if point_.x() - tip_width < padding:\n                if 2*point.x() < screen_rect.width():\n                    horizontal = 'Right'\n                else:\n                    horizontal = 'Left'\n            else:\n                horizontal = 'Left'\n        pos = getattr(cursor_rect, '%s%s' %(vertical, horizontal))\n        adjusted_point = text_edit.mapToGlobal(pos())\n        if vertical == 'top':\n            point.setY(adjusted_point.y() - tip_height - padding)\n        if horizontal == 'Left':\n            point.setX(adjusted_point.x() - tip_width - padding)\n\n        self.move(point)\n        self.show()\n        return True", "response": "Show the specified tip at the current cursor location."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _leave_event_hide(self):\n        if (self.hide_timer_on and not self._hide_timer.isActive() and\n            # If Enter events always came after Leave events, we wouldn't need\n            # this check. But on Mac OS, it sometimes happens the other way\n            # around when the tooltip is created.\n            self.app.topLevelAt(QCursor.pos()) != self):\n            self._hide_timer.start(800, self)", "response": "Hides the tooltip after some time has passed."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate the tip based on user cursor position.", "response": "def _cursor_position_changed(self):\n        \"\"\" Updates the tip based on user cursor movement.\n        \"\"\"\n        cursor = self._text_edit.textCursor()\n        position = cursor.position()\n        document = self._text_edit.document()\n        char = to_text_string(document.characterAt(position - 1))\n        if position <= self._start_position:\n            self.hide()\n        elif char == ')':\n            pos, _ = self._find_parenthesis(position - 1, forward=False)\n            if pos == -1:\n                self.hide()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndetect if text has mixed EOL characters", "response": "def has_mixed_eol_chars(text):\r\n    \"\"\"Detect if text has mixed EOL characters\"\"\"\r\n    eol_chars = get_eol_chars(text)\r\n    if eol_chars is None:\r\n        return False\r\n    correct_text = eol_chars.join((text+eol_chars).splitlines())\r\n    return repr(correct_text) != repr(text)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nnormalizes the text to include the given eol.", "response": "def normalize_eols(text, eol='\\n'):\r\n    \"\"\"Use the same eol's in text\"\"\"\r\n    for eol_char, _ in EOL_CHARS:\r\n        if eol_char != eol:\r\n            text = text.replace(eol_char, eol)\r\n    return text"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntests if passed string is the name of a Python builtin object", "response": "def is_builtin(text):\r\n    \"\"\"Test if passed string is the name of a Python builtin object\"\"\"\r\n    from spyder.py3compat import builtins\r\n    return text in [str(name) for name in dir(builtins)\r\n                    if not name.startswith('_')]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a new Python object in source_code at offset", "response": "def get_primary_at(source_code, offset, retry=True):\r\n    \"\"\"Return Python object in *source_code* at *offset*\r\n    Periods to the left of the cursor are carried forward \r\n      e.g. 'functools.par^tial' would yield 'functools.partial'\r\n    Retry prevents infinite recursion: retry only once\r\n    \"\"\"\r\n    obj = ''\r\n    left = re.split(r\"[^0-9a-zA-Z_.]\", source_code[:offset])\r\n    if left and left[-1]:\r\n        obj = left[-1]\r\n    right = re.split(r\"\\W\", source_code[offset:])\r\n    if right and right[0]:\r\n        obj += right[0]\r\n    if obj and obj[0].isdigit():\r\n        obj = ''\r\n    # account for opening chars with no text to the right\r\n    if not obj and retry and offset and source_code[offset - 1] in '([.':\r\n        return get_primary_at(source_code, offset - 1, retry=False)\r\n    return obj"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef split_source(source_code):\r\n    '''Split source code into lines\r\n    '''\r\n    eol_chars = get_eol_chars(source_code)\r\n    if eol_chars:\r\n        return source_code.split(eol_chars)\r\n    else:\r\n        return [source_code]", "response": "Split source code into lines\r\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_identifiers(source_code):\r\n    '''Split source code into python identifier-like tokens'''\r\n    tokens = set(re.split(r\"[^0-9a-zA-Z_.]\", source_code))\r\n    valid = re.compile(r'[a-zA-Z_]')\r\n    return [token for token in tokens if re.match(valid, token)]", "response": "Split source code into python identifier - like tokens"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the individual components of a given file path", "response": "def path_components(path):\r\n    \"\"\"\r\n    Return the individual components of a given file path\r\n    string (for the local operating system).\r\n\r\n    Taken from https://stackoverflow.com/q/21498939/438386\r\n    \"\"\"\r\n    components = []\r\n    # The loop guarantees that the returned components can be\r\n    # os.path.joined with the path separator and point to the same\r\n    # location:    \r\n    while True:\r\n        (new_path, tail) = os.path.split(path)  # Works on any platform\r\n        components.append(tail)        \r\n        if new_path == path:  # Root (including drive, on Windows) reached\r\n            break\r\n        path = new_path\r\n    components.append(new_path)    \r\n    components.reverse()  # First component first \r\n    return components"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef differentiate_prefix(path_components0, path_components1):\r\n    longest_prefix = []\r\n    root_comparison = False\r\n    common_elmt = None\r\n    for index, (elmt0, elmt1) in enumerate(zip(path_components0, path_components1)):\r\n        if elmt0 != elmt1:\r\n            if index == 2:\r\n                root_comparison = True\r\n            break\r\n        else:\r\n            common_elmt = elmt0\r\n        longest_prefix.append(elmt0)\r\n    file_name_length = len(path_components0[len(path_components0) - 1])\r\n    path_0 = os.path.join(*path_components0)[:-file_name_length - 1]\r\n    if len(longest_prefix) > 0:\r\n        longest_path_prefix = os.path.join(*longest_prefix)\r\n        longest_prefix_length = len(longest_path_prefix) + 1\r\n        if path_0[longest_prefix_length:] != '' and not root_comparison:\r\n            path_0_components = path_components(path_0[longest_prefix_length:])\r\n            if path_0_components[0] == ''and path_0_components[1] == ''and len(\r\n                                        path_0[longest_prefix_length:]) > 20:\r\n                path_0_components.insert(2, common_elmt)\r\n                path_0 = os.path.join(*path_0_components)\r\n            else:\r\n                path_0 = path_0[longest_prefix_length:]\r\n        elif not root_comparison:\r\n            path_0 = common_elmt\r\n        elif sys.platform.startswith('linux') and path_0 == '':\r\n            path_0 = '/'\r\n    return path_0", "response": "Returns the differentiated prefix of the given two iterables."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef disambiguate_fname(files_path_list, filename):\r\n    fname = os.path.basename(filename)\r\n    same_name_files = get_same_name_files(files_path_list, fname)\r\n    if len(same_name_files) > 1:\r\n        compare_path = shortest_path(same_name_files)\r\n        if compare_path == filename:\r\n            same_name_files.remove(path_components(filename))\r\n            compare_path = shortest_path(same_name_files)    \r\n        diff_path = differentiate_prefix(path_components(filename),\r\n                                             path_components(compare_path))\r\n        diff_path_length = len(diff_path)\r\n        path_component = path_components(diff_path)\r\n        if (diff_path_length > 20 and len(path_component) > 2):\r\n            if path_component[0] != '/' and path_component[0] != '':\r\n                path_component = [path_component[0], '...', \r\n                                          path_component[-1]]\r\n            else:\r\n                path_component = [path_component[2], '...', \r\n                                          path_component[-1]]\r\n            diff_path = os.path.join(*path_component)\r\n        fname = fname + \" - \" + diff_path    \r\n    return fname", "response": "Get tab title without ambiguation."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget a list of the path components of the files with the same name.", "response": "def get_same_name_files(files_path_list, filename):\r\n    \"\"\"Get a list of the path components of the files with the same name.\"\"\"\r\n    same_name_files = []\r\n    for fname in files_path_list:\r\n        if filename == os.path.basename(fname):\r\n            same_name_files.append(path_components(fname))\r\n    return same_name_files"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add(self, decorations):\n        added = 0\n        if isinstance(decorations, list):\n            not_repeated = set(decorations) - set(self._decorations)\n            self._decorations.extend(list(not_repeated))\n            added = len(not_repeated)\n        elif decorations not in self._decorations:\n            self._decorations.append(decorations)\n            added = 1\n\n        if added > 0:\n            self._order_decorations()\n            self.update()\n        return added", "response": "Adds text decorations on a CodeEditor instance."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef remove(self, decoration):\n        try:\n            self._decorations.remove(decoration)\n            self.update()\n            return True\n        except ValueError:\n            return False\n        except RuntimeError:\n            # This is needed to fix issue 9173\n            pass", "response": "Removes a text decoration from the editor."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupdate editor extra selections with added decorations.", "response": "def update(self):\n        \"\"\"Update editor extra selections with added decorations.\n\n        NOTE: Update TextDecorations to use editor font, using a different\n        font family and point size could cause unwanted behaviors.\n        \"\"\"\n        font = self.editor.font()\n        for decoration in self._decorations:\n            try:\n                decoration.format.setFont(\n                        font, QTextCharFormat.FontPropertiesSpecifiedOnly)\n            except (TypeError, AttributeError):  # Qt < 5.3\n                decoration.format.setFontFamily(font.family())\n                decoration.format.setFontPointSize(font.pointSize())\n        self.editor.setExtraSelections(self._decorations)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _order_decorations(self):\n        def order_function(sel):\n            end = sel.cursor.selectionEnd()\n            start = sel.cursor.selectionStart()\n            return sel.draw_order, -(end - start)\n\n        self._decorations = sorted(self._decorations,\n                                   key=order_function)", "response": "Order decorations according to draw_order and size of selection."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_signature(self, content):\n        data = content.get('data', {})\n        text = data.get('text/plain', '')\n        if text:\n            text = ANSI_OR_SPECIAL_PATTERN.sub('', text)\n            self._control.current_prompt_pos = self._prompt_pos\n            line = self._control.get_current_line_to_cursor()\n            name = line[:-1].split('(')[-1]   # Take last token after a (\n            name = name.split('.')[-1]        # Then take last token after a .\n            # Clean name from invalid chars\n            try:\n                name = self.clean_invalid_var_chars(name).split('_')[-1]\n            except:\n                pass\n            argspec = getargspecfromtext(text)\n            if argspec:\n                # This covers cases like np.abs, whose docstring is\n                # the same as np.absolute and because of that a proper\n                # signature can't be obtained correctly\n                signature = name + argspec\n            else:\n                signature = getsignaturefromtext(text, name)\n\n            # Remove docstring for uniformity with editor\n            signature = signature.split('Docstring:')[0]\n\n            return signature\n        else:\n            return ''", "response": "Get signature from inspect reply content"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_defined(self, objtxt, force_import=False):\n        if self._reading:\n            return\n        wait_loop = QEventLoop()\n        self.sig_got_reply.connect(wait_loop.quit)\n        self.silent_exec_method(\n            \"get_ipython().kernel.is_defined('%s', force_import=%s)\"\n            % (objtxt, force_import))\n        wait_loop.exec_()\n\n        # Remove loop connection and loop\n        self.sig_got_reply.disconnect(wait_loop.quit)\n        wait_loop = None\n\n        return self._kernel_reply", "response": "Return True if object is defined"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_doc(self, objtxt):\n        if self._reading:\n            return\n        wait_loop = QEventLoop()\n        self.sig_got_reply.connect(wait_loop.quit)\n        self.silent_exec_method(\"get_ipython().kernel.get_doc('%s')\" % objtxt)\n        wait_loop.exec_()\n\n        # Remove loop connection and loop\n        self.sig_got_reply.disconnect(wait_loop.quit)\n        wait_loop = None\n\n        return self._kernel_reply", "response": "Get object documentation dictionary"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _handle_inspect_reply(self, rep):\n        cursor = self._get_cursor()\n        info = self._request_info.get('call_tip')\n        if info and info.id == rep['parent_header']['msg_id'] and \\\n          info.pos == cursor.position():\n            content = rep['content']\n            if content.get('status') == 'ok' and content.get('found', False):\n                signature = self.get_signature(content)\n                if signature:\n                    # TODO: Pass the language from the Console to the calltip\n                    self._control.show_calltip(signature, color='#999999',\n                                               is_python=True)", "response": "Reimplement call tips to only show signatures using the same\n        style from our Editor and External Console too"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _encode_json(obj):\n    '''\n    Encode object as json str.\n    '''\n    def _dump_obj(obj):\n        if isinstance(obj, dict):\n            return obj\n        d = dict()\n        for k in dir(obj):\n            if not k.startswith('_'):\n                d[k] = getattr(obj, k)\n        return d\n    return json.dumps(obj, default=_dump_obj)", "response": "Encode object as json str."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef authorize_url(self, state=None):\n        '''\n        Generate authorize_url.\n\n        >>> GitHub(client_id='3ebf94c5776d565bcf75').authorize_url()\n        'https://github.com/login/oauth/authorize?client_id=3ebf94c5776d565bcf75'\n        '''\n        if not self._client_id:\n            raise ApiAuthError('No client id.')\n        kw = dict(client_id=self._client_id)\n        if self._redirect_uri:\n            kw['redirect_uri'] = self._redirect_uri\n        if self._scope:\n            kw['scope'] = self._scope\n        if state:\n            kw['state'] = state\n        return 'https://github.com/login/oauth/authorize?%s' % _encode_params(kw)", "response": "Generate authorize_url.\n\n        >>> GitHub(client_id='3ebf94c5776d565bcf75').authorize_url()\n        'https://github.com/login/oauth/authorize?client_id=3ebf94c5776d565bcf75'"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting an access token.", "response": "def get_access_token(self, code, state=None):\n        '''\n        In callback url: http://host/callback?code=123&state=xyz\n\n        use code and state to get an access token.\n        '''\n        kw = dict(client_id=self._client_id, client_secret=self._client_secret, code=code)\n        if self._redirect_uri:\n            kw['redirect_uri'] = self._redirect_uri\n        if state:\n            kw['state'] = state\n        opener = build_opener(HTTPSHandler)\n        request = Request('https://github.com/login/oauth/access_token', data=_encode_params(kw))\n        request.get_method = _METHOD_MAP['POST']\n        request.add_header('Accept', 'application/json')\n        try:\n            response = opener.open(request, timeout=TIMEOUT)\n            r = _parse_json(response.read())\n            if 'error' in r:\n                raise ApiAuthError(str(r.error))\n            return str(r.access_token)\n        except HTTPError as e:\n            raise ApiAuthError('HTTPError when get access token')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef send_request(req=None, method=None, requires_response=True):\n    if req is None:\n        return functools.partial(send_request, method=method,\n                                 requires_response=requires_response)\n\n    @functools.wraps(req)\n    def wrapper(self, *args, **kwargs):\n        params = req(self, *args, **kwargs)\n        _id = self.send(method, params, requires_response)\n        return _id\n    wrapper._sends = method\n    return wrapper", "response": "Decorator that returns a function that calls req and then sends its results via ZMQ."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef class_register(cls):\n    cls.handler_registry = {}\n    cls.sender_registry = {}\n    for method_name in dir(cls):\n        method = getattr(cls, method_name)\n        if hasattr(method, '_handle'):\n            cls.handler_registry.update({method._handle: method_name})\n        if hasattr(method, '_sends'):\n            cls.sender_registry.update({method._sends: method_name})\n    return cls", "response": "Class decorator that allows to map LSP method names to class methods."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rowCount(self, index=QModelIndex()):\r\n        if self.total_rows <= self.rows_loaded:\r\n            return self.total_rows\r\n        else:\r\n            return self.rows_loaded", "response": "Returns the number of rows in the table."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_value(self, index):\r\n        if index.column() == 0:\r\n            return self.keys[ index.row() ]\r\n        elif index.column() == 1:\r\n            return self.types[ index.row() ]\r\n        elif index.column() == 2:\r\n            return self.sizes[ index.row() ]\r\n        else:\r\n            return self._data[ self.keys[index.row()] ]", "response": "Return the value of the given index"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_bgcolor(self, index):\r\n        if index.column() == 0:\r\n            color = QColor(Qt.lightGray)\r\n            color.setAlphaF(.05)\r\n        elif index.column() < 3:\r\n            color = QColor(Qt.lightGray)\r\n            color.setAlphaF(.2)\r\n        else:\r\n            color = QColor(Qt.lightGray)\r\n            color.setAlphaF(.3)\r\n        return color", "response": "Returns the background color depending on value"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef data(self, index, role=Qt.DisplayRole):\r\n        if not index.isValid():\r\n            return to_qvariant()\r\n        value = self.get_value(index)\r\n        if index.column() == 3 and self.remote:\r\n            value = value['view']\r\n        if index.column() == 3:\r\n            display = value_to_display(value, minmax=self.minmax)\r\n        else:\r\n            if is_type_text_string(value):\r\n                display = to_text_string(value, encoding=\"utf-8\")\r\n            else:\r\n                display = to_text_string(value)\r\n        if role == Qt.DisplayRole:\r\n            return to_qvariant(display)\r\n        elif role == Qt.EditRole:\r\n            return to_qvariant(value_to_display(value))\r\n        elif role == Qt.TextAlignmentRole:\r\n            if index.column() == 3:\r\n                if len(display.splitlines()) < 3:\r\n                    return to_qvariant(int(Qt.AlignLeft|Qt.AlignVCenter))\r\n                else:\r\n                    return to_qvariant(int(Qt.AlignLeft|Qt.AlignTop))\r\n            else:\r\n                return to_qvariant(int(Qt.AlignLeft|Qt.AlignVCenter))\r\n        elif role == Qt.BackgroundColorRole:\r\n            return to_qvariant( self.get_bgcolor(index) )\r\n        elif role == Qt.FontRole:\r\n            return to_qvariant(get_font(font_size_delta=DEFAULT_SMALL_DELTA))\r\n        return to_qvariant()", "response": "Returns the data for the item at the given index"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets value of the column at the given index", "response": "def set_value(self, index, value):\r\n        \"\"\"Set value\"\"\"\r\n        self._data[ self.keys[index.row()] ] = value\r\n        self.showndata[ self.keys[index.row()] ] = value\r\n        self.sizes[index.row()] = get_size(value)\r\n        self.types[index.row()] = get_human_readable_type(value)\r\n        self.sig_setting_data.emit()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_bgcolor(self, index):\r\n        value = self.get_value(index)\r\n        if index.column() < 3:\r\n            color = ReadOnlyCollectionsModel.get_bgcolor(self, index)\r\n        else:\r\n            if self.remote:\r\n                color_name = value['color']\r\n            else:\r\n                color_name = get_color_name(value)\r\n            color = QColor(color_name)\r\n            color.setAlphaF(.2)\r\n        return color", "response": "Get background color depending on value"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the value of the item at the given index.", "response": "def setData(self, index, value, role=Qt.EditRole):\r\n        \"\"\"Cell content change\"\"\"\r\n        if not index.isValid():\r\n            return False\r\n        if index.column() < 3:\r\n            return False\r\n        value = display_to_value(value, self.get_value(index),\r\n                                 ignore_errors=True)\r\n        self.set_value(index, value)\r\n        self.dataChanged.emit(index, index)\r\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef show_warning(self, index):\r\n        try:\r\n            val_size = index.model().sizes[index.row()]\r\n            val_type = index.model().types[index.row()]\r\n        except:\r\n            return False\r\n        if val_type in ['list', 'set', 'tuple', 'dict'] and \\\r\n                int(val_size) > 1e5:\r\n            return True\r\n        else:\r\n            return False", "response": "Returns True if the user is trying to view a big variable associated to a Tablemodel index False otherwise"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef setEditorData(self, editor, index):\r\n        value = self.get_value(index)\r\n        if isinstance(editor, QLineEdit):\r\n            if is_binary_string(value):\r\n                try:\r\n                    value = to_text_string(value, 'utf8')\r\n                except:\r\n                    pass\r\n            if not is_text_string(value):\r\n                value = repr(value)\r\n            editor.setText(value)\r\n        elif isinstance(editor, QDateEdit):\r\n            editor.setDate(value)\r\n        elif isinstance(editor, QDateTimeEdit):\r\n            editor.setDateTime(QDateTime(value.date(), value.time()))", "response": "Overriding method setEditorData\r\n            Model --> Editor"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef setModelData(self, editor, model, index):\r\n        if not hasattr(model, \"set_value\"):\r\n            # Read-only mode\r\n            return\r\n        \r\n        if isinstance(editor, QLineEdit):\r\n            value = editor.text()\r\n            try:\r\n                value = display_to_value(to_qvariant(value),\r\n                                         self.get_value(index),\r\n                                         ignore_errors=False)\r\n            except Exception as msg:\r\n                raise\r\n                QMessageBox.critical(editor, _(\"Edit item\"),\r\n                                     _(\"<b>Unable to assign data to item.</b>\"\r\n                                       \"<br><br>Error message:<br>%s\"\r\n                                       ) % str(msg))\r\n                return\r\n        elif isinstance(editor, QDateEdit):\r\n            qdate = editor.date()\r\n            value = datetime.date( qdate.year(), qdate.month(), qdate.day() )\r\n        elif isinstance(editor, QDateTimeEdit):\r\n            qdatetime = editor.dateTime()\r\n            qdate = qdatetime.date()\r\n            qtime = qdatetime.time()\r\n            value = datetime.datetime( qdate.year(), qdate.month(),\r\n                                       qdate.day(), qtime.hour(),\r\n                                       qtime.minute(), qtime.second() )\r\n        else:\r\n            # Should not happen...\r\n            raise RuntimeError(\"Unsupported editor widget\")\r\n        self.set_value(index, value)", "response": "Override setModelData with a different editor widget and model"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nallowing user to drag files", "response": "def dragEnterEvent(self, event):\r\n        \"\"\"Allow user to drag files\"\"\"\r\n        if mimedata2url(event.mimeData()):\r\n            event.accept()\r\n        else:\r\n            event.ignore()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dragMoveEvent(self, event):\r\n        if mimedata2url(event.mimeData()):\r\n            event.setDropAction(Qt.CopyAction)\r\n            event.accept()\r\n        else:\r\n            event.ignore()", "response": "Allow user to move files"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dropEvent(self, event):\r\n        urls = mimedata2url(event.mimeData())\r\n        if urls:\r\n            event.setDropAction(Qt.CopyAction)\r\n            event.accept()\r\n            self.sig_files_dropped.emit(urls)\r\n        else:\r\n            event.ignore()", "response": "Allow user to drop supported files"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef toggle_minmax(self, state):\r\n        self.sig_option_changed.emit('minmax', state)\r\n        self.model.minmax = state", "response": "Toggle min max display for numpy arrays"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets format of the dataframe in the current model", "response": "def set_dataframe_format(self, new_format):\r\n        \"\"\"\r\n        Set format to use in DataframeEditor.\r\n\r\n        Args:\r\n            new_format (string): e.g. \"%.3f\"\r\n        \"\"\"\r\n        self.sig_option_changed.emit('dataframe_format', new_format)\r\n        self.model.dataframe_format = new_format"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef edit_item(self):\r\n        index = self.currentIndex()\r\n        if not index.isValid():\r\n            return\r\n        # TODO: Remove hard coded \"Value\" column number (3 here)\r\n        self.edit(index.child(index.row(), 3))", "response": "Edit the item in the tree."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nremove item from the list", "response": "def remove_item(self):\r\n        \"\"\"Remove item\"\"\"\r\n        indexes = self.selectedIndexes()\r\n        if not indexes:\r\n            return\r\n        for index in indexes:\r\n            if not index.isValid():\r\n                return\r\n        one = _(\"Do you want to remove the selected item?\")\r\n        more = _(\"Do you want to remove all selected items?\")\r\n        answer = QMessageBox.question(self, _( \"Remove\"),\r\n                                      one if len(indexes) == 1 else more,\r\n                                      QMessageBox.Yes | QMessageBox.No)\r\n        if answer == QMessageBox.Yes:\r\n            idx_rows = unsorted_unique([idx.row() for idx in indexes])\r\n            keys = [ self.model.keys[idx_row] for idx_row in idx_rows ]\r\n            self.remove_values(keys)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef copy_item(self, erase_original=False):\r\n        indexes = self.selectedIndexes()\r\n        if not indexes:\r\n            return\r\n        idx_rows = unsorted_unique([idx.row() for idx in indexes])\r\n        if len(idx_rows) > 1 or not indexes[0].isValid():\r\n            return\r\n        orig_key = self.model.keys[idx_rows[0]]\r\n        if erase_original:\r\n            title = _('Rename')\r\n            field_text = _('New variable name:')\r\n        else:\r\n            title = _('Duplicate')\r\n            field_text = _('Variable name:')\r\n        data = self.model.get_data()\r\n        if isinstance(data, (list, set)):\r\n            new_key, valid = len(data), True\r\n        else:\r\n            new_key, valid = QInputDialog.getText(self, title, field_text,\r\n                                                  QLineEdit.Normal, orig_key)\r\n        if valid and to_text_string(new_key):\r\n            new_key = try_to_eval(to_text_string(new_key))\r\n            if new_key == orig_key:\r\n                return\r\n            self.copy_value(orig_key, new_key)\r\n            if erase_original:\r\n                self.remove_values([orig_key])", "response": "Copy item from selected index to selected item."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef insert_item(self):\r\n        index = self.currentIndex()\r\n        if not index.isValid():\r\n            row = self.model.rowCount()\r\n        else:\r\n            row = index.row()\r\n        data = self.model.get_data()\r\n        if isinstance(data, list):\r\n            key = row\r\n            data.insert(row, '')\r\n        elif isinstance(data, dict):\r\n            key, valid = QInputDialog.getText(self, _( 'Insert'), _( 'Key:'),\r\n                                              QLineEdit.Normal)\r\n            if valid and to_text_string(key):\r\n                key = try_to_eval(to_text_string(key))\r\n            else:\r\n                return\r\n        else:\r\n            return\r\n        value, valid = QInputDialog.getText(self, _('Insert'), _('Value:'),\r\n                                            QLineEdit.Normal)\r\n        if valid and to_text_string(value):\r\n            self.new_value(key, try_to_eval(to_text_string(value)))", "response": "Insert item in the model"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef plot_item(self, funcname):\r\n        index = self.currentIndex()\r\n        if self.__prepare_plot():\r\n            key = self.model.get_key(index)\r\n            try:\r\n                self.plot(key, funcname)\r\n            except (ValueError, TypeError) as error:\r\n                QMessageBox.critical(self, _( \"Plot\"),\r\n                                     _(\"<b>Unable to plot data.</b>\"\r\n                                       \"<br><br>Error message:<br>%s\"\r\n                                       ) % str(error))", "response": "Plot the item of the current item"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsave array of all the related objects in the current directory.", "response": "def save_array(self):\r\n        \"\"\"Save array\"\"\"\r\n        title = _( \"Save array\")\r\n        if self.array_filename is None:\r\n            self.array_filename = getcwd_or_home()\r\n        self.redirect_stdio.emit(False)\r\n        filename, _selfilter = getsavefilename(self, title,\r\n                                               self.array_filename,\r\n                                               _(\"NumPy arrays\")+\" (*.npy)\")\r\n        self.redirect_stdio.emit(True)\r\n        if filename:\r\n            self.array_filename = filename\r\n            data = self.delegate.get_value( self.currentIndex() )\r\n            try:\r\n                import numpy as np\r\n                np.save(self.array_filename, data)\r\n            except Exception as error:\r\n                QMessageBox.critical(self, title,\r\n                                     _(\"<b>Unable to save array</b>\"\r\n                                       \"<br><br>Error message:<br>%s\"\r\n                                       ) % str(error))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef copy(self):\r\n        clipboard = QApplication.clipboard()\r\n        clipl = []\r\n        for idx in self.selectedIndexes():\r\n            if not idx.isValid():\r\n                continue\r\n            obj = self.delegate.get_value(idx)\r\n            # Check if we are trying to copy a numpy array, and if so make sure\r\n            # to copy the whole thing in a tab separated format\r\n            if isinstance(obj, (ndarray, MaskedArray)) \\\r\n              and ndarray is not FakeObject:\r\n                if PY3:\r\n                    output = io.BytesIO()\r\n                else:\r\n                    output = io.StringIO()\r\n                try:\r\n                    np_savetxt(output, obj, delimiter='\\t')\r\n                except:\r\n                    QMessageBox.warning(self, _(\"Warning\"),\r\n                                        _(\"It was not possible to copy \"\r\n                                          \"this array\"))\r\n                    return\r\n                obj = output.getvalue().decode('utf-8')\r\n                output.close()\r\n            elif isinstance(obj, (DataFrame, Series)) \\\r\n              and DataFrame is not FakeObject:\r\n                output = io.StringIO()\r\n                try:\r\n                    obj.to_csv(output, sep='\\t', index=True, header=True)\r\n                except Exception:\r\n                    QMessageBox.warning(self, _(\"Warning\"),\r\n                                        _(\"It was not possible to copy \"\r\n                                          \"this dataframe\"))\r\n                    return\r\n                if PY3:\r\n                    obj = output.getvalue()\r\n                else:\r\n                    obj = output.getvalue().decode('utf-8')\r\n                output.close()\r\n            elif is_binary_string(obj):\r\n                obj = to_text_string(obj, 'utf8')\r\n            else:\r\n                obj = to_text_string(obj)\r\n            clipl.append(obj)\r\n        clipboard.setText('\\n'.join(clipl))", "response": "Copy text to clipboard"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef import_from_string(self, text, title=None):\r\n        data = self.model.get_data()\r\n        # Check if data is a dict\r\n        if not hasattr(data, \"keys\"):\r\n            return\r\n        editor = ImportWizard(self, text, title=title,\r\n                              contents_title=_(\"Clipboard contents\"),\r\n                              varname=fix_reference_name(\"data\",\r\n                                                         blacklist=list(data.keys())))\r\n        if editor.exec_():\r\n            var_name, clip_data = editor.get_data()\r\n            self.new_value(var_name, clip_data)", "response": "Import data from string"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef paste(self):\r\n        clipboard = QApplication.clipboard()\r\n        cliptext = ''\r\n        if clipboard.mimeData().hasText():\r\n            cliptext = to_text_string(clipboard.text())\r\n        if cliptext.strip():\r\n            self.import_from_string(cliptext, title=_(\"Import from clipboard\"))\r\n        else:\r\n            QMessageBox.warning(self, _( \"Empty clipboard\"),\r\n                                _(\"Nothing to be imported from clipboard.\"))", "response": "Pastes the text from the clipboard and imports it into the current object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef remove_values(self, keys):\r\n        data = self.model.get_data()\r\n        for key in sorted(keys, reverse=True):\r\n            data.pop(key)\r\n            self.set_data(data)", "response": "Remove values from data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncopy value from one object to another.", "response": "def copy_value(self, orig_key, new_key):\r\n        \"\"\"Copy value\"\"\"\r\n        data = self.model.get_data()\r\n        if isinstance(data, list):\r\n            data.append(data[orig_key])\r\n        if isinstance(data, set):\r\n            data.add(data[orig_key])\r\n        else:\r\n            data[new_key] = data[orig_key]\r\n        self.set_data(data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef new_value(self, key, value):\r\n        data = self.model.get_data()\r\n        data[key] = value\r\n        self.set_data(data)", "response": "Create a new value in the data"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning True if variable is a list or a tuple", "response": "def is_list(self, key):\r\n        \"\"\"Return True if variable is a list or a tuple\"\"\"\r\n        data = self.model.get_data()\r\n        return isinstance(data[key], (tuple, list))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_set(self, key):\r\n        data = self.model.get_data()\r\n        return isinstance(data[key], set)", "response": "Return True if variable is a set"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn True if variable is a numpy array", "response": "def is_array(self, key):\r\n        \"\"\"Return True if variable is a numpy array\"\"\"\r\n        data = self.model.get_data()\r\n        return isinstance(data[key], (ndarray, MaskedArray))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_image(self, key):\r\n        data = self.model.get_data()\r\n        return isinstance(data[key], Image)", "response": "Return True if variable is a PIL. Image"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_dict(self, key):\r\n        data = self.model.get_data()\r\n        return isinstance(data[key], dict)", "response": "Return True if variable is a dictionary"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_array_shape(self, key):\r\n        data = self.model.get_data()\r\n        return data[key].shape", "response": "Return array s shape"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_array_ndim(self, key):\r\n        data = self.model.get_data()\r\n        return data[key].ndim", "response": "Return array s ndim"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nediting item with given key", "response": "def oedit(self, key):\r\n        \"\"\"Edit item\"\"\"\r\n        data = self.model.get_data()\r\n        from spyder.plugins.variableexplorer.widgets.objecteditor import (\r\n                oedit)\r\n        oedit(data[key])"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nshows item s image", "response": "def imshow(self, key):\r\n        \"\"\"Show item's image\"\"\"\r\n        data = self.model.get_data()\r\n        import spyder.pyplot as plt\r\n        plt.figure()\r\n        plt.imshow(data[key])\r\n        plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nshowing image with key", "response": "def show_image(self, key):\r\n        \"\"\"Show image (item is a PIL image)\"\"\"\r\n        data = self.model.get_data()\r\n        data[key].show()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef save_and_close_enable(self):\r\n        if self.btn_save_and_close:\r\n            self.btn_save_and_close.setEnabled(True)\r\n            self.btn_save_and_close.setAutoDefault(True)\r\n            self.btn_save_and_close.setDefault(True)", "response": "Handle the data change event to enable the save and close button."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_value(self, name):\r\n        value = self.shellwidget.get_value(name)\r\n        # Reset temporal variable where value is saved to\r\n        # save memory\r\n        self.shellwidget._kernel_value = None\r\n        return value", "response": "Get the value of a variable"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a new value in data", "response": "def new_value(self, name, value):\r\n        \"\"\"Create new value in data\"\"\"\r\n        try:\r\n            # We need to enclose values in a list to be able to send\r\n            # them to the kernel in Python 2\r\n            svalue = [cloudpickle.dumps(value, protocol=PICKLE_PROTOCOL)]\r\n\r\n            # Needed to prevent memory leaks. See issue 7158\r\n            if len(svalue) < MAX_SERIALIZED_LENGHT:\r\n                self.shellwidget.set_value(name, svalue)\r\n            else:\r\n                QMessageBox.warning(self, _(\"Warning\"),\r\n                                    _(\"The object you are trying to modify is \"\r\n                                      \"too big to be sent back to the kernel. \"\r\n                                      \"Therefore, your modifications won't \"\r\n                                      \"take place.\"))\r\n        except TypeError as e:\r\n            QMessageBox.critical(self, _(\"Error\"),\r\n                                 \"TypeError: %s\" % to_text_string(e))\r\n        self.shellwidget.refresh_namespacebrowser()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves values from data", "response": "def remove_values(self, names):\r\n        \"\"\"Remove values from data\"\"\"\r\n        for name in names:\r\n            self.shellwidget.remove_value(name)\r\n        self.shellwidget.refresh_namespacebrowser()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncopying value from the current namespace browser to the new namespace browser", "response": "def copy_value(self, orig_name, new_name):\r\n        \"\"\"Copy value\"\"\"\r\n        self.shellwidget.copy_value(orig_name, new_name)\r\n        self.shellwidget.refresh_namespacebrowser()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef plot(self, name, funcname):\r\n        sw = self.shellwidget\r\n        if sw._reading:\r\n            sw.dbg_exec_magic('varexp', '--%s %s' % (funcname, name))\r\n        else:\r\n            sw.execute(\"%%varexp --%s %s\" % (funcname, name))", "response": "Plot a single item"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef imshow(self, name):\r\n        sw = self.shellwidget\r\n        if sw._reading:\r\n            sw.dbg_exec_magic('varexp', '--imshow %s' % name)\r\n        else:\r\n            sw.execute(\"%%varexp --imshow %s\" % name)", "response": "Show item s image"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nshows image in the shell widget", "response": "def show_image(self, name):\r\n        \"\"\"Show image (item is a PIL image)\"\"\"\r\n        command = \"%s.show()\" % name\r\n        sw = self.shellwidget\r\n        if sw._reading:\r\n            sw.kernel_client.input(command)\r\n        else:\r\n            sw.execute(command)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting kernel id from the connection file", "response": "def kernel_id(self):\r\n        \"\"\"Get kernel id\"\"\"\r\n        if self.connection_file is not None:\r\n            json_file = osp.basename(self.connection_file)\r\n            return json_file.split('.json')[0]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef stderr_file(self):\r\n        stderr_file = None\r\n        if self.connection_file is not None:\r\n            stderr_file = self.kernel_id + '.stderr'\r\n            if self.stderr_dir is not None:\r\n                stderr_file = osp.join(self.stderr_dir, stderr_file)\r\n            else:\r\n                try:\r\n                    stderr_file = osp.join(get_temp_dir(), stderr_file)\r\n                except (IOError, OSError):\r\n                    stderr_file = None\r\n        return stderr_file", "response": "Returns the filename to save kernel stderr output."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef stderr_handle(self):\r\n        if self.stderr_file is not None:\r\n            # Needed to prevent any error that could appear.\r\n            # See issue 6267\r\n            try:\r\n                handle = codecs.open(self.stderr_file, 'w', encoding='utf-8')\r\n            except Exception:\r\n                handle = None\r\n        else:\r\n            handle = None\r\n\r\n        return handle", "response": "Get handle to stderr_file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove stderr_file associated with the client.", "response": "def remove_stderr_file(self):\r\n        \"\"\"Remove stderr_file associated with the client.\"\"\"\r\n        try:\r\n            # Defer closing the stderr_handle until the client\r\n            # is closed because jupyter_client needs it open\r\n            # while it tries to restart the kernel\r\n            self.stderr_handle.close()\r\n            os.remove(self.stderr_file)\r\n        except Exception:\r\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconfigures the shellwidget after kernel is started", "response": "def configure_shellwidget(self, give_focus=True):\r\n        \"\"\"Configure shellwidget after kernel is started\"\"\"\r\n        if give_focus:\r\n            self.get_control().setFocus()\r\n\r\n        # Set exit callback\r\n        self.shellwidget.set_exit_callback()\r\n\r\n        # To save history\r\n        self.shellwidget.executing.connect(self.add_to_history)\r\n\r\n        # For Mayavi to run correctly\r\n        self.shellwidget.executing.connect(\r\n            self.shellwidget.set_backend_for_mayavi)\r\n\r\n        # To update history after execution\r\n        self.shellwidget.executed.connect(self.update_history)\r\n\r\n        # To update the Variable Explorer after execution\r\n        self.shellwidget.executed.connect(\r\n            self.shellwidget.refresh_namespacebrowser)\r\n\r\n        # To enable the stop button when executing a process\r\n        self.shellwidget.executing.connect(self.enable_stop_button)\r\n\r\n        # To disable the stop button after execution stopped\r\n        self.shellwidget.executed.connect(self.disable_stop_button)\r\n\r\n        # To show kernel restarted/died messages\r\n        self.shellwidget.sig_kernel_restarted.connect(\r\n            self.kernel_restarted_message)\r\n\r\n        # To correctly change Matplotlib backend interactively\r\n        self.shellwidget.executing.connect(\r\n            self.shellwidget.change_mpl_backend)\r\n\r\n        # To show env and sys.path contents\r\n        self.shellwidget.sig_show_syspath.connect(self.show_syspath)\r\n        self.shellwidget.sig_show_env.connect(self.show_env)\r\n\r\n        # To sync with working directory toolbar\r\n        self.shellwidget.executed.connect(self.shellwidget.get_cwd)\r\n\r\n        # To apply style\r\n        self.set_color_scheme(self.shellwidget.syntax_style, reset=False)\r\n\r\n        # To hide the loading page\r\n        self.shellwidget.sig_prompt_ready.connect(self._hide_loading_page)\r\n\r\n        # Show possible errors when setting Matplotlib backend\r\n        self.shellwidget.sig_prompt_ready.connect(\r\n            self._show_mpl_backend_errors)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nshow kernel initialization errors in infowidget.", "response": "def show_kernel_error(self, error):\r\n        \"\"\"Show kernel initialization errors in infowidget.\"\"\"\r\n        # Replace end of line chars with <br>\r\n        eol = sourcecode.get_eol_chars(error)\r\n        if eol:\r\n            error = error.replace(eol, '<br>')\r\n\r\n        # Don't break lines in hyphens\r\n        # From https://stackoverflow.com/q/7691569/438386\r\n        error = error.replace('-', '&#8209')\r\n\r\n        # Create error page\r\n        message = _(\"An error ocurred while starting the kernel\")\r\n        kernel_error_template = Template(KERNEL_ERROR)\r\n        self.info_page = kernel_error_template.substitute(\r\n            css_path=self.css_path,\r\n            message=message,\r\n            error=error)\r\n\r\n        # Show error\r\n        self.set_info_page()\r\n        self.shellwidget.hide()\r\n        self.infowidget.show()\r\n\r\n        # Tell the client we're in error mode\r\n        self.is_error_shown = True"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the name of the client", "response": "def get_name(self):\r\n        \"\"\"Return client name\"\"\"\r\n        if self.given_name is None:\r\n            # Name according to host\r\n            if self.hostname is None:\r\n                name = _(\"Console\")\r\n            else:\r\n                name = self.hostname\r\n            # Adding id to name\r\n            client_id = self.id_['int_id'] + u'/' + self.id_['str_id']\r\n            name = name + u' ' + client_id\r\n        elif self.given_name in [\"Pylab\", \"SymPy\", \"Cython\"]:\r\n            client_id = self.id_['int_id'] + u'/' + self.id_['str_id']\r\n            name = self.given_name + u' ' + client_id\r\n        else:\r\n            name = self.given_name + u'/' + self.id_['str_id']\r\n        return name"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the text widget or similar to give focus to", "response": "def get_control(self):\r\n        \"\"\"Return the text widget (or similar) to give focus to\"\"\"\r\n        # page_control is the widget used for paging\r\n        page_control = self.shellwidget._page_control\r\n        if page_control and page_control.isVisible():\r\n            return page_control\r\n        else:\r\n            return self.shellwidget._control"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the options menu", "response": "def get_options_menu(self):\r\n        \"\"\"Return options menu\"\"\"\r\n        env_action = create_action(\r\n                        self,\r\n                        _(\"Show environment variables\"),\r\n                        icon=ima.icon('environ'),\r\n                        triggered=self.shellwidget.get_env\r\n                     )\r\n\r\n        syspath_action = create_action(\r\n                            self,\r\n                            _(\"Show sys.path contents\"),\r\n                            icon=ima.icon('syspath'),\r\n                            triggered=self.shellwidget.get_syspath\r\n                         )\r\n\r\n        self.show_time_action.setChecked(self.show_elapsed_time)\r\n        additional_actions = [MENU_SEPARATOR,\r\n                              env_action,\r\n                              syspath_action,\r\n                              self.show_time_action]\r\n\r\n        if self.menu_actions is not None:\r\n            console_menu = self.menu_actions + additional_actions\r\n            return console_menu\r\n\r\n        else:\r\n            return additional_actions"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of toolbar buttons.", "response": "def get_toolbar_buttons(self):\r\n        \"\"\"Return toolbar buttons list.\"\"\"\r\n        buttons = []\r\n\r\n        # Code to add the stop button\r\n        if self.stop_button is None:\r\n            self.stop_button = create_toolbutton(\r\n                                   self,\r\n                                   text=_(\"Stop\"),\r\n                                   icon=self.stop_icon,\r\n                                   tip=_(\"Stop the current command\"))\r\n            self.disable_stop_button()\r\n            # set click event handler\r\n            self.stop_button.clicked.connect(self.stop_button_click_handler)\r\n            if is_dark_interface():\r\n                self.stop_button.setStyleSheet(\"QToolButton{padding: 3px;}\")\r\n        if self.stop_button is not None:\r\n            buttons.append(self.stop_button)\r\n\r\n        # Reset namespace button\r\n        if self.reset_button is None:\r\n            self.reset_button = create_toolbutton(\r\n                                    self,\r\n                                    text=_(\"Remove\"),\r\n                                    icon=ima.icon('editdelete'),\r\n                                    tip=_(\"Remove all variables\"),\r\n                                    triggered=self.reset_namespace)\r\n            if is_dark_interface():\r\n                self.reset_button.setStyleSheet(\"QToolButton{padding: 3px;}\")\r\n        if self.reset_button is not None:\r\n            buttons.append(self.reset_button)\r\n\r\n        if self.options_button is None:\r\n            options = self.get_options_menu()\r\n            if options:\r\n                self.options_button = create_toolbutton(self,\r\n                        text=_('Options'), icon=ima.icon('tooloptions'))\r\n                self.options_button.setPopupMode(QToolButton.InstantPopup)\r\n                menu = QMenu(self)\r\n                add_actions(menu, options)\r\n                self.options_button.setMenu(menu)\r\n        if self.options_button is not None:\r\n            buttons.append(self.options_button)\r\n\r\n        return buttons"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_actions_to_context_menu(self, menu):\r\n        inspect_action = create_action(self, _(\"Inspect current object\"),\r\n                                    QKeySequence(get_shortcut('console',\r\n                                                    'inspect current object')),\r\n                                    icon=ima.icon('MessageBoxInformation'),\r\n                                    triggered=self.inspect_object)\r\n\r\n        clear_line_action = create_action(self, _(\"Clear line or block\"),\r\n                                          QKeySequence(get_shortcut(\r\n                                                  'console',\r\n                                                  'clear line')),\r\n                                          triggered=self.clear_line)\r\n\r\n        reset_namespace_action = create_action(self, _(\"Remove all variables\"),\r\n                                               QKeySequence(get_shortcut(\r\n                                                       'ipython_console',\r\n                                                       'reset namespace')),\r\n                                               icon=ima.icon('editdelete'),\r\n                                               triggered=self.reset_namespace)\r\n\r\n        clear_console_action = create_action(self, _(\"Clear console\"),\r\n                                             QKeySequence(get_shortcut('console',\r\n                                                               'clear shell')),\r\n                                             triggered=self.clear_console)\r\n\r\n        quit_action = create_action(self, _(\"&Quit\"), icon=ima.icon('exit'),\r\n                                    triggered=self.exit_callback)\r\n\r\n        add_actions(menu, (None, inspect_action, clear_line_action,\r\n                           clear_console_action, reset_namespace_action,\r\n                           None, quit_action))\r\n        return menu", "response": "Add actions to IPython context menu"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_font(self, font):\r\n        self.shellwidget._control.setFont(font)\r\n        self.shellwidget.font = font", "response": "Set IPython widget s font"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset IPython color scheme.", "response": "def set_color_scheme(self, color_scheme, reset=True):\r\n        \"\"\"Set IPython color scheme.\"\"\"\r\n        # Needed to handle not initialized kernel_client\r\n        # See issue 6996\r\n        try:\r\n            self.shellwidget.set_color_scheme(color_scheme, reset)\r\n        except AttributeError:\r\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef restart_kernel(self):\r\n        sw = self.shellwidget\r\n\r\n        if not running_under_pytest() and self.ask_before_restart:\r\n            message = _('Are you sure you want to restart the kernel?')\r\n            buttons = QMessageBox.Yes | QMessageBox.No\r\n            result = QMessageBox.question(self, _('Restart kernel?'),\r\n                                          message, buttons)\r\n        else:\r\n            result = None\r\n\r\n        if (result == QMessageBox.Yes or\r\n                running_under_pytest() or\r\n                not self.ask_before_restart):\r\n            if sw.kernel_manager:\r\n                if self.infowidget.isVisible():\r\n                    self.infowidget.hide()\r\n                    sw.show()\r\n                try:\r\n                    sw.kernel_manager.restart_kernel(\r\n                        stderr=self.stderr_handle)\r\n                except RuntimeError as e:\r\n                    sw._append_plain_text(\r\n                        _('Error restarting kernel: %s\\n') % e,\r\n                        before_prompt=True\r\n                    )\r\n                else:\r\n                    # For issue 6235.  IPython was changing the setting of\r\n                    # %colors on windows by assuming it was using a dark\r\n                    # background.  This corrects it based on the scheme.\r\n                    self.set_color_scheme(sw.syntax_style)\r\n                    sw._append_html(_(\"<br>Restarting kernel...\\n<hr><br>\"),\r\n                                    before_prompt=False)\r\n            else:\r\n                sw._append_plain_text(\r\n                    _('Cannot restart a kernel not started by Spyder\\n'),\r\n                    before_prompt=True\r\n                )", "response": "Restart the associated kernel."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nshow kernel restarted messages.", "response": "def kernel_restarted_message(self, msg):\r\n        \"\"\"Show kernel restarted/died messages.\"\"\"\r\n        if not self.is_error_shown:\r\n            # If there are kernel creation errors, jupyter_client will\r\n            # try to restart the kernel and qtconsole prints a\r\n            # message about it.\r\n            # So we read the kernel's stderr_file and display its\r\n            # contents in the client instead of the usual message shown\r\n            # by qtconsole.\r\n            try:\r\n                stderr = self._read_stderr()\r\n            except Exception:\r\n                stderr = None\r\n            if stderr:\r\n                self.show_kernel_error('<tt>%s</tt>' % stderr)\r\n        else:\r\n            self.shellwidget._append_html(\"<br>%s<hr><br>\" % msg,\r\n                                          before_prompt=False)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreset the namespace by removing all names defined by the user", "response": "def reset_namespace(self):\r\n        \"\"\"Resets the namespace by removing all names defined by the user\"\"\"\r\n        self.shellwidget.reset_namespace(warning=self.reset_warning,\r\n                                         message=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nshows sys. path contents.", "response": "def show_syspath(self, syspath):\r\n        \"\"\"Show sys.path contents.\"\"\"\r\n        if syspath is not None:\r\n            editor = CollectionsEditor(self)\r\n            editor.setup(syspath, title=\"sys.path contents\", readonly=True,\r\n                         width=600, icon=ima.icon('syspath'))\r\n            self.dialog_manager.show(editor)\r\n        else:\r\n            return"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef show_time(self, end=False):\r\n        if self.time_label is None:\r\n            return\r\n\r\n        elapsed_time = time.monotonic() - self.t0\r\n        # System time changed to past date, so reset start.\r\n        if elapsed_time < 0:\r\n            self.t0 = time.monotonic()\r\n            elapsed_time = 0\r\n        if elapsed_time > 24 * 3600:  # More than a day...!\r\n            fmt = \"%d %H:%M:%S\"\r\n        else:\r\n            fmt = \"%H:%M:%S\"\r\n        if end:\r\n            color = \"#AAAAAA\"\r\n        else:\r\n            color = \"#AA6655\"\r\n        text = \"<span style=\\'color: %s\\'><b>%s\" \\\r\n               \"</b></span>\" % (color,\r\n                                time.strftime(fmt, time.gmtime(elapsed_time)))\r\n        self.time_label.setText(text)", "response": "Text to show in time_label."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nslotting to show or hide elapsed time label.", "response": "def set_elapsed_time_visible(self, state):\r\n        \"\"\"Slot to show/hide elapsed time label.\"\"\"\r\n        self.show_elapsed_time = state\r\n        if self.time_label is not None:\r\n            self.time_label.setVisible(state)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset current info page.", "response": "def set_info_page(self):\r\n        \"\"\"Set current info_page.\"\"\"\r\n        if self.info_page is not None:\r\n            self.infowidget.setHtml(\r\n                self.info_page,\r\n                QUrl.fromLocalFile(self.css_path)\r\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates html page to show while the kernel is starting", "response": "def _create_loading_page(self):\r\n        \"\"\"Create html page to show while the kernel is starting\"\"\"\r\n        loading_template = Template(LOADING)\r\n        loading_img = get_image_path('loading_sprites.png')\r\n        if os.name == 'nt':\r\n            loading_img = loading_img.replace('\\\\', '/')\r\n        message = _(\"Connecting to kernel...\")\r\n        page = loading_template.substitute(css_path=self.css_path,\r\n                                           loading_img=loading_img,\r\n                                           message=message)\r\n        return page"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates html page to show while the kernel is starting", "response": "def _create_blank_page(self):\r\n        \"\"\"Create html page to show while the kernel is starting\"\"\"\r\n        loading_template = Template(BLANK)\r\n        page = loading_template.substitute(css_path=self.css_path)\r\n        return page"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nshow animation while the kernel is loading.", "response": "def _show_loading_page(self):\r\n        \"\"\"Show animation while the kernel is loading.\"\"\"\r\n        self.shellwidget.hide()\r\n        self.infowidget.show()\r\n        self.info_page = self.loading_page\r\n        self.set_info_page()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nhide animation shown while the kernel is loading.", "response": "def _hide_loading_page(self):\r\n        \"\"\"Hide animation shown while the kernel is loading.\"\"\"\r\n        self.infowidget.hide()\r\n        self.shellwidget.show()\r\n        self.info_page = self.blank_page\r\n        self.set_info_page()\r\n        self.shellwidget.sig_prompt_ready.disconnect(self._hide_loading_page)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread the stderr file of the kernel.", "response": "def _read_stderr(self):\r\n        \"\"\"Read the stderr file of the kernel.\"\"\"\r\n        # We need to read stderr_file as bytes to be able to\r\n        # detect its encoding with chardet\r\n        f = open(self.stderr_file, 'rb')\r\n\r\n        try:\r\n            stderr_text = f.read()\r\n\r\n            # This is needed to avoid showing an empty error message\r\n            # when the kernel takes too much time to start.\r\n            # See issue 8581\r\n            if not stderr_text:\r\n                return ''\r\n\r\n            # This is needed since the stderr file could be encoded\r\n            # in something different to utf-8.\r\n            # See issue 4191\r\n            encoding = get_coding(stderr_text)\r\n            stderr_text = to_text_string(stderr_text, encoding)\r\n            return stderr_text\r\n        finally:\r\n            f.close()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _show_mpl_backend_errors(self):\r\n        if not self.external_kernel:\r\n            self.shellwidget.silent_execute(\r\n                    \"get_ipython().kernel._show_mpl_backend_errors()\")\r\n        self.shellwidget.sig_prompt_ready.disconnect(\r\n            self._show_mpl_backend_errors)", "response": "Show possible errors when setting the selected Matplotlib backend."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _calculate_position(self, at_line=None, at_position=None,\r\n                            at_point=None):\r\n        \"\"\"\r\n        Calculate a global point position `QPoint(x, y)`, for a given\r\n        line, local cursor position, or local point.\r\n        \"\"\"\r\n        # Check that no option or only one option is given:\r\n        if [at_line, at_position, at_point].count(None) < 2:\r\n            raise Exception('Provide no argument or only one argument!')\r\n\r\n        # Saving cursor position:\r\n        if at_position is None:\r\n            at_position = self.get_position('cursor')\r\n\r\n        # FIXME: What id this used for?\r\n        self.calltip_position = at_position\r\n\r\n        if at_point is not None:\r\n            # Showing tooltip at point position\r\n            cx, cy = at_point.x(), at_point.y()\r\n        elif at_line is not None:\r\n            # Showing tooltip at line\r\n            cx = 5\r\n            cursor = QTextCursor(self.document().findBlockByNumber(at_line-1))\r\n            cy = self.cursorRect(cursor).top()\r\n        else:\r\n            # Showing tooltip at cursor position\r\n            # FIXME: why is at_position not being used to calculate the\r\n            # coordinates?\r\n            cx, cy = self.get_coordinates('cursor')\r\n\r\n        # Calculate vertical delta\r\n        font = self.font()\r\n        delta = font.pointSize() + 5\r\n\r\n        # Map to global coordinates\r\n        point = self.mapToGlobal(QPoint(cx, cy))\r\n        point = self.calculate_real_position(point)\r\n        point.setY(point.y() + delta)\r\n\r\n        return point", "response": "Calculates a global point position for a given line local cursor position or local point."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nupdates the background stylesheet to make it lighter.", "response": "def _update_stylesheet(self, widget):\r\n        \"\"\"Update the background stylesheet to make it lighter.\"\"\"\r\n        if is_dark_interface():\r\n            css = qdarkstyle.load_stylesheet_from_environment()\r\n            widget.setStyleSheet(css)\r\n            palette = widget.palette()\r\n            background = palette.color(palette.Window).lighter(150).name()\r\n            border = palette.color(palette.Window).lighter(200).name()\r\n            name = widget.__class__.__name__\r\n            widget.setObjectName(name)\r\n            extra_css = '''\r\n                {0}#{0} {{\r\n                    background-color:{1};\r\n                    border: 1px solid {2};\r\n                }}'''.format(name, background, border)\r\n            widget.setStyleSheet(css + extra_css)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _format_text(self, title, text, color, ellide=False):\r\n        template = '''\r\n            <div style=\\'font-family: \"{font_family}\";\r\n                        font-size: {title_size}pt;\r\n                        color: {color}\\'>\r\n                <b>{title}</b>\r\n            </div>\r\n        '''\r\n        if text:\r\n            template += '''\r\n            <hr>\r\n            <div style=\\'font-family: \"{font_family}\";\r\n                        font-size: {text_size}pt;\r\n                        color: {color_text}\\'>\r\n                {text}\r\n            </div>\r\n            '''\r\n\r\n        # Prepare text\r\n        if isinstance(text, list):\r\n            text = \"\\n    \".join(text)\r\n\r\n        text = text.replace('\\n', '<br>')\r\n\r\n        # Ellide the text if it is too long\r\n        if ellide and len(text) > self.calltip_size:\r\n            text = text[:self.calltip_size] + \" ...\"\r\n\r\n        # Get current font properties\r\n        font = self.font()\r\n        font_family = font.family()\r\n        title_size = font.pointSize()\r\n        text_size = title_size - 1 if title_size > 9 else title_size\r\n\r\n        tiptext = template.format(\r\n            font_family=font_family,\r\n            title_size=title_size,\r\n            text_size=text_size,\r\n            color=color,\r\n            color_text=self._DEFAULT_TEXT_COLOR,\r\n            title=title,\r\n            text=text,\r\n        )\r\n\r\n        return tiptext", "response": "Create a text message with the given title and text."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nformat a signature for display.", "response": "def _format_signature(self, signature, doc='', parameter='',\r\n                          parameter_doc='', color=_DEFAULT_TITLE_COLOR,\r\n                          is_python=False):\r\n        \"\"\"\r\n        Create HTML template for signature.\r\n\r\n        This template will include indent after the method name, a highlight\r\n        color for the active parameter and highlights for special chars.\r\n        \"\"\"\r\n        active_parameter_template = (\r\n            '<span style=\\'font-family:\"{font_family}\";'\r\n            'font-size:{font_size}pt;'\r\n            'color:{color}\\'>'\r\n            '<b>{parameter}</b>'\r\n            '</span>'\r\n        )\r\n        chars_template = (\r\n            '<span style=\"color:{0};'.format(self._CHAR_HIGHLIGHT_COLOR) +\r\n            'font-weight:bold\">{char}'\r\n            '</span>'\r\n        )\r\n\r\n        def handle_sub(matchobj):\r\n            \"\"\"\r\n            Handle substitution of active parameter template.\r\n\r\n            This ensures the correct highlight of the active parameter.\r\n            \"\"\"\r\n            match = matchobj.group(0)\r\n            new = match.replace(parameter, active_parameter_template)\r\n            return new\r\n\r\n        # Remove duplicate spaces\r\n        signature = ' '.join(signature.split())\r\n\r\n        # Replace ay initial spaces\r\n        signature = signature.replace('( ', '(')\r\n\r\n        # Process signature template\r\n        pattern = r'[\\*|(|\\s](' + parameter + r')[,|)|\\s|=]'\r\n        formatted_lines = []\r\n        name = signature.split('(')[0]\r\n        indent = ' ' * (len(name) + 1)\r\n        rows = textwrap.wrap(signature, width=60, subsequent_indent=indent)\r\n        for row in rows:\r\n            # Add template to highlight the active parameter\r\n            row = re.sub(pattern, handle_sub, row)\r\n            row = row.replace(' ', '&nbsp;')\r\n            row = row.replace('span&nbsp;', 'span ')\r\n            if is_python:\r\n                for char in ['(', ')', ',', '*', '**']:\r\n                    new_char = chars_template.format(char=char)\r\n                    row = row.replace(char, new_char)\r\n            formatted_lines.append(row)\r\n        title_template = '<br>'.join(formatted_lines)\r\n\r\n        # Get current font properties\r\n        font = self.font()\r\n        font_size = font.pointSize()\r\n        font_family = font.family()\r\n\r\n        # Format title to display active parameter\r\n        title = title_template.format(\r\n            font_size=font_size,\r\n            font_family=font_family,\r\n            color=self._PARAMETER_HIGHLIGHT_COLOR,\r\n            parameter=parameter,\r\n        )\r\n\r\n        # Process documentation\r\n        # TODO: To be included in a separate PR\r\n        # active = active_parameter_template.format(\r\n        #     font_size=font_size,\r\n        #     font_family=font_family,\r\n        #     color=self._PARAMETER_HIGHLIGHT_COLOR,\r\n        #     parameter=parameter,\r\n        # )\r\n        # if doc is not None and len(doc) > 0:\r\n        #     text_doc = doc.split('\\n')[0]\r\n        # else:\r\n        #     text_doc = ''\r\n\r\n        # if parameter_doc is not None and len(parameter_doc) > 0:\r\n        #     text_prefix = text_doc + '<br><hr><br>param: ' + active\r\n        #     text = parameter_doc\r\n        # else:\r\n        #     text_prefix = ''\r\n        #     text = ''\r\n\r\n        # formatted_lines = []\r\n        # rows = textwrap.wrap(text, width=60)\r\n        # for row in rows:\r\n        #     row = row.replace(' ', '&nbsp;')\r\n\r\n        # if text_prefix:\r\n        #     text = text_prefix + '<br><br>' + '<br>'.join(rows)\r\n        # else:\r\n        #     text = '<br>'.join(rows)\r\n        # text += '<br>'\r\n\r\n        # Format text\r\n        tiptext = self._format_text(title, '', color)\r\n\r\n        return tiptext, rows"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef show_calltip(self, signature, doc='', parameter='', parameter_doc='',\r\n                     color=_DEFAULT_TITLE_COLOR, is_python=False):\r\n        \"\"\"\r\n        Show calltip.\r\n\r\n        Calltips look like tooltips but will not disappear if mouse hovers\r\n        them. They are useful for displaying signature information on methods\r\n        and functions.\r\n        \"\"\"\r\n        # Find position of calltip\r\n        point = self._calculate_position()\r\n\r\n        # Format text\r\n        tiptext, wrapped_lines = self._format_signature(\r\n            signature,\r\n            doc,\r\n            parameter,\r\n            parameter_doc,\r\n            color,\r\n            is_python,\r\n        )\r\n\r\n        self._update_stylesheet(self.calltip_widget)\r\n\r\n        # Show calltip\r\n        self.calltip_widget.show_tip(point, tiptext, wrapped_lines)", "response": "Show the calltip for the specified signature."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nshowing tooltip. Tooltips will disappear if mouse hovers them. They are meant for quick inspections.", "response": "def show_tooltip(self, title, text, color=_DEFAULT_TITLE_COLOR,\r\n                     at_line=None, at_position=None, at_point=None):\r\n        \"\"\"\r\n        Show tooltip.\r\n\r\n        Tooltips will disappear if mouse hovers them. They are meant for quick\r\n        inspections.\r\n        \"\"\"\r\n        if text is not None and len(text) != 0:\r\n            # Find position of calltip\r\n            point = self._calculate_position(\r\n                at_line=at_line,\r\n                at_position=at_position,\r\n                at_point=at_point,\r\n            )\r\n\r\n            # Format text\r\n            tiptext = self._format_text(title, text, color, ellide=True)\r\n\r\n            self._update_stylesheet(self.tooltip_widget)\r\n\r\n            # Display tooltip\r\n            self.tooltip_widget.show_tip(point, tiptext)\r\n            self.tooltip_widget.show()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting end - of - line characters from text.", "response": "def set_eol_chars(self, text):\r\n        \"\"\"Set widget end-of-line (EOL) characters from text (analyzes text)\"\"\"\r\n        if not is_text_string(text): # testing for QString (PyQt API#1)\r\n            text = to_text_string(text)\r\n        eol_chars = sourcecode.get_eol_chars(text)\r\n        is_document_modified = eol_chars is not None and self.eol_chars is not None\r\n        self.eol_chars = eol_chars\r\n        if is_document_modified:\r\n            self.document().setModified(True)\r\n            if self.sig_eol_chars_changed is not None:\r\n                self.sig_eol_chars_changed.emit(eol_chars)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_position(self, subject):\r\n        cursor = self.textCursor()\r\n        if subject == 'cursor':\r\n            pass\r\n        elif subject == 'sol':\r\n            cursor.movePosition(QTextCursor.StartOfBlock)\r\n        elif subject == 'eol':\r\n            cursor.movePosition(QTextCursor.EndOfBlock)\r\n        elif subject == 'eof':\r\n            cursor.movePosition(QTextCursor.End)\r\n        elif subject == 'sof':\r\n            cursor.movePosition(QTextCursor.Start)\r\n        else:\r\n            # Assuming that input argument was already a position\r\n            return subject\r\n        return cursor.position()", "response": "Get the position of the given subject from the start of the text edit area."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef move_cursor(self, chars=0):\r\n        direction = QTextCursor.Right if chars > 0 else QTextCursor.Left\r\n        for _i in range(abs(chars)):\r\n            self.moveCursor(direction, QTextCursor.MoveAnchor)", "response": "Move cursor to left or right."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_cursor_on_first_line(self):\r\n        cursor = self.textCursor()\r\n        cursor.movePosition(QTextCursor.StartOfBlock)\r\n        return cursor.atStart()", "response": "Return True if cursor is on the first line"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn True if cursor is on the last line", "response": "def is_cursor_on_last_line(self):\r\n        \"\"\"Return True if cursor is on the last line\"\"\"\r\n        cursor = self.textCursor()\r\n        cursor.movePosition(QTextCursor.EndOfBlock)\r\n        return cursor.atEnd()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_cursor_before(self, position, char_offset=0):\r\n        position = self.get_position(position) + char_offset\r\n        cursor = self.textCursor()\r\n        cursor.movePosition(QTextCursor.End)\r\n        if position < cursor.position():\r\n            cursor.setPosition(position)\r\n            return self.textCursor() < cursor", "response": "Return True if cursor is before position"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef move_cursor_to_next(self, what='word', direction='left'):\r\n        self.__move_cursor_anchor(what, direction, QTextCursor.MoveAnchor)", "response": "Move cursor to next what character or right."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef extend_selection_to_next(self, what='word', direction='left'):\r\n        self.__move_cursor_anchor(what, direction, QTextCursor.KeepAnchor)", "response": "Extend selection to next what character or right"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the text line at the specified line number.", "response": "def get_text_line(self, line_nb):\r\n        \"\"\"Return text line at line number *line_nb*\"\"\"\r\n        # Taking into account the case when a file ends in an empty line,\r\n        # since splitlines doesn't return that line as the last element\r\n        # TODO: Make this function more efficient\r\n        try:\r\n            return to_text_string(self.toPlainText()).splitlines()[line_nb]\r\n        except IndexError:\r\n            return self.get_line_separator()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the text between the specified position_from and position_to.", "response": "def get_text(self, position_from, position_to):\r\n        \"\"\"\r\n        Return text between *position_from* and *position_to*\r\n        Positions may be positions or 'sol', 'eol', 'sof', 'eof' or 'cursor'\r\n        \"\"\"\r\n        cursor = self.__select_text(position_from, position_to)\r\n        text = to_text_string(cursor.selectedText())\r\n        all_text = position_from == 'sof' and position_to == 'eof'\r\n        if text and not all_text:\r\n            while text.endswith(\"\\n\"):\r\n                text = text[:-1]\r\n            while text.endswith(u\"\\u2029\"):\r\n                text = text[:-1]\r\n        return text"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn character at position with the given offset.", "response": "def get_character(self, position, offset=0):\r\n        \"\"\"Return character at *position* with the given offset.\"\"\"\r\n        position = self.get_position(position) + offset\r\n        cursor = self.textCursor()\r\n        cursor.movePosition(QTextCursor.End)\r\n        if position < cursor.position():\r\n            cursor.setPosition(position)\r\n            cursor.movePosition(QTextCursor.Right,\r\n                                QTextCursor.KeepAnchor)\r\n            return to_text_string(cursor.selectedText())\r\n        else:\r\n            return ''"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_current_word_and_position(self, completion=False):\r\n        cursor = self.textCursor()\r\n\r\n        if cursor.hasSelection():\r\n            # Removes the selection and moves the cursor to the left side\r\n            # of the selection: this is required to be able to properly\r\n            # select the whole word under cursor (otherwise, the same word is\r\n            # not selected when the cursor is at the right side of it):\r\n            cursor.setPosition(min([cursor.selectionStart(),\r\n                                    cursor.selectionEnd()]))\r\n        else:\r\n            # Checks if the first character to the right is a white space\r\n            # and if not, moves the cursor one word to the left (otherwise,\r\n            # if the character to the left do not match the \"word regexp\"\r\n            # (see below), the word to the left of the cursor won't be\r\n            # selected), but only if the first character to the left is not a\r\n            # white space too.\r\n            def is_space(move):\r\n                curs = self.textCursor()\r\n                curs.movePosition(move, QTextCursor.KeepAnchor)\r\n                return not to_text_string(curs.selectedText()).strip()\r\n            if not completion:\r\n                if is_space(QTextCursor.NextCharacter):\r\n                    if is_space(QTextCursor.PreviousCharacter):\r\n                        return\r\n                    cursor.movePosition(QTextCursor.WordLeft)\r\n            else:\r\n                def is_special_character(move):\r\n                    curs = self.textCursor()\r\n                    curs.movePosition(move, QTextCursor.KeepAnchor)\r\n                    text_cursor = to_text_string(curs.selectedText()).strip()\r\n                    return len(re.findall(r'([^\\d\\W]\\w*)',\r\n                                          text_cursor, re.UNICODE)) == 0\r\n                if is_space(QTextCursor.PreviousCharacter):\r\n                    return\r\n                if (is_special_character(QTextCursor.NextCharacter)):\r\n                    cursor.movePosition(QTextCursor.WordLeft)\r\n\r\n        cursor.select(QTextCursor.WordUnderCursor)\r\n        text = to_text_string(cursor.selectedText())\r\n        # find a valid python variable name\r\n        match = re.findall(r'([^\\d\\W]\\w*)', text, re.UNICODE)\r\n        if match:\r\n            return match[0], cursor.selectionStart()", "response": "Returns current word at cursor position and the start position"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning current word i. e. word at cursor position", "response": "def get_current_word(self, completion=False):\r\n        \"\"\"Return current word, i.e. word at cursor position\"\"\"\r\n        ret = self.get_current_word_and_position(completion)\r\n        if ret is not None:\r\n            return ret[0]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning current line s text", "response": "def get_current_line(self):\r\n        \"\"\"Return current line's text\"\"\"\r\n        cursor = self.textCursor()\r\n        cursor.select(QTextCursor.BlockUnderCursor)\r\n        return to_text_string(cursor.selectedText())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the line at the given coordinates.", "response": "def get_line_at(self, coordinates):\r\n        \"\"\"Return line at *coordinates* (QPoint)\"\"\"\r\n        cursor = self.cursorForPosition(coordinates)\r\n        cursor.select(QTextCursor.BlockUnderCursor)\r\n        return to_text_string(cursor.selectedText()).replace(u'\\u2029', '')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_word_at(self, coordinates):\r\n        cursor = self.cursorForPosition(coordinates)\r\n        cursor.select(QTextCursor.WordUnderCursor)\r\n        return to_text_string(cursor.selectedText())", "response": "Return the word at the given coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_block_indentation(self, block_nb):\r\n        text = to_text_string(self.document().findBlockByNumber(block_nb).text())\r\n        text = text.replace(\"\\t\", \" \"*self.tab_stop_width_spaces)\r\n        return len(text)-len(text.lstrip())", "response": "Return line indentation (character number)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_selection_bounds(self):\r\n        cursor = self.textCursor()\r\n        start, end = cursor.selectionStart(), cursor.selectionEnd()\r\n        block_start = self.document().findBlock(start)\r\n        block_end = self.document().findBlock(end)\r\n        return sorted([block_start.blockNumber(), block_end.blockNumber()])", "response": "Return the selection bounds of the current page."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the selected text in unicode", "response": "def get_selected_text(self):\r\n        \"\"\"\r\n        Return text selected by current text cursor, converted in unicode\r\n\r\n        Replace the unicode line separator character \\u2029 by\r\n        the line separator characters returned by get_line_separator\r\n        \"\"\"\r\n        return to_text_string(self.textCursor().selectedText()).replace(u\"\\u2029\",\r\n                                                     self.get_line_separator())"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreplace selected text by text using regular expression text substitution", "response": "def replace(self, text, pattern=None):\r\n        \"\"\"Replace selected text by *text*\r\n        If *pattern* is not None, replacing selected text using regular\r\n        expression text substitution\"\"\"\r\n        cursor = self.textCursor()\r\n        cursor.beginEditBlock()\r\n        if pattern is not None:\r\n            seltxt = to_text_string(cursor.selectedText())\r\n        cursor.removeSelectedText()\r\n        if pattern is not None:\r\n            text = re.sub(to_text_string(pattern),\r\n                          to_text_string(text), to_text_string(seltxt))\r\n        cursor.insertText(text)\r\n        cursor.endEditBlock()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfind the next occurrence of a given text in the document.", "response": "def find_text(self, text, changed=True, forward=True, case=False,\r\n                  words=False, regexp=False):\r\n        \"\"\"Find text\"\"\"\r\n        cursor = self.textCursor()\r\n        findflag = QTextDocument.FindFlag()\r\n\r\n        if not forward:\r\n            findflag = findflag | QTextDocument.FindBackward\r\n\r\n        if case:\r\n            findflag = findflag | QTextDocument.FindCaseSensitively\r\n\r\n        moves = [QTextCursor.NoMove]\r\n        if forward:\r\n            moves += [QTextCursor.NextWord, QTextCursor.Start]\r\n            if changed:\r\n                if to_text_string(cursor.selectedText()):\r\n                    new_position = min([cursor.selectionStart(),\r\n                                        cursor.selectionEnd()])\r\n                    cursor.setPosition(new_position)\r\n                else:\r\n                    cursor.movePosition(QTextCursor.PreviousWord)\r\n        else:\r\n            moves += [QTextCursor.End]\r\n\r\n        if regexp:\r\n            text = to_text_string(text)\r\n        else:\r\n            text = re.escape(to_text_string(text))\r\n\r\n        if QT55_VERSION:\r\n            pattern = QRegularExpression(u\"\\\\b{}\\\\b\".format(text) if words else\r\n                                         text)\r\n            if case:\r\n                pattern.setPatternOptions(\r\n                    QRegularExpression.CaseInsensitiveOption)\r\n        else:\r\n            pattern = QRegExp(u\"\\\\b{}\\\\b\".format(text)\r\n                              if words else text, Qt.CaseSensitive if case else\r\n                              Qt.CaseInsensitive, QRegExp.RegExp2)\r\n\r\n        for move in moves:\r\n            cursor.movePosition(move)\r\n            if regexp and '\\\\n' in text:\r\n                # Multiline regular expression\r\n                found_cursor = self.find_multiline_pattern(pattern, cursor,\r\n                                                           findflag)\r\n            else:\r\n                # Single line find: using the QTextDocument's find function,\r\n                # probably much more efficient than ours\r\n                found_cursor = self.document().find(pattern, cursor, findflag)\r\n            if found_cursor is not None and not found_cursor.isNull():\r\n                self.setTextCursor(found_cursor)\r\n                return True\r\n\r\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_number_matches(self, pattern, source_text='', case=False,\r\n                           regexp=False):\r\n        \"\"\"Get the number of matches for the searched text.\"\"\"\r\n        pattern = to_text_string(pattern)\r\n        if not pattern:\r\n            return 0\r\n\r\n        if not regexp:\r\n            pattern = re.escape(pattern)\r\n\r\n        if not source_text:\r\n            source_text = to_text_string(self.toPlainText())\r\n\r\n        try:\r\n            if case:\r\n                regobj = re.compile(pattern)\r\n            else:\r\n                regobj = re.compile(pattern, re.IGNORECASE)\r\n        except sre_constants.error:\r\n            return None\r\n\r\n        number_matches = 0\r\n        for match in regobj.finditer(source_text):\r\n            number_matches += 1\r\n\r\n        return number_matches", "response": "Get the number of matches for the searched text."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_match_number(self, pattern, case=False, regexp=False):\r\n        position = self.textCursor().position()\r\n        source_text = self.get_text(position_from='sof', position_to=position)\r\n        match_number = self.get_number_matches(pattern,\r\n                                               source_text=source_text,\r\n                                               case=case, regexp=regexp)\r\n        return match_number", "response": "Get the number of the match for the searched text."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nshowing Pointing Hand Cursor on error messages", "response": "def mouseMoveEvent(self, event):\r\n        \"\"\"Show Pointing Hand Cursor on error messages\"\"\"\r\n        text = self.get_line_at(event.pos())\r\n        if get_error_match(text):\r\n            if not self.__cursor_changed:\r\n                QApplication.setOverrideCursor(QCursor(Qt.PointingHandCursor))\r\n                self.__cursor_changed = True\r\n            event.accept()\r\n            return\r\n        if self.__cursor_changed:\r\n            QApplication.restoreOverrideCursor()\r\n            self.__cursor_changed = False\r\n        self.QT_CLASS.mouseMoveEvent(self, event)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef leaveEvent(self, event):\r\n        if self.__cursor_changed:\r\n            QApplication.restoreOverrideCursor()\r\n            self.__cursor_changed = False\r\n        self.QT_CLASS.leaveEvent(self, event)", "response": "If cursor has not been restored yet, do it now"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nshowing object info in the Help plugin", "response": "def show_object_info(self, text, call=False, force=False):\r\n        \"\"\"Show signature calltip and/or docstring in the Help plugin\"\"\"\r\n        text = to_text_string(text)\r\n\r\n        # Show docstring\r\n        help_enabled = self.help_enabled or force\r\n        if force and self.help is not None:\r\n            self.help.dockwidget.setVisible(True)\r\n            self.help.dockwidget.raise_()\r\n        if help_enabled and (self.help is not None) and \\\r\n           (self.help.dockwidget.isVisible()):\r\n            # Help widget exists and is visible\r\n            if hasattr(self, 'get_doc'):\r\n                self.help.set_shell(self)\r\n            else:\r\n                self.help.set_shell(self.parent())\r\n            self.help.set_object_text(text, ignore_unknown=False)\r\n            self.setFocus() # if help was not at top level, raising it to\r\n                            # top will automatically give it focus because of\r\n                            # the visibility_changed signal, so we must give\r\n                            # focus back to shell\r\n\r\n        # Show calltip\r\n        if call and getattr(self, 'calltips', None):\r\n            # Display argument list if this is a function call\r\n            iscallable = self.iscallable(text)\r\n            if iscallable is not None:\r\n                if iscallable:\r\n                    arglist = self.get_arglist(text)\r\n                    name =  text.split('.')[-1]\r\n                    argspec = signature = ''\r\n                    if isinstance(arglist, bool):\r\n                        arglist = []\r\n                    if arglist:\r\n                        argspec = '(' + ''.join(arglist) + ')'\r\n                    else:\r\n                        doc = self.get__doc__(text)\r\n                        if doc is not None:\r\n                            # This covers cases like np.abs, whose docstring is\r\n                            # the same as np.absolute and because of that a\r\n                            # proper signature can't be obtained correctly\r\n                            argspec = getargspecfromtext(doc)\r\n                            if not argspec:\r\n                                signature = getsignaturefromtext(doc, name)\r\n                    if argspec or signature:\r\n                        if argspec:\r\n                            tiptext = name + argspec\r\n                        else:\r\n                            tiptext = signature\r\n                        # TODO: Select language and pass it to call\r\n                        self.show_calltip(tiptext, color='#2D62FF', is_python=True)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_history_filename(self):\r\n        if self.history_filename and not osp.isfile(self.history_filename):\r\n            try:\r\n                encoding.writelines(self.INITHISTORY, self.history_filename)\r\n            except EnvironmentError:\r\n                pass", "response": "Create history_filename with INITHISTORY if it doesn t exist."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding a new entry to the history file.", "response": "def add_to_history(self, command):\r\n        \"\"\"Add command to history\"\"\"\r\n        command = to_text_string(command)\r\n        if command in ['', '\\n'] or command.startswith('Traceback'):\r\n            return\r\n        if command.endswith('\\n'):\r\n            command = command[:-1]\r\n        self.histidx = None\r\n        if len(self.history) > 0 and self.history[-1] == command:\r\n            return\r\n        self.history.append(command)\r\n        text = os.linesep + command\r\n\r\n        # When the first entry will be written in history file,\r\n        # the separator will be append first:\r\n        if self.history_filename not in self.HISTORY_FILENAMES:\r\n            self.HISTORY_FILENAMES.append(self.history_filename)\r\n            text = self.SEPARATOR + text\r\n        # Needed to prevent errors when writing history to disk\r\n        # See issue 6431\r\n        try:\r\n            encoding.write(text, self.history_filename, mode='ab')\r\n        except EnvironmentError:\r\n            pass\r\n        if self.append_to_history is not None:\r\n            self.append_to_history.emit(self.history_filename, text)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef browse_history(self, backward):\r\n        if self.is_cursor_before('eol') and self.hist_wholeline:\r\n            self.hist_wholeline = False\r\n        tocursor = self.get_current_line_to_cursor()\r\n        text, self.histidx = self.find_in_history(tocursor, self.histidx,\r\n                                                  backward)\r\n        if text is not None:\r\n            if self.hist_wholeline:\r\n                self.clear_line()\r\n                self.insert_text(text)\r\n            else:\r\n                cursor_position = self.get_position('cursor')\r\n                # Removing text from cursor to the end of the line\r\n                self.remove_text('cursor', 'eol')\r\n                # Inserting history text\r\n                self.insert_text(text)\r\n                self.set_cursor_position(cursor_position)", "response": "Browse history from the cursor to the end of the line."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef find_in_history(self, tocursor, start_idx, backward):\r\n        if start_idx is None:\r\n            start_idx = len(self.history)\r\n        # Finding text in history\r\n        step = -1 if backward else 1\r\n        idx = start_idx\r\n        if len(tocursor) == 0 or self.hist_wholeline:\r\n            idx += step\r\n            if idx >= len(self.history) or len(self.history) == 0:\r\n                return \"\", len(self.history)\r\n            elif idx < 0:\r\n                idx = 0\r\n            self.hist_wholeline = True\r\n            return self.history[idx], idx\r\n        else:\r\n            for index in range(len(self.history)):\r\n                idx = (start_idx+step*(index+1)) % len(self.history)\r\n                entry = self.history[idx]\r\n                if entry.startswith(tocursor):\r\n                    return entry[len(tocursor):], idx\r\n            else:\r\n                return None, start_idx", "response": "Find text tocursor in history from index start_idx"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a QKeySequence representation of the provided event.", "response": "def keyevent_to_keyseq(self, event):\r\n        \"\"\"Return a QKeySequence representation of the provided QKeyEvent.\"\"\"\r\n        self.keyPressEvent(event)\r\n        event.accept()\r\n        return self.keySequence()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\noverriding setText to set the tooltip and text", "response": "def setText(self, sequence):\r\n        \"\"\"Qt method extension.\"\"\"\r\n        self.setToolTip(sequence)\r\n        super(ShortcutLineEdit, self).setText(sequence)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_text(self, text):\r\n        text = text.strip()\r\n        new_text = self.text() + text\r\n        self.setText(new_text)", "response": "Set the filter text."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef keyPressEvent(self, event):\r\n        key = event.key()\r\n        if key in [Qt.Key_Up]:\r\n            self._parent.previous_row()\r\n        elif key in [Qt.Key_Down]:\r\n            self._parent.next_row()\r\n        elif key in [Qt.Key_Enter, Qt.Key_Return]:\r\n            self._parent.show_editor()\r\n        else:\r\n            super(ShortcutFinder, self).keyPressEvent(event)", "response": "Override keyPressEvent to handle key press events."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef setup(self):\r\n        # Widgets\r\n        icon_info = HelperToolButton()\r\n        icon_info.setIcon(get_std_icon('MessageBoxInformation'))\r\n        layout_icon_info = QVBoxLayout()\r\n        layout_icon_info.setContentsMargins(0, 0, 0, 0)\r\n        layout_icon_info.setSpacing(0)\r\n        layout_icon_info.addWidget(icon_info)\r\n        layout_icon_info.addStretch(100)\r\n\r\n        self.label_info = QLabel()\r\n        self.label_info.setText(\r\n            _(\"Press the new shortcut and select 'Ok' to confirm, \"\r\n              \"click 'Cancel' to revert to the previous state, \"\r\n              \"or use 'Clear' to unbind the command from a shortcut.\"))\r\n        self.label_info.setAlignment(Qt.AlignTop | Qt.AlignLeft)\r\n        self.label_info.setWordWrap(True)\r\n        layout_info = QHBoxLayout()\r\n        layout_info.setContentsMargins(0, 0, 0, 0)\r\n        layout_info.addLayout(layout_icon_info)\r\n        layout_info.addWidget(self.label_info)\r\n        layout_info.setStretch(1, 100)\r\n\r\n        self.label_current_sequence = QLabel(_(\"Current shortcut:\"))\r\n        self.text_current_sequence = QLabel(self.current_sequence)\r\n\r\n        self.label_new_sequence = QLabel(_(\"New shortcut:\"))\r\n        self.text_new_sequence = ShortcutLineEdit(self)\r\n        self.text_new_sequence.setPlaceholderText(_(\"Press shortcut.\"))\r\n\r\n        self.helper_button = HelperToolButton()\r\n        self.helper_button.setIcon(QIcon())\r\n        self.label_warning = QLabel()\r\n        self.label_warning.setWordWrap(True)\r\n        self.label_warning.setAlignment(Qt.AlignTop | Qt.AlignLeft)\r\n\r\n        self.button_default = QPushButton(_('Default'))\r\n        self.button_ok = QPushButton(_('Ok'))\r\n        self.button_ok.setEnabled(False)\r\n        self.button_clear = QPushButton(_('Clear'))\r\n        self.button_cancel = QPushButton(_('Cancel'))\r\n        button_box = QHBoxLayout()\r\n        button_box.addWidget(self.button_default)\r\n        button_box.addStretch(100)\r\n        button_box.addWidget(self.button_ok)\r\n        button_box.addWidget(self.button_clear)\r\n        button_box.addWidget(self.button_cancel)\r\n\r\n        # New Sequence button box\r\n        self.btn_clear_sequence = create_toolbutton(\r\n            self, icon=ima.icon('editclear'),\r\n            tip=_(\"Clear all entered key sequences\"),\r\n            triggered=self.clear_new_sequence)\r\n        self.button_back_sequence = create_toolbutton(\r\n            self, icon=ima.icon('ArrowBack'),\r\n            tip=_(\"Remove last key sequence entered\"),\r\n            triggered=self.back_new_sequence)\r\n\r\n        newseq_btnbar = QHBoxLayout()\r\n        newseq_btnbar.setSpacing(0)\r\n        newseq_btnbar.setContentsMargins(0, 0, 0, 0)\r\n        newseq_btnbar.addWidget(self.button_back_sequence)\r\n        newseq_btnbar.addWidget(self.btn_clear_sequence)\r\n\r\n        # Setup widgets\r\n        self.setWindowTitle(_('Shortcut: {0}').format(self.name))\r\n        self.helper_button.setToolTip('')\r\n        style = \"\"\"\r\n            QToolButton {\r\n              margin:1px;\r\n              border: 0px solid grey;\r\n              padding:0px;\r\n              border-radius: 0px;\r\n            }\"\"\"\r\n        self.helper_button.setStyleSheet(style)\r\n        icon_info.setToolTip('')\r\n        icon_info.setStyleSheet(style)\r\n\r\n        # Layout\r\n        layout_sequence = QGridLayout()\r\n        layout_sequence.setContentsMargins(0, 0, 0, 0)\r\n        layout_sequence.addLayout(layout_info, 0, 0, 1, 4)\r\n        layout_sequence.addItem(QSpacerItem(15, 15), 1, 0, 1, 4)\r\n        layout_sequence.addWidget(self.label_current_sequence, 2, 0)\r\n        layout_sequence.addWidget(self.text_current_sequence, 2, 2)\r\n        layout_sequence.addWidget(self.label_new_sequence, 3, 0)\r\n        layout_sequence.addWidget(self.helper_button, 3, 1)\r\n        layout_sequence.addWidget(self.text_new_sequence, 3, 2)\r\n        layout_sequence.addLayout(newseq_btnbar, 3, 3)\r\n        layout_sequence.addWidget(self.label_warning, 4, 2, 1, 2)\r\n        layout_sequence.setColumnStretch(2, 100)\r\n        layout_sequence.setRowStretch(4, 100)\r\n\r\n        layout = QVBoxLayout()\r\n        layout.addLayout(layout_sequence)\r\n        layout.addSpacing(5)\r\n        layout.addLayout(button_box)\r\n        self.setLayout(layout)\r\n\r\n        # Signals\r\n        self.button_ok.clicked.connect(self.accept_override)\r\n        self.button_clear.clicked.connect(self.unbind_shortcut)\r\n        self.button_cancel.clicked.connect(self.reject)\r\n        self.button_default.clicked.connect(self.set_sequence_to_default)\r\n\r\n        # Set all widget to no focus so that we can register <Tab> key\r\n        # press event.\r\n        widgets = (\r\n            self.label_warning, self.helper_button, self.text_new_sequence,\r\n            self.button_clear, self.button_default, self.button_cancel,\r\n            self.button_ok, self.btn_clear_sequence, self.button_back_sequence)\r\n        for w in widgets:\r\n            w.setFocusPolicy(Qt.NoFocus)\r\n            w.clearFocus()", "response": "Setup the ShortcutEditor with the provided arguments."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check_conflicts(self):\r\n        conflicts = []\r\n        if len(self._qsequences) == 0:\r\n            return conflicts\r\n\r\n        new_qsequence = self.new_qsequence\r\n        for shortcut in self.shortcuts:\r\n            shortcut_qsequence = QKeySequence.fromString(str(shortcut.key))\r\n            if shortcut_qsequence.isEmpty():\r\n                continue\r\n            if (shortcut.context, shortcut.name) == (self.context, self.name):\r\n                continue\r\n            if shortcut.context in [self.context, '_'] or self.context == '_':\r\n                if (shortcut_qsequence.matches(new_qsequence) or\r\n                        new_qsequence.matches(shortcut_qsequence)):\r\n                    conflicts.append(shortcut)\r\n        return conflicts", "response": "Check if any of the shortcuts for conflicts."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if the first sub - sequence of the new key sequence is valid.", "response": "def check_singlekey(self):\r\n        \"\"\"Check if the first sub-sequence of the new key sequence is valid.\"\"\"\r\n        if len(self._qsequences) == 0:\r\n            return True\r\n        else:\r\n            keystr = self._qsequences[0]\r\n            valid_single_keys = (EDITOR_SINGLE_KEYS if\r\n                                 self.context == 'editor' else SINGLE_KEYS)\r\n            if any((m in keystr for m in ('Ctrl', 'Alt', 'Shift', 'Meta'))):\r\n                return True\r\n            else:\r\n                # This means that the the first subsequence is composed of\r\n                # a single key with no modifier.\r\n                valid_single_keys = (EDITOR_SINGLE_KEYS if\r\n                                     self.context == 'editor' else SINGLE_KEYS)\r\n                if any((k == keystr for k in valid_single_keys)):\r\n                    return True\r\n                else:\r\n                    return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate the warning label buttons state and sequence text.", "response": "def update_warning(self):\r\n        \"\"\"Update the warning label, buttons state and sequence text.\"\"\"\r\n        new_qsequence = self.new_qsequence\r\n        new_sequence = self.new_sequence\r\n        self.text_new_sequence.setText(\r\n            new_qsequence.toString(QKeySequence.NativeText))\r\n\r\n        conflicts = self.check_conflicts()\r\n        if len(self._qsequences) == 0:\r\n            warning = SEQUENCE_EMPTY\r\n            tip = ''\r\n            icon = QIcon()\r\n        elif conflicts:\r\n            warning = SEQUENCE_CONFLICT\r\n            template = '<i>{0}<b>{1}</b>{2}</i>'\r\n            tip_title = _('The new shortcut conflicts with:') + '<br>'\r\n            tip_body = ''\r\n            for s in conflicts:\r\n                tip_body += ' - {0}: {1}<br>'.format(s.context, s.name)\r\n            tip_body = tip_body[:-4]  # Removing last <br>\r\n            tip_override = '<br>Press <b>OK</b> to unbind '\r\n            tip_override += 'it' if len(conflicts) == 1 else 'them'\r\n            tip_override += ' and assign it to <b>{}</b>'.format(self.name)\r\n            tip = template.format(tip_title, tip_body, tip_override)\r\n            icon = get_std_icon('MessageBoxWarning')\r\n        elif new_sequence in BLACKLIST:\r\n            warning = IN_BLACKLIST\r\n            template = '<i>{0}<b>{1}</b></i>'\r\n            tip_title = _('Forbidden key sequence!') + '<br>'\r\n            tip_body = ''\r\n            use = BLACKLIST[new_sequence]\r\n            if use is not None:\r\n                tip_body = use\r\n            tip = template.format(tip_title, tip_body)\r\n            icon = get_std_icon('MessageBoxWarning')\r\n        elif self.check_singlekey() is False or self.check_ascii() is False:\r\n            warning = INVALID_KEY\r\n            template = '<i>{0}</i>'\r\n            tip = _('Invalid key sequence entered') + '<br>'\r\n            icon = get_std_icon('MessageBoxWarning')\r\n        else:\r\n            warning = NO_WARNING\r\n            tip = 'This shortcut is valid.'\r\n            icon = get_std_icon('DialogApplyButton')\r\n\r\n        self.warning = warning\r\n        self.conflicts = conflicts\r\n\r\n        self.helper_button.setIcon(icon)\r\n        self.button_ok.setEnabled(\r\n            self.warning in [NO_WARNING, SEQUENCE_CONFLICT])\r\n        self.label_warning.setText(tip)\r\n        # Everytime after update warning message, update the label height\r\n        new_height = self.label_warning.sizeHint().height()\r\n        self.label_warning.setMaximumHeight(new_height)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_sequence_from_str(self, sequence):\r\n        self._qsequences = [QKeySequence(s) for s in sequence.split(', ')]\r\n        self.update_warning()", "response": "This method is used to set the new QKeySequence of the\r\n        shortcut editor from a string."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_sequence_to_default(self):\r\n        sequence = CONF.get_default(\r\n            'shortcuts', \"{}/{}\".format(self.context, self.name))\r\n        self._qsequences = sequence.split(', ')\r\n        self.update_warning()", "response": "Set the new sequence to the default value defined in the config file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef accept_override(self):\r\n        conflicts = self.check_conflicts()\r\n        if conflicts:\r\n            for shortcut in conflicts:\r\n                shortcut.key = ''\r\n        self.accept()", "response": "Unbind all conflicted shortcuts and accept the new one"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef current_index(self):\r\n        i = self._parent.proxy_model.mapToSource(self._parent.currentIndex())\r\n        return i", "response": "Get the currently selected index in the parent table view."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sortByName(self):\r\n        self.shortcuts = sorted(self.shortcuts,\r\n                                key=lambda x: x.context+'/'+x.name)\r\n        self.reset()", "response": "Sort the list of shortcuts by name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the data for the item at the given index", "response": "def data(self, index, role=Qt.DisplayRole):\r\n        \"\"\"Qt Override.\"\"\"\r\n        row = index.row()\r\n        if not index.isValid() or not (0 <= row < len(self.shortcuts)):\r\n            return to_qvariant()\r\n\r\n        shortcut = self.shortcuts[row]\r\n        key = shortcut.key\r\n        column = index.column()\r\n\r\n        if role == Qt.DisplayRole:\r\n            if column == CONTEXT:\r\n                return to_qvariant(shortcut.context)\r\n            elif column == NAME:\r\n                color = self.text_color\r\n                if self._parent == QApplication.focusWidget():\r\n                    if self.current_index().row() == row:\r\n                        color = self.text_color_highlight\r\n                    else:\r\n                        color = self.text_color\r\n                text = self.rich_text[row]\r\n                text = '<p style=\"color:{0}\">{1}</p>'.format(color, text)\r\n                return to_qvariant(text)\r\n            elif column == SEQUENCE:\r\n                text = QKeySequence(key).toString(QKeySequence.NativeText)\r\n                return to_qvariant(text)\r\n            elif column == SEARCH_SCORE:\r\n                # Treating search scores as a table column simplifies the\r\n                # sorting once a score for a specific string in the finder\r\n                # has been defined. This column however should always remain\r\n                # hidden.\r\n                return to_qvariant(self.scores[row])\r\n        elif role == Qt.TextAlignmentRole:\r\n            return to_qvariant(int(Qt.AlignHCenter | Qt.AlignVCenter))\r\n        return to_qvariant()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the data for the header of the user.", "response": "def headerData(self, section, orientation, role=Qt.DisplayRole):\r\n        \"\"\"Qt Override.\"\"\"\r\n        if role == Qt.TextAlignmentRole:\r\n            if orientation == Qt.Horizontal:\r\n                return to_qvariant(int(Qt.AlignHCenter | Qt.AlignVCenter))\r\n            return to_qvariant(int(Qt.AlignRight | Qt.AlignVCenter))\r\n        if role != Qt.DisplayRole:\r\n            return to_qvariant()\r\n        if orientation == Qt.Horizontal:\r\n            if section == CONTEXT:\r\n                return to_qvariant(_(\"Context\"))\r\n            elif section == NAME:\r\n                return to_qvariant(_(\"Name\"))\r\n            elif section == SEQUENCE:\r\n                return to_qvariant(_(\"Shortcut\"))\r\n            elif section == SEARCH_SCORE:\r\n                return to_qvariant(_(\"Score\"))\r\n        return to_qvariant()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating the data of the item at the given index.", "response": "def setData(self, index, value, role=Qt.EditRole):\r\n        \"\"\"Qt Override.\"\"\"\r\n        if index.isValid() and 0 <= index.row() < len(self.shortcuts):\r\n            shortcut = self.shortcuts[index.row()]\r\n            column = index.column()\r\n            text = from_qvariant(value, str)\r\n            if column == SEQUENCE:\r\n                shortcut.key = text\r\n            self.dataChanged.emit(index, index)\r\n            return True\r\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update_search_letters(self, text):\r\n        self.letters = text\r\n        names = [shortcut.name for shortcut in self.shortcuts]\r\n        results = get_search_scores(text, names, template='<b>{0}</b>')\r\n        self.normal_text, self.rich_text, self.scores = zip(*results)\r\n        self.reset()", "response": "Update search letters with text input in search box."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting regular expression for filter.", "response": "def set_filter(self, text):\r\n        \"\"\"Set regular expression for filter.\"\"\"\r\n        self.pattern = get_search_regex(text)\r\n        if self.pattern:\r\n            self._parent.setSortingEnabled(False)\r\n        else:\r\n            self._parent.setSortingEnabled(True)\r\n        self.invalidateFilter()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn True if the row with the given row_num is accepted False otherwise.", "response": "def filterAcceptsRow(self, row_num, parent):\r\n        \"\"\"Qt override.\r\n\r\n        Reimplemented from base class to allow the use of custom filtering.\r\n        \"\"\"\r\n        model = self.sourceModel()\r\n        name = model.row(row_num).name\r\n        r = re.search(self.pattern, name)\r\n\r\n        if r is None:\r\n            return False\r\n        else:\r\n            return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef focusOutEvent(self, e):\r\n        self.source_model.update_active_row()\r\n        super(ShortcutsTable, self).focusOutEvent(e)", "response": "Override focusOutEvent to update the active row"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\noverrides focusInEvent to select the current row.", "response": "def focusInEvent(self, e):\r\n        \"\"\"Qt Override.\"\"\"\r\n        super(ShortcutsTable, self).focusInEvent(e)\r\n        self.selectRow(self.currentIndex().row())"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadjusts column size based on contents.", "response": "def adjust_cells(self):\r\n        \"\"\"Adjust column size based on contents.\"\"\"\r\n        self.resizeColumnsToContents()\r\n        fm = self.horizontalHeader().fontMetrics()\r\n        names = [fm.width(s.name + ' '*9) for s in self.source_model.shortcuts]\r\n        self.setColumnWidth(NAME, max(names))\r\n        self.horizontalHeader().setStretchLastSection(True)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_shortcuts(self):\r\n        shortcuts = []\r\n        for context, name, keystr in iter_shortcuts():\r\n            shortcut = Shortcut(context, name, keystr)\r\n            shortcuts.append(shortcut)\r\n        shortcuts = sorted(shortcuts, key=lambda x: x.context+x.name)\r\n        # Store the original order of shortcuts\r\n        for i, shortcut in enumerate(shortcuts):\r\n            shortcut.index = i\r\n        self.source_model.shortcuts = shortcuts\r\n        self.source_model.scores = [0]*len(shortcuts)\r\n        self.source_model.rich_text = [s.name for s in shortcuts]\r\n        self.source_model.reset()\r\n        self.adjust_cells()\r\n        self.sortByColumn(CONTEXT, Qt.AscendingOrder)", "response": "Load shortcuts and assign to table model."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check_shortcuts(self):\r\n        conflicts = []\r\n        for index, sh1 in enumerate(self.source_model.shortcuts):\r\n            if index == len(self.source_model.shortcuts)-1:\r\n                break\r\n            if str(sh1.key) == '':\r\n                continue\r\n            for sh2 in self.source_model.shortcuts[index+1:]:\r\n                if sh2 is sh1:\r\n                    continue\r\n                if str(sh2.key) == str(sh1.key) \\\r\n                   and (sh1.context == sh2.context or sh1.context == '_' or\r\n                        sh2.context == '_'):\r\n                    conflicts.append((sh1, sh2))\r\n        if conflicts:\r\n            self.parent().show_this_page.emit()\r\n            cstr = \"\\n\".join(['%s <---> %s' % (sh1, sh2)\r\n                              for sh1, sh2 in conflicts])\r\n            QMessageBox.warning(self, _(\"Conflicts\"),\r\n                                _(\"The following conflicts have been \"\r\n                                  \"detected:\")+\"\\n\"+cstr, QMessageBox.Ok)", "response": "Check if any of the shortcuts are not conflicts."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsaving shortcuts from table model.", "response": "def save_shortcuts(self):\r\n        \"\"\"Save shortcuts from table model.\"\"\"\r\n        self.check_shortcuts()\r\n        for shortcut in self.source_model.shortcuts:\r\n            shortcut.save()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates setup and display the shortcut editor dialog.", "response": "def show_editor(self):\r\n        \"\"\"Create, setup and display the shortcut editor dialog.\"\"\"\r\n        index = self.proxy_model.mapToSource(self.currentIndex())\r\n        row, column = index.row(), index.column()\r\n        shortcuts = self.source_model.shortcuts\r\n        context = shortcuts[row].context\r\n        name = shortcuts[row].name\r\n\r\n        sequence_index = self.source_model.index(row, SEQUENCE)\r\n        sequence = sequence_index.data()\r\n\r\n        dialog = ShortcutEditor(self, context, name, sequence, shortcuts)\r\n\r\n        if dialog.exec_():\r\n            new_sequence = dialog.new_sequence\r\n            self.source_model.setData(sequence_index, new_sequence)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_regex(self, regex=None, reset=False):\r\n        if reset:\r\n            text = ''\r\n        else:\r\n            text = self.finder.text().replace(' ', '').lower()\r\n\r\n        self.proxy_model.set_filter(text)\r\n        self.source_model.update_search_letters(text)\r\n        self.sortByColumn(SEARCH_SCORE, Qt.AscendingOrder)\r\n\r\n        if self.last_regex != regex:\r\n            self.selectRow(0)\r\n        self.last_regex = regex", "response": "Update the regex text for the shortcut finder."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmoves to next row from currently selected row.", "response": "def next_row(self):\r\n        \"\"\"Move to next row from currently selected row.\"\"\"\r\n        row = self.currentIndex().row()\r\n        rows = self.proxy_model.rowCount()\r\n        if row + 1 == rows:\r\n            row = -1\r\n        self.selectRow(row + 1)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef previous_row(self):\r\n        row = self.currentIndex().row()\r\n        rows = self.proxy_model.rowCount()\r\n        if row == 0:\r\n            row = rows\r\n        self.selectRow(row - 1)", "response": "Move to previous row from currently selected row."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef keyPressEvent(self, event):\r\n        key = event.key()\r\n        if key in [Qt.Key_Enter, Qt.Key_Return]:\r\n            self.show_editor()\r\n        elif key in [Qt.Key_Tab]:\r\n            self.finder.setFocus()\r\n        elif key in [Qt.Key_Backtab]:\r\n            self.parent().reset_btn.setFocus()\r\n        elif key in [Qt.Key_Up, Qt.Key_Down, Qt.Key_Left, Qt.Key_Right]:\r\n            super(ShortcutsTable, self).keyPressEvent(event)\r\n        elif key not in [Qt.Key_Escape, Qt.Key_Space]:\r\n            text = event.text()\r\n            if text:\r\n                if re.search(VALID_FINDER_CHARS, text) is not None:\r\n                    self.finder.setFocus()\r\n                    self.finder.set_text(text)\r\n        elif key in [Qt.Key_Escape]:\r\n            self.finder.keyPressEvent(event)", "response": "Override this method to handle key press events."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef reset_to_default(self):\r\n        reset = QMessageBox.warning(self, _(\"Shortcuts reset\"),\r\n                                    _(\"Do you want to reset \"\r\n                                      \"to default values?\"),\r\n                                    QMessageBox.Yes | QMessageBox.No)\r\n        if reset == QMessageBox.No:\r\n            return\r\n        reset_shortcuts()\r\n        self.main.apply_shortcuts()\r\n        self.table.load_shortcuts()\r\n        self.load_from_conf()\r\n        self.set_modified(False)", "response": "Reset to default values of the shortcuts making a confirmation."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets a color scheme from config using its name", "response": "def get_color_scheme(name):\r\n    \"\"\"Get a color scheme from config using its name\"\"\"\r\n    name = name.lower()\r\n    scheme = {}\r\n    for key in COLOR_SCHEME_KEYS:\r\n        try:\r\n            scheme[key] = CONF.get('appearance', name+'/'+key)\r\n        except:\r\n            scheme[key] = CONF.get('appearance', 'spyder/'+key)\r\n    return scheme"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef make_python_patterns(additional_keywords=[], additional_builtins=[]):\r\n    \"Strongly inspired from idlelib.ColorDelegator.make_pat\"\r\n    kwlist = keyword.kwlist + additional_keywords\r\n    builtinlist = [str(name) for name in dir(builtins)\r\n                   if not name.startswith('_')] + additional_builtins\r\n    repeated = set(kwlist) & set(builtinlist)\r\n    for repeated_element in repeated:\r\n        kwlist.remove(repeated_element)\r\n    kw = r\"\\b\" + any(\"keyword\", kwlist) + r\"\\b\"\r\n    builtin = r\"([^.'\\\"\\\\#]\\b|^)\" + any(\"builtin\", builtinlist) + r\"\\b\"\r\n    comment = any(\"comment\", [r\"#[^\\n]*\"])\r\n    instance = any(\"instance\", [r\"\\bself\\b\",\r\n                                r\"\\bcls\\b\",\r\n                                (r\"^\\s*@([a-zA-Z_][a-zA-Z0-9_]*)\"\r\n                                     r\"(\\.[a-zA-Z_][a-zA-Z0-9_]*)*\")])\r\n    number_regex = [r\"\\b[+-]?[0-9]+[lLjJ]?\\b\",\r\n                    r\"\\b[+-]?0[xX][0-9A-Fa-f]+[lL]?\\b\",\r\n                    r\"\\b[+-]?0[oO][0-7]+[lL]?\\b\",\r\n                    r\"\\b[+-]?0[bB][01]+[lL]?\\b\",\r\n                    r\"\\b[+-]?[0-9]+(?:\\.[0-9]+)?(?:[eE][+-]?[0-9]+)?[jJ]?\\b\"]\r\n    if PY3:\r\n        prefix = \"r|u|R|U|f|F|fr|Fr|fR|FR|rf|rF|Rf|RF|b|B|br|Br|bR|BR|rb|rB|Rb|RB\"\r\n    else:\r\n        prefix = \"r|u|ur|R|U|UR|Ur|uR|b|B|br|Br|bR|BR\"\r\n    sqstring =     r\"(\\b(%s))?'[^'\\\\\\n]*(\\\\.[^'\\\\\\n]*)*'?\" % prefix\r\n    dqstring =     r'(\\b(%s))?\"[^\"\\\\\\n]*(\\\\.[^\"\\\\\\n]*)*\"?' % prefix\r\n    uf_sqstring =  r\"(\\b(%s))?'[^'\\\\\\n]*(\\\\.[^'\\\\\\n]*)*(\\\\)$(?!')$\" % prefix\r\n    uf_dqstring =  r'(\\b(%s))?\"[^\"\\\\\\n]*(\\\\.[^\"\\\\\\n]*)*(\\\\)$(?!\")$' % prefix\r\n    sq3string =    r\"(\\b(%s))?'''[^'\\\\]*((\\\\.|'(?!''))[^'\\\\]*)*(''')?\" % prefix\r\n    dq3string =    r'(\\b(%s))?\"\"\"[^\"\\\\]*((\\\\.|\"(?!\"\"))[^\"\\\\]*)*(\"\"\")?' % prefix\r\n    uf_sq3string = r\"(\\b(%s))?'''[^'\\\\]*((\\\\.|'(?!''))[^'\\\\]*)*(\\\\)?(?!''')$\" \\\r\n                   % prefix\r\n    uf_dq3string = r'(\\b(%s))?\"\"\"[^\"\\\\]*((\\\\.|\"(?!\"\"))[^\"\\\\]*)*(\\\\)?(?!\"\"\")$' \\\r\n                   % prefix\r\n    # Needed to achieve correct highlighting in Python 3.6+\r\n    # See issue 7324\r\n    if PY36_OR_MORE:\r\n        # Based on\r\n        # https://github.com/python/cpython/blob/\r\n        # 81950495ba2c36056e0ce48fd37d514816c26747/Lib/tokenize.py#L117\r\n        # In order: Hexnumber, Binnumber, Octnumber, Decnumber,\r\n        # Pointfloat + Exponent, Expfloat, Imagnumber\r\n        number_regex = [\r\n                r\"\\b[+-]?0[xX](?:_?[0-9A-Fa-f])+[lL]?\\b\",\r\n                r\"\\b[+-]?0[bB](?:_?[01])+[lL]?\\b\",\r\n                r\"\\b[+-]?0[oO](?:_?[0-7])+[lL]?\\b\",\r\n                r\"\\b[+-]?(?:0(?:_?0)*|[1-9](?:_?[0-9])*)[lL]?\\b\",\r\n                r\"\\b((\\.[0-9](?:_?[0-9])*')|\\.[0-9](?:_?[0-9])*)\"\r\n                \"([eE][+-]?[0-9](?:_?[0-9])*)?[jJ]?\\b\",\r\n                r\"\\b[0-9](?:_?[0-9])*([eE][+-]?[0-9](?:_?[0-9])*)?[jJ]?\\b\",\r\n                r\"\\b[0-9](?:_?[0-9])*[jJ]\\b\"]\r\n    number = any(\"number\", number_regex)\r\n\r\n    string = any(\"string\", [sq3string, dq3string, sqstring, dqstring])\r\n    ufstring1 = any(\"uf_sqstring\", [uf_sqstring])\r\n    ufstring2 = any(\"uf_dqstring\", [uf_dqstring])\r\n    ufstring3 = any(\"uf_sq3string\", [uf_sq3string])\r\n    ufstring4 = any(\"uf_dq3string\", [uf_dq3string])\r\n    return \"|\".join([instance, kw, builtin, comment,\r\n                     ufstring1, ufstring2, ufstring3, ufstring4, string,\r\n                     number, any(\"SYNC\", [r\"\\n\"])])", "response": "Strongly inspired from idlelib. ColorDelegator. make_pat"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_code_cell_name(text):\r\n    name = text.strip().lstrip(\"#% \")\r\n    if name.startswith(\"<codecell>\"):\r\n        name = name[10:].lstrip()\r\n    elif name.startswith(\"In[\"):\r\n        name = name[2:]\r\n        if name.endswith(\"]:\"):\r\n            name = name[:-1]\r\n        name = name.strip()\r\n    return name", "response": "Returns a code cell name from a code cell comment."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef make_generic_c_patterns(keywords, builtins,\r\n                            instance=None, define=None, comment=None):\r\n    \"Strongly inspired from idlelib.ColorDelegator.make_pat\"\r\n    kw = r\"\\b\" + any(\"keyword\", keywords.split()) + r\"\\b\"\r\n    builtin = r\"\\b\" + any(\"builtin\", builtins.split()+C_TYPES.split()) + r\"\\b\"\r\n    if comment is None:\r\n        comment = any(\"comment\", [r\"//[^\\n]*\", r\"\\/\\*(.*?)\\*\\/\"])\r\n    comment_start = any(\"comment_start\", [r\"\\/\\*\"])\r\n    comment_end = any(\"comment_end\", [r\"\\*\\/\"])\r\n    if instance is None:\r\n        instance = any(\"instance\", [r\"\\bthis\\b\"])\r\n    number = any(\"number\",\r\n                 [r\"\\b[+-]?[0-9]+[lL]?\\b\",\r\n                  r\"\\b[+-]?0[xX][0-9A-Fa-f]+[lL]?\\b\",\r\n                  r\"\\b[+-]?[0-9]+(?:\\.[0-9]+)?(?:[eE][+-]?[0-9]+)?\\b\"])\r\n    sqstring = r\"(\\b[rRuU])?'[^'\\\\\\n]*(\\\\.[^'\\\\\\n]*)*'?\"\r\n    dqstring = r'(\\b[rRuU])?\"[^\"\\\\\\n]*(\\\\.[^\"\\\\\\n]*)*\"?'\r\n    string = any(\"string\", [sqstring, dqstring])\r\n    if define is None:\r\n        define = any(\"define\", [r\"#[^\\n]*\"])\r\n    return \"|\".join([instance, kw, comment, string, number,\r\n                     comment_start, comment_end, builtin,\r\n                     define, any(\"SYNC\", [r\"\\n\"])])", "response": "Strongly inspired from idlelib. ColorDelegator. make_pat"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef make_nsis_patterns():\r\n    \"Strongly inspired from idlelib.ColorDelegator.make_pat\"\r\n    kwstr1 = 'Abort AddBrandingImage AddSize AllowRootDirInstall AllowSkipFiles AutoCloseWindow BGFont BGGradient BrandingText BringToFront Call CallInstDLL Caption ClearErrors CompletedText ComponentText CopyFiles CRCCheck CreateDirectory CreateFont CreateShortCut Delete DeleteINISec DeleteINIStr DeleteRegKey DeleteRegValue DetailPrint DetailsButtonText DirText DirVar DirVerify EnableWindow EnumRegKey EnumRegValue Exec ExecShell ExecWait Exch ExpandEnvStrings File FileBufSize FileClose FileErrorText FileOpen FileRead FileReadByte FileSeek FileWrite FileWriteByte FindClose FindFirst FindNext FindWindow FlushINI Function FunctionEnd GetCurInstType GetCurrentAddress GetDlgItem GetDLLVersion GetDLLVersionLocal GetErrorLevel GetFileTime GetFileTimeLocal GetFullPathName GetFunctionAddress GetInstDirError GetLabelAddress GetTempFileName Goto HideWindow ChangeUI CheckBitmap Icon IfAbort IfErrors IfFileExists IfRebootFlag IfSilent InitPluginsDir InstallButtonText InstallColors InstallDir InstallDirRegKey InstProgressFlags InstType InstTypeGetText InstTypeSetText IntCmp IntCmpU IntFmt IntOp IsWindow LangString LicenseBkColor LicenseData LicenseForceSelection LicenseLangString LicenseText LoadLanguageFile LogSet LogText MessageBox MiscButtonText Name OutFile Page PageCallbacks PageEx PageExEnd Pop Push Quit ReadEnvStr ReadINIStr ReadRegDWORD ReadRegStr Reboot RegDLL Rename ReserveFile Return RMDir SearchPath Section SectionEnd SectionGetFlags SectionGetInstTypes SectionGetSize SectionGetText SectionIn SectionSetFlags SectionSetInstTypes SectionSetSize SectionSetText SendMessage SetAutoClose SetBrandingImage SetCompress SetCompressor SetCompressorDictSize SetCtlColors SetCurInstType SetDatablockOptimize SetDateSave SetDetailsPrint SetDetailsView SetErrorLevel SetErrors SetFileAttributes SetFont SetOutPath SetOverwrite SetPluginUnload SetRebootFlag SetShellVarContext SetSilent ShowInstDetails ShowUninstDetails ShowWindow SilentInstall SilentUnInstall Sleep SpaceTexts StrCmp StrCpy StrLen SubCaption SubSection SubSectionEnd UninstallButtonText UninstallCaption UninstallIcon UninstallSubCaption UninstallText UninstPage UnRegDLL Var VIAddVersionKey VIProductVersion WindowIcon WriteINIStr WriteRegBin WriteRegDWORD WriteRegExpandStr WriteRegStr WriteUninstaller XPStyle'\r\n    kwstr2 = 'all alwaysoff ARCHIVE auto both bzip2 components current custom details directory false FILE_ATTRIBUTE_ARCHIVE FILE_ATTRIBUTE_HIDDEN FILE_ATTRIBUTE_NORMAL FILE_ATTRIBUTE_OFFLINE FILE_ATTRIBUTE_READONLY FILE_ATTRIBUTE_SYSTEM FILE_ATTRIBUTE_TEMPORARY force grey HIDDEN hide IDABORT IDCANCEL IDIGNORE IDNO IDOK IDRETRY IDYES ifdiff ifnewer instfiles instfiles lastused leave left level license listonly lzma manual MB_ABORTRETRYIGNORE MB_DEFBUTTON1 MB_DEFBUTTON2 MB_DEFBUTTON3 MB_DEFBUTTON4 MB_ICONEXCLAMATION MB_ICONINFORMATION MB_ICONQUESTION MB_ICONSTOP MB_OK MB_OKCANCEL MB_RETRYCANCEL MB_RIGHT MB_SETFOREGROUND MB_TOPMOST MB_YESNO MB_YESNOCANCEL nevershow none NORMAL off OFFLINE on READONLY right RO show silent silentlog SYSTEM TEMPORARY text textonly true try uninstConfirm windows zlib'\r\n    kwstr3 = 'MUI_ABORTWARNING MUI_ABORTWARNING_CANCEL_DEFAULT MUI_ABORTWARNING_TEXT MUI_BGCOLOR MUI_COMPONENTSPAGE_CHECKBITMAP MUI_COMPONENTSPAGE_NODESC MUI_COMPONENTSPAGE_SMALLDESC MUI_COMPONENTSPAGE_TEXT_COMPLIST MUI_COMPONENTSPAGE_TEXT_DESCRIPTION_INFO MUI_COMPONENTSPAGE_TEXT_DESCRIPTION_TITLE MUI_COMPONENTSPAGE_TEXT_INSTTYPE MUI_COMPONENTSPAGE_TEXT_TOP MUI_CUSTOMFUNCTION_ABORT MUI_CUSTOMFUNCTION_GUIINIT MUI_CUSTOMFUNCTION_UNABORT MUI_CUSTOMFUNCTION_UNGUIINIT MUI_DESCRIPTION_TEXT MUI_DIRECTORYPAGE_BGCOLOR MUI_DIRECTORYPAGE_TEXT_DESTINATION MUI_DIRECTORYPAGE_TEXT_TOP MUI_DIRECTORYPAGE_VARIABLE MUI_DIRECTORYPAGE_VERIFYONLEAVE MUI_FINISHPAGE_BUTTON MUI_FINISHPAGE_CANCEL_ENABLED MUI_FINISHPAGE_LINK MUI_FINISHPAGE_LINK_COLOR MUI_FINISHPAGE_LINK_LOCATION MUI_FINISHPAGE_NOAUTOCLOSE MUI_FINISHPAGE_NOREBOOTSUPPORT MUI_FINISHPAGE_REBOOTLATER_DEFAULT MUI_FINISHPAGE_RUN MUI_FINISHPAGE_RUN_FUNCTION MUI_FINISHPAGE_RUN_NOTCHECKED MUI_FINISHPAGE_RUN_PARAMETERS MUI_FINISHPAGE_RUN_TEXT MUI_FINISHPAGE_SHOWREADME MUI_FINISHPAGE_SHOWREADME_FUNCTION MUI_FINISHPAGE_SHOWREADME_NOTCHECKED MUI_FINISHPAGE_SHOWREADME_TEXT MUI_FINISHPAGE_TEXT MUI_FINISHPAGE_TEXT_LARGE MUI_FINISHPAGE_TEXT_REBOOT MUI_FINISHPAGE_TEXT_REBOOTLATER MUI_FINISHPAGE_TEXT_REBOOTNOW MUI_FINISHPAGE_TITLE MUI_FINISHPAGE_TITLE_3LINES MUI_FUNCTION_DESCRIPTION_BEGIN MUI_FUNCTION_DESCRIPTION_END MUI_HEADER_TEXT MUI_HEADER_TRANSPARENT_TEXT MUI_HEADERIMAGE MUI_HEADERIMAGE_BITMAP MUI_HEADERIMAGE_BITMAP_NOSTRETCH MUI_HEADERIMAGE_BITMAP_RTL MUI_HEADERIMAGE_BITMAP_RTL_NOSTRETCH MUI_HEADERIMAGE_RIGHT MUI_HEADERIMAGE_UNBITMAP MUI_HEADERIMAGE_UNBITMAP_NOSTRETCH MUI_HEADERIMAGE_UNBITMAP_RTL MUI_HEADERIMAGE_UNBITMAP_RTL_NOSTRETCH MUI_HWND MUI_ICON MUI_INSTALLCOLORS MUI_INSTALLOPTIONS_DISPLAY MUI_INSTALLOPTIONS_DISPLAY_RETURN MUI_INSTALLOPTIONS_EXTRACT MUI_INSTALLOPTIONS_EXTRACT_AS MUI_INSTALLOPTIONS_INITDIALOG MUI_INSTALLOPTIONS_READ MUI_INSTALLOPTIONS_SHOW MUI_INSTALLOPTIONS_SHOW_RETURN MUI_INSTALLOPTIONS_WRITE MUI_INSTFILESPAGE_ABORTHEADER_SUBTEXT MUI_INSTFILESPAGE_ABORTHEADER_TEXT MUI_INSTFILESPAGE_COLORS MUI_INSTFILESPAGE_FINISHHEADER_SUBTEXT MUI_INSTFILESPAGE_FINISHHEADER_TEXT MUI_INSTFILESPAGE_PROGRESSBAR MUI_LANGDLL_ALLLANGUAGES MUI_LANGDLL_ALWAYSSHOW MUI_LANGDLL_DISPLAY MUI_LANGDLL_INFO MUI_LANGDLL_REGISTRY_KEY MUI_LANGDLL_REGISTRY_ROOT MUI_LANGDLL_REGISTRY_VALUENAME MUI_LANGDLL_WINDOWTITLE MUI_LANGUAGE MUI_LICENSEPAGE_BGCOLOR MUI_LICENSEPAGE_BUTTON MUI_LICENSEPAGE_CHECKBOX MUI_LICENSEPAGE_CHECKBOX_TEXT MUI_LICENSEPAGE_RADIOBUTTONS MUI_LICENSEPAGE_RADIOBUTTONS_TEXT_ACCEPT MUI_LICENSEPAGE_RADIOBUTTONS_TEXT_DECLINE MUI_LICENSEPAGE_TEXT_BOTTOM MUI_LICENSEPAGE_TEXT_TOP MUI_PAGE_COMPONENTS MUI_PAGE_CUSTOMFUNCTION_LEAVE MUI_PAGE_CUSTOMFUNCTION_PRE MUI_PAGE_CUSTOMFUNCTION_SHOW MUI_PAGE_DIRECTORY MUI_PAGE_FINISH MUI_PAGE_HEADER_SUBTEXT MUI_PAGE_HEADER_TEXT MUI_PAGE_INSTFILES MUI_PAGE_LICENSE MUI_PAGE_STARTMENU MUI_PAGE_WELCOME MUI_RESERVEFILE_INSTALLOPTIONS MUI_RESERVEFILE_LANGDLL MUI_SPECIALINI MUI_STARTMENU_GETFOLDER MUI_STARTMENU_WRITE_BEGIN MUI_STARTMENU_WRITE_END MUI_STARTMENUPAGE_BGCOLOR MUI_STARTMENUPAGE_DEFAULTFOLDER MUI_STARTMENUPAGE_NODISABLE MUI_STARTMENUPAGE_REGISTRY_KEY MUI_STARTMENUPAGE_REGISTRY_ROOT MUI_STARTMENUPAGE_REGISTRY_VALUENAME MUI_STARTMENUPAGE_TEXT_CHECKBOX MUI_STARTMENUPAGE_TEXT_TOP MUI_UI MUI_UI_COMPONENTSPAGE_NODESC MUI_UI_COMPONENTSPAGE_SMALLDESC MUI_UI_HEADERIMAGE MUI_UI_HEADERIMAGE_RIGHT MUI_UNABORTWARNING MUI_UNABORTWARNING_CANCEL_DEFAULT MUI_UNABORTWARNING_TEXT MUI_UNCONFIRMPAGE_TEXT_LOCATION MUI_UNCONFIRMPAGE_TEXT_TOP MUI_UNFINISHPAGE_NOAUTOCLOSE MUI_UNFUNCTION_DESCRIPTION_BEGIN MUI_UNFUNCTION_DESCRIPTION_END MUI_UNGETLANGUAGE MUI_UNICON MUI_UNPAGE_COMPONENTS MUI_UNPAGE_CONFIRM MUI_UNPAGE_DIRECTORY MUI_UNPAGE_FINISH MUI_UNPAGE_INSTFILES MUI_UNPAGE_LICENSE MUI_UNPAGE_WELCOME MUI_UNWELCOMEFINISHPAGE_BITMAP MUI_UNWELCOMEFINISHPAGE_BITMAP_NOSTRETCH MUI_UNWELCOMEFINISHPAGE_INI MUI_WELCOMEFINISHPAGE_BITMAP MUI_WELCOMEFINISHPAGE_BITMAP_NOSTRETCH MUI_WELCOMEFINISHPAGE_CUSTOMFUNCTION_INIT MUI_WELCOMEFINISHPAGE_INI MUI_WELCOMEPAGE_TEXT MUI_WELCOMEPAGE_TITLE MUI_WELCOMEPAGE_TITLE_3LINES'\r\n    bistr = 'addincludedir addplugindir AndIf cd define echo else endif error execute If ifdef ifmacrodef ifmacrondef ifndef include insertmacro macro macroend onGUIEnd onGUIInit onInit onInstFailed onInstSuccess onMouseOverSection onRebootFailed onSelChange onUserAbort onVerifyInstDir OrIf packhdr system undef verbose warning'\r\n    instance = any(\"instance\", [r'\\$\\{.*?\\}', r'\\$[A-Za-z0-9\\_]*'])\r\n    define = any(\"define\", [r\"\\![^\\n]*\"])\r\n    comment = any(\"comment\", [r\"\\;[^\\n]*\", r\"\\#[^\\n]*\", r\"\\/\\*(.*?)\\*\\/\"])\r\n    return make_generic_c_patterns(kwstr1+' '+kwstr2+' '+kwstr3, bistr,\r\n                                   instance=instance, define=define,\r\n                                   comment=comment)", "response": "Strongly inspired from idlelib. ColorDelegator. make_pat"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef make_yaml_patterns():\r\n    \"Strongly inspired from sublime highlighter \"\r\n    kw = any(\"keyword\", [r\":|>|-|\\||\\[|\\]|[A-Za-z][\\w\\s\\-\\_ ]+(?=:)\"])\r\n    links = any(\"normal\", [r\"#:[^\\n]*\"])\r\n    comment = any(\"comment\", [r\"#[^\\n]*\"])\r\n    number = any(\"number\",\r\n                 [r\"\\b[+-]?[0-9]+[lL]?\\b\",\r\n                  r\"\\b[+-]?0[xX][0-9A-Fa-f]+[lL]?\\b\",\r\n                  r\"\\b[+-]?[0-9]+(?:\\.[0-9]+)?(?:[eE][+-]?[0-9]+)?\\b\"])\r\n    sqstring = r\"(\\b[rRuU])?'[^'\\\\\\n]*(\\\\.[^'\\\\\\n]*)*'?\"\r\n    dqstring = r'(\\b[rRuU])?\"[^\"\\\\\\n]*(\\\\.[^\"\\\\\\n]*)*\"?'\r\n    string = any(\"string\", [sqstring, dqstring])\r\n    return \"|\".join([kw, string, number, links, comment, \r\n                     any(\"SYNC\", [r\"\\n\"])])", "response": "Strongly inspired from sublime highlighter"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef make_html_patterns():\r\n    tags = any(\"builtin\", [r\"<\", r\"[\\?/]?>\", r\"(?<=<).*?(?=[ >])\"])\r\n    keywords = any(\"keyword\", [r\" [\\w:-]*?(?==)\"])\r\n    string = any(\"string\", [r'\".*?\"'])\r\n    comment = any(\"comment\", [r\"<!--.*?-->\"])\r\n    multiline_comment_start = any(\"multiline_comment_start\", [r\"<!--\"])\r\n    multiline_comment_end = any(\"multiline_comment_end\", [r\"-->\"])\r\n    return \"|\".join([comment, multiline_comment_start,\r\n                     multiline_comment_end, tags, keywords, string])", "response": "Strongly inspired from idlelib. ColorDelegator. make_pat"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nhighlight a block of text.", "response": "def highlightBlock(self, text):\r\n        \"\"\"\r\n        Highlights a block of text. Please do not override, this method.\r\n        Instead you should implement\r\n        :func:`spyder.utils.syntaxhighplighters.SyntaxHighlighter.highlight_block`.\r\n\r\n        :param text: text to highlight.\r\n        \"\"\"\r\n        self.highlight_block(text)\r\n\r\n        # Process blocks for fold detection\r\n        current_block = self.currentBlock()\r\n        previous_block = self._find_prev_non_blank_block(current_block)\r\n        if self.editor:\r\n            if self.fold_detector is not None:\r\n                self.fold_detector._editor = weakref.ref(self.editor)\r\n                self.fold_detector.process_block(\r\n                    current_block, previous_block, text)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef highlight_spaces(self, text, offset=0):\r\n        flags_text = self.document().defaultTextOption().flags()\r\n        show_blanks =  flags_text & QTextOption.ShowTabsAndSpaces\r\n        if show_blanks:\r\n            format_leading = self.formats.get(\"leading\", None)\r\n            format_trailing = self.formats.get(\"trailing\", None)\r\n            match = self.BLANKPROG.search(text, offset)\r\n            while match:\r\n                start, end = match.span()\r\n                start = max([0, start+offset])\r\n                end = max([0, end+offset])\r\n                # Format trailing spaces at the end of the line. \r\n                if end == len(text) and format_trailing is not None:\r\n                    self.setFormat(start, end, format_trailing)\r\n                # Format leading spaces, e.g. indentation.\r\n                if start == 0 and format_leading is not None:\r\n                    self.setFormat(start, end, format_leading)\r\n                format = self.format(start)\r\n                color_foreground = format.foreground().color()\r\n                alpha_new = self.BLANK_ALPHA_FACTOR * color_foreground.alphaF()\r\n                color_foreground.setAlphaF(alpha_new)\r\n                self.setFormat(start, end-start, color_foreground)\r\n                match = self.BLANKPROG.search(text, match.end())", "response": "Highlights the blank spaces at the specified offset in the text."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nimplement specific highlight for Python.", "response": "def highlight_block(self, text):\r\n        \"\"\"Implement specific highlight for Python.\"\"\"\r\n        text = to_text_string(text)\r\n        prev_state = tbh.get_state(self.currentBlock().previous())\r\n        if prev_state == self.INSIDE_DQ3STRING:\r\n            offset = -4\r\n            text = r'\"\"\" '+text\r\n        elif prev_state == self.INSIDE_SQ3STRING:\r\n            offset = -4\r\n            text = r\"''' \"+text\r\n        elif prev_state == self.INSIDE_DQSTRING:\r\n            offset = -2\r\n            text = r'\" '+text\r\n        elif prev_state == self.INSIDE_SQSTRING:\r\n            offset = -2\r\n            text = r\"' \"+text\r\n        else:\r\n            offset = 0\r\n            prev_state = self.NORMAL\r\n        \r\n        oedata = None\r\n        import_stmt = None\r\n\r\n        self.setFormat(0, len(text), self.formats[\"normal\"])\r\n        \r\n        state = self.NORMAL\r\n        match = self.PROG.search(text)\r\n        while match:\r\n            for key, value in list(match.groupdict().items()):\r\n                if value:\r\n                    start, end = match.span(key)\r\n                    start = max([0, start+offset])\r\n                    end = max([0, end+offset])\r\n                    if key == \"uf_sq3string\":\r\n                        self.setFormat(start, end-start,\r\n                                       self.formats[\"string\"])\r\n                        state = self.INSIDE_SQ3STRING\r\n                    elif key == \"uf_dq3string\":\r\n                        self.setFormat(start, end-start,\r\n                                       self.formats[\"string\"])\r\n                        state = self.INSIDE_DQ3STRING\r\n                    elif key == \"uf_sqstring\":\r\n                        self.setFormat(start, end-start,\r\n                                       self.formats[\"string\"])\r\n                        state = self.INSIDE_SQSTRING\r\n                    elif key == \"uf_dqstring\":\r\n                        self.setFormat(start, end-start,\r\n                                       self.formats[\"string\"])\r\n                        state = self.INSIDE_DQSTRING\r\n                    else:\r\n                        self.setFormat(start, end-start, self.formats[key])\r\n                        if key == \"comment\":\r\n                            if text.lstrip().startswith(self.cell_separators):\r\n                                self.found_cell_separators = True\r\n                                oedata = OutlineExplorerData()\r\n                                oedata.text = to_text_string(text).strip()\r\n                                # cell_head: string contaning the first group\r\n                                # of '%'s in the cell header\r\n                                cell_head = re.search(r\"%+|$\",\r\n                                                      text.lstrip()).group()\r\n                                if cell_head == '':\r\n                                    oedata.cell_level = 0\r\n                                else:\r\n                                    oedata.cell_level = len(cell_head) - 2\r\n                                oedata.fold_level = start\r\n                                oedata.def_type = OutlineExplorerData.CELL\r\n                                oedata.def_name = get_code_cell_name(text)\r\n                            elif self.OECOMMENT.match(text.lstrip()):\r\n                                oedata = OutlineExplorerData()\r\n                                oedata.text = to_text_string(text).strip()\r\n                                oedata.fold_level = start\r\n                                oedata.def_type = OutlineExplorerData.COMMENT\r\n                                oedata.def_name = text.strip()\r\n                        elif key == \"keyword\":\r\n                            if value in (\"def\", \"class\"):\r\n                                match1 = self.IDPROG.match(text, end)\r\n                                if match1:\r\n                                    start1, end1 = match1.span(1)\r\n                                    self.setFormat(start1, end1-start1,\r\n                                                   self.formats[\"definition\"])\r\n                                    oedata = OutlineExplorerData()\r\n                                    oedata.text = to_text_string(text)\r\n                                    oedata.fold_level = (len(text)\r\n                                                         - len(text.lstrip()))\r\n                                    oedata.def_type = self.DEF_TYPES[\r\n                                                        to_text_string(value)]\r\n                                    oedata.def_name = text[start1:end1]\r\n                                    oedata.color = self.formats[\"definition\"]\r\n                            elif value in (\"elif\", \"else\", \"except\", \"finally\",\r\n                                           \"for\", \"if\", \"try\", \"while\",\r\n                                           \"with\"):\r\n                                if text.lstrip().startswith(value):\r\n                                    oedata = OutlineExplorerData()\r\n                                    oedata.text = to_text_string(text).strip()\r\n                                    oedata.fold_level = start\r\n                                    oedata.def_type = \\\r\n                                        OutlineExplorerData.STATEMENT\r\n                                    oedata.def_name = text.strip()\r\n                            elif value == \"import\":\r\n                                import_stmt = text.strip()\r\n                                # color all the \"as\" words on same line, except\r\n                                # if in a comment; cheap approximation to the\r\n                                # truth\r\n                                if '#' in text:\r\n                                    endpos = text.index('#')\r\n                                else:\r\n                                    endpos = len(text)\r\n                                while True:\r\n                                    match1 = self.ASPROG.match(text, end,\r\n                                                               endpos)\r\n                                    if not match1:\r\n                                        break\r\n                                    start, end = match1.span(1)\r\n                                    self.setFormat(start, end-start,\r\n                                                   self.formats[\"keyword\"])\r\n                    \r\n            match = self.PROG.search(text, match.end())\r\n        \r\n        tbh.set_state(self.currentBlock(), state)\r\n        \r\n        # Use normal format for indentation and trailing spaces.\r\n        self.formats['leading'] = self.formats['normal']\r\n        self.formats['trailing'] = self.formats['normal']\r\n        self.highlight_spaces(text, offset)\r\n        \r\n        if oedata is not None:\r\n            block_nb = self.currentBlock().blockNumber()\r\n            self.outlineexplorer_data[block_nb] = oedata\r\n            self.outlineexplorer_data['found_cell_separators'] = self.found_cell_separators\r\n        if import_stmt is not None:\r\n            block_nb = self.currentBlock().blockNumber()\r\n            self.import_statements[block_nb] = import_stmt"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nimplement highlight specific for C ++ and C ++.", "response": "def highlight_block(self, text):\r\n        \"\"\"Implement highlight specific for C/C++.\"\"\"\r\n        text = to_text_string(text)\r\n        inside_comment = tbh.get_state(self.currentBlock().previous()) == self.INSIDE_COMMENT\r\n        self.setFormat(0, len(text),\r\n                       self.formats[\"comment\" if inside_comment else \"normal\"])\r\n        \r\n        match = self.PROG.search(text)\r\n        index = 0\r\n        while match:\r\n            for key, value in list(match.groupdict().items()):\r\n                if value:\r\n                    start, end = match.span(key)\r\n                    index += end-start\r\n                    if key == \"comment_start\":\r\n                        inside_comment = True\r\n                        self.setFormat(start, len(text)-start,\r\n                                       self.formats[\"comment\"])\r\n                    elif key == \"comment_end\":\r\n                        inside_comment = False\r\n                        self.setFormat(start, end-start,\r\n                                       self.formats[\"comment\"])\r\n                    elif inside_comment:\r\n                        self.setFormat(start, end-start,\r\n                                       self.formats[\"comment\"])\r\n                    elif key == \"define\":\r\n                        self.setFormat(start, end-start,\r\n                                       self.formats[\"number\"])\r\n                    else:\r\n                        self.setFormat(start, end-start, self.formats[key])\r\n                    \r\n            match = self.PROG.search(text, match.end())\r\n        \r\n        self.highlight_spaces(text)\r\n        \r\n        last_state = self.INSIDE_COMMENT if inside_comment else self.NORMAL\r\n        tbh.set_state(self.currentBlock(), last_state)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nimplementing highlight specific for Fortran.", "response": "def highlight_block(self, text):\r\n        \"\"\"Implement highlight specific for Fortran.\"\"\"\r\n        text = to_text_string(text)\r\n        self.setFormat(0, len(text), self.formats[\"normal\"])\r\n        \r\n        match = self.PROG.search(text)\r\n        index = 0\r\n        while match:\r\n            for key, value in list(match.groupdict().items()):\r\n                if value:\r\n                    start, end = match.span(key)\r\n                    index += end-start\r\n                    self.setFormat(start, end-start, self.formats[key])\r\n                    if value.lower() in (\"subroutine\", \"module\", \"function\"):\r\n                        match1 = self.IDPROG.match(text, end)\r\n                        if match1:\r\n                            start1, end1 = match1.span(1)\r\n                            self.setFormat(start1, end1-start1,\r\n                                           self.formats[\"definition\"])\r\n                    \r\n            match = self.PROG.search(text, match.end())\r\n        \r\n        self.highlight_spaces(text)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef highlight_block(self, text):\r\n        text = to_text_string(text)\r\n        if text.startswith((\"c\", \"C\")):\r\n            self.setFormat(0, len(text), self.formats[\"comment\"])\r\n            self.highlight_spaces(text)\r\n        else:\r\n            FortranSH.highlight_block(self, text)\r\n            self.setFormat(0, 5, self.formats[\"comment\"])\r\n            self.setFormat(73, max([73, len(text)]),\r\n                           self.formats[\"comment\"])", "response": "Implement highlight specific for Fortran77 and FortranSH."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nimplement highlight specific Diff / Patch files.", "response": "def highlight_block(self, text):\r\n        \"\"\"Implement highlight specific Diff/Patch files.\"\"\"\r\n        text = to_text_string(text)\r\n        if text.startswith(\"+++\"):\r\n            self.setFormat(0, len(text), self.formats[\"keyword\"])\r\n        elif text.startswith(\"---\"):\r\n            self.setFormat(0, len(text), self.formats[\"keyword\"])\r\n        elif text.startswith(\"+\"):\r\n            self.setFormat(0, len(text), self.formats[\"string\"])\r\n        elif text.startswith(\"-\"):\r\n            self.setFormat(0, len(text), self.formats[\"number\"])\r\n        elif text.startswith(\"@\"):\r\n            self.setFormat(0, len(text), self.formats[\"builtin\"])\r\n        \r\n        self.highlight_spaces(text)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef highlight_block(self, text):\r\n        text = to_text_string(text)\r\n        previous_state = tbh.get_state(self.currentBlock().previous())\r\n        \r\n        if previous_state == self.COMMENT:\r\n            self.setFormat(0, len(text), self.formats[\"comment\"])\r\n        else:\r\n            previous_state = self.NORMAL\r\n            self.setFormat(0, len(text), self.formats[\"normal\"])\r\n        \r\n        tbh.set_state(self.currentBlock(), previous_state)\r\n        match = self.PROG.search(text)        \r\n\r\n        match_count = 0\r\n        n_characters = len(text)\r\n        # There should never be more matches than characters in the text.\r\n        while match and match_count < n_characters:\r\n            match_dict = match.groupdict()\r\n            for key, value in list(match_dict.items()):\r\n                if value:\r\n                    start, end = match.span(key)\r\n                    if previous_state == self.COMMENT:\r\n                        if key == \"multiline_comment_end\":\r\n                            tbh.set_state(self.currentBlock(), self.NORMAL)\r\n                            self.setFormat(end, len(text),\r\n                                           self.formats[\"normal\"])\r\n                        else:\r\n                            tbh.set_state(self.currentBlock(), self.COMMENT)\r\n                            self.setFormat(0, len(text),\r\n                                           self.formats[\"comment\"])\r\n                    else:\r\n                        if key == \"multiline_comment_start\":\r\n                            tbh.set_state(self.currentBlock(), self.COMMENT)\r\n                            self.setFormat(start, len(text),\r\n                                           self.formats[\"comment\"])\r\n                        else:\r\n                            tbh.set_state(self.currentBlock(), self.NORMAL)\r\n                            try:\r\n                                self.setFormat(start, end-start,\r\n                                               self.formats[key])\r\n                            except KeyError:\r\n                                # happens with unmatched end-of-comment;\r\n                                # see issue 1462\r\n                                pass\r\n            \r\n            match = self.PROG.search(text, match.end())\r\n            match_count += 1\r\n        \r\n        self.highlight_spaces(text)", "response": "Implement highlight specific for CSS and HTML."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse the complete text and stores the format for each character.", "response": "def make_charlist(self):\r\n        \"\"\"Parses the complete text and stores format for each character.\"\"\"\r\n\r\n        def worker_output(worker, output, error):\r\n            \"\"\"Worker finished callback.\"\"\"\r\n            self._charlist = output\r\n            if error is None and output:\r\n                self._allow_highlight = True\r\n                self.rehighlight()\r\n            self._allow_highlight = False\r\n\r\n        text = to_text_string(self.document().toPlainText())\r\n        tokens = self._lexer.get_tokens(text)\r\n\r\n        # Before starting a new worker process make sure to end previous\r\n        # incarnations\r\n        self._worker_manager.terminate_all()\r\n\r\n        worker = self._worker_manager.create_python_worker(\r\n            self._make_charlist,\r\n            tokens,\r\n            self._tokmap,\r\n            self.formats,\r\n        )\r\n        worker.sig_finished.connect(worker_output)\r\n        worker.start()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses the text and stores the format code for each character in the list.", "response": "def _make_charlist(self, tokens, tokmap, formats):\r\n        \"\"\"\r\n        Parses the complete text and stores format for each character.\r\n\r\n        Uses the attached lexer to parse into a list of tokens and Pygments\r\n        token types.  Then breaks tokens into individual letters, each with a\r\n        Spyder token type attached.  Stores this list as self._charlist.\r\n\r\n        It's attached to the contentsChange signal of the parent QTextDocument\r\n        so that the charlist is updated whenever the document changes.\r\n        \"\"\"\r\n\r\n        def _get_fmt(typ):\r\n            \"\"\"Get the Spyder format code for the given Pygments token type.\"\"\"\r\n            # Exact matches first\r\n            if typ in tokmap:\r\n                return tokmap[typ]\r\n            # Partial (parent-> child) matches\r\n            for key, val in tokmap.items():\r\n                if typ in key: # Checks if typ is a subtype of key.\r\n                    return val\r\n\r\n            return 'normal'\r\n\r\n        charlist = []\r\n        for typ, token in tokens:\r\n            fmt = formats[_get_fmt(typ)]\r\n            for letter in token:\r\n                charlist.append((fmt, letter))\r\n\r\n        return charlist"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting all submodules of a given module", "response": "def get_submodules(mod):\r\n    \"\"\"Get all submodules of a given module\"\"\"\r\n    def catch_exceptions(module):\r\n        pass\r\n    try:\r\n        m = __import__(mod)\r\n        submodules = [mod]\r\n        submods = pkgutil.walk_packages(m.__path__, m.__name__ + '.',\r\n                                        catch_exceptions)\r\n        for sm in submods:\r\n            sm_name = sm[1]\r\n            submodules.append(sm_name)\r\n    except ImportError:\r\n        return []\r\n    except:\r\n        return [mod]\r\n    \r\n    return submodules"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_preferred_submodules():\r\n    # Path to the modules database\r\n    modules_path = get_conf_path('db')\r\n\r\n    # Modules database\r\n    modules_db = PickleShareDB(modules_path)\r\n\r\n    if 'submodules' in modules_db:\r\n        return modules_db['submodules']\r\n\r\n    submodules = []\r\n\r\n    for m in PREFERRED_MODULES:\r\n        submods = get_submodules(m)\r\n        submodules += submods\r\n    \r\n    modules_db['submodules'] = submodules\r\n    return submodules", "response": "Get all submodules of the main scientific modules and others of our\r\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning true if version is stable i. e. with letters in the final component.", "response": "def is_stable_version(version):\r\n    \"\"\"\r\n    Return true if version is stable, i.e. with letters in the final component.\r\n\r\n    Stable version examples: ``1.2``, ``1.3.4``, ``1.0.5``.\r\n    Non-stable version examples: ``1.3.4beta``, ``0.1.0rc1``, ``3.0.0dev0``.\r\n    \"\"\"\r\n    if not isinstance(version, tuple):\r\n        version = version.split('.')\r\n    last_part = version[-1]\r\n\r\n    if not re.search(r'[a-zA-Z]', last_part):\r\n        return True\r\n    else:\r\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning whether the dev configuration directory should be used.", "response": "def use_dev_config_dir(use_dev_config_dir=USE_DEV_CONFIG_DIR):\r\n    \"\"\"Return whether the dev configuration directory should used.\"\"\"\r\n    if use_dev_config_dir is not None:\r\n        if use_dev_config_dir.lower() in {'false', '0'}:\r\n            use_dev_config_dir = False\r\n    else:\r\n        use_dev_config_dir = DEV or not is_stable_version(__version__)\r\n    return use_dev_config_dir"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef debug_print(*message):\r\n    warnings.warn(\"debug_print is deprecated; use the logging module instead.\")\r\n    if get_debug_level():\r\n        ss = STDOUT\r\n        if PY3:\r\n            # This is needed after restarting and using debug_print\r\n            for m in message:\r\n                ss.buffer.write(str(m).encode('utf-8'))\r\n            print('', file=ss)\r\n        else:\r\n            print(*message, file=ss)", "response": "Output debug messages to stdout"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_home_dir():\r\n    try:\r\n        # expanduser() returns a raw byte string which needs to be\r\n        # decoded with the codec that the OS is using to represent\r\n        # file paths.\r\n        path = encoding.to_unicode_from_fs(osp.expanduser('~'))\r\n    except Exception:\r\n        path = ''\r\n\r\n    if osp.isdir(path):\r\n        return path\r\n    else:\r\n        # Get home from alternative locations\r\n        for env_var in ('HOME', 'USERPROFILE', 'TMP'):\r\n            # os.environ.get() returns a raw byte string which needs to be\r\n            # decoded with the codec that the OS is using to represent\r\n            # environment variables.\r\n            path = encoding.to_unicode_from_fs(os.environ.get(env_var, ''))\r\n            if osp.isdir(path):\r\n                return path\r\n            else:\r\n                path = ''\r\n\r\n        if not path:\r\n            raise RuntimeError('Please set the environment variable HOME to '\r\n                               'your user/home directory path so Spyder can '\r\n                               'start properly.')", "response": "Returns the user home directory path for the current user."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the path to a temp clean configuration dir for tests and safe mode.", "response": "def get_clean_conf_dir():\r\n    \"\"\"\r\n    Return the path to a temp clean configuration dir, for tests and safe mode.\r\n    \"\"\"\r\n    if sys.platform.startswith(\"win\"):\r\n        current_user = ''\r\n    else:\r\n        current_user = '-' + str(getpass.getuser())\r\n\r\n    conf_dir = osp.join(str(tempfile.gettempdir()),\r\n                        'pytest-spyder{0!s}'.format(current_user),\r\n                        SUBFOLDER)\r\n    return conf_dir"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_conf_path(filename=None):\r\n    # Define conf_dir\r\n    if running_under_pytest() or SAFE_MODE:\r\n        # Use clean config dir if running tests or the user requests it.\r\n        conf_dir = get_clean_conf_dir()\r\n    elif sys.platform.startswith('linux'):\r\n        # This makes us follow the XDG standard to save our settings\r\n        # on Linux, as it was requested on Issue 2629\r\n        xdg_config_home = os.environ.get('XDG_CONFIG_HOME', '')\r\n        if not xdg_config_home:\r\n            xdg_config_home = osp.join(get_home_dir(), '.config')\r\n        if not osp.isdir(xdg_config_home):\r\n            os.makedirs(xdg_config_home)\r\n        conf_dir = osp.join(xdg_config_home, SUBFOLDER)\r\n    else:\r\n        conf_dir = osp.join(get_home_dir(), SUBFOLDER)\r\n\r\n    # Create conf_dir\r\n    if not osp.isdir(conf_dir):\r\n        if running_under_pytest() or SAFE_MODE:\r\n            os.makedirs(conf_dir)\r\n        else:\r\n            os.mkdir(conf_dir)\r\n    if filename is None:\r\n        return conf_dir\r\n    else:\r\n        return osp.join(conf_dir, filename)", "response": "Return absolute path to the config file with the specified filename."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_module_path(modname):\r\n    return osp.abspath(osp.dirname(sys.modules[modname].__file__))", "response": "Return module *modname * base path"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_module_data_path(modname, relpath=None, attr_name='DATAPATH'):\r\n    datapath = getattr(sys.modules[modname], attr_name, '')\r\n    if datapath:\r\n        return datapath\r\n    else:\r\n        datapath = get_module_path(modname)\r\n        parentdir = osp.join(datapath, osp.pardir)\r\n        if osp.isfile(parentdir):\r\n            # Parent directory is not a directory but the 'library.zip' file:\r\n            # this is either a py2exe or a cx_Freeze distribution\r\n            datapath = osp.abspath(osp.join(osp.join(parentdir, osp.pardir),\r\n                                            modname))\r\n        if relpath is not None:\r\n            datapath = osp.abspath(osp.join(datapath, relpath))\r\n        return datapath", "response": "Returns the path to the data directory of a module."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_module_source_path(modname, basename=None):\r\n    srcpath = get_module_path(modname)\r\n    parentdir = osp.join(srcpath, osp.pardir)\r\n    if osp.isfile(parentdir):\r\n        # Parent directory is not a directory but the 'library.zip' file:\r\n        # this is either a py2exe or a cx_Freeze distribution\r\n        srcpath = osp.abspath(osp.join(osp.join(parentdir, osp.pardir),\r\n                                       modname))\r\n    if basename is not None:\r\n        srcpath = osp.abspath(osp.join(srcpath, basename))\r\n    return srcpath", "response": "Return the absolute path of the module containing the basename"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_image_path(name, default=\"not_found.png\"):\r\n    for img_path in IMG_PATH:\r\n        full_path = osp.join(img_path, name)\r\n        if osp.isfile(full_path):\r\n            return osp.abspath(full_path)\r\n    if default is not None:\r\n        img_path = osp.join(get_module_path('spyder'), 'images')\r\n        return osp.abspath(osp.join(img_path, default))", "response": "Return image absolute path"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_available_translations():\r\n    locale_path = get_module_data_path(\"spyder\", relpath=\"locale\",\r\n                                       attr_name='LOCALEPATH')\r\n    listdir = os.listdir(locale_path)\r\n    langs = [d for d in listdir if osp.isdir(osp.join(locale_path, d))]\r\n    langs = [DEFAULT_LANGUAGE] + langs\r\n\r\n    # Remove disabled languages\r\n    langs = list( set(langs) - set(DISABLED_LANGUAGES) )\r\n\r\n    # Check that there is a language code available in case a new translation\r\n    # is added, to ensure LANGUAGE_CODES is updated.\r\n    for lang in langs:\r\n        if lang not in LANGUAGE_CODES:\r\n            error = ('Update LANGUAGE_CODES (inside config/base.py) if a new '\r\n                     'translation has been added to Spyder')\r\n            print(error)  # spyder: test-skip\r\n            return ['en']\r\n    return langs", "response": "Returns a list of available translations for Spyder based on the folders found in the LOCALEPATH folder."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_interface_language():\r\n\r\n    # Solves issue #3627\r\n    try:\r\n        locale_language = locale.getdefaultlocale()[0]\r\n    except ValueError:\r\n        locale_language = DEFAULT_LANGUAGE\r\n\r\n    # Tests expect English as the interface language\r\n    if running_under_pytest():\r\n        locale_language = DEFAULT_LANGUAGE\r\n\r\n    language = DEFAULT_LANGUAGE\r\n\r\n    if locale_language is not None:\r\n        spyder_languages = get_available_translations()\r\n        for lang in spyder_languages:\r\n            if locale_language == lang:\r\n                language = locale_language\r\n                break\r\n            elif locale_language.startswith(lang) or \\\r\n              lang.startswith(locale_language):\r\n                language = lang\r\n                break\r\n\r\n    return language", "response": "Returns the language of the most recent available language for the given locale."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nloading language setting from language config file if it exists otherwise use the local settings.", "response": "def load_lang_conf():\r\n    \"\"\"\r\n    Load language setting from language config file if it exists, otherwise\r\n    try to use the local settings if Spyder provides a translation, or\r\n    return the default if no translation provided.\r\n    \"\"\"\r\n    if osp.isfile(LANG_FILE):\r\n        with open(LANG_FILE, 'r') as f:\r\n            lang = f.read()\r\n    else:\r\n        lang = get_interface_language()\r\n        save_lang_conf(lang)\r\n\r\n    # Save language again if it's been disabled\r\n    if lang.strip('\\n') in DISABLED_LANGUAGES:\r\n        lang = DEFAULT_LANGUAGE\r\n        save_lang_conf(lang)\r\n\r\n    return lang"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_translation(modname, dirname=None):\r\n    if dirname is None:\r\n        dirname = modname\r\n\r\n    def translate_dumb(x):\r\n        \"\"\"Dumb function to not use translations.\"\"\"\r\n        if not is_unicode(x):\r\n            return to_text_string(x, \"utf-8\")\r\n        return x\r\n\r\n    locale_path = get_module_data_path(dirname, relpath=\"locale\",\r\n                                       attr_name='LOCALEPATH')\r\n\r\n    # If LANG is defined in Ubuntu, a warning message is displayed,\r\n    # so in Unix systems we define the LANGUAGE variable.\r\n    language = load_lang_conf()\r\n    if os.name == 'nt':\r\n        # Trying to set LANG on Windows can fail when Spyder is\r\n        # run with admin privileges.\r\n        # Fixes issue 6886\r\n        try:\r\n            os.environ[\"LANG\"] = language      # Works on Windows\r\n        except Exception:\r\n            return translate_dumb\r\n    else:\r\n        os.environ[\"LANGUAGE\"] = language  # Works on Linux\r\n \r\n    import gettext\r\n    try:\r\n        _trans = gettext.translation(modname, locale_path, codeset=\"utf-8\")\r\n        lgettext = _trans.lgettext\r\n        def translate_gettext(x):\r\n            if not PY3 and is_unicode(x):\r\n                x = x.encode(\"utf-8\")\r\n            y = lgettext(x)\r\n            if is_text_string(y) and PY3:\r\n                return y\r\n            else:\r\n                return to_text_string(y, \"utf-8\")\r\n        return translate_gettext\r\n    except Exception:\r\n        return translate_dumb", "response": "Returns a callback that returns the translation of the module modname."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef reset_config_files():\r\n    print(\"*** Reset Spyder settings to defaults ***\", file=STDERR)\r\n    for fname in SAVED_CONFIG_FILES:\r\n        cfg_fname = get_conf_path(fname)\r\n        if osp.isfile(cfg_fname) or osp.islink(cfg_fname):\r\n            os.remove(cfg_fname)\r\n        elif osp.isdir(cfg_fname):\r\n            shutil.rmtree(cfg_fname)\r\n        else:\r\n            continue\r\n        print(\"removing:\", cfg_fname, file=STDERR)", "response": "Remove all config files that are not in SAVED_CONFIG_FILES"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nregisters plugin in Spyder s main window", "response": "def register_plugin(self):\r\n        \"\"\"Register plugin in Spyder's main window\"\"\"\r\n        ipyconsole = self.main.ipyconsole\r\n        treewidget = self.fileexplorer.treewidget\r\n\r\n        self.main.add_dockwidget(self)\r\n        self.fileexplorer.sig_open_file.connect(self.main.open_file)\r\n        self.register_widget_shortcuts(treewidget)\r\n\r\n        treewidget.sig_edit.connect(self.main.editor.load)\r\n        treewidget.sig_removed.connect(self.main.editor.removed)\r\n        treewidget.sig_removed_tree.connect(self.main.editor.removed_tree)\r\n        treewidget.sig_renamed.connect(self.main.editor.renamed)\r\n        treewidget.sig_renamed_tree.connect(self.main.editor.renamed_tree)\r\n        treewidget.sig_create_module.connect(self.main.editor.new)\r\n        treewidget.sig_new_file.connect(lambda t: self.main.editor.new(text=t))\r\n        treewidget.sig_open_interpreter.connect(\r\n            ipyconsole.create_client_from_path)\r\n        treewidget.redirect_stdio.connect(\r\n            self.main.redirect_internalshell_stdio)\r\n        treewidget.sig_run.connect(\r\n            lambda fname:\r\n            ipyconsole.run_script(fname, osp.dirname(fname), '', False, False,\r\n                                  False, True))\r\n        treewidget.sig_open_dir.connect(\r\n            lambda dirname:\r\n            self.main.workingdirectory.chdir(dirname,\r\n                                             refresh_explorer=False,\r\n                                             refresh_console=True))\r\n\r\n        self.main.editor.open_dir.connect(self.chdir)\r\n\r\n        # Signal \"set_explorer_cwd(QString)\" will refresh only the\r\n        # contents of path passed by the signal in explorer:\r\n        self.main.workingdirectory.set_explorer_cwd.connect(\r\n                     lambda directory: self.refresh_plugin(new_path=directory,\r\n                                                           force_current=True))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrefreshing the explorer widget", "response": "def refresh_plugin(self, new_path=None, force_current=True):\r\n        \"\"\"Refresh explorer widget\"\"\"\r\n        self.fileexplorer.treewidget.update_history(new_path)\r\n        self.fileexplorer.treewidget.refresh(new_path,\r\n                                             force_current=force_current)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns True if obj is type text string False otherwise.", "response": "def is_type_text_string(obj):\r\n    \"\"\"Return True if `obj` is type text string, False if it is anything else,\r\n    like an instance of a class that extends the basestring class.\"\"\"\r\n    if PY2:\r\n        # Python 2\r\n        return type(obj) in [str, unicode]\r\n    else:\r\n        # Python 3\r\n        return type(obj) in [str, bytes]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_binary_string(obj, encoding=None):\r\n    if PY2:\r\n        # Python 2\r\n        if encoding is None:\r\n            return str(obj)\r\n        else:\r\n            return obj.encode(encoding)\r\n    else:\r\n        # Python 3\r\n        return bytes(obj, 'utf-8' if encoding is None else encoding)", "response": "Convert obj to binary string in Python 3 or str in Python 2"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef save_history(self):\r\n        open(self.LOG_PATH, 'w').write(\"\\n\".join( \\\r\n                [to_text_string(self.pydocbrowser.url_combo.itemText(index))\r\n                 for index in range(self.pydocbrowser.url_combo.count())]))", "response": "Save history to a text file in user home directory"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef visibility_changed(self, enable):\r\n        super(SpyderPluginWidget, self).visibility_changed(enable)\r\n        if enable and not self.pydocbrowser.is_server_running():\r\n            self.pydocbrowser.initialize()", "response": "Override visibility has changed"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the widget to give focus to when the dockwidget is raised on top - level", "response": "def get_focus_widget(self):\r\n        \"\"\"\r\n        Return the widget to give focus to when\r\n        this plugin's dockwidget is raised on top-level\r\n        \"\"\"\r\n        self.pydocbrowser.url_combo.lineEdit().selectAll()\r\n        return self.pydocbrowser.url_combo"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef closing_plugin(self, cancelable=False):\r\n        self.save_history()\r\n        self.set_option('zoom_factor',\r\n                        self.pydocbrowser.webview.get_zoom_factor())\r\n        return True", "response": "Perform actions before parent main window is closed"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef synchronize(self):\r\n        answer = QMessageBox.question(self, _(\"Synchronize\"),\r\n            _(\"This will synchronize Spyder's path list with \"\r\n                    \"<b>PYTHONPATH</b> environment variable for current user, \"\r\n                    \"allowing you to run your Python modules outside Spyder \"\r\n                    \"without having to configure sys.path. \"\r\n                    \"<br>Do you want to clear contents of PYTHONPATH before \"\r\n                    \"adding Spyder's path list?\"),\r\n            QMessageBox.Yes | QMessageBox.No | QMessageBox.Cancel)\r\n        if answer == QMessageBox.Cancel:\r\n            return\r\n        elif answer == QMessageBox.Yes:\r\n            remove = True\r\n        else:\r\n            remove = False\r\n        from spyder.utils.environ import (get_user_env, set_user_env,\r\n                                          listdict2envdict)\r\n        env = get_user_env()\r\n        if remove:\r\n            ppath = self.active_pathlist+self.ro_pathlist\r\n        else:\r\n            ppath = env.get('PYTHONPATH', [])\r\n            if not isinstance(ppath, list):\r\n                ppath = [ppath]\r\n            ppath = [path for path in ppath\r\n                     if path not in (self.active_pathlist+self.ro_pathlist)]\r\n            ppath.extend(self.active_pathlist+self.ro_pathlist)\r\n        env['PYTHONPATH'] = ppath\r\n        set_user_env(listdict2envdict(env), parent=self)", "response": "Synchronize Spyder s path list with the current user s Python path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_list(self):\r\n        self.listwidget.clear()\r\n        for name in self.pathlist+self.ro_pathlist:\r\n            item = QListWidgetItem(name)\r\n            item.setIcon(ima.icon('DirClosedIcon'))\r\n            if name in self.ro_pathlist:\r\n                item.setFlags(Qt.NoItemFlags | Qt.ItemIsUserCheckable)\r\n                item.setCheckState(Qt.Checked)\r\n            elif name in self.not_active_pathlist:\r\n                item.setFlags(item.flags() | Qt.ItemIsUserCheckable)\r\n                item.setCheckState(Qt.Unchecked)\r\n            else:\r\n                item.setFlags(item.flags() | Qt.ItemIsUserCheckable)\r\n                item.setCheckState(Qt.Checked)\r\n            self.listwidget.addItem(item)\r\n        self.refresh()", "response": "Update the list with the new ones"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrefreshing the list widget with the current items.", "response": "def refresh(self, row=None):\r\n        \"\"\"Refresh widget\"\"\"\r\n        for widget in self.selection_widgets:\r\n            widget.setEnabled(self.listwidget.currentItem() is not None)\r\n        not_empty = self.listwidget.count() > 0\r\n        if self.sync_button is not None:\r\n            self.sync_button.setEnabled(not_empty)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef eventFilter(self, widget, event):\r\n        if ((event.type() == QEvent.MouseButtonPress and\r\n                 not self.geometry().contains(event.globalPos())) or\r\n                (event.type() == QEvent.KeyPress and\r\n                 event.key() == Qt.Key_Escape)):\r\n            # Exits editing\r\n            self.hide()\r\n            self.setFocus(False)\r\n            return True\r\n\r\n        # Event is not interessant, raise to parent\r\n        return QLineEdit.eventFilter(self, widget, event)", "response": "Catch clicks outside the object and ESC key press."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nactivates the edit tab.", "response": "def edit_tab(self, index):\r\n        \"\"\"Activate the edit tab.\"\"\"\r\n\r\n        # Sets focus, shows cursor\r\n        self.setFocus(True)\r\n\r\n        # Updates tab index\r\n        self.tab_index = index\r\n\r\n        # Gets tab size and shrinks to avoid overlapping tab borders\r\n        rect = self.main.tabRect(index)\r\n        rect.adjust(1, 1, -2, -1)\r\n\r\n        # Sets size\r\n        self.setFixedSize(rect.size())\r\n\r\n        # Places on top of the tab\r\n        self.move(self.main.mapToGlobal(rect.topLeft()))\r\n\r\n        # Copies tab name and selects all\r\n        text = self.main.tabText(index)\r\n        text = text.replace(u'&', u'')\r\n        if self.split_char:\r\n            text = text.split(self.split_char)[self.split_index]\r\n\r\n        self.setText(text)\r\n        self.selectAll()\r\n\r\n        if not self.isVisible():\r\n            # Makes editor visible\r\n            self.show()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\noverride Qt method to handle drop events.", "response": "def dropEvent(self, event):\r\n        \"\"\"Override Qt method\"\"\"\r\n        mimeData = event.mimeData()\r\n        index_from = int(mimeData.data(\"source-index\"))\r\n        index_to = self.tabAt(event.pos())\r\n        if index_to == -1:\r\n            index_to = self.count()\r\n        if int(mimeData.data(\"tabbar-id\")) != id(self):\r\n            tabwidget_from = to_text_string(mimeData.data(\"tabwidget-id\"))\r\n            \r\n            # We pass self object ID as a QString, because otherwise it would \r\n            # depend on the platform: long for 64bit, int for 32bit. Replacing \r\n            # by long all the time is not working on some 32bit platforms \r\n            # (see Issue 1094, Issue 1098)\r\n            self.sig_move_tab[(str, int, int)].emit(tabwidget_from, index_from,\r\n                                                    index_to)\r\n            event.acceptProposedAction()\r\n        elif index_from != index_to:\r\n            self.sig_move_tab.emit(index_from, index_to)\r\n            event.acceptProposedAction()\r\n        QTabBar.dropEvent(self, event)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\noverrides Qt method to trigger the tab name editor.", "response": "def mouseDoubleClickEvent(self, event):\r\n        \"\"\"Override Qt method to trigger the tab name editor.\"\"\"\r\n        if self.rename_tabs is True and \\\r\n                event.buttons() == Qt.MouseButtons(Qt.LeftButton):\r\n            # Tab index\r\n            index = self.tabAt(event.pos())\r\n            if index >= 0:\r\n                # Tab is valid, call tab name editor\r\n                self.tab_name_editor.edit_tab(index)\r\n        else:\r\n            # Event is not interesting, raise to parent\r\n            QTabBar.mouseDoubleClickEvent(self, event)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating browse tabs menu", "response": "def update_browse_tabs_menu(self):\r\n        \"\"\"Update browse tabs menu\"\"\"\r\n        self.browse_tabs_menu.clear()\r\n        names = []\r\n        dirnames = []\r\n        for index in range(self.count()):\r\n            if self.menu_use_tooltips:\r\n                text = to_text_string(self.tabToolTip(index))\r\n            else:\r\n                text = to_text_string(self.tabText(index))\r\n            names.append(text)\r\n            if osp.isfile(text):\r\n                # Testing if tab names are filenames\r\n                dirnames.append(osp.dirname(text))\r\n        offset = None\r\n        \r\n        # If tab names are all filenames, removing common path:\r\n        if len(names) == len(dirnames):\r\n            common = get_common_path(dirnames)\r\n            if common is None:\r\n                offset = None\r\n            else:\r\n                offset = len(common)+1\r\n                if offset <= 3:\r\n                    # Common path is not a path but a drive letter...\r\n                    offset = None\r\n\r\n        for index, text in enumerate(names):\r\n            tab_action = create_action(self, text[offset:],\r\n                                       icon=self.tabIcon(index),\r\n                                       toggled=lambda state, index=index:\r\n                                               self.setCurrentIndex(index),\r\n                                       tip=self.tabToolTip(index))\r\n            tab_action.setChecked(index == self.currentIndex())\r\n            self.browse_tabs_menu.addAction(tab_action)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_corner_widgets(self, corner_widgets):\r\n        assert isinstance(corner_widgets, dict)\r\n        assert all(key in (Qt.TopLeftCorner, Qt.TopRightCorner)\r\n                   for key in corner_widgets)\r\n        self.corner_widgets.update(corner_widgets)\r\n        for corner, widgets in list(self.corner_widgets.items()):\r\n            cwidget = QWidget()\r\n            cwidget.hide()\r\n            prev_widget = self.cornerWidget(corner)\r\n            if prev_widget:\r\n                prev_widget.close()\r\n            self.setCornerWidget(cwidget, corner)\r\n            clayout = QHBoxLayout()\r\n            clayout.setContentsMargins(0, 0, 0, 0)\r\n            for widget in widgets:\r\n                if isinstance(widget, int):\r\n                    clayout.addSpacing(widget)\r\n                else:\r\n                    clayout.addWidget(widget)\r\n            cwidget.setLayout(clayout)\r\n            cwidget.show()", "response": "Set tabs corner widgets"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\noverrides Qt method to handle context menu events", "response": "def contextMenuEvent(self, event):\r\n        \"\"\"Override Qt method\"\"\"\r\n        self.setCurrentIndex(self.tabBar().tabAt(event.pos()))\r\n        if self.menu:\r\n            self.menu.popup(event.globalPos())"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef mousePressEvent(self, event):\r\n        if event.button() == Qt.MidButton:\r\n            index = self.tabBar().tabAt(event.pos())\r\n            if index >= 0:\r\n                self.sig_close_tab.emit(index)\r\n                event.accept()\r\n                return\r\n        QTabWidget.mousePressEvent(self, event)", "response": "Override Qt method to emit close signal when tab is clicked."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\noverrides Qt method to handle key press events.", "response": "def keyPressEvent(self, event):\r\n        \"\"\"Override Qt method\"\"\"\r\n        ctrl = event.modifiers() & Qt.ControlModifier\r\n        key = event.key()\r\n        handled = False\r\n        if ctrl and self.count() > 0:\r\n            index = self.currentIndex()\r\n            if key == Qt.Key_PageUp:\r\n                if index > 0:\r\n                    self.setCurrentIndex(index - 1)\r\n                else:\r\n                    self.setCurrentIndex(self.count() - 1)\r\n                handled = True\r\n            elif key == Qt.Key_PageDown:\r\n                if index < self.count() - 1:\r\n                    self.setCurrentIndex(index + 1)\r\n                else:\r\n                    self.setCurrentIndex(0)\r\n                handled = True\r\n        if not handled:\r\n            QTabWidget.keyPressEvent(self, event)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nnavigates to the next tab.", "response": "def tab_navigate(self, delta=1):\r\n        \"\"\"Ctrl+Tab\"\"\"\r\n        if delta > 0 and self.currentIndex() == self.count()-1:\r\n            index = delta-1\r\n        elif delta < 0 and self.currentIndex() == 0:\r\n            index = self.count()+delta\r\n        else:\r\n            index = self.currentIndex()+delta\r\n        self.setCurrentIndex(index)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the close function for the tabs.", "response": "def set_close_function(self, func):\r\n        \"\"\"Setting Tabs close function\r\n        None -> tabs are not closable\"\"\"\r\n        state = func is not None\r\n        if state:\r\n            self.sig_close_tab.connect(func)\r\n        try:\r\n            # Assuming Qt >= 4.5\r\n            QTabWidget.setTabsClosable(self, state)\r\n            self.tabCloseRequested.connect(func)\r\n        except AttributeError:\r\n            # Workaround for Qt < 4.5\r\n            close_button = create_toolbutton(self, triggered=func,\r\n                                             icon=ima.icon('fileclose'),\r\n                                             tip=_(\"Close current tab\"))\r\n            self.setCornerWidget(close_button if state else None)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef move_tab(self, index_from, index_to):\r\n        self.move_data.emit(index_from, index_to)\r\n\r\n        tip, text = self.tabToolTip(index_from), self.tabText(index_from)\r\n        icon, widget = self.tabIcon(index_from), self.widget(index_from)\r\n        current_widget = self.currentWidget()\r\n        \r\n        self.removeTab(index_from)\r\n        self.insertTab(index_to, widget, icon, text)\r\n        self.setTabToolTip(index_to, tip)\r\n        \r\n        self.setCurrentWidget(current_widget)\r\n        self.move_tab_finished.emit()", "response": "Move tab inside a tabwidget"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmoving tab from a tabwidget to another", "response": "def move_tab_from_another_tabwidget(self, tabwidget_from,\r\n                                        index_from, index_to):\r\n        \"\"\"Move tab from a tabwidget to another\"\"\"\r\n            \r\n        # We pass self object IDs as QString objs, because otherwise it would \r\n        # depend on the platform: long for 64bit, int for 32bit. Replacing \r\n        # by long all the time is not working on some 32bit platforms \r\n        # (see Issue 1094, Issue 1098)\r\n        self.sig_move_tab.emit(tabwidget_from, to_text_string(id(self)),\r\n                               index_from, index_to)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_run_configuration(fname):\r\n    configurations = _get_run_configurations()\r\n    for filename, options in configurations:\r\n        if fname == filename:\r\n            runconf = RunConfiguration()\r\n            runconf.set(options)\r\n            return runconf", "response": "Return a RunConfiguration object for the given script name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nselecting directory in the current directory.", "response": "def select_directory(self):\r\n        \"\"\"Select directory\"\"\"\r\n        basedir = to_text_string(self.wd_edit.text())\r\n        if not osp.isdir(basedir):\r\n            basedir = getcwd_or_home()\r\n        directory = getexistingdirectory(self, _(\"Select directory\"), basedir)\r\n        if directory:\r\n            self.wd_edit.setText(directory)\r\n            self.dir = directory"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding widgets to dialog vertical layout", "response": "def add_widgets(self, *widgets_or_spacings):\r\n        \"\"\"Add widgets/spacing to dialog vertical layout\"\"\"\r\n        layout = self.layout()\r\n        for widget_or_spacing in widgets_or_spacings:\r\n            if isinstance(widget_or_spacing, int):\r\n                layout.addSpacing(widget_or_spacing)\r\n            else:\r\n                layout.addWidget(widget_or_spacing)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates dialog button box and add it to the dialog layout", "response": "def add_button_box(self, stdbtns):\r\n        \"\"\"Create dialog button box and add it to the dialog layout\"\"\"\r\n        bbox = QDialogButtonBox(stdbtns)\r\n        run_btn = bbox.addButton(_(\"Run\"), QDialogButtonBox.AcceptRole)\r\n        run_btn.clicked.connect(self.run_btn_clicked)\r\n        bbox.accepted.connect(self.accept)\r\n        bbox.rejected.connect(self.reject)\r\n        btnlayout = QHBoxLayout()\r\n        btnlayout.addStretch(1)\r\n        btnlayout.addWidget(bbox)\r\n        self.layout().addLayout(btnlayout)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef setup(self, fname):\r\n        self.filename = fname\r\n        self.runconfigoptions = RunConfigOptions(self)\r\n        self.runconfigoptions.set(RunConfiguration(fname).get())\r\n        self.add_widgets(self.runconfigoptions)\r\n        self.add_button_box(QDialogButtonBox.Cancel)\r\n        self.setWindowTitle(_(\"Run settings for %s\") % osp.basename(fname))", "response": "Setup Run Configuration dialog with filename fname"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef accept(self):\r\n        if not self.runconfigoptions.is_valid():\r\n            return\r\n        configurations = _get_run_configurations()\r\n        configurations.insert(0, (self.filename, self.runconfigoptions.get()))\r\n        _set_run_configurations(configurations)\r\n        QDialog.accept(self)", "response": "Reimplement Qt method accept"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef setup(self, fname):\r\n        combo_label = QLabel(_(\"Select a run configuration:\"))\r\n        self.combo = QComboBox()\r\n        self.combo.setMaxVisibleItems(20)\r\n        self.combo.setSizeAdjustPolicy(QComboBox.AdjustToMinimumContentsLength)\r\n        self.combo.setSizePolicy(QSizePolicy.Expanding, QSizePolicy.Fixed)\r\n        \r\n        self.stack = QStackedWidget()\r\n\r\n        configurations = _get_run_configurations()\r\n        for index, (filename, options) in enumerate(configurations):\r\n            if fname == filename:\r\n                break\r\n        else:\r\n            # There is no run configuration for script *fname*:\r\n            # creating a temporary configuration that will be kept only if\r\n            # dialog changes are accepted by the user\r\n            configurations.insert(0, (fname, RunConfiguration(fname).get()))\r\n            index = 0\r\n        for filename, options in configurations:\r\n            widget = RunConfigOptions(self)\r\n            widget.set(options)\r\n            self.combo.addItem(filename)\r\n            self.stack.addWidget(widget)\r\n        self.combo.currentIndexChanged.connect(self.stack.setCurrentIndex)\r\n        self.combo.setCurrentIndex(index)\r\n\r\n        self.add_widgets(combo_label, self.combo, 10, self.stack)\r\n        self.add_button_box(QDialogButtonBox.Ok|QDialogButtonBox.Cancel)\r\n\r\n        self.setWindowTitle(_(\"Run configuration per file\"))", "response": "Setup Run Configuration dialog with filename fname"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef paintEvent(self, event):\n        painter = QPainter(self)\n        painter.fillRect(event.rect(), self.editor.sideareas_color)\n        # This is needed to make that the font size of line numbers\n        # be the same as the text one when zooming\n        # See Issue 2296 and 4811\n        font = self.editor.font()\n        font_height = self.editor.fontMetrics().height()\n\n        active_block = self.editor.textCursor().block()\n        active_line_number = active_block.blockNumber() + 1\n\n        def draw_pixmap(ytop, pixmap):\n            if not QT55_VERSION:\n                pixmap_height = pixmap.height()\n            else:\n                # scale pixmap height to device independent pixels\n                pixmap_height = pixmap.height() / pixmap.devicePixelRatio()\n            painter.drawPixmap(0, ytop + (font_height-pixmap_height) / 2,\n                               pixmap)\n\n        for top, line_number, block in self.editor.visible_blocks:\n            if self._margin:\n                if line_number == active_line_number:\n                    font.setWeight(font.Bold)\n                    painter.setFont(font)\n                    painter.setPen(self.editor.normal_color)\n                else:\n                    font.setWeight(font.Normal)\n                    painter.setFont(font)\n                    painter.setPen(self.linenumbers_color)\n\n                painter.drawText(0, top, self.width(),\n                                 font_height,\n                                 Qt.AlignRight | Qt.AlignBottom,\n                                 to_text_string(line_number))\n\n            data = block.userData()\n            if self._markers_margin and data:\n                if data.code_analysis:\n                    for source, code, severity, message in data.code_analysis:\n                        error = severity == DiagnosticSeverity.ERROR\n                        if error:\n                            break\n                    if error:\n                        draw_pixmap(top, self.error_pixmap)\n                    else:\n                        draw_pixmap(top, self.warning_pixmap)\n                if data.todo:\n                    draw_pixmap(top, self.todo_pixmap)", "response": "Override Qt method. Painting line number area."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\noverriding Qt method. mouseMoveEvent Show code analisis if left button pressed select lines.", "response": "def mouseMoveEvent(self, event):\n        \"\"\"Override Qt method.\n\n        Show code analisis, if left button pressed select lines.\n        \"\"\"\n        line_number = self.editor.get_linenumber_from_mouse_event(event)\n        block = self.editor.document().findBlockByNumber(line_number-1)\n        data = block.userData()\n\n        # this disables pyflakes messages if there is an active drag/selection\n        # operation\n        check = self._released == -1\n        if data and data.code_analysis and check:\n            self.editor.show_code_analysis_results(line_number,\n                                                   data)\n        else:\n            self.editor.hide_tooltip()\n\n        if event.buttons() == Qt.LeftButton:\n            self._released = line_number\n            self.editor.select_lines(self._pressed, self._released)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef mousePressEvent(self, event):\n        line_number = self.editor.get_linenumber_from_mouse_event(event)\n        self._pressed = line_number\n        self._released = line_number\n        self.editor.select_lines(self._pressed,\n                                 self._released)", "response": "Override Qt method mousePressEvent Select line and starts selection\n       "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef compute_width(self):\n        if not self._enabled:\n            return 0\n        digits = 1\n        maxb = max(1, self.editor.blockCount())\n        while maxb >= 10:\n            maxb /= 10\n            digits += 1\n        if self._margin:\n            margin = 3+self.editor.fontMetrics().width('9'*digits)\n        else:\n            margin = 0\n        return margin+self.get_markers_margin()", "response": "Compute and return line number area width"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef setup_margins(self, linenumbers=True, markers=True):\n        self._margin = linenumbers\n        self._markers_margin = markers\n        self.set_enabled(linenumbers or markers)", "response": "Setup margin settings for this locale"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef process_python_symbol_data(oedata):\n    symbol_list = []\n    for key in oedata:\n        val = oedata[key]\n        if val and key != 'found_cell_separators':\n            if val.is_class_or_function():\n                symbol_list.append((key, val.def_name, val.fold_level,\n                                    val.get_token()))\n    return sorted(symbol_list)", "response": "Returns a list with line number definition name fold level and token."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_python_symbol_icons(oedata):\n    class_icon = ima.icon('class')\n    method_icon = ima.icon('method')\n    function_icon = ima.icon('function')\n    private_icon = ima.icon('private1')\n    super_private_icon = ima.icon('private2')\n\n    symbols = process_python_symbol_data(oedata)\n\n    # line - 1, name, fold level\n    fold_levels = sorted(list(set([s[2] for s in symbols])))\n    parents = [None]*len(symbols)\n    icons = [None]*len(symbols)\n    indexes = []\n\n    parent = None\n    for level in fold_levels:\n        for index, item in enumerate(symbols):\n            line, name, fold_level, token = item\n            if index in indexes:\n                continue\n\n            if fold_level == level:\n                indexes.append(index)\n                parent = item\n            else:\n                parents[index] = parent\n\n    for index, item in enumerate(symbols):\n        parent = parents[index]\n\n        if item[-1] == 'def':\n            icons[index] = function_icon\n        elif item[-1] == 'class':\n            icons[index] = class_icon\n        else:\n            icons[index] = QIcon()\n\n        if parent is not None:\n            if parent[-1] == 'class':\n                if item[-1] == 'def' and item[1].startswith('__'):\n                    icons[index] = super_private_icon\n                elif item[-1] == 'def' and item[1].startswith('_'):\n                    icons[index] = private_icon\n                else:\n                    icons[index] = method_icon\n\n    return icons", "response": "Return a list of icons for a python file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef shorten_paths(path_list, is_unsaved):\n    # TODO: at the end, if the path is too long, should do a more dumb kind of\n    # shortening, but not completely dumb.\n\n    # Convert the path strings to a list of tokens and start building the\n    # new_path using the drive\n    path_list = path_list[:]  # Make a local copy\n    new_path_list = []\n\n    for ii, (path, is_unsav) in enumerate(zip(path_list, is_unsaved)):\n        if is_unsav:\n            new_path_list.append(_('unsaved file'))\n            path_list[ii] = None\n        else:\n            drive, path = osp.splitdrive(osp.dirname(path))\n            new_path_list.append(drive + osp.sep)\n            path_list[ii] = [part for part in path.split(osp.sep) if part]\n\n    def recurse_level(level_idx):\n        sep = os.sep\n\n        # If toks are all empty we need not have recursed here\n        if not any(level_idx.values()):\n            return\n\n        # Firstly, find the longest common prefix for all in the level\n        # s = len of longest common prefix\n        sample_toks = list(level_idx.values())[0]\n        if not sample_toks:\n            s = 0\n        else:\n            for s, sample_val in enumerate(sample_toks):\n                if not all(len(toks) > s and toks[s] == sample_val\n                           for toks in level_idx.values()):\n                    break\n\n        # Shorten longest common prefix\n        if s == 0:\n            short_form = ''\n        else:\n            if s == 1:\n                short_form = sample_toks[0]\n            elif s == 2:\n                short_form = sample_toks[0] + sep + sample_toks[1]\n            else:\n                short_form = \"...\" + sep + sample_toks[s-1]\n            for idx in level_idx:\n                new_path_list[idx] += short_form + sep\n                level_idx[idx] = level_idx[idx][s:]\n\n        # Group the remaining bit after the common prefix, shorten, and recurse\n        while level_idx:\n            k, group = 0, level_idx  # k is length of the group's common prefix\n            while True:\n                # Abort if we've gone beyond end of one or more in the group\n                prospective_group = {idx: toks for idx, toks\n                                     in group.items() if len(toks) == k}\n                if prospective_group:\n                    if k == 0:  # we spit out the group with no suffix\n                        group = prospective_group\n                    break\n                # Only keep going if all n still match on the kth token\n                _, sample_toks = next(iteritems(group))\n                prospective_group = {idx: toks for idx, toks\n                                     in group.items()\n                                     if toks[k] == sample_toks[k]}\n                if len(prospective_group) == len(group) or k == 0:\n                    group = prospective_group\n                    k += 1\n                else:\n                    break\n            _, sample_toks = next(iteritems(group))\n            if k == 0:\n                short_form = ''\n            elif k == 1:\n                short_form = sample_toks[0]\n            elif k == 2:\n                short_form = sample_toks[0] + sep + sample_toks[1]\n            else:  # k > 2\n                short_form = sample_toks[0] + \"...\" + sep + sample_toks[k-1]\n            for idx in group.keys():\n                new_path_list[idx] += short_form + (sep if k > 0 else '')\n                del level_idx[idx]\n            recurse_level({idx: toks[k:] for idx, toks in group.items()})\n\n    recurse_level({i: pl for i, pl in enumerate(path_list) if pl})\n\n    return [path.rstrip(os.sep) for path in new_path_list]", "response": "Takes a list of paths and tries to intelligently shorten them all."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncall when the focus goes out of this widget.", "response": "def focusOutEvent(self, event):\n        \"\"\"\n        Detect when the focus goes out of this widget.\n\n        This is used to make the file switcher leave focus on the\n        last selected file by the user.\n        \"\"\"\n        self.clicked_outside = True\n        return super(QLineEdit, self).focusOutEvent(event)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef save_initial_state(self):\n        paths = self.paths\n        self.initial_widget = self.get_widget()\n        self.initial_cursors = {}\n\n        for i, editor in enumerate(self.widgets):\n            if editor is self.initial_widget:\n                self.initial_path = paths[i]\n            # This try is needed to make the fileswitcher work with \n            # plugins that does not have a textCursor.\n            try:\n                self.initial_cursors[paths[i]] = editor.textCursor()\n            except AttributeError:\n                pass", "response": "Save initial cursors and initial active widget."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef restore_initial_state(self):\n        self.list.clear()\n        self.is_visible = False\n        widgets = self.widgets_by_path\n\n        if not self.edit.clicked_outside:\n            for path in self.initial_cursors:\n                cursor = self.initial_cursors[path]\n                if path in widgets:\n                    self.set_editor_cursor(widgets[path], cursor)\n\n            if self.initial_widget in self.paths_by_widget:\n                index = self.paths.index(self.initial_path)\n                self.sig_goto_file.emit(index)", "response": "Restores initial cursors and initial active editor."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\npositions the file switcher dialog.", "response": "def set_dialog_position(self):\n        \"\"\"Positions the file switcher dialog.\"\"\"\n        parent = self.parent()\n        geo = parent.geometry()\n        width = self.list.width()  # This has been set in setup\n        left = parent.geometry().width()/2 - width/2\n        # Note: the +1 pixel on the top makes it look better\n        if isinstance(parent, QMainWindow):\n            top = (parent.toolbars_menu.geometry().height() +\n                   parent.menuBar().geometry().height() + 1)\n        else:\n            top = self.plugins_tabs[0][0].tabBar().geometry().height() + 1\n\n        while parent:\n            geo = parent.geometry()\n            top += geo.top()\n            left += geo.left()\n            parent = parent.parent()\n\n        self.move(left, top)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_item_size(self, content):\n        strings = []\n        if content:\n            for rich_text in content:\n                label = QLabel(rich_text)\n                label.setTextFormat(Qt.PlainText)\n                strings.append(label.text())\n                fm = label.fontMetrics()\n\n            return (max([fm.width(s) * 1.3 for s in strings]), fm.height())", "response": "Get the max size for the elements of a list of items as a QLabel."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadjusting the width and height of the list based on the relative size of the parent and content.", "response": "def fix_size(self, content):\n        \"\"\"\n        Adjusts the width and height of the file switcher\n        based on the relative size of the parent and content.\n        \"\"\"\n        # Update size of dialog based on relative size of the parent\n        if content:\n            width, height = self.get_item_size(content)\n\n            # Width\n            parent = self.parent()\n            relative_width = parent.geometry().width() * 0.65\n            if relative_width > self.MAX_WIDTH:\n                relative_width = self.MAX_WIDTH\n            self.list.setMinimumWidth(relative_width)\n\n            # Height\n            if len(content) < 15:\n                max_entries = len(content)\n            else:\n                max_entries = 15\n            max_height = height * max_entries * 1.7\n            self.list.setMinimumHeight(max_height)\n\n            # Resize\n            self.list.resize(relative_width, self.list.height())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef select_row(self, steps):\n        row = self.current_row() + steps\n        if 0 <= row < self.count():\n            self.set_current_row(row)", "response": "Select a row in the list widget based on a number of steps with direction."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nselect previous row in list widget.", "response": "def previous_row(self):\n        \"\"\"Select previous row in list widget.\"\"\"\n        if self.mode == self.SYMBOL_MODE:\n            self.select_row(-1)\n            return\n        prev_row = self.current_row() - 1\n        if prev_row >= 0:\n            title = self.list.item(prev_row).text()\n        else:\n            title = ''\n        if prev_row == 0 and '</b></big><br>' in title:\n            self.list.scrollToTop()\n        elif '</b></big><br>' in title:\n            # Select the next previous row, the one following is a title\n            self.select_row(-2)\n        else:\n            self.select_row(-1)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nselect next row in list widget.", "response": "def next_row(self):\n        \"\"\"Select next row in list widget.\"\"\"\n        if self.mode == self.SYMBOL_MODE:\n            self.select_row(+1)\n            return\n        next_row = self.current_row() + 1\n        if next_row < self.count():\n            if '</b></big><br>' in self.list.item(next_row).text():\n                # Select the next next row, the one following is a title\n                self.select_row(+2)\n            else:\n                self.select_row(+1)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_stack_index(self, stack_index, plugin_index):\n        other_plugins_count = sum([other_tabs[0].count() \\\n                                   for other_tabs in \\\n                                   self.plugins_tabs[:plugin_index]])\n        real_index = stack_index - other_plugins_count\n\n        return real_index", "response": "Get the real index of the selected item."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the data object of the plugin s current tab manager.", "response": "def get_plugin_data(self, plugin):\n        \"\"\"Get the data object of the plugin's current tab manager.\"\"\"\n        # The data object is named \"data\" in the editor plugin while it is\n        # named \"clients\" in the notebook plugin.\n        try:\n            data = plugin.get_current_tab_manager().data\n        except AttributeError:\n            data = plugin.get_current_tab_manager().clients\n\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the tabwidget of the plugin s current tab manager.", "response": "def get_plugin_tabwidget(self, plugin):\n        \"\"\"Get the tabwidget of the plugin's current tab manager.\"\"\"\n        # The tab widget is named \"tabs\" in the editor plugin while it is\n        # named \"tabwidget\" in the notebook plugin.\n        try:\n            tabwidget = plugin.get_current_tab_manager().tabs\n        except AttributeError:\n            tabwidget = plugin.get_current_tab_manager().tabwidget\n\n        return tabwidget"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_widget(self, index=None, path=None, tabs=None):\n        if (index and tabs) or (path and tabs):\n            return tabs.widget(index)\n        elif self.plugin:\n            return self.get_plugin_tabwidget(self.plugin).currentWidget()\n        else:\n            return self.plugins_tabs[0][0].currentWidget()", "response": "Get widget by index."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the cursor of an editor.", "response": "def set_editor_cursor(self, editor, cursor):\n        \"\"\"Set the cursor of an editor.\"\"\"\n        pos = cursor.position()\n        anchor = cursor.anchor()\n\n        new_cursor = QTextCursor()\n        if pos == anchor:\n            new_cursor.movePosition(pos)\n        else:\n            new_cursor.movePosition(anchor)\n            new_cursor.movePosition(pos, QTextCursor.KeepAnchor)\n        editor.setTextCursor(cursor)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngo to specified line number in current active editor.", "response": "def goto_line(self, line_number):\n        \"\"\"Go to specified line number in current active editor.\"\"\"\n        if line_number:\n            line_number = int(line_number)\n            try:\n                self.plugin.go_to_line(line_number)\n            except AttributeError:\n                pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef item_selection_changed(self):\n        row = self.current_row()\n        if self.count() and row >= 0:\n            if '</b></big><br>' in self.list.currentItem().text() and row == 0:\n                self.next_row()\n            if self.mode == self.FILE_MODE:\n                try:\n                    stack_index = self.paths.index(self.filtered_path[row])\n                    self.plugin = self.widgets[stack_index][1]\n                    self.goto_line(self.line_number)\n                    try:\n                        self.plugin.switch_to_plugin()\n                        self.raise_()\n                    except AttributeError:\n                        # The widget using the fileswitcher is not a plugin\n                        pass\n                    self.edit.setFocus()\n                except ValueError:\n                    pass\n            else:\n                line_number = self.filtered_symbol_lines[row]\n                self.goto_line(line_number)", "response": "List widget item selection change handler."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef setup_file_list(self, filter_text, current_path):\n        short_paths = shorten_paths(self.paths, self.save_status)\n        paths = self.paths\n        icons = self.icons\n        results = []\n        trying_for_line_number = ':' in filter_text\n\n        # Get optional line number\n        if trying_for_line_number:\n            filter_text, line_number = filter_text.split(':')\n            if line_number == '':\n                line_number = None\n            # Get all the available filenames\n            scores = get_search_scores('', self.filenames,\n                                       template=\"<b>{0}</b>\")\n        else:\n            line_number = None\n            # Get all available filenames and get the scores for\n            # \"fuzzy\" matching\n            scores = get_search_scores(filter_text, self.filenames,\n                                       template=\"<b>{0}</b>\")\n\n        # Get max width to determine if shortpaths should be used\n        max_width = self.get_item_size(paths)[0]\n        self.fix_size(paths)\n\n        # Build the text that will appear on the list widget\n        rich_font = CONF.get('appearance', 'rich_font/size', 10)\n        if sys.platform == 'darwin':\n            path_text_font_size = rich_font\n            filename_text_font_size = path_text_font_size + 2\n        elif os.name == 'nt':\n            path_text_font_size = rich_font\n            filename_text_font_size = path_text_font_size + 1\n        elif is_ubuntu():\n            path_text_font_size = rich_font - 2\n            filename_text_font_size = path_text_font_size + 1\n        else:\n            path_text_font_size = rich_font\n            filename_text_font_size = path_text_font_size + 1\n\n        for index, score in enumerate(scores):\n            text, rich_text, score_value = score\n            if score_value != -1:\n                text_item = (\"<span style='color:{0:}; font-size:{1:}pt'>{2:}\"\n                             \"</span>\").format(ima.MAIN_FG_COLOR,\n                                               filename_text_font_size,\n                                               rich_text.replace('&', ''))\n                if trying_for_line_number:\n                    text_item += \" [{0:} {1:}]\".format(self.line_count[index],\n                                                       _(\"lines\"))\n                if max_width > self.list.width():\n                    text_item += (u\" &nbsp; <span style='color:{0:};\"\n                                  \"font-size:{1:}pt'>{2:}\"\n                                  \"</span>\").format(self.PATH_FG_COLOR,\n                                                    path_text_font_size,\n                                                    short_paths[index])\n                else:\n                    text_item += (u\" &nbsp; <span style='color:{0:};\"\n                                  \"font-size:{1:}pt'>{2:}\"\n                                  \"</span>\").format(self.PATH_FG_COLOR,\n                                                    path_text_font_size,\n                                                    paths[index])\n                if (trying_for_line_number and self.line_count[index] != 0 or\n                        not trying_for_line_number):\n                    results.append((score_value, index, text_item))\n\n        # Sort the obtained scores and populate the list widget\n        self.filtered_path = []\n        plugin = None\n        for result in sorted(results):\n            index = result[1]\n            path = paths[index]\n            if sys.platform == 'darwin':\n                scale_factor = 0.9\n            elif os.name == 'nt':\n                scale_factor = 0.8\n            elif is_ubuntu():\n                scale_factor = 0.6\n            else:\n                scale_factor = 0.9\n            icon = ima.get_icon_by_extension(path, scale_factor)\n            text = ''\n            try:\n                title = self.widgets[index][1].get_plugin_title().split(' - ')\n                if plugin != title[0]:\n                    plugin = title[0]\n                    text += (\"<br><big style='color:{0:}'>\"\n                             \"<b>{1:}</b></big><br>\").format(ima.MAIN_FG_COLOR,\n                                                             plugin)\n                    item = QListWidgetItem(text)\n                    item.setToolTip(path)\n                    item.setSizeHint(QSize(0, 25))\n                    item.setFlags(Qt.ItemIsEditable)\n                    self.list.addItem(item)\n                    self.filtered_path.append(path)\n            except:\n                # The widget using the fileswitcher is not a plugin\n                pass\n            text = ''\n            text += result[-1]\n            item = QListWidgetItem(icon, text)\n            item.setToolTip(path)\n            item.setSizeHint(QSize(0, 25))\n            self.list.addItem(item)\n            self.filtered_path.append(path)\n\n        # To adjust the delegate layout for KDE themes\n        self.list.files_list = True\n\n        # Move selected item in list accordingly and update list size\n        if current_path in self.filtered_path:\n            self.set_current_row(self.filtered_path.index(current_path))\n        elif self.filtered_path:\n            self.set_current_row(0)\n\n        # If a line number is searched look for it\n        self.line_number = line_number\n        self.goto_line(line_number)", "response": "Setup file list widget content for file list display."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef setup(self):\n        if len(self.plugins_tabs) == 0:\n            self.close()\n            return\n\n        self.list.clear()\n        current_path = self.current_path\n        filter_text = self.filter_text\n\n        # Get optional line or symbol to define mode and method handler\n        trying_for_symbol = ('@' in self.filter_text)\n\n        if trying_for_symbol:\n            self.mode = self.SYMBOL_MODE\n            self.setup_symbol_list(filter_text, current_path)\n        else:\n            self.mode = self.FILE_MODE\n            self.setup_file_list(filter_text, current_path)\n\n        # Set position according to size\n        self.set_dialog_position()", "response": "Setup list widget content."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_plugin(self, plugin, tabs, data, icon):\n        self.plugins_tabs.append((tabs, plugin))\n        self.plugins_data.append((data, icon))\n        self.plugins_instances.append(plugin)", "response": "Add a plugin to display its files."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if the file is a binary file.", "response": "def is_binary(filename):\n    \"\"\"\n    :param filename: File to check.\n    :returns: True if it's a binary file, otherwise False.\n    \"\"\"\n    logger.debug('is_binary: %(filename)r', locals())\n\n    # Check if the file extension is in a list of known binary types\n    binary_extensions = ['pyc', 'iso', 'zip', 'pdf']\n    for ext in binary_extensions:\n        if filename.endswith(ext):\n            return True\n\n    # Check if the starting chunk is a binary string\n    chunk = get_starting_chunk(filename)\n    return is_binary_string(chunk)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning support status dict if path is under VCS root", "response": "def get_vcs_info(path):\r\n    \"\"\"Return support status dict if path is under VCS root\"\"\"\r\n    for info in SUPPORTED:\r\n        vcs_path = osp.join(path, info['rootdir'])\r\n        if osp.isdir(vcs_path):\r\n            return info"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the VCS root directory path", "response": "def get_vcs_root(path):\r\n    \"\"\"Return VCS root directory path\r\n    Return None if path is not within a supported VCS repository\"\"\"\r\n    previous_path = path\r\n    while get_vcs_info(path) is None:\r\n        path = abspardir(path)\r\n        if path == previous_path:\r\n            return\r\n        else:\r\n            previous_path = path\r\n    return osp.abspath(path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef run_vcs_tool(path, action):\r\n    info = get_vcs_info(get_vcs_root(path))\r\n    tools = info['actions'][action]\r\n    for tool, args in tools:\r\n        if programs.find_program(tool):\r\n            if not running_under_pytest():\r\n                programs.run_program(tool, args, cwd=path)\r\n            else:\r\n                return True\r\n            return\r\n    else:\r\n        cmdnames = [name for name, args in tools]\r\n        raise ActionToolNotFound(info['name'], action, cmdnames)", "response": "Run the corresponding VCS tool on the given path"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_hg_revision(repopath):\r\n    try:\r\n        assert osp.isdir(osp.join(repopath, '.hg'))\r\n        proc = programs.run_program('hg', ['id', '-nib', repopath])\r\n        output, _err = proc.communicate()\r\n        # output is now: ('eba7273c69df+ 2015+ default\\n', None)\r\n        # Split 2 times max to allow spaces in branch names.\r\n        return tuple(output.decode().strip().split(None, 2))\r\n    except (subprocess.CalledProcessError, AssertionError, AttributeError,\r\n            OSError):\r\n        return (None, None, None)", "response": "Return Mercurial revision for the repository located at repopath"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_git_revision(repopath):\r\n    try:\r\n        git = programs.find_program('git')\r\n        assert git is not None and osp.isdir(osp.join(repopath, '.git'))\r\n        commit = programs.run_program(git, ['rev-parse', '--short', 'HEAD'],\r\n                                      cwd=repopath).communicate()\r\n        commit = commit[0].strip()\r\n        if PY3:\r\n            commit = commit.decode(sys.getdefaultencoding())\r\n\r\n        # Branch\r\n        branches = programs.run_program(git, ['branch'],\r\n                                        cwd=repopath).communicate()\r\n        branches = branches[0]\r\n        if PY3:\r\n            branches = branches.decode(sys.getdefaultencoding())\r\n        branches = branches.split('\\n')\r\n        active_branch = [b for b in branches if b.startswith('*')]\r\n        if len(active_branch) != 1:\r\n            branch = None\r\n        else:\r\n            branch = active_branch[0].split(None, 1)[1]\r\n\r\n        return commit, branch\r\n    except (subprocess.CalledProcessError, AssertionError, AttributeError,\r\n            OSError):\r\n        return None, None", "response": "Returns the latest commit hash and branch for the repository located at repopath"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a list of tags branches and files modified by the git refs.", "response": "def get_git_refs(repopath):\r\n    \"\"\"\r\n    Return Git active branch, state, branches (plus tags).\r\n    \"\"\"\r\n    tags = []\r\n    branches = []\r\n    branch = ''\r\n    files_modifed = []\r\n\r\n    if os.path.isfile(repopath):\r\n        repopath = os.path.dirname(repopath)\r\n\r\n    try:\r\n\r\n        git = programs.find_program('git')\r\n\r\n        # Files modified\r\n        out, err = programs.run_program(\r\n            git, ['status', '-s'],\r\n            cwd=repopath,\r\n        ).communicate()\r\n\r\n        if PY3:\r\n            out = out.decode(sys.getdefaultencoding())\r\n        files_modifed = [line.strip() for line in out.split('\\n') if line]\r\n\r\n        # Tags\r\n        out, err = programs.run_program(\r\n            git, ['tag'],\r\n            cwd=repopath,\r\n        ).communicate()\r\n\r\n        if PY3:\r\n            out = out.decode(sys.getdefaultencoding())\r\n        tags = [line.strip() for line in out.split('\\n') if line]\r\n\r\n        # Branches\r\n        out, err = programs.run_program(\r\n            git, ['branch', '-a'],\r\n            cwd=repopath,\r\n        ).communicate()\r\n\r\n        if PY3:\r\n            out = out.decode(sys.getdefaultencoding())\r\n\r\n        lines = [line.strip() for line in out.split('\\n') if line]\r\n        for line in lines:\r\n            if line.startswith('*'):\r\n                line = line.replace('*', '').strip()\r\n                branch = line\r\n\r\n            branches.append(line)\r\n\r\n    except (subprocess.CalledProcessError, AttributeError, OSError):\r\n        pass\r\n\r\n    return branches + tags, branch, files_modifed"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_module_or_package(path):\r\n    is_module = osp.isfile(path) and osp.splitext(path)[1] in ('.py', '.pyw')\r\n    is_package = osp.isdir(path) and osp.isfile(osp.join(path, '__init__.py'))\r\n    return is_module or is_package", "response": "Return True if path is a Python module or package"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfilter tab keys and process double tab keys.", "response": "def event(self, event):\r\n        \"\"\"Qt Override.\r\n\r\n        Filter tab keys and process double tab keys.\r\n        \"\"\"\r\n        if (event.type() == QEvent.KeyPress) and (event.key() == Qt.Key_Tab):\r\n            self.sig_tab_pressed.emit(True)\r\n            self.numpress += 1\r\n            if self.numpress == 1:\r\n                self.presstimer = QTimer.singleShot(400, self.handle_keypress)\r\n            return True\r\n        return QComboBox.event(self, event)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef keyPressEvent(self, event):\r\n        if event.key() == Qt.Key_Return or event.key() == Qt.Key_Enter:\r\n            if self.add_current_text_if_valid():\r\n                self.selected()\r\n                self.hide_completer()\r\n        elif event.key() == Qt.Key_Escape:\r\n            self.set_current_text(self.selected_text)\r\n            self.hide_completer()\r\n        else:\r\n            QComboBox.keyPressEvent(self, event)", "response": "Handle key press events."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_text(self, text):\r\n        index = self.findText(text)\r\n        while index != -1:\r\n            self.removeItem(index)\r\n            index = self.findText(text)\r\n        self.insertItem(0, text)\r\n        index = self.findText('')\r\n        if index != -1:\r\n            self.removeItem(index)\r\n            self.insertItem(0, '')\r\n            if text != '':\r\n                self.setCurrentIndex(1)\r\n            else:\r\n                self.setCurrentIndex(0)\r\n        else:\r\n            self.setCurrentIndex(0)", "response": "Add text to combo box"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds current text to combo box history if valid", "response": "def add_current_text_if_valid(self):\r\n        \"\"\"Add current text to combo box history if valid\"\"\"\r\n        valid = self.is_valid(self.currentText())\r\n        if valid or valid is None:\r\n            self.add_current_text()\r\n            return True\r\n        else:\r\n            self.set_current_text(self.selected_text)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef show_tip(self, tip=\"\"):\r\n        QToolTip.showText(self.mapToGlobal(self.pos()), tip, self)", "response": "Show the tip for this item."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nhandles focus in event restoring to display the status icon.", "response": "def focusInEvent(self, event):\r\n        \"\"\"Handle focus in event restoring to display the status icon.\"\"\"\r\n        show_status = getattr(self.lineEdit(), 'show_status_icon', None)\r\n        if show_status:\r\n            show_status()\r\n        QComboBox.focusInEvent(self, event)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef focusOutEvent(self, event):\r\n        # Calling asynchronously the 'add_current_text' to avoid crash\r\n        # https://groups.google.com/group/spyderlib/browse_thread/thread/2257abf530e210bd\r\n        if not self.is_valid():\r\n            lineedit = self.lineEdit()\r\n            QTimer.singleShot(50, lambda: lineedit.setText(self.selected_text))\r\n\r\n        hide_status = getattr(self.lineEdit(), 'hide_status_icon', None)\r\n        if hide_status:\r\n            hide_status()\r\n        QComboBox.focusOutEvent(self, event)", "response": "Handle focus out event restoring the last valid selected path."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfind available completion options.", "response": "def _complete_options(self):\r\n        \"\"\"Find available completion options.\"\"\"\r\n        text = to_text_string(self.currentText())\r\n        opts = glob.glob(text + \"*\")\r\n        opts = sorted([opt for opt in opts if osp.isdir(opt)])\r\n        self.setCompleter(QCompleter(opts, self))\r\n        return opts"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef double_tab_complete(self):\r\n        opts = self._complete_options()\r\n        if len(opts) > 1:\r\n            self.completer().complete()", "response": "If several options available a double tab displays options."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef tab_complete(self):\r\n        opts = self._complete_options()\r\n        if len(opts) == 1:\r\n            self.set_current_text(opts[0] + os.sep)\r\n            self.hide_completer()", "response": "Tab complete the current option."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_valid(self, qstr=None):\r\n        if qstr is None:\r\n            qstr = self.currentText()\r\n        return osp.isdir(to_text_string(qstr))", "response": "Return True if string is valid"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef selected(self):\r\n        self.selected_text = self.currentText()\r\n        self.valid.emit(True, True)\r\n        self.open_dir.emit(self.selected_text)", "response": "Action to be executed when a valid item has been selected"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_current_text(self):\r\n        text = self.currentText()\r\n        if osp.isdir(text) and text:\r\n            if text[-1] == os.sep:\r\n                text = text[:-1]\r\n        self.add_text(text)", "response": "Add current text to combo box history."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a tooltip to the currently highlighted item", "response": "def add_tooltip_to_highlighted_item(self, index):\r\n        \"\"\"\r\n        Add a tooltip showing the full path of the currently highlighted item\r\n        of the PathComboBox.\r\n        \"\"\"\r\n        self.setItemData(index, self.itemText(index), Qt.ToolTipRole)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_valid(self, qstr=None):\r\n        if qstr is None:\r\n            qstr = self.currentText()\r\n        return QUrl(qstr).isValid()", "response": "Return True if string is valid"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_valid(self, qstr=None):\r\n        if qstr is None:\r\n            qstr = self.currentText()\r\n        return osp.isfile(to_text_string(qstr))", "response": "Return True if string is valid"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn True if string is valid", "response": "def is_valid(self, qstr=None):\r\n        \"\"\"Return True if string is valid\"\"\"\r\n        if qstr is None:\r\n            qstr = self.currentText()\r\n        return is_module_or_package(to_text_string(qstr))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nactioning to be executed when a valid item has been selected", "response": "def selected(self):\r\n        \"\"\"Action to be executed when a valid item has been selected\"\"\"\r\n        EditableComboBox.selected(self)\r\n        self.open_dir.emit(self.currentText())"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nbinds historylog instance to this console", "response": "def set_historylog(self, historylog):\r\n        \"\"\"Bind historylog instance to this console\r\n        Not used anymore since v2.0\"\"\"\r\n        historylog.add_history(self.shell.history_filename)\r\n        self.shell.append_to_history.connect(historylog.append_to_history)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef closing_plugin(self, cancelable=False):\r\n        self.dialog_manager.close_all()\r\n        self.shell.exit_interpreter()\r\n        return True", "response": "Perform actions before parent main window is closed"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list of actions related to the plugin", "response": "def get_plugin_actions(self):\r\n        \"\"\"Return a list of actions related to plugin\"\"\"\r\n        quit_action = create_action(self, _(\"&Quit\"),\r\n                                    icon=ima.icon('exit'), \r\n                                    tip=_(\"Quit\"),\r\n                                    triggered=self.quit)\r\n        self.register_shortcut(quit_action, \"_\", \"Quit\", \"Ctrl+Q\")\r\n        run_action = create_action(self, _(\"&Run...\"), None,\r\n                            ima.icon('run_small'),\r\n                            _(\"Run a Python script\"),\r\n                            triggered=self.run_script)\r\n        environ_action = create_action(self,\r\n                            _(\"Environment variables...\"),\r\n                            icon=ima.icon('environ'),\r\n                            tip=_(\"Show and edit environment variables\"\r\n                                        \" (for current session)\"),\r\n                            triggered=self.show_env)\r\n        syspath_action = create_action(self,\r\n                            _(\"Show sys.path contents...\"),\r\n                            icon=ima.icon('syspath'),\r\n                            tip=_(\"Show (read-only) sys.path\"),\r\n                            triggered=self.show_syspath)\r\n        buffer_action = create_action(self,\r\n                            _(\"Buffer...\"), None,\r\n                            tip=_(\"Set maximum line count\"),\r\n                            triggered=self.change_max_line_count)\r\n        exteditor_action = create_action(self,\r\n                            _(\"External editor path...\"), None, None,\r\n                            _(\"Set external editor executable path\"),\r\n                            triggered=self.change_exteditor)\r\n        wrap_action = create_action(self,\r\n                            _(\"Wrap lines\"),\r\n                            toggled=self.toggle_wrap_mode)\r\n        wrap_action.setChecked(self.get_option('wrap'))\r\n        codecompletion_action = create_action(self,\r\n                                          _(\"Automatic code completion\"),\r\n                                          toggled=self.toggle_codecompletion)\r\n        codecompletion_action.setChecked(self.get_option('codecompletion/auto'))\r\n        \r\n        option_menu = QMenu(_('Internal console settings'), self)\r\n        option_menu.setIcon(ima.icon('tooloptions'))\r\n        add_actions(option_menu, (buffer_action, wrap_action,\r\n                                  codecompletion_action,\r\n                                  exteditor_action))\r\n                    \r\n        plugin_actions = [None, run_action, environ_action, syspath_action,\r\n                          option_menu, MENU_SEPARATOR, quit_action,\r\n                          self.undock_action]\r\n\r\n        return plugin_actions"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nregister plugin in Spyder s main window", "response": "def register_plugin(self):\r\n        \"\"\"Register plugin in Spyder's main window\"\"\"\r\n        self.focus_changed.connect(self.main.plugin_focus_changed)\r\n        self.main.add_dockwidget(self)\r\n        # Connecting the following signal once the dockwidget has been created:\r\n        self.shell.exception_occurred.connect(self.exception_occurred)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef exception_occurred(self, text, is_traceback):\r\n        # Skip errors without traceback or dismiss\r\n        if (not is_traceback and self.error_dlg is None) or self.dismiss_error:\r\n            return\r\n\r\n        if CONF.get('main', 'show_internal_errors'):\r\n            if self.error_dlg is None:\r\n                self.error_dlg = SpyderErrorDialog(self)\r\n                self.error_dlg.close_btn.clicked.connect(self.close_error_dlg)\r\n                self.error_dlg.rejected.connect(self.remove_error_dlg)\r\n                self.error_dlg.details.go_to_error.connect(self.go_to_error)\r\n                self.error_dlg.show()\r\n            self.error_dlg.append_traceback(text)\r\n        elif DEV or get_debug_level():\r\n            self.dockwidget.show()\r\n            self.dockwidget.raise_()", "response": "An exception occurred in the internal console."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nruns a Python script", "response": "def run_script(self, filename=None, silent=False, set_focus=False,\r\n                   args=None):\r\n        \"\"\"Run a Python script\"\"\"\r\n        if filename is None:\r\n            self.shell.interpreter.restore_stds()\r\n            filename, _selfilter = getopenfilename(\r\n                    self, _(\"Run Python script\"), getcwd_or_home(),\r\n                    _(\"Python scripts\")+\" (*.py ; *.pyw ; *.ipy)\")\r\n            self.shell.interpreter.redirect_stds()\r\n            if filename:\r\n                os.chdir( osp.dirname(filename) )\r\n                filename = osp.basename(filename)\r\n            else:\r\n                return\r\n        logger.debug(\"Running script with %s\", args)\r\n        filename = osp.abspath(filename)\r\n        rbs = remove_backslashes\r\n        command = \"runfile('%s', args='%s')\" % (rbs(filename), rbs(args))\r\n        if set_focus:\r\n            self.shell.setFocus()\r\n        if self.dockwidget and not self.ismaximized:\r\n            self.dockwidget.setVisible(True)\r\n            self.dockwidget.raise_()\r\n        self.shell.write(command+'\\n')\r\n        self.shell.run_command(command)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef go_to_error(self, text):\r\n        match = get_error_match(to_text_string(text))\r\n        if match:\r\n            fname, lnb = match.groups()\r\n            self.edit_script(fname, int(lnb))", "response": "Go to error if relevant"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef execute_lines(self, lines):\r\n        self.shell.execute_lines(to_text_string(lines))\r\n        self.shell.setFocus()", "response": "Execute lines and give focus to shell"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchanges external editor path", "response": "def change_exteditor(self):\r\n        \"\"\"Change external editor path\"\"\"\r\n        path, valid = QInputDialog.getText(self, _('External editor'),\r\n                          _('External editor executable path:'),\r\n                          QLineEdit.Normal,\r\n                          self.get_option('external_editor/path'))\r\n        if valid:\r\n            self.set_option('external_editor/path', to_text_string(path))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef toggle_codecompletion(self, checked):\r\n        self.shell.set_codecompletion_auto(checked)\r\n        self.set_option('codecompletion/auto', checked)", "response": "Toggle automatic code completion"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef dragEnterEvent(self, event):\r\n        source = event.mimeData()\r\n        if source.hasUrls():\r\n            if mimedata2url(source):\r\n                event.acceptProposedAction()\r\n            else:\r\n                event.ignore()\r\n        elif source.hasText():\r\n            event.acceptProposedAction()", "response": "Reimplement Qt method\r\n            Inform Qt about the types of data that the widget accepts"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef install():\r\n    # Create Spyder start menu folder\r\n    # Don't use CSIDL_COMMON_PROGRAMS because it requres admin rights\r\n    # This is consistent with use of CSIDL_DESKTOPDIRECTORY below\r\n    # CSIDL_COMMON_PROGRAMS =\r\n    # C:\\ProgramData\\Microsoft\\Windows\\Start Menu\\Programs\r\n    # CSIDL_PROGRAMS =\r\n    # C:\\Users\\<username>\\AppData\\Roaming\\Microsoft\\Windows\\Start Menu\\Programs\r\n    start_menu = osp.join(get_special_folder_path('CSIDL_PROGRAMS'),\r\n                          'Spyder (Py%i.%i %i bit)' % (sys.version_info[0],\r\n                                                       sys.version_info[1],\r\n                                                       struct.calcsize('P')*8))\r\n    if not osp.isdir(start_menu):\r\n        os.mkdir(start_menu)\r\n        directory_created(start_menu)\r\n\r\n    # Create Spyder start menu entries\r\n    python = osp.abspath(osp.join(sys.prefix, 'python.exe'))\r\n    pythonw = osp.abspath(osp.join(sys.prefix, 'pythonw.exe'))\r\n    script = osp.abspath(osp.join(sys.prefix, 'scripts', 'spyder'))\r\n    if not osp.exists(script): # if not installed to the site scripts dir\r\n        script = osp.abspath(osp.join(osp.dirname(osp.abspath(__file__)), 'spyder'))\r\n    workdir = \"%HOMEDRIVE%%HOMEPATH%\"\r\n    import distutils.sysconfig\r\n    lib_dir = distutils.sysconfig.get_python_lib(plat_specific=1)\r\n    ico_dir = osp.join(lib_dir, 'spyder', 'windows')\r\n    # if user is running -install manually then icons are in Scripts/\r\n    if not osp.isdir(ico_dir):\r\n        ico_dir = osp.dirname(osp.abspath(__file__))\r\n\r\n    desc = 'The Scientific Python Development Environment'\r\n    fname = osp.join(start_menu, 'Spyder (full).lnk')\r\n    create_shortcut(python, desc, fname, '\"%s\"' % script, workdir,\r\n                    osp.join(ico_dir, 'spyder.ico'))\r\n    file_created(fname)\r\n\r\n    fname = osp.join(start_menu, 'Spyder-Reset all settings.lnk')\r\n    create_shortcut(python, 'Reset Spyder settings to defaults',\r\n                    fname, '\"%s\" --reset' % script, workdir)\r\n    file_created(fname)\r\n\r\n    current = True  # only affects current user\r\n    root = winreg.HKEY_CURRENT_USER if current else winreg.HKEY_LOCAL_MACHINE\r\n    winreg.SetValueEx(winreg.CreateKey(root, KEY_C1 % (\"\", EWS)),\r\n                      \"\", 0, winreg.REG_SZ,\r\n                      '\"%s\" \"%s\\Scripts\\spyder\" \"%%1\"' % (pythonw, sys.prefix))\r\n    winreg.SetValueEx(winreg.CreateKey(root, KEY_C1 % (\"NoCon\", EWS)),\r\n                      \"\", 0, winreg.REG_SZ,\r\n                      '\"%s\" \"%s\\Scripts\\spyder\" \"%%1\"' % (pythonw, sys.prefix))\r\n\r\n    # Create desktop shortcut file\r\n    desktop_folder = get_special_folder_path(\"CSIDL_DESKTOPDIRECTORY\")\r\n    fname = osp.join(desktop_folder, 'Spyder.lnk')\r\n    desc = 'The Scientific Python Development Environment'\r\n    create_shortcut(pythonw, desc, fname, '\"%s\"' % script, workdir,\r\n                    osp.join(ico_dir, 'spyder.ico'))\r\n    file_created(fname)", "response": "Function executed when running the script with the - install switch"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef remove():\r\n    current = True  # only affects current user\r\n    root = winreg.HKEY_CURRENT_USER if current else winreg.HKEY_LOCAL_MACHINE\r\n    for key in (KEY_C1 % (\"\", EWS), KEY_C1 % (\"NoCon\", EWS),\r\n                KEY_C0 % (\"\", EWS), KEY_C0 % (\"NoCon\", EWS)):\r\n        try:\r\n            winreg.DeleteKey(root, key)\r\n        except WindowsError:\r\n            pass\r\n        else:\r\n            if not is_bdist_wininst:\r\n                print(\"Successfully removed Spyder shortcuts from Windows \"\\\r\n                      \"Explorer context menu.\", file=sys.stdout)\r\n    if not is_bdist_wininst:\r\n        # clean up desktop\r\n        desktop_folder = get_special_folder_path(\"CSIDL_DESKTOPDIRECTORY\")\r\n        fname = osp.join(desktop_folder, 'Spyder.lnk')\r\n        if osp.isfile(fname):\r\n            try:\r\n                os.remove(fname)\r\n            except OSError:\r\n                print(\"Failed to remove %s; you may be able to remove it \"\\\r\n                      \"manually.\" % fname, file=sys.stderr)\r\n            else:\r\n                print(\"Successfully removed Spyder shortcuts from your desktop.\",\r\n                      file=sys.stdout)\r\n        # clean up startmenu\r\n        start_menu = osp.join(get_special_folder_path('CSIDL_PROGRAMS'),\r\n                              'Spyder (Py%i.%i %i bit)' % (sys.version_info[0],\r\n                                                           sys.version_info[1],\r\n                                                           struct.calcsize('P')*8))\r\n        if osp.isdir(start_menu):\r\n            for fname in os.listdir(start_menu):\r\n                try:\r\n                    os.remove(osp.join(start_menu,fname))\r\n                except OSError:\r\n                    print(\"Failed to remove %s; you may be able to remove it \"\\\r\n                          \"manually.\" % fname, file=sys.stderr)\r\n                else:\r\n                    print(\"Successfully removed Spyder shortcuts from your \"\\\r\n                          \" start menu.\", file=sys.stdout)\r\n            try:\r\n                os.rmdir(start_menu)\r\n            except OSError:\r\n                print(\"Failed to remove %s; you may be able to remove it \"\\\r\n                      \"manually.\" % fname, file=sys.stderr)\r\n            else:\r\n                print(\"Successfully removed Spyder shortcut folder from your \"\\\r\n                      \" start menu.\", file=sys.stdout)", "response": "Function executed when running the script with the - remove switch"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef paintEvent(self, event):\r\n        painter = QPainter(self)\r\n        painter.setRenderHint(QPainter.Antialiasing)\r\n        # Decoration\r\n        painter.fillPath(self.path_current, QBrush(self.color))\r\n        painter.strokePath(self.path_decoration, QPen(self.color_decoration,\r\n                                                      self.stroke_decoration))", "response": "Override Qt method to paint event"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\noverrides Qt method to reject the user input.", "response": "def reject(self):\r\n        \"\"\"Override Qt method\"\"\"\r\n        if not self.is_fade_running():\r\n            key = Qt.Key_Escape\r\n            self.key_pressed = key\r\n            self.sig_key_pressed.emit()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _set_data(self):\r\n        self.setting_data = True\r\n        step, steps, frames = self.step_current, self.steps, self.frames\r\n        current = '{0}/{1}'.format(step + 1, steps)\r\n        frame = frames[step]\r\n\r\n        combobox_frames = [u\"{0}. {1}\".format(i+1, f['title'])\r\n                           for i, f in enumerate(frames)]\r\n\r\n        title, content, image = '', '', None\r\n        widgets, dockwidgets, decoration = None, None, None\r\n        run = None\r\n\r\n        # Check if entry exists in dic and act accordingly\r\n        if 'title' in frame:\r\n            title = frame['title']\r\n\r\n        if 'content' in frame:\r\n            content = frame['content']\r\n\r\n        if 'widgets' in frame:\r\n            widget_names = frames[step]['widgets']\r\n            # Get the widgets based on their name\r\n            widgets, dockwidgets = self._process_widgets(widget_names,\r\n                                                         self.spy_window)\r\n        self.widgets = widgets\r\n        self.dockwidgets = dockwidgets\r\n\r\n        if 'decoration' in frame:\r\n            widget_names = frames[step]['decoration']\r\n            deco, decoration = self._process_widgets(widget_names,\r\n                                                     self.spy_window)\r\n            self.decoration = decoration\r\n\r\n        if 'image' in frame:\r\n            image = frames[step]['image']\r\n\r\n        if 'interact' in frame:\r\n            self.canvas.set_interaction(frame['interact'])\r\n            if frame['interact']:\r\n                self._set_modal(False, [self.tips])\r\n            else:\r\n                self._set_modal(True, [self.tips])\r\n        else:\r\n            self.canvas.set_interaction(False)\r\n            self._set_modal(True, [self.tips])\r\n\r\n        if 'run' in frame:\r\n            # Asume that the frist widget is the console\r\n            run = frame['run']\r\n            self.run = run\r\n\r\n        self.tips.set_data(title, content, current, image, run,\r\n                           frames=combobox_frames, step=step)\r\n        self._check_buttons()\r\n\r\n        # Make canvas black when starting a new place of decoration\r\n        self.canvas.update_widgets(dockwidgets)\r\n        self.canvas.update_decoration(decoration)\r\n        self.setting_data = False", "response": "Set data that is displayed in each step of the tour."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef lost_focus(self):\r\n        if (self.is_running and not self.any_has_focus() and\r\n            not self.setting_data and not self.hidden):\r\n            self.hide_tips()", "response": "Confirm if the tour loses focus and hides the tips."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef gain_focus(self):\r\n        if (self.is_running and self.any_has_focus() and \r\n            not self.setting_data and self.hidden):\r\n            self.unhide_tips()", "response": "Confirm if the tour regains focus and unhides the tips."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning if any of the components has focus.", "response": "def any_has_focus(self):\r\n        \"\"\"Returns if tour or any of its components has focus.\"\"\"\r\n        f = (self.hasFocus() or self.parent.hasFocus() or \r\n             self.tips.hasFocus() or self.canvas.hasFocus())\r\n        return f"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets file language from filename", "response": "def get_file_language(filename, text=None):\r\n    \"\"\"Get file language from filename\"\"\"\r\n    ext = osp.splitext(filename)[1]\r\n    if ext.startswith('.'):\r\n        ext = ext[1:] # file extension with leading dot\r\n    language = ext\r\n    if not ext:\r\n        if text is None:\r\n            text, _enc = encoding.read(filename)\r\n        for line in text.splitlines():\r\n            if not line.strip():\r\n                continue\r\n            if line.startswith('#!'):\r\n               shebang = line[2:]\r\n               if 'python' in shebang:\r\n                   language = 'python'\r\n            else:\r\n                break\r\n    return language"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nlining edit s text has changed", "response": "def text_has_changed(self, text):\r\n        \"\"\"Line edit's text has changed\"\"\"\r\n        text = to_text_string(text)\r\n        if text:\r\n            self.lineno = int(text)\r\n        else:\r\n            self.lineno = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsaves fig to fname in the format specified by fmt.", "response": "def save_figure_tofile(fig, fmt, fname):\n    \"\"\"Save fig to fname in the format specified by fmt.\"\"\"\n    root, ext = osp.splitext(fname)\n    if ext == '.png' and fmt == 'image/svg+xml':\n        qimg = svg_to_image(fig)\n        qimg.save(fname)\n    else:\n        if fmt == 'image/svg+xml' and is_unicode(fig):\n            fig = fig.encode('utf-8')\n\n        with open(fname, 'wb') as f:\n            f.write(fig)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_unique_figname(dirname, root, ext):\n    i = 1\n    figname = root + '_%d' % i + ext\n    while True:\n        if osp.exists(osp.join(dirname, figname)):\n            i += 1\n            figname = root + '_%d' % i + ext\n        else:\n            return osp.join(dirname, figname)", "response": "Get a unique filename for a given directory root and extension."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef setup(self, mute_inline_plotting=None, show_plot_outline=None):\n        assert self.shellwidget is not None\n\n        self.mute_inline_plotting = mute_inline_plotting\n        self.show_plot_outline = show_plot_outline\n\n        if self.figviewer is not None:\n            self.mute_inline_action.setChecked(mute_inline_plotting)\n            self.show_plot_outline_action.setChecked(show_plot_outline)\n            return\n\n        self.figviewer = FigureViewer(background_color=self.background_color)\n        self.figviewer.setStyleSheet(\"FigureViewer{\"\n                                     \"border: 1px solid lightgrey;\"\n                                     \"border-top-width: 0px;\"\n                                     \"border-bottom-width: 0px;\"\n                                     \"border-left-width: 0px;\"\n                                     \"}\")\n        self.thumbnails_sb = ThumbnailScrollBar(\n            self.figviewer, background_color=self.background_color)\n\n        # Option actions :\n        self.setup_option_actions(mute_inline_plotting, show_plot_outline)\n\n        # Create the layout :\n        main_widget = QSplitter()\n        main_widget.addWidget(self.figviewer)\n        main_widget.addWidget(self.thumbnails_sb)\n        main_widget.setFrameStyle(QScrollArea().frameStyle())\n\n        self.tools_layout = QHBoxLayout()\n        toolbar = self.setup_toolbar()\n        for widget in toolbar:\n            self.tools_layout.addWidget(widget)\n        self.tools_layout.addStretch()\n        self.setup_options_button()\n\n        layout = create_plugin_layout(self.tools_layout, main_widget)\n        self.setLayout(layout)", "response": "Setup the figure browser with provided settings."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef setup_option_actions(self, mute_inline_plotting, show_plot_outline):\n        self.setup_in_progress = True\n        self.mute_inline_action = create_action(\n            self, _(\"Mute inline plotting\"),\n            tip=_(\"Mute inline plotting in the ipython console.\"),\n            toggled=lambda state:\n            self.option_changed('mute_inline_plotting', state)\n            )\n        self.mute_inline_action.setChecked(mute_inline_plotting)\n\n        self.show_plot_outline_action = create_action(\n            self, _(\"Show plot outline\"),\n            tip=_(\"Show the plot outline.\"),\n            toggled=self.show_fig_outline_in_viewer\n            )\n        self.show_plot_outline_action.setChecked(show_plot_outline)\n\n        self.actions = [self.mute_inline_action, self.show_plot_outline_action]\n        self.setup_in_progress = False", "response": "Setup the actions to show in the cog menu."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating shortcuts for this widget.", "response": "def create_shortcuts(self):\n        \"\"\"Create shortcuts for this widget.\"\"\"\n        # Configurable\n        copyfig = config_shortcut(self.copy_figure, context='plots',\n                                  name='copy', parent=self)\n        prevfig = config_shortcut(self.go_previous_thumbnail, context='plots',\n                                  name='previous figure', parent=self)\n        nextfig = config_shortcut(self.go_next_thumbnail, context='plots',\n                                  name='next figure', parent=self)\n\n        return [copyfig, prevfig, nextfig]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef option_changed(self, option, value):\n        setattr(self, to_text_string(option), value)\n        self.shellwidget.set_namespace_view_settings()\n        if self.setup_in_progress is False:\n            self.sig_option_changed.emit(option, value)", "response": "Handle when the value of an option has changed"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndraws a frame around the figure viewer if state is True.", "response": "def show_fig_outline_in_viewer(self, state):\n        \"\"\"Draw a frame around the figure viewer if state is True.\"\"\"\n        if state is True:\n            self.figviewer.figcanvas.setStyleSheet(\n                    \"FigureCanvas{border: 1px solid lightgrey;}\")\n        else:\n            self.figviewer.figcanvas.setStyleSheet(\"FigureCanvas{}\")\n        self.option_changed('show_plot_outline', state)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_shellwidget(self, shellwidget):\n        self.shellwidget = shellwidget\n        shellwidget.set_figurebrowser(self)\n        shellwidget.sig_new_inline_figure.connect(self._handle_new_figure)", "response": "Bind the shellwidget instance to the figure browser"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef copy_figure(self):\n        if self.figviewer and self.figviewer.figcanvas.fig:\n            self.figviewer.figcanvas.copy_figure()", "response": "Copy figure from figviewer to clipboard."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets a new figure in the figure canvas.", "response": "def load_figure(self, fig, fmt):\n        \"\"\"Set a new figure in the figure canvas.\"\"\"\n        self.figcanvas.load_figure(fig, fmt)\n        self.scale_image()\n        self.figcanvas.repaint()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef eventFilter(self, widget, event):\n\n        # ---- Zooming\n        if event.type() == QEvent.Wheel:\n            modifiers = QApplication.keyboardModifiers()\n            if modifiers == Qt.ControlModifier:\n                if event.angleDelta().y() > 0:\n                    self.zoom_in()\n                else:\n                    self.zoom_out()\n                return True\n            else:\n                return False\n\n        # ---- Panning\n        # Set ClosedHandCursor:\n        elif event.type() == QEvent.MouseButtonPress:\n            if event.button() == Qt.LeftButton:\n                QApplication.setOverrideCursor(Qt.ClosedHandCursor)\n                self._ispanning = True\n                self.xclick = event.globalX()\n                self.yclick = event.globalY()\n\n        # Reset Cursor:\n        elif event.type() == QEvent.MouseButtonRelease:\n            QApplication.restoreOverrideCursor()\n            self._ispanning = False\n\n        # Move  ScrollBar:\n        elif event.type() == QEvent.MouseMove:\n            if self._ispanning:\n                dx = self.xclick - event.globalX()\n                self.xclick = event.globalX()\n\n                dy = self.yclick - event.globalY()\n                self.yclick = event.globalY()\n\n                scrollBarH = self.horizontalScrollBar()\n                scrollBarH.setValue(scrollBarH.value() + dx)\n\n                scrollBarV = self.verticalScrollBar()\n                scrollBarV.setValue(scrollBarV.value() + dy)\n\n        return QWidget.eventFilter(self, widget, event)", "response": "A filter to control the zooming and panning of the figure canvas."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef zoom_in(self):\n        if self._scalefactor <= self._sfmax:\n            self._scalefactor += 1\n            self.scale_image()\n            self._adjust_scrollbar(self._scalestep)\n            self.sig_zoom_changed.emit(self.get_scaling())", "response": "Scale the image up by one scale step."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef zoom_out(self):\n        if self._scalefactor >= self._sfmin:\n            self._scalefactor -= 1\n            self.scale_image()\n            self._adjust_scrollbar(1/self._scalestep)\n            self.sig_zoom_changed.emit(self.get_scaling())", "response": "Scale the image down by one scale step."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef scale_image(self):\n        new_width = int(self.figcanvas.fwidth *\n                        self._scalestep ** self._scalefactor)\n        new_height = int(self.figcanvas.fheight *\n                         self._scalestep ** self._scalefactor)\n        self.figcanvas.setFixedSize(new_width, new_height)", "response": "Scale the image size."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _adjust_scrollbar(self, f):\n        # Adjust horizontal scrollbar :\n        hb = self.horizontalScrollBar()\n        hb.setValue(int(f * hb.value() + ((f - 1) * hb.pageStep()/2)))\n\n        # Adjust the vertical scrollbar :\n        vb = self.verticalScrollBar()\n        vb.setValue(int(f * vb.value() + ((f - 1) * vb.pageStep()/2)))", "response": "Adjust the horizontal and vertical scrollbar to take into account the zooming of the figure."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef setup_arrow_buttons(self):\n        # Get the height of the up/down arrow of the default vertical\n        # scrollbar :\n        vsb = self.scrollarea.verticalScrollBar()\n        style = vsb.style()\n        opt = QStyleOptionSlider()\n        vsb.initStyleOption(opt)\n        vsb_up_arrow = style.subControlRect(\n                QStyle.CC_ScrollBar, opt, QStyle.SC_ScrollBarAddLine, self)\n\n        # Setup the up and down arrow button :\n        up_btn = up_btn = QPushButton(icon=ima.icon('last_edit_location'))\n        up_btn.setFlat(True)\n        up_btn.setFixedHeight(vsb_up_arrow.size().height())\n        up_btn.clicked.connect(self.go_up)\n\n        down_btn = QPushButton(icon=ima.icon('folding.arrow_down_on'))\n        down_btn.setFlat(True)\n        down_btn.setFixedHeight(vsb_up_arrow.size().height())\n        down_btn.clicked.connect(self.go_down)\n\n        return up_btn, down_btn", "response": "Setup the up and down arrow buttons that are placed at the top and the bottom of the scrollarea."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef save_all_figures_as(self):\n        self.redirect_stdio.emit(False)\n        dirname = getexistingdirectory(self, caption='Save all figures',\n                                       basedir=getcwd_or_home())\n        self.redirect_stdio.emit(True)\n        if dirname:\n            return self.save_all_figures_todir(dirname)", "response": "Save all the figures to a file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef save_all_figures_todir(self, dirname):\n        fignames = []\n        for thumbnail in self._thumbnails:\n            fig = thumbnail.canvas.fig\n            fmt = thumbnail.canvas.fmt\n            fext = {'image/png': '.png',\n                    'image/jpeg': '.jpg',\n                    'image/svg+xml': '.svg'}[fmt]\n\n            figname = get_unique_figname(dirname, 'Figure', fext)\n            save_figure_tofile(fig, fmt, figname)\n            fignames.append(figname)\n        return fignames", "response": "Save all figures in dirname."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsave the currently selected figure as.", "response": "def save_current_figure_as(self):\n        \"\"\"Save the currently selected figure.\"\"\"\n        if self.current_thumbnail is not None:\n            self.save_figure_as(self.current_thumbnail.canvas.fig,\n                                self.current_thumbnail.canvas.fmt)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsaving the figure to a file.", "response": "def save_figure_as(self, fig, fmt):\n        \"\"\"Save the figure to a file.\"\"\"\n        fext, ffilt = {\n            'image/png': ('.png', 'PNG (*.png)'),\n            'image/jpeg': ('.jpg', 'JPEG (*.jpg;*.jpeg;*.jpe;*.jfif)'),\n            'image/svg+xml': ('.svg', 'SVG (*.svg);;PNG (*.png)')}[fmt]\n\n        figname = get_unique_figname(getcwd_or_home(), 'Figure', fext)\n\n        self.redirect_stdio.emit(False)\n        fname, fext = getsavefilename(\n            parent=self.parent(), caption='Save Figure',\n            basedir=figname, filters=ffilt,\n            selectedfilter='', options=None)\n        self.redirect_stdio.emit(True)\n\n        if fname:\n            save_figure_tofile(fig, fmt, fname)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the currently selected thumbnail.", "response": "def set_current_thumbnail(self, thumbnail):\n        \"\"\"Set the currently selected thumbnail.\"\"\"\n        self.current_thumbnail = thumbnail\n        self.figure_viewer.load_figure(\n                thumbnail.canvas.fig, thumbnail.canvas.fmt)\n        for thumbnail in self._thumbnails:\n            thumbnail.highlight_canvas(thumbnail == self.current_thumbnail)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef go_previous_thumbnail(self):\n        if self.current_thumbnail is not None:\n            index = self._thumbnails.index(self.current_thumbnail) - 1\n            index = index if index >= 0 else len(self._thumbnails) - 1\n            self.set_current_index(index)\n            self.scroll_to_item(index)", "response": "Select the thumbnail previous to the currently selected one."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nselecting thumbnail next to the currently selected one.", "response": "def go_next_thumbnail(self):\n        \"\"\"Select thumbnail next to the currently selected one.\"\"\"\n        if self.current_thumbnail is not None:\n            index = self._thumbnails.index(self.current_thumbnail) + 1\n            index = 0 if index >= len(self._thumbnails) else index\n            self.set_current_index(index)\n            self.scroll_to_item(index)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nscrolls to the selected item of ThumbnailScrollBar.", "response": "def scroll_to_item(self, index):\n        \"\"\"Scroll to the selected item of ThumbnailScrollBar.\"\"\"\n        spacing_between_items = self.scene.verticalSpacing()\n        height_view = self.scrollarea.viewport().height()\n        height_item = self.scene.itemAt(index).sizeHint().height()\n        height_view_excluding_item = max(0, height_view - height_item)\n\n        height_of_top_items = spacing_between_items\n        for i in range(index):\n            item = self.scene.itemAt(i)\n            height_of_top_items += item.sizeHint().height()\n            height_of_top_items += spacing_between_items\n\n        pos_scroll = height_of_top_items - height_view_excluding_item // 2\n\n        vsb = self.scrollarea.verticalScrollBar()\n        vsb.setValue(pos_scroll)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef go_up(self):\n        vsb = self.scrollarea.verticalScrollBar()\n        vsb.setValue(int(vsb.value() - vsb.singleStep()))", "response": "Scroll the scrollbar of the scrollarea up by a single step."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef setup_gui(self):\n        layout = QGridLayout(self)\n        layout.setContentsMargins(0, 0, 0, 0)\n        layout.addWidget(self.canvas, 0, 1)\n        layout.addLayout(self.setup_toolbar(), 0, 3, 2, 1)\n\n        layout.setColumnStretch(0, 100)\n        layout.setColumnStretch(2, 100)\n        layout.setRowStretch(1, 100)", "response": "Setup the main layout of the widget."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset a colored frame around the FigureCanvas.", "response": "def highlight_canvas(self, highlight):\n        \"\"\"\n        Set a colored frame around the FigureCanvas if highlight is True.\n        \"\"\"\n        colorname = self.canvas.palette().highlight().color().name()\n        if highlight:\n            self.canvas.setStyleSheet(\n                    \"FigureCanvas{border: 1px solid %s;}\" % colorname)\n        else:\n            self.canvas.setStyleSheet(\"FigureCanvas{}\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nemit a signal when the toolbutton to save the figure is clicked.", "response": "def emit_save_figure(self):\n        \"\"\"\n        Emit a signal when the toolbutton to save the figure is clicked.\n        \"\"\"\n        self.sig_save_figure.emit(self.canvas.fig, self.canvas.fmt)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef copy_figure(self):\n        if self.fmt in ['image/png', 'image/jpeg']:\n            qpixmap = QPixmap()\n            qpixmap.loadFromData(self.fig, self.fmt.upper())\n            QApplication.clipboard().setImage(qpixmap.toImage())\n        elif self.fmt == 'image/svg+xml':\n            svg_to_clipboard(self.fig)\n        else:\n            return\n\n        self.blink_figure()", "response": "Copy figure to clipboard."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef clear_canvas(self):\n        self.fig = None\n        self.fmt = None\n        self._qpix_buffer = []\n        self.repaint()", "response": "Clear the figure that was painted on the widget."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nload the figure from a png jpg or svg image convert it in a QPixmap and force a repaint of the widget.", "response": "def load_figure(self, fig, fmt):\n        \"\"\"\n        Load the figure from a png, jpg, or svg image, convert it in\n        a QPixmap, and force a repaint of the widget.\n        \"\"\"\n        self.fig = fig\n        self.fmt = fmt\n\n        if fmt in ['image/png', 'image/jpeg']:\n            self._qpix_orig = QPixmap()\n            self._qpix_orig.loadFromData(fig, fmt.upper())\n        elif fmt == 'image/svg+xml':\n            self._qpix_orig = QPixmap(svg_to_image(fig))\n\n        self._qpix_buffer = [self._qpix_orig]\n        self.fwidth = self._qpix_orig.width()\n        self.fheight = self._qpix_orig.height()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef paintEvent(self, event):\n        super(FigureCanvas, self).paintEvent(event)\n        # Prepare the rect on which the image is going to be painted :\n        fw = self.frameWidth()\n        rect = QRect(0 + fw, 0 + fw,\n                     self.size().width() - 2 * fw,\n                     self.size().height() - 2 * fw)\n\n        if self.fig is None or self._blink_flag:\n            return\n\n        # Check/update the qpixmap buffer :\n        qpix2paint = None\n        for qpix in self._qpix_buffer:\n            if qpix.size().width() == rect.width():\n                qpix2paint = qpix\n                break\n        else:\n            if self.fmt in ['image/png', 'image/jpeg']:\n                qpix2paint = self._qpix_orig.scaledToWidth(\n                    rect.width(), mode=Qt.SmoothTransformation)\n            elif self.fmt == 'image/svg+xml':\n                qpix2paint = QPixmap(svg_to_image(self.fig, rect.size()))\n            self._qpix_buffer.append(qpix2paint)\n\n        if qpix2paint is not None:\n            # Paint the image on the widget :\n            qp = QPainter()\n            qp.begin(self)\n            qp.drawPixmap(rect, qpix2paint)\n            qp.end()", "response": "Override to paint a custom image on the Widget."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets UMR excluded modules name list", "response": "def set_umr_namelist(self):\r\n        \"\"\"Set UMR excluded modules name list\"\"\"\r\n        arguments, valid = QInputDialog.getText(self, _('UMR'),\r\n                                  _(\"Set the list of excluded modules as \"\r\n                                    \"this: <i>numpy, scipy</i>\"),\r\n                                  QLineEdit.Normal,\r\n                                  \", \".join(self.get_option('umr/namelist')))\r\n        if valid:\r\n            arguments = to_text_string(arguments)\r\n            if arguments:\r\n                namelist = arguments.replace(' ', '').split(',')\r\n                fixed_namelist = []\r\n                non_ascii_namelist = []\r\n                for module_name in namelist:\r\n                    if PY2:\r\n                        if all(ord(c) < 128 for c in module_name):\r\n                            if programs.is_module_installed(module_name):\r\n                                fixed_namelist.append(module_name)\r\n                        else:\r\n                            QMessageBox.warning(self, _('Warning'),\r\n                            _(\"You are working with Python 2, this means that \"\r\n                              \"you can not import a module that contains non-\"\r\n                              \"ascii characters.\"), QMessageBox.Ok)\r\n                            non_ascii_namelist.append(module_name)\r\n                    elif programs.is_module_installed(module_name):\r\n                        fixed_namelist.append(module_name)\r\n                invalid = \", \".join(set(namelist)-set(fixed_namelist)-\r\n                                    set(non_ascii_namelist))\r\n                if invalid:\r\n                    QMessageBox.warning(self, _('UMR'),\r\n                                        _(\"The following modules are not \"\r\n                                          \"installed on your machine:\\n%s\"\r\n                                          ) % invalid, QMessageBox.Ok)\r\n                QMessageBox.information(self, _('UMR'),\r\n                                    _(\"Please note that these changes will \"\r\n                                      \"be applied only to new Python/IPython \"\r\n                                      \"consoles\"), QMessageBox.Ok)\r\n            else:\r\n                fixed_namelist = []\r\n            self.set_option('umr/namelist', fixed_namelist)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_custom_interpreters_list(self, value):\r\n        custom_list = self.get_option('custom_interpreters_list')\r\n        if value not in custom_list and value != get_python_executable():\r\n            custom_list.append(value)\r\n            self.set_option('custom_interpreters_list', custom_list)", "response": "Update the list of interpreters used and the current one."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck that the used custom interpreters are still valid.", "response": "def validate_custom_interpreters_list(self):\r\n        \"\"\"Check that the used custom interpreters are still valid.\"\"\"\r\n        custom_list = self.get_option('custom_interpreters_list')\r\n        valid_custom_list = []\r\n        for value in custom_list:\r\n            if (osp.isfile(value) and programs.is_python_interpreter(value)\r\n                    and value != get_python_executable()):\r\n                valid_custom_list.append(value)\r\n        self.set_option('custom_interpreters_list', valid_custom_list)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef on_install(self, editor):\n        EditorExtension.on_install(self, editor)\n        self.setParent(editor)\n        self.setPalette(QApplication.instance().palette())\n        self.setFont(QApplication.instance().font())\n        self.editor.panels.refresh()\n        self._background_brush = QBrush(QColor(\n            self.palette().window().color()))\n        self._foreground_pen = QPen(QColor(\n            self.palette().windowText().color()))\n\n        if self.position == self.Position.FLOATING:\n            self.setAttribute(Qt.WA_TransparentForMouseEvents)", "response": "Sets the current editor instance as the parent widget."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfill the panel background using QPalette.", "response": "def paintEvent(self, event):\n        \"\"\"Fills the panel background using QPalette.\"\"\"\n        if self.isVisible() and self.position != self.Position.FLOATING:\n            # fill background\n            self._background_brush = QBrush(QColor(\n                self.editor.sideareas_color))\n            self._foreground_pen = QPen(QColor(\n                self.palette().windowText().color()))\n            painter = QPainter(self)\n            painter.fillRect(event.rect(), self._background_brush)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef setVisible(self, visible):\n        logger.debug('%s visibility changed', self.name)\n        super(Panel, self).setVisible(visible)\n        if self.editor:\n            self.editor.panels.refresh()", "response": "Override this method to set the visible state of the Panel."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_geometry(self, crect):\n        x0, y0, width, height = self.geometry()\n\n        if width is None:\n            width = crect.width()\n        if height is None:\n            height = crect.height()\n\n        # Calculate editor coordinates with their offsets\n        offset = self.editor.contentOffset()\n        x = self.editor.blockBoundingGeometry(self.editor.firstVisibleBlock())\\\n            .translated(offset.x(), offset.y()).left() \\\n            + self.editor.document().documentMargin() \\\n            + self.editor.panels.margin_size(Panel.Position.LEFT)\n        y = crect.top() + self.editor.panels.margin_size(Panel.Position.TOP)\n\n        self.setGeometry(QRect(x+x0, y+y0, width, height))", "response": "Set the geometry for floating panels."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef openssh_tunnel(self, lport, rport, server, remoteip='127.0.0.1',\n                   keyfile=None, password=None, timeout=0.4):\n    \"\"\"\n    We decided to replace pyzmq's openssh_tunnel method to work around\n    issue https://github.com/zeromq/pyzmq/issues/589 which was solved\n    in pyzmq https://github.com/zeromq/pyzmq/pull/615\n    \"\"\"\n    ssh = \"ssh \"\n    if keyfile:\n        ssh += \"-i \" + keyfile\n\n    if ':' in server:\n        server, port = server.split(':')\n        ssh += \" -p %s\" % port\n\n    cmd = \"%s -O check %s\" % (ssh, server)\n    (output, exitstatus) = pexpect.run(cmd, withexitstatus=True)\n    if not exitstatus:\n        pid = int(output[output.find(\"(pid=\")+5:output.find(\")\")])\n        cmd = \"%s -O forward -L 127.0.0.1:%i:%s:%i %s\" % (\n            ssh, lport, remoteip, rport, server)\n        (output, exitstatus) = pexpect.run(cmd, withexitstatus=True)\n        if not exitstatus:\n            atexit.register(_stop_tunnel, cmd.replace(\"-O forward\",\n                                                      \"-O cancel\",\n                                                      1))\n            return pid\n    cmd = \"%s -f -S none -L 127.0.0.1:%i:%s:%i %s sleep %i\" % (\n                                  ssh, lport, remoteip, rport, server, timeout)\n\n    # pop SSH_ASKPASS from env\n    env = os.environ.copy()\n    env.pop('SSH_ASKPASS', None)\n\n    ssh_newkey = 'Are you sure you want to continue connecting'\n    tunnel = pexpect.spawn(cmd, env=env)\n    failed = False\n    while True:\n        try:\n            i = tunnel.expect([ssh_newkey, '[Pp]assword:'], timeout=.1)\n            if i == 0:\n                host = server.split('@')[-1]\n                question = _(\"The authenticity of host <b>%s</b> can't be \"\n                             \"established. Are you sure you want to continue \"\n                             \"connecting?\") % host\n                reply = QMessageBox.question(self, _('Warning'), question,\n                                             QMessageBox.Yes | QMessageBox.No,\n                                             QMessageBox.No)\n                if reply == QMessageBox.Yes:\n                    tunnel.sendline('yes')\n                    continue\n                else:\n                    tunnel.sendline('no')\n                    raise RuntimeError(\n                       _(\"The authenticity of the host can't be established\"))\n            if i == 1 and password is not None:\n                tunnel.sendline(password)\n        except pexpect.TIMEOUT:\n            continue\n        except pexpect.EOF:\n            if tunnel.exitstatus:\n                raise RuntimeError(_(\"Tunnel '%s' failed to start\") % cmd)\n            else:\n                return tunnel.pid\n        else:\n            if failed or password is None:\n                raise RuntimeError(_(\"Could not connect to remote host\"))\n                # TODO: Use this block when pyzmq bug #620 is fixed\n                # # Prompt a passphrase dialog to the user for a second attempt\n                # password, ok = QInputDialog.getText(self, _('Password'),\n                #             _('Enter password for: ') + server,\n                #             echo=QLineEdit.Password)\n                # if ok is False:\n                #      raise RuntimeError('Could not connect to remote host.')\n            tunnel.sendline(password)\n            failed = True", "response": "Tunnels a new node to a new server."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconfiguring associated namespace browser widget", "response": "def configure_namespacebrowser(self):\n        \"\"\"Configure associated namespace browser widget\"\"\"\n        # Update namespace view\n        self.sig_namespace_view.connect(lambda data:\n            self.namespacebrowser.process_remote_view(data))\n\n        # Update properties of variables\n        self.sig_var_properties.connect(lambda data:\n            self.namespacebrowser.set_var_properties(data))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the namespace view settings", "response": "def set_namespace_view_settings(self):\n        \"\"\"Set the namespace view settings\"\"\"\n        if self.namespacebrowser:\n            settings = to_text_string(\n                self.namespacebrowser.get_view_settings())\n            code =(u\"get_ipython().kernel.namespace_view_settings = %s\" %\n                   settings)\n            self.silent_execute(code)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nasks kernel for a value", "response": "def get_value(self, name):\n        \"\"\"Ask kernel for a value\"\"\"\n        code = u\"get_ipython().kernel.get_value('%s')\" % name\n        if self._reading:\n            method = self.kernel_client.input\n            code = u'!' + code\n        else:\n            method = self.silent_execute\n\n        # Wait until the kernel returns the value\n        wait_loop = QEventLoop()\n        self.sig_got_reply.connect(wait_loop.quit)\n        method(code)\n        wait_loop.exec_()\n\n        # Remove loop connection and loop\n        self.sig_got_reply.disconnect(wait_loop.quit)\n        wait_loop = None\n\n        # Handle exceptions\n        if self._kernel_value is None:\n            if self._kernel_reply:\n                msg = self._kernel_reply[:]\n                self._kernel_reply = None\n                raise ValueError(msg)\n\n        return self._kernel_value"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_value(self, name, value):\n        value = to_text_string(value)\n        code = u\"get_ipython().kernel.set_value('%s', %s, %s)\" % (name, value,\n                                                                  PY2)\n\n        if self._reading:\n            self.kernel_client.input(u'!' + code)\n        else:\n            self.silent_execute(code)", "response": "Set value for a variable"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove a variable from the current process.", "response": "def remove_value(self, name):\n        \"\"\"Remove a variable\"\"\"\n        code = u\"get_ipython().kernel.remove_value('%s')\" % name\n        if self._reading:\n            self.kernel_client.input(u'!' + code)\n        else:\n            self.silent_execute(code)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef copy_value(self, orig_name, new_name):\n        code = u\"get_ipython().kernel.copy_value('%s', '%s')\" % (orig_name,\n                                                                 new_name)\n        if self._reading:\n            self.kernel_client.input(u'!' + code)\n        else:\n            self.silent_execute(code)", "response": "Copy a variable from the current thread to another thread."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nhandling the spyder messages.", "response": "def _handle_spyder_msg(self, msg):\n        \"\"\"\n        Handle internal spyder messages\n        \"\"\"\n        spyder_msg_type = msg['content'].get('spyder_msg_type')\n        if spyder_msg_type == 'data':\n            # Deserialize data\n            try:\n                if PY2:\n                    value = cloudpickle.loads(msg['buffers'][0])\n                else:\n                    value = cloudpickle.loads(bytes(msg['buffers'][0]))\n            except Exception as msg:\n                self._kernel_value = None\n                self._kernel_reply = repr(msg)\n            else:\n                self._kernel_value = value\n            self.sig_got_reply.emit()\n            return\n        elif spyder_msg_type == 'pdb_state':\n            pdb_state = msg['content']['pdb_state']\n            if pdb_state is not None and isinstance(pdb_state, dict):\n                self.refresh_from_pdb(pdb_state)\n        elif spyder_msg_type == 'pdb_continue':\n            # Run Pdb continue to get to the first breakpoint\n            # Fixes 2034\n            self.write_to_stdin('continue')\n        elif spyder_msg_type == 'set_breakpoints':\n            self.set_spyder_breakpoints(force=True)\n        else:\n            logger.debug(\"No such spyder message type: %s\" % spyder_msg_type)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _handle_execute_reply(self, msg):\n        msg_id = msg['parent_header']['msg_id']\n        info = self._request_info['execute'].get(msg_id)\n        # unset reading flag, because if execute finished, raw_input can't\n        # still be pending.\n        self._reading = False\n\n        # Refresh namespacebrowser after the kernel starts running\n        exec_count = msg['content'].get('execution_count', '')\n        if exec_count == 0 and self._kernel_is_starting:\n            if self.namespacebrowser is not None:\n                self.set_namespace_view_settings()\n                self.refresh_namespacebrowser()\n            self._kernel_is_starting = False\n            self.ipyclient.t0 = time.monotonic()\n\n        # Handle silent execution of kernel methods\n        if info and info.kind == 'silent_exec_method' and not self._hidden:\n            self.handle_exec_method(msg)\n            self._request_info['execute'].pop(msg_id)\n        else:\n            super(NamepaceBrowserWidget, self)._handle_execute_reply(msg)", "response": "Reimplemented to handle communications between Spyder and kernel kernels and kernel kernels"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _handle_status(self, msg):\n        state = msg['content'].get('execution_state', '')\n        msg_type = msg['parent_header'].get('msg_type', '')\n        if state == 'starting':\n            # This is needed to show the time a kernel\n            # has been alive in each console.\n            self.ipyclient.t0 = time.monotonic()\n            self.ipyclient.timer.timeout.connect(self.ipyclient.show_time)\n            self.ipyclient.timer.start(1000)\n\n            # This handles restarts when the kernel dies\n            # unexpectedly\n            if not self._kernel_is_starting:\n                self._kernel_is_starting = True\n        elif state == 'idle' and msg_type == 'shutdown_request':\n            # This handles restarts asked by the user\n            if self.namespacebrowser is not None:\n                self.set_namespace_view_settings()\n                self.refresh_namespacebrowser()\n            self.ipyclient.t0 = time.monotonic()\n        else:\n            super(NamepaceBrowserWidget, self)._handle_status(msg)", "response": "Reimplemented to refresh the namespacebrowser after kernel dies\n           "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninitialize plugin position in Spyder s main window layout.", "response": "def initialize_plugin_in_mainwindow_layout(self):\n        \"\"\"\n        If this is the first time the plugin is shown, perform actions to\n        initialize plugin position in Spyder's window layout.\n\n        Use on_first_registration to define the actions to be run\n        by your plugin\n        \"\"\"\n        if self.get_option('first_time', True):\n            try:\n                self.on_first_registration()\n            except NotImplementedError:\n                return\n            self.set_option('first_time', False)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupdates plugin title i. e. dockwidget or window title", "response": "def update_plugin_title(self):\n        \"\"\"Update plugin title, i.e. dockwidget or window title\"\"\"\n        if self.dockwidget is not None:\n            win = self.dockwidget\n        elif self.undocked_window is not None:\n            win = self.undocked_window\n        else:\n            return\n        win.setWindowTitle(self.get_plugin_title())"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_dockwidget(self):\n        # Creating dock widget\n        dock = SpyderDockWidget(self.get_plugin_title(), self.main)\n\n        # Set properties\n        dock.setObjectName(self.__class__.__name__+\"_dw\")\n        dock.setAllowedAreas(self.ALLOWED_AREAS)\n        dock.setFeatures(self.FEATURES)\n        dock.setWidget(self)\n        self.update_margins()\n        dock.visibilityChanged.connect(self.visibility_changed)\n        dock.topLevelChanged.connect(self.on_top_level_changed)\n        dock.sig_plugin_closed.connect(self.plugin_closed)\n        self.dockwidget = dock\n        if self.shortcut is not None:\n            sc = QShortcut(QKeySequence(self.shortcut), self.main,\n                            self.switch_to_plugin)\n            self.register_shortcut(sc, \"_\", \"Switch to %s\" % self.CONF_SECTION)\n        return (dock, self.LOCATION)", "response": "Create a new dock widget and add it to the main window"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_configwidget(self, parent):\n        if self.CONFIGWIDGET_CLASS is not None:\n            configwidget = self.CONFIGWIDGET_CLASS(self, parent)\n            configwidget.initialize()\n            return configwidget", "response": "Create a configuration dialog box page widget."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_plugin_font(self, rich_text=False):\n\n        if rich_text:\n            option = 'rich_font'\n            font_size_delta = self.RICH_FONT_SIZE_DELTA\n        else:\n            option = 'font'\n            font_size_delta = self.FONT_SIZE_DELTA\n\n        return get_font(option=option, font_size_delta=font_size_delta)", "response": "Get plugin font option."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef show_message(self, message, timeout=0):\n        self.main.statusBar().showMessage(message, timeout)", "response": "Show message in main window s status bar"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_toggle_view_action(self):\n        title = self.get_plugin_title()\n        if self.CONF_SECTION == 'editor':\n            title = _('Editor')\n        if self.shortcut is not None:\n            action = create_action(self, title,\n                             toggled=lambda checked: self.toggle_view(checked),\n                             shortcut=QKeySequence(self.shortcut),\n                             context=Qt.WidgetShortcut)\n        else:\n            action = create_action(self, title, toggled=lambda checked:\n                                                self.toggle_view(checked))\n        self.toggle_view_action = action", "response": "Associate a toggle view action with each plugin"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntoggles view of the dockwidget.", "response": "def toggle_view(self, checked):\n        \"\"\"Toggle view\"\"\"\n        if not self.dockwidget:\n            return\n        if checked:\n            self.dockwidget.show()\n            self.dockwidget.raise_()\n        else:\n            self.dockwidget.hide()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef close_window(self):\n        if self.undocked_window is not None:\n            self.undocked_window.close()\n            self.undocked_window = None\n\n            # Oddly, these actions can appear disabled after the Dock\n            # action is pressed\n            self.undock_action.setDisabled(False)\n            self.close_plugin_action.setDisabled(False)", "response": "Close the window that contains this plugin."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a QMainWindow instance containing this plugin.", "response": "def create_window(self):\n        \"\"\"Create a QMainWindow instance containing this plugin.\"\"\"\n        self.undocked_window = window = PluginWindow(self)\n        window.setAttribute(Qt.WA_DeleteOnClose)\n        icon = self.get_plugin_icon()\n        if is_text_string(icon):\n            icon = self.get_icon(icon)\n        window.setWindowIcon(icon)\n        window.setWindowTitle(self.get_plugin_title())\n        window.setCentralWidget(self)\n        window.resize(self.size())\n        self.refresh_plugin()\n\n        self.dockwidget.setFloating(False)\n        self.dockwidget.setVisible(False)\n\n        window.show()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef on_top_level_changed(self, top_level):\n        if top_level:\n            self.undock_action.setDisabled(True)\n        else:\n            self.undock_action.setDisabled(False)", "response": "Actions to perform when a plugin is undocked to be moved."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef keyPressEvent(self, event):\n        event, text, key, ctrl, shift = restore_keyevent(event)\n        cursor_position = self.get_position('cursor')\n\n        if cursor_position < self.header_end_pos:\n            self.restrict_cursor_position(self.header_end_pos, 'eof')\n        elif key == Qt.Key_Delete:\n            if self.has_selected_text():\n                self.remove_text()\n            else:\n                self.stdkey_clear()\n        elif key == Qt.Key_Backspace:\n            if self.has_selected_text():\n                self.remove_text()\n            elif self.header_end_pos == cursor_position:\n                return\n            else:\n                self.stdkey_backspace()\n        elif key == Qt.Key_X and ctrl:\n            self.cut()\n        else:\n            CodeEditor.keyPressEvent(self, event)", "response": "Reimplemented Qt Method to avoid removing the header."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\naction to take when the submit button is clicked on the github window.", "response": "def _submit_to_github(self):\n        \"\"\"Action to take when pressing the submit button.\"\"\"\n        # Get reference to the main window\n        if self.parent() is not None:\n            if getattr(self.parent(), 'main', False):\n                # This covers the case when the dialog is attached\n                # to the internal console\n                main = self.parent().main\n            else:\n                # Else the dialog is attached to the main window\n                # directly\n                main = self.parent()\n        else:\n            main = None\n\n        # Getting description and traceback\n        title = self.title.text()\n        description = self.input_description.toPlainText()\n        traceback = self.error_traceback[:-1]  # Remove last EOL\n\n        # Render issue\n        if main is not None:\n            issue_text = main.render_issue(description=description,\n                                           traceback=traceback)\n        else:\n            issue_text = description\n\n        try:\n            if main is None:\n                org = 'ccordoba12'\n            else:\n                org = 'spyder-ide'\n            github_backend = GithubBackend(org, 'spyder', parent_widget=main)\n            github_report = github_backend.send_report(title, issue_text)\n            if github_report:\n                self.close()\n        except Exception:\n            ret = QMessageBox.question(\n                      self, _('Error'),\n                      _(\"An error occurred while trying to send the issue to \"\n                        \"Github automatically. Would you like to open it \"\n                        \"manually?<br><br>\"\n                        \"If so, please make sure to paste your clipboard \"\n                        \"into the issue report box that will appear in a new \"\n                        \"browser tab before clicking <i>Submit</i> on that \"\n                        \"page.\"))\n            if ret in [QMessageBox.Yes, QMessageBox.Ok]:\n                QApplication.clipboard().setText(issue_text)\n                issue_body = (\n                    \" \\n<!---   *** BEFORE SUBMITTING: PASTE CLIPBOARD HERE \"\n                    \"TO COMPLETE YOUR REPORT ***   ---!>\\n\")\n                if main is not None:\n                    main.report_issue(body=issue_body, title=title,\n                                      open_webpage=True)\n                else:\n                    pass"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nshowing traceback on its own dialog", "response": "def _show_details(self):\n        \"\"\"Show traceback on its own dialog\"\"\"\n        if self.details.isVisible():\n            self.details.hide()\n            self.details_btn.setText(_('Show details'))\n        else:\n            self.resize(570, 700)\n            self.details.document().setPlainText('')\n            self.details.append_text_to_shell(self.error_traceback,\n                                              error=True,\n                                              prompt=False)\n            self.details.show()\n            self.details_btn.setText(_('Hide details'))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef unmatched_brackets_in_line(self, text, closing_brackets_type=None):\n        if closing_brackets_type is None:\n            opening_brackets = self.BRACKETS_LEFT.values()\n            closing_brackets = self.BRACKETS_RIGHT.values()\n        else:\n            closing_brackets = [closing_brackets_type]\n            opening_brackets = [{')': '(', '}': '{',\n                                 ']': '['}[closing_brackets_type]]\n        block = self.editor.textCursor().block()\n        line_pos = block.position()\n        for pos, char in enumerate(text):\n            if char in opening_brackets:\n                match = self.editor.find_brace_match(line_pos+pos, char,\n                                                     forward=True)\n                if (match is None) or (match > line_pos+len(text)):\n                    return True\n            if char in closing_brackets:\n                match = self.editor.find_brace_match(line_pos+pos, char,\n                                                     forward=False)\n                if (match is None) or (match < line_pos):\n                    return True\n        return False", "response": "Checks if there is an unmatched brackets in the text."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _autoinsert_brackets(self, key):\n        char = self.BRACKETS_CHAR[key]\n        pair = self.BRACKETS_PAIR[key]\n\n        line_text = self.editor.get_text('sol', 'eol')\n        line_to_cursor = self.editor.get_text('sol', 'cursor')\n        cursor = self.editor.textCursor()\n        trailing_text = self.editor.get_text('cursor', 'eol').strip()\n\n        if self.editor.has_selected_text():\n            text = self.editor.get_selected_text()\n            self.editor.insert_text(\"{0}{1}{2}\".format(pair[0], text, pair[1]))\n            # Keep text selected, for inserting multiple brackets\n            cursor.movePosition(QTextCursor.Left, QTextCursor.MoveAnchor, 1)\n            cursor.movePosition(QTextCursor.Left, QTextCursor.KeepAnchor,\n                                len(text))\n            self.editor.setTextCursor(cursor)\n        elif key in self.BRACKETS_LEFT:\n            if (not trailing_text or\n                    trailing_text[0] in self.BRACKETS_RIGHT.values() or\n                    trailing_text[0] in [',', ':', ';']):\n                # Automatic insertion of brackets\n                self.editor.insert_text(pair)\n                cursor.movePosition(QTextCursor.PreviousCharacter)\n                self.editor.setTextCursor(cursor)\n            else:\n                self.editor.insert_text(char)\n            if char in self.editor.signature_completion_characters:\n                self.editor.request_signature()\n        elif key in self.BRACKETS_RIGHT:\n            if (self.editor.next_char() == char and\n                    not self.editor.textCursor().atBlockEnd() and\n                    not self.unmatched_brackets_in_line(\n                        cursor.block().text(), char)):\n                # Overwrite an existing brackets if all in line are matched\n                cursor.movePosition(QTextCursor.NextCharacter,\n                                    QTextCursor.KeepAnchor, 1)\n                cursor.clearSelection()\n                self.editor.setTextCursor(cursor)\n            else:\n                self.editor.insert_text(char)", "response": "Control automatic insertation of brackets in various situations."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating a list of montages of specified size and shape of each image in image_list.", "response": "def build_montages(image_list, image_shape, montage_shape):\n    \"\"\"\n    ---------------------------------------------------------------------------------------------\n    author: Kyle Hounslow\n    ---------------------------------------------------------------------------------------------\n    Converts a list of single images into a list of 'montage' images of specified rows and columns.\n    A new montage image is started once rows and columns of montage image is filled.\n    Empty space of incomplete montage images are filled with black pixels\n    ---------------------------------------------------------------------------------------------\n    :param image_list: python list of input images\n    :param image_shape: tuple, size each image will be resized to for display (width, height)\n    :param montage_shape: tuple, shape of image montage (width, height)\n    :return: list of montage images in numpy array format\n    ---------------------------------------------------------------------------------------------\n\n    example usage:\n\n    # load single image\n    img = cv2.imread('lena.jpg')\n    # duplicate image 25 times\n    num_imgs = 25\n    img_list = []\n    for i in xrange(num_imgs):\n        img_list.append(img)\n    # convert image list into a montage of 256x256 images tiled in a 5x5 montage\n    montages = make_montages_of_images(img_list, (256, 256), (5, 5))\n    # iterate through montages and display\n    for montage in montages:\n        cv2.imshow('montage image', montage)\n        cv2.waitKey(0)\n\n    ----------------------------------------------------------------------------------------------\n    \"\"\"\n    if len(image_shape) != 2:\n        raise Exception('image shape must be list or tuple of length 2 (rows, cols)')\n    if len(montage_shape) != 2:\n        raise Exception('montage shape must be list or tuple of length 2 (rows, cols)')\n    image_montages = []\n    # start with black canvas to draw images onto\n    montage_image = np.zeros(shape=(image_shape[1] * (montage_shape[1]), image_shape[0] * montage_shape[0], 3),\n                          dtype=np.uint8)\n    cursor_pos = [0, 0]\n    start_new_img = False\n    for img in image_list:\n        if type(img).__module__ != np.__name__:\n            raise Exception('input of type {} is not a valid numpy array'.format(type(img)))\n        start_new_img = False\n        img = cv2.resize(img, image_shape)\n        # draw image to black canvas\n        montage_image[cursor_pos[1]:cursor_pos[1] + image_shape[1], cursor_pos[0]:cursor_pos[0] + image_shape[0]] = img\n        cursor_pos[0] += image_shape[0]  # increment cursor x position\n        if cursor_pos[0] >= montage_shape[0] * image_shape[0]:\n            cursor_pos[1] += image_shape[1]  # increment cursor y position\n            cursor_pos[0] = 0\n            if cursor_pos[1] >= montage_shape[1] * image_shape[1]:\n                cursor_pos = [0, 0]\n                image_montages.append(montage_image)\n                # reset black canvas\n                montage_image = np.zeros(shape=(image_shape[1] * (montage_shape[1]), image_shape[0] * montage_shape[0], 3),\n                                      dtype=np.uint8)\n                start_new_img = True\n    if start_new_img is False:\n        image_montages.append(montage_image)  # add unfinished montage\n    return image_montages"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadjusting the brightness and contrast of an image.", "response": "def adjust_brightness_contrast(image, brightness=0., contrast=0.):\n    \"\"\"\n    Adjust the brightness and/or contrast of an image\n\n    :param image: OpenCV BGR image\n    :param contrast: Float, contrast adjustment with 0 meaning no change\n    :param brightness: Float, brightness adjustment with 0 meaning no change\n    \"\"\"\n    beta = 0\n    # See the OpenCV docs for more info on the `beta` parameter to addWeighted\n    # https://docs.opencv.org/3.4.2/d2/de8/group__core__array.html#gafafb2513349db3bcff51f54ee5592a19\n    return cv2.addWeighted(image,\n                           1 + float(contrast) / 100.,\n                           image,\n                           beta,\n                           float(brightness))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef put_text(img, text, org, font_face, font_scale, color, thickness=1, line_type=8, bottom_left_origin=False):\n    # Break out drawing coords\n    x, y = org\n\n    # Break text into list of text lines\n    text_lines = text.split('\\n')\n\n    # Get height of text lines in pixels (height of all lines is the same)\n    _, line_height = cv2.getTextSize('', font_face, font_scale, thickness)[0]\n    # Set distance between lines in pixels\n    line_gap = line_height // 3\n\n    for i, text_line in enumerate(text_lines):\n        # Find total size of text block before this line\n        line_y_adjustment = i * (line_gap + line_height)\n\n        # Move text down from original line based on line number\n        if not bottom_left_origin:\n            line_y = y + line_y_adjustment\n        else:\n            line_y = y - line_y_adjustment\n\n        # Draw text\n        cv2.putText(img,\n                    text=text_lines[i],\n                    org=(x, line_y),\n                    fontFace=font_face,\n                    fontScale=font_scale,\n                    color=color,\n                    thickness=thickness,\n                    lineType=line_type,\n                    bottomLeftOrigin=bottom_left_origin)", "response": "Utility for drawing text with line breaks."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef put_centered_text(img, text, font_face, font_scale, color, thickness=1, line_type=8):\n    # Save img dimensions\n    img_h, img_w = img.shape[:2]\n\n    # Break text into list of text lines\n    text_lines = text.split('\\n')\n\n    # Get height of text lines in pixels (height of all lines is the same; width differs)\n    _, line_height = cv2.getTextSize('', font_face, font_scale, thickness)[0]\n    # Set distance between lines in pixels\n    line_gap = line_height // 3\n\n    # Calculate total text block height for centering\n    text_block_height = len(text_lines) * (line_height + line_gap)\n    text_block_height -= line_gap  # There's one less gap than lines\n\n    for i, text_line in enumerate(text_lines):\n        # Get width of text line in pixels (height of all lines is the same)\n        line_width, _ = cv2.getTextSize(text_line, font_face, font_scale, thickness)[0]\n\n        # Center line with image dimensions\n        x = (img_w - line_width) // 2\n        y = (img_h + line_height) // 2\n\n        # Find total size of text block before this line\n        line_adjustment = i * (line_gap + line_height)\n\n        # Adjust line y and re-center relative to total text block height\n        y += line_adjustment - text_block_height // 2 + line_gap\n\n        # Draw text\n        cv2.putText(img,\n                    text=text_lines[i],\n                    org=(x, y),\n                    fontFace=font_face,\n                    fontScale=font_scale,\n                    color=color,\n                    thickness=thickness,\n                    lineType=line_type)", "response": "Utility for drawing vertically & horizontally centered text with line breaks."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef corners_to_keypoints(corners):\n    if corners is None:\n        keypoints = []\n    else:\n        keypoints = [cv2.KeyPoint(kp[0][0], kp[0][1], 1) for kp in corners]\n\n    return keypoints", "response": "function to take the corners from cv2. GoodFeaturesToTrack and return cv2. KeyPoints"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef path_to_url(path):\n    path = os.path.normpath(os.path.abspath(path))\n    url = urlparse.urljoin(\"file:\", urllib2.pathname2url(path))\n    return url", "response": "Convert a path to a file URL."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn True if path is a directory containing a setup. py file.", "response": "def is_installable_dir(path):\n    \"\"\"Return True if `path` is a directory containing a setup.py file.\"\"\"\n    if not os.path.isdir(path):\n        return False\n    setup_py = os.path.join(path, \"setup.py\")\n    if os.path.isfile(setup_py):\n        return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nretrieves a setting value.", "response": "def setting(self, setting_name, default=None):  # type: (str) -> Any\n        \"\"\"\n        Retrieve a setting value.\n        \"\"\"\n        keys = setting_name.split(\".\")\n\n        config = self._content\n        for key in keys:\n            if key not in config:\n                return default\n\n            config = config[key]\n\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_requires_for_build_wheel(config_settings=None):\n    poetry = Poetry.create(\".\")\n\n    main, _ = SdistBuilder.convert_dependencies(poetry.package, poetry.package.requires)\n\n    return main", "response": "Returns a list of requirements for building a wheel."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef build_wheel(wheel_directory, config_settings=None, metadata_directory=None):\n    poetry = Poetry.create(\".\")\n\n    return unicode(\n        WheelBuilder.make_in(\n            poetry, SystemEnv(Path(sys.prefix)), NullIO(), Path(wheel_directory)\n        )\n    )", "response": "Builds a wheel from a directory."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbuild an sdist and places it in sdist_directory", "response": "def build_sdist(sdist_directory, config_settings=None):\n    \"\"\"Builds an sdist, places it in sdist_directory\"\"\"\n    poetry = Poetry.create(\".\")\n\n    path = SdistBuilder(poetry, SystemEnv(Path(sys.prefix)), NullIO()).build(\n        Path(sdist_directory)\n    )\n\n    return unicode(path.name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns all external incompatibilities in this incompatibility s derivation graph.", "response": "def external_incompatibilities(self):  # type: () -> Generator[Incompatibility]\n        \"\"\"\n        Returns all external incompatibilities in this incompatibility's\n        derivation graph.\n        \"\"\"\n        if isinstance(self._cause, ConflictCause):\n            cause = self._cause  # type: ConflictCause\n            for incompatibility in cause.conflict.external_incompatibilities:\n                yield incompatibility\n\n            for incompatibility in cause.other.external_incompatibilities:\n                yield incompatibility\n        else:\n            yield self"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef decide(self, package):  # type: (Package) -> None\n        # When we make a new decision after backtracking, count an additional\n        # attempted solution. If we backtrack multiple times in a row, though, we\n        # only want to count one, since we haven't actually started attempting a\n        # new solution.\n        if self._backtracking:\n            self._attempted_solutions += 1\n\n        self._backtracking = False\n        self._decisions[package.name] = package\n\n        self._assign(\n            Assignment.decision(package, self.decision_level, len(self._assignments))\n        )", "response": "Adds an assignment of package as a decision and increments the decision level."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef derive(\n        self, dependency, is_positive, cause\n    ):  # type: (Dependency, bool, Incompatibility) -> None\n        \"\"\"\n        Adds an assignment of package as a derivation.\n        \"\"\"\n        self._assign(\n            Assignment.derivation(\n                dependency,\n                is_positive,\n                cause,\n                self.decision_level,\n                len(self._assignments),\n            )\n        )", "response": "Adds an assignment of package as a derivation."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding an Assignment to _assignments and _positive or _negative.", "response": "def _assign(self, assignment):  # type: (Assignment) -> None\n        \"\"\"\n        Adds an Assignment to _assignments and _positive or _negative.\n        \"\"\"\n        self._assignments.append(assignment)\n        self._register(assignment)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nremove all assignments made after the given decision level.", "response": "def backtrack(self, decision_level):  # type: (int) -> None\n        \"\"\"\n        Resets the current decision level to decision_level, and removes all\n        assignments made after that level.\n        \"\"\"\n        self._backtracking = True\n\n        packages = set()\n        while self._assignments[-1].decision_level > decision_level:\n            removed = self._assignments.pop(-1)\n            packages.add(removed.dependency.name)\n            if removed.is_decision():\n                del self._decisions[removed.dependency.name]\n\n        # Re-compute _positive and _negative for the packages that were removed.\n        for package in packages:\n            if package in self._positive:\n                del self._positive[package]\n\n            if package in self._negative:\n                del self._negative[package]\n\n        for assignment in self._assignments:\n            if assignment.dependency.name in packages:\n                self._register(assignment)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _register(self, assignment):  # type: (Assignment) -> None\n        name = assignment.dependency.name\n        old_positive = self._positive.get(name)\n        if old_positive is not None:\n            self._positive[name] = old_positive.intersect(assignment)\n\n            return\n\n        ref = assignment.dependency.name\n        negative_by_ref = self._negative.get(name)\n        old_negative = None if negative_by_ref is None else negative_by_ref.get(ref)\n        if old_negative is None:\n            term = assignment\n        else:\n            term = assignment.intersect(old_negative)\n\n        if term.is_positive():\n            if name in self._negative:\n                del self._negative[name]\n\n            self._positive[name] = term\n        else:\n            if name not in self._negative:\n                self._negative[name] = {}\n\n            self._negative[name][ref] = term", "response": "Registers an Assignment in _positive or _negative."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the first Assignment that satisfies term.", "response": "def satisfier(self, term):  # type: (Term) -> Assignment\n        \"\"\"\n        Returns the first Assignment in this solution such that the sublist of\n        assignments up to and including that entry collectively satisfies term.\n        \"\"\"\n        assigned_term = None  # type: Term\n\n        for assignment in self._assignments:\n            if assignment.dependency.name != term.dependency.name:\n                continue\n\n            if (\n                not assignment.dependency.is_root\n                and not assignment.dependency.name == term.dependency.name\n            ):\n                if not assignment.is_positive():\n                    continue\n\n                assert not term.is_positive()\n\n                return assignment\n\n            if assigned_term is None:\n                assigned_term = assignment\n            else:\n                assigned_term = assigned_term.intersect(assignment)\n\n            # As soon as we have enough assignments to satisfy term, return them.\n            if assigned_term.satisfies(term):\n                return assignment\n\n        raise RuntimeError(\"[BUG] {} is not satisfied.\".format(term))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check(cls, config, strict=False):  # type: (dict, bool) -> Dict[str, List[str]]\n        result = {\"errors\": [], \"warnings\": []}\n        # Schema validation errors\n        validation_errors = validate_object(config, \"poetry-schema\")\n\n        result[\"errors\"] += validation_errors\n\n        if strict:\n            # If strict, check the file more thoroughly\n\n            # Checking license\n            license = config.get(\"license\")\n            if license:\n                try:\n                    license_by_id(license)\n                except ValueError:\n                    result[\"errors\"].append(\"{} is not a valid license\".format(license))\n\n            if \"dependencies\" in config:\n                python_versions = config[\"dependencies\"][\"python\"]\n                if python_versions == \"*\":\n                    result[\"warnings\"].append(\n                        \"A wildcard Python dependency is ambiguous. \"\n                        \"Consider specifying a more explicit one.\"\n                    )\n\n            # Checking for scripts with extras\n            if \"scripts\" in config:\n                scripts = config[\"scripts\"]\n                for name, script in scripts.items():\n                    if not isinstance(script, dict):\n                        continue\n\n                    extras = script[\"extras\"]\n                    for extra in extras:\n                        if extra not in config[\"extras\"]:\n                            result[\"errors\"].append(\n                                'Script \"{}\" requires extra \"{}\" which is not defined.'.format(\n                                    name, extra\n                                )\n                            )\n\n        return result", "response": "Checks the validity of a configuration object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef find_packages(self, include):\n        pkgdir = None\n        if include.source is not None:\n            pkgdir = str(include.base)\n\n        base = str(include.elements[0].parent)\n\n        pkg_name = include.package\n        pkg_data = defaultdict(list)\n        # Undocumented distutils feature:\n        # the empty string matches all package names\n        pkg_data[\"\"].append(\"*\")\n        packages = [pkg_name]\n        subpkg_paths = set()\n\n        def find_nearest_pkg(rel_path):\n            parts = rel_path.split(os.sep)\n            for i in reversed(range(1, len(parts))):\n                ancestor = \"/\".join(parts[:i])\n                if ancestor in subpkg_paths:\n                    pkg = \".\".join([pkg_name] + parts[:i])\n                    return pkg, \"/\".join(parts[i:])\n\n            # Relative to the top-level package\n            return pkg_name, Path(rel_path).as_posix()\n\n        excluded_files = self.find_excluded_files()\n        for path, dirnames, filenames in os.walk(str(base), topdown=True):\n            if os.path.basename(path) == \"__pycache__\":\n                continue\n\n            from_top_level = os.path.relpath(path, base)\n            if from_top_level == \".\":\n                continue\n\n            is_subpkg = \"__init__.py\" in filenames\n            if is_subpkg:\n                subpkg_paths.add(from_top_level)\n                parts = from_top_level.split(os.sep)\n                packages.append(\".\".join([pkg_name] + parts))\n            else:\n                pkg, from_nearest_pkg = find_nearest_pkg(from_top_level)\n\n                data_elements = [\n                    f.relative_to(self._path)\n                    for f in Path(path).glob(\"*\")\n                    if not f.is_dir()\n                ]\n\n                data = [e for e in data_elements if not self.is_excluded(e)]\n                if not data:\n                    continue\n\n                if len(data) == len(data_elements):\n                    pkg_data[pkg].append(pjoin(from_nearest_pkg, \"*\"))\n                else:\n                    for d in data:\n                        if d.is_dir():\n                            continue\n\n                        pkg_data[pkg] += [pjoin(from_nearest_pkg, d.name) for d in data]\n\n        # Sort values in pkg_data\n        pkg_data = {k: sorted(v) for (k, v) in pkg_data.items() if v}\n\n        return pkgdir, sorted(packages), pkg_data", "response": "Find all packages and data."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef clean_tarinfo(cls, tar_info):\n        ti = copy(tar_info)\n        ti.uid = 0\n        ti.gid = 0\n        ti.uname = \"\"\n        ti.gname = \"\"\n        ti.mode = normalize_file_permissions(ti.mode)\n\n        return ti", "response": "Clean metadata from a TarInfo object to make it more reproducible."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get(cls):  # type: () -> Shell\n        if cls._shell is not None:\n            return cls._shell\n\n        try:\n            name, path = detect_shell(os.getpid())\n        except (RuntimeError, ShellDetectionFailure):\n            raise RuntimeError(\"Unable to detect the current shell.\")\n\n        cls._shell = cls(name, path)\n\n        return cls._shell", "response": "Returns the current shell."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef search_for(self, dependency):  # type: (Dependency) -> List[Package]\n        if dependency.is_root:\n            return PackageCollection(dependency, [self._package])\n\n        for constraint in self._search_for.keys():\n            if (\n                constraint.name == dependency.name\n                and constraint.constraint.intersect(dependency.constraint)\n                == dependency.constraint\n            ):\n                packages = [\n                    p\n                    for p in self._search_for[constraint]\n                    if dependency.constraint.allows(p.version)\n                ]\n\n                packages.sort(\n                    key=lambda p: (\n                        not p.is_prerelease() and not dependency.allows_prereleases(),\n                        p.version,\n                    ),\n                    reverse=True,\n                )\n\n                return PackageCollection(dependency, packages)\n\n        if dependency.is_vcs():\n            packages = self.search_for_vcs(dependency)\n        elif dependency.is_file():\n            packages = self.search_for_file(dependency)\n        elif dependency.is_directory():\n            packages = self.search_for_directory(dependency)\n        else:\n            constraint = dependency.constraint\n\n            packages = self._pool.find_packages(\n                dependency.name,\n                constraint,\n                extras=dependency.extras,\n                allow_prereleases=dependency.allows_prereleases(),\n            )\n\n            packages.sort(\n                key=lambda p: (\n                    not p.is_prerelease() and not dependency.allows_prereleases(),\n                    p.version,\n                ),\n                reverse=True,\n            )\n\n        self._search_for[dependency] = packages\n\n        return PackageCollection(dependency, packages)", "response": "Search for the specifications that match the given dependency."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsearch for the specifications that match the given VCS dependency.", "response": "def search_for_vcs(self, dependency):  # type: (VCSDependency) -> List[Package]\n        \"\"\"\n        Search for the specifications that match the given VCS dependency.\n\n        Basically, we clone the repository in a temporary directory\n        and get the information we need by checking out the specified reference.\n        \"\"\"\n        if dependency.vcs != \"git\":\n            raise ValueError(\"Unsupported VCS dependency {}\".format(dependency.vcs))\n\n        tmp_dir = Path(mkdtemp(prefix=\"pypoetry-git-{}\".format(dependency.name)))\n\n        try:\n            git = Git()\n            git.clone(dependency.source, tmp_dir)\n            git.checkout(dependency.reference, tmp_dir)\n            revision = git.rev_parse(dependency.reference, tmp_dir).strip()\n\n            if dependency.tag or dependency.rev:\n                revision = dependency.reference\n\n            directory_dependency = DirectoryDependency(\n                dependency.name,\n                tmp_dir,\n                category=dependency.category,\n                optional=dependency.is_optional(),\n            )\n            for extra in dependency.extras:\n                directory_dependency.extras.append(extra)\n\n            package = self.search_for_directory(directory_dependency)[0]\n\n            package.source_type = \"git\"\n            package.source_url = dependency.source\n            package.source_reference = revision\n        except Exception:\n            raise\n        finally:\n            safe_rmtree(str(tmp_dir))\n\n        return [package]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef incompatibilities_for(\n        self, package\n    ):  # type: (DependencyPackage) -> List[Incompatibility]\n        \"\"\"\n        Returns incompatibilities that encapsulate a given package's dependencies,\n        or that it can't be safely selected.\n\n        If multiple subsequent versions of this package have the same\n        dependencies, this will return incompatibilities that reflect that. It\n        won't return incompatibilities that have already been returned by a\n        previous call to _incompatibilities_for().\n        \"\"\"\n        if package.is_root():\n            dependencies = package.all_requires\n        else:\n            dependencies = package.requires\n\n            if not package.python_constraint.allows_all(\n                self._package.python_constraint\n            ):\n                intersection = package.python_constraint.intersect(\n                    package.dependency.transitive_python_constraint\n                )\n                difference = package.dependency.transitive_python_constraint.difference(\n                    intersection\n                )\n                if (\n                    package.dependency.transitive_python_constraint.is_any()\n                    or self._package.python_constraint.intersect(\n                        package.dependency.python_constraint\n                    ).is_empty()\n                    or intersection.is_empty()\n                    or not difference.is_empty()\n                ):\n                    return [\n                        Incompatibility(\n                            [Term(package.to_dependency(), True)],\n                            PythonCause(\n                                package.python_versions, self._package.python_versions\n                            ),\n                        )\n                    ]\n\n        dependencies = [\n            dep\n            for dep in dependencies\n            if dep.name not in self.UNSAFE_PACKAGES\n            and self._package.python_constraint.allows_any(dep.python_constraint)\n        ]\n\n        return [\n            Incompatibility(\n                [Term(package.to_dependency(), True), Term(dep, False)],\n                DependencyCause(),\n            )\n            for dep in dependencies\n        ]", "response": "Returns incompatibilities that encapsulate a given package s dependencies and that it can t be safely selected."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef solve(self):  # type: () -> SolverResult\n        start = time.time()\n        root_dependency = Dependency(self._root.name, self._root.version)\n        root_dependency.is_root = True\n\n        self._add_incompatibility(\n            Incompatibility([Term(root_dependency, False)], RootCause())\n        )\n\n        try:\n            next = self._root.name\n            while next is not None:\n                self._propagate(next)\n                next = self._choose_package_version()\n\n            return self._result()\n        except Exception:\n            raise\n        finally:\n            self._log(\n                \"Version solving took {:.3f} seconds.\\n\"\n                \"Tried {} solutions.\".format(\n                    time.time() - start, self._solution.attempted_solutions\n                )\n            )", "response": "Solve the version of the root package."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nperforming unit propagation on incompatibilities transitively related to package and returns None.", "response": "def _propagate(self, package):  # type: (str) -> None\n        \"\"\"\n        Performs unit propagation on incompatibilities transitively\n        related to package to derive new assignments for _solution.\n        \"\"\"\n        changed = set()\n        changed.add(package)\n\n        while changed:\n            package = changed.pop()\n\n            # Iterate in reverse because conflict resolution tends to produce more\n            # general incompatibilities as time goes on. If we look at those first,\n            # we can derive stronger assignments sooner and more eagerly find\n            # conflicts.\n            for incompatibility in reversed(self._incompatibilities[package]):\n                result = self._propagate_incompatibility(incompatibility)\n\n                if result is _conflict:\n                    # If the incompatibility is satisfied by the solution, we use\n                    # _resolve_conflict() to determine the root cause of the conflict as a\n                    # new incompatibility.\n                    #\n                    # It also backjumps to a point in the solution\n                    # where that incompatibility will allow us to derive new assignments\n                    # that avoid the conflict.\n                    root_cause = self._resolve_conflict(incompatibility)\n\n                    # Back jumping erases all the assignments we did at the previous\n                    # decision level, so we clear [changed] and refill it with the\n                    # newly-propagated assignment.\n                    changed.clear()\n                    changed.add(str(self._propagate_incompatibility(root_cause)))\n                    break\n                elif result is not None:\n                    changed.add(result)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _propagate_incompatibility(\n        self, incompatibility\n    ):  # type: (Incompatibility) -> Union[str, _conflict, None]\n        \"\"\"\n        If incompatibility is almost satisfied by _solution, adds the\n        negation of the unsatisfied term to _solution.\n\n        If incompatibility is satisfied by _solution, returns _conflict. If\n        incompatibility is almost satisfied by _solution, returns the\n        unsatisfied term's package name.\n\n        Otherwise, returns None.\n        \"\"\"\n        # The first entry in incompatibility.terms that's not yet satisfied by\n        # _solution, if one exists. If we find more than one, _solution is\n        # inconclusive for incompatibility and we can't deduce anything.\n        unsatisfied = None\n\n        for term in incompatibility.terms:\n            relation = self._solution.relation(term)\n\n            if relation == SetRelation.DISJOINT:\n                # If term is already contradicted by _solution, then\n                # incompatibility is contradicted as well and there's nothing new we\n                # can deduce from it.\n                return\n            elif relation == SetRelation.OVERLAPPING:\n                # If more than one term is inconclusive, we can't deduce anything about\n                # incompatibility.\n                if unsatisfied is not None:\n                    return\n\n                # If exactly one term in incompatibility is inconclusive, then it's\n                # almost satisfied and [term] is the unsatisfied term. We can add the\n                # inverse of the term to _solution.\n                unsatisfied = term\n\n        # If *all* terms in incompatibility are satisfied by _solution, then\n        # incompatibility is satisfied and we have a conflict.\n        if unsatisfied is None:\n            return _conflict\n\n        self._log(\n            \"derived: {}{}\".format(\n                \"not \" if unsatisfied.is_positive() else \"\", unsatisfied.dependency\n            )\n        )\n\n        self._solution.derive(\n            unsatisfied.dependency, not unsatisfied.is_positive(), incompatibility\n        )\n\n        return unsatisfied.dependency.name", "response": "Propagate an incompatibility to the _solution."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nresolves a conflict incompatibility with the incompatibility that is not satisfied by _solution.", "response": "def _resolve_conflict(\n        self, incompatibility\n    ):  # type: (Incompatibility) -> Incompatibility\n        \"\"\"\n        Given an incompatibility that's satisfied by _solution,\n        The `conflict resolution`_ constructs a new incompatibility that encapsulates the root\n        cause of the conflict and backtracks _solution until the new\n        incompatibility will allow _propagate() to deduce new assignments.\n\n        Adds the new incompatibility to _incompatibilities and returns it.\n\n        .. _conflict resolution: https://github.com/dart-lang/pub/tree/master/doc/solver.md#conflict-resolution\n        \"\"\"\n        self._log(\"conflict: {}\".format(incompatibility))\n\n        new_incompatibility = False\n        while not incompatibility.is_failure():\n            # The term in incompatibility.terms that was most recently satisfied by\n            # _solution.\n            most_recent_term = None\n\n            # The earliest assignment in _solution such that incompatibility is\n            # satisfied by _solution up to and including this assignment.\n            most_recent_satisfier = None\n\n            # The difference between most_recent_satisfier and most_recent_term;\n            # that is, the versions that are allowed by most_recent_satisfier and not\n            # by most_recent_term. This is None if most_recent_satisfier totally\n            # satisfies most_recent_term.\n            difference = None\n\n            # The decision level of the earliest assignment in _solution *before*\n            # most_recent_satisfier such that incompatibility is satisfied by\n            # _solution up to and including this assignment plus\n            # most_recent_satisfier.\n            #\n            # Decision level 1 is the level where the root package was selected. It's\n            # safe to go back to decision level 0, but stopping at 1 tends to produce\n            # better error messages, because references to the root package end up\n            # closer to the final conclusion that no solution exists.\n            previous_satisfier_level = 1\n\n            for term in incompatibility.terms:\n                satisfier = self._solution.satisfier(term)\n\n                if most_recent_satisfier is None:\n                    most_recent_term = term\n                    most_recent_satisfier = satisfier\n                elif most_recent_satisfier.index < satisfier.index:\n                    previous_satisfier_level = max(\n                        previous_satisfier_level, most_recent_satisfier.decision_level\n                    )\n                    most_recent_term = term\n                    most_recent_satisfier = satisfier\n                    difference = None\n                else:\n                    previous_satisfier_level = max(\n                        previous_satisfier_level, satisfier.decision_level\n                    )\n\n                if most_recent_term == term:\n                    # If most_recent_satisfier doesn't satisfy most_recent_term on its\n                    # own, then the next-most-recent satisfier may be the one that\n                    # satisfies the remainder.\n                    difference = most_recent_satisfier.difference(most_recent_term)\n                    if difference is not None:\n                        previous_satisfier_level = max(\n                            previous_satisfier_level,\n                            self._solution.satisfier(difference.inverse).decision_level,\n                        )\n\n            # If most_recent_identifier is the only satisfier left at its decision\n            # level, or if it has no cause (indicating that it's a decision rather\n            # than a derivation), then incompatibility is the root cause. We then\n            # backjump to previous_satisfier_level, where incompatibility is\n            # guaranteed to allow _propagate to produce more assignments.\n            if (\n                previous_satisfier_level < most_recent_satisfier.decision_level\n                or most_recent_satisfier.cause is None\n            ):\n                self._solution.backtrack(previous_satisfier_level)\n                if new_incompatibility:\n                    self._add_incompatibility(incompatibility)\n\n                return incompatibility\n\n            # Create a new incompatibility by combining incompatibility with the\n            # incompatibility that caused most_recent_satisfier to be assigned. Doing\n            # this iteratively constructs an incompatibility that's guaranteed to be\n            # true (that is, we know for sure no solution will satisfy the\n            # incompatibility) while also approximating the intuitive notion of the\n            # \"root cause\" of the conflict.\n            new_terms = []\n            for term in incompatibility.terms:\n                if term != most_recent_term:\n                    new_terms.append(term)\n\n            for term in most_recent_satisfier.cause.terms:\n                if term.dependency != most_recent_satisfier.dependency:\n                    new_terms.append(term)\n\n            # The most_recent_satisfier may not satisfy most_recent_term on its own\n            # if there are a collection of constraints on most_recent_term that\n            # only satisfy it together. For example, if most_recent_term is\n            # `foo\u00a0^1.0.0` and _solution contains `[foo >=1.0.0,\n            # foo\u00a0<2.0.0]`, then most_recent_satisfier will be `foo <2.0.0` even\n            # though it doesn't totally satisfy `foo ^1.0.0`.\n            #\n            # In this case, we add `not (most_recent_satisfier \\ most_recent_term)` to\n            # the incompatibility as well, See the `algorithm documentation`_ for\n            # details.\n            #\n            # .. _algorithm documentation: https://github.com/dart-lang/pub/tree/master/doc/solver.md#conflict-resolution\n            if difference is not None:\n                new_terms.append(difference.inverse)\n\n            incompatibility = Incompatibility(\n                new_terms, ConflictCause(incompatibility, most_recent_satisfier.cause)\n            )\n            new_incompatibility = True\n\n            partially = \"\" if difference is None else \" partially\"\n            bang = \"!\"\n            self._log(\n                \"{} {} is{} satisfied by {}\".format(\n                    bang, most_recent_term, partially, most_recent_satisfier\n                )\n            )\n            self._log(\n                '{} which is caused by \"{}\"'.format(bang, most_recent_satisfier.cause)\n            )\n            self._log(\"{} thus: {}\".format(bang, incompatibility))\n\n        raise SolveFailure(incompatibility)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _choose_package_version(self):  # type: () -> Union[str, None]\n        unsatisfied = self._solution.unsatisfied\n        if not unsatisfied:\n            return\n\n        # Prefer packages with as few remaining versions as possible,\n        # so that if a conflict is necessary it's forced quickly.\n        def _get_min(dependency):\n            if dependency.name in self._use_latest:\n                # If we're forced to use the latest version of a package, it effectively\n                # only has one version to choose from.\n                return 1\n\n            if dependency.name in self._locked:\n                return 1\n\n            try:\n                return len(self._provider.search_for(dependency))\n            except ValueError:\n                return 0\n\n        if len(unsatisfied) == 1:\n            dependency = unsatisfied[0]\n        else:\n            dependency = min(*unsatisfied, key=_get_min)\n\n        locked = self._get_locked(dependency.name)\n        if locked is None or not dependency.constraint.allows(locked.version):\n            try:\n                packages = self._provider.search_for(dependency)\n            except ValueError as e:\n                self._add_incompatibility(\n                    Incompatibility([Term(dependency, True)], PackageNotFoundCause(e))\n                )\n                return dependency.name\n\n            try:\n                version = packages[0]\n            except IndexError:\n                version = None\n        else:\n            version = locked\n\n        if version is None:\n            # If there are no versions that satisfy the constraint,\n            # add an incompatibility that indicates that.\n            self._add_incompatibility(\n                Incompatibility([Term(dependency, True)], NoVersionsCause())\n            )\n\n            return dependency.name\n\n        version = self._provider.complete_package(version)\n\n        conflict = False\n        for incompatibility in self._provider.incompatibilities_for(version):\n            self._add_incompatibility(incompatibility)\n\n            # If an incompatibility is already satisfied, then selecting version\n            # would cause a conflict.\n            #\n            # We'll continue adding its dependencies, then go back to\n            # unit propagation which will guide us to choose a better version.\n            conflict = conflict or all(\n                [\n                    term.dependency.name == dependency.name\n                    or self._solution.satisfies(term)\n                    for term in incompatibility.terms\n                ]\n            )\n\n        if not conflict:\n            self._solution.decide(version)\n            self._log(\n                \"selecting {} ({})\".format(version.name, version.full_pretty_version)\n            )\n\n        return dependency.name", "response": "Tries to select a version of a required package. Returns None if no version is required."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _result(self):  # type: () -> SolverResult\n        decisions = self._solution.decisions\n\n        return SolverResult(\n            self._root,\n            [p for p in decisions if not p.is_root()],\n            self._solution.attempted_solutions,\n        )", "response": "Creates a SolverResult from the _solution and the _root."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrun a command inside the Python environment.", "response": "def run(self, bin, *args, **kwargs):\n        \"\"\"\n        Run a command inside the Python environment.\n        \"\"\"\n        bin = self._bin(bin)\n\n        cmd = [bin] + list(args)\n        shell = kwargs.get(\"shell\", False)\n        call = kwargs.pop(\"call\", False)\n        input_ = kwargs.pop(\"input_\", None)\n\n        if shell:\n            cmd = list_to_shell_command(cmd)\n\n        try:\n            if self._is_windows:\n                kwargs[\"shell\"] = True\n\n            if input_:\n                p = subprocess.Popen(\n                    cmd,\n                    stdin=subprocess.PIPE,\n                    stdout=subprocess.PIPE,\n                    stderr=subprocess.STDOUT,\n                    **kwargs\n                )\n                output = p.communicate(encode(input_))[0]\n            elif call:\n                return subprocess.call(cmd, stderr=subprocess.STDOUT, **kwargs)\n            else:\n                output = subprocess.check_output(\n                    cmd, stderr=subprocess.STDOUT, **kwargs\n                )\n        except CalledProcessError as e:\n            raise EnvCommandError(e)\n\n        return decode(output)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _bin(self, bin):  # type: (str) -> str\n        bin_path = (self._bin_dir / bin).with_suffix(\".exe\" if self._is_windows else \"\")\n        if not bin_path.exists():\n            return bin\n\n        return str(bin_path)", "response": "Return the path to the given executable."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_abbr_impl(env):\n    impl = env.python_implementation\n\n    if impl == \"PyPy\":\n        return \"pp\"\n    elif impl == \"Jython\":\n        return \"jy\"\n    elif impl == \"IronPython\":\n        return \"ip\"\n    elif impl == \"CPython\":\n        return \"cp\"\n\n    raise LookupError(\"Unknown Python implementation: \" + impl)", "response": "Return abbreviated implementation name."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_flag(env, var, fallback, expected=True, warn=True):\n    val = env.config_var(var)\n    if val is None:\n        if warn:\n            warnings.warn(\n                \"Config variable '{0}' is unset, Python ABI tag may \"\n                \"be incorrect\".format(var),\n                RuntimeWarning,\n                2,\n            )\n        return fallback()\n    return val == expected", "response": "Get the SOABI flag if the needed config variable is unset or unavailable."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndetermines if the given package matches this dependency.", "response": "def accepts(self, package):  # type: (poetry.packages.Package) -> bool\n        \"\"\"\n        Determines if the given package matches this dependency.\n        \"\"\"\n        return (\n            self._name == package.name\n            and self._constraint.allows(package.version)\n            and (not package.is_prerelease() or self.allows_prereleases())\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the dependency as optional.", "response": "def deactivate(self):\n        \"\"\"\n        Set the dependency as optional.\n        \"\"\"\n        if not self._optional:\n            self._optional = True\n\n        self._activated = False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef install(self, version, upgrade=False):\n        print(\"Installing version: \" + colorize(\"info\", version))\n\n        self.make_lib(version)\n        self.make_bin()\n        self.make_env()\n        self.update_path()\n\n        return 0", "response": "Installs Poetry in the current directory."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef make_lib(self, version):\n        if os.path.exists(POETRY_LIB_BACKUP):\n            shutil.rmtree(POETRY_LIB_BACKUP)\n\n        # Backup the current installation\n        if os.path.exists(POETRY_LIB):\n            shutil.copytree(POETRY_LIB, POETRY_LIB_BACKUP)\n            shutil.rmtree(POETRY_LIB)\n\n        try:\n            self._make_lib(version)\n        except Exception:\n            if not os.path.exists(POETRY_LIB_BACKUP):\n                raise\n\n            shutil.copytree(POETRY_LIB_BACKUP, POETRY_LIB)\n            shutil.rmtree(POETRY_LIB_BACKUP)\n\n            raise\n        finally:\n            if os.path.exists(POETRY_LIB_BACKUP):\n                shutil.rmtree(POETRY_LIB_BACKUP)", "response": "Packs everything into a single lib directory."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update_path(self):\n        if WINDOWS:\n            return self.add_to_windows_path()\n\n        # Updating any profile we can on UNIX systems\n        export_string = self.get_export_string()\n\n        addition = \"\\n{}\\n\".format(export_string)\n\n        updated = []\n        profiles = self.get_unix_profiles()\n        for profile in profiles:\n            if not os.path.exists(profile):\n                continue\n\n            with open(profile, \"r\") as f:\n                content = f.read()\n\n            if addition not in content:\n                with open(profile, \"a\") as f:\n                    f.write(addition)\n\n                updated.append(os.path.relpath(profile, HOME))", "response": "Update the path of the local cache file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn whether this term satisfies another.", "response": "def satisfies(self, other):  # type: (Term) -> bool\n        \"\"\"\n        Returns whether this term satisfies another.\n        \"\"\"\n        return (\n            self.dependency.name == other.dependency.name\n            and self.relation(other) == SetRelation.SUBSET\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the relationship between the package versions of this term and another.", "response": "def relation(self, other):  # type: (Term) -> int\n        \"\"\"\n        Returns the relationship between the package versions\n        allowed by this term and another.\n        \"\"\"\n        if self.dependency.name != other.dependency.name:\n            raise ValueError(\n                \"{} should refer to {}\".format(other, self.dependency.name)\n            )\n\n        other_constraint = other.constraint\n\n        if other.is_positive():\n            if self.is_positive():\n                if not self._compatible_dependency(other.dependency):\n                    return SetRelation.DISJOINT\n\n                # foo ^1.5.0 is a subset of foo ^1.0.0\n                if other_constraint.allows_all(self.constraint):\n                    return SetRelation.SUBSET\n\n                # foo ^2.0.0 is disjoint with foo ^1.0.0\n                if not self.constraint.allows_any(other_constraint):\n                    return SetRelation.DISJOINT\n\n                return SetRelation.OVERLAPPING\n            else:\n                if not self._compatible_dependency(other.dependency):\n                    return SetRelation.OVERLAPPING\n\n                # not foo ^1.0.0 is disjoint with foo ^1.5.0\n                if self.constraint.allows_all(other_constraint):\n                    return SetRelation.DISJOINT\n\n                # not foo ^1.5.0 overlaps foo ^1.0.0\n                # not foo ^2.0.0 is a superset of foo ^1.5.0\n                return SetRelation.OVERLAPPING\n        else:\n            if self.is_positive():\n                if not self._compatible_dependency(other.dependency):\n                    return SetRelation.SUBSET\n\n                # foo ^2.0.0 is a subset of not foo ^1.0.0\n                if not other_constraint.allows_any(self.constraint):\n                    return SetRelation.SUBSET\n\n                # foo ^1.5.0 is disjoint with not foo ^1.0.0\n                if other_constraint.allows_all(self.constraint):\n                    return SetRelation.DISJOINT\n\n                # foo ^1.0.0 overlaps not foo ^1.5.0\n                return SetRelation.OVERLAPPING\n            else:\n                if not self._compatible_dependency(other.dependency):\n                    return SetRelation.OVERLAPPING\n\n                # not foo ^1.0.0 is a subset of not foo ^1.5.0\n                if self.constraint.allows_all(other_constraint):\n                    return SetRelation.SUBSET\n\n                # not foo ^2.0.0 overlaps not foo ^1.0.0\n                # not foo ^1.5.0 is a superset of not foo ^1.0.0\n                return SetRelation.OVERLAPPING"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef intersect(self, other):  # type: (Term) -> Union[Term, None]\n        if self.dependency.name != other.dependency.name:\n            raise ValueError(\n                \"{} should refer to {}\".format(other, self.dependency.name)\n            )\n\n        if self._compatible_dependency(other.dependency):\n            if self.is_positive() != other.is_positive():\n                # foo ^1.0.0 \u2229 not foo ^1.5.0 \u2192 foo >=1.0.0 <1.5.0\n                positive = self if self.is_positive() else other\n                negative = other if self.is_positive() else self\n\n                return self._non_empty_term(\n                    positive.constraint.difference(negative.constraint), True\n                )\n            elif self.is_positive():\n                # foo ^1.0.0 \u2229 foo >=1.5.0 <3.0.0 \u2192 foo ^1.5.0\n                return self._non_empty_term(\n                    self.constraint.intersect(other.constraint), True\n                )\n            else:\n                # not foo ^1.0.0 \u2229 not foo >=1.5.0 <3.0.0 \u2192 not foo >=1.0.0 <3.0.0\n                return self._non_empty_term(\n                    self.constraint.union(other.constraint), False\n                )\n        elif self.is_positive() != other.is_positive():\n            return self if self.is_positive() else other\n        else:\n            return", "response": "Returns a Term that represents the packages\n            allowed by both this and other."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef find_files_to_add(self, exclude_build=True):  # type: (bool) -> list\n        to_add = []\n\n        for include in self._module.includes:\n            for file in include.elements:\n                if \"__pycache__\" in str(file):\n                    continue\n\n                if file.is_dir():\n                    continue\n\n                file = file.relative_to(self._path)\n\n                if self.is_excluded(file) and isinstance(include, PackageInclude):\n                    continue\n\n                if file.suffix == \".pyc\":\n                    continue\n\n                if file in to_add:\n                    # Skip duplicates\n                    continue\n\n                self._io.writeln(\n                    \" - Adding: <comment>{}</comment>\".format(str(file)),\n                    verbosity=self._io.VERBOSITY_VERY_VERBOSE,\n                )\n                to_add.append(file)\n\n        # Include project files\n        self._io.writeln(\n            \" - Adding: <comment>pyproject.toml</comment>\",\n            verbosity=self._io.VERBOSITY_VERY_VERBOSE,\n        )\n        to_add.append(Path(\"pyproject.toml\"))\n\n        # If a license file exists, add it\n        for license_file in self._path.glob(\"LICENSE*\"):\n            self._io.writeln(\n                \" - Adding: <comment>{}</comment>\".format(\n                    license_file.relative_to(self._path)\n                ),\n                verbosity=self._io.VERBOSITY_VERY_VERBOSE,\n            )\n            to_add.append(license_file.relative_to(self._path))\n\n        # If a README is specificed we need to include it\n        # to avoid errors\n        if \"readme\" in self._poetry.local_config:\n            readme = self._path / self._poetry.local_config[\"readme\"]\n            if readme.exists():\n                self._io.writeln(\n                    \" - Adding: <comment>{}</comment>\".format(\n                        readme.relative_to(self._path)\n                    ),\n                    verbosity=self._io.VERBOSITY_VERY_VERBOSE,\n                )\n                to_add.append(readme.relative_to(self._path))\n\n        # If a build script is specified and explicitely required\n        # we add it to the list of files\n        if self._package.build and not exclude_build:\n            to_add.append(Path(self._package.build))\n\n        return sorted(to_add)", "response": "Finds all files to add to the tarball."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse(\n    version, strict=False  # type: str  # type: bool\n):  # type:(...) -> Union[Version, LegacyVersion]\n    \"\"\"\n    Parse the given version string and return either a :class:`Version` object\n    or a LegacyVersion object depending on if the given version is\n    a valid PEP 440 version or a legacy version.\n\n    If strict=True only PEP 440 versions will be accepted.\n    \"\"\"\n    try:\n        return Version(version)\n    except InvalidVersion:\n        if strict:\n            raise\n\n        return LegacyVersion(version)", "response": "Parses the given version string and returns either a Version object or LegacyVersion object depending on if the given version string is a PEP 440 version or a LegacyVersion object depending on if the given version string is a PEP 440 version."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks whether the lock file is still up to date with the current hash.", "response": "def is_fresh(self):  # type: () -> bool\n        \"\"\"\n        Checks whether the lock file is still up to date with the current hash.\n        \"\"\"\n        lock = self._lock.read()\n        metadata = lock.get(\"metadata\", {})\n\n        if \"content-hash\" in metadata:\n            return self._content_hash == lock[\"metadata\"][\"content-hash\"]\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a repository of locked packages.", "response": "def locked_repository(\n        self, with_dev_reqs=False\n    ):  # type: (bool) -> poetry.repositories.Repository\n        \"\"\"\n        Searches and returns a repository of locked packages.\n        \"\"\"\n        if not self.is_locked():\n            return poetry.repositories.Repository()\n\n        lock_data = self.lock_data\n        packages = poetry.repositories.Repository()\n\n        if with_dev_reqs:\n            locked_packages = lock_data[\"package\"]\n        else:\n            locked_packages = [\n                p for p in lock_data[\"package\"] if p[\"category\"] == \"main\"\n            ]\n\n        if not locked_packages:\n            return packages\n\n        for info in locked_packages:\n            package = poetry.packages.Package(\n                info[\"name\"], info[\"version\"], info[\"version\"]\n            )\n            package.description = info.get(\"description\", \"\")\n            package.category = info[\"category\"]\n            package.optional = info[\"optional\"]\n            package.hashes = lock_data[\"metadata\"][\"hashes\"][info[\"name\"]]\n            package.python_versions = info[\"python-versions\"]\n\n            if \"marker\" in info:\n                package.marker = parse_marker(info[\"marker\"])\n            else:\n                # Compatibility for old locks\n                if \"requirements\" in info:\n                    dep = poetry.packages.Dependency(\"foo\", \"0.0.0\")\n                    for name, value in info[\"requirements\"].items():\n                        if name == \"python\":\n                            dep.python_versions = value\n                        elif name == \"platform\":\n                            dep.platform = value\n\n                    split_dep = dep.to_pep_508(False).split(\";\")\n                    if len(split_dep) > 1:\n                        package.marker = parse_marker(split_dep[1].strip())\n\n            for dep_name, constraint in info.get(\"dependencies\", {}).items():\n                if isinstance(constraint, list):\n                    for c in constraint:\n                        package.add_dependency(dep_name, c)\n\n                    continue\n\n                package.add_dependency(dep_name, constraint)\n\n            if \"source\" in info:\n                package.source_type = info[\"source\"][\"type\"]\n                package.source_url = info[\"source\"][\"url\"]\n                package.source_reference = info[\"source\"][\"reference\"]\n\n            packages.add_package(package)\n\n        return packages"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_content_hash(self):  # type: () -> str\n        content = self._local_config\n\n        relevant_content = {}\n        for key in self._relevant_keys:\n            relevant_content[key] = content.get(key)\n\n        content_hash = sha256(\n            json.dumps(relevant_content, sort_keys=True).encode()\n        ).hexdigest()\n\n        return content_hash", "response": "Returns the sha256 hash of the sorted content of the pyproject file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nloading installed packages. For now, it uses the pip \"freeze\" command.", "response": "def load(cls, env):  # type: (Env) -> InstalledRepository\n        \"\"\"\n        Load installed packages.\n\n        For now, it uses the pip \"freeze\" command.\n        \"\"\"\n        repo = cls()\n\n        freeze_output = env.run(\"pip\", \"freeze\")\n        for line in freeze_output.split(\"\\n\"):\n            if \"==\" in line:\n                name, version = re.split(\"={2,3}\", line)\n                repo.add_package(Package(name, version, version))\n            elif line.startswith(\"-e \"):\n                line = line[3:].strip()\n                if line.startswith(\"git+\"):\n                    url = line.lstrip(\"git+\")\n                    if \"@\" in url:\n                        url, rev = url.rsplit(\"@\", 1)\n                    else:\n                        rev = \"master\"\n\n                    name = url.split(\"/\")[-1].rstrip(\".git\")\n                    if \"#egg=\" in rev:\n                        rev, name = rev.split(\"#egg=\")\n\n                    package = Package(name, \"0.0.0\")\n                    package.source_type = \"git\"\n                    package.source_url = url\n                    package.source_reference = rev\n\n                    repo.add_package(package)\n\n        return repo"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef find_best_candidate(\n        self,\n        package_name,  # type: str\n        target_package_version=None,  # type:  Union[str, None]\n        allow_prereleases=False,  # type: bool\n    ):  # type: (...) -> Union[Package, bool]\n        \"\"\"\n        Given a package name and optional version,\n        returns the latest Package that matches\n        \"\"\"\n        if target_package_version:\n            constraint = parse_constraint(target_package_version)\n        else:\n            constraint = parse_constraint(\"*\")\n\n        candidates = self._pool.find_packages(\n            package_name, constraint, allow_prereleases=allow_prereleases\n        )\n\n        if not candidates:\n            return False\n\n        dependency = Dependency(package_name, constraint)\n\n        # Select highest version if we have many\n        package = candidates[0]\n        for candidate in candidates:\n            if candidate.is_prerelease() and not dependency.allows_prereleases():\n                continue\n\n            # Select highest version of the two\n            if package.version < candidate.version:\n                package = candidate\n\n        return package", "response": "Given a package name and optional version returns the latest Package that matches the given version."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef clean_link(self, url):\n        return self._clean_re.sub(lambda match: \"%%%2x\" % ord(match.group(0)), url)", "response": "Makes sure a link is fully encoded."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef package(\n        self, name, version, extras=None\n    ):  # type: (...) -> poetry.packages.Package\n        \"\"\"\n        Retrieve the release information.\n\n        This is a heavy task which takes time.\n        We have to download a package to get the dependencies.\n        We also need to download every file matching this release\n        to get the various hashes.\n\n        Note that, this will be cached so the subsequent operations\n        should be much faster.\n        \"\"\"\n        try:\n            index = self._packages.index(\n                poetry.packages.Package(name, version, version)\n            )\n\n            return self._packages[index]\n        except ValueError:\n            if extras is None:\n                extras = []\n\n            release_info = self.get_release_info(name, version)\n\n            package = poetry.packages.Package(name, version, version)\n            if release_info[\"requires_python\"]:\n                package.python_versions = release_info[\"requires_python\"]\n\n            package.source_type = \"legacy\"\n            package.source_url = self._url\n            package.source_reference = self.name\n\n            requires_dist = release_info[\"requires_dist\"] or []\n            for req in requires_dist:\n                try:\n                    dependency = dependency_from_pep_508(req)\n                except InvalidMarker:\n                    # Invalid marker\n                    # We strip the markers hoping for the best\n                    req = req.split(\";\")[0]\n\n                    dependency = dependency_from_pep_508(req)\n                except ValueError:\n                    # Likely unable to parse constraint so we skip it\n                    self._log(\n                        \"Invalid constraint ({}) found in {}-{} dependencies, \"\n                        \"skipping\".format(req, package.name, package.version),\n                        level=\"debug\",\n                    )\n                    continue\n\n                if dependency.in_extras:\n                    for extra in dependency.in_extras:\n                        if extra not in package.extras:\n                            package.extras[extra] = []\n\n                        package.extras[extra].append(dependency)\n\n                if not dependency.is_optional():\n                    package.requires.append(dependency)\n\n            # Adding description\n            package.description = release_info.get(\"summary\", \"\")\n\n            # Adding hashes information\n            package.hashes = release_info[\"digests\"]\n\n            # Activate extra dependencies\n            for extra in extras:\n                if extra in package.extras:\n                    for dep in package.extras[extra]:\n                        dep.activate()\n\n                    package.requires += package.extras[extra]\n\n            self._packages.append(package)\n\n            return package", "response": "Returns a new package with the given name version and optional dependencies."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nregister a new logger.", "response": "def register_logger(self, logger):\n        \"\"\"\n        Register a new logger.\n        \"\"\"\n        handler = CommandHandler(self)\n        handler.setFormatter(CommandFormatter())\n        logger.handlers = [handler]\n        logger.propagate = False\n\n        output = self.output\n        level = logging.WARNING\n        if output.is_debug():\n            level = logging.DEBUG\n        elif output.is_very_verbose() or output.is_verbose():\n            level = logging.INFO\n\n        logger.setLevel(level)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _register(self, session, url):\n        dist = self._poetry.file.parent / \"dist\"\n        file = dist / \"{}-{}.tar.gz\".format(\n            self._package.name, normalize_version(self._package.version.text)\n        )\n\n        if not file.exists():\n            raise RuntimeError('\"{0}\" does not exist.'.format(file.name))\n\n        data = self.post_data(file)\n        data.update({\":action\": \"submit\", \"protocol_version\": \"1\"})\n\n        data_to_send = self._prepare_data(data)\n        encoder = MultipartEncoder(data_to_send)\n        resp = session.post(\n            url,\n            data=encoder,\n            allow_redirects=False,\n            headers={\"Content-Type\": encoder.content_type},\n        )\n\n        resp.raise_for_status()\n\n        return resp", "response": "Register a package to a repository."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfind packages on the remote server.", "response": "def find_packages(\n        self,\n        name,  # type: str\n        constraint=None,  # type: Union[VersionConstraint, str, None]\n        extras=None,  # type: Union[list, None]\n        allow_prereleases=False,  # type: bool\n    ):  # type: (...) -> List[Package]\n        \"\"\"\n        Find packages on the remote server.\n        \"\"\"\n        if constraint is None:\n            constraint = \"*\"\n\n        if not isinstance(constraint, VersionConstraint):\n            constraint = parse_constraint(constraint)\n\n        if isinstance(constraint, VersionRange):\n            if (\n                constraint.max is not None\n                and constraint.max.is_prerelease()\n                or constraint.min is not None\n                and constraint.min.is_prerelease()\n            ):\n                allow_prereleases = True\n\n        info = self.get_package_info(name)\n\n        packages = []\n\n        for version, release in info[\"releases\"].items():\n            if not release:\n                # Bad release\n                self._log(\n                    \"No release information found for {}-{}, skipping\".format(\n                        name, version\n                    ),\n                    level=\"debug\",\n                )\n                continue\n\n            try:\n                package = Package(name, version)\n            except ParseVersionError:\n                self._log(\n                    'Unable to parse version \"{}\" for the {} package, skipping'.format(\n                        version, name\n                    ),\n                    level=\"debug\",\n                )\n                continue\n\n            if package.is_prerelease() and not allow_prereleases:\n                continue\n\n            if not constraint or (constraint and constraint.allows(package.version)):\n                if extras is not None:\n                    package.requires_extras = extras\n\n                packages.append(package)\n\n        self._log(\n            \"{} packages found for {} {}\".format(len(packages), name, str(constraint)),\n            level=\"debug\",\n        )\n\n        return packages"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the package information given its name.", "response": "def get_package_info(self, name):  # type: (str) -> dict\n        \"\"\"\n        Return the package information given its name.\n\n        The information is returned from the cache if it exists\n        or retrieved from the remote server.\n        \"\"\"\n        if self._disable_cache:\n            return self._get_package_info(name)\n\n        return self._cache.store(\"packages\").remember_forever(\n            name, lambda: self._get_package_info(name)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the release information given a package name and a version.", "response": "def get_release_info(self, name, version):  # type: (str, str) -> dict\n        \"\"\"\n        Return the release information given a package name and a version.\n\n        The information is returned from the cache if it exists\n        or retrieved from the remote server.\n        \"\"\"\n        if self._disable_cache:\n            return self._get_release_info(name, version)\n\n        cached = self._cache.remember_forever(\n            \"{}:{}\".format(name, version), lambda: self._get_release_info(name, version)\n        )\n\n        cache_version = cached.get(\"_cache_version\", \"0.0.0\")\n        if parse_constraint(cache_version) != self.CACHE_VERSION:\n            # The cache must be updated\n            self._log(\n                \"The cache for {} {} is outdated. Refreshing.\".format(name, version),\n                level=\"debug\",\n            )\n            cached = self._get_release_info(name, version)\n\n            self._cache.forever(\"{}:{}\".format(name, version), cached)\n\n        return cached"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbuilding a wheel in the dist directory optionally upload it.", "response": "def make(cls, poetry, env, io):\n        \"\"\"Build a wheel in the dist/ directory, and optionally upload it.\"\"\"\n        cls.make_in(poetry, env, io)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the win folder name from the registry.", "response": "def _get_win_folder_from_registry(csidl_name):\n    \"\"\"\n    This is a fallback technique at best. I'm not sure if using the\n    registry for this guarantees us the correct answer for all CSIDL_*\n    names.\n    \"\"\"\n    import _winreg\n\n    shell_folder_name = {\n        \"CSIDL_APPDATA\": \"AppData\",\n        \"CSIDL_COMMON_APPDATA\": \"Common AppData\",\n        \"CSIDL_LOCAL_APPDATA\": \"Local AppData\",\n    }[csidl_name]\n\n    key = _winreg.OpenKey(\n        _winreg.HKEY_CURRENT_USER,\n        r\"Software\\Microsoft\\Windows\\CurrentVersion\\Explorer\\Shell Folders\",\n    )\n    directory, _type = _winreg.QueryValueEx(key, shell_folder_name)\n    return directory"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the installer for locking only.", "response": "def lock(self):  # type: () -> Installer\n        \"\"\"\n        Prepare the installer for locking only.\n        \"\"\"\n        self.update()\n        self.execute_operations(False)\n        self._lock = True\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _execute(self, operation):  # type: (Operation) -> None\n        method = operation.job_type\n\n        getattr(self, \"_execute_{}\".format(method))(operation)", "response": "Execute a given operation."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns all packages required by extras.", "response": "def _get_extra_packages(self, repo):\n        \"\"\"\n        Returns all packages required by extras.\n\n        Maybe we just let the solver handle it?\n        \"\"\"\n        if self._update:\n            extras = {k: [d.name for d in v] for k, v in self._package.extras.items()}\n        else:\n            extras = self._locker.lock_data.get(\"extras\", {})\n\n        extra_packages = []\n        for extra_name, packages in extras.items():\n            if extra_name not in self._extras:\n                continue\n\n            extra_packages += [Dependency(p, \"*\") for p in packages]\n\n        def _extra_packages(packages):\n            pkgs = []\n            for package in packages:\n                for pkg in repo.packages:\n                    if pkg.name == package.name:\n                        pkgs.append(package)\n                        pkgs += _extra_packages(pkg.requires)\n\n                        break\n\n            return pkgs\n\n        return _extra_packages(extra_packages)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _fetch_json(self):\n        print(\"Fetching from url: \" + self.graph_url)\n        resp = urlopen(self.graph_url).read()\n        return json.loads(resp.decode('utf-8'))", "response": "Returns the json representation of the dep graph"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsearching for jobs matching the given job name prefix.", "response": "def prefix_search(self, job_name_prefix):\n        \"\"\"Searches for jobs matching the given ``job_name_prefix``.\"\"\"\n        json = self._fetch_json()\n        jobs = json['response']\n        for job in jobs:\n            if job.startswith(job_name_prefix):\n                yield self._build_results(jobs, job)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef status_search(self, status):\n        json = self._fetch_json()\n        jobs = json['response']\n        for job in jobs:\n            job_info = jobs[job]\n            if job_info['status'].lower() == status.lower():\n                yield self._build_results(jobs, job)", "response": "Searches for jobs matching the given status."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef move(self, old_path, new_path, raise_if_exists=False):\n        if raise_if_exists and os.path.exists(new_path):\n            raise FileAlreadyExists('Destination exists: %s' % new_path)\n        d = os.path.dirname(new_path)\n        if d and not os.path.exists(d):\n            self.mkdir(d)\n        try:\n            os.rename(old_path, new_path)\n        except OSError as err:\n            if err.errno == errno.EXDEV:\n                new_path_tmp = '%s-%09d' % (new_path, random.randint(0, 999999999))\n                shutil.copy(old_path, new_path_tmp)\n                os.rename(new_path_tmp, new_path)\n                os.remove(old_path)\n            else:\n                raise err", "response": "Move file atomically. If source and destination are located\n        on different filesystems, atomicity is approximated\n        but cannot be guaranteed."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rename_dont_move(self, path, dest):\n        self.move(path, dest, raise_if_exists=True)", "response": "Rename path to dest but don t move it into the dest folder."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating all parent folders if they do not exist.", "response": "def makedirs(self):\n        \"\"\"\n        Create all parent folders if they do not exist.\n        \"\"\"\n        normpath = os.path.normpath(self.path)\n        parentfolder = os.path.dirname(normpath)\n        if parentfolder:\n            try:\n                os.makedirs(parentfolder)\n            except OSError:\n                pass"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntrack the status of a job in the current node.", "response": "def track_job(job_id):\n    \"\"\"\n    Tracking is done by requesting each job and then searching for whether the job\n    has one of the following states:\n    - \"RUN\",\n    - \"PEND\",\n    - \"SSUSP\",\n    - \"EXIT\"\n    based on the LSF documentation\n    \"\"\"\n    cmd = \"bjobs -noheader -o stat {}\".format(job_id)\n    track_job_proc = subprocess.Popen(\n        cmd, stdout=subprocess.PIPE, shell=True)\n    status = track_job_proc.communicate()[0].strip('\\n')\n    return status"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fetch_task_failures(self):\n        error_file = os.path.join(self.tmp_dir, \"job.err\")\n        if os.path.isfile(error_file):\n            with open(error_file, \"r\") as f_err:\n                errors = f_err.readlines()\n        else:\n            errors = ''\n        return errors", "response": "Read in the error file from bsub\n        Read in the error file from bsub\n       "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fetch_task_output(self):\n        # Read in the output file\n        if os.path.isfile(os.path.join(self.tmp_dir, \"job.out\")):\n            with open(os.path.join(self.tmp_dir, \"job.out\"), \"r\") as f_out:\n                outputs = f_out.readlines()\n        else:\n            outputs = ''\n        return outputs", "response": "Read in the output file and return the list of the task outputs"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nruns the job and return the result.", "response": "def _run_job(self):\n        \"\"\"\n        Build a bsub argument that will run lsf_runner.py on the directory we've specified.\n        \"\"\"\n\n        args = []\n\n        if isinstance(self.output(), list):\n            log_output = os.path.split(self.output()[0].path)\n        else:\n            log_output = os.path.split(self.output().path)\n\n        args += [\"bsub\", \"-q\", self.queue_flag]\n        args += [\"-n\", str(self.n_cpu_flag)]\n        args += [\"-M\", str(self.memory_flag)]\n        args += [\"-R\", \"rusage[%s]\" % self.resource_flag]\n        args += [\"-W\", str(self.runtime_flag)]\n        if self.job_name_flag:\n            args += [\"-J\", str(self.job_name_flag)]\n        args += [\"-o\", os.path.join(log_output[0], \"job.out\")]\n        args += [\"-e\", os.path.join(log_output[0], \"job.err\")]\n        if self.extra_bsub_args:\n            args += self.extra_bsub_args.split()\n\n        # Find where the runner file is\n        runner_path = os.path.abspath(lsf_runner.__file__)\n\n        args += [runner_path]\n        args += [self.tmp_dir]\n\n        # That should do it. Let the world know what we're doing.\n        LOGGER.info(\"### LSF SUBMISSION ARGS: %s\",\n                    \" \".join([str(a) for a in args]))\n\n        # Submit the job\n        run_job_proc = subprocess.Popen(\n            [str(a) for a in args],\n            stdin=subprocess.PIPE, stdout=subprocess.PIPE, cwd=self.tmp_dir)\n        output = run_job_proc.communicate()[0]\n\n        # ASSUMPTION\n        # The result will be of the format\n        # Job <123> is submitted ot queue <myqueue>\n        # So get the number in those first brackets.\n        # I cannot think of a better workaround that leaves logic on the Task side of things.\n        LOGGER.info(\"### JOB SUBMISSION OUTPUT: %s\", str(output))\n        self.job_id = int(output.split(\"<\")[1].split(\">\")[0])\n        LOGGER.info(\n            \"Job %ssubmitted as job %s\",\n            self.job_name_flag + ' ',\n            str(self.job_id)\n        )\n\n        self._track_job()\n\n        # If we want to save the job temporaries, then do so\n        # We'll move them to be next to the job output\n        if self.save_job_info:\n            LOGGER.info(\"Saving up temporary bits\")\n\n            # dest_dir = self.output().path\n            shutil.move(self.tmp_dir, \"/\".join(log_output[0:-1]))\n\n        # Now delete the temporaries, if they're there.\n        self._finish()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating the data in the target format.", "response": "def run(self):\n        \"\"\"\n        Generates :py:attr:`~.UserItemMatrix.data_size` elements.\n        Writes this data in \\\\ separated value format into the target :py:func:`~/.UserItemMatrix.output`.\n\n        The data has the following elements:\n\n        * `user` is the default Elasticsearch id field,\n        * `track`: the text,\n        * `rating`: the day when the data was created.\n\n        \"\"\"\n        w = self.output().open('w')\n        for user in range(self.data_size):\n            track = int(random.random() * self.data_size)\n            w.write('%d\\\\%d\\\\%f' % (user, track, 1.0))\n        w.close()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef output(self):\n        return luigi.contrib.hdfs.HdfsTarget('data-matrix', format=luigi.format.Gzip)", "response": "Returns the target output for this task."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting UTC time string to time. struct_time return datetime. struct_time type", "response": "def from_utc(utcTime, fmt=None):\n    \"\"\"convert UTC time string to time.struct_time: change datetime.datetime to time, return time.struct_time type\"\"\"\n    if fmt is None:\n        try_formats = [\"%Y-%m-%d %H:%M:%S.%f\", \"%Y-%m-%d %H:%M:%S\"]\n    else:\n        try_formats = [fmt]\n\n    for fmt in try_formats:\n        try:\n            time_struct = datetime.datetime.strptime(utcTime, fmt)\n        except ValueError:\n            pass\n        else:\n            date = int(time.mktime(time_struct.timetuple()))\n            return date\n    else:\n        raise ValueError(\"No UTC format matches {}\".format(utcTime))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef run(api_port=8082, address=None, unix_socket=None, scheduler=None):\n    if scheduler is None:\n        scheduler = Scheduler()\n\n    # load scheduler state\n    scheduler.load()\n\n    _init_api(\n        scheduler=scheduler,\n        api_port=api_port,\n        address=address,\n        unix_socket=unix_socket,\n    )\n\n    # prune work DAG every 60 seconds\n    pruner = tornado.ioloop.PeriodicCallback(scheduler.prune, 60000)\n    pruner.start()\n\n    def shutdown_handler(signum, frame):\n        exit_handler()\n        sys.exit(0)\n\n    @atexit.register\n    def exit_handler():\n        logger.info(\"Scheduler instance shutting down\")\n        scheduler.dump()\n        stop()\n\n    signal.signal(signal.SIGINT, shutdown_handler)\n    signal.signal(signal.SIGTERM, shutdown_handler)\n    if os.name == 'nt':\n        signal.signal(signal.SIGBREAK, shutdown_handler)\n    else:\n        signal.signal(signal.SIGQUIT, shutdown_handler)\n\n    logger.info(\"Scheduler starting up\")\n\n    tornado.ioloop.IOLoop.instance().start()", "response": "Runs one instance of the API server."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget queried columns names.", "response": "def get_soql_fields(soql):\n    \"\"\"\n    Gets queried columns names.\n    \"\"\"\n    soql_fields = re.search('(?<=select)(?s)(.*)(?=from)', soql, re.IGNORECASE)     # get fields\n    soql_fields = re.sub(' ', '', soql_fields.group())                              # remove extra spaces\n    soql_fields = re.sub('\\t', '', soql_fields)                                     # remove tabs\n    fields = re.split(',|\\n|\\r|', soql_fields)                                      # split on commas and newlines\n    fields = [field for field in fields if field != '']                             # remove empty strings\n    return fields"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse the results of the get_record_cache_list_of_objects_in_dict call into the dictionary depth of data", "response": "def parse_results(fields, data):\n    \"\"\"\n    Traverses ordered dictionary, calls _traverse_results() to recursively read into the dictionary depth of data\n    \"\"\"\n    master = []\n\n    for record in data['records']:  # for each 'record' in response\n        row = [None] * len(fields)  # create null list the length of number of columns\n        for obj, value in record.iteritems():  # for each obj in record\n            if not isinstance(value, (dict, list, tuple)):  # if not data structure\n                if obj in fields:\n                    row[fields.index(obj)] = ensure_utf(value)\n\n            elif isinstance(value, dict) and obj != 'attributes':  # traverse down into object\n                path = obj\n                _traverse_results(value, fields, row, path)\n\n        master.append(row)\n    return master"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _traverse_results(value, fields, row, path):\n    for f, v in value.iteritems():  # for each item in obj\n        field_name = '{path}.{name}'.format(path=path, name=f) if path else f\n\n        if not isinstance(v, (dict, list, tuple)):  # if not data structure\n            if field_name in fields:\n                row[fields.index(field_name)] = ensure_utf(v)\n\n        elif isinstance(v, dict) and f != 'attributes':  # it is a dict\n            _traverse_results(v, fields, row, field_name)", "response": "Traverse through ordered dict and recursively calls _parse_results."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef merge_batch_results(self, result_ids):\n        outfile = open(self.output().path, 'w')\n\n        if self.content_type.lower() == 'csv':\n            for i, result_id in enumerate(result_ids):\n                with open(\"%s.%d\" % (self.output().path, i), 'r') as f:\n                    header = f.readline()\n                    if i == 0:\n                        outfile.write(header)\n                    for line in f:\n                        outfile.write(line)\n        else:\n            raise Exception(\"Batch result merging not implemented for %s\" % self.content_type)\n\n        outfile.close()", "response": "Merges the results of a multi - result batch bulk query into a single - result batch bulk query."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef start_session(self):\n        if self.has_active_session():\n            raise Exception(\"Session already in progress.\")\n\n        response = requests.post(self._get_login_url(),\n                                 headers=self._get_login_headers(),\n                                 data=self._get_login_xml())\n        response.raise_for_status()\n\n        root = ET.fromstring(response.text)\n        for e in root.iter(\"%ssessionId\" % self.SOAP_NS):\n            if self.session_id:\n                raise Exception(\"Invalid login attempt.  Multiple session ids found.\")\n            self.session_id = e.text\n\n        for e in root.iter(\"%sserverUrl\" % self.SOAP_NS):\n            if self.server_url:\n                raise Exception(\"Invalid login attempt.  Multiple server urls found.\")\n            self.server_url = e.text\n\n        if not self.has_active_session():\n            raise Exception(\"Invalid login attempt resulted in null sessionId [%s] and/or serverUrl [%s].\" %\n                            (self.session_id, self.server_url))\n        self.hostname = urlsplit(self.server_url).hostname", "response": "Starts a Salesforce session and determines which SF instance to use for future requests."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nqueries the Salesforce API for the specified set of local resources.", "response": "def query(self, query, **kwargs):\n        \"\"\"\n        Return the result of a Salesforce SOQL query as a dict decoded from the Salesforce response JSON payload.\n\n        :param query: the SOQL query to send to Salesforce, e.g. \"SELECT id from Lead WHERE email = 'a@b.com'\"\n        \"\"\"\n        params = {'q': query}\n        response = requests.get(self._get_norm_query_url(),\n                                headers=self._get_rest_headers(),\n                                params=params,\n                                **kwargs)\n        if response.status_code != requests.codes.ok:\n            raise Exception(response.content)\n\n        return response.json()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef query_more(self, next_records_identifier, identifier_is_url=False, **kwargs):\n        if identifier_is_url:\n            # Don't use `self.base_url` here because the full URI is provided\n            url = (u'https://{instance}{next_record_url}'\n                   .format(instance=self.hostname,\n                           next_record_url=next_records_identifier))\n        else:\n            url = self._get_norm_query_url() + '{next_record_id}'\n            url = url.format(next_record_id=next_records_identifier)\n        response = requests.get(url, headers=self._get_rest_headers(), **kwargs)\n\n        response.raise_for_status()\n\n        return response.json()", "response": "Query the Salesforce server for more results."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef query_all(self, query, **kwargs):\n        # Make the initial query to Salesforce\n        response = self.query(query, **kwargs)\n\n        # get fields\n        fields = get_soql_fields(query)\n\n        # put fields and first page of results into a temp list to be written to TempFile\n        tmp_list = [fields]\n        tmp_list.extend(parse_results(fields, response))\n\n        tmp_dir = luigi.configuration.get_config().get('salesforce', 'local-tmp-dir', None)\n        tmp_file = tempfile.TemporaryFile(mode='a+b', dir=tmp_dir)\n\n        writer = csv.writer(tmp_file)\n        writer.writerows(tmp_list)\n\n        # The number of results might have exceeded the Salesforce batch limit\n        # so check whether there are more results and retrieve them if so.\n\n        length = len(response['records'])\n        while not response['done']:\n            response = self.query_more(response['nextRecordsUrl'], identifier_is_url=True, **kwargs)\n\n            writer.writerows(parse_results(fields, response))\n            length += len(response['records'])\n            if not length % 10000:\n                logger.info('Requested {0} lines...'.format(length))\n\n        logger.info('Requested a total of {0} lines.'.format(length))\n\n        tmp_file.seek(0)\n        return tmp_file", "response": "Query the Salesforce server for all entries."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nallow you to make a direct REST call to the path", "response": "def restful(self, path, params):\n        \"\"\"\n        Allows you to make a direct REST call if you know the path\n        Arguments:\n        :param path: The path of the request. Example: sobjects/User/ABC123/password'\n        :param params: dict of parameters to pass to the path\n        \"\"\"\n\n        url = self._get_norm_base_url() + path\n        response = requests.get(url, headers=self._get_rest_headers(), params=params)\n\n        if response.status_code != 200:\n            raise Exception(response)\n        json_result = response.json(object_pairs_hook=OrderedDict)\n        if len(json_result) == 0:\n            return None\n        else:\n            return json_result"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_operation_job(self, operation, obj, external_id_field_name=None, content_type=None):\n        if not self.has_active_session():\n            self.start_session()\n\n        response = requests.post(self._get_create_job_url(),\n                                 headers=self._get_create_job_headers(),\n                                 data=self._get_create_job_xml(operation, obj, external_id_field_name, content_type))\n        response.raise_for_status()\n\n        root = ET.fromstring(response.text)\n        job_id = root.find('%sid' % self.API_NS).text\n        return job_id", "response": "Creates a new SF job that for doing any operation."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_job_details(self, job_id):\n        response = requests.get(self._get_job_details_url(job_id))\n\n        response.raise_for_status()\n\n        return response", "response": "Gets all details for an existing job"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef abort_job(self, job_id):\n        response = requests.post(self._get_abort_job_url(job_id),\n                                 headers=self._get_abort_job_headers(),\n                                 data=self._get_abort_job_xml())\n        response.raise_for_status()\n\n        return response", "response": "Abort an existing job."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef close_job(self, job_id):\n        if not job_id or not self.has_active_session():\n            raise Exception(\"Can not close job without valid job_id and an active session.\")\n\n        response = requests.post(self._get_close_job_url(job_id),\n                                 headers=self._get_close_job_headers(),\n                                 data=self._get_close_job_xml())\n        response.raise_for_status()\n\n        return response", "response": "Closes a job with the given id."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_batch(self, job_id, data, file_type):\n        if not job_id or not self.has_active_session():\n            raise Exception(\"Can not create a batch without a valid job_id and an active session.\")\n\n        headers = self._get_create_batch_content_headers(file_type)\n        headers['Content-Length'] = str(len(data))\n\n        response = requests.post(self._get_create_batch_url(job_id),\n                                 headers=headers,\n                                 data=data)\n        response.raise_for_status()\n\n        root = ET.fromstring(response.text)\n        batch_id = root.find('%sid' % self.API_NS).text\n        return batch_id", "response": "Creates a batch with either a string of data or a file containing data."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nblocking until the batch is complete or failed.", "response": "def block_on_batch(self, job_id, batch_id, sleep_time_seconds=5, max_wait_time_seconds=-1):\n        \"\"\"\n        Blocks until @batch_id is completed or failed.\n        :param job_id:\n        :param batch_id:\n        :param sleep_time_seconds:\n        :param max_wait_time_seconds:\n        \"\"\"\n        if not job_id or not batch_id or not self.has_active_session():\n            raise Exception(\"Can not block on a batch without a valid batch_id, job_id and an active session.\")\n\n        start_time = time.time()\n        status = {}\n        while max_wait_time_seconds < 0 or time.time() - start_time < max_wait_time_seconds:\n            status = self._get_batch_info(job_id, batch_id)\n            logger.info(\"Batch %s Job %s in state %s.  %s records processed.  %s records failed.\" %\n                        (batch_id, job_id, status['state'], status['num_processed'], status['num_failed']))\n            if status['state'].lower() in [\"completed\", \"failed\"]:\n                return status\n            time.sleep(sleep_time_seconds)\n\n        raise Exception(\"Batch did not complete in %s seconds.  Final status was: %s\" % (sleep_time_seconds, status))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the batch result for a given job and batch id.", "response": "def get_batch_results(self, job_id, batch_id):\n        \"\"\"\n        DEPRECATED: Use `get_batch_result_ids`\n        \"\"\"\n        warnings.warn(\"get_batch_results is deprecated and only returns one batch result. Please use get_batch_result_ids\")\n        return self.get_batch_result_ids(job_id, batch_id)[0]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget result IDs of a batch that has completed processing.", "response": "def get_batch_result_ids(self, job_id, batch_id):\n        \"\"\"\n        Get result IDs of a batch that has completed processing.\n\n        :param job_id: job_id as returned by 'create_operation_job(...)'\n        :param batch_id: batch_id as returned by 'create_batch(...)'\n        :return: list of batch result IDs to be used in 'get_batch_result(...)'\n        \"\"\"\n        response = requests.get(self._get_batch_results_url(job_id, batch_id),\n                                headers=self._get_batch_info_headers())\n        response.raise_for_status()\n\n        root = ET.fromstring(response.text)\n        result_ids = [r.text for r in root.findall('%sresult' % self.API_NS)]\n\n        return result_ids"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the result from Salesforce as whatever type was originally sent in create_batch ( xml or csv.", "response": "def get_batch_result(self, job_id, batch_id, result_id):\n        \"\"\"\n        Gets result back from Salesforce as whatever type was originally sent in create_batch (xml, or csv).\n        :param job_id:\n        :param batch_id:\n        :param result_id:\n\n        \"\"\"\n        response = requests.get(self._get_batch_result_url(job_id, batch_id, result_id),\n                                headers=self._get_session_headers())\n        response.raise_for_status()\n\n        return response.content"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing the state column from qstat output for given job_id. Returns u if the first job matching job_id is not found. Returns u if the qstat output is empty.", "response": "def _parse_qstat_state(qstat_out, job_id):\n    \"\"\"Parse \"state\" column from `qstat` output for given job_id\n\n    Returns state for the *first* job matching job_id. Returns 'u' if\n    `qstat` output is empty or job_id is not found.\n\n    \"\"\"\n    if qstat_out.strip() == '':\n        return 'u'\n    lines = qstat_out.split('\\n')\n    # skip past header\n    while not lines.pop(0).startswith('---'):\n        pass\n    for line in lines:\n        if line:\n            job, prior, name, user, state = line.strip().split()[0:5]\n            if int(job) == int(job_id):\n                return state\n    return 'u'"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nbuilding command to submit to SGE queue via qsub", "response": "def _build_qsub_command(cmd, job_name, outfile, errfile, pe, n_cpu):\n    \"\"\"Submit shell command to SGE queue via `qsub`\"\"\"\n    qsub_template = \"\"\"echo {cmd} | qsub -o \":{outfile}\" -e \":{errfile}\" -V -r y -pe {pe} {n_cpu} -N {job_name}\"\"\"\n    return qsub_template.format(\n        cmd=cmd, job_name=job_name, outfile=outfile, errfile=errfile,\n        pe=pe, n_cpu=n_cpu)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _dump(self, out_dir=''):\n        with self.no_unpicklable_properties():\n            self.job_file = os.path.join(out_dir, 'job-instance.pickle')\n            if self.__module__ == '__main__':\n                d = pickle.dumps(self)\n                module_name = os.path.basename(sys.argv[0]).rsplit('.', 1)[0]\n                d = d.replace('(c__main__', \"(c\" + module_name)\n                open(self.job_file, \"w\").write(d)\n            else:\n                pickle.dump(self, open(self.job_file, \"w\"))", "response": "Dump instance to file."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwrites data in JSON format into the task s output target.", "response": "def run(self):\n        \"\"\"\n        Writes data in JSON format into the task's output target.\n\n        The data objects have the following attributes:\n\n        * `_id` is the default Elasticsearch id field,\n        * `text`: the text,\n        * `date`: the day when the data was created.\n\n        \"\"\"\n        today = datetime.date.today()\n        with self.output().open('w') as output:\n            for i in range(5):\n                output.write(json.dumps({'_id': i, 'text': 'Hi %s' % i,\n                                         'date': str(today)}))\n                output.write('\\n')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_autoconfig_client(client_cache=_AUTOCONFIG_CLIENT):\n    try:\n        return client_cache.client\n    except AttributeError:\n        configured_client = hdfs_config.get_configured_hdfs_client()\n        if configured_client == \"webhdfs\":\n            client_cache.client = hdfs_webhdfs_client.WebHdfsClient()\n        elif configured_client == \"snakebite\":\n            client_cache.client = hdfs_snakebite_client.SnakebiteHdfsClient()\n        elif configured_client == \"snakebite_with_hadoopcli_fallback\":\n            client_cache.client = luigi.contrib.target.CascadingClient([\n                hdfs_snakebite_client.SnakebiteHdfsClient(),\n                hdfs_hadoopcli_clients.create_hadoopcli_client(),\n            ])\n        elif configured_client == \"hadoopcli\":\n            client_cache.client = hdfs_hadoopcli_clients.create_hadoopcli_client()\n        else:\n            raise Exception(\"Unknown hdfs client \" + configured_client)\n        return client_cache.client", "response": "Creates the client based on the configuration."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef wrap_traceback(traceback):\n    if email().format == 'html':\n        try:\n            from pygments import highlight\n            from pygments.lexers import PythonTracebackLexer\n            from pygments.formatters import HtmlFormatter\n            with_pygments = True\n        except ImportError:\n            with_pygments = False\n\n        if with_pygments:\n            formatter = HtmlFormatter(noclasses=True)\n            wrapped = highlight(traceback, PythonTracebackLexer(), formatter)\n        else:\n            wrapped = '<pre>%s</pre>' % traceback\n    else:\n        wrapped = traceback\n\n    return wrapped", "response": "Wraps a traceback into a single HTML block."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsend an email to AWS SES.", "response": "def send_email_ses(sender, subject, message, recipients, image_png):\n    \"\"\"\n    Sends notification through AWS SES.\n\n    Does not handle access keys.  Use either\n      1/ configuration file\n      2/ EC2 instance profile\n\n    See also https://boto3.readthedocs.io/en/latest/guide/configuration.html.\n    \"\"\"\n    from boto3 import client as boto3_client\n\n    client = boto3_client('ses')\n\n    msg_root = generate_email(sender, subject, message, recipients, image_png)\n    response = client.send_raw_email(Source=sender,\n                                     Destinations=recipients,\n                                     RawMessage={'Data': msg_root.as_string()})\n\n    logger.debug((\"Message sent to SES.\\nMessageId: {},\\nRequestId: {},\\n\"\n                 \"HTTPSStatusCode: {}\").format(response['MessageId'],\n                                               response['ResponseMetadata']['RequestId'],\n                                               response['ResponseMetadata']['HTTPStatusCode']))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsends an email to SNS.", "response": "def send_email_sns(sender, subject, message, topic_ARN, image_png):\n    \"\"\"\n    Sends notification through AWS SNS. Takes Topic ARN from recipients.\n\n    Does not handle access keys.  Use either\n      1/ configuration file\n      2/ EC2 instance profile\n\n    See also https://boto3.readthedocs.io/en/latest/guide/configuration.html.\n    \"\"\"\n    from boto3 import resource as boto3_resource\n\n    sns = boto3_resource('sns')\n    topic = sns.Topic(topic_ARN[0])\n\n    # Subject is max 100 chars\n    if len(subject) > 100:\n        subject = subject[0:48] + '...' + subject[-49:]\n\n    response = topic.publish(Subject=subject, Message=message)\n\n    logger.debug((\"Message sent to SNS.\\nMessageId: {},\\nRequestId: {},\\n\"\n                 \"HTTPSStatusCode: {}\").format(response['MessageId'],\n                                               response['ResponseMetadata']['RequestId'],\n                                               response['ResponseMetadata']['HTTPStatusCode']))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsending an email to the specified recipients.", "response": "def send_email(subject, message, sender, recipients, image_png=None):\n    \"\"\"\n    Decides whether to send notification. Notification is cancelled if there are\n    no recipients or if stdout is onto tty or if in debug mode.\n\n    Dispatches on config value email.method.  Default is 'smtp'.\n    \"\"\"\n    notifiers = {\n        'ses': send_email_ses,\n        'sendgrid': send_email_sendgrid,\n        'smtp': send_email_smtp,\n        'sns': send_email_sns,\n    }\n\n    subject = _prefix(subject)\n    if not recipients or recipients == (None,):\n        return\n\n    if _email_disabled_reason():\n        logger.info(\"Not sending email to %r because %s\",\n                    recipients, _email_disabled_reason())\n        return\n\n    # Clean the recipients lists to allow multiple email addresses, comma\n    # separated in luigi.cfg\n    recipients_tmp = []\n    for r in recipients:\n        recipients_tmp.extend([a.strip() for a in r.split(',') if a.strip()])\n\n    # Replace original recipients with the clean list\n    recipients = recipients_tmp\n\n    logger.info(\"Sending email to %r\", recipients)\n\n    # Get appropriate sender and call it to send the notification\n    email_sender = notifiers[email().method]\n    email_sender(sender, subject, message, recipients, image_png)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef send_error_email(subject, message, additional_recipients=None):\n    recipients = _email_recipients(additional_recipients)\n    sender = email().sender\n    send_email(\n        subject=subject,\n        message=message,\n        sender=sender,\n        recipients=recipients\n    )", "response": "Sends an email to the configured error email."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nformat a message body for an error email related to a luigi. task. Task object", "response": "def format_task_error(headline, task, command, formatted_exception=None):\n    \"\"\"\n    Format a message body for an error email related to a luigi.task.Task\n\n    :param headline: Summary line for the message\n    :param task: `luigi.task.Task` instance where this error occurred\n    :param formatted_exception: optional string showing traceback\n\n    :return: message body\n    \"\"\"\n\n    if formatted_exception:\n        formatted_exception = wrap_traceback(formatted_exception)\n    else:\n        formatted_exception = \"\"\n\n    if email().format == 'html':\n        msg_template = textwrap.dedent('''\n        <html>\n        <body>\n        <h2>{headline}</h2>\n\n        <table style=\"border-top: 1px solid black; border-bottom: 1px solid black\">\n        <thead>\n        <tr><th>name</th><td>{name}</td></tr>\n        </thead>\n        <tbody>\n        {param_rows}\n        </tbody>\n        </table>\n        </pre>\n\n        <h2>Command line</h2>\n        <pre>\n        {command}\n        </pre>\n\n        <h2>Traceback</h2>\n        {traceback}\n        </body>\n        </html>\n        ''')\n\n        str_params = task.to_str_params()\n        params = '\\n'.join('<tr><th>{}</th><td>{}</td></tr>'.format(*items) for items in str_params.items())\n        body = msg_template.format(headline=headline, name=task.task_family, param_rows=params,\n                                   command=command, traceback=formatted_exception)\n    else:\n        msg_template = textwrap.dedent('''\\\n        {headline}\n\n        Name: {name}\n\n        Parameters:\n        {params}\n\n        Command line:\n          {command}\n\n        {traceback}\n        ''')\n\n        str_params = task.to_str_params()\n        max_width = max([0] + [len(x) for x in str_params.keys()])\n        params = '\\n'.join('  {:{width}}: {}'.format(*items, width=max_width) for items in str_params.items())\n        body = msg_template.format(headline=headline, name=task.task_family, params=params,\n                                   command=command, traceback=formatted_exception)\n\n    return body"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef exists(self, path):\n        import hdfs\n        try:\n            self.client.status(path)\n            return True\n        except hdfs.util.HdfsError as e:\n            if str(e).startswith('File does not exist: '):\n                return False\n            else:\n                raise e", "response": "Returns true if the path exists and false otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef mkdir(self, path, parents=True, mode=0o755, raise_if_exists=False):\n        if not parents or raise_if_exists:\n            warnings.warn('webhdfs mkdir: parents/raise_if_exists not implemented')\n        permission = int(oct(mode)[2:])  # Convert from int(decimal) to int(octal)\n        self.client.makedirs(path, permission=permission)", "response": "Create a directory in the HDFS."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nopens the target for reading or writing.", "response": "def open(self, mode):\n        \"\"\"\n        Open the target for reading or writing\n\n        :param char mode:\n            'r' for reading and 'w' for writing.\n\n            'b' is not supported and will be stripped if used. For binary mode, use `format`\n        :return:\n            * :class:`.ReadableAzureBlobFile` if 'r'\n            * :class:`.AtomicAzureBlobFile` if 'w'\n        \"\"\"\n        if mode not in ('r', 'w'):\n            raise ValueError(\"Unsupported open mode '%s'\" % mode)\n        if mode == 'r':\n            return self.format.pipe_reader(ReadableAzureBlobFile(self.container, self.blob, self.client, self.download_when_reading, **self.azure_blob_options))\n        else:\n            return self.format.pipe_writer(AtomicAzureBlobFile(self.container, self.blob, self.client, **self.azure_blob_options))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run(self):\n        with self.output().open('w') as output:\n            for _ in range(1000):\n                output.write('{} {} {}\\n'.format(\n                    random.randint(0, 999),\n                    random.randint(0, 999),\n                    random.randint(0, 999)))", "response": "Generates bogus data and writes it into the target."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef requires(self):\n        if self.use_spark:\n            return AggregateArtistsSpark(self.date_interval)\n        else:\n            return AggregateArtists(self.date_interval)", "response": "This task s dependencies."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_path(self):\n        md5_hash = hashlib.md5(self.task_id.encode()).hexdigest()\n        logger.debug('Hash %s corresponds to task %s', md5_hash, self.task_id)\n\n        return os.path.join(self.temp_dir, str(self.unique.value), md5_hash)", "response": "Returns a temporary file path based on the MD5 hash generated with the task s name and its arguments"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a temporary file to mark the task as done", "response": "def done(self):\n        \"\"\"\n        Creates temporary file to mark the task as `done`\n        \"\"\"\n        logger.info('Marking %s as done', self)\n\n        fn = self.get_path()\n        try:\n            os.makedirs(os.path.dirname(fn))\n        except OSError:\n            pass\n        open(fn, 'w').close()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef exists(self, path):\n        (bucket, key) = self._path_to_bucket_and_key(path)\n\n        # root always exists\n        if self._is_root(key):\n            return True\n\n        # file\n        if self._exists(bucket, key):\n            return True\n\n        if self.isdir(path):\n            return True\n\n        logger.debug('Path %s does not exist', path)\n        return False", "response": "Does provided path exist on S3?"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nremove a file or directory from S3.", "response": "def remove(self, path, recursive=True):\n        \"\"\"\n        Remove a file or directory from S3.\n        :param path: File or directory to remove\n        :param recursive: Boolean indicator to remove object and children\n        :return: Boolean indicator denoting success of the removal of 1 or more files\n        \"\"\"\n        if not self.exists(path):\n            logger.debug('Could not delete %s; path does not exist', path)\n            return False\n\n        (bucket, key) = self._path_to_bucket_and_key(path)\n        s3_bucket = self.s3.Bucket(bucket)\n        # root\n        if self._is_root(key):\n            raise InvalidDeleteException('Cannot delete root of bucket at path %s' % path)\n\n        # file\n        if self._exists(bucket, key):\n            self.s3.meta.client.delete_object(Bucket=bucket, Key=key)\n            logger.debug('Deleting %s from bucket %s', key, bucket)\n            return True\n\n        if self.isdir(path) and not recursive:\n            raise InvalidDeleteException('Path %s is a directory. Must use recursive delete' % path)\n\n        delete_key_list = [{'Key': obj.key} for obj in s3_bucket.objects.filter(Prefix=self._add_path_delimiter(key))]\n\n        # delete the directory marker file if it exists\n        if self._exists(bucket, '{}{}'.format(key, S3_DIRECTORY_MARKER_SUFFIX_0)):\n            delete_key_list.append({'Key': '{}{}'.format(key, S3_DIRECTORY_MARKER_SUFFIX_0)})\n\n        if len(delete_key_list) > 0:\n            n = 1000\n            for i in range(0, len(delete_key_list), n):\n                self.s3.meta.client.delete_objects(Bucket=bucket, Delete={'Objects': delete_key_list[i: i + n]})\n            return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the object summary at the path", "response": "def get_key(self, path):\n        \"\"\"\n        Returns the object summary at the path\n        \"\"\"\n        (bucket, key) = self._path_to_bucket_and_key(path)\n\n        if self._exists(bucket, key):\n            return self.s3.ObjectSummary(bucket, key)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nputting an object stored locally to an S3 path.", "response": "def put(self, local_path, destination_s3_path, **kwargs):\n        \"\"\"\n        Put an object stored locally to an S3 path.\n        :param local_path: Path to source local file\n        :param destination_s3_path: URL for target S3 location\n        :param kwargs: Keyword arguments are passed to the boto function `put_object`\n        \"\"\"\n        self._check_deprecated_argument(**kwargs)\n\n        # put the file\n        self.put_multipart(local_path, destination_s3_path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nputting a string to an S3 path.", "response": "def put_string(self, content, destination_s3_path, **kwargs):\n        \"\"\"\n        Put a string to an S3 path.\n        :param content: Data str\n        :param destination_s3_path: URL for target S3 location\n        :param kwargs: Keyword arguments are passed to the boto3 function `put_object`\n        \"\"\"\n        self._check_deprecated_argument(**kwargs)\n        (bucket, key) = self._path_to_bucket_and_key(destination_s3_path)\n\n        # put the file\n        self.s3.meta.client.put_object(\n            Key=key, Bucket=bucket, Body=content, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nputting an object stored locally to an S3 path and upload it to an S3 path.", "response": "def put_multipart(self, local_path, destination_s3_path, part_size=DEFAULT_PART_SIZE, **kwargs):\n        \"\"\"\n        Put an object stored locally to an S3 path\n        using S3 multi-part upload (for files > 8Mb).\n        :param local_path: Path to source local file\n        :param destination_s3_path: URL for target S3 location\n        :param part_size: Part size in bytes. Default: 8388608 (8MB)\n        :param kwargs: Keyword arguments are passed to the boto function `upload_fileobj` as ExtraArgs\n        \"\"\"\n        self._check_deprecated_argument(**kwargs)\n\n        from boto3.s3.transfer import TransferConfig\n        # default part size for boto3 is 8Mb, changing it to fit part_size\n        # provided as a parameter\n        transfer_config = TransferConfig(multipart_chunksize=part_size)\n\n        (bucket, key) = self._path_to_bucket_and_key(destination_s3_path)\n\n        self.s3.meta.client.upload_fileobj(\n            Fileobj=open(local_path, 'rb'), Bucket=bucket, Key=key, Config=transfer_config, ExtraArgs=kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncopy object from one S3 location to another.", "response": "def copy(self, source_path, destination_path, threads=DEFAULT_THREADS, start_time=None, end_time=None,\n             part_size=DEFAULT_PART_SIZE, **kwargs):\n        \"\"\"\n        Copy object(s) from one S3 location to another. Works for individual keys or entire directories.\n        When files are larger than `part_size`, multipart uploading will be used.\n        :param source_path: The `s3://` path of the directory or key to copy from\n        :param destination_path: The `s3://` path of the directory or key to copy to\n        :param threads: Optional argument to define the number of threads to use when copying (min: 3 threads)\n        :param start_time: Optional argument to copy files with modified dates after start_time\n        :param end_time: Optional argument to copy files with modified dates before end_time\n        :param part_size: Part size in bytes\n        :param kwargs: Keyword arguments are passed to the boto function `copy` as ExtraArgs\n        :returns tuple (number_of_files_copied, total_size_copied_in_bytes)\n        \"\"\"\n\n        # don't allow threads to be less than 3\n        threads = 3 if threads < 3 else threads\n\n        if self.isdir(source_path):\n            return self._copy_dir(source_path, destination_path, threads=threads,\n                                  start_time=start_time, end_time=end_time, part_size=part_size, **kwargs)\n\n        # If the file isn't a directory just perform a simple copy\n        else:\n            return self._copy_file(source_path, destination_path, threads=threads, part_size=part_size, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting an object stored in S3 and write it to a local path.", "response": "def get(self, s3_path, destination_local_path):\n        \"\"\"\n        Get an object stored in S3 and write it to a local path.\n        \"\"\"\n        (bucket, key) = self._path_to_bucket_and_key(s3_path)\n        # download the file\n        self.s3.meta.client.download_file(bucket, key, destination_local_path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the contents of an object stored in S3 as bytes.", "response": "def get_as_bytes(self, s3_path):\n        \"\"\"\n        Get the contents of an object stored in S3 as bytes\n\n        :param s3_path: URL for target S3 location\n        :return: File contents as pure bytes\n        \"\"\"\n        (bucket, key) = self._path_to_bucket_and_key(s3_path)\n        obj = self.s3.Object(bucket, key)\n        contents = obj.get()['Body'].read()\n        return contents"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the contents of an object stored in S3 as a string.", "response": "def get_as_string(self, s3_path, encoding='utf-8'):\n        \"\"\"\n        Get the contents of an object stored in S3 as string.\n\n        :param s3_path: URL for target S3 location\n        :param encoding: Encoding to decode bytes to string\n        :return: File contents as a string\n        \"\"\"\n        content = self.get_as_bytes(s3_path)\n        return content.decode(encoding)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef isdir(self, path):\n        (bucket, key) = self._path_to_bucket_and_key(path)\n\n        s3_bucket = self.s3.Bucket(bucket)\n\n        # root is a directory\n        if self._is_root(key):\n            return True\n\n        for suffix in (S3_DIRECTORY_MARKER_SUFFIX_0,\n                       S3_DIRECTORY_MARKER_SUFFIX_1):\n            try:\n                self.s3.meta.client.get_object(\n                    Bucket=bucket, Key=key + suffix)\n            except botocore.exceptions.ClientError as e:\n                if not e.response['Error']['Code'] in ['NoSuchKey', '404']:\n                    raise\n            else:\n                return True\n\n        # files with this prefix\n        key_path = self._add_path_delimiter(key)\n        s3_bucket_list_result = list(itertools.islice(\n            s3_bucket.objects.filter(Prefix=key_path), 1))\n        if s3_bucket_list_result:\n            return True\n\n        return False", "response": "Is the parameter S3 path a directory?"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef listdir(self, path, start_time=None, end_time=None, return_key=False):\n        (bucket, key) = self._path_to_bucket_and_key(path)\n\n        # grab and validate the bucket\n        s3_bucket = self.s3.Bucket(bucket)\n\n        key_path = self._add_path_delimiter(key)\n        key_path_len = len(key_path)\n        for item in s3_bucket.objects.filter(Prefix=key_path):\n            last_modified_date = item.last_modified\n            if (\n                # neither are defined, list all\n                (not start_time and not end_time) or\n                # start defined, after start\n                (start_time and not end_time and start_time < last_modified_date) or\n                # end defined, prior to end\n                (not start_time and end_time and last_modified_date < end_time) or\n                (start_time and end_time and start_time <\n                 last_modified_date < end_time)  # both defined, between\n            ):\n                if return_key:\n                    yield item\n                else:\n                    yield self._add_path_delimiter(path) + item.key[key_path_len:]", "response": "Get an iterable with S3 folder contents."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef exists(self, path, mtime=None):\n        self._connect()\n\n        if self.sftp:\n            exists = self._sftp_exists(path, mtime)\n        else:\n            exists = self._ftp_exists(path, mtime)\n\n        self._close()\n\n        return exists", "response": "Return True if file or directory at path exist False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef remove(self, path, recursive=True):\n        self._connect()\n\n        if self.sftp:\n            self._sftp_remove(path, recursive)\n        else:\n            self._ftp_remove(path, recursive)\n\n        self._close()", "response": "Removes a file or directory at the given path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef put(self, local_path, path, atomic=True):\n        self._connect()\n\n        if self.sftp:\n            self._sftp_put(local_path, path, atomic)\n        else:\n            self._ftp_put(local_path, path, atomic)\n\n        self._close()", "response": "Put file from local filesystem to FTP."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get(self, path, local_path):\n        normpath = os.path.normpath(local_path)\n        folder = os.path.dirname(normpath)\n        if folder and not os.path.exists(folder):\n            os.makedirs(folder)\n\n        tmp_local_path = local_path + '-luigi-tmp-%09d' % random.randrange(0, 1e10)\n\n        # download file\n        self._connect()\n\n        if self.sftp:\n            self._sftp_get(path, tmp_local_path)\n        else:\n            self._ftp_get(path, tmp_local_path)\n\n        self._close()\n\n        os.rename(tmp_local_path, local_path)", "response": "Download file from SFTP to local filesystem."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef listdir(self, path='.'):\n        self._connect()\n\n        if self.sftp:\n            contents = self._sftp_listdir(path)\n        else:\n            contents = self._ftp_listdir(path)\n\n        self._close()\n\n        return contents", "response": "Gets an list of the contents of path in FTP or FTP."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef open(self, mode):\n        if mode == 'w':\n            return self.format.pipe_writer(AtomicFtpFile(self._fs, self.path))\n\n        elif mode == 'r':\n            temp_dir = os.path.join(tempfile.gettempdir(), 'luigi-contrib-ftp')\n            self.__tmp_path = temp_dir + '/' + self.path.lstrip('/') + '-luigi-tmp-%09d' % random.randrange(0, 1e10)\n            # download file to local\n            self._fs.get(self.path, self.__tmp_path)\n\n            return self.format.pipe_reader(\n                FileWrapper(io.BufferedReader(io.FileIO(self.__tmp_path, 'r')))\n            )\n        else:\n            raise Exception(\"mode must be 'r' or 'w' (got: %s)\" % mode)", "response": "Open the FileSystem target."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_collection(self):\n        db_mongo = self._mongo_client[self._index]\n        return db_mongo[self._collection]", "response": "Return targeted mongo collection to query on\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read(self):\n        result = self.get_collection().aggregate([\n            {'$match': {'_id': self._document_id}},\n            {'$project': {'_value': '$' + self._path, '_id': False}}\n        ])\n\n        for doc in result:\n            if '_value' not in doc:\n                break\n\n            return doc['_value']", "response": "Read the target value of the object"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwriting value to the target", "response": "def write(self, value):\n        \"\"\"\n        Write value to the target\n        \"\"\"\n        self.get_collection().update_one(\n            {'_id': self._document_id},\n            {'$set': {self._path: value}},\n            upsert=True\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef read(self):\n        cursor = self.get_collection().find(\n            {\n                '_id': {'$in': self._document_ids},\n                self._field: {'$exists': True}\n            },\n            {self._field: True}\n        )\n\n        return {doc['_id']: doc[self._field] for doc in cursor}", "response": "Read the targets value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write(self, values):\n        # Insert only for docs targeted by the target\n        filtered = {_id: value for _id, value in values.items() if _id in self._document_ids}\n\n        if not filtered:\n            return\n\n        bulk = self.get_collection().initialize_ordered_bulk_op()\n        for _id, value in filtered.items():\n            bulk.find({'_id': _id}).upsert() \\\n                    .update_one({'$set': {self._field: value}})\n\n        bulk.execute()", "response": "Writes the values to the targeted documents\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting documents id with missing targeted field", "response": "def get_empty_ids(self):\n        \"\"\"\n        Get documents id with missing targeted field\n        \"\"\"\n        cursor = self.get_collection().find(\n            {\n                '_id': {'$in': self._document_ids},\n                self._field: {'$exists': True}\n            },\n            {'_id': True}\n        )\n\n        return set(self._document_ids) - {doc['_id'] for doc in cursor}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting logging settings from config file section logging.", "response": "def _section(cls, opts):\n        \"\"\"Get logging settings from config file section \"logging\".\"\"\"\n        if isinstance(cls.config, LuigiConfigParser):\n            return False\n        try:\n            logging_config = cls.config['logging']\n        except (TypeError, KeyError, NoSectionError):\n            return False\n        logging.config.dictConfig(logging_config)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _conf(cls, opts):\n        logging_conf = cls.config.get('core', 'logging_conf_file', None)\n        if logging_conf is None:\n            return False\n\n        if not os.path.exists(logging_conf):\n            # FileNotFoundError added only in Python 3.3\n            # https://docs.python.org/3/whatsnew/3.3.html#pep-3151-reworking-the-os-and-io-exception-hierarchy\n            raise OSError(\"Error: Unable to locate specified logging configuration file!\")\n\n        logging.config.fileConfig(logging_conf)\n        return True", "response": "Setup logging via ini - file from logging_conf_file option."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_configured_hdfs_client():\n    config = hdfs()\n    custom = config.client\n    conf_usinf_snakebite = [\n        \"snakebite_with_hadoopcli_fallback\",\n        \"snakebite\",\n    ]\n    if six.PY3 and (custom in conf_usinf_snakebite):\n        warnings.warn(\n            \"snakebite client not compatible with python3 at the moment\"\n            \"falling back on hadoopcli\",\n            stacklevel=2\n        )\n        return \"hadoopcli\"\n    return custom", "response": "Returns the client that is configured for hadoopcli"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef tmppath(path=None, include_unix_username=True):\n    addon = \"luigitemp-%08d\" % random.randrange(1e9)\n    temp_dir = '/tmp'  # default tmp dir if none is specified in config\n\n    # 1. Figure out to which temporary directory to place\n    configured_hdfs_tmp_dir = hdfs().tmp_dir\n    if configured_hdfs_tmp_dir is not None:\n        # config is superior\n        base_dir = configured_hdfs_tmp_dir\n    elif path is not None:\n        # need to copy correct schema and network location\n        parsed = urlparse(path)\n        base_dir = urlunparse((parsed.scheme, parsed.netloc, temp_dir, '', '', ''))\n    else:\n        # just system temporary directory\n        base_dir = temp_dir\n\n    # 2. Figure out what to place\n    if path is not None:\n        if path.startswith(temp_dir + '/'):\n            # Not 100%, but some protection from directories like /tmp/tmp/file\n            subdir = path[len(temp_dir):]\n        else:\n            # Protection from /tmp/hdfs:/dir/file\n            parsed = urlparse(path)\n            subdir = parsed.path\n        subdir = subdir.lstrip('/') + '-'\n    else:\n        # just return any random temporary location\n        subdir = ''\n\n    if include_unix_username:\n        subdir = os.path.join(getpass.getuser(), subdir)\n\n    return os.path.join(base_dir, subdir + addon)", "response": "Generate a temporary path for the current node."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _wait_for_consistency(checker):\n    for _ in xrange(EVENTUAL_CONSISTENCY_MAX_SLEEPS):\n        if checker():\n            return\n\n        time.sleep(EVENTUAL_CONSISTENCY_SLEEP_INTERVAL)\n\n    logger.warning('Exceeded wait for eventual GCS consistency - this may be a'\n                   'bug in the library or something is terribly wrong.')", "response": "Wait until GCS reports something is true."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef move(self, source_path, destination_path):\n        self.copy(source_path, destination_path)\n        self.remove(source_path)", "response": "Rename an object from one GCS location to another."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef listdir(self, path):\n        bucket, obj = self._path_to_bucket_and_key(path)\n\n        obj_prefix = self._add_path_delimiter(obj)\n        if self._is_root(obj_prefix):\n            obj_prefix = ''\n\n        obj_prefix_len = len(obj_prefix)\n        for it in self._list_iter(bucket, obj_prefix):\n            yield self._add_path_delimiter(path) + it['name'][obj_prefix_len:]", "response": "Get an iterable with GCS folder contents."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nyields full object URIs matching the given wildcard.", "response": "def list_wildcard(self, wildcard_path):\n        \"\"\"Yields full object URIs matching the given wildcard.\n\n        Currently only the '*' wildcard after the last path delimiter is supported.\n\n        (If we need \"full\" wildcard functionality we should bring in gsutil dependency with its\n        https://github.com/GoogleCloudPlatform/gsutil/blob/master/gslib/wildcard_iterator.py...)\n        \"\"\"\n        path, wildcard_obj = wildcard_path.rsplit('/', 1)\n        assert '*' not in path, \"The '*' wildcard character is only supported after the last '/'\"\n        wildcard_parts = wildcard_obj.split('*')\n        assert len(wildcard_parts) == 2, \"Only one '*' wildcard is supported\"\n\n        for it in self.listdir(path):\n            if it.startswith(path + '/' + wildcard_parts[0]) and it.endswith(wildcard_parts[1]) and \\\n                   len(it) >= len(path + '/' + wildcard_parts[0]) + len(wildcard_parts[1]):\n                yield it"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndownload the contents of the object to local file system.", "response": "def download(self, path, chunksize=None, chunk_callback=lambda _: False):\n        \"\"\"Downloads the object contents to local file system.\n\n        Optionally stops after the first chunk for which chunk_callback returns True.\n        \"\"\"\n        chunksize = chunksize or self.chunksize\n        bucket, obj = self._path_to_bucket_and_key(path)\n\n        with tempfile.NamedTemporaryFile(delete=False) as fp:\n            # We can't return the tempfile reference because of a bug in python: http://bugs.python.org/issue18879\n            return_fp = _DeleteOnCloseFile(fp.name, 'r')\n\n            # Special case empty files because chunk-based downloading doesn't work.\n            result = self.client.objects().get(bucket=bucket, object=obj).execute()\n            if int(result['size']) == 0:\n                return return_fp\n\n            request = self.client.objects().get_media(bucket=bucket, object=obj)\n            downloader = http.MediaIoBaseDownload(fp, request, chunksize=chunksize)\n\n            attempts = 0\n            done = False\n            while not done:\n                error = None\n                try:\n                    _, done = downloader.next_chunk()\n                    if chunk_callback(fp):\n                        done = True\n                except errors.HttpError as err:\n                    error = err\n                    if err.resp.status < 500:\n                        raise\n                    logger.warning('Error downloading file, retrying', exc_info=True)\n                except RETRYABLE_ERRORS as err:\n                    logger.warning('Error downloading file, retrying', exc_info=True)\n                    error = err\n\n                if error:\n                    attempts += 1\n                    if attempts >= NUM_RETRIES:\n                        raise error\n                else:\n                    attempts = 0\n\n        return return_fp"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_subprocess(self, command):\n\n        def subprocess_setup():\n            # Python installs a SIGPIPE handler by default. This is usually not what\n            # non-Python subprocesses expect.\n            signal.signal(signal.SIGPIPE, signal.SIG_DFL)\n\n        return subprocess.Popen(command,\n                                stdin=self._input_pipe,\n                                stdout=subprocess.PIPE,\n                                preexec_fn=subprocess_setup,\n                                close_fds=True)", "response": "Create a subprocess for the given command."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _finish(self):\n        if self._process.returncode is None:\n            self._process.stdin.flush()\n            self._process.stdin.close()\n            self._process.wait()\n            self.closed = True", "response": "Closes and waits for subprocess to exit."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check_complete(task, out_queue):\n    logger.debug(\"Checking if %s is complete\", task)\n    try:\n        is_complete = task.complete()\n    except Exception:\n        is_complete = TracebackWrapper(traceback.format_exc())\n    out_queue.put((task, is_complete))", "response": "Checks if task is complete puts the result to out_queue."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd a new task to the scheduler.", "response": "def _add_task(self, *args, **kwargs):\n        \"\"\"\n        Call ``self._scheduler.add_task``, but store the values too so we can\n        implement :py:func:`luigi.execution_summary.summary`.\n        \"\"\"\n        task_id = kwargs['task_id']\n        status = kwargs['status']\n        runnable = kwargs['runnable']\n        task = self._scheduled_tasks.get(task_id)\n        if task:\n            self._add_task_history.append((task, status, runnable))\n            kwargs['owners'] = task._owner_list()\n\n        if task_id in self._batch_running_tasks:\n            for batch_task in self._batch_running_tasks.pop(task_id):\n                self._add_task_history.append((batch_task, status, True))\n\n        if task and kwargs.get('params'):\n            kwargs['param_visibilities'] = task._get_param_visibilities()\n\n        self._scheduler.add_task(*args, **kwargs)\n\n        logger.info('Informed scheduler that task   %s   has status   %s', task_id, status)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add(self, task, multiprocess=False, processes=0):\n        if self._first_task is None and hasattr(task, 'task_id'):\n            self._first_task = task.task_id\n        self.add_succeeded = True\n        if multiprocess:\n            queue = multiprocessing.Manager().Queue()\n            pool = multiprocessing.Pool(processes=processes if processes > 0 else None)\n        else:\n            queue = DequeQueue()\n            pool = SingleProcessPool()\n        self._validate_task(task)\n        pool.apply_async(check_complete, [task, queue])\n\n        # we track queue size ourselves because len(queue) won't work for multiprocessing\n        queue_size = 1\n        try:\n            seen = {task.task_id}\n            while queue_size:\n                current = queue.get()\n                queue_size -= 1\n                item, is_complete = current\n                for next in self._add(item, is_complete):\n                    if next.task_id not in seen:\n                        self._validate_task(next)\n                        seen.add(next.task_id)\n                        pool.apply_async(check_complete, [next, queue])\n                        queue_size += 1\n        except (KeyboardInterrupt, TaskException):\n            raise\n        except Exception as ex:\n            self.add_succeeded = False\n            formatted_traceback = traceback.format_exc()\n            self._log_unexpected_error(task)\n            task.trigger_event(Event.BROKEN_TASK, task, ex)\n            self._email_unexpected_error(task, formatted_traceback)\n            raise\n        finally:\n            pool.close()\n            pool.join()\n        return self.add_succeeded", "response": "Adds a Task to the queue and checks if it s successfully scheduled or completed before. Returns True if the task and its dependencies were successfully scheduled or completed before."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nremoving dead children from the result queue.", "response": "def _purge_children(self):\n        \"\"\"\n        Find dead children and put a response on the result queue.\n\n        :return:\n        \"\"\"\n        for task_id, p in six.iteritems(self._running_tasks):\n            if not p.is_alive() and p.exitcode:\n                error_msg = 'Task {} died unexpectedly with exit code {}'.format(task_id, p.exitcode)\n                p.task.trigger_event(Event.PROCESS_FAILURE, p.task, error_msg)\n            elif p.timeout_time is not None and time.time() > float(p.timeout_time) and p.is_alive():\n                p.terminate()\n                error_msg = 'Task {} timed out after {} seconds and was terminated.'.format(task_id, p.worker_timeout)\n                p.task.trigger_event(Event.TIMEOUT, p.task, error_msg)\n            else:\n                continue\n\n            logger.info(error_msg)\n            self._task_result_queue.put((task_id, FAILED, error_msg, [], []))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns True if a worker should stay alive given.", "response": "def _keep_alive(self, get_work_response):\n        \"\"\"\n        Returns true if a worker should stay alive given.\n\n        If worker-keep-alive is not set, this will always return false.\n        For an assistant, it will always return the value of worker-keep-alive.\n        Otherwise, it will return true for nonzero n_pending_tasks.\n\n        If worker-count-uniques is true, it will also\n        require that one of the tasks is unique to this worker.\n        \"\"\"\n        if not self._config.keep_alive:\n            return False\n        elif self._assistant:\n            return True\n        elif self._config.count_last_scheduled:\n            return get_work_response.n_pending_last_scheduled > 0\n        elif self._config.count_uniques:\n            return get_work_response.n_unique_pending > 0\n        elif get_work_response.n_pending_tasks == 0:\n            return False\n        elif not self._config.max_keep_alive_idle_duration:\n            return True\n        elif not self._idle_since:\n            return True\n        else:\n            time_to_shutdown = self._idle_since + self._config.max_keep_alive_idle_duration - datetime.datetime.now()\n            logger.debug(\"[%s] %s until shutdown\", self._id, time_to_shutdown)\n            return time_to_shutdown > datetime.timedelta(0)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef run(self):\n        logger.info('Running Worker with %d processes', self.worker_processes)\n\n        sleeper = self._sleeper()\n        self.run_succeeded = True\n\n        self._add_worker()\n\n        while True:\n            while len(self._running_tasks) >= self.worker_processes > 0:\n                logger.debug('%d running tasks, waiting for next task to finish', len(self._running_tasks))\n                self._handle_next_task()\n\n            get_work_response = self._get_work()\n\n            if get_work_response.worker_state == WORKER_STATE_DISABLED:\n                self._start_phasing_out()\n\n            if get_work_response.task_id is None:\n                if not self._stop_requesting_work:\n                    self._log_remote_tasks(get_work_response)\n                if len(self._running_tasks) == 0:\n                    self._idle_since = self._idle_since or datetime.datetime.now()\n                    if self._keep_alive(get_work_response):\n                        six.next(sleeper)\n                        continue\n                    else:\n                        break\n                else:\n                    self._handle_next_task()\n                    continue\n\n            # task_id is not None:\n            logger.debug(\"Pending tasks: %s\", get_work_response.n_pending_tasks)\n            self._run_task(get_work_response.task_id)\n\n        while len(self._running_tasks):\n            logger.debug('Shut down Worker, %d more tasks to go', len(self._running_tasks))\n            self._handle_next_task()\n\n        return self.run_succeeded", "response": "This method runs the worker process."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nensuring the database schema is up to date with the codebase.", "response": "def _upgrade_schema(engine):\n    \"\"\"\n    Ensure the database schema is up to date with the codebase.\n\n    :param engine: SQLAlchemy engine of the underlying database.\n    \"\"\"\n    inspector = reflection.Inspector.from_engine(engine)\n    with engine.connect() as conn:\n\n        # Upgrade 1.  Add task_id column and index to tasks\n        if 'task_id' not in [x['name'] for x in inspector.get_columns('tasks')]:\n            logger.warning('Upgrading DbTaskHistory schema: Adding tasks.task_id')\n            conn.execute('ALTER TABLE tasks ADD COLUMN task_id VARCHAR(200)')\n            conn.execute('CREATE INDEX ix_task_id ON tasks (task_id)')\n\n        # Upgrade 2. Alter value column to be TEXT, note that this is idempotent so no if-guard\n        if 'mysql' in engine.dialect.name:\n            conn.execute('ALTER TABLE task_parameters MODIFY COLUMN value TEXT')\n        elif 'oracle' in engine.dialect.name:\n            conn.execute('ALTER TABLE task_parameters MODIFY value TEXT')\n        elif 'mssql' in engine.dialect.name:\n            conn.execute('ALTER TABLE task_parameters ALTER COLUMN value TEXT')\n        elif 'postgresql' in engine.dialect.name:\n            if str([x for x in inspector.get_columns('task_parameters')\n                    if x['name'] == 'value'][0]['type']) != 'TEXT':\n                conn.execute('ALTER TABLE task_parameters ALTER COLUMN value TYPE TEXT')\n        elif 'sqlite' in engine.dialect.name:\n            # SQLite does not support changing column types. A database file will need\n            # to be used to pickup this migration change.\n            for i in conn.execute('PRAGMA table_info(task_parameters);').fetchall():\n                if i['name'] == 'value' and i['type'] != 'TEXT':\n                    logger.warning(\n                        'SQLite can not change column types. Please use a new database '\n                        'to pickup column type changes.'\n                    )\n        else:\n            logger.warning(\n                'SQLAlcheny dialect {} could not be migrated to the TEXT type'.format(\n                    engine.dialect\n                )\n            )"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfinds all tasks with the given name and the same parameters as the kwargs.", "response": "def find_all_by_parameters(self, task_name, session=None, **task_params):\n        \"\"\"\n        Find tasks with the given task_name and the same parameters as the kwargs.\n        \"\"\"\n        with self._session(session) as session:\n            query = session.query(TaskRecord).join(TaskEvent).filter(TaskRecord.name == task_name)\n            for (k, v) in six.iteritems(task_params):\n                alias = sqlalchemy.orm.aliased(TaskParameter)\n                query = query.join(alias).filter(alias.name == k, alias.value == v)\n\n            tasks = query.order_by(TaskEvent.ts)\n            for task in tasks:\n                # Sanity check\n                assert all(k in task.parameters and v == str(task.parameters[k].value) for (k, v) in six.iteritems(task_params))\n\n                yield task"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning all tasks that have been updated.", "response": "def find_all_runs(self, session=None):\n        \"\"\"\n        Return all tasks that have been updated.\n        \"\"\"\n        with self._session(session) as session:\n            return session.query(TaskRecord).all()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn all running or failed events.", "response": "def find_all_events(self, session=None):\n        \"\"\"\n        Return all running/failed/done events.\n        \"\"\"\n        with self._session(session) as session:\n            return session.query(TaskEvent).all()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_task_by_id(self, id, session=None):\n        with self._session(session) as session:\n            return session.query(TaskRecord).get(id)", "response": "Find task with the given record ID."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_authenticate_kwargs(oauth_credentials=None, http_=None):\n    if oauth_credentials:\n        authenticate_kwargs = {\n            \"credentials\": oauth_credentials\n        }\n    elif http_:\n        authenticate_kwargs = {\n            \"http\": http_\n        }\n    else:\n        # neither http_ or credentials provided\n        try:\n            # try default credentials\n            credentials, _ = google.auth.default()\n            authenticate_kwargs = {\n                \"credentials\": credentials\n            }\n        except google.auth.exceptions.DefaultCredentialsError:\n            # try http using httplib2\n            authenticate_kwargs = {\n                \"http\": httplib2.Http()\n            }\n\n    return authenticate_kwargs", "response": "Returns a dictionary with keyword arguments for use with discovery\n           "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a credential string for the provided task.", "response": "def _credentials(self):\n        \"\"\"\n        Return a credential string for the provided task. If no valid\n        credentials are set, raise a NotImplementedError.\n        \"\"\"\n\n        if self.aws_account_id and self.aws_arn_role_name:\n            return 'aws_iam_role=arn:aws:iam::{id}:role/{role}'.format(\n                id=self.aws_account_id,\n                role=self.aws_arn_role_name\n            )\n        elif self.aws_access_key_id and self.aws_secret_access_key:\n            return 'aws_access_key_id={key};aws_secret_access_key={secret}{opt}'.format(\n                key=self.aws_access_key_id,\n                secret=self.aws_secret_access_key,\n                opt=';token={}'.format(self.aws_session_token) if self.aws_session_token else ''\n            )\n        else:\n            raise NotImplementedError(\"Missing Credentials. \"\n                                      \"Ensure one of the pairs of auth args below are set \"\n                                      \"in a configuration file, environment variables or by \"\n                                      \"being overridden in the task: \"\n                                      \"'aws_access_key_id' AND 'aws_secret_access_key' OR \"\n                                      \"'aws_account_id' AND 'aws_arn_role_name'\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef do_prune(self):\n        if self.prune_table and self.prune_column and self.prune_date:\n            return True\n        elif self.prune_table or self.prune_column or self.prune_date:\n            raise Exception('override zero or all prune variables')\n        else:\n            return False", "response": "Return True if prune_table prune_column and prune_date are implemented."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates the schema in the database.", "response": "def create_schema(self, connection):\n        \"\"\"\n        Will create the schema in the database\n        \"\"\"\n        if '.' not in self.table:\n            return\n\n        query = 'CREATE SCHEMA IF NOT EXISTS {schema_name};'.format(schema_name=self.table.split('.')[0])\n        connection.cursor().execute(query)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_table(self, connection):\n        if len(self.columns[0]) == 1:\n            # only names of columns specified, no types\n            raise NotImplementedError(\"create_table() not implemented \"\n                                      \"for %r and columns types not \"\n                                      \"specified\" % self.table)\n        elif len(self.columns[0]) == 2:\n            # if columns is specified as (name, type) tuples\n            coldefs = ','.join(\n                '{name} {type}'.format(\n                    name=name,\n                    type=type) for name, type in self.columns\n            )\n\n            table_constraints = ''\n            if self.table_constraints != '':\n                table_constraints = ', ' + self.table_constraints\n\n            query = (\"CREATE {type} TABLE \"\n                     \"{table} ({coldefs} {table_constraints}) \"\n                     \"{table_attributes}\").format(\n                type=self.table_type,\n                table=self.table,\n                coldefs=coldefs,\n                table_constraints=table_constraints,\n                table_attributes=self.table_attributes)\n\n            connection.cursor().execute(query)\n        elif len(self.columns[0]) == 3:\n            # if columns is specified as (name, type, encoding) tuples\n            # possible column encodings: https://docs.aws.amazon.com/redshift/latest/dg/c_Compression_encodings.html\n            coldefs = ','.join(\n                '{name} {type} ENCODE {encoding}'.format(\n                    name=name,\n                    type=type,\n                    encoding=encoding) for name, type, encoding in self.columns\n            )\n\n            table_constraints = ''\n            if self.table_constraints != '':\n                table_constraints = ',' + self.table_constraints\n\n            query = (\"CREATE {type} TABLE \"\n                     \"{table} ({coldefs} {table_constraints}) \"\n                     \"{table_attributes}\").format(\n                type=self.table_type,\n                table=self.table,\n                coldefs=coldefs,\n                table_constraints=table_constraints,\n                table_attributes=self.table_attributes)\n\n            connection.cursor().execute(query)\n        else:\n            raise ValueError(\"create_table() found no columns for %r\"\n                             % self.table)", "response": "Creates the target table for the current object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run(self):\n        if not (self.table):\n            raise Exception(\"table need to be specified\")\n\n        path = self.s3_load_path()\n        output = self.output()\n        connection = output.connect()\n        cursor = connection.cursor()\n\n        self.init_copy(connection)\n        self.copy(cursor, path)\n        self.post_copy(cursor)\n\n        if self.enable_metadata_columns:\n            self.post_copy_metacolumns(cursor)\n\n        # update marker table\n        output.touch(connection)\n        connection.commit()\n\n        # commit and clean up\n        connection.close()", "response": "This method runs the main entry point."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncopy a file into the database.", "response": "def copy(self, cursor, f):\n        \"\"\"\n        Defines copying from s3 into redshift.\n\n        If both key-based and role-based credentials are provided, role-based will be used.\n        \"\"\"\n        logger.info(\"Inserting file: %s\", f)\n        colnames = ''\n        if self.columns and len(self.columns) > 0:\n            colnames = \",\".join([x[0] for x in self.columns])\n            colnames = '({})'.format(colnames)\n\n        cursor.execute(\"\"\"\n         COPY {table} {colnames} from '{source}'\n         CREDENTIALS '{creds}'\n         {options}\n         ;\"\"\".format(\n            table=self.table,\n            colnames=colnames,\n            source=f,\n            creds=self._credentials(),\n            options=self.copy_options)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef does_schema_exist(self, connection):\n\n        if '.' in self.table:\n            query = (\"select 1 as schema_exists \"\n                     \"from pg_namespace \"\n                     \"where nspname = lower(%s) limit 1\")\n        else:\n            return True\n\n        cursor = connection.cursor()\n        try:\n            schema = self.table.split('.')[0]\n            cursor.execute(query, [schema])\n            result = cursor.fetchone()\n            return bool(result)\n        finally:\n            cursor.close()", "response": "Determine whether the schema already exists."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef does_table_exist(self, connection):\n\n        if '.' in self.table:\n            query = (\"select 1 as table_exists \"\n                     \"from information_schema.tables \"\n                     \"where table_schema = lower(%s) and table_name = lower(%s) limit 1\")\n        else:\n            query = (\"select 1 as table_exists \"\n                     \"from pg_table_def \"\n                     \"where tablename = lower(%s) limit 1\")\n        cursor = connection.cursor()\n        try:\n            cursor.execute(query, tuple(self.table.split('.')))\n            result = cursor.fetchone()\n            return bool(result)\n        finally:\n            cursor.close()", "response": "Determine whether the table exists."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef init_copy(self, connection):\n        if not self.does_schema_exist(connection):\n            logger.info(\"Creating schema for %s\", self.table)\n            self.create_schema(connection)\n\n        if not self.does_table_exist(connection):\n            logger.info(\"Creating table %s\", self.table)\n            self.create_table(connection)\n\n        if self.enable_metadata_columns:\n            self._add_metadata_columns(connection)\n\n        if self.do_truncate_table:\n            logger.info(\"Truncating table %s\", self.table)\n            self.truncate_table(connection)\n\n        if self.do_prune():\n            logger.info(\"Removing %s older than %s from %s\", self.prune_column, self.prune_date, self.prune_table)\n            self.prune(connection)", "response": "Perform pre - copy sql - such as creating table truncating table and removing data older than x."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef post_copy(self, cursor):\n        logger.info('Executing post copy queries')\n        for query in self.queries:\n            cursor.execute(query)", "response": "Executes post - copy queries on the database."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nperforms post - copy to fill metadata columns.", "response": "def post_copy_metacolums(self, cursor):\n        \"\"\"\n        Performs post-copy to fill metadata columns.\n        \"\"\"\n        logger.info('Executing post copy metadata queries')\n        for query in self.metadata_queries:\n            cursor.execute(query)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef copy(self, cursor, f):\n\n        logger.info(\"Inserting file: %s\", f)\n        cursor.execute(\"\"\"\n         COPY %s from '%s'\n         CREDENTIALS '%s'\n         JSON AS '%s' %s\n         %s\n         ;\"\"\" % (self.table, f, self._credentials(),\n                 self.jsonpath, self.copy_json_options, self.copy_options))", "response": "Copy a file from s3 into redshift."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a RedshiftTarget representing the inserted dataset. Normally you don t override this.", "response": "def output(self):\n        \"\"\"\n        Returns a RedshiftTarget representing the inserted dataset.\n\n        Normally you don't override this.\n        \"\"\"\n        # uses class name as a meta-table\n        return RedshiftTarget(\n            host=self.host,\n            database=self.database,\n            user=self.user,\n            password=self.password,\n            table=self.__class__.__name__,\n            update_id=self.update_id)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nkilling any open Redshift sessions for the given database.", "response": "def run(self):\n        \"\"\"\n        Kill any open Redshift sessions for the given database.\n        \"\"\"\n        connection = self.output().connect()\n        # kill any sessions other than ours and\n        # internal Redshift sessions (rdsdb)\n        query = (\"select pg_terminate_backend(process) \"\n                 \"from STV_SESSIONS \"\n                 \"where db_name=%s \"\n                 \"and user_name != 'rdsdb' \"\n                 \"and process != pg_backend_pid()\")\n        cursor = connection.cursor()\n        logger.info('Killing all open Redshift sessions for database: %s', self.database)\n        try:\n            cursor.execute(query, (self.database,))\n            cursor.close()\n            connection.commit()\n        except psycopg2.DatabaseError as e:\n            if e.message and 'EOF' in e.message:\n                # sometimes this operation kills the current session.\n                # rebuild the connection. Need to pause for 30-60 seconds\n                # before Redshift will allow us back in.\n                connection.close()\n                logger.info('Pausing %s seconds for Redshift to reset connection', self.connection_reset_wait_seconds)\n                time.sleep(self.connection_reset_wait_seconds)\n                logger.info('Reconnecting to Redshift')\n                connection = self.output().connect()\n            else:\n                raise\n\n        try:\n            self.output().touch(connection)\n            connection.commit()\n        finally:\n            connection.close()\n\n        logger.info('Done killing all open Redshift sessions for database: %s', self.database)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a list of dates in this date interval.", "response": "def dates(self):\n        ''' Returns a list of dates in this date interval.'''\n        dates = []\n        d = self.date_a\n        while d < self.date_b:\n            dates.append(d)\n            d += datetime.timedelta(1)\n\n        return dates"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef hours(self):\n        ''' Same as dates() but returns 24 times more info: one for each hour.'''\n        for date in self.dates():\n            for hour in xrange(24):\n                yield datetime.datetime.combine(date, datetime.time(hour))", "response": "Returns a generator that yields the 24 times more info for each hour."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run(self):\n        with self.output().open('w') as outfile:\n            print(\"data 0 200 10 50 60\", file=outfile)\n            print(\"data 1 190 9 52 60\", file=outfile)\n            print(\"data 2 200 10 52 60\", file=outfile)\n            print(\"data 3 195 1 52 60\", file=outfile)", "response": "This method is called by the base class to write the data to the target output file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncopy the contents of a single file path to dest.", "response": "def copy(self, path, dest, raise_if_exists=False):\n        \"\"\"\n        Copies the contents of a single file path to dest\n        \"\"\"\n        if raise_if_exists and dest in self.get_all_data():\n            raise RuntimeError('Destination exists: %s' % path)\n        contents = self.get_all_data()[path]\n        self.get_all_data()[dest] = contents"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef remove(self, path, recursive=True, skip_trash=True):\n        if recursive:\n            to_delete = []\n            for s in self.get_all_data().keys():\n                if s.startswith(path):\n                    to_delete.append(s)\n            for s in to_delete:\n                self.get_all_data().pop(s)\n        else:\n            self.get_all_data().pop(path)", "response": "Removes the given mockfile."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmoving a single file from path to dest.", "response": "def move(self, path, dest, raise_if_exists=False):\n        \"\"\"\n        Moves a single file from path to dest\n        \"\"\"\n        if raise_if_exists and dest in self.get_all_data():\n            raise RuntimeError('Destination exists: %s' % path)\n        contents = self.get_all_data().pop(path)\n        self.get_all_data()[dest] = contents"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmove the containing file to the given path.", "response": "def move(self, path, raise_if_exists=False):\n        \"\"\"\n        Call MockFileSystem's move command\n        \"\"\"\n        self.fs.move(self.path, path, raise_if_exists)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _recursively_freeze(value):\n    if isinstance(value, Mapping):\n        return _FrozenOrderedDict(((k, _recursively_freeze(v)) for k, v in value.items()))\n    elif isinstance(value, list) or isinstance(value, tuple):\n        return tuple(_recursively_freeze(v) for v in value)\n    return value", "response": "Recursively walks a mapping and lists and converts them to _FrozenOrderedDict and tuples respectively."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nloading the default value from the config. Returns _no_value if it doesn t exist.", "response": "def _get_value_from_config(self, section, name):\n        \"\"\"Loads the default from the config. Returns _no_value if it doesn't exist\"\"\"\n\n        conf = configuration.get_config()\n\n        try:\n            value = conf.get(section, name)\n        except (NoSectionError, NoOptionError, KeyError):\n            return _no_value\n\n        return self.parse(value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _value_iterator(self, task_name, param_name):\n        cp_parser = CmdlineParser.get_instance()\n        if cp_parser:\n            dest = self._parser_global_dest(param_name, task_name)\n            found = getattr(cp_parser.known_args, dest, None)\n            yield (self._parse_or_no_value(found), None)\n        yield (self._get_value_from_config(task_name, param_name), None)\n        if self._config_path:\n            yield (self._get_value_from_config(self._config_path['section'], self._config_path['name']),\n                   'The use of the configuration [{}] {} is deprecated. Please use [{}] {}'.format(\n                       self._config_path['section'], self._config_path['name'], task_name, param_name))\n        yield (self._default, None)", "response": "Yields the values of the given task and parameter."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _parse_list(self, xs):\n        if not self._is_batchable():\n            raise NotImplementedError('No batch method found')\n        elif not xs:\n            raise ValueError('Empty parameter list passed to parse_list')\n        else:\n            return self._batch_method(map(self.parse, xs))", "response": "Parse a list of values from the scheduler."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse(self, s):\n        return datetime.datetime.strptime(s, self.date_format).date()", "response": "Parses a date string formatted like YYYY - MM - DD."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef serialize(self, dt):\n        if dt is None:\n            return str(dt)\n        return dt.strftime(self.date_format)", "response": "Converts the date to a string using the date_format attribute."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _add_months(self, date, months):\n        year = date.year + (date.month + months - 1) // 12\n        month = (date.month + months - 1) % 12 + 1\n        return datetime.date(year=year, month=month, day=1)", "response": "Add months to date."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nclamp dt to every Nth : py : attr : ~_DatetimeParameterBase. interval starting at self. start.", "response": "def normalize(self, dt):\n        \"\"\"\n        Clamp dt to every Nth :py:attr:`~_DatetimeParameterBase.interval` starting at\n        :py:attr:`~_DatetimeParameterBase.start`.\n        \"\"\"\n        if dt is None:\n            return None\n\n        dt = self._convert_to_dt(dt)\n\n        dt = dt.replace(microsecond=0)  # remove microseconds, to avoid float rounding issues.\n        delta = (dt - self.start).total_seconds()\n        granularity = (self._timedelta * self.interval).total_seconds()\n        return dt - datetime.timedelta(seconds=delta % granularity)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse(self, val):\n        s = str(val).lower()\n        if s == \"true\":\n            return True\n        elif s == \"false\":\n            return False\n        else:\n            raise ValueError(\"cannot interpret '{}' as boolean\".format(val))", "response": "Parses a bool from the string matching true or false ignoring case."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing a luigi. date_interval. DateInterval from the input string and returns a luigi. date_interval. DatetimeInterval object.", "response": "def parse(self, s):\n        \"\"\"\n        Parses a :py:class:`~luigi.date_interval.DateInterval` from the input.\n\n        see :py:mod:`luigi.date_interval`\n          for details on the parsing of DateIntervals.\n        \"\"\"\n        # TODO: can we use xml.utils.iso8601 or something similar?\n\n        from luigi import date_interval as d\n\n        for cls in [d.Year, d.Month, d.Week, d.Date, d.Custom]:\n            i = cls.parse(s)\n            if i:\n                return i\n\n        raise ValueError('Invalid date interval - could not be parsed')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse(self, input):\n        result = self._parseIso8601(input)\n        if not result:\n            result = self._parseSimple(input)\n        if result is not None:\n            return result\n        else:\n            raise ParameterException(\"Invalid time delta - could not parse %s\" % input)", "response": "Parses a time delta from the input."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting datetime. timedelta to a string", "response": "def serialize(self, x):\n        \"\"\"\n        Converts datetime.timedelta to a string\n\n        :param x: the value to serialize.\n        \"\"\"\n        weeks = x.days // 7\n        days = x.days % 7\n        hours = x.seconds // 3600\n        minutes = (x.seconds % 3600) // 60\n        seconds = (x.seconds % 3600) % 60\n        result = \"{} w {} d {} h {} m {} s\".format(weeks, days, hours, minutes, seconds)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse(self, x):\n        # Since the result of json.dumps(tuple) differs from a tuple string, we must handle either case.\n        # A tuple string may come from a config file or from cli execution.\n\n        # t = ((1, 2), (3, 4))\n        # t_str = '((1,2),(3,4))'\n        # t_json_str = json.dumps(t)\n        # t_json_str == '[[1, 2], [3, 4]]'\n        # json.loads(t_json_str) == t\n        # json.loads(t_str) == ValueError: No JSON object could be decoded\n\n        # Therefore, if json.loads(x) returns a ValueError, try ast.literal_eval(x).\n        # ast.literal_eval(t_str) == t\n        try:\n            # loop required to parse tuple of tuples\n            return tuple(tuple(x) for x in json.loads(x, object_pairs_hook=_FrozenOrderedDict))\n        except (ValueError, TypeError):\n            return tuple(literal_eval(x))", "response": "Parse an individual value from the input."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run(self):\n        count = {}\n\n        # NOTE: self.input() actually returns an element for the InputText.output() target\n        for f in self.input():  # The input() method is a wrapper around requires() that returns Target objects\n            for line in f.open('r'):  # Target objects are a file system/format abstraction and this will return a file stream object\n                for word in line.strip().split():\n                    count[word] = count.get(word, 0) + 1\n\n        # output data\n        f = self.output().open('w')\n        for word, count in six.iteritems(count):\n            f.write(\"%s\\t%d\\n\" % (word, count))\n        f.close()", "response": "Run the count of words for each of the target objects created by this class."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a tar archive containing the files for the packages listed in packages.", "response": "def create_packages_archive(packages, filename):\n    \"\"\"\n    Create a tar archive which will contain the files for the packages listed in packages.\n    \"\"\"\n    import tarfile\n    tar = tarfile.open(filename, \"w\")\n\n    def add(src, dst):\n        logger.debug('adding to tar: %s -> %s', src, dst)\n        tar.add(src, dst)\n\n    def add_files_for_package(sub_package_path, root_package_path, root_package_name):\n        for root, dirs, files in os.walk(sub_package_path):\n            if '.svn' in dirs:\n                dirs.remove('.svn')\n            for f in files:\n                if not f.endswith(\".pyc\") and not f.startswith(\".\"):\n                    add(dereference(root + \"/\" + f), root.replace(root_package_path, root_package_name) + \"/\" + f)\n\n    for package in packages:\n        # Put a submodule's entire package in the archive. This is the\n        # magic that usually packages everything you need without\n        # having to attach packages/modules explicitly\n        if not getattr(package, \"__path__\", None) and '.' in package.__name__:\n            package = __import__(package.__name__.rpartition('.')[0], None, None, 'non_empty')\n\n        n = package.__name__.replace(\".\", \"/\")\n\n        if getattr(package, \"__path__\", None):\n            # TODO: (BUG) picking only the first path does not\n            # properly deal with namespaced packages in different\n            # directories\n            p = package.__path__[0]\n\n            if p.endswith('.egg') and os.path.isfile(p):\n                raise 'egg files not supported!!!'\n                # Add the entire egg file\n                # p = p[:p.find('.egg') + 4]\n                # add(dereference(p), os.path.basename(p))\n\n            else:\n                # include __init__ files from parent projects\n                root = []\n                for parent in package.__name__.split('.')[0:-1]:\n                    root.append(parent)\n                    module_name = '.'.join(root)\n                    directory = '/'.join(root)\n\n                    add(dereference(__import__(module_name, None, None, 'non_empty').__path__[0] + \"/__init__.py\"),\n                        directory + \"/__init__.py\")\n\n                add_files_for_package(p, p, n)\n\n                # include egg-info directories that are parallel:\n                for egg_info_path in glob.glob(p + '*.egg-info'):\n                    logger.debug(\n                        'Adding package metadata to archive for \"%s\" found at \"%s\"',\n                        package.__name__,\n                        egg_info_path\n                    )\n                    add_files_for_package(egg_info_path, p, n)\n\n        else:\n            f = package.__file__\n            if f.endswith(\"pyc\"):\n                f = f[:-3] + \"py\"\n            if n.find(\".\") == -1:\n                add(dereference(f), os.path.basename(f))\n            else:\n                add(dereference(f), n + \".py\")\n    tar.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run_and_track_hadoop_job(arglist, tracking_url_callback=None, env=None):\n    logger.info('%s', subprocess.list2cmdline(arglist))\n\n    def write_luigi_history(arglist, history):\n        \"\"\"\n        Writes history to a file in the job's output directory in JSON format.\n        Currently just for tracking the job ID in a configuration where\n        no history is stored in the output directory by Hadoop.\n        \"\"\"\n        history_filename = configuration.get_config().get('core', 'history-filename', '')\n        if history_filename and '-output' in arglist:\n            output_dir = arglist[arglist.index('-output') + 1]\n            f = luigi.contrib.hdfs.HdfsTarget(os.path.join(output_dir, history_filename)).open('w')\n            f.write(json.dumps(history))\n            f.close()\n\n    def track_process(arglist, tracking_url_callback, env=None):\n        # Dump stdout to a temp file, poll stderr and log it\n        temp_stdout = tempfile.TemporaryFile('w+t')\n        proc = subprocess.Popen(arglist, stdout=temp_stdout, stderr=subprocess.PIPE, env=env, close_fds=True, universal_newlines=True)\n\n        # We parse the output to try to find the tracking URL.\n        # This URL is useful for fetching the logs of the job.\n        tracking_url = None\n        job_id = None\n        application_id = None\n        err_lines = []\n\n        with HadoopRunContext() as hadoop_context:\n            while proc.poll() is None:\n                err_line = proc.stderr.readline()\n                err_lines.append(err_line)\n                err_line = err_line.strip()\n                if err_line:\n                    logger.info('%s', err_line)\n                err_line = err_line.lower()\n                tracking_url_match = TRACKING_RE.search(err_line)\n                if tracking_url_match:\n                    tracking_url = tracking_url_match.group('url')\n                    try:\n                        tracking_url_callback(tracking_url)\n                    except Exception as e:\n                        logger.error(\"Error in tracking_url_callback, disabling! %s\", e)\n\n                        def tracking_url_callback(x):\n                            return None\n                if err_line.find('running job') != -1:\n                    # hadoop jar output\n                    job_id = err_line.split('running job: ')[-1]\n                if err_line.find('submitted hadoop job:') != -1:\n                    # scalding output\n                    job_id = err_line.split('submitted hadoop job: ')[-1]\n                if err_line.find('submitted application ') != -1:\n                    application_id = err_line.split('submitted application ')[-1]\n                hadoop_context.job_id = job_id\n                hadoop_context.application_id = application_id\n\n        # Read the rest + stdout\n        err = ''.join(err_lines + [an_err_line for an_err_line in proc.stderr])\n        temp_stdout.seek(0)\n        out = ''.join(temp_stdout.readlines())\n\n        if proc.returncode == 0:\n            write_luigi_history(arglist, {'job_id': job_id})\n            return (out, err)\n\n        # Try to fetch error logs if possible\n        message = 'Streaming job failed with exit code %d. ' % proc.returncode\n        if not tracking_url:\n            raise HadoopJobError(message + 'Also, no tracking url found.', out, err)\n\n        try:\n            task_failures = fetch_task_failures(tracking_url)\n        except Exception as e:\n            raise HadoopJobError(message + 'Additionally, an error occurred when fetching data from %s: %s' %\n                                 (tracking_url, e), out, err)\n\n        if not task_failures:\n            raise HadoopJobError(message + 'Also, could not fetch output from tasks.', out, err)\n        else:\n            raise HadoopJobError(message + 'Output from tasks below:\\n%s' % task_failures, out, err)\n\n    if tracking_url_callback is None:\n        def tracking_url_callback(x): return None\n\n    return track_process(arglist, tracking_url_callback, env)", "response": "Runs a job and tracks it."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfetch the actual task logs from the task tracker.", "response": "def fetch_task_failures(tracking_url):\n    \"\"\"\n    Uses mechanize to fetch the actual task logs from the task tracker.\n\n    This is highly opportunistic, and we might not succeed.\n    So we set a low timeout and hope it works.\n    If it does not, it's not the end of the world.\n\n    TODO: Yarn has a REST API that we should probably use instead:\n    http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/WebServicesIntro.html\n    \"\"\"\n    import mechanize\n    timeout = 3.0\n    failures_url = tracking_url.replace('jobdetails.jsp', 'jobfailures.jsp') + '&cause=failed'\n    logger.debug('Fetching data from %s', failures_url)\n    b = mechanize.Browser()\n    b.open(failures_url, timeout=timeout)\n    links = list(b.links(text_regex='Last 4KB'))  # For some reason text_regex='All' doesn't work... no idea why\n    links = random.sample(links, min(10, len(links)))  # Fetch a random subset of all failed tasks, so not to be biased towards the early fails\n    error_text = []\n    for link in links:\n        task_url = link.url.replace('&start=-4097', '&start=-100000')  # Increase the offset\n        logger.debug('Fetching data from %s', task_url)\n        b2 = mechanize.Browser()\n        try:\n            r = b2.open(task_url, timeout=timeout)\n            data = r.read()\n        except Exception as e:\n            logger.debug('Error fetching data from %s: %s', task_url, e)\n            continue\n        # Try to get the hex-encoded traceback back from the output\n        for exc in re.findall(r'luigi-exc-hex=[0-9a-f]+', data):\n            error_text.append('---------- %s:' % task_url)\n            error_text.append(exc.split('=')[-1].decode('hex'))\n\n    return '\\n'.join(error_text)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_pool(self):\n        if self.pool:\n            return self.pool\n        if hadoop().pool:\n            return hadoop().pool", "response": "Protected method to get the pool name"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the job runner for this job.", "response": "def job_runner(self):\n        # We recommend that you define a subclass, override this method and set up your own config\n        \"\"\"\n        Get the MapReduce runner for this job.\n\n        If all outputs are HdfsTargets, the DefaultHadoopJobRunner will be used.\n        Otherwise, the LocalJobRunner which streams all data through the local machine\n        will be used (great for testing).\n        \"\"\"\n        outputs = luigi.task.flatten(self.output())\n        for output in outputs:\n            if not isinstance(output, luigi.contrib.hdfs.HdfsTarget):\n                warnings.warn(\"Job is using one or more non-HdfsTarget outputs\" +\n                              \" so it will be run in local mode\")\n                return LocalJobRunner()\n        else:\n            return DefaultHadoopJobRunner()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef writer(self, outputs, stdout, stderr=sys.stderr):\n        for output in outputs:\n            try:\n                output = flatten(output)\n                if self.data_interchange_format == \"json\":\n                    # Only dump one json string, and skip another one, maybe key or value.\n                    output = filter(lambda x: x, output)\n                else:\n                    # JSON is already serialized, so we put `self.serialize` in a else statement.\n                    output = map(self.serialize, output)\n                print(\"\\t\".join(output), file=stdout)\n            except BaseException:\n                print(output, file=stderr)\n                raise", "response": "This method writes the records to stdout and stderr."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef incr_counter(self, *args, **kwargs):\n        threshold = kwargs.get(\"threshold\", self.batch_counter_default)\n        if len(args) == 2:\n            # backwards compatibility with existing hadoop jobs\n            group_name, count = args\n            key = (group_name,)\n        else:\n            group, name, count = args\n            key = (group, name)\n\n        ct = self._counter_dict.get(key, 0)\n        ct += count\n        if ct >= threshold:\n            new_arg = list(key) + [ct]\n            self._incr_counter(*new_arg)\n            ct = 0\n        self._counter_dict[key] = ct", "response": "Increments a Hadoop counter."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _flush_batch_incr_counter(self):\n        for key, count in six.iteritems(self._counter_dict):\n            if count == 0:\n                continue\n            args = list(key) + [count]\n            self._incr_counter(*args)\n            self._counter_dict[key] = 0", "response": "Increments any unflushed counter values."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nincrements a Hadoop counter.", "response": "def _incr_counter(self, *args):\n        \"\"\"\n        Increments a Hadoop counter.\n\n        Note that this seems to be a bit slow, ~1 ms\n\n        Don't overuse this function by updating very frequently.\n        \"\"\"\n        if len(args) == 2:\n            # backwards compatibility with existing hadoop jobs\n            group_name, count = args\n            print('reporter:counter:%s,%s' % (group_name, count), file=sys.stderr)\n        else:\n            group, name, count = args\n            print('reporter:counter:%s,%s,%s' % (group, name, count), file=sys.stderr)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndumping the instance to a file.", "response": "def dump(self, directory=''):\n        \"\"\"\n        Dump instance to file.\n        \"\"\"\n        with self.no_unpicklable_properties():\n            file_name = os.path.join(directory, 'job-instance.pickle')\n            if self.__module__ == '__main__':\n                d = pickle.dumps(self)\n                module_name = os.path.basename(sys.argv[0]).rsplit('.', 1)[0]\n                d = d.replace(b'(c__main__', \"(c\" + module_name)\n                open(file_name, \"wb\").write(d)\n\n            else:\n                pickle.dump(self, open(file_name, \"wb\"))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\niterating over input and call the mapper for each item.", "response": "def _map_input(self, input_stream):\n        \"\"\"\n        Iterate over input and call the mapper for each item.\n        If the job has a parser defined, the return values from the parser will\n        be passed as arguments to the mapper.\n\n        If the input is coded output from a previous run,\n        the arguments will be splitted in key and value.\n        \"\"\"\n        for record in self.reader(input_stream):\n            for output in self.mapper(*record):\n                yield output\n        if self.final_mapper != NotImplemented:\n            for output in self.final_mapper():\n                yield output\n        self._flush_batch_incr_counter()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _reduce_input(self, inputs, reducer, final=NotImplemented):\n        for key, values in groupby(inputs, key=lambda x: self.internal_serialize(x[0])):\n            for output in reducer(self.deserialize(key), (v[1] for v in values)):\n                yield output\n        if final != NotImplemented:\n            for output in final():\n                yield output\n        self._flush_batch_incr_counter()", "response": "Iterate over input and call the reducer for each unique key."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrunning the mapper on the hadoop node.", "response": "def run_mapper(self, stdin=sys.stdin, stdout=sys.stdout):\n        \"\"\"\n        Run the mapper on the hadoop node.\n        \"\"\"\n        self.init_hadoop()\n        self.init_mapper()\n        outputs = self._map_input((line[:-1] for line in stdin))\n        if self.reducer == NotImplemented:\n            self.writer(outputs, stdout)\n        else:\n            self.internal_writer(outputs, stdout)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run_reducer(self, stdin=sys.stdin, stdout=sys.stdout):\n        self.init_hadoop()\n        self.init_reducer()\n        outputs = self._reduce_input(self.internal_reader((line[:-1] for line in stdin)), self.reducer, self.final_reducer)\n        self.writer(outputs, stdout)", "response": "Run the reducer on the hadoop node."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef touch(self, connection=None):\n        self.create_marker_table()\n\n        if connection is None:\n            # TODO: test this\n            connection = self.connect()\n            connection.autocommit = True  # if connection created here, we commit it here\n\n        if self.use_db_timestamps:\n            connection.cursor().execute(\n                \"\"\"INSERT INTO {marker_table} (update_id, target_table)\n                   VALUES (%s, %s)\n                \"\"\".format(marker_table=self.marker_table),\n                (self.update_id, self.table))\n        else:\n            connection.cursor().execute(\n                \"\"\"INSERT INTO {marker_table} (update_id, target_table, inserted)\n                         VALUES (%s, %s, %s);\n                    \"\"\".format(marker_table=self.marker_table),\n                (self.update_id, self.table,\n                 datetime.datetime.now()))", "response": "Mark this update as complete."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget a psycopg2 connection object to the database where the table is.", "response": "def connect(self):\n        \"\"\"\n        Get a psycopg2 connection object to the database where the table is.\n        \"\"\"\n        connection = psycopg2.connect(\n            host=self.host,\n            port=self.port,\n            database=self.database,\n            user=self.user,\n            password=self.password)\n        connection.set_client_encoding('utf-8')\n        return connection"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates marker table if it doesn t exist.", "response": "def create_marker_table(self):\n        \"\"\"\n        Create marker table if it doesn't exist.\n\n        Using a separate connection since the transaction might have to be reset.\n        \"\"\"\n        connection = self.connect()\n        connection.autocommit = True\n        cursor = connection.cursor()\n        if self.use_db_timestamps:\n            sql = \"\"\" CREATE TABLE {marker_table} (\n                      update_id TEXT PRIMARY KEY,\n                      target_table TEXT,\n                      inserted TIMESTAMP DEFAULT NOW())\n                  \"\"\".format(marker_table=self.marker_table)\n        else:\n            sql = \"\"\" CREATE TABLE {marker_table} (\n                      update_id TEXT PRIMARY KEY,\n                      target_table TEXT,\n                      inserted TIMESTAMP);\n                  \"\"\".format(marker_table=self.marker_table)\n\n        try:\n            cursor.execute(sql)\n        except psycopg2.ProgrammingError as e:\n            if e.pgcode == psycopg2.errorcodes.DUPLICATE_TABLE:\n                pass\n            else:\n                raise\n        connection.close()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn tuples or lists corresponding to each row in the table.", "response": "def rows(self):\n        \"\"\"\n        Return/yield tuples or lists corresponding to each row to be inserted.\n        \"\"\"\n        with self.input().open('r') as fobj:\n            for line in fobj:\n                yield line.strip('\\n').split('\\t')"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmaps the value of a column of the table to a string.", "response": "def map_column(self, value):\n        \"\"\"\n        Applied to each column of every row returned by `rows`.\n\n        Default behaviour is to escape special characters and identify any self.null_values.\n        \"\"\"\n        if value in self.null_values:\n            return r'\\\\N'\n        else:\n            return default_escape(six.text_type(value))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a PostgresTarget representing the inserted dataset. Normally you don t override this.", "response": "def output(self):\n        \"\"\"\n        Returns a PostgresTarget representing the inserted dataset.\n\n        Normally you don't override this.\n        \"\"\"\n        return PostgresTarget(\n            host=self.host,\n            database=self.database,\n            user=self.user,\n            password=self.password,\n            table=self.table,\n            update_id=self.update_id,\n            port=self.port\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run(self):\n        if not (self.table and self.columns):\n            raise Exception(\"table and columns need to be specified\")\n\n        connection = self.output().connect()\n        # transform all data generated by rows() using map_column and write data\n        # to a temporary file for import using postgres COPY\n        tmp_dir = luigi.configuration.get_config().get('postgres', 'local-tmp-dir', None)\n        tmp_file = tempfile.TemporaryFile(dir=tmp_dir)\n        n = 0\n        for row in self.rows():\n            n += 1\n            if n % 100000 == 0:\n                logger.info(\"Wrote %d lines\", n)\n            rowstr = self.column_separator.join(self.map_column(val) for val in row)\n            rowstr += \"\\n\"\n            tmp_file.write(rowstr.encode('utf-8'))\n\n        logger.info(\"Done writing, importing at %s\", datetime.datetime.now())\n        tmp_file.seek(0)\n\n        # attempt to copy the data into postgres\n        # if it fails because the target table doesn't exist\n        # try to create it by running self.create_table\n        for attempt in range(2):\n            try:\n                cursor = connection.cursor()\n                self.init_copy(connection)\n                self.copy(cursor, tmp_file)\n                self.post_copy(connection)\n                if self.enable_metadata_columns:\n                    self.post_copy_metacolumns(cursor)\n            except psycopg2.ProgrammingError as e:\n                if e.pgcode == psycopg2.errorcodes.UNDEFINED_TABLE and attempt == 0:\n                    # if first attempt fails with \"relation not found\", try creating table\n                    logger.info(\"Creating table %s\", self.table)\n                    connection.reset()\n                    self.create_table(connection)\n                else:\n                    raise\n            else:\n                break\n\n        # mark as complete in same transaction\n        self.output().touch(connection)\n\n        # commit and clean up\n        connection.commit()\n        connection.close()\n        tmp_file.close()", "response": "Runs the main entry point for the SQL database."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_config(parser=PARSER):\n    parser_class = PARSERS[parser]\n    _check_parser(parser_class, parser)\n    return parser_class.instance()", "response": "Get configs singleton for parser"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding config path into parser.", "response": "def add_config_path(path):\n    \"\"\"Select config parser by file extension and add path into parser.\n    \"\"\"\n    if not os.path.isfile(path):\n        warnings.warn(\"Config file does not exist: {path}\".format(path=path))\n        return False\n\n    # select parser by file extension\n    _base, ext = os.path.splitext(path)\n    if ext and ext[1:] in PARSERS:\n        parser = ext[1:]\n    else:\n        parser = PARSER\n    parser_class = PARSERS[parser]\n\n    _check_parser(parser_class, parser)\n    if parser != PARSER:\n        msg = (\n            \"Config for {added} parser added, but used {used} parser. \"\n            \"Set up right parser via env var: \"\n            \"export LUIGI_CONFIG_PARSER={added}\"\n        )\n        warnings.warn(msg.format(added=parser, used=PARSER))\n\n    # add config path to parser\n    parser_class.add_config_path(path)\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nruns either the mapper, combiner, or reducer from the class instance in the file \"job-instance.pickle\". Arguments: kind -- is either map, combiner, or reduce", "response": "def main(args=None, stdin=sys.stdin, stdout=sys.stdout, print_exception=print_exception):\n    \"\"\"\n    Run either the mapper, combiner, or reducer from the class instance in the file \"job-instance.pickle\".\n\n    Arguments:\n\n    kind -- is either map, combiner, or reduce\n    \"\"\"\n    try:\n        # Set up logging.\n        logging.basicConfig(level=logging.WARN)\n\n        kind = args is not None and args[1] or sys.argv[1]\n        Runner().run(kind, stdin=stdin, stdout=stdout)\n    except Exception as exc:\n        # Dump encoded data that we will try to fetch using mechanize\n        print_exception(exc)\n        raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nruns luigi with command line parsing but raise SystemExit with the configured exit code.", "response": "def run_with_retcodes(argv):\n    \"\"\"\n    Run luigi with command line parsing, but raise ``SystemExit`` with the configured exit code.\n\n    Note: Usually you use the luigi binary directly and don't call this function yourself.\n\n    :param argv: Should (conceptually) be ``sys.argv[1:]``\n    \"\"\"\n    logger = logging.getLogger('luigi-interface')\n    with luigi.cmdline_parser.CmdlineParser.global_instance(argv):\n        retcodes = retcode()\n\n    worker = None\n    try:\n        worker = luigi.interface._run(argv).worker\n    except luigi.interface.PidLockAlreadyTakenExit:\n        sys.exit(retcodes.already_running)\n    except Exception:\n        # Some errors occur before logging is set up, we set it up now\n        env_params = luigi.interface.core()\n        InterfaceLogging.setup(env_params)\n        logger.exception(\"Uncaught exception in luigi\")\n        sys.exit(retcodes.unhandled_exception)\n\n    with luigi.cmdline_parser.CmdlineParser.global_instance(argv):\n        task_sets = luigi.execution_summary._summary_dict(worker)\n        root_task = luigi.execution_summary._root_task(worker)\n        non_empty_categories = {k: v for k, v in task_sets.items() if v}.keys()\n\n    def has(status):\n        assert status in luigi.execution_summary._ORDERED_STATUSES\n        return status in non_empty_categories\n\n    codes_and_conds = (\n        (retcodes.missing_data, has('still_pending_ext')),\n        (retcodes.task_failed, has('failed')),\n        (retcodes.already_running, has('run_by_other_worker')),\n        (retcodes.scheduling_error, has('scheduling_error')),\n        (retcodes.not_run, has('not_run')),\n    )\n    expected_ret_code = max(code * (1 if cond else 0) for code, cond in codes_and_conds)\n\n    if expected_ret_code == 0 and \\\n       root_task not in task_sets[\"completed\"] and \\\n       root_task not in task_sets[\"already_done\"]:\n        sys.exit(retcodes.not_run)\n    else:\n        sys.exit(expected_ret_code)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef find_deps_cli():\n    '''\n    Finds all tasks on all paths from provided CLI task\n    '''\n    cmdline_args = sys.argv[1:]\n    with CmdlineParser.global_instance(cmdline_args) as cp:\n        return find_deps(cp.get_task_obj(), upstream().family)", "response": "Find all tasks on all paths from provided CLI task\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a task s output as a string", "response": "def get_task_output_description(task_output):\n    '''\n    Returns a task's output as a string\n    '''\n    output_description = \"n/a\"\n\n    if isinstance(task_output, RemoteTarget):\n        output_description = \"[SSH] {0}:{1}\".format(task_output._fs.remote_context.host, task_output.path)\n    elif isinstance(task_output, S3Target):\n        output_description = \"[S3] {0}\".format(task_output.path)\n    elif isinstance(task_output, FileSystemTarget):\n        output_description = \"[FileSystem] {0}\".format(task_output.path)\n    elif isinstance(task_output, PostgresTarget):\n        output_description = \"[DB] {0}:{1}\".format(task_output.host, task_output.table)\n    else:\n        output_description = \"to be determined\"\n\n    return output_description"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _constrain_glob(glob, paths, limit=5):\n\n    def digit_set_wildcard(chars):\n        \"\"\"\n        Makes a wildcard expression for the set, a bit readable, e.g. [1-5].\n        \"\"\"\n        chars = sorted(chars)\n        if len(chars) > 1 and ord(chars[-1]) - ord(chars[0]) == len(chars) - 1:\n            return '[%s-%s]' % (chars[0], chars[-1])\n        else:\n            return '[%s]' % ''.join(chars)\n\n    current = {glob: paths}\n    while True:\n        pos = list(current.keys())[0].find('[0-9]')\n        if pos == -1:\n            # no wildcard expressions left to specialize in the glob\n            return list(current.keys())\n        char_sets = {}\n        for g, p in six.iteritems(current):\n            char_sets[g] = sorted({path[pos] for path in p})\n        if sum(len(s) for s in char_sets.values()) > limit:\n            return [g.replace('[0-9]', digit_set_wildcard(char_sets[g]), 1) for g in current]\n        for g, s in six.iteritems(char_sets):\n            for c in s:\n                new_glob = g.replace('[0-9]', c, 1)\n                new_paths = list(filter(lambda p: p[pos] == c, current[g]))\n                current[new_glob] = new_paths\n            del current[g]", "response": "Constrain globs to a list of more specific globs that cover paths and not too much extra."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef most_common(items):\n    counts = {}\n    for i in items:\n        counts.setdefault(i, 0)\n        counts[i] += 1\n    return max(six.iteritems(counts), key=operator.itemgetter(1))", "response": "Returns the most common item in the list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbuild a glob listing existing output paths.", "response": "def _get_per_location_glob(tasks, outputs, regexes):\n    \"\"\"\n    Builds a glob listing existing output paths.\n\n    Esoteric reverse engineering, but worth it given that (compared to an\n    equivalent contiguousness guarantee by naive complete() checks)\n    requests to the filesystem are cut by orders of magnitude, and users\n    don't even have to retrofit existing tasks anyhow.\n    \"\"\"\n    paths = [o.path for o in outputs]\n    # naive, because some matches could be confused by numbers earlier\n    # in path, e.g. /foo/fifa2000k/bar/2000-12-31/00\n    matches = [r.search(p) for r, p in zip(regexes, paths)]\n\n    for m, p, t in zip(matches, paths, tasks):\n        if m is None:\n            raise NotImplementedError(\"Couldn't deduce datehour representation in output path %r of task %s\" % (p, t))\n\n    n_groups = len(matches[0].groups())\n    # the most common position of every group is likely\n    # to be conclusive hit or miss\n    positions = [most_common((m.start(i), m.end(i)) for m in matches)[0] for i in range(1, n_groups + 1)]\n\n    glob = list(paths[0])  # FIXME sanity check that it's the same for all paths\n    for start, end in positions:\n        glob = glob[:start] + ['[0-9]'] * (end - start) + glob[end:]\n    # chop off the last path item\n    # (wouldn't need to if `hadoop fs -ls -d` equivalent were available)\n    return ''.join(glob).rsplit('/', 1)[0]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nyield a set of filesystems and globs per every output location of a task.", "response": "def _get_filesystems_and_globs(datetime_to_task, datetime_to_re):\n    \"\"\"\n    Yields a (filesystem, glob) tuple per every output location of task.\n\n    The task can have one or several FileSystemTarget outputs.\n\n    For convenience, the task can be a luigi.WrapperTask,\n    in which case outputs of all its dependencies are considered.\n    \"\"\"\n    # probe some scattered datetimes unlikely to all occur in paths, other than by being sincere datetime parameter's representations\n    # TODO limit to [self.start, self.stop) so messages are less confusing? Done trivially it can kill correctness\n    sample_datetimes = [datetime(y, m, d, h) for y in range(2000, 2050, 10) for m in range(1, 4) for d in range(5, 8) for h in range(21, 24)]\n    regexes = [re.compile(datetime_to_re(d)) for d in sample_datetimes]\n    sample_tasks = [datetime_to_task(d) for d in sample_datetimes]\n    sample_outputs = [flatten_output(t) for t in sample_tasks]\n\n    for o, t in zip(sample_outputs, sample_tasks):\n        if len(o) != len(sample_outputs[0]):\n            raise NotImplementedError(\"Outputs must be consistent over time, sorry; was %r for %r and %r for %r\" % (o, t, sample_outputs[0], sample_tasks[0]))\n            # TODO fall back on requiring last couple of days? to avoid astonishing blocking when changes like that are deployed\n            # erm, actually it's not hard to test entire hours_back..hours_forward and split into consistent subranges FIXME?\n        for target in o:\n            if not isinstance(target, FileSystemTarget):\n                raise NotImplementedError(\"Output targets must be instances of FileSystemTarget; was %r for %r\" % (target, t))\n\n    for o in zip(*sample_outputs):  # transposed, so here we're iterating over logical outputs, not datetimes\n        glob = _get_per_location_glob(sample_tasks, o, regexes)\n        yield o[0].fs, glob"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _list_existing(filesystem, glob, paths):\n    globs = _constrain_glob(glob, paths)\n    time_start = time.time()\n    listing = []\n    for g in sorted(globs):\n        logger.debug('Listing %s', g)\n        if filesystem.exists(g):\n            listing.extend(filesystem.listdir(g))\n    logger.debug('%d %s listings took %f s to return %d items',\n                 len(globs), filesystem.__class__.__name__, time.time() - time_start, len(listing))\n    return set(listing)", "response": "List all the paths that do in fact exist. Returns a set of all the paths that do in fact exist."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef infer_bulk_complete_from_fs(datetimes, datetime_to_task, datetime_to_re):\n    filesystems_and_globs_by_location = _get_filesystems_and_globs(datetime_to_task, datetime_to_re)\n    paths_by_datetime = [[o.path for o in flatten_output(datetime_to_task(d))] for d in datetimes]\n    listing = set()\n    for (f, g), p in zip(filesystems_and_globs_by_location, zip(*paths_by_datetime)):  # transposed, so here we're iterating over logical outputs, not datetimes\n        listing |= _list_existing(f, g, p)\n\n    # quickly learn everything that's missing\n    missing_datetimes = []\n    for d, p in zip(datetimes, paths_by_datetime):\n        if not set(p) <= listing:\n            missing_datetimes.append(d)\n\n    return missing_datetimes", "response": "Infer bulk completion from a list of datetimes."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the class name of the current instance.", "response": "def of_cls(self):\n        \"\"\"\n        DONT USE. Will be deleted soon. Use ``self.of``!\n        \"\"\"\n        if isinstance(self.of, six.string_types):\n            warnings.warn('When using Range programatically, dont pass \"of\" param as string!')\n            return Register.get_task_cls(self.of)\n        return self.of"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nemit the range events.", "response": "def _emit_metrics(self, missing_datetimes, finite_start, finite_stop):\n        \"\"\"\n        For consistent metrics one should consider the entire range, but\n        it is open (infinite) if stop or start is None.\n\n        Hence make do with metrics respective to the finite simplification.\n        \"\"\"\n        datetimes = self.finite_datetimes(\n            finite_start if self.start is None else min(finite_start, self.parameter_to_datetime(self.start)),\n            finite_stop if self.stop is None else max(finite_stop, self.parameter_to_datetime(self.stop)))\n\n        delay_in_jobs = len(datetimes) - datetimes.index(missing_datetimes[0]) if datetimes and missing_datetimes else 0\n        self.trigger_event(RangeEvent.DELAY, self.of_cls.task_family, delay_in_jobs)\n\n        expected_count = len(datetimes)\n        complete_count = expected_count - len(missing_datetimes)\n        self.trigger_event(RangeEvent.COMPLETE_COUNT, self.of_cls.task_family, complete_count)\n        self.trigger_event(RangeEvent.COMPLETE_FRACTION, self.of_cls.task_family, float(complete_count) / expected_count if expected_count else 1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef missing_datetimes(self, finite_datetimes):\n        return [d for d in finite_datetimes if not self._instantiate_task_cls(self.datetime_to_parameter(d)).complete()]", "response": "Returns a list of missing datetimes."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _missing_datetimes(self, finite_datetimes):\n        try:\n            return self.missing_datetimes(finite_datetimes)\n        except TypeError as ex:\n            if 'missing_datetimes()' in repr(ex):\n                warnings.warn('In your Range* subclass, missing_datetimes() should only take 1 argument (see latest docs)')\n                return self.missing_datetimes(self.of_cls, finite_datetimes)\n            else:\n                raise", "response": "Wrapper for missing_datetimes that returns a list of missing_datetimes."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngive a dictionary of parameters will extract the ranged task parameter value", "response": "def parameters_to_datetime(self, p):\n        \"\"\"\n        Given a dictionary of parameters, will extract the ranged task parameter value\n        \"\"\"\n        dt = p[self._param_name]\n        return datetime(dt.year, dt.month, dt.day)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef finite_datetimes(self, finite_start, finite_stop):\n        datehour_start = datetime(finite_start.year, finite_start.month, finite_start.day, finite_start.hour)\n        datehours = []\n        for i in itertools.count():\n            t = datehour_start + timedelta(hours=i)\n            if t >= finite_stop:\n                return datehours\n            if t >= finite_start:\n                datehours.append(t)", "response": "Simply returns the points in time that correspond to whole hours."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef finite_datetimes(self, finite_start, finite_stop):\n        # Validate that the minutes_interval can divide 60 and it is greater than 0 and lesser than 60\n        if not (0 < self.minutes_interval < 60):\n            raise ParameterException('minutes-interval must be within 0..60')\n        if (60 / self.minutes_interval) * self.minutes_interval != 60:\n            raise ParameterException('minutes-interval does not evenly divide 60')\n        # start of a complete interval, e.g. 20:13 and the interval is 5 -> 20:10\n        start_minute = int(finite_start.minute/self.minutes_interval)*self.minutes_interval\n        datehour_start = datetime(\n            year=finite_start.year,\n            month=finite_start.month,\n            day=finite_start.day,\n            hour=finite_start.hour,\n            minute=start_minute)\n        datehours = []\n        for i in itertools.count():\n            t = datehour_start + timedelta(minutes=i*self.minutes_interval)\n            if t >= finite_stop:\n                return datehours\n            if t >= finite_start:\n                datehours.append(t)", "response": "Simply returns the points in time that correspond to a whole number of minutes intervals."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmarks this update as complete.", "response": "def touch(self, connection=None):\n        \"\"\"\n        Mark this update as complete.\n\n        IMPORTANT, If the marker table doesn't exist,\n        the connection transaction will be aborted and the connection reset.\n        Then the marker table will be created.\n        \"\"\"\n        self.create_marker_table()\n\n        if connection is None:\n            connection = self.connect()\n\n        connection.execute_non_query(\n            \"\"\"IF NOT EXISTS(SELECT 1\n                            FROM {marker_table}\n                            WHERE update_id = %(update_id)s)\n                    INSERT INTO {marker_table} (update_id, target_table)\n                        VALUES (%(update_id)s, %(table)s)\n                ELSE\n                    UPDATE t\n                    SET target_table = %(table)s\n                        , inserted = GETDATE()\n                    FROM {marker_table} t\n                    WHERE update_id = %(update_id)s\n              \"\"\".format(marker_table=self.marker_table),\n            {\"update_id\": self.update_id, \"table\": self.table})\n\n        # make sure update is properly marked\n        assert self.exists(connection)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a SQL Server connection and return a connection object", "response": "def connect(self):\n        \"\"\"\n        Create a SQL Server connection and return a connection object\n        \"\"\"\n        connection = _mssql.connect(user=self.user,\n                                    password=self.password,\n                                    server=self.host,\n                                    port=self.port,\n                                    database=self.database)\n        return connection"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates marker table if it doesn t exist.", "response": "def create_marker_table(self):\n        \"\"\"\n        Create marker table if it doesn't exist.\n        Use a separate connection since the transaction might have to be reset.\n        \"\"\"\n        connection = self.connect()\n        try:\n            connection.execute_non_query(\n                \"\"\" CREATE TABLE {marker_table} (\n                        id            BIGINT    NOT NULL IDENTITY(1,1),\n                        update_id     VARCHAR(128)  NOT NULL,\n                        target_table  VARCHAR(128),\n                        inserted      DATETIME DEFAULT(GETDATE()),\n                        PRIMARY KEY (update_id)\n                    )\n                \"\"\"\n                .format(marker_table=self.marker_table)\n            )\n        except _mssql.MSSQLDatabaseException as e:\n            # Table already exists code\n            if e.number == 2714:\n                pass\n            else:\n                raise\n        connection.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretrieves an opener for the given protocol name", "response": "def get_opener(self, name):\n        \"\"\"Retrieve an opener for the given protocol\n\n        :param name: name of the opener to open\n        :type name: string\n        :raises NoOpenerError: if no opener has been registered of that name\n\n        \"\"\"\n        if name not in self.registry:\n            raise NoOpenerError(\"No opener for %s\" % name)\n        index = self.registry[name]\n        return self.openers[index]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd an opener to the registry", "response": "def add(self, opener):\n        \"\"\"Adds an opener to the registry\n\n        :param opener: Opener object\n        :type opener: Opener inherited object\n\n        \"\"\"\n\n        index = len(self.openers)\n        self.openers[index] = opener\n        for name in opener.names:\n            self.registry[name] = index"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef open(self, target_uri, **kwargs):\n        target = urlsplit(target_uri, scheme=self.default_opener)\n\n        opener = self.get_opener(target.scheme)\n        query = opener.conform_query(target.query)\n\n        target = opener.get_target(\n            target.scheme,\n            target.path,\n            target.fragment,\n            target.username,\n            target.password,\n            target.hostname,\n            target.port,\n            query,\n            **kwargs\n        )\n        target.opener_path = target_uri\n\n        return target", "response": "Open target uri.\n\n        :param target_uri: Uri to open\n        :type target_uri: string\n\n        :returns: Target object"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef conform_query(cls, query):\n        query = parse_qs(query, keep_blank_values=True)\n\n        # Remove any unexpected keywords from the query string.\n        if cls.filter_kwargs:\n            query = {x: y for x, y in query.items() if x in cls.allowed_kwargs}\n\n        for key, vals in query.items():\n            # Multiple values of the same name could be passed use first\n            # Also params without strings will be treated as true values\n            if cls.allowed_kwargs.get(key, False):\n                val = json.loads(vals[0] or 'true')\n            else:\n                val = vals[0] or 'true'\n\n            query[key] = val\n\n        return query", "response": "Converts the query string from a target uri uses\n            cls. allowed_kwargs and cls. filter_kwargs to drive logic."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_target(cls, scheme, path, fragment, username,\n                   password, hostname, port, query, **kwargs):\n        \"\"\"Override this method to use values from the parsed uri to initialize\n        the expected target.\n\n        \"\"\"\n        raise NotImplementedError(\"get_target must be overridden\")", "response": "Override this method to use values from the parsed uri to initialize\nTaxonomy the expected target."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nscheduling a list of tasks and runs them.", "response": "def _schedule_and_run(tasks, worker_scheduler_factory=None, override_defaults=None):\n    \"\"\"\n    :param tasks:\n    :param worker_scheduler_factory:\n    :param override_defaults:\n    :return: True if all tasks and their dependencies were successfully run (or already completed);\n             False if any error occurred. It will return a detailed response of type LuigiRunResult\n             instead of a boolean if detailed_summary=True.\n    \"\"\"\n\n    if worker_scheduler_factory is None:\n        worker_scheduler_factory = _WorkerSchedulerFactory()\n    if override_defaults is None:\n        override_defaults = {}\n    env_params = core(**override_defaults)\n\n    InterfaceLogging.setup(env_params)\n\n    kill_signal = signal.SIGUSR1 if env_params.take_lock else None\n    if (not env_params.no_lock and\n            not(lock.acquire_for(env_params.lock_pid_dir, env_params.lock_size, kill_signal))):\n        raise PidLockAlreadyTakenExit()\n\n    if env_params.local_scheduler:\n        sch = worker_scheduler_factory.create_local_scheduler()\n    else:\n        if env_params.scheduler_url != '':\n            url = env_params.scheduler_url\n        else:\n            url = 'http://{host}:{port:d}/'.format(\n                host=env_params.scheduler_host,\n                port=env_params.scheduler_port,\n            )\n        sch = worker_scheduler_factory.create_remote_scheduler(url=url)\n\n    worker = worker_scheduler_factory.create_worker(\n        scheduler=sch, worker_processes=env_params.workers, assistant=env_params.assistant)\n\n    success = True\n    logger = logging.getLogger('luigi-interface')\n    with worker:\n        for t in tasks:\n            success &= worker.add(t, env_params.parallel_scheduling, env_params.parallel_scheduling_processes)\n        logger.info('Done scheduling tasks')\n        success &= worker.run()\n    luigi_run_result = LuigiRunResult(worker, success)\n    logger.info(luigi_run_result.summary_text)\n    return luigi_run_result"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nruns luigi command line using argparse.", "response": "def run(*args, **kwargs):\n    \"\"\"\n    Please dont use. Instead use `luigi` binary.\n\n    Run from cmdline using argparse.\n\n    :param use_dynamic_argparse: Deprecated and ignored\n    \"\"\"\n    luigi_run_result = _run(*args, **kwargs)\n    return luigi_run_result if kwargs.get('detailed_summary') else luigi_run_result.scheduling_succeeded"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrun the given tasks in luigi.", "response": "def build(tasks, worker_scheduler_factory=None, detailed_summary=False, **env_params):\n    \"\"\"\n    Run internally, bypassing the cmdline parsing.\n\n    Useful if you have some luigi code that you want to run internally.\n    Example:\n\n    .. code-block:: python\n\n        luigi.build([MyTask1(), MyTask2()], local_scheduler=True)\n\n    One notable difference is that `build` defaults to not using\n    the identical process lock. Otherwise, `build` would only be\n    callable once from each process.\n\n    :param tasks:\n    :param worker_scheduler_factory:\n    :param env_params:\n    :return: True if there were no scheduling errors, even if tasks may fail.\n    \"\"\"\n    if \"no_lock\" not in env_params:\n        env_params[\"no_lock\"] = True\n\n    luigi_run_result = _schedule_and_run(tasks, worker_scheduler_factory, override_defaults=env_params)\n    return luigi_run_result if detailed_summary else luigi_run_result.scheduling_succeeded"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmark this update as complete.", "response": "def touch(self, connection=None):\n        \"\"\"\n        Mark this update as complete.\n\n        IMPORTANT, If the marker table doesn't exist,\n        the connection transaction will be aborted and the connection reset.\n        Then the marker table will be created.\n        \"\"\"\n        self.create_marker_table()\n\n        if connection is None:\n            connection = self.connect()\n            connection.autocommit = True  # if connection created here, we commit it here\n\n        connection.cursor().execute(\n            \"\"\"INSERT INTO {marker_table} (update_id, target_table)\n               VALUES (%s, %s)\n               ON DUPLICATE KEY UPDATE\n               update_id = VALUES(update_id)\n            \"\"\".format(marker_table=self.marker_table),\n            (self.update_id, self.table)\n        )\n        # make sure update is properly marked\n        assert self.exists(connection)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating marker table if it doesn t exist.", "response": "def create_marker_table(self):\n        \"\"\"\n        Create marker table if it doesn't exist.\n\n        Using a separate connection since the transaction might have to be reset.\n        \"\"\"\n        connection = self.connect(autocommit=True)\n        cursor = connection.cursor()\n        try:\n            cursor.execute(\n                \"\"\" CREATE TABLE {marker_table} (\n                        id            BIGINT(20)    NOT NULL AUTO_INCREMENT,\n                        update_id     VARCHAR(128)  NOT NULL,\n                        target_table  VARCHAR(128),\n                        inserted      TIMESTAMP DEFAULT NOW(),\n                        PRIMARY KEY (update_id),\n                        KEY id (id)\n                    )\n                \"\"\"\n                .format(marker_table=self.marker_table)\n            )\n        except mysql.connector.Error as e:\n            if e.errno == errorcode.ER_TABLE_EXISTS_ERROR:\n                pass\n            else:\n                raise\n        connection.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef run(self):\n        if not (self.table and self.columns):\n            raise Exception(\"table and columns need to be specified\")\n\n        connection = self.output().connect()\n\n        # attempt to copy the data into mysql\n        # if it fails because the target table doesn't exist\n        # try to create it by running self.create_table\n        for attempt in range(2):\n            try:\n                cursor = connection.cursor()\n                print(\"caling init copy...\")\n                self.init_copy(connection)\n                self.copy(cursor)\n                self.post_copy(connection)\n                if self.enable_metadata_columns:\n                    self.post_copy_metacolumns(cursor)\n            except Error as err:\n                if err.errno == errorcode.ER_NO_SUCH_TABLE and attempt == 0:\n                    # if first attempt fails with \"relation not found\", try creating table\n                    # logger.info(\"Creating table %s\", self.table)\n                    connection.reconnect()\n                    self.create_table(connection)\n                else:\n                    raise\n            else:\n                break\n\n        # mark as complete in same transaction\n        self.output().touch(connection)\n        connection.commit()\n        connection.close()", "response": "Runs the main loop of the main loop."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fix_paths(job):\n    tmp_files = []\n    args = []\n    for x in job.args():\n        if isinstance(x, luigi.contrib.hdfs.HdfsTarget):  # input/output\n            if x.exists() or not job.atomic_output():  # input\n                args.append(x.path)\n            else:  # output\n                x_path_no_slash = x.path[:-1] if x.path[-1] == '/' else x.path\n                y = luigi.contrib.hdfs.HdfsTarget(x_path_no_slash + '-luigi-tmp-%09d' % random.randrange(0, 1e10))\n                tmp_files.append((y, x_path_no_slash))\n                logger.info('Using temp path: %s for path %s', y.path, x.path)\n                args.append(y.path)\n        else:\n            try:\n                # hopefully the target has a path to use\n                args.append(x.path)\n            except AttributeError:\n                # if there's no path then hope converting it to a string will work\n                args.append(str(x))\n\n    return (tmp_files, args)", "response": "Coerce input arguments to use temporary files when used for output."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_active_queue(self):\n\n        # Get dict of active queues keyed by name\n        queues = {q['jobQueueName']: q for q in self._client.describe_job_queues()['jobQueues']\n                  if q['state'] == 'ENABLED' and q['status'] == 'VALID'}\n        if not queues:\n            raise Exception('No job queues with state=ENABLED and status=VALID')\n\n        # Pick the first queue as default\n        return list(queues.keys())[0]", "response": "Get name of first active job queue"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nretrieves the first job ID matching the given name", "response": "def get_job_id_from_name(self, job_name):\n        \"\"\"Retrieve the first job ID matching the given name\"\"\"\n        jobs = self._client.list_jobs(jobQueue=self._queue, jobStatus='RUNNING')['jobSummaryList']\n        matching_jobs = [job for job in jobs if job['jobName'] == job_name]\n        if matching_jobs:\n            return matching_jobs[0]['jobId']"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_job_status(self, job_id):\n        response = self._client.describe_jobs(jobs=[job_id])\n\n        # Error checking\n        status_code = response['ResponseMetadata']['HTTPStatusCode']\n        if status_code != 200:\n            msg = 'Job status request received status code {0}:\\n{1}'\n            raise Exception(msg.format(status_code, response))\n\n        return response['jobs'][0]['status']", "response": "Retrieve task statuses from ECS API\n\n       "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving log stream from CloudWatch", "response": "def get_logs(self, log_stream_name, get_last=50):\n        \"\"\"Retrieve log stream from CloudWatch\"\"\"\n        response = self._log_client.get_log_events(\n            logGroupName='/aws/batch/job',\n            logStreamName=log_stream_name,\n            startFromHead=False)\n        events = response['events']\n        return '\\n'.join(e['message'] for e in events[-get_last:])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef submit_job(self, job_definition, parameters, job_name=None, queue=None):\n        if job_name is None:\n            job_name = _random_id()\n        response = self._client.submit_job(\n            jobName=job_name,\n            jobQueue=queue or self.get_active_queue(),\n            jobDefinition=job_definition,\n            parameters=parameters\n        )\n        return response['jobId']", "response": "Wrap submit_job with useful defaults"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwait until the job is finished or raise BatchJobException", "response": "def wait_on_job(self, job_id):\n        \"\"\"Poll task status until STOPPED\"\"\"\n\n        while True:\n            status = self.get_job_status(job_id)\n            if status == 'SUCCEEDED':\n                logger.info('Batch job {} SUCCEEDED'.format(job_id))\n                return True\n            elif status == 'FAILED':\n                # Raise and notify if job failed\n                jobs = self._client.describe_jobs(jobs=[job_id])['jobs']\n                job_str = json.dumps(jobs, indent=4)\n                logger.debug('Job details:\\n' + job_str)\n\n                log_stream_name = jobs[0]['attempts'][0]['container']['logStreamName']\n                logs = self.get_logs(log_stream_name)\n                raise BatchJobException('Job {} failed: {}'.format(\n                    job_id, logs))\n\n            time.sleep(self.poll_time)\n            logger.debug('Batch job status for job {0}: {1}'.format(\n                job_id, status))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef register_job_definition(self, json_fpath):\n        with open(json_fpath) as f:\n            job_def = json.load(f)\n        response = self._client.register_job_definition(**job_def)\n        status_code = response['ResponseMetadata']['HTTPStatusCode']\n        if status_code != 200:\n            msg = 'Register job definition request received status code {0}:\\n{1}'\n            raise Exception(msg.format(status_code, response))\n        return response", "response": "Register a job definition with AWS Batch using a JSON file"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef main(args=sys.argv):\n    try:\n        tarball = \"--no-tarball\" not in args\n        # Set up logging.\n        logging.basicConfig(level=logging.WARN)\n        work_dir = args[1]\n        assert os.path.exists(work_dir), \"First argument to sge_runner.py must be a directory that exists\"\n        project_dir = args[2]\n        sys.path.append(project_dir)\n        _do_work_on_compute_node(work_dir, tarball)\n    except Exception as e:\n        # Dump encoded data that we will try to fetch using mechanize\n        print(e)\n        raise", "response": "This is the main entry point for the sge_runner. py."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef print_tree(task, indent='', last=True):\n    '''\n    Return a string representation of the tasks, their statuses/parameters in a dependency tree format\n    '''\n    # dont bother printing out warnings about tasks with no output\n    with warnings.catch_warnings():\n        warnings.filterwarnings(action='ignore', message='Task .* without outputs has no custom complete\\\\(\\\\) method')\n        is_task_complete = task.complete()\n    is_complete = (bcolors.OKGREEN + 'COMPLETE' if is_task_complete else bcolors.OKBLUE + 'PENDING') + bcolors.ENDC\n    name = task.__class__.__name__\n    params = task.to_str_params(only_significant=True)\n    result = '\\n' + indent\n    if(last):\n        result += '\u2514\u2500--'\n        indent += '   '\n    else:\n        result += '|--'\n        indent += '|  '\n    result += '[{0}-{1} ({2})]'.format(name, params, is_complete)\n    children = flatten(task.requires())\n    for index, child in enumerate(children):\n        result += print_tree(child, indent, (index+1) == len(children))\n    return result", "response": "Return a string representation of the tasks their statuses and parameters in a dependency tree format"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _urljoin(base, url):\n    parsed = urlparse(base)\n    scheme = parsed.scheme\n    return urlparse(\n        urljoin(parsed._replace(scheme='http').geturl(), url)\n    )._replace(scheme=scheme).geturl()", "response": "Join relative URLs to base URL like urllib. parse. urljoin but support\n    arbitrary URIs."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef dataset_exists(self, dataset):\n\n        try:\n            response = self.client.datasets().get(projectId=dataset.project_id,\n                                                  datasetId=dataset.dataset_id).execute()\n            if dataset.location is not None:\n                fetched_location = response.get('location')\n                if dataset.location != fetched_location:\n                    raise Exception('''Dataset already exists with regional location {}. Can't use {}.'''.format(\n                        fetched_location if fetched_location is not None else 'unspecified',\n                        dataset.location))\n\n        except http.HttpError as ex:\n            if ex.resp.status == 404:\n                return False\n            raise\n\n        return True", "response": "Returns whether the given dataset exists."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef table_exists(self, table):\n        if not self.dataset_exists(table.dataset):\n            return False\n\n        try:\n            self.client.tables().get(projectId=table.project_id,\n                                     datasetId=table.dataset_id,\n                                     tableId=table.table_id).execute()\n        except http.HttpError as ex:\n            if ex.resp.status == 404:\n                return False\n            raise\n\n        return True", "response": "Returns whether the given table exists."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a new dataset with the default permissions.", "response": "def make_dataset(self, dataset, raise_if_exists=False, body=None):\n        \"\"\"Creates a new dataset with the default permissions.\n\n           :param dataset:\n           :type dataset: BQDataset\n           :param raise_if_exists: whether to raise an exception if the dataset already exists.\n           :raises luigi.target.FileAlreadyExists: if raise_if_exists=True and the dataset exists\n        \"\"\"\n\n        if body is None:\n            body = {}\n\n        try:\n            # Construct a message body in the format required by\n            # https://developers.google.com/resources/api-libraries/documentation/bigquery/v2/python/latest/bigquery_v2.datasets.html#insert\n            body['datasetReference'] = {\n                'projectId': dataset.project_id,\n                'datasetId': dataset.dataset_id\n            }\n            if dataset.location is not None:\n                body['location'] = dataset.location\n            self.client.datasets().insert(projectId=dataset.project_id, body=body).execute()\n        except http.HttpError as ex:\n            if ex.resp.status == 409:\n                if raise_if_exists:\n                    raise luigi.target.FileAlreadyExists()\n            else:\n                raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef delete_dataset(self, dataset, delete_nonempty=True):\n\n        if not self.dataset_exists(dataset):\n            return\n\n        self.client.datasets().delete(projectId=dataset.project_id,\n                                      datasetId=dataset.dataset_id,\n                                      deleteContents=delete_nonempty).execute()", "response": "Deletes a dataset if it exists."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndeletes a table if it exists.", "response": "def delete_table(self, table):\n        \"\"\"Deletes a table, if it exists.\n\n           :param table:\n           :type table: BQTable\n        \"\"\"\n\n        if not self.table_exists(table):\n            return\n\n        self.client.tables().delete(projectId=table.project_id,\n                                    datasetId=table.dataset_id,\n                                    tableId=table.table_id).execute()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the list of datasets in a given project.", "response": "def list_datasets(self, project_id):\n        \"\"\"Returns the list of datasets in a given project.\n\n           :param project_id:\n           :type project_id: str\n        \"\"\"\n\n        request = self.client.datasets().list(projectId=project_id,\n                                              maxResults=1000)\n        response = request.execute()\n\n        while response is not None:\n            for ds in response.get('datasets', []):\n                yield ds['datasetReference']['datasetId']\n\n            request = self.client.datasets().list_next(request, response)\n            if request is None:\n                break\n\n            response = request.execute()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the list of tables in a given dataset.", "response": "def list_tables(self, dataset):\n        \"\"\"Returns the list of tables in a given dataset.\n\n           :param dataset:\n           :type dataset: BQDataset\n        \"\"\"\n\n        request = self.client.tables().list(projectId=dataset.project_id,\n                                            datasetId=dataset.dataset_id,\n                                            maxResults=1000)\n        response = request.execute()\n\n        while response is not None:\n            for t in response.get('tables', []):\n                yield t['tableReference']['tableId']\n\n            request = self.client.tables().list_next(request, response)\n            if request is None:\n                break\n\n            response = request.execute()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the SQL query for a view.", "response": "def get_view(self, table):\n        \"\"\"Returns the SQL query for a view, or None if it doesn't exist or is not a view.\n\n        :param table: The table containing the view.\n        :type table: BQTable\n        \"\"\"\n\n        request = self.client.tables().get(projectId=table.project_id,\n                                           datasetId=table.dataset_id,\n                                           tableId=table.table_id)\n\n        try:\n            response = request.execute()\n        except http.HttpError as ex:\n            if ex.resp.status == 404:\n                return None\n            raise\n\n        return response['view']['query'] if 'view' in response else None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef update_view(self, table, view):\n\n        body = {\n            'tableReference': {\n                'projectId': table.project_id,\n                'datasetId': table.dataset_id,\n                'tableId': table.table_id\n            },\n            'view': {\n                'query': view\n            }\n        }\n\n        if self.table_exists(table):\n            self.client.tables().update(projectId=table.project_id,\n                                        datasetId=table.dataset_id,\n                                        tableId=table.table_id,\n                                        body=body).execute()\n        else:\n            self.client.tables().insert(projectId=table.project_id,\n                                        datasetId=table.dataset_id,\n                                        body=body).execute()", "response": "Updates the SQL query for a view."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrun a BigQuery job.", "response": "def run_job(self, project_id, body, dataset=None):\n        \"\"\"Runs a BigQuery \"job\". See the documentation for the format of body.\n\n           .. note::\n               You probably don't need to use this directly. Use the tasks defined below.\n\n           :param dataset:\n           :type dataset: BQDataset\n        \"\"\"\n\n        if dataset and not self.dataset_exists(dataset):\n            self.make_dataset(dataset)\n\n        new_job = self.client.jobs().insert(projectId=project_id, body=body).execute()\n        job_id = new_job['jobReference']['jobId']\n        logger.info('Started import job %s:%s', project_id, job_id)\n        while True:\n            status = self.client.jobs().get(projectId=project_id, jobId=job_id).execute(num_retries=10)\n            if status['status']['state'] == 'DONE':\n                if status['status'].get('errorResult'):\n                    raise Exception('BigQuery job failed: {}'.format(status['status']['errorResult']))\n                return\n\n            logger.info('Waiting for job %s:%s to complete...', project_id, job_id)\n            time.sleep(5)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef copy(self,\n             source_table,\n             dest_table,\n             create_disposition=CreateDisposition.CREATE_IF_NEEDED,\n             write_disposition=WriteDisposition.WRITE_TRUNCATE):\n        \"\"\"Copies (or appends) a table to another table.\n\n            :param source_table:\n            :type source_table: BQTable\n            :param dest_table:\n            :type dest_table: BQTable\n            :param create_disposition: whether to create the table if needed\n            :type create_disposition: CreateDisposition\n            :param write_disposition: whether to append/truncate/fail if the table exists\n            :type write_disposition: WriteDisposition\n        \"\"\"\n\n        job = {\n            \"configuration\": {\n                \"copy\": {\n                    \"sourceTable\": {\n                        \"projectId\": source_table.project_id,\n                        \"datasetId\": source_table.dataset_id,\n                        \"tableId\": source_table.table_id,\n                    },\n                    \"destinationTable\": {\n                        \"projectId\": dest_table.project_id,\n                        \"datasetId\": dest_table.dataset_id,\n                        \"tableId\": dest_table.table_id,\n                    },\n                    \"createDisposition\": create_disposition,\n                    \"writeDisposition\": write_disposition,\n                }\n            }\n        }\n\n        self.run_job(dest_table.project_id, job, dataset=dest_table.dataset)", "response": "Copies or appends a table to another table."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef source_uris(self):\n        return [x.path for x in luigi.task.flatten(self.input())]", "response": "The fully - qualified URIs that point to your data in Google Cloud Storage."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef Popen(self, cmd, **kwargs):\n        prefixed_cmd = self._prepare_cmd(cmd)\n        return subprocess.Popen(prefixed_cmd, **kwargs)", "response": "A wrapper for subprocess. Popen."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_output(self, cmd):\n        p = self.Popen(cmd, stdout=subprocess.PIPE)\n        output, _ = p.communicate()\n        if p.returncode != 0:\n            raise RemoteCalledProcessError(p.returncode, cmd, self.host, output=output)\n        return output", "response": "Execute a shell command remotely and return the output."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef tunnel(self, local_port, remote_port=None, remote_host=\"localhost\"):\n        tunnel_host = \"{0}:{1}:{2}\".format(local_port, remote_host, remote_port)\n        proc = self.Popen(\n            # cat so we can shut down gracefully by closing stdin\n            [\"-L\", tunnel_host, \"echo -n ready && cat\"],\n            stdin=subprocess.PIPE,\n            stdout=subprocess.PIPE,\n        )\n        # make sure to get the data so we know the connection is established\n        ready = proc.stdout.read(5)\n        assert ready == b\"ready\", \"Didn't get ready from remote echo\"\n        yield  # user code executed here\n        proc.communicate()\n        assert proc.returncode == 0, \"Tunnel process did an unclean exit (returncode %s)\" % (proc.returncode,)", "response": "Tunnel a local host and remote port."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef isdir(self, path):\n        try:\n            self.remote_context.check_output([\"test\", \"-d\", path])\n        except subprocess.CalledProcessError as e:\n            if e.returncode == 1:\n                return False\n            else:\n                raise\n        return True", "response": "Return True if directory at path exist False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves file or directory at location path.", "response": "def remove(self, path, recursive=True):\n        \"\"\"\n        Remove file or directory at location `path`.\n        \"\"\"\n        if recursive:\n            cmd = [\"rm\", \"-r\", path]\n        else:\n            cmd = [\"rm\", path]\n\n        self.remote_context.check_output(cmd)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn command of process.", "response": "def getpcmd(pid):\n    \"\"\"\n    Returns command of process.\n\n    :param pid:\n    \"\"\"\n    if os.name == \"nt\":\n        # Use wmic command instead of ps on Windows.\n        cmd = 'wmic path win32_process where ProcessID=%s get Commandline 2> nul' % (pid, )\n        with os.popen(cmd, 'r') as p:\n            lines = [line for line in p.readlines() if line.strip(\"\\r\\n \") != \"\"]\n            if lines:\n                _, val = lines\n                return val\n    elif sys.platform == \"darwin\":\n        # Use pgrep instead of /proc on macOS.\n        pidfile = \".%d.pid\" % (pid, )\n        with open(pidfile, 'w') as f:\n            f.write(str(pid))\n        try:\n            p = Popen(['pgrep', '-lf', '-F', pidfile], stdout=PIPE)\n            stdout, _ = p.communicate()\n            line = stdout.decode('utf8').strip()\n            if line:\n                _, scmd = line.split(' ', 1)\n                return scmd\n        finally:\n            os.unlink(pidfile)\n    else:\n        # Use the /proc filesystem\n        # At least on android there have been some issues with not all\n        # process infos being readable. In these cases using the `ps` command\n        # worked. See the pull request at\n        # https://github.com/spotify/luigi/pull/1876\n        try:\n            with open('/proc/{0}/cmdline'.format(pid), 'r') as fh:\n                if six.PY3:\n                    return fh.read().replace('\\0', ' ').rstrip()\n                else:\n                    return fh.read().replace('\\0', ' ').decode('utf8').rstrip()\n        except IOError:\n            # the system may not allow reading the command line\n            # of a process owned by another user\n            pass\n\n    # Fallback instead of None, for e.g. Cygwin where -o is an \"unknown option\" for the ps command:\n    return '[PROCESS_WITH_PID={}]'.format(pid)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef acquire_for(pid_dir, num_available=1, kill_signal=None):\n\n    my_pid, my_cmd, pid_file = get_info(pid_dir)\n\n    # Create a pid file if it does not exist\n    try:\n        os.mkdir(pid_dir)\n        os.chmod(pid_dir, 0o777)\n    except OSError as exc:\n        if exc.errno != errno.EEXIST:\n            raise\n        pass\n\n    # Let variable \"pids\" be all pids who exist in the .pid-file who are still\n    # about running the same command.\n    pids = {pid for pid in _read_pids_file(pid_file) if getpcmd(pid) == my_cmd}\n\n    if kill_signal is not None:\n        for pid in pids:\n            os.kill(pid, kill_signal)\n        print('Sent kill signal to Pids: {}'.format(pids))\n        # We allow for the killer to progress, yet we don't want these to stack\n        # up! So we only allow it once.\n        num_available += 1\n\n    if len(pids) >= num_available:\n        # We are already running under a different pid\n        print('Pid(s) {} already running'.format(pids))\n        if kill_signal is not None:\n            print('Note: There have (probably) been 1 other \"--take-lock\"'\n                  ' process which continued to run! Probably no need to run'\n                  ' this one as well.')\n        return False\n\n    _write_pids_file(pid_file, pids | {my_pid})\n\n    return True", "response": "Acquires a new process with the same name."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_failure(self):\n        failure_time = time.time()\n\n        if not self.first_failure_time:\n            self.first_failure_time = failure_time\n\n        self.failures.append(failure_time)", "response": "Add a failure event with the current timestamp."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef num_failures(self):\n        min_time = time.time() - self.window\n\n        while self.failures and self.failures[0] < min_time:\n            self.failures.popleft()\n\n        return len(self.failures)", "response": "Return the number of failures in the window."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn True if the assistant is trivial.", "response": "def is_trivial_worker(self, state):\n        \"\"\"\n        If it's not an assistant having only tasks that are without\n        requirements.\n\n        We have to pass the state parameter for optimization reasons.\n        \"\"\"\n        if self.assistant:\n            return False\n        return all(not task.resources for task in self.get_tasks(state, PENDING))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef num_pending_tasks(self):\n        return len(self._status_tasks[PENDING]) + len(self._status_tasks[RUNNING])", "response": "Return how many pending tasks are in the queue. O ( 1 )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _update_priority(self, task, prio, worker):\n        task.priority = prio = max(prio, task.priority)\n        for dep in task.deps or []:\n            t = self._state.get_task(dep)\n            if t is not None and prio > t.priority:\n                self._update_priority(t, prio, worker)", "response": "Update priority of the given task."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_task(self, task_id=None, status=PENDING, runnable=True,\n                 deps=None, new_deps=None, expl=None, resources=None,\n                 priority=0, family='', module=None, params=None, param_visibilities=None, accepts_messages=False,\n                 assistant=False, tracking_url=None, worker=None, batchable=None,\n                 batch_id=None, retry_policy_dict=None, owners=None, **kwargs):\n        \"\"\"\n        * add task identified by task_id if it doesn't exist\n        * if deps is not None, update dependency list\n        * update status of task\n        * add additional workers/stakeholders\n        * update priority when needed\n        \"\"\"\n        assert worker is not None\n        worker_id = worker\n        worker = self._update_worker(worker_id)\n\n        resources = {} if resources is None else resources.copy()\n\n        if retry_policy_dict is None:\n            retry_policy_dict = {}\n\n        retry_policy = self._generate_retry_policy(retry_policy_dict)\n\n        if worker.enabled:\n            _default_task = self._make_task(\n                task_id=task_id, status=PENDING, deps=deps, resources=resources,\n                priority=priority, family=family, module=module, params=params, param_visibilities=param_visibilities,\n            )\n        else:\n            _default_task = None\n\n        task = self._state.get_task(task_id, setdefault=_default_task)\n\n        if task is None or (task.status != RUNNING and not worker.enabled):\n            return\n\n        # for setting priority, we'll sometimes create tasks with unset family and params\n        if not task.family:\n            task.family = family\n        if not getattr(task, 'module', None):\n            task.module = module\n        if not getattr(task, 'param_visibilities', None):\n            task.param_visibilities = _get_default(param_visibilities, {})\n        if not task.params:\n            task.set_params(params)\n\n        if batch_id is not None:\n            task.batch_id = batch_id\n        if status == RUNNING and not task.worker_running:\n            task.worker_running = worker_id\n            if batch_id:\n                # copy resources_running of the first batch task\n                batch_tasks = self._state.get_batch_running_tasks(batch_id)\n                task.resources_running = batch_tasks[0].resources_running.copy()\n            task.time_running = time.time()\n\n        if accepts_messages is not None:\n            task.accepts_messages = accepts_messages\n\n        if tracking_url is not None or task.status != RUNNING:\n            task.tracking_url = tracking_url\n            if task.batch_id is not None:\n                for batch_task in self._state.get_batch_running_tasks(task.batch_id):\n                    batch_task.tracking_url = tracking_url\n\n        if batchable is not None:\n            task.batchable = batchable\n\n        if task.remove is not None:\n            task.remove = None  # unmark task for removal so it isn't removed after being added\n\n        if expl is not None:\n            task.expl = expl\n            if task.batch_id is not None:\n                for batch_task in self._state.get_batch_running_tasks(task.batch_id):\n                    batch_task.expl = expl\n\n        task_is_not_running = task.status not in (RUNNING, BATCH_RUNNING)\n        task_started_a_run = status in (DONE, FAILED, RUNNING)\n        running_on_this_worker = task.worker_running == worker_id\n        if task_is_not_running or (task_started_a_run and running_on_this_worker) or new_deps:\n            # don't allow re-scheduling of task while it is running, it must either fail or succeed on the worker actually running it\n            if status != task.status or status == PENDING:\n                # Update the DB only if there was a acctual change, to prevent noise.\n                # We also check for status == PENDING b/c that's the default value\n                # (so checking for status != task.status woule lie)\n                self._update_task_history(task, status)\n            self._state.set_status(task, PENDING if status == SUSPENDED else status, self._config)\n\n        if status == FAILED and self._config.batch_emails:\n            batched_params, _ = self._state.get_batcher(worker_id, family)\n            if batched_params:\n                unbatched_params = {\n                    param: value\n                    for param, value in six.iteritems(task.params)\n                    if param not in batched_params\n                }\n            else:\n                unbatched_params = task.params\n            try:\n                expl_raw = json.loads(expl)\n            except ValueError:\n                expl_raw = expl\n\n            self._email_batcher.add_failure(\n                task.pretty_id, task.family, unbatched_params, expl_raw, owners)\n            if task.status == DISABLED:\n                self._email_batcher.add_disable(\n                    task.pretty_id, task.family, unbatched_params, owners)\n\n        if deps is not None:\n            task.deps = set(deps)\n\n        if new_deps is not None:\n            task.deps.update(new_deps)\n\n        if resources is not None:\n            task.resources = resources\n\n        if worker.enabled and not assistant:\n            task.stakeholders.add(worker_id)\n\n            # Task dependencies might not exist yet. Let's create dummy tasks for them for now.\n            # Otherwise the task dependencies might end up being pruned if scheduling takes a long time\n            for dep in task.deps or []:\n                t = self._state.get_task(dep, setdefault=self._make_task(task_id=dep, status=UNKNOWN, deps=None, priority=priority))\n                t.stakeholders.add(worker_id)\n\n        self._update_priority(task, priority, worker_id)\n\n        # Because some tasks (non-dynamic dependencies) are `_make_task`ed\n        # before we know their retry_policy, we always set it here\n        task.retry_policy = retry_policy\n\n        if runnable and status != FAILED and worker.enabled:\n            task.workers.add(worker_id)\n            self._state.get_worker(worker_id).tasks.add(task)\n            task.runnable = runnable", "response": "Add a new task to the internal state."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _traverse_graph(self, root_task_id, seen=None, dep_func=None, include_done=True):\n\n        if seen is None:\n            seen = set()\n        elif root_task_id in seen:\n            return {}\n\n        if dep_func is None:\n            def dep_func(t):\n                return t.deps\n\n        seen.add(root_task_id)\n        serialized = {}\n        queue = collections.deque([root_task_id])\n        while queue:\n            task_id = queue.popleft()\n\n            task = self._state.get_task(task_id)\n            if task is None or not task.family:\n                logger.debug('Missing task for id [%s]', task_id)\n\n                # NOTE : If a dependency is missing from self._state there is no way to deduce the\n                #        task family and parameters.\n                family_match = TASK_FAMILY_RE.match(task_id)\n                family = family_match.group(1) if family_match else UNKNOWN\n                params = {'task_id': task_id}\n                serialized[task_id] = {\n                    'deps': [],\n                    'status': UNKNOWN,\n                    'workers': [],\n                    'start_time': UNKNOWN,\n                    'params': params,\n                    'name': family,\n                    'display_name': task_id,\n                    'priority': 0,\n                }\n            else:\n                deps = dep_func(task)\n                if not include_done:\n                    deps = list(self._filter_done(deps))\n                serialized[task_id] = self._serialize_task(task_id, deps=deps)\n                for dep in sorted(deps):\n                    if dep not in seen:\n                        seen.add(dep)\n                        queue.append(dep)\n\n            if task_id != root_task_id:\n                del serialized[task_id]['display_name']\n            if len(serialized) >= self._config.max_graph_nodes:\n                break\n\n        return serialized", "response": "This method traverses the dependency graph and returns the serialized node_id to the node_id."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nquery for a subset of tasks by status.", "response": "def task_list(self, status='', upstream_status='', limit=True, search=None, max_shown_tasks=None,\n                  **kwargs):\n        \"\"\"\n        Query for a subset of tasks by status.\n        \"\"\"\n        if not search:\n            count_limit = max_shown_tasks or self._config.max_shown_tasks\n            pre_count = self._state.get_active_task_count_for_status(status)\n            if limit and pre_count > count_limit:\n                return {'num_tasks': -1 if upstream_status else pre_count}\n        self.prune()\n\n        result = {}\n        upstream_status_table = {}  # used to memoize upstream status\n        if search is None:\n            def filter_func(_):\n                return True\n        else:\n            terms = search.split()\n\n            def filter_func(t):\n                return all(term in t.pretty_id for term in terms)\n\n        tasks = self._state.get_active_tasks_by_status(status) if status else self._state.get_active_tasks()\n        for task in filter(filter_func, tasks):\n            if task.status != PENDING or not upstream_status or upstream_status == self._upstream_status(task.id, upstream_status_table):\n                serialized = self._serialize_task(task.id, include_deps=False)\n                result[task.id] = serialized\n        if limit and len(result) > (max_shown_tasks or self._config.max_shown_tasks):\n            return {'num_tasks': len(result)}\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a list of all the resources in the cluster.", "response": "def resource_list(self):\n        \"\"\"\n        Resources usage info and their consumers (tasks).\n        \"\"\"\n        self.prune()\n        resources = [\n            dict(\n                name=resource,\n                num_total=r_dict['total'],\n                num_used=r_dict['used']\n            ) for resource, r_dict in six.iteritems(self.resources())]\n        if self._resources is not None:\n            consumers = collections.defaultdict(dict)\n            for task in self._state.get_active_tasks_by_status(RUNNING):\n                if task.status == RUNNING and task.resources:\n                    for resource, amount in six.iteritems(task.resources):\n                        consumers[resource][task.id] = self._serialize_task(task.id, include_deps=False)\n            for resource in resources:\n                tasks = consumers[resource['name']]\n                resource['num_consumer'] = len(tasks)\n                resource['running'] = tasks\n        return resources"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting total resources and available ones", "response": "def resources(self):\n        ''' get total resources and available ones '''\n        used_resources = self._used_resources()\n        ret = collections.defaultdict(dict)\n        for resource, total in six.iteritems(self._resources):\n            ret[resource]['total'] = total\n            if resource in used_resources:\n                ret[resource]['used'] = used_resources[resource]\n            else:\n                ret[resource]['used'] = 0\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef task_search(self, task_str, **kwargs):\n        self.prune()\n        result = collections.defaultdict(dict)\n        for task in self._state.get_active_tasks():\n            if task.id.find(task_str) != -1:\n                serialized = self._serialize_task(task.id, include_deps=False)\n                result[task.status][task.id] = serialized\n        return result", "response": "Search for a subset of tasks by task_id."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef exists(self):\n        path = self.path\n        if '*' in path or '?' in path or '[' in path or '{' in path:\n            logger.warning(\"Using wildcards in path %s might lead to processing of an incomplete dataset; \"\n                           \"override exists() to suppress the warning.\", path)\n        return self.fs.exists(path)", "response": "Returns True if the path for this FileSystemTarget exists ; False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef temporary_path(self):\n        num = random.randrange(0, 1e10)\n        slashless_path = self.path.rstrip('/').rstrip(\"\\\\\")\n        _temp_path = '{}-luigi-tmp-{:010}{}'.format(\n            slashless_path,\n            num,\n            self._trailing_slash())\n        # TODO: os.path doesn't make sense here as it's os-dependent\n        tmp_dir = os.path.dirname(slashless_path)\n        if tmp_dir:\n            self.fs.mkdir(tmp_dir, parents=True, raise_if_exists=False)\n\n        yield _temp_path\n        # We won't reach here if there was an user exception.\n        self.fs.rename_dont_move(_temp_path, self.path)", "response": "A context manager that creates a temporary file and returns the path to the new file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate an id for the indicator document.", "response": "def marker_index_document_id(self):\n        \"\"\"\n        Generate an id for the indicator document.\n        \"\"\"\n        params = '%s:%s:%s' % (self.index, self.doc_type, self.update_id)\n        return hashlib.sha1(params.encode('utf-8')).hexdigest()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmarks this update as complete.", "response": "def touch(self):\n        \"\"\"\n        Mark this update as complete.\n\n        The document id would be sufficent but,\n        for documentation,\n        we index the parameters `update_id`, `target_index`, `target_doc_type` and `date` as well.\n        \"\"\"\n        self.create_marker_index()\n        self.es.index(index=self.marker_index, doc_type=self.marker_doc_type,\n                      id=self.marker_index_document_id(), body={\n                          'update_id': self.update_id,\n                          'target_index': self.index,\n                          'target_doc_type': self.doc_type,\n                          'date': datetime.datetime.now()})\n        self.es.indices.flush(index=self.marker_index)\n        self.ensure_hist_size()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntest if this task has been run.", "response": "def exists(self):\n        \"\"\"\n        Test, if this task has been run.\n        \"\"\"\n        try:\n            self.es.get(index=self.marker_index, doc_type=self.marker_doc_type, id=self.marker_index_document_id())\n            return True\n        except elasticsearch.NotFoundError:\n            logger.debug('Marker document not found.')\n        except elasticsearch.ElasticsearchException as err:\n            logger.warn(err)\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating the marker index if it does not exist.", "response": "def create_marker_index(self):\n        \"\"\"\n        Create the index that will keep track of the tasks if necessary.\n        \"\"\"\n        if not self.es.indices.exists(index=self.marker_index):\n            self.es.indices.create(index=self.marker_index)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nshrink the history of updates for the current index to self. marker_index_hist_size.", "response": "def ensure_hist_size(self):\n        \"\"\"\n        Shrink the history of updates for\n        a `index/doc_type` combination down to `self.marker_index_hist_size`.\n        \"\"\"\n        if self.marker_index_hist_size == 0:\n            return\n        result = self.es.search(index=self.marker_index,\n                                doc_type=self.marker_doc_type,\n                                body={'query': {\n                                    'term': {'target_index': self.index}}},\n                                sort=('date:desc',))\n\n        for i, hit in enumerate(result.get('hits').get('hits'), start=1):\n            if i > self.marker_index_hist_size:\n                marker_document_id = hit.get('_id')\n                self.es.delete(id=marker_document_id, index=self.marker_index,\n                               doc_type=self.marker_doc_type)\n        self.es.indices.flush(index=self.marker_index)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nyielding all the documents in the document store.", "response": "def _docs(self):\n        \"\"\"\n        Since `self.docs` may yield documents that do not explicitly contain `_index` or `_type`,\n        add those attributes here, if necessary.\n        \"\"\"\n        iterdocs = iter(self.docs())\n        first = next(iterdocs)\n        needs_parsing = False\n        if isinstance(first, six.string_types):\n            needs_parsing = True\n        elif isinstance(first, dict):\n            pass\n        else:\n            raise RuntimeError('Document must be either JSON strings or dict.')\n        for doc in itertools.chain([first], iterdocs):\n            if needs_parsing:\n                doc = json.loads(doc)\n            if '_index' not in doc:\n                doc['_index'] = self.index\n            if '_type' not in doc:\n                doc['_type'] = self.doc_type\n            yield doc"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_index(self):\n        es = self._init_connection()\n        if not es.indices.exists(index=self.index):\n            es.indices.create(index=self.index, body=self.settings)", "response": "Create the target index."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndeletes the index if it exists.", "response": "def delete_index(self):\n        \"\"\"\n        Delete the index, if it exists.\n        \"\"\"\n        es = self._init_connection()\n        if es.indices.exists(index=self.index):\n            es.indices.delete(index=self.index)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef output(self):\n        return ElasticsearchTarget(\n            host=self.host,\n            port=self.port,\n            http_auth=self.http_auth,\n            index=self.index,\n            doc_type=self.doc_type,\n            update_id=self.update_id(),\n            marker_index_hist_size=self.marker_index_hist_size,\n            timeout=self.timeout,\n            extra_elasticsearch_args=self.extra_elasticsearch_args\n        )", "response": "Returns a ElasticsearchTarget representing the inserted dataset. Normally you don t override this."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run(self):\n        if self.purge_existing_index:\n            self.delete_index()\n        self.create_index()\n        es = self._init_connection()\n        if self.mapping:\n            es.indices.put_mapping(index=self.index, doc_type=self.doc_type,\n                                   body=self.mapping)\n        es.indices.put_settings({\"index\": {\"refresh_interval\": \"-1\"}},\n                                index=self.index)\n\n        bulk(es, self._docs(), chunk_size=self.chunk_size,\n             raise_on_error=self.raise_on_error)\n\n        es.indices.put_settings({\"index\": {\"refresh_interval\": \"1s\"}},\n                                index=self.index)\n        es.indices.refresh()\n        self.output().touch()", "response": "Run task, namely:\n\n        * purge existing index, if requested (`purge_existing_index`),\n        * create the index, if missing,\n        * apply mappings, if given,\n        * set refresh interval to -1 (disable) for performance reasons,\n        * bulk index in batches of size `chunk_size` (2000),\n        * set refresh interval to 1s,\n        * refresh Elasticsearch,\n        * create entry in marker index."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the value of the section option using method.", "response": "def _get_with_default(self, method, section, option, default, expected_type=None, **kwargs):\n        \"\"\"\n        Gets the value of the section/option using method.\n\n        Returns default if value is not found.\n\n        Raises an exception if the default value is not None and doesn't match the expected_type.\n        \"\"\"\n        try:\n            try:\n                # Underscore-style is the recommended configuration style\n                option = option.replace('-', '_')\n                return method(self, section, option, **kwargs)\n            except (NoOptionError, NoSectionError):\n                # Support dash-style option names (with deprecation warning).\n                option_alias = option.replace('_', '-')\n                value = method(self, section, option_alias, **kwargs)\n                warn = 'Configuration [{s}] {o} (with dashes) should be avoided. Please use underscores: {u}.'.format(\n                    s=section, o=option_alias, u=option)\n                warnings.warn(warn, DeprecationWarning)\n                return value\n        except (NoOptionError, NoSectionError):\n            if default is LuigiConfigParser.NO_DEFAULT:\n                raise\n            if expected_type is not None and default is not None and \\\n               not isinstance(default, expected_type):\n                raise\n            return default"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npolling job status while active", "response": "def __track_job(self):\n        \"\"\"Poll job status while active\"\"\"\n        while not self.__verify_job_has_started():\n            time.sleep(self.__POLL_TIME)\n            self.__logger.debug(\"Waiting for Kubernetes job \" + self.uu_name + \" to start\")\n        self.__print_kubectl_hints()\n\n        status = self.__get_job_status()\n        while status == \"RUNNING\":\n            self.__logger.debug(\"Kubernetes job \" + self.uu_name + \" is running\")\n            time.sleep(self.__POLL_TIME)\n            status = self.__get_job_status()\n\n        assert status != \"FAILED\", \"Kubernetes job \" + self.uu_name + \" failed\"\n\n        # status == \"SUCCEEDED\"\n        self.__logger.info(\"Kubernetes job \" + self.uu_name + \" succeeded\")\n        self.signal_complete()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef __verify_job_has_started(self):\n        # Verify that the job started\n        self.__get_job()\n\n        # Verify that the pod started\n        pods = self.__get_pods()\n\n        assert len(pods) > 0, \"No pod scheduled by \" + self.uu_name\n        for pod in pods:\n            status = pod.obj['status']\n            for cont_stats in status.get('containerStatuses', []):\n                if 'terminated' in cont_stats['state']:\n                    t = cont_stats['state']['terminated']\n                    err_msg = \"Pod %s %s (exit code %d). Logs: `kubectl logs pod/%s`\" % (\n                        pod.name, t['reason'], t['exitCode'], pod.name)\n                    assert t['exitCode'] == 0, err_msg\n\n                if 'waiting' in cont_stats['state']:\n                    wr = cont_stats['state']['waiting']['reason']\n                    assert wr == 'ContainerCreating', \"Pod %s %s. Logs: `kubectl logs pod/%s`\" % (\n                        pod.name, wr, pod.name)\n\n            for cond in status.get('conditions', []):\n                if 'message' in cond:\n                    if cond['reason'] == 'ContainersNotReady':\n                        return False\n                    assert cond['status'] != 'False', \\\n                        \"[ERROR] %s - %s\" % (cond['reason'], cond['message'])\n        return True", "response": "Verifies that the job has successfully started."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfigure out the status of the Kubernetes job and return it.", "response": "def __get_job_status(self):\n        \"\"\"Return the Kubernetes job status\"\"\"\n        # Figure out status and return it\n        job = self.__get_job()\n\n        if \"succeeded\" in job.obj[\"status\"] and job.obj[\"status\"][\"succeeded\"] > 0:\n            job.scale(replicas=0)\n            if self.print_pod_logs_on_exit:\n                self.__print_pod_logs()\n            if self.delete_on_success:\n                self.__delete_job_cascade(job)\n            return \"SUCCEEDED\"\n\n        if \"failed\" in job.obj[\"status\"]:\n            failed_cnt = job.obj[\"status\"][\"failed\"]\n            self.__logger.debug(\"Kubernetes job \" + self.uu_name\n                                + \" status.failed: \" + str(failed_cnt))\n            if self.print_pod_logs_on_exit:\n                self.__print_pod_logs()\n            if failed_cnt > self.max_retrials:\n                job.scale(replicas=0)  # avoid more retrials\n                return \"FAILED\"\n        return \"RUNNING\""}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns an engine instance creating it if it doesn t exist.", "response": "def engine(self):\n        \"\"\"\n        Return an engine instance, creating it if it doesn't exist.\n\n        Recreate the engine connection if it wasn't originally created\n        by the current process.\n        \"\"\"\n        pid = os.getpid()\n        conn = SQLAlchemyTarget._engine_dict.get(self.connection_string)\n        if not conn or conn.pid != pid:\n            # create and reset connection\n            engine = sqlalchemy.create_engine(\n                self.connection_string,\n                connect_args=self.connect_args,\n                echo=self.echo\n            )\n            SQLAlchemyTarget._engine_dict[self.connection_string] = self.Connection(engine, pid)\n        return SQLAlchemyTarget._engine_dict[self.connection_string].engine"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmark this update as complete.", "response": "def touch(self):\n        \"\"\"\n        Mark this update as complete.\n        \"\"\"\n        if self.marker_table_bound is None:\n            self.create_marker_table()\n\n        table = self.marker_table_bound\n        id_exists = self.exists()\n        with self.engine.begin() as conn:\n            if not id_exists:\n                ins = table.insert().values(update_id=self.update_id, target_table=self.target_table,\n                                            inserted=datetime.datetime.now())\n            else:\n                ins = table.update().where(sqlalchemy.and_(table.c.update_id == self.update_id,\n                                                           table.c.target_table == self.target_table)).\\\n                    values(update_id=self.update_id, target_table=self.target_table,\n                           inserted=datetime.datetime.now())\n            conn.execute(ins)\n        assert self.exists()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_marker_table(self):\n        if self.marker_table is None:\n            self.marker_table = luigi.configuration.get_config().get('sqlalchemy', 'marker-table', 'table_updates')\n\n        engine = self.engine\n\n        with engine.begin() as con:\n            metadata = sqlalchemy.MetaData()\n            if not con.dialect.has_table(con, self.marker_table):\n                self.marker_table_bound = sqlalchemy.Table(\n                    self.marker_table, metadata,\n                    sqlalchemy.Column(\"update_id\", sqlalchemy.String(128), primary_key=True),\n                    sqlalchemy.Column(\"target_table\", sqlalchemy.String(128)),\n                    sqlalchemy.Column(\"inserted\", sqlalchemy.DateTime, default=datetime.datetime.now()))\n                metadata.create_all(engine)\n            else:\n                metadata.reflect(only=[self.marker_table], bind=engine)\n                self.marker_table_bound = metadata.tables[self.marker_table]", "response": "Create marker table if it doesn t exist."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate the target table.", "response": "def create_table(self, engine):\n        \"\"\"\n        Override to provide code for creating the target table.\n\n        By default it will be created using types specified in columns.\n        If the table exists, then it binds to the existing table.\n\n        If overridden, use the provided connection object for setting up the table in order to\n        create the table and insert data using the same transaction.\n        :param engine: The sqlalchemy engine instance\n        :type engine: object\n        \"\"\"\n        def construct_sqla_columns(columns):\n            retval = [sqlalchemy.Column(*c[0], **c[1]) for c in columns]\n            return retval\n\n        needs_setup = (len(self.columns) == 0) or (False in [len(c) == 2 for c in self.columns]) if not self.reflect else False\n        if needs_setup:\n            # only names of columns specified, no types\n            raise NotImplementedError(\"create_table() not implemented for %r and columns types not specified\" % self.table)\n        else:\n            # if columns is specified as (name, type) tuples\n            with engine.begin() as con:\n\n                if self.schema:\n                    metadata = sqlalchemy.MetaData(schema=self.schema)\n                else:\n                    metadata = sqlalchemy.MetaData()\n\n                try:\n                    if not con.dialect.has_table(con, self.table, self.schema or None):\n                        sqla_columns = construct_sqla_columns(self.columns)\n                        self.table_bound = sqlalchemy.Table(self.table, metadata, *sqla_columns)\n                        metadata.create_all(engine)\n                    else:\n                        full_table = '.'.join([self.schema, self.table]) if self.schema else self.table\n                        metadata.reflect(only=[self.table], bind=engine)\n                        self.table_bound = metadata.tables[full_table]\n                except Exception as e:\n                    self._logger.exception(self.table + str(e))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef copy(self, conn, ins_rows, table_bound):\n        bound_cols = dict((c, sqlalchemy.bindparam(\"_\" + c.key)) for c in table_bound.columns)\n        ins = table_bound.insert().values(bound_cols)\n        conn.execute(ins, ins_rows)", "response": "This method copies the data from ins_rows into the object holding the key in table_bound."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nruns the work() method from the class instance in the file \"job-instance.pickle\".", "response": "def main(args=sys.argv):\n    \"\"\"Run the work() method from the class instance in the file \"job-instance.pickle\".\n    \"\"\"\n    try:\n        # Set up logging.\n        logging.basicConfig(level=logging.WARN)\n        work_dir = args[1]\n        assert os.path.exists(work_dir), \"First argument to lsf_runner.py must be a directory that exists\"\n        do_work_on_compute_node(work_dir)\n    except Exception as exc:\n        # Dump encoded data that we will try to fetch using mechanize\n        print(exc)\n        raise"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rename(self, path, raise_if_exists=False):\n        if isinstance(path, HdfsTarget):\n            path = path.path\n        if raise_if_exists and self.fs.exists(path):\n            raise RuntimeError('Destination exists: %s' % path)\n        self.fs.rename(self.path, path)", "response": "Rename the object at the given path."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef move(self, path, raise_if_exists=False):\n        self.rename(path, raise_if_exists=raise_if_exists)", "response": "Rename the current object to the given path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_writable(self):\n        if \"/\" in self.path:\n            # example path: /log/ap/2013-01-17/00\n            parts = self.path.split(\"/\")\n            # start with the full path and then up the tree until we can check\n            length = len(parts)\n            for part in range(length):\n                path = \"/\".join(parts[0:length - part]) + \"/\"\n                if self.fs.exists(path):\n                    # if the path exists and we can write there, great!\n                    if self._is_writable(path):\n                        return True\n                    # if it exists and we can't =( sad panda\n                    else:\n                        return False\n            # We went through all parts of the path and we still couldn't find\n            # one that exists.\n            return False", "response": "Returns True if the file is writable False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _partition_tasks(worker):\n    task_history = worker._add_task_history\n    pending_tasks = {task for(task, status, ext) in task_history if status == 'PENDING'}\n    set_tasks = {}\n    set_tasks[\"completed\"] = {task for (task, status, ext) in task_history if status == 'DONE' and task in pending_tasks}\n    set_tasks[\"already_done\"] = {task for (task, status, ext) in task_history\n                                 if status == 'DONE' and task not in pending_tasks and task not in set_tasks[\"completed\"]}\n    set_tasks[\"ever_failed\"] = {task for (task, status, ext) in task_history if status == 'FAILED'}\n    set_tasks[\"failed\"] = set_tasks[\"ever_failed\"] - set_tasks[\"completed\"]\n    set_tasks[\"scheduling_error\"] = {task for(task, status, ext) in task_history if status == 'UNKNOWN'}\n    set_tasks[\"still_pending_ext\"] = {task for (task, status, ext) in task_history\n                                      if status == 'PENDING' and task not in set_tasks[\"ever_failed\"] and task not in set_tasks[\"completed\"] and not ext}\n    set_tasks[\"still_pending_not_ext\"] = {task for (task, status, ext) in task_history\n                                          if status == 'PENDING' and task not in set_tasks[\"ever_failed\"] and task not in set_tasks[\"completed\"] and ext}\n    set_tasks[\"run_by_other_worker\"] = set()\n    set_tasks[\"upstream_failure\"] = set()\n    set_tasks[\"upstream_missing_dependency\"] = set()\n    set_tasks[\"upstream_run_by_other_worker\"] = set()\n    set_tasks[\"upstream_scheduling_error\"] = set()\n    set_tasks[\"not_run\"] = set()\n    return set_tasks", "response": "Takes a worker and sorts out tasks based on their status."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _populate_unknown_statuses(set_tasks):\n    visited = set()\n    for task in set_tasks[\"still_pending_not_ext\"]:\n        _depth_first_search(set_tasks, task, visited)", "response": "Populate unknown statuses with the ones that are not run."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_str(task_dict, extra_indent):\n    summary_length = execution_summary().summary_length\n\n    lines = []\n    task_names = sorted(task_dict.keys())\n    for task_family in task_names:\n        tasks = task_dict[task_family]\n        tasks = sorted(tasks, key=lambda x: str(x))\n        prefix_size = 8 if extra_indent else 4\n        prefix = ' ' * prefix_size\n\n        line = None\n\n        if summary_length > 0 and len(lines) >= summary_length:\n            line = prefix + \"...\"\n            lines.append(line)\n            break\n        if len(tasks[0].get_params()) == 0:\n            line = prefix + '- {0} {1}()'.format(len(tasks), str(task_family))\n        elif _get_len_of_params(tasks[0]) > 60 or len(str(tasks[0])) > 200 or \\\n                (len(tasks) == 2 and len(tasks[0].get_params()) > 1 and (_get_len_of_params(tasks[0]) > 40 or len(str(tasks[0])) > 100)):\n            \"\"\"\n            This is to make sure that there is no really long task in the output\n            \"\"\"\n            line = prefix + '- {0} {1}(...)'.format(len(tasks), task_family)\n        elif len((tasks[0].get_params())) == 1:\n            attributes = {getattr(task, tasks[0].get_params()[0][0]) for task in tasks}\n            param_class = tasks[0].get_params()[0][1]\n            first, last = _ranging_attributes(attributes, param_class)\n            if first is not None and last is not None and len(attributes) > 3:\n                param_str = '{0}...{1}'.format(param_class.serialize(first), param_class.serialize(last))\n            else:\n                param_str = '{0}'.format(_get_str_one_parameter(tasks))\n            line = prefix + '- {0} {1}({2}={3})'.format(len(tasks), task_family, tasks[0].get_params()[0][0], param_str)\n        else:\n            ranging = False\n            params = _get_set_of_params(tasks)\n            unique_param_keys = list(_get_unique_param_keys(params))\n            if len(unique_param_keys) == 1:\n                unique_param, = unique_param_keys\n                attributes = params[unique_param]\n                param_class = unique_param[1]\n                first, last = _ranging_attributes(attributes, param_class)\n                if first is not None and last is not None and len(attributes) > 2:\n                    ranging = True\n                    line = prefix + '- {0} {1}({2}'.format(len(tasks), task_family, _get_str_ranging_multiple_parameters(first, last, tasks, unique_param))\n            if not ranging:\n                if len(tasks) == 1:\n                    line = prefix + '- {0} {1}'.format(len(tasks), tasks[0])\n                if len(tasks) == 2:\n                    line = prefix + '- {0} {1} and {2}'.format(len(tasks), tasks[0], tasks[1])\n                if len(tasks) > 2:\n                    line = prefix + '- {0} {1} ...'.format(len(tasks), tasks[0])\n        lines.append(line)\n    return '\\n'.join(lines)", "response": "This returns a string for each status\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _ranging_attributes(attributes, param_class):\n    next_attributes = {param_class.next_in_enumeration(attribute) for attribute in attributes}\n    in_first = attributes.difference(next_attributes)\n    in_second = next_attributes.difference(attributes)\n    if len(in_first) == 1 and len(in_second) == 1:\n        for x in attributes:\n            if {param_class.next_in_enumeration(x)} == in_second:\n                return next(iter(in_first)), x\n    return None, None", "response": "Checks if there is a continuous range\n            and returns the next element of the attributes."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_comments(group_tasks):\n    comments = {}\n    for status, human in _COMMENTS:\n        num_tasks = _get_number_of_tasks_for(status, group_tasks)\n        if num_tasks:\n            space = \"    \" if status in _PENDING_SUB_STATUSES else \"\"\n            comments[status] = '{space}* {num_tasks} {human}:\\n'.format(\n                space=space,\n                num_tasks=num_tasks,\n                human=human)\n    return comments", "response": "Get the human readable comments and quantities for the task types."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_run_by_other_worker(worker):\n    task_sets = _get_external_workers(worker).values()\n    return functools.reduce(lambda a, b: a | b, task_sets, set())", "response": "This returns a set of the tasks that are being run by other worker"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _group_tasks_by_name_and_status(task_dict):\n    group_status = {}\n    for task in task_dict:\n        if task.task_family not in group_status:\n            group_status[task.task_family] = []\n        group_status[task.task_family].append(task)\n    return group_status", "response": "Takes a dictionary with sets of tasks grouped by their status and task name and returns a dictionary with an array of tasks grouped by their status and task name and task name"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _tasks_status(set_tasks):\n    if set_tasks[\"ever_failed\"]:\n        if not set_tasks[\"failed\"]:\n            return LuigiStatusCode.SUCCESS_WITH_RETRY\n        else:\n            if set_tasks[\"scheduling_error\"]:\n                return LuigiStatusCode.FAILED_AND_SCHEDULING_FAILED\n            return LuigiStatusCode.FAILED\n    elif set_tasks[\"scheduling_error\"]:\n        return LuigiStatusCode.SCHEDULING_FAILED\n    elif set_tasks[\"not_run\"]:\n        return LuigiStatusCode.NOT_RUN\n    elif set_tasks[\"still_pending_ext\"]:\n        return LuigiStatusCode.MISSING_EXT\n    else:\n        return LuigiStatusCode.SUCCESS", "response": "Returns a LuigiStatusCode object for a set of tasks"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a canonical string used to identify a particular task", "response": "def task_id_str(task_family, params):\n    \"\"\"\n    Returns a canonical string used to identify a particular task\n\n    :param task_family: The task family (class name) of the task\n    :param params: a dict mapping parameter names to their serialized values\n    :return: A unique, shortened identifier corresponding to the family and params\n    \"\"\"\n    # task_id is a concatenation of task family, the first values of the first 3 parameters\n    # sorted by parameter name and a md5hash of the family/parameters as a cananocalised json.\n    param_str = json.dumps(params, separators=(',', ':'), sort_keys=True)\n    param_hash = hashlib.md5(param_str.encode('utf-8')).hexdigest()\n\n    param_summary = '_'.join(p[:TASK_ID_TRUNCATE_PARAMS]\n                             for p in (params[p] for p in sorted(params)[:TASK_ID_INCLUDE_PARAMS]))\n    param_summary = TASK_ID_INVALID_CHAR_REGEX.sub('_', param_summary)\n\n    return '{}_{}_{}'.format(task_family, param_summary, param_hash[:TASK_ID_TRUNCATE_HASH])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef externalize(taskclass_or_taskobject):\n    # Seems like with python < 3.3 copy.copy can't copy classes\n    # and objects with specified metaclass http://bugs.python.org/issue11480\n    compatible_copy = copy.copy if six.PY3 else copy.deepcopy\n    copied_value = compatible_copy(taskclass_or_taskobject)\n    if copied_value is taskclass_or_taskobject:\n        # Assume it's a class\n        clazz = taskclass_or_taskobject\n\n        @_task_wraps(clazz)\n        class _CopyOfClass(clazz):\n            # How to copy a class: http://stackoverflow.com/a/9541120/621449\n            _visible_in_registry = False\n        _CopyOfClass.run = None\n        return _CopyOfClass\n    else:\n        # We assume it's an object\n        copied_value.run = None\n        return copied_value", "response": "Returns an externalized version of a Task."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getpaths(struct):\n    if isinstance(struct, Task):\n        return struct.output()\n    elif isinstance(struct, dict):\n        return struct.__class__((k, getpaths(v)) for k, v in six.iteritems(struct))\n    elif isinstance(struct, (list, tuple)):\n        return struct.__class__(getpaths(r) for r in struct)\n    else:\n        # Remaining case: assume struct is iterable...\n        try:\n            return [getpaths(r) for r in struct]\n        except TypeError:\n            raise Exception('Cannot map %s to Task/dict/list' % str(struct))", "response": "Maps all Tasks in a structured data object to their. output."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of all items in structured output", "response": "def flatten(struct):\n    \"\"\"\n    Creates a flat list of all all items in structured output (dicts, lists, items):\n\n    .. code-block:: python\n\n        >>> sorted(flatten({'a': 'foo', 'b': 'bar'}))\n        ['bar', 'foo']\n        >>> sorted(flatten(['foo', ['bar', 'troll']]))\n        ['bar', 'foo', 'troll']\n        >>> flatten('foo')\n        ['foo']\n        >>> flatten(42)\n        [42]\n    \"\"\"\n    if struct is None:\n        return []\n    flat = []\n    if isinstance(struct, dict):\n        for _, result in six.iteritems(struct):\n            flat += flatten(result)\n        return flat\n    if isinstance(struct, six.string_types):\n        return [struct]\n\n    try:\n        # if iterable\n        iterator = iter(struct)\n    except TypeError:\n        return [struct]\n\n    for result in iterator:\n        flat += flatten(result)\n    return flat"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a flattened list of all output targets of a task.", "response": "def flatten_output(task):\n    \"\"\"\n    Lists all output targets by recursively walking output-less (wrapper) tasks.\n\n    FIXME order consistently.\n    \"\"\"\n    r = flatten(task.output())\n    if not r:\n        for dep in flatten(task.requires()):\n            r += flatten_output(dep)\n    return r"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the Bite object for this Luigi instance.", "response": "def get_bite(self):\n        \"\"\"\n        If Luigi has forked, we have a different PID, and need to reconnect.\n        \"\"\"\n        config = hdfs_config.hdfs()\n        if self.pid != os.getpid() or not self._bite:\n            client_kwargs = dict(filter(\n                lambda k_v: k_v[1] is not None and k_v[1] != '', six.iteritems({\n                    'hadoop_version': config.client_version,\n                    'effective_user': config.effective_user,\n                })\n            ))\n            if config.snakebite_autoconfig:\n                \"\"\"\n                This is fully backwards compatible with the vanilla Client and can be used for a non HA cluster as well.\n                This client tries to read ``${HADOOP_PATH}/conf/hdfs-site.xml`` to get the address of the namenode.\n                The behaviour is the same as Client.\n                \"\"\"\n                from snakebite.client import AutoConfigClient\n                self._bite = AutoConfigClient(**client_kwargs)\n            else:\n                from snakebite.client import Client\n                self._bite = Client(config.namenode_host, config.namenode_port, **client_kwargs)\n        return self._bite"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nuse snakebite. rename if available.", "response": "def move(self, path, dest):\n        \"\"\"\n        Use snakebite.rename, if available.\n\n        :param path: source file(s)\n        :type path: either a string or sequence of strings\n        :param dest: destination file (single input) or directory (multiple)\n        :type dest: string\n        :return: list of renamed items\n        \"\"\"\n        parts = dest.rstrip('/').split('/')\n        if len(parts) > 1:\n            dir_path = '/'.join(parts[0:-1])\n            if not self.exists(dir_path):\n                self.mkdir(dir_path, parents=True)\n        return list(self.get_bite().rename(self.list_path(path), dest))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nusing snakebite. rename_dont_move to rename the source file.", "response": "def rename_dont_move(self, path, dest):\n        \"\"\"\n        Use snakebite.rename_dont_move, if available.\n\n        :param path: source path (single input)\n        :type path: string\n        :param dest: destination path\n        :type dest: string\n        :return: True if succeeded\n        :raises: snakebite.errors.FileAlreadyExistsException\n        \"\"\"\n        from snakebite.errors import FileAlreadyExistsException\n        try:\n            self.get_bite().rename2(path, dest, overwriteDest=False)\n        except FileAlreadyExistsException:\n            # Unfortunately python2 don't allow exception chaining.\n            raise luigi.target.FileAlreadyExists()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nuses snakebite. delete if available.", "response": "def remove(self, path, recursive=True, skip_trash=False):\n        \"\"\"\n        Use snakebite.delete, if available.\n\n        :param path: delete-able file(s) or directory(ies)\n        :type path: either a string or a sequence of strings\n        :param recursive: delete directories trees like \\\\*nix: rm -r\n        :type recursive: boolean, default is True\n        :param skip_trash: do or don't move deleted items into the trash first\n        :type skip_trash: boolean, default is False (use trash)\n        :return: list of deleted items\n        \"\"\"\n        return list(self.get_bite().delete(self.list_path(path), recurse=recursive))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nuse snakebite. chmod if available.", "response": "def chmod(self, path, permissions, recursive=False):\n        \"\"\"\n        Use snakebite.chmod, if available.\n\n        :param path: update-able file(s)\n        :type path: either a string or sequence of strings\n        :param permissions: \\\\*nix style permission number\n        :type permissions: octal\n        :param recursive: change just listed entry(ies) or all in directories\n        :type recursive: boolean, default is False\n        :return: list of all changed items\n        \"\"\"\n        if type(permissions) == str:\n            permissions = int(permissions, 8)\n        return list(self.get_bite().chmod(self.list_path(path),\n                                          permissions, recursive))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef chown(self, path, owner, group, recursive=False):\n        bite = self.get_bite()\n        if owner:\n            if group:\n                return all(bite.chown(self.list_path(path), \"%s:%s\" % (owner, group),\n                                      recurse=recursive))\n            return all(bite.chown(self.list_path(path), owner, recurse=recursive))\n        return list(bite.chgrp(self.list_path(path), group, recurse=recursive))", "response": "Use snakebite. chown to change owner and group of a file or directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef count(self, path):\n        try:\n            res = self.get_bite().count(self.list_path(path)).next()\n            dir_count = res['directoryCount']\n            file_count = res['fileCount']\n            content_size = res['spaceConsumed']\n        except StopIteration:\n            dir_count = file_count = content_size = 0\n        return {'content_size': content_size, 'dir_count': dir_count,\n                'file_count': file_count}", "response": "Use snakebite. count if available."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get(self, path, local_destination):\n        return list(self.get_bite().copyToLocal(self.list_path(path),\n                                                local_destination))", "response": "Use snakebite. copyToLocal if available."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_merge(self, path, local_destination):\n        return list(self.get_bite().getmerge(path=path, dst=local_destination))", "response": "Uses snakebite. getmerge to implement this."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nuses snakebite s mkdir method if available.", "response": "def mkdir(self, path, parents=True, mode=0o755, raise_if_exists=False):\n        \"\"\"\n        Use snakebite.mkdir, if available.\n\n        Snakebite's mkdir method allows control over full path creation, so by\n        default, tell it to build a full path to work like ``hadoop fs -mkdir``.\n\n        :param path: HDFS path to create\n        :type path: string\n        :param parents: create any missing parent directories\n        :type parents: boolean, default is True\n        :param mode: \\\\*nix style owner/group/other permissions\n        :type mode: octal, default 0755\n        \"\"\"\n        result = list(self.get_bite().mkdir(self.list_path(path),\n                                            create_parent=parents, mode=mode))\n        if raise_if_exists and \"ile exists\" in result[0].get('error', ''):\n            raise luigi.target.FileAlreadyExists(\"%s exists\" % (path, ))\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef listdir(self, path, ignore_directories=False, ignore_files=False,\n                include_size=False, include_type=False, include_time=False,\n                recursive=False):\n        \"\"\"\n        Use snakebite.ls to get the list of items in a directory.\n\n        :param path: the directory to list\n        :type path: string\n        :param ignore_directories: if True, do not yield directory entries\n        :type ignore_directories: boolean, default is False\n        :param ignore_files: if True, do not yield file entries\n        :type ignore_files: boolean, default is False\n        :param include_size: include the size in bytes of the current item\n        :type include_size: boolean, default is False (do not include)\n        :param include_type: include the type (d or f) of the current item\n        :type include_type: boolean, default is False (do not include)\n        :param include_time: include the last modification time of the current item\n        :type include_time: boolean, default is False (do not include)\n        :param recursive: list subdirectory contents\n        :type recursive: boolean, default is False (do not recurse)\n        :return: yield with a string, or if any of the include_* settings are\n            true, a tuple starting with the path, and include_* items in order\n        \"\"\"\n        bite = self.get_bite()\n        for entry in bite.ls(self.list_path(path), recurse=recursive):\n            if ignore_directories and entry['file_type'] == 'd':\n                continue\n            if ignore_files and entry['file_type'] == 'f':\n                continue\n            rval = [entry['path'], ]\n            if include_size:\n                rval.append(entry['length'])\n            if include_type:\n                rval.append(entry['file_type'])\n            if include_time:\n                rval.append(datetime.datetime.fromtimestamp(entry['modification_time'] / 1000))\n            if len(rval) > 1:\n                yield tuple(rval)\n            else:\n                yield rval[0]", "response": "Return a list of items in a directory."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nimport task dynamically given a module and a task name.", "response": "def load_task(module, task_name, params_str):\n    \"\"\"\n    Imports task dynamically given a module and a task name.\n    \"\"\"\n    if module is not None:\n        __import__(module)\n    task_cls = Register.get_task_cls(task_name)\n    return task_cls.from_str_params(params_str)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the name of the task class.", "response": "def task_family(cls):\n        \"\"\"\n        Internal note: This function will be deleted soon.\n        \"\"\"\n        if not cls.get_task_namespace():\n            return cls.__name__\n        else:\n            return \"{}.{}\".format(cls.get_task_namespace(), cls.__name__)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_reg(cls):\n        # We have to do this on-demand in case task names have changed later\n        reg = dict()\n        for task_cls in cls._reg:\n            if not task_cls._visible_in_registry:\n                continue\n\n            name = task_cls.get_task_family()\n            if name in reg and \\\n                    (reg[name] == Register.AMBIGUOUS_CLASS or  # Check so issubclass doesn't crash\n                     not issubclass(task_cls, reg[name])):\n                # Registering two different classes - this means we can't instantiate them by name\n                # The only exception is if one class is a subclass of the other. In that case, we\n                # instantiate the most-derived class (this fixes some issues with decorator wrappers).\n                reg[name] = Register.AMBIGUOUS_CLASS\n            else:\n                reg[name] = task_cls\n\n        return reg", "response": "Return all of the registered classes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn an unambiguous class or raises an exception.", "response": "def get_task_cls(cls, name):\n        \"\"\"\n        Returns an unambiguous class or raises an exception.\n        \"\"\"\n        task_cls = cls._get_reg().get(name)\n        if not task_cls:\n            raise TaskClassNotFoundException(cls._missing_task_msg(name))\n\n        if task_cls == cls.AMBIGUOUS_CLASS:\n            raise TaskClassAmbigiousException('Task %r is ambiguous' % name)\n        return task_cls"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning all parameters for all Task.", "response": "def get_all_params(cls):\n        \"\"\"\n        Compiles and returns all parameters for all :py:class:`Task`.\n\n        :return: a generator of tuples (TODO: we should make this more elegant)\n        \"\"\"\n        for task_name, task_cls in six.iteritems(cls._get_reg()):\n            if task_cls == cls.AMBIGUOUS_CLASS:\n                continue\n            for param_name, param_obj in task_cls.get_params():\n                yield task_name, (not task_cls.use_cmdline_section), param_name, param_obj"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _module_parents(module_name):\n        '''\n        >>> list(Register._module_parents('a.b'))\n        ['a.b', 'a', '']\n        '''\n        spl = module_name.split('.')\n        for i in range(len(spl), 0, -1):\n            yield '.'.join(spl[0:i])\n        if module_name:\n            yield ''", "response": "Generator over the parents of a module name."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_task_statuses(task_ids, cluster):\n    response = client.describe_tasks(tasks=task_ids, cluster=cluster)\n\n    # Error checking\n    if response['failures'] != []:\n        raise Exception('There were some failures:\\n{0}'.format(\n            response['failures']))\n    status_code = response['ResponseMetadata']['HTTPStatusCode']\n    if status_code != 200:\n        msg = 'Task status request received status code {0}:\\n{1}'\n        raise Exception(msg.format(status_code, response))\n\n    return [t['lastStatus'] for t in response['tasks']]", "response": "Retrieve task statuses from ECS API"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _track_tasks(task_ids, cluster):\n    while True:\n        statuses = _get_task_statuses(task_ids, cluster)\n        if all([status == 'STOPPED' for status in statuses]):\n            logger.info('ECS tasks {0} STOPPED'.format(','.join(task_ids)))\n            break\n        time.sleep(POLL_TIME)\n        logger.debug('ECS task status for tasks {0}: {1}'.format(task_ids, statuses))", "response": "Poll task status until STOPPED"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates the target table.", "response": "def create_table(self, connection):\n        \"\"\"\n        Override to provide code for creating the target table.\n\n        By default it will be created using types (optionally) specified in columns.\n\n        If overridden, use the provided connection object for setting up the table in order to\n        create the table and insert data using the same transaction.\n        \"\"\"\n        if len(self.columns[0]) == 1:\n            # only names of columns specified, no types\n            raise NotImplementedError(\"create_table() not implemented for %r and columns types not specified\" % self.table)\n        elif len(self.columns[0]) == 2:\n            # if columns is specified as (name, type) tuples\n            coldefs = ','.join(\n                '{name} {type}'.format(name=name, type=type) for name, type in self.columns\n            )\n            query = \"CREATE TABLE {table} ({coldefs})\".format(table=self.table, coldefs=coldefs)\n            connection.cursor().execute(query)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef init_copy(self, connection):\n\n        # TODO: remove this after sufficient time so most people using the\n        # clear_table attribtue will have noticed it doesn't work anymore\n        if hasattr(self, \"clear_table\"):\n            raise Exception(\"The clear_table attribute has been removed. Override init_copy instead!\")\n\n        if self.enable_metadata_columns:\n            self._add_metadata_columns(connection.cursor())", "response": "Initialize the copy of the database."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting all the values in task_instance that are found in task_cls.", "response": "def common_params(task_instance, task_cls):\n    \"\"\"\n    Grab all the values in task_instance that are found in task_cls.\n    \"\"\"\n    if not isinstance(task_cls, task.Register):\n        raise TypeError(\"task_cls must be an uninstantiated Task\")\n\n    task_instance_param_names = dict(task_instance.get_params()).keys()\n    task_cls_params_dict = dict(task_cls.get_params())\n    task_cls_param_names = task_cls_params_dict.keys()\n    common_param_names = set(task_instance_param_names).intersection(set(task_cls_param_names))\n    common_param_vals = [(key, task_cls_params_dict[key]) for key in common_param_names]\n    common_kwargs = dict((key, task_instance.param_kwargs[key]) for key in common_param_names)\n    vals = dict(task_instance.get_param_values(common_param_vals, [], common_kwargs))\n    return vals"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlets a task call methods on subtask(s). The way this works is that the subtask is run as a part of the task, but the task itself doesn't have to care about the requirements of the subtasks. The subtask doesn't exist from the scheduler's point of view, and its dependencies are instead required by the main task. Example: .. code-block:: python class PowersOfN(luigi.Task): n = luigi.IntParameter() def f(self, x): return x ** self.n @delegates class T(luigi.Task): def subtasks(self): return PowersOfN(5) def run(self): print self.subtasks().f(42)", "response": "def delegates(task_that_delegates):\n    \"\"\" Lets a task call methods on subtask(s).\n\n    The way this works is that the subtask is run as a part of the task, but\n    the task itself doesn't have to care about the requirements of the subtasks.\n    The subtask doesn't exist from the scheduler's point of view, and\n    its dependencies are instead required by the main task.\n\n    Example:\n\n    .. code-block:: python\n\n        class PowersOfN(luigi.Task):\n            n = luigi.IntParameter()\n            def f(self, x): return x ** self.n\n\n        @delegates\n        class T(luigi.Task):\n            def subtasks(self): return PowersOfN(5)\n            def run(self): print self.subtasks().f(42)\n    \"\"\"\n    if not hasattr(task_that_delegates, 'subtasks'):\n        # This method can (optionally) define a couple of delegate tasks that\n        # will be accessible as interfaces, meaning that the task can access\n        # those tasks and run methods defined on them, etc\n        raise AttributeError('%s needs to implement the method \"subtasks\"' % task_that_delegates)\n\n    @task._task_wraps(task_that_delegates)\n    class Wrapped(task_that_delegates):\n\n        def deps(self):\n            # Overrides method in base class\n            return task.flatten(self.requires()) + task.flatten([t.deps() for t in task.flatten(self.subtasks())])\n\n        def run(self):\n            for t in task.flatten(self.subtasks()):\n                t.run()\n            task_that_delegates.run(self)\n\n    return Wrapped"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef previous(task):\n    params = task.get_params()\n    previous_params = {}\n    previous_date_params = {}\n\n    for param_name, param_obj in params:\n        param_value = getattr(task, param_name)\n\n        if isinstance(param_obj, parameter.DateParameter):\n            previous_date_params[param_name] = param_value - datetime.timedelta(days=1)\n        elif isinstance(param_obj, parameter.DateSecondParameter):\n            previous_date_params[param_name] = param_value - datetime.timedelta(seconds=1)\n        elif isinstance(param_obj, parameter.DateMinuteParameter):\n            previous_date_params[param_name] = param_value - datetime.timedelta(minutes=1)\n        elif isinstance(param_obj, parameter.DateHourParameter):\n            previous_date_params[param_name] = param_value - datetime.timedelta(hours=1)\n        elif isinstance(param_obj, parameter.DateIntervalParameter):\n            previous_date_params[param_name] = param_value.prev()\n        else:\n            previous_params[param_name] = param_value\n\n    previous_params.update(previous_date_params)\n\n    if len(previous_date_params) == 0:\n        raise NotImplementedError(\"No task parameter - can't determine previous task\")\n    elif len(previous_date_params) > 1:\n        raise NotImplementedError(\"Too many date-related task parameters - can't determine previous task\")\n    else:\n        return task.clone(**previous_params)", "response": "Returns a Task that has the same parameters as the task."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_hadoopcli_client():\n    version = hdfs_config.get_configured_hadoop_version()\n    if version == \"cdh4\":\n        return HdfsClient()\n    elif version == \"cdh3\":\n        return HdfsClientCdh3()\n    elif version == \"apache1\":\n        return HdfsClientApache1()\n    else:\n        raise ValueError(\"Error: Unknown version specified in Hadoop version\"\n                         \"configuration parameter\")", "response": "Create a new instance of the Hadoop CLI client."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef exists(self, path):\n\n        cmd = load_hadoop_cmd() + ['fs', '-stat', path]\n        logger.debug('Running file existence check: %s', subprocess.list2cmdline(cmd))\n        p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, close_fds=True, universal_newlines=True)\n        stdout, stderr = p.communicate()\n        if p.returncode == 0:\n            return True\n        else:\n            not_found_pattern = \"^.*No such file or directory$\"\n            not_found_re = re.compile(not_found_pattern)\n            for line in stderr.split('\\n'):\n                if not_found_re.match(line):\n                    return False\n            raise hdfs_error.HDFSCliError(cmd, p.returncode, stdout, stderr)", "response": "Check if a file or directory exists."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a directory in Hadoop.", "response": "def mkdir(self, path, parents=True, raise_if_exists=False):\n        \"\"\"\n        No explicit -p switch, this version of Hadoop always creates parent directories.\n        \"\"\"\n        try:\n            self.call_check(load_hadoop_cmd() + ['fs', '-mkdir', path])\n        except hdfs_error.HDFSCliError as ex:\n            if \"File exists\" in ex.stderr:\n                if raise_if_exists:\n                    raise FileAlreadyExists(ex.stderr)\n            else:\n                raise"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrunning the Hive command line and returns stdout.", "response": "def run_hive(args, check_return_code=True):\n    \"\"\"\n    Runs the `hive` from the command line, passing in the given args, and\n    returning stdout.\n\n    With the apache release of Hive, so of the table existence checks\n    (which are done using DESCRIBE do not exit with a return code of 0\n    so we need an option to ignore the return code and just return stdout for parsing\n    \"\"\"\n    cmd = load_hive_cmd() + args\n    p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    stdout, stderr = p.communicate()\n    if check_return_code and p.returncode != 0:\n        raise HiveCommandError(\"Hive command: {0} failed with error code: {1}\".format(\" \".join(cmd), p.returncode),\n                               stdout, stderr)\n    return stdout.decode('utf-8')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run_hive_script(script):\n    if not os.path.isfile(script):\n        raise RuntimeError(\"Hive script: {0} does not exist.\".format(script))\n    return run_hive(['-f', script])", "response": "Runs the given script in hive and returns stdout."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\npreparing the outputs for hive command.", "response": "def prepare_outputs(self, job):\n        \"\"\"\n        Called before job is started.\n\n        If output is a `FileSystemTarget`, create parent directories so the hive command won't fail\n        \"\"\"\n        outputs = flatten(job.output())\n        for o in outputs:\n            if isinstance(o, FileSystemTarget):\n                parent_dir = os.path.dirname(o.path)\n                if parent_dir and not o.fs.exists(parent_dir):\n                    logger.info(\"Creating parent directory %r\", parent_dir)\n                    try:\n                        # there is a possible race condition\n                        # which needs to be handled here\n                        o.fs.mkdir(parent_dir)\n                    except FileAlreadyExists:\n                        pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef path(self):\n        location = self.client.table_location(self.table, self.database)\n        if not location:\n            raise Exception(\"Couldn't find location for table: {0}\".format(str(self)))\n        return location", "response": "Returns the path to the table in HDFS."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef touch(self):\n        marker_key = self.marker_key()\n        self.redis_client.hset(marker_key, 'update_id', self.update_id)\n        self.redis_client.hset(marker_key, 'date', datetime.datetime.now())\n\n        if self.expire is not None:\n            self.redis_client.expire(marker_key, self.expire)", "response": "Mark this update as complete."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmean to be used as a context manager.", "response": "def global_instance(cls, cmdline_args, allow_override=False):\n        \"\"\"\n        Meant to be used as a context manager.\n        \"\"\"\n        orig_value = cls._instance\n        assert (orig_value is None) or allow_override\n        new_value = None\n        try:\n            new_value = CmdlineParser(cmdline_args)\n            cls._instance = new_value\n            yield new_value\n        finally:\n            assert cls._instance is new_value\n            cls._instance = orig_value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nexit with help message if not.", "response": "def _possibly_exit_with_help(parser, known_args):\n        \"\"\"\n        Check if the user passed --help[-all], if so, print a message and exit.\n        \"\"\"\n        if known_args.core_help or known_args.core_help_all:\n            parser.print_help()\n            sys.exit()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes path given current file and relative path.", "response": "def relpath(self, current_file, rel_path):\n        \"\"\"\n        Compute path given current file and relative path.\n        \"\"\"\n        script_dir = os.path.dirname(os.path.abspath(current_file))\n        rel_path = os.path.abspath(os.path.join(script_dir, rel_path))\n        return rel_path"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef args(self):\n        arglist = []\n        for k, v in six.iteritems(self.requires_hadoop()):\n            arglist.append('--' + k)\n            arglist.extend([t.output().path for t in flatten(v)])\n        arglist.extend(['--output', self.output()])\n        arglist.extend(self.job_args())\n        return arglist", "response": "Returns an array of args to pass to the job."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_event(self, event):\n        if not isinstance(event, event_pb2.Event):\n            raise TypeError(\"Expected an event_pb2.Event proto, \"\n                            \" but got %s\" % type(event))\n        self._async_writer.write(event.SerializeToString())", "response": "Adds an event to the event file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef flush(self):\n        '''Write all the enqueued bytestring before this flush call to disk.\n        Block until all the above bytestring are written.\n        '''\n        with self._lock:\n            if self._closed:\n                raise IOError('Writer is closed')\n            self._byte_queue.join()\n            self._writer.flush()", "response": "Flush all the enqueued bytestring before this flush call to disk."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef close(self):\n        '''Closes the underlying writer, flushing any pending writes first.'''\n        if not self._closed:\n            with self._lock:\n                if not self._closed:\n                    self._closed = True\n                    self._worker.stop()\n                    self._writer.flush()\n                    self._writer.close()", "response": "Closes the underlying writer flushing any pending writes first."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nextract device name from a tf. Event proto carrying tensor value.", "response": "def _extract_device_name_from_event(event):\n  \"\"\"Extract device name from a tf.Event proto carrying tensor value.\"\"\"\n  plugin_data_content = json.loads(\n      tf.compat.as_str(event.summary.value[0].metadata.plugin_data.content))\n  return plugin_data_content['device']"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _comm_tensor_data(device_name,\n                      node_name,\n                      maybe_base_expanded_node_name,\n                      output_slot,\n                      debug_op,\n                      tensor_value,\n                      wall_time):\n  \"\"\"Create a dict() as the outgoing data in the tensor data comm route.\n\n  Note: The tensor data in the comm route does not include the value of the\n  tensor in its entirety in general. Only if a tensor satisfies the following\n  conditions will its entire value be included in the return value of this\n  method:\n  1. Has a numeric data type (e.g., float32, int32) and has fewer than 5\n     elements.\n  2. Is a string tensor and has fewer than 5 elements. Each string element is\n     up to 40 bytes.\n\n  Args:\n    device_name: Name of the device that the tensor is on.\n    node_name: (Original) name of the node that produces the tensor.\n    maybe_base_expanded_node_name: Possbily base-expanded node name.\n    output_slot: Output slot number.\n    debug_op: Name of the debug op.\n    tensor_value: Value of the tensor, as a numpy.ndarray.\n    wall_time: Wall timestamp for the tensor.\n\n  Returns:\n    A dict representing the tensor data.\n  \"\"\"\n  output_slot = int(output_slot)\n  logger.info(\n      'Recording tensor value: %s, %d, %s', node_name, output_slot, debug_op)\n  tensor_values = None\n  if isinstance(tensor_value, debug_data.InconvertibleTensorProto):\n    if not tensor_value.initialized:\n      tensor_dtype = UNINITIALIZED_TAG\n      tensor_shape = UNINITIALIZED_TAG\n    else:\n      tensor_dtype = UNSUPPORTED_TAG\n      tensor_shape = UNSUPPORTED_TAG\n    tensor_values = NA_TAG\n  else:\n    tensor_dtype = tensor_helper.translate_dtype(tensor_value.dtype)\n    tensor_shape = tensor_value.shape\n\n    # The /comm endpoint should respond with tensor values only if the tensor is\n    # small enough. Otherwise, the detailed values sould be queried through a\n    # dedicated tensor_data that supports slicing.\n    if tensor_helper.numel(tensor_shape) < 5:\n      _, _, tensor_values = tensor_helper.array_view(tensor_value)\n      if tensor_dtype == 'string' and tensor_value is not None:\n        tensor_values = tensor_helper.process_buffers_for_display(\n            tensor_values, limit=STRING_ELEMENT_MAX_LEN)\n\n  return {\n      'type': 'tensor',\n      'timestamp': wall_time,\n      'data': {\n          'device_name': device_name,\n          'node_name': node_name,\n          'maybe_base_expanded_node_name': maybe_base_expanded_node_name,\n          'output_slot': output_slot,\n          'debug_op': debug_op,\n          'dtype': tensor_dtype,\n          'shape': tensor_shape,\n          'values': tensor_values,\n      },\n  }", "response": "This function is used to create a dict that represents the tensor data in the comm route."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a GraphDef proto to the graph list.", "response": "def add_graph(self, run_key, device_name, graph_def, debug=False):\n    \"\"\"Add a GraphDef.\n\n    Args:\n      run_key: A key for the run, containing information about the feeds,\n        fetches, and targets.\n      device_name: The name of the device that the `GraphDef` is for.\n      graph_def: An instance of the `GraphDef` proto.\n      debug: Whether `graph_def` consists of the debug ops.\n    \"\"\"\n    graph_dict = (self._run_key_to_debug_graphs if debug else\n                  self._run_key_to_original_graphs)\n    if not run_key in graph_dict:\n      graph_dict[run_key] = dict()  # Mapping device_name to GraphDef.\n    graph_dict[run_key][tf.compat.as_str(device_name)] = (\n        debug_graphs_helper.DebugGraphWrapper(graph_def))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the runtime GraphDef protos associated with a run key.", "response": "def get_graphs(self, run_key, debug=False):\n    \"\"\"Get the runtime GraphDef protos associated with a run key.\n\n    Args:\n      run_key: A Session.run kay.\n      debug: Whether the debugger-decoratedgraph is to be retrieved.\n\n    Returns:\n      A `dict` mapping device name to `GraphDef` protos.\n    \"\"\"\n    graph_dict = (self._run_key_to_debug_graphs if debug else\n                  self._run_key_to_original_graphs)\n    graph_wrappers = graph_dict.get(run_key, {})\n    graph_defs = dict()\n    for device_name, wrapper in graph_wrappers.items():\n      graph_defs[device_name] = wrapper.graph_def\n    return graph_defs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the runtime GraphDef proto associated with a run key and a device.", "response": "def get_graph(self, run_key, device_name, debug=False):\n    \"\"\"Get the runtime GraphDef proto associated with a run key and a device.\n\n    Args:\n      run_key: A Session.run kay.\n      device_name: Name of the device in question.\n      debug: Whether the debugger-decoratedgraph is to be retrieved.\n\n    Returns:\n      A `GraphDef` proto.\n    \"\"\"\n    return self.get_graphs(run_key, debug=debug).get(device_name, None)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the base - expanded node name for a given node name.", "response": "def get_maybe_base_expanded_node_name(self, node_name, run_key, device_name):\n    \"\"\"Obtain possibly base-expanded node name.\n\n    Base-expansion is the transformation of a node name which happens to be the\n    name scope of other nodes in the same graph. For example, if two nodes,\n    called 'a/b' and 'a/b/read' in a graph, the name of the first node will\n    be base-expanded to 'a/b/(b)'.\n\n    This method uses caching to avoid unnecessary recomputation.\n\n    Args:\n      node_name: Name of the node.\n      run_key: The run key to which the node belongs.\n      graph_def: GraphDef to which the node belongs.\n\n    Raises:\n      ValueError: If `run_key` and/or `device_name` do not exist in the record.\n    \"\"\"\n    device_name = tf.compat.as_str(device_name)\n    if run_key not in self._run_key_to_original_graphs:\n      raise ValueError('Unknown run_key: %s' % run_key)\n    if device_name not in self._run_key_to_original_graphs[run_key]:\n      raise ValueError(\n          'Unknown device for run key \"%s\": %s' % (run_key, device_name))\n    return self._run_key_to_original_graphs[\n        run_key][device_name].maybe_base_expanded_node_name(node_name)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef on_core_metadata_event(self, event):\n    core_metadata = json.loads(event.log_message.message)\n    input_names = ','.join(core_metadata['input_names'])\n    output_names = ','.join(core_metadata['output_names'])\n    target_nodes = ','.join(core_metadata['target_nodes'])\n\n    self._run_key = RunKey(input_names, output_names, target_nodes)\n    if not self._graph_defs:\n      self._graph_defs_arrive_first = False\n    else:\n      for device_name in self._graph_defs:\n        self._add_graph_def(device_name, self._graph_defs[device_name])\n\n    self._outgoing_channel.put(_comm_metadata(self._run_key, event.wall_time))\n\n    # Wait for acknowledgement from client. Blocks until an item is got.\n    logger.info('on_core_metadata_event() waiting for client ack (meta)...')\n    self._incoming_channel.get()\n    logger.info('on_core_metadata_event() client ack received (meta).')", "response": "Implementation of the core metadata - carrying Event proto callback."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrecording the summary values based on an updated message from the debugger.", "response": "def on_value_event(self, event):\n    \"\"\"Records the summary values based on an updated message from the debugger.\n\n    Logs an error message if writing the event to disk fails.\n\n    Args:\n      event: The Event proto to be processed.\n    \"\"\"\n    if not event.summary.value:\n      logger.info('The summary of the event lacks a value.')\n      return None\n\n    # The node name property in the event proto is actually a watch key, which\n    # is a concatenation of several pieces of data.\n    watch_key = event.summary.value[0].node_name\n    tensor_value = debug_data.load_tensor_from_event(event)\n    device_name = _extract_device_name_from_event(event)\n    node_name, output_slot, debug_op = (\n        event.summary.value[0].node_name.split(':'))\n    maybe_base_expanded_node_name = (\n        self._run_states.get_maybe_base_expanded_node_name(node_name,\n                                                           self._run_key,\n                                                           device_name))\n    self._tensor_store.add(watch_key, tensor_value)\n    self._outgoing_channel.put(_comm_tensor_data(\n        device_name, node_name, maybe_base_expanded_node_name, output_slot,\n        debug_op, tensor_value, event.wall_time))\n\n    logger.info('on_value_event(): waiting for client ack (tensors)...')\n    self._incoming_channel.get()\n    logger.info('on_value_event(): client ack received (tensor).')\n\n    # Determine if the particular debug watch key is in the current list of\n    # breakpoints. If it is, send an EventReply() to unblock the debug op.\n    if self._is_debug_node_in_breakpoints(event.summary.value[0].node_name):\n      logger.info('Sending empty EventReply for breakpoint: %s',\n                      event.summary.value[0].node_name)\n      # TODO(cais): Support receiving and sending tensor value from front-end.\n      return debug_service_pb2.EventReply()\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a DebuggedSourceFile proto.", "response": "def add_debugged_source_file(self, debugged_source_file):\n    \"\"\"Add a DebuggedSourceFile proto.\"\"\"\n    # TODO(cais): Should the key include a host name, for certain distributed\n    #   cases?\n    key = debugged_source_file.file_path\n    self._source_file_host[key] = debugged_source_file.host\n    self._source_file_last_modified[key] = debugged_source_file.last_modified\n    self._source_file_bytes[key] = debugged_source_file.bytes\n    self._source_file_content[key] = debugged_source_file.lines"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the traceback of an op in the latest version of the TF graph.", "response": "def get_op_traceback(self, op_name):\n    \"\"\"Get the traceback of an op in the latest version of the TF graph.\n\n    Args:\n      op_name: Name of the op.\n\n    Returns:\n      Creation traceback of the op, in the form of a list of 2-tuples:\n        (file_path, lineno)\n\n    Raises:\n      ValueError: If the op with the given name cannot be found in the latest\n        version of the graph that this SourceManager instance has received, or\n        if this SourceManager instance has not received any graph traceback yet.\n    \"\"\"\n    if not self._graph_traceback:\n      raise ValueError('No graph traceback has been received yet.')\n    for op_log_entry in self._graph_traceback.log_entries:\n      if op_log_entry.name == op_name:\n        return self._code_def_to_traceback_list(op_log_entry.code_def)\n    raise ValueError(\n        'No op named \"%s\" can be found in the graph of the latest version '\n        ' (%d).' % (op_name, self._graph_version))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of tuples that map line numbers to the list of ops created at lines of a source file.", "response": "def get_file_tracebacks(self, file_path):\n    \"\"\"Get the lists of ops created at lines of a specified source file.\n\n    Args:\n      file_path: Path to the source file.\n\n    Returns:\n      A dict mapping line number to a list of 2-tuples,\n        `(op_name, stack_position)`\n      `op_name` is the name of the name of the op whose creation traceback\n        includes the line.\n      `stack_position` is the position of the line in the op's creation\n        traceback, represented as a 0-based integer.\n\n    Raises:\n      ValueError: If `file_path` does not point to a source file that has been\n        received by this instance of `SourceManager`.\n    \"\"\"\n    if file_path not in self._source_file_content:\n      raise ValueError(\n          'Source file of path \"%s\" has not been received by this instance of '\n          'SourceManager.' % file_path)\n\n    lineno_to_op_names_and_stack_position = dict()\n    for op_log_entry in self._graph_traceback.log_entries:\n      for stack_pos, trace in enumerate(op_log_entry.code_def.traces):\n        if self._graph_traceback.id_to_string[trace.file_id] == file_path:\n          if trace.lineno not in lineno_to_op_names_and_stack_position:\n            lineno_to_op_names_and_stack_position[trace.lineno] = []\n          lineno_to_op_names_and_stack_position[trace.lineno].append(\n              (op_log_entry.name, stack_pos))\n    return lineno_to_op_names_and_stack_position"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef query_tensor_store(self,\n                         watch_key,\n                         time_indices=None,\n                         slicing=None,\n                         mapping=None):\n    \"\"\"Query tensor store for a given debugged tensor value.\n\n    Args:\n      watch_key: The watch key of the debugged tensor being sought. Format:\n        <node_name>:<output_slot>:<debug_op>\n        E.g., Dense_1/MatMul:0:DebugIdentity.\n      time_indices: Optional time indices string By default, the lastest time\n        index ('-1') is returned.\n      slicing: Optional slicing string.\n      mapping: Optional mapping string, e.g., 'image/png'.\n\n    Returns:\n      If mapping is `None`, the possibly sliced values as a nested list of\n        values or its mapped format. A `list` of nested `list` of values,\n      If mapping is not `None`, the format of the return value will depend on\n        the mapping.\n    \"\"\"\n    return self._tensor_store.query(watch_key,\n                                    time_indices=time_indices,\n                                    slicing=slicing,\n                                    mapping=mapping)", "response": "Query the tensor store for a given debugged tensor value."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef Respond(request,\n            content,\n            content_type,\n            code=200,\n            expires=0,\n            content_encoding=None,\n            encoding='utf-8'):\n  \"\"\"Construct a werkzeug Response.\n\n  Responses are transmitted to the browser with compression if: a) the browser\n  supports it; b) it's sane to compress the content_type in question; and c)\n  the content isn't already compressed, as indicated by the content_encoding\n  parameter.\n\n  Browser and proxy caching is completely disabled by default. If the expires\n  parameter is greater than zero then the response will be able to be cached by\n  the browser for that many seconds; however, proxies are still forbidden from\n  caching so that developers can bypass the cache with Ctrl+Shift+R.\n\n  For textual content that isn't JSON, the encoding parameter is used as the\n  transmission charset which is automatically appended to the Content-Type\n  header. That is unless of course the content_type parameter contains a\n  charset parameter. If the two disagree, the characters in content will be\n  transcoded to the latter.\n\n  If content_type declares a JSON media type, then content MAY be a dict, list,\n  tuple, or set, in which case this function has an implicit composition with\n  json_util.Cleanse and json.dumps. The encoding parameter is used to decode\n  byte strings within the JSON object; therefore transmitting binary data\n  within JSON is not permitted. JSON is transmitted as ASCII unless the\n  content_type parameter explicitly defines a charset parameter, in which case\n  the serialized JSON bytes will use that instead of escape sequences.\n\n  Args:\n    request: A werkzeug Request object. Used mostly to check the\n      Accept-Encoding header.\n    content: Payload data as byte string, unicode string, or maybe JSON.\n    content_type: Media type and optionally an output charset.\n    code: Numeric HTTP status code to use.\n    expires: Second duration for browser caching.\n    content_encoding: Encoding if content is already encoded, e.g. 'gzip'.\n    encoding: Input charset if content parameter has byte strings.\n\n  Returns:\n    A werkzeug Response object (a WSGI application).\n  \"\"\"\n\n  mimetype = _EXTRACT_MIMETYPE_PATTERN.search(content_type).group(0)\n  charset_match = _EXTRACT_CHARSET_PATTERN.search(content_type)\n  charset = charset_match.group(1) if charset_match else encoding\n  textual = charset_match or mimetype in _TEXTUAL_MIMETYPES\n  if (mimetype in _JSON_MIMETYPES and\n      isinstance(content, (dict, list, set, tuple))):\n    content = json.dumps(json_util.Cleanse(content, encoding),\n                         ensure_ascii=not charset_match)\n  if charset != encoding:\n    content = tf.compat.as_text(content, encoding)\n  content = tf.compat.as_bytes(content, charset)\n  if textual and not charset_match and mimetype not in _JSON_MIMETYPES:\n    content_type += '; charset=' + charset\n  gzip_accepted = _ALLOWS_GZIP_PATTERN.search(\n      request.headers.get('Accept-Encoding', ''))\n  # Automatically gzip uncompressed text data if accepted.\n  if textual and not content_encoding and gzip_accepted:\n    out = six.BytesIO()\n    # Set mtime to zero to make payload for a given input deterministic.\n    with gzip.GzipFile(fileobj=out, mode='wb', compresslevel=3, mtime=0) as f:\n      f.write(content)\n    content = out.getvalue()\n    content_encoding = 'gzip'\n\n  content_length = len(content)\n  direct_passthrough = False\n  # Automatically streamwise-gunzip precompressed data if not accepted.\n  if content_encoding == 'gzip' and not gzip_accepted:\n    gzip_file = gzip.GzipFile(fileobj=six.BytesIO(content), mode='rb')\n    # Last 4 bytes of gzip formatted data (little-endian) store the original\n    # content length mod 2^32; we just assume it's the content length. That\n    # means we can't streamwise-gunzip >4 GB precompressed file; this is ok.\n    content_length = struct.unpack('<I', content[-4:])[0]\n    content = werkzeug.wsgi.wrap_file(request.environ, gzip_file)\n    content_encoding = None\n    direct_passthrough = True\n\n  headers = []\n  headers.append(('Content-Length', str(content_length)))\n  if content_encoding:\n    headers.append(('Content-Encoding', content_encoding))\n  if expires > 0:\n    e = wsgiref.handlers.format_date_time(time.time() + float(expires))\n    headers.append(('Expires', e))\n    headers.append(('Cache-Control', 'private, max-age=%d' % expires))\n  else:\n    headers.append(('Expires', '0'))\n    headers.append(('Cache-Control', 'no-cache, must-revalidate'))\n\n  if request.method == 'HEAD':\n    content = None\n\n  return werkzeug.wrappers.Response(\n      response=content, status=code, headers=headers, content_type=content_type,\n      direct_passthrough=direct_passthrough)", "response": "Construct a werkzeug Response object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfind the longest parent - path of path in path_set.", "response": "def _find_longest_parent_path(path_set, path):\n  \"\"\"Finds the longest \"parent-path\" of 'path' in 'path_set'.\n\n  This function takes and returns \"path-like\" strings which are strings\n  made of strings separated by os.sep. No file access is performed here, so\n  these strings need not correspond to actual files in some file-system..\n  This function returns the longest ancestor path\n  For example, for path_set=[\"/foo/bar\", \"/foo\", \"/bar/foo\"] and\n  path=\"/foo/bar/sub_dir\", returns \"/foo/bar\".\n\n  Args:\n    path_set: set of path-like strings -- e.g. a list of strings separated by\n      os.sep. No actual disk-access is performed here, so these need not\n      correspond to actual files.\n    path: a path-like string.\n\n  Returns:\n    The element in path_set which is the longest parent directory of 'path'.\n  \"\"\"\n  # This could likely be more efficiently implemented with a trie\n  # data-structure, but we don't want to add an extra dependency for that.\n  while path not in path_set:\n    if not path:\n      return None\n    path = os.path.dirname(path)\n  return path"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert a google. protobuf. Value message to a string representation.", "response": "def _protobuf_value_to_string(value):\n  \"\"\"Returns a string representation of given google.protobuf.Value message.\n\n  Args:\n    value: google.protobuf.Value message. Assumed to be of type 'number',\n      'string' or 'bool'.\n  \"\"\"\n  value_in_json = json_format.MessageToJson(value)\n  if value.HasField(\"string_value\"):\n    # Remove the quotations.\n    return value_in_json[1:-1]\n  return value_in_json"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfinds the experiment associated with the metadata. EXPERIMENT_TAG tag.", "response": "def _find_experiment_tag(self):\n    \"\"\"Finds the experiment associcated with the metadata.EXPERIMENT_TAG tag.\n\n    Caches the experiment if it was found.\n\n    Returns:\n      The experiment or None if no such experiment is found.\n    \"\"\"\n    with self._experiment_from_tag_lock:\n      if self._experiment_from_tag is None:\n        mapping = self.multiplexer.PluginRunToTagToContent(\n            metadata.PLUGIN_NAME)\n        for tag_to_content in mapping.values():\n          if metadata.EXPERIMENT_TAG in tag_to_content:\n            self._experiment_from_tag = metadata.parse_experiment_plugin_data(\n                tag_to_content[metadata.EXPERIMENT_TAG])\n            break\n    return self._experiment_from_tag"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes a minimal Experiment protocol buffer by scanning the runs.", "response": "def _compute_experiment_from_runs(self):\n    \"\"\"Computes a minimal Experiment protocol buffer by scanning the runs.\"\"\"\n    hparam_infos = self._compute_hparam_infos()\n    if not hparam_infos:\n      return None\n    metric_infos = self._compute_metric_infos()\n    return api_pb2.Experiment(hparam_infos=hparam_infos,\n                              metric_infos=metric_infos)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute a list of api_pb2. HParamInfo messages from the current run tag info.", "response": "def _compute_hparam_infos(self):\n    \"\"\"Computes a list of api_pb2.HParamInfo from the current run, tag info.\n\n    Finds all the SessionStartInfo messages and collects the hparams values\n    appearing in each one. For each hparam attempts to deduce a type that fits\n    all its values. Finally, sets the 'domain' of the resulting HParamInfo\n    to be discrete if the type is string and the number of distinct values is\n    small enough.\n\n    Returns:\n      A list of api_pb2.HParamInfo messages.\n    \"\"\"\n    run_to_tag_to_content = self.multiplexer.PluginRunToTagToContent(\n        metadata.PLUGIN_NAME)\n    # Construct a dict mapping an hparam name to its list of values.\n    hparams = collections.defaultdict(list)\n    for tag_to_content in run_to_tag_to_content.values():\n      if metadata.SESSION_START_INFO_TAG not in tag_to_content:\n        continue\n      start_info = metadata.parse_session_start_info_plugin_data(\n          tag_to_content[metadata.SESSION_START_INFO_TAG])\n      for (name, value) in six.iteritems(start_info.hparams):\n        hparams[name].append(value)\n\n    # Try to construct an HParamInfo for each hparam from its name and list\n    # of values.\n    result = []\n    for (name, values) in six.iteritems(hparams):\n      hparam_info = self._compute_hparam_info_from_values(name, values)\n      if hparam_info is not None:\n        result.append(hparam_info)\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _compute_hparam_info_from_values(self, name, values):\n    # Figure out the type from the values.\n    # Ignore values whose type is not listed in api_pb2.DataType\n    # If all values have the same type, then that is the type used.\n    # Otherwise, the returned type is DATA_TYPE_STRING.\n    result = api_pb2.HParamInfo(name=name, type=api_pb2.DATA_TYPE_UNSET)\n    distinct_values = set(\n        _protobuf_value_to_string(v) for v in values if _protobuf_value_type(v))\n    for v in values:\n      v_type = _protobuf_value_type(v)\n      if not v_type:\n        continue\n      if result.type == api_pb2.DATA_TYPE_UNSET:\n        result.type = v_type\n      elif result.type != v_type:\n        result.type = api_pb2.DATA_TYPE_STRING\n      if result.type == api_pb2.DATA_TYPE_STRING:\n        # A string result.type does not change, so we can exit the loop.\n        break\n\n    # If we couldn't figure out a type, then we can't compute the hparam_info.\n    if result.type == api_pb2.DATA_TYPE_UNSET:\n      return None\n\n    # If the result is a string, set the domain to be the distinct values if\n    # there aren't too many of them.\n    if (result.type == api_pb2.DATA_TYPE_STRING\n        and len(distinct_values) <= self._max_domain_discrete_len):\n      result.domain_discrete.extend(distinct_values)\n\n    return result", "response": "Computes an HParamInfo message from the name and list of values."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes the list of metric names from all the scalar runs.", "response": "def _compute_metric_names(self):\n    \"\"\"Computes the list of metric names from all the scalar (run, tag) pairs.\n\n    The return value is a list of (tag, group) pairs representing the metric\n    names. The list is sorted in Python tuple-order (lexicographical).\n\n    For example, if the scalar (run, tag) pairs are:\n    (\"exp/session1\", \"loss\")\n    (\"exp/session2\", \"loss\")\n    (\"exp/session2/eval\", \"loss\")\n    (\"exp/session2/validation\", \"accuracy\")\n    (\"exp/no-session\", \"loss_2\"),\n    and the runs corresponding to sessions are \"exp/session1\", \"exp/session2\",\n    this method will return [(\"loss\", \"\"), (\"loss\", \"/eval\"), (\"accuracy\",\n    \"/validation\")]\n\n    More precisely, each scalar (run, tag) pair is converted to a (tag, group)\n    metric name, where group is the suffix of run formed by removing the\n    longest prefix which is a session run. If no session run is a prefix of\n    'run', the pair is skipped.\n\n    Returns:\n      A python list containing pairs. Each pair is a (tag, group) pair\n      representing a metric name used in some session.\n    \"\"\"\n    session_runs = self._build_session_runs_set()\n    metric_names_set = set()\n    run_to_tag_to_content = self.multiplexer.PluginRunToTagToContent(\n        scalar_metadata.PLUGIN_NAME)\n    for (run, tag_to_content) in six.iteritems(run_to_tag_to_content):\n      session = _find_longest_parent_path(session_runs, run)\n      if not session:\n        continue\n      group = os.path.relpath(run, session)\n      # relpath() returns \".\" for the 'session' directory, we use an empty\n      # string.\n      if group == \".\":\n        group = \"\"\n      metric_names_set.update((tag, group) for tag in tag_to_content.keys())\n    metric_names_list = list(metric_names_set)\n    # Sort metrics for determinism.\n    metric_names_list.sort()\n    return metric_names_list"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nhandle the request specified on construction. Returns an Experiment object.", "response": "def run(self):\n    \"\"\"Handles the request specified on construction.\n\n    Returns:\n      An Experiment object.\n\n    \"\"\"\n    experiment = self._context.experiment()\n    if experiment is None:\n      raise error.HParamsError(\n          \"Can't find an HParams-plugin experiment data in\"\n          \" the log directory. Note that it takes some time to\"\n          \" scan the log directory; if you just started\"\n          \" Tensorboard it could be that we haven't finished\"\n          \" scanning it yet. Consider trying again in a\"\n          \" few seconds.\")\n    return experiment"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef experiment_pb(\n    hparam_infos,\n    metric_infos,\n    user='',\n    description='',\n    time_created_secs=None):\n  \"\"\"Creates a summary that defines a hyperparameter-tuning experiment.\n\n  Args:\n    hparam_infos: Array of api_pb2.HParamInfo messages. Describes the\n        hyperparameters used in the experiment.\n    metric_infos: Array of api_pb2.MetricInfo messages. Describes the metrics\n        used in the experiment. See the documentation at the top of this file\n        for how to populate this.\n    user: String. An id for the user running the experiment\n    description: String. A description for the experiment. May contain markdown.\n    time_created_secs: float. The time the experiment is created in seconds\n    since the UNIX epoch. If None uses the current time.\n\n  Returns:\n    A summary protobuffer containing the experiment definition.\n  \"\"\"\n  if time_created_secs is None:\n    time_created_secs = time.time()\n  experiment = api_pb2.Experiment(\n      description=description,\n      user=user,\n      time_created_secs=time_created_secs,\n      hparam_infos=hparam_infos,\n      metric_infos=metric_infos)\n  return _summary(metadata.EXPERIMENT_TAG,\n                  plugin_data_pb2.HParamsPluginData(experiment=experiment))", "response": "Creates a summary that defines a hyperparameter - tuning experiment."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef session_start_pb(hparams,\n                     model_uri='',\n                     monitor_url='',\n                     group_name='',\n                     start_time_secs=None):\n  \"\"\"Constructs a SessionStartInfo protobuffer.\n\n  Creates a summary that contains a training session metadata information.\n  One such summary per training session should be created. Each should have\n  a different run.\n\n  Args:\n    hparams: A dictionary with string keys. Describes the hyperparameter values\n             used in the session, mapping each hyperparameter name to its value.\n             Supported value types are  `bool`, `int`, `float`, `str`, `list`,\n             `tuple`.\n             The type of value must correspond to the type of hyperparameter\n             (defined in the corresponding api_pb2.HParamInfo member of the\n             Experiment protobuf) as follows:\n\n              +-----------------+---------------------------------+\n              |Hyperparameter   | Allowed (Python) value types    |\n              |type             |                                 |\n              +-----------------+---------------------------------+\n              |DATA_TYPE_BOOL   | bool                            |\n              |DATA_TYPE_FLOAT64| int, float                      |\n              |DATA_TYPE_STRING | six.string_types, tuple, list   |\n              +-----------------+---------------------------------+\n\n             Tuple and list instances will be converted to their string\n             representation.\n    model_uri: See the comment for the field with the same name of\n               plugin_data_pb2.SessionStartInfo.\n    monitor_url: See the comment for the field with the same name of\n                 plugin_data_pb2.SessionStartInfo.\n    group_name:  See the comment for the field with the same name of\n                 plugin_data_pb2.SessionStartInfo.\n    start_time_secs: float. The time to use as the session start time.\n                     Represented as seconds since the UNIX epoch. If None uses\n                     the current time.\n  Returns:\n    The summary protobuffer mentioned above.\n  \"\"\"\n  if start_time_secs is None:\n    start_time_secs = time.time()\n  session_start_info = plugin_data_pb2.SessionStartInfo(\n      model_uri=model_uri,\n      monitor_url=monitor_url,\n      group_name=group_name,\n      start_time_secs=start_time_secs)\n  for (hp_name, hp_val) in six.iteritems(hparams):\n    if isinstance(hp_val, (float, int)):\n      session_start_info.hparams[hp_name].number_value = hp_val\n    elif isinstance(hp_val, six.string_types):\n      session_start_info.hparams[hp_name].string_value = hp_val\n    elif isinstance(hp_val, bool):\n      session_start_info.hparams[hp_name].bool_value = hp_val\n    elif isinstance(hp_val, (list, tuple)):\n      session_start_info.hparams[hp_name].string_value = str(hp_val)\n    else:\n      raise TypeError('hparams[%s]=%s has type: %s which is not supported' %\n                      (hp_name, hp_val, type(hp_val)))\n  return _summary(metadata.SESSION_START_INFO_TAG,\n                  plugin_data_pb2.HParamsPluginData(\n                      session_start_info=session_start_info))", "response": "Constructs a SessionStartInfo protobuffer."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconstruct a SessionEndInfo protobuffer.", "response": "def session_end_pb(status, end_time_secs=None):\n  \"\"\"Constructs a SessionEndInfo protobuffer.\n\n  Creates a summary that contains status information for a completed\n  training session. Should be exported after the training session is completed.\n  One such summary per training session should be created. Each should have\n  a different run.\n\n  Args:\n    status: A tensorboard.hparams.Status enumeration value denoting the\n        status of the session.\n    end_time_secs: float. The time to use as the session end time. Represented\n        as seconds since the unix epoch. If None uses the current time.\n\n  Returns:\n    The summary protobuffer mentioned above.\n  \"\"\"\n  if end_time_secs is None:\n    end_time_secs = time.time()\n\n  session_end_info = plugin_data_pb2.SessionEndInfo(status=status,\n                                                    end_time_secs=end_time_secs)\n  return _summary(metadata.SESSION_END_INFO_TAG,\n                  plugin_data_pb2.HParamsPluginData(\n                      session_end_info=session_end_info))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _summary(tag, hparams_plugin_data):\n  summary = tf.compat.v1.Summary()\n  summary.value.add(\n      tag=tag,\n      metadata=metadata.create_summary_metadata(hparams_plugin_data))\n  return summary", "response": "Returns a summary holding the given HParamsPluginData message."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlists all the plugins that have registered assets in logdir.", "response": "def ListPlugins(logdir):\n  \"\"\"List all the plugins that have registered assets in logdir.\n\n  If the plugins_dir does not exist, it returns an empty list. This maintains\n  compatibility with old directories that have no plugins written.\n\n  Args:\n    logdir: A directory that was created by a TensorFlow events writer.\n\n  Returns:\n    a list of plugin names, as strings\n  \"\"\"\n  plugins_dir = os.path.join(logdir, _PLUGINS_DIR)\n  try:\n    entries = tf.io.gfile.listdir(plugins_dir)\n  except tf.errors.NotFoundError:\n    return []\n  # Strip trailing slashes, which listdir() includes for some filesystems\n  # for subdirectories, after using them to bypass IsDirectory().\n  return [x.rstrip('/') for x in entries\n          if x.endswith('/') or _IsDirectory(plugins_dir, x)]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlists all the assets that are available for given plugin in a logdir.", "response": "def ListAssets(logdir, plugin_name):\n  \"\"\"List all the assets that are available for given plugin in a logdir.\n\n  Args:\n    logdir: A directory that was created by a TensorFlow summary.FileWriter.\n    plugin_name: A string name of a plugin to list assets for.\n\n  Returns:\n    A string list of available plugin assets. If the plugin subdirectory does\n    not exist (either because the logdir doesn't exist, or because the plugin\n    didn't register) an empty list is returned.\n  \"\"\"\n  plugin_dir = PluginDirectory(logdir, plugin_name)\n  try:\n    # Strip trailing slashes, which listdir() includes for some filesystems.\n    return [x.rstrip('/') for x in tf.io.gfile.listdir(plugin_dir)]\n  except tf.errors.NotFoundError:\n    return []"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretrieves a particular plugin asset from a logdir.", "response": "def RetrieveAsset(logdir, plugin_name, asset_name):\n  \"\"\"Retrieve a particular plugin asset from a logdir.\n\n  Args:\n    logdir: A directory that was created by a TensorFlow summary.FileWriter.\n    plugin_name: The plugin we want an asset from.\n    asset_name: The name of the requested asset.\n\n  Returns:\n    string contents of the plugin asset.\n\n  Raises:\n    KeyError: if the asset does not exist.\n  \"\"\"\n\n  asset_path = os.path.join(PluginDirectory(logdir, plugin_name), asset_name)\n  try:\n    with tf.io.gfile.GFile(asset_path, \"r\") as f:\n      return f.read()\n  except tf.errors.NotFoundError:\n    raise KeyError(\"Asset path %s not found\" % asset_path)\n  except tf.errors.OpError as e:\n    raise KeyError(\"Couldn't read asset path: %s, OpError %s\" % (asset_path, e))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef distributions_impl(self, tag, run):\n    (histograms, mime_type) = self._histograms_plugin.histograms_impl(\n        tag, run, downsample_to=self.SAMPLE_SIZE)\n    return ([self._compress(histogram) for histogram in histograms],\n            mime_type)", "response": "Result of the form ( body mime_type )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngive a tag and single run return an array of compressed histograms.", "response": "def distributions_route(self, request):\n    \"\"\"Given a tag and single run, return an array of compressed histograms.\"\"\"\n    tag = request.args.get('tag')\n    run = request.args.get('run')\n    try:\n      (body, mime_type) = self.distributions_impl(tag, run)\n      code = 200\n    except ValueError as e:\n      (body, mime_type) = (str(e), 'text/plain')\n      code = 400\n    return http_util.Respond(request, body, mime_type, code=code)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads new values from the file system.", "response": "def Load(self):\n    \"\"\"Loads new values.\n\n    The watcher will load from one path at a time; as soon as that path stops\n    yielding events, it will move on to the next path. We assume that old paths\n    are never modified after a newer path has been written. As a result, Load()\n    can be called multiple times in a row without losing events that have not\n    been yielded yet. In other words, we guarantee that every event will be\n    yielded exactly once.\n\n    Yields:\n      All values that have not been yielded yet.\n\n    Raises:\n      DirectoryDeletedError: If the directory has been permanently deleted\n        (as opposed to being temporarily unavailable).\n    \"\"\"\n    try:\n      for event in self._LoadInternal():\n        yield event\n    except tf.errors.OpError:\n      if not tf.io.gfile.exists(self._directory):\n        raise DirectoryDeletedError(\n            'Directory %s has been permanently deleted' % self._directory)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _LoadInternal(self):\n\n    # If the loader exists, check it for a value.\n    if not self._loader:\n      self._InitializeLoader()\n\n    while True:\n      # Yield all the new events in the path we're currently loading from.\n      for event in self._loader.Load():\n        yield event\n\n      next_path = self._GetNextPath()\n      if not next_path:\n        logger.info('No path found after %s', self._path)\n        # Current path is empty and there are no new paths, so we're done.\n        return\n\n      # There's a new path, so check to make sure there weren't any events\n      # written between when we finished reading the current path and when we\n      # checked for the new one. The sequence of events might look something\n      # like this:\n      #\n      # 1. Event #1 written to path #1.\n      # 2. We check for events and yield event #1 from path #1\n      # 3. We check for events and see that there are no more events in path #1.\n      # 4. Event #2 is written to path #1.\n      # 5. Event #3 is written to path #2.\n      # 6. We check for a new path and see that path #2 exists.\n      #\n      # Without this loop, we would miss event #2. We're also guaranteed by the\n      # loader contract that no more events will be written to path #1 after\n      # events start being written to path #2, so we don't have to worry about\n      # that.\n      for event in self._loader.Load():\n        yield event\n\n      logger.info('Directory watcher advancing from %s to %s', self._path,\n                      next_path)\n\n      # Advance to the next path and start over.\n      self._SetPath(next_path)", "response": "Internal implementation of Load."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the current path to watch for new events.", "response": "def _SetPath(self, path):\n    \"\"\"Sets the current path to watch for new events.\n\n    This also records the size of the old path, if any. If the size can't be\n    found, an error is logged.\n\n    Args:\n      path: The full path of the file to watch.\n    \"\"\"\n    old_path = self._path\n    if old_path and not io_wrapper.IsCloudPath(old_path):\n      try:\n        # We're done with the path, so store its size.\n        size = tf.io.gfile.stat(old_path).length\n        logger.debug('Setting latest size of %s to %d', old_path, size)\n        self._finalized_sizes[old_path] = size\n      except tf.errors.OpError as e:\n        logger.error('Unable to get size of %s: %s', old_path, e)\n\n    self._path = path\n    self._loader = self._loader_factory(path)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the next path to load from.", "response": "def _GetNextPath(self):\n    \"\"\"Gets the next path to load from.\n\n    This function also does the checking for out-of-order writes as it iterates\n    through the paths.\n\n    Returns:\n      The next path to load events from, or None if there are no more paths.\n    \"\"\"\n    paths = sorted(path\n                   for path in io_wrapper.ListDirectoryAbsolute(self._directory)\n                   if self._path_filter(path))\n    if not paths:\n      return None\n\n    if self._path is None:\n      return paths[0]\n\n    # Don't bother checking if the paths are GCS (which we can't check) or if\n    # we've already detected an OOO write.\n    if not io_wrapper.IsCloudPath(paths[0]) and not self._ooo_writes_detected:\n      # Check the previous _OOO_WRITE_CHECK_COUNT paths for out of order writes.\n      current_path_index = bisect.bisect_left(paths, self._path)\n      ooo_check_start = max(0, current_path_index - self._OOO_WRITE_CHECK_COUNT)\n      for path in paths[ooo_check_start:current_path_index]:\n        if self._HasOOOWrite(path):\n          self._ooo_writes_detected = True\n          break\n\n    next_paths = list(path\n                      for path in paths\n                      if self._path is None or path > self._path)\n    if next_paths:\n      return min(next_paths)\n    else:\n      return None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn whether the path has an out - of - order write.", "response": "def _HasOOOWrite(self, path):\n    \"\"\"Returns whether the path has had an out-of-order write.\"\"\"\n    # Check the sizes of each path before the current one.\n    size = tf.io.gfile.stat(path).length\n    old_size = self._finalized_sizes.get(path, None)\n    if size != old_size:\n      if old_size is None:\n        logger.error('File %s created after file %s even though it\\'s '\n                         'lexicographically earlier', path, self._path)\n      else:\n        logger.error('File %s updated even though the current file is %s',\n                         path, self._path)\n      return True\n    else:\n      return False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of examples from the provided path.", "response": "def example_protos_from_path(path,\n                             num_examples=10,\n                             start_index=0,\n                             parse_examples=True,\n                             sampling_odds=1,\n                             example_class=tf.train.Example):\n  \"\"\"Returns a number of examples from the provided path.\n\n  Args:\n    path: A string path to the examples.\n    num_examples: The maximum number of examples to return from the path.\n    parse_examples: If true then parses the serialized proto from the path into\n        proto objects. Defaults to True.\n    sampling_odds: Odds of loading an example, used for sampling. When >= 1\n        (the default), then all examples are loaded.\n    example_class: tf.train.Example or tf.train.SequenceExample class to load.\n        Defaults to tf.train.Example.\n\n  Returns:\n    A list of Example protos or serialized proto strings at the path.\n\n  Raises:\n    InvalidUserInputError: If examples cannot be procured from the path.\n  \"\"\"\n\n  def append_examples_from_iterable(iterable, examples):\n    for value in iterable:\n      if sampling_odds >= 1 or random.random() < sampling_odds:\n        examples.append(\n            example_class.FromString(value) if parse_examples else value)\n        if len(examples) >= num_examples:\n          return\n\n  examples = []\n\n  if path.endswith('.csv'):\n    def are_floats(values):\n      for value in values:\n        try:\n          float(value)\n        except ValueError:\n          return False\n      return True\n    csv.register_dialect('CsvDialect', skipinitialspace=True)\n    rows = csv.DictReader(open(path), dialect='CsvDialect')\n    for row in rows:\n      if sampling_odds < 1 and random.random() > sampling_odds:\n        continue\n      example = tf.train.Example()\n      for col in row.keys():\n          # Parse out individual values from vertical-bar-delimited lists\n          values = [val.strip() for val in row[col].split('|')]\n          if are_floats(values):\n            example.features.feature[col].float_list.value.extend(\n              [float(val) for val in values])\n          else:\n            example.features.feature[col].bytes_list.value.extend(\n              [val.encode('utf-8') for val in values])\n      examples.append(\n        example if parse_examples else example.SerializeToString())\n      if len(examples) >= num_examples:\n        break\n    return examples\n\n  filenames = filepath_to_filepath_list(path)\n  compression_types = [\n      '',  # no compression (distinct from `None`!)\n      'GZIP',\n      'ZLIB',\n  ]\n  current_compression_idx = 0\n  current_file_index = 0\n  while (current_file_index < len(filenames) and\n         current_compression_idx < len(compression_types)):\n    try:\n      record_iterator = tf.compat.v1.python_io.tf_record_iterator(\n          path=filenames[current_file_index],\n          options=tf.io.TFRecordOptions(\n              compression_types[current_compression_idx]))\n      append_examples_from_iterable(record_iterator, examples)\n      current_file_index += 1\n      if len(examples) >= num_examples:\n        break\n    except tf.errors.DataLossError:\n      current_compression_idx += 1\n    except (IOError, tf.errors.NotFoundError) as e:\n      raise common_utils.InvalidUserInputError(e)\n\n  if examples:\n    return examples\n  else:\n    raise common_utils.InvalidUserInputError(\n        'No examples found at ' + path +\n        '. Valid formats are TFRecord files.')"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsends an RPC request to the Servomatic prediction service.", "response": "def call_servo(examples, serving_bundle):\n  \"\"\"Send an RPC request to the Servomatic prediction service.\n\n  Args:\n    examples: A list of examples that matches the model spec.\n    serving_bundle: A `ServingBundle` object that contains the information to\n      make the serving request.\n\n  Returns:\n    A ClassificationResponse or RegressionResponse proto.\n  \"\"\"\n  parsed_url = urlparse('http://' + serving_bundle.inference_address)\n  channel = implementations.insecure_channel(parsed_url.hostname,\n                                             parsed_url.port)\n  stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\n\n  if serving_bundle.use_predict:\n    request = predict_pb2.PredictRequest()\n  elif serving_bundle.model_type == 'classification':\n    request = classification_pb2.ClassificationRequest()\n  else:\n    request = regression_pb2.RegressionRequest()\n  request.model_spec.name = serving_bundle.model_name\n  if serving_bundle.model_version is not None:\n    request.model_spec.version.value = serving_bundle.model_version\n  if serving_bundle.signature is not None:\n    request.model_spec.signature_name = serving_bundle.signature\n\n  if serving_bundle.use_predict:\n    # tf.compat.v1 API used here to convert tf.example into proto. This\n    # utility file is bundled in the witwidget pip package which has a dep\n    # on TensorFlow.\n    request.inputs[serving_bundle.predict_input_tensor].CopyFrom(\n      tf.compat.v1.make_tensor_proto(\n        values=[ex.SerializeToString() for ex in examples],\n        dtype=types_pb2.DT_STRING))\n  else:\n    request.input.example_list.examples.extend(examples)\n\n  if serving_bundle.use_predict:\n    return common_utils.convert_predict_response(\n      stub.Predict(request, 30.0), serving_bundle) # 30 secs timeout\n  elif serving_bundle.model_type == 'classification':\n    return stub.Classify(request, 30.0)  # 30 secs timeout\n  else:\n    return stub.Regress(request, 30.0)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert a value to a new - style value if necessary and possible.", "response": "def migrate_value(value):\n  \"\"\"Convert `value` to a new-style value, if necessary and possible.\n\n  An \"old-style\" value is a value that uses any `value` field other than\n  the `tensor` field. A \"new-style\" value is a value that uses the\n  `tensor` field. TensorBoard continues to support old-style values on\n  disk; this method converts them to new-style values so that further\n  code need only deal with one data format.\n\n  Arguments:\n    value: A `Summary.Value` object. This argument is not modified.\n\n  Returns:\n    If the `value` is an old-style value for which there is a new-style\n    equivalent, the result is the new-style value. Otherwise---if the\n    value is already new-style or does not yet have a new-style\n    equivalent---the value will be returned unchanged.\n\n  :type value: Summary.Value\n  :rtype: Summary.Value\n  \"\"\"\n  handler = {\n      'histo': _migrate_histogram_value,\n      'image': _migrate_image_value,\n      'audio': _migrate_audio_value,\n      'simple_value': _migrate_scalar_value,\n  }.get(value.WhichOneof('value'))\n  return handler(value) if handler else value"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_plugin_apps(self):\n    return {\n        '/infer': self._infer,\n        '/update_example': self._update_example,\n        '/examples_from_path': self._examples_from_path_handler,\n        '/sprite': self._serve_sprite,\n        '/duplicate_example': self._duplicate_example,\n        '/delete_example': self._delete_example,\n        '/infer_mutants': self._infer_mutants_handler,\n        '/eligible_features': self._eligible_features_from_example_handler,\n    }", "response": "Obtains a mapping between routes and handlers. Stores the logdir."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nhandles the examples_path request.", "response": "def _examples_from_path_handler(self, request):\n    \"\"\"Returns JSON of the specified examples.\n\n    Args:\n      request: A request that should contain 'examples_path' and 'max_examples'.\n\n    Returns:\n      JSON of up to max_examlpes of the examples in the path.\n    \"\"\"\n    examples_count = int(request.args.get('max_examples'))\n    examples_path = request.args.get('examples_path')\n    sampling_odds = float(request.args.get('sampling_odds'))\n    self.example_class = (tf.train.SequenceExample\n        if request.args.get('sequence_examples') == 'true'\n        else tf.train.Example)\n    try:\n      platform_utils.throw_if_file_access_not_allowed(examples_path,\n                                                      self._logdir,\n                                                      self._has_auth_group)\n      example_strings = platform_utils.example_protos_from_path(\n          examples_path, examples_count, parse_examples=False,\n          sampling_odds=sampling_odds, example_class=self.example_class)\n      self.examples = [\n          self.example_class.FromString(ex) for ex in example_strings]\n      self.generate_sprite(example_strings)\n      json_examples = [\n          json_format.MessageToJson(example) for example in self.examples\n      ]\n      self.updated_example_indices = set(range(len(json_examples)))\n      return http_util.Respond(\n          request,\n          {'examples': json_examples,\n           'sprite': True if self.sprite else False}, 'application/json')\n    except common_utils.InvalidUserInputError as e:\n      return http_util.Respond(request, {'error': e.message},\n                               'application/json', code=400)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _update_example(self, request):\n    if request.method != 'POST':\n      return http_util.Respond(request, {'error': 'invalid non-POST request'},\n                               'application/json', code=405)\n    example_json = request.form['example']\n    index = int(request.form['index'])\n    if index >= len(self.examples):\n      return http_util.Respond(request, {'error': 'invalid index provided'},\n                               'application/json', code=400)\n    new_example = self.example_class()\n    json_format.Parse(example_json, new_example)\n    self.examples[index] = new_example\n    self.updated_example_indices.add(index)\n    self.generate_sprite([ex.SerializeToString() for ex in self.examples])\n    return http_util.Respond(request, {}, 'application/json')", "response": "Updates the specified example."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nduplicate the specified example.", "response": "def _duplicate_example(self, request):\n    \"\"\"Duplicates the specified example.\n\n    Args:\n      request: A request that should contain 'index'.\n\n    Returns:\n      An empty response.\n    \"\"\"\n    index = int(request.args.get('index'))\n    if index >= len(self.examples):\n      return http_util.Respond(request, {'error': 'invalid index provided'},\n                               'application/json', code=400)\n    new_example = self.example_class()\n    new_example.CopyFrom(self.examples[index])\n    self.examples.append(new_example)\n    self.updated_example_indices.add(len(self.examples) - 1)\n    self.generate_sprite([ex.SerializeToString() for ex in self.examples])\n    return http_util.Respond(request, {}, 'application/json')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _delete_example(self, request):\n    index = int(request.args.get('index'))\n    if index >= len(self.examples):\n      return http_util.Respond(request, {'error': 'invalid index provided'},\n                               'application/json', code=400)\n    del self.examples[index]\n    self.updated_example_indices = set([\n        i if i < index else i - 1 for i in self.updated_example_indices])\n    self.generate_sprite([ex.SerializeToString() for ex in self.examples])\n    return http_util.Respond(request, {}, 'application/json')", "response": "Deletes the specified example."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse comma separated request arguments into a tuple of lists for model parameters and model names.", "response": "def _parse_request_arguments(self, request):\n    \"\"\"Parses comma separated request arguments\n\n    Args:\n      request: A request that should contain 'inference_address', 'model_name',\n        'model_version', 'model_signature'.\n\n    Returns:\n      A tuple of lists for model parameters\n    \"\"\"\n    inference_addresses = request.args.get('inference_address').split(',')\n    model_names = request.args.get('model_name').split(',')\n    model_versions = request.args.get('model_version').split(',')\n    model_signatures = request.args.get('model_signature').split(',')\n    if len(model_names) != len(inference_addresses):\n      raise common_utils.InvalidUserInputError('Every model should have a ' +\n                                                'name and address.')\n    return inference_addresses, model_names, model_versions, model_signatures"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninfer the vz - line - chart for a feature.", "response": "def _infer(self, request):\n    \"\"\"Returns JSON for the `vz-line-chart`s for a feature.\n\n    Args:\n      request: A request that should contain 'inference_address', 'model_name',\n        'model_type, 'model_version', 'model_signature' and 'label_vocab_path'.\n\n    Returns:\n      A list of JSON objects, one for each chart.\n    \"\"\"\n    label_vocab = inference_utils.get_label_vocab(\n      request.args.get('label_vocab_path'))\n\n    try:\n      if request.method != 'GET':\n        logger.error('%s requests are forbidden.', request.method)\n        return http_util.Respond(request, {'error': 'invalid non-GET request'},\n                                    'application/json', code=405)\n\n      (inference_addresses, model_names, model_versions,\n          model_signatures) = self._parse_request_arguments(request)\n\n      indices_to_infer = sorted(self.updated_example_indices)\n      examples_to_infer = [self.examples[index] for index in indices_to_infer]\n      infer_objs = []\n      for model_num in xrange(len(inference_addresses)):\n        serving_bundle = inference_utils.ServingBundle(\n            inference_addresses[model_num],\n            model_names[model_num],\n            request.args.get('model_type'),\n            model_versions[model_num],\n            model_signatures[model_num],\n            request.args.get('use_predict') == 'true',\n            request.args.get('predict_input_tensor'),\n            request.args.get('predict_output_tensor'))\n        infer_objs.append(inference_utils.run_inference_for_inference_results(\n          examples_to_infer, serving_bundle))\n\n      resp = {'indices': indices_to_infer, 'results': infer_objs}\n      self.updated_example_indices = set()\n      return http_util.Respond(request, {'inferences': json.dumps(resp),\n                                         'vocab': json.dumps(label_vocab)},\n                               'application/json')\n    except common_utils.InvalidUserInputError as e:\n      return http_util.Respond(request, {'error': e.message},\n                               'application/json', code=400)\n    except AbortionError as e:\n      return http_util.Respond(request, {'error': e.details},\n                               'application/json', code=400)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _eligible_features_from_example_handler(self, request):\n    features_list = inference_utils.get_eligible_features(\n      self.examples[0: NUM_EXAMPLES_TO_SCAN], NUM_MUTANTS)\n    return http_util.Respond(request, features_list, 'application/json')", "response": "Returns a list of JSON objects for each feature in the example."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _infer_mutants_handler(self, request):\n    try:\n      if request.method != 'GET':\n        logger.error('%s requests are forbidden.', request.method)\n        return http_util.Respond(request, {'error': 'invalid non-GET request'},\n                                 'application/json', code=405)\n\n      example_index = int(request.args.get('example_index', '0'))\n      feature_name = request.args.get('feature_name')\n      examples = (self.examples if example_index == -1\n          else [self.examples[example_index]])\n\n      (inference_addresses, model_names, model_versions,\n          model_signatures) = self._parse_request_arguments(request)\n\n      serving_bundles = []\n      for model_num in xrange(len(inference_addresses)):\n        serving_bundles.append(inference_utils.ServingBundle(\n            inference_addresses[model_num],\n            model_names[model_num],\n            request.args.get('model_type'),\n            model_versions[model_num],\n            model_signatures[model_num],\n            request.args.get('use_predict') == 'true',\n            request.args.get('predict_input_tensor'),\n            request.args.get('predict_output_tensor')))\n\n      viz_params = inference_utils.VizParams(\n          request.args.get('x_min'), request.args.get('x_max'),\n          self.examples[0:NUM_EXAMPLES_TO_SCAN], NUM_MUTANTS,\n          request.args.get('feature_index_pattern'))\n      json_mapping = inference_utils.mutant_charts_for_feature(\n          examples, feature_name, serving_bundles, viz_params)\n      return http_util.Respond(request, json_mapping, 'application/json')\n    except common_utils.InvalidUserInputError as e:\n      return http_util.Respond(request, {'error': e.message},\n                               'application/json', code=400)", "response": "Infer mutants for a feature."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nserving a pre - gzipped static asset from the zip file.", "response": "def _serve_asset(self, path, gzipped_asset_bytes, request):\n    \"\"\"Serves a pre-gzipped static asset from the zip file.\"\"\"\n    mimetype = mimetypes.guess_type(path)[0] or 'application/octet-stream'\n    return http_util.Respond(\n        request, gzipped_asset_bytes, mimetype, content_encoding='gzip')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nserving a JSON object containing some base properties used by the frontend.", "response": "def _serve_environment(self, request):\n    \"\"\"Serve a JSON object containing some base properties used by the frontend.\n\n    * data_location is either a path to a directory or an address to a\n      database (depending on which mode TensorBoard is running in).\n    * window_title is the title of the TensorBoard web page.\n    \"\"\"\n    return http_util.Respond(\n        request,\n        {\n            'data_location': self._logdir or self._db_uri,\n            'mode': 'db' if self._db_uri else 'logdir',\n            'window_title': self._window_title,\n        },\n        'application/json')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nserving a JSON array of run names ordered by started time.", "response": "def _serve_runs(self, request):\n    \"\"\"Serve a JSON array of run names, ordered by run started time.\n\n    Sort order is by started time (aka first event time) with empty times sorted\n    last, and then ties are broken by sorting on the run name.\n    \"\"\"\n    if self._db_connection_provider:\n      db = self._db_connection_provider()\n      cursor = db.execute('''\n        SELECT\n          run_name,\n          started_time IS NULL as started_time_nulls_last,\n          started_time\n        FROM Runs\n        ORDER BY started_time_nulls_last, started_time, run_name\n      ''')\n      run_names = [row[0] for row in cursor]\n    else:\n      # Python's list.sort is stable, so to order by started time and\n      # then by name, we can just do the sorts in the reverse order.\n      run_names = sorted(self._multiplexer.Runs())\n      def get_first_event_timestamp(run_name):\n        try:\n          return self._multiplexer.FirstEventTimestamp(run_name)\n        except ValueError as e:\n          logger.warn(\n              'Unable to get first event timestamp for run %s: %s', run_name, e)\n          # Put runs without a timestamp at the end.\n          return float('inf')\n      run_names.sort(key=get_first_event_timestamp)\n    return http_util.Respond(request, run_names, 'application/json')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _serve_experiments(self, request):\n    results = self.list_experiments_impl()\n    return http_util.Respond(request, results, 'application/json')", "response": "Serve a JSON array of experiments."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nserves a list of JSON runs of an experiment.", "response": "def _serve_experiment_runs(self, request):\n    \"\"\"Serve a JSON runs of an experiment, specified with query param\n    `experiment`, with their nested data, tag, populated. Runs returned are\n    ordered by started time (aka first event time) with empty times sorted last,\n    and then ties are broken by sorting on the run name. Tags are sorted by\n    its name, displayName, and lastly, inserted time.\n    \"\"\"\n    results = []\n    if self._db_connection_provider:\n      exp_id = request.args.get('experiment')\n      runs_dict = collections.OrderedDict()\n\n      db = self._db_connection_provider()\n      cursor = db.execute('''\n        SELECT\n          Runs.run_id,\n          Runs.run_name,\n          Runs.started_time,\n          Runs.started_time IS NULL as started_time_nulls_last,\n          Tags.tag_id,\n          Tags.tag_name,\n          Tags.display_name,\n          Tags.plugin_name,\n          Tags.inserted_time\n        From Runs\n        LEFT JOIN Tags ON Runs.run_id = Tags.run_id\n        WHERE Runs.experiment_id = ?\n        AND (Tags.tag_id IS NULL OR Tags.plugin_name IS NOT NULL)\n        ORDER BY started_time_nulls_last,\n          Runs.started_time,\n          Runs.run_name,\n          Runs.run_id,\n          Tags.tag_name,\n          Tags.display_name,\n          Tags.inserted_time;\n      ''', (exp_id,))\n      for row in cursor:\n        run_id = row[0]\n        if not run_id in runs_dict:\n          runs_dict[run_id] = {\n            \"id\": run_id,\n            \"name\": row[1],\n            \"startTime\": math.floor(row[2]),\n            \"tags\": [],\n          }\n        # tag can be missing.\n        if row[4]:\n          runs_dict[run_id].get(\"tags\").append({\n            \"id\": row[4],\n            \"displayName\": row[6],\n            \"name\": row[5],\n            \"pluginName\": row[7],\n          })\n      results = list(runs_dict.values())\n    return http_util.Respond(request, results, 'application/json')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef define_flags(self, parser):\n    parser.add_argument(\n        '--logdir',\n        metavar='PATH',\n        type=str,\n        default='',\n        help='''\\\nDirectory where TensorBoard will look to find TensorFlow event files\nthat it can display. TensorBoard will recursively walk the directory\nstructure rooted at logdir, looking for .*tfevents.* files.\n\nYou may also pass a comma separated list of log directories, and\nTensorBoard will watch each directory. You can also assign names to\nindividual log directories by putting a colon between the name and the\npath, as in:\n\n`tensorboard --logdir=name1:/path/to/logs/1,name2:/path/to/logs/2`\\\n''')\n\n    parser.add_argument(\n        '--host',\n        metavar='ADDR',\n        type=str,\n        default='',\n        help='''\\\nWhat host to listen to. Defaults to serving on all interfaces. Other\ncommonly used values are 127.0.0.1 (localhost) and :: (for IPv6).\\\n''')\n\n    parser.add_argument(\n        '--port',\n        metavar='PORT',\n        type=lambda s: (None if s == \"default\" else int(s)),\n        default=\"default\",\n        help='''\\\nPort to serve TensorBoard on. Pass 0 to request an unused port selected\nby the operating system, or pass \"default\" to try to bind to the default\nport (%s) but search for a nearby free port if the default port is\nunavailable. (default: \"default\").\\\n''' % DEFAULT_PORT)\n\n    parser.add_argument(\n        '--purge_orphaned_data',\n        metavar='BOOL',\n        # Custom str-to-bool converter since regular bool() doesn't work.\n        type=lambda v: {'true': True, 'false': False}.get(v.lower(), v),\n        choices=[True, False],\n        default=True,\n        help='''\\\nWhether to purge data that may have been orphaned due to TensorBoard\nrestarts. Setting --purge_orphaned_data=False can be used to debug data\ndisappearance. (default: %(default)s)\\\n''')\n\n    parser.add_argument(\n        '--reload_interval',\n        metavar='SECONDS',\n        type=float,\n        default=5.0,\n        help='''\\\nHow often the backend should load more data, in seconds. Set to 0 to\nload just once at startup and a negative number to never reload at all.\nNot relevant for DB read-only mode. (default: %(default)s)\\\n''')\n\n    parser.add_argument(\n        '--db',\n        metavar='URI',\n        type=str,\n        default='',\n        help='''\\\n[experimental] sets SQL database URI and enables DB backend mode, which is\nread-only unless --db_import is also passed.\\\n''')\n\n    parser.add_argument(\n        '--db_import',\n        action='store_true',\n        help='''\\\n[experimental] enables DB read-and-import mode, which in combination with\n--logdir imports event files into a DB backend on the fly. The backing DB is\ntemporary unless --db is also passed to specify a DB path to use.\\\n''')\n\n    parser.add_argument(\n        '--db_import_use_op',\n        action='store_true',\n        help='''\\\n[experimental] in combination with --db_import, if passed, use TensorFlow's\nimport_event() op for importing event data, otherwise use TensorBoard's own\nsqlite ingestion logic.\\\n''')\n\n    parser.add_argument(\n        '--inspect',\n        action='store_true',\n        help='''\\\nPrints digests of event files to command line.\n\nThis is useful when no data is shown on TensorBoard, or the data shown\nlooks weird.\n\nMust specify one of `logdir` or `event_file` flag.\n\nExample usage:\n  `tensorboard --inspect --logdir mylogdir --tag loss`\n\nSee tensorboard/backend/event_processing/event_file_inspector.py for more info.\\\n''')\n\n    # This flag has a \"_tb\" suffix to avoid conflicting with an internal flag\n    # named --version.  Note that due to argparse auto-expansion of unambiguous\n    # flag prefixes, you can still invoke this as `tensorboard --version`.\n    parser.add_argument(\n        '--version_tb',\n        action='store_true',\n        help='Prints the version of Tensorboard')\n\n    parser.add_argument(\n        '--tag',\n        metavar='TAG',\n        type=str,\n        default='',\n        help='tag to query for; used with --inspect')\n\n    parser.add_argument(\n        '--event_file',\n        metavar='PATH',\n        type=str,\n        default='',\n        help='''\\\nThe particular event file to query for. Only used if --inspect is\npresent and --logdir is not specified.\\\n''')\n\n    parser.add_argument(\n        '--path_prefix',\n        metavar='PATH',\n        type=str,\n        default='',\n        help='''\\\nAn optional, relative prefix to the path, e.g. \"/path/to/tensorboard\".\nresulting in the new base url being located at\nlocalhost:6006/path/to/tensorboard under default settings. A leading\nslash is required when specifying the path_prefix, however trailing\nslashes can be omitted. The path_prefix can be leveraged for path based\nrouting of an elb when the website base_url is not available e.g.\n\"example.site.com/path/to/tensorboard/\".\\\n''')\n\n    parser.add_argument(\n        '--window_title',\n        metavar='TEXT',\n        type=str,\n        default='',\n        help='changes title of browser window')\n\n    parser.add_argument(\n        '--max_reload_threads',\n        metavar='COUNT',\n        type=int,\n        default=1,\n        help='''\\\nThe max number of threads that TensorBoard can use to reload runs. Not\nrelevant for db read-only mode. Each thread reloads one run at a time.\n(default: %(default)s)\\\n''')\n\n    parser.add_argument(\n        '--reload_task',\n        metavar='TYPE',\n        type=str,\n        default='auto',\n        choices=['auto', 'thread', 'process', 'blocking'],\n        help='''\\\n[experimental] The mechanism to use for the background data reload task.\nThe default \"auto\" option will conditionally use threads for legacy reloading\nand a child process for DB import reloading. The \"process\" option is only\nuseful with DB import mode. The \"blocking\" option will block startup until\nreload finishes, and requires --load_interval=0. (default: %(default)s)\\\n''')\n\n    parser.add_argument(\n        '--samples_per_plugin',\n        type=str,\n        default='',\n        help='''\\\nAn optional comma separated list of plugin_name=num_samples pairs to\nexplicitly specify how many samples to keep per tag for that plugin. For\nunspecified plugins, TensorBoard randomly downsamples logged summaries\nto reasonable values to prevent out-of-memory errors for long running\njobs. This flag allows fine control over that downsampling. Note that 0\nmeans keep all samples of that type. For instance \"scalars=500,images=0\"\nkeeps 500 scalars and all images. Most users should not need to set this\nflag.\\\n''')", "response": "Adds standard TensorBoard CLI flags to parser."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfix standard TensorBoard CLI flags to parser.", "response": "def fix_flags(self, flags):\n    \"\"\"Fixes standard TensorBoard CLI flags to parser.\"\"\"\n    FlagsError = base_plugin.FlagsError\n    if flags.version_tb:\n      pass\n    elif flags.inspect:\n      if flags.logdir and flags.event_file:\n        raise FlagsError(\n            'Must specify either --logdir or --event_file, but not both.')\n      if not (flags.logdir or flags.event_file):\n        raise FlagsError('Must specify either --logdir or --event_file.')\n    elif not flags.db and not flags.logdir:\n      raise FlagsError('A logdir or db must be specified. '\n                       'For example `tensorboard --logdir mylogdir` '\n                       'or `tensorboard --db sqlite:~/.tensorboard.db`. '\n                       'Run `tensorboard --helpfull` for details and examples.')\n\n    if flags.path_prefix.endswith('/'):\n      flags.path_prefix = flags.path_prefix[:-1]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef put(self, message):\n    with self._outgoing_lock:\n      self._outgoing.append(message)\n      self._outgoing_counter += 1\n\n      # Check to see if there are pending queues waiting for the item.\n      if self._outgoing_counter in self._outgoing_pending_queues:\n        for q in self._outgoing_pending_queues[self._outgoing_counter]:\n          q.put(message)\n        del self._outgoing_pending_queues[self._outgoing_counter]", "response": "Put a message into the outgoing message stack."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get(self, pos):\n    if pos <= 0:\n      raise ValueError('Invalid pos %d: pos must be > 0' % pos)\n    with self._outgoing_lock:\n      if self._outgoing_counter >= pos:\n        # If the stack already has the requested position, return the value\n        # immediately.\n        return self._outgoing[pos - 1], self._outgoing_counter\n      else:\n        # If the stack has not reached the requested position yet, create a\n        # queue and block on get().\n        if pos not in self._outgoing_pending_queues:\n          self._outgoing_pending_queues[pos] = []\n        q = queue.Queue(maxsize=1)\n        self._outgoing_pending_queues[pos].append(q)\n\n    value = q.get()\n    with self._outgoing_lock:\n      return value, self._outgoing_counter", "response": "Get the item at the specified position from the outgoing message stack."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrun custom scalar demo and generate event files.", "response": "def run():\n  \"\"\"Run custom scalar demo and generate event files.\"\"\"\n  step = tf.compat.v1.placeholder(tf.float32, shape=[])\n\n  with tf.name_scope('loss'):\n    # Specify 2 different loss values, each tagged differently.\n    summary_lib.scalar('foo', tf.pow(0.9, step))\n    summary_lib.scalar('bar', tf.pow(0.85, step + 2))\n\n    # Log metric baz as well as upper and lower bounds for a margin chart.\n    middle_baz_value = step + 4 * tf.random.uniform([]) - 2\n    summary_lib.scalar('baz', middle_baz_value)\n    summary_lib.scalar('baz_lower',\n                       middle_baz_value - 6.42 - tf.random.uniform([]))\n    summary_lib.scalar('baz_upper',\n                       middle_baz_value + 6.42 + tf.random.uniform([]))\n\n  with tf.name_scope('trigFunctions'):\n    summary_lib.scalar('cosine', tf.cos(step))\n    summary_lib.scalar('sine', tf.sin(step))\n    summary_lib.scalar('tangent', tf.tan(step))\n\n  merged_summary = tf.compat.v1.summary.merge_all()\n\n  with tf.compat.v1.Session() as sess, tf.summary.FileWriter(LOGDIR) as writer:\n    # We only need to specify the layout once (instead of per step).\n    layout_summary = summary_lib.custom_scalar_pb(\n        layout_pb2.Layout(category=[\n            layout_pb2.Category(\n                title='losses',\n                chart=[\n                    layout_pb2.Chart(\n                        title='losses',\n                        multiline=layout_pb2.MultilineChartContent(\n                            tag=[r'loss(?!.*margin.*)'],)),\n                    layout_pb2.Chart(\n                        title='baz',\n                        margin=layout_pb2.MarginChartContent(\n                            series=[\n                                layout_pb2.MarginChartContent.Series(\n                                    value='loss/baz/scalar_summary',\n                                    lower='loss/baz_lower/scalar_summary',\n                                    upper='loss/baz_upper/scalar_summary'\n                                ),\n                            ],)),\n                ]),\n            layout_pb2.Category(\n                title='trig functions',\n                chart=[\n                    layout_pb2.Chart(\n                        title='wave trig functions',\n                        multiline=layout_pb2.MultilineChartContent(\n                            tag=[\n                                r'trigFunctions/cosine', r'trigFunctions/sine'\n                            ],)),\n                    # The range of tangent is different. Give it its own chart.\n                    layout_pb2.Chart(\n                        title='tan',\n                        multiline=layout_pb2.MultilineChartContent(\n                            tag=[r'trigFunctions/tangent'],)),\n                ],\n                # This category we care less about. Make it initially closed.\n                closed=True),\n        ]))\n    writer.add_summary(layout_summary)\n\n    for i in xrange(42):\n      summary = sess.run(merged_summary, feed_dict={step: i})\n      writer.add_summary(summary, global_step=i)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the plugin if possible.", "response": "def load(self, context):\n    \"\"\"Returns the plugin, if possible.\n\n    Args:\n      context: The TBContext flags.\n\n    Returns:\n      A InteractiveInferencePlugin instance or None if it couldn't be loaded.\n    \"\"\"\n    try:\n      # pylint: disable=g-import-not-at-top,unused-import\n      import tensorflow\n    except ImportError:\n      return\n    # pylint: disable=line-too-long,g-import-not-at-top\n    from tensorboard.plugins.interactive_inference.interactive_inference_plugin import InteractiveInferencePlugin\n    return InteractiveInferencePlugin(context)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nvisualizing the embeddings for the current projector.", "response": "def visualize_embeddings(summary_writer, config):\n  \"\"\"Stores a config file used by the embedding projector.\n\n  Args:\n    summary_writer: The summary writer used for writing events.\n    config: `tf.contrib.tensorboard.plugins.projector.ProjectorConfig`\n      proto that holds the configuration for the projector such as paths to\n      checkpoint files and metadata files for the embeddings. If\n      `config.model_checkpoint_path` is none, it defaults to the\n      `logdir` used by the summary_writer.\n\n  Raises:\n    ValueError: If the summary writer does not have a `logdir`.\n  \"\"\"\n  logdir = summary_writer.get_logdir()\n\n  # Sanity checks.\n  if logdir is None:\n    raise ValueError('Summary writer must have a logdir')\n\n  # Saving the config file in the logdir.\n  config_pbtxt = _text_format.MessageToString(config)\n  path = os.path.join(logdir, _projector_plugin.PROJECTOR_FILENAME)\n  with tf.io.gfile.GFile(path, 'w') as f:\n    f.write(config_pbtxt)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _wrap_define_function(original_function):\n\n    def wrapper(*args, **kwargs):\n        \"\"\"Wrapper function that turns old keyword names to new ones.\"\"\"\n        has_old_names = False\n        for old_name, new_name in _six.iteritems(_RENAMED_ARGUMENTS):\n            if old_name in kwargs:\n                has_old_names = True\n                value = kwargs.pop(old_name)\n                kwargs[new_name] = value\n        if has_old_names:\n            _logging.warning(\n                \"Use of the keyword argument names (flag_name, default_value, \"\n                \"docstring) is deprecated, please use (name, default, help) instead.\"\n            )\n        return original_function(*args, **kwargs)\n\n    return wrapper", "response": "Wraps absl. flags s define functions so tf. flags accepts old names."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a tuple storing the evaluations of the specified metric in the specified session.", "response": "def run_tag_from_session_and_metric(session_name, metric_name):\n  \"\"\"Returns a (run,tag) tuple storing the evaluations of the specified metric.\n\n  Args:\n    session_name: str.\n    metric_name: MetricName protobuffer.\n  Returns: (run, tag) tuple.\n  \"\"\"\n  assert isinstance(session_name, six.string_types)\n  assert isinstance(metric_name, api_pb2.MetricName)\n  # os.path.join() will append a final slash if the group is empty; it seems\n  # like multiplexer.Tensors won't recognize paths that end with a '/' so\n  # we normalize the result of os.path.join() to remove the final '/' in that\n  # case.\n  run = os.path.normpath(os.path.join(session_name, metric_name.group))\n  tag = metric_name.tag\n  return run, tag"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef last_metric_eval(multiplexer, session_name, metric_name):\n  try:\n    run, tag = run_tag_from_session_and_metric(session_name, metric_name)\n    tensor_events = multiplexer.Tensors(run=run, tag=tag)\n  except KeyError as e:\n    raise KeyError(\n        'Can\\'t find metric %s for session: %s. Underlying error message: %s'\n        % (metric_name, session_name, e))\n  last_event = tensor_events[-1]\n  # TODO(erez): Raise HParamsError if the tensor is not a 0-D real scalar.\n  return (last_event.wall_time,\n          last_event.step,\n          tf.make_ndarray(last_event.tensor_proto).item())", "response": "Returns the last evaluation of the given metric at the given session."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef index_impl(self):\n    if self._db_connection_provider:\n      # Read tags from the database.\n      db = self._db_connection_provider()\n      cursor = db.execute('''\n        SELECT\n          Tags.tag_name,\n          Tags.display_name,\n          Runs.run_name\n        FROM Tags\n        JOIN Runs\n          ON Tags.run_id = Runs.run_id\n        WHERE\n          Tags.plugin_name = ?\n      ''', (metadata.PLUGIN_NAME,))\n      result = collections.defaultdict(dict)\n      for row in cursor:\n        tag_name, display_name, run_name = row\n        result[run_name][tag_name] = {\n            'displayName': display_name,\n            # TODO(chihuahua): Populate the description. Currently, the tags\n            # table does not link with the description table.\n            'description': '',\n        }\n      return result\n\n    runs = self._multiplexer.Runs()\n    result = {run: {} for run in runs}\n\n    mapping = self._multiplexer.PluginRunToTagToContent(metadata.PLUGIN_NAME)\n    for (run, tag_to_content) in six.iteritems(mapping):\n      for (tag, content) in six.iteritems(tag_to_content):\n        content = metadata.parse_plugin_metadata(content)\n        summary_metadata = self._multiplexer.SummaryMetadata(run, tag)\n        result[run][tag] = {'displayName': summary_metadata.display_name,\n                            'description': plugin_util.markdown_to_safe_html(\n                                summary_metadata.summary_description)}\n\n    return result", "response": "Return a dictionary of all the runs in the multithreading Sequentially."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef scalars_impl(self, tag, run, experiment, output_format):\n    if self._db_connection_provider:\n      db = self._db_connection_provider()\n      # We select for steps greater than -1 because the writer inserts\n      # placeholder rows en masse. The check for step filters out those rows.\n      cursor = db.execute('''\n        SELECT\n          Tensors.step,\n          Tensors.computed_time,\n          Tensors.data,\n          Tensors.dtype\n        FROM Tensors\n        JOIN Tags\n          ON Tensors.series = Tags.tag_id\n        JOIN Runs\n          ON Tags.run_id = Runs.run_id\n        WHERE\n          /* For backwards compatibility, ignore the experiment id\n             for matching purposes if it is empty. */\n          (:exp == '' OR Runs.experiment_id == CAST(:exp AS INT))\n          AND Runs.run_name = :run\n          AND Tags.tag_name = :tag\n          AND Tags.plugin_name = :plugin\n          AND Tensors.shape = ''\n          AND Tensors.step > -1\n        ORDER BY Tensors.step\n      ''', dict(exp=experiment, run=run, tag=tag, plugin=metadata.PLUGIN_NAME))\n      values = [(wall_time, step, self._get_value(data, dtype_enum))\n                for (step, wall_time, data, dtype_enum) in cursor]\n    else:\n      tensor_events = self._multiplexer.Tensors(run, tag)\n      values = [(tensor_event.wall_time,\n                 tensor_event.step,\n                 tensor_util.make_ndarray(tensor_event.tensor_proto).item())\n                for tensor_event in tensor_events]\n\n    if output_format == OutputFormat.CSV:\n      string_io = StringIO()\n      writer = csv.writer(string_io)\n      writer.writerow(['Wall time', 'Step', 'Value'])\n      writer.writerows(values)\n      return (string_io.getvalue(), 'text/csv')\n    else:\n      return (values, 'application/json')", "response": "Returns the scalars for a single run."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the value for a scalar event given blob and dtype enum.", "response": "def _get_value(self, scalar_data_blob, dtype_enum):\n    \"\"\"Obtains value for scalar event given blob and dtype enum.\n\n    Args:\n      scalar_data_blob: The blob obtained from the database.\n      dtype_enum: The enum representing the dtype.\n\n    Returns:\n      The scalar value.\n    \"\"\"\n    tensorflow_dtype = tf.DType(dtype_enum)\n    buf = np.frombuffer(scalar_data_blob, dtype=tensorflow_dtype.as_numpy_dtype)\n    return np.asscalar(buf)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngiving a tag and single run return array of ScalarEvents.", "response": "def scalars_route(self, request):\n    \"\"\"Given a tag and single run, return array of ScalarEvents.\"\"\"\n    # TODO: return HTTP status code for malformed requests\n    tag = request.args.get('tag')\n    run = request.args.get('run')\n    experiment = request.args.get('experiment')\n    output_format = request.args.get('format')\n    (body, mime_type) = self.scalars_impl(tag, run, experiment, output_format)\n    return http_util.Respond(request, body, mime_type)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a run to the multiplexer.", "response": "def AddRun(self, path, name=None):\n    \"\"\"Add a run to the multiplexer.\n\n    If the name is not specified, it is the same as the path.\n\n    If a run by that name exists, and we are already watching the right path,\n      do nothing. If we are watching a different path, replace the event\n      accumulator.\n\n    If `Reload` has been called, it will `Reload` the newly created\n    accumulators.\n\n    Args:\n      path: Path to the event files (or event directory) for given run.\n      name: Name of the run to add. If not provided, is set to path.\n\n    Returns:\n      The `EventMultiplexer`.\n    \"\"\"\n    name = name or path\n    accumulator = None\n    with self._accumulators_mutex:\n      if name not in self._accumulators or self._paths[name] != path:\n        if name in self._paths and self._paths[name] != path:\n          # TODO(@dandelionmane) - Make it impossible to overwrite an old path\n          # with a new path (just give the new path a distinct name)\n          logger.warn('Conflict for name %s: old path %s, new path %s',\n                             name, self._paths[name], path)\n        logger.info('Constructing EventAccumulator for %s', path)\n        accumulator = event_accumulator.EventAccumulator(\n            path,\n            size_guidance=self._size_guidance,\n            tensor_size_guidance=self._tensor_size_guidance,\n            purge_orphaned_data=self.purge_orphaned_data)\n        self._accumulators[name] = accumulator\n        self._paths[name] = path\n    if accumulator:\n      if self._reload_called:\n        accumulator.Reload()\n    return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds runs from a directory.", "response": "def AddRunsFromDirectory(self, path, name=None):\n    \"\"\"Load runs from a directory; recursively walks subdirectories.\n\n    If path doesn't exist, no-op. This ensures that it is safe to call\n      `AddRunsFromDirectory` multiple times, even before the directory is made.\n\n    If path is a directory, load event files in the directory (if any exist) and\n      recursively call AddRunsFromDirectory on any subdirectories. This mean you\n      can call AddRunsFromDirectory at the root of a tree of event logs and\n      TensorBoard will load them all.\n\n    If the `EventMultiplexer` is already loaded this will cause\n    the newly created accumulators to `Reload()`.\n    Args:\n      path: A string path to a directory to load runs from.\n      name: Optionally, what name to apply to the runs. If name is provided\n        and the directory contains run subdirectories, the name of each subrun\n        is the concatenation of the parent name and the subdirectory name. If\n        name is provided and the directory contains event files, then a run\n        is added called \"name\" and with the events from the path.\n\n    Raises:\n      ValueError: If the path exists and isn't a directory.\n\n    Returns:\n      The `EventMultiplexer`.\n    \"\"\"\n    logger.info('Starting AddRunsFromDirectory: %s', path)\n    for subdir in io_wrapper.GetLogdirSubdirectories(path):\n      logger.info('Adding run from directory %s', subdir)\n      rpath = os.path.relpath(subdir, path)\n      subname = os.path.join(name, rpath) if name else rpath\n      self.AddRun(subdir, name=subname)\n    logger.info('Done with AddRunsFromDirectory: %s', path)\n    return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncall `Reload` on every `EventAccumulator`.", "response": "def Reload(self):\n    \"\"\"Call `Reload` on every `EventAccumulator`.\"\"\"\n    logger.info('Beginning EventMultiplexer.Reload()')\n    self._reload_called = True\n    # Build a list so we're safe even if the list of accumulators is modified\n    # even while we're reloading.\n    with self._accumulators_mutex:\n      items = list(self._accumulators.items())\n    items_queue = queue.Queue()\n    for item in items:\n      items_queue.put(item)\n\n    # Methods of built-in python containers are thread-safe so long as the GIL\n    # for the thread exists, but we might as well be careful.\n    names_to_delete = set()\n    names_to_delete_mutex = threading.Lock()\n\n    def Worker():\n      \"\"\"Keeps reloading accumulators til none are left.\"\"\"\n      while True:\n        try:\n          name, accumulator = items_queue.get(block=False)\n        except queue.Empty:\n          # No more runs to reload.\n          break\n\n        try:\n          accumulator.Reload()\n        except (OSError, IOError) as e:\n          logger.error('Unable to reload accumulator %r: %s', name, e)\n        except directory_watcher.DirectoryDeletedError:\n          with names_to_delete_mutex:\n            names_to_delete.add(name)\n        finally:\n          items_queue.task_done()\n\n    if self._max_reload_threads > 1:\n      num_threads = min(\n          self._max_reload_threads, len(items))\n      logger.info('Starting %d threads to reload runs', num_threads)\n      for i in xrange(num_threads):\n        thread = threading.Thread(target=Worker, name='Reloader %d' % i)\n        thread.daemon = True\n        thread.start()\n      items_queue.join()\n    else:\n      logger.info(\n          'Reloading runs serially (one after another) on the main '\n          'thread.')\n      Worker()\n\n    with self._accumulators_mutex:\n      for name in names_to_delete:\n        logger.warn('Deleting accumulator %r', name)\n        del self._accumulators[name]\n    logger.info('Finished with EventMultiplexer.Reload()')\n    return self"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef PluginAssets(self, plugin_name):\n    with self._accumulators_mutex:\n      # To avoid nested locks, we construct a copy of the run-accumulator map\n      items = list(six.iteritems(self._accumulators))\n\n    return {run: accum.PluginAssets(plugin_name) for run, accum in items}", "response": "Returns a dictionary that maps from run_name to a list of plugin\n        objects."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nretrieving the contents for a specific plugin asset from a run.", "response": "def RetrievePluginAsset(self, run, plugin_name, asset_name):\n    \"\"\"Return the contents for a specific plugin asset from a run.\n\n    Args:\n      run: The string name of the run.\n      plugin_name: The string name of a plugin.\n      asset_name: The string name of an asset.\n\n    Returns:\n      The string contents of the plugin asset.\n\n    Raises:\n      KeyError: If the asset is not available.\n    \"\"\"\n    accumulator = self.GetAccumulator(run)\n    return accumulator.RetrievePluginAsset(plugin_name, asset_name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nretrieves the scalar events associated with a run and tag.", "response": "def Scalars(self, run, tag):\n    \"\"\"Retrieve the scalar events associated with a run and tag.\n\n    Args:\n      run: A string name of the run for which values are retrieved.\n      tag: A string name of the tag for which values are retrieved.\n\n    Raises:\n      KeyError: If the run is not found, or the tag is not available for\n        the given run.\n\n    Returns:\n      An array of `event_accumulator.ScalarEvents`.\n    \"\"\"\n    accumulator = self.GetAccumulator(run)\n    return accumulator.Scalars(tag)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef RunMetadata(self, run, tag):\n    accumulator = self.GetAccumulator(run)\n    return accumulator.RunMetadata(tag)", "response": "Returns the protobuf data structure for a particular TensorFlow run and tag."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nretrieve the audio events associated with a run and tag.", "response": "def Audio(self, run, tag):\n    \"\"\"Retrieve the audio events associated with a run and tag.\n\n    Args:\n      run: A string name of the run for which values are retrieved.\n      tag: A string name of the tag for which values are retrieved.\n\n    Raises:\n      KeyError: If the run is not found, or the tag is not available for\n        the given run.\n\n    Returns:\n      An array of `event_accumulator.AudioEvents`.\n    \"\"\"\n    accumulator = self.GetAccumulator(run)\n    return accumulator.Audio(tag)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef Tensors(self, run, tag):\n    accumulator = self.GetAccumulator(run)\n    return accumulator.Tensors(tag)", "response": "Retrieves the tensor events associated with a run and tag."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef PluginRunToTagToContent(self, plugin_name):\n    mapping = {}\n    for run in self.Runs():\n      try:\n        tag_to_content = self.GetAccumulator(run).PluginTagToContent(\n            plugin_name)\n      except KeyError:\n        # This run lacks content for the plugin. Try the next run.\n        continue\n      mapping[run] = tag_to_content\n    return mapping", "response": "Returns a 2 - layer dictionary of the form run to content."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the summary metadata for the given tag on the given run.", "response": "def SummaryMetadata(self, run, tag):\n    \"\"\"Return the summary metadata for the given tag on the given run.\n\n    Args:\n      run: A string name of the run for which summary metadata is to be\n        retrieved.\n      tag: A string name of the tag whose summary metadata is to be\n        retrieved.\n\n    Raises:\n      KeyError: If the run is not found, or the tag is not available for\n        the given run.\n\n    Returns:\n      A `SummaryMetadata` protobuf.\n    \"\"\"\n    accumulator = self.GetAccumulator(run)\n    return accumulator.SummaryMetadata(tag)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef Runs(self):\n    with self._accumulators_mutex:\n      # To avoid nested locks, we construct a copy of the run-accumulator map\n      items = list(six.iteritems(self._accumulators))\n    return {run_name: accumulator.Tags() for run_name, accumulator in items}", "response": "Return all the run names in the EventMultiplexer."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef text(name, data, step=None, description=None):\n  summary_metadata = metadata.create_summary_metadata(\n      display_name=None, description=description)\n  # TODO(https://github.com/tensorflow/tensorboard/issues/2109): remove fallback\n  summary_scope = (\n      getattr(tf.summary.experimental, 'summary_scope', None) or\n      tf.summary.summary_scope)\n  with summary_scope(\n      name, 'text_summary', values=[data, step]) as (tag, _):\n    tf.debugging.assert_type(data, tf.string)\n    return tf.summary.write(\n        tag=tag, tensor=data, step=step, metadata=summary_metadata)", "response": "Writes a text summary."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a text tf. Summary protobuf.", "response": "def text_pb(tag, data, description=None):\n  \"\"\"Create a text tf.Summary protobuf.\n\n  Arguments:\n    tag: String tag for the summary.\n    data: A Python bytestring (of type bytes), a Unicode string, or a numpy data\n      array of those types.\n    description: Optional long-form description for this summary, as a `str`.\n      Markdown is supported. Defaults to empty.\n\n  Raises:\n    TypeError: If the type of the data is unsupported.\n\n  Returns:\n    A `tf.Summary` protobuf object.\n  \"\"\"\n  try:\n    tensor = tensor_util.make_tensor_proto(data, dtype=np.object)\n  except TypeError as e:\n    raise TypeError('tensor must be of type string', e)\n  summary_metadata = metadata.create_summary_metadata(\n      display_name=None, description=description)\n  summary = summary_pb2.Summary()\n  summary.value.add(tag=tag,\n                    metadata=summary_metadata,\n                    tensor=tensor)\n  return summary"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run_main():\n  program.setup_environment()\n\n  if getattr(tf, '__version__', 'stub') == 'stub':\n    print(\"TensorFlow installation not found - running with reduced feature set.\",\n          file=sys.stderr)\n\n  tensorboard = program.TensorBoard(default.get_plugins(),\n                                    program.get_default_assets_zip_provider())\n  try:\n    from absl import app\n    # Import this to check that app.run() will accept the flags_parser argument.\n    from absl.flags import argparse_flags\n    app.run(tensorboard.main, flags_parser=tensorboard.configure)\n    raise AssertionError(\"absl.app.run() shouldn't return\")\n  except ImportError:\n    pass\n  except base_plugin.FlagsError as e:\n    print(\"Error: %s\" % e, file=sys.stderr)\n    sys.exit(1)\n\n  tensorboard.configure(sys.argv)\n  sys.exit(tensorboard.main())", "response": "Initializes flags and calls main method."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_summary_metadata(display_name, description):\n  content = plugin_data_pb2.ImagePluginData(version=PROTO_VERSION)\n  metadata = summary_pb2.SummaryMetadata(\n      display_name=display_name,\n      summary_description=description,\n      plugin_data=summary_pb2.SummaryMetadata.PluginData(\n          plugin_name=PLUGIN_NAME,\n          content=content.SerializeToString()))\n  return metadata", "response": "Create a summary_pb2. SummaryMetadata proto for image plugin data."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a legacy audio summary op for use in a TensorFlow graph.", "response": "def op(name,\n       audio,\n       sample_rate,\n       labels=None,\n       max_outputs=3,\n       encoding=None,\n       display_name=None,\n       description=None,\n       collections=None):\n  \"\"\"Create a legacy audio summary op for use in a TensorFlow graph.\n\n  Arguments:\n    name: A unique name for the generated summary node.\n    audio: A `Tensor` representing audio data with shape `[k, t, c]`,\n      where `k` is the number of audio clips, `t` is the number of\n      frames, and `c` is the number of channels. Elements should be\n      floating-point values in `[-1.0, 1.0]`. Any of the dimensions may\n      be statically unknown (i.e., `None`).\n    sample_rate: An `int` or rank-0 `int32` `Tensor` that represents the\n      sample rate, in Hz. Must be positive.\n    labels: Optional `string` `Tensor`, a vector whose length is the\n      first dimension of `audio`, where `labels[i]` contains arbitrary\n      textual information about `audio[i]`. (For instance, this could be\n      some text that a TTS system was supposed to produce.) Markdown is\n      supported. Contents should be UTF-8.\n    max_outputs: Optional `int` or rank-0 integer `Tensor`. At most this\n      many audio clips will be emitted at each step. When more than\n      `max_outputs` many clips are provided, the first `max_outputs`\n      many clips will be used and the rest silently discarded.\n    encoding: A constant `str` (not string tensor) indicating the\n      desired encoding. You can choose any format you like, as long as\n      it's \"wav\". Please see the \"API compatibility note\" below.\n    display_name: Optional name for this summary in TensorBoard, as a\n      constant `str`. Defaults to `name`.\n    description: Optional long-form description for this summary, as a\n      constant `str`. Markdown is supported. Defaults to empty.\n    collections: Optional list of graph collections keys. The new\n      summary op is added to these collections. Defaults to\n      `[Graph Keys.SUMMARIES]`.\n\n  Returns:\n    A TensorFlow summary op.\n\n  API compatibility note: The default value of the `encoding`\n  argument is _not_ guaranteed to remain unchanged across TensorBoard\n  versions. In the future, we will by default encode as FLAC instead of\n  as WAV. If the specific format is important to you, please provide a\n  file format explicitly.\n  \"\"\"\n  # TODO(nickfelt): remove on-demand imports once dep situation is fixed.\n  import tensorflow  # for contrib\n  import tensorflow.compat.v1 as tf\n\n  if display_name is None:\n    display_name = name\n  if encoding is None:\n    encoding = 'wav'\n\n  if encoding == 'wav':\n    encoding = metadata.Encoding.Value('WAV')\n    encoder = functools.partial(tensorflow.contrib.ffmpeg.encode_audio,\n                                samples_per_second=sample_rate,\n                                file_format='wav')\n  else:\n    raise ValueError('Unknown encoding: %r' % encoding)\n\n  with tf.name_scope(name), \\\n       tf.control_dependencies([tf.assert_rank(audio, 3)]):\n    limited_audio = audio[:max_outputs]\n    encoded_audio = tf.map_fn(encoder, limited_audio,\n                              dtype=tf.string,\n                              name='encode_each_audio')\n    if labels is None:\n      limited_labels = tf.tile([''], tf.shape(input=limited_audio)[:1])\n    else:\n      limited_labels = labels[:max_outputs]\n    tensor = tf.transpose(a=tf.stack([encoded_audio, limited_labels]))\n    summary_metadata = metadata.create_summary_metadata(\n        display_name=display_name,\n        description=description,\n        encoding=encoding)\n    return tf.summary.tensor_summary(name='audio_summary',\n                                     tensor=tensor,\n                                     collections=collections,\n                                     summary_metadata=summary_metadata)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a legacy audio summary protobuf.", "response": "def pb(name,\n       audio,\n       sample_rate,\n       labels=None,\n       max_outputs=3,\n       encoding=None,\n       display_name=None,\n       description=None):\n  \"\"\"Create a legacy audio summary protobuf.\n\n  This behaves as if you were to create an `op` with the same arguments\n  (wrapped with constant tensors where appropriate) and then execute\n  that summary op in a TensorFlow session.\n\n  Arguments:\n    name: A unique name for the generated summary node.\n    audio: An `np.array` representing audio data with shape `[k, t, c]`,\n      where `k` is the number of audio clips, `t` is the number of\n      frames, and `c` is the number of channels. Elements should be\n      floating-point values in `[-1.0, 1.0]`.\n    sample_rate: An `int` that represents the sample rate, in Hz.\n      Must be positive.\n    labels: Optional list (or rank-1 `np.array`) of textstrings or UTF-8\n      bytestrings whose length is the first dimension of `audio`, where\n      `labels[i]` contains arbitrary textual information about\n      `audio[i]`. (For instance, this could be some text that a TTS\n      system was supposed to produce.) Markdown is supported.\n    max_outputs: Optional `int`. At most this many audio clips will be\n      emitted. When more than `max_outputs` many clips are provided, the\n      first `max_outputs` many clips will be used and the rest silently\n      discarded.\n    encoding: A constant `str` indicating the desired encoding. You\n      can choose any format you like, as long as it's \"wav\". Please see\n      the \"API compatibility note\" below.\n    display_name: Optional name for this summary in TensorBoard, as a\n      `str`. Defaults to `name`.\n    description: Optional long-form description for this summary, as a\n      `str`. Markdown is supported. Defaults to empty.\n\n  Returns:\n    A `tf.Summary` protobuf object.\n\n  API compatibility note: The default value of the `encoding`\n  argument is _not_ guaranteed to remain unchanged across TensorBoard\n  versions. In the future, we will by default encode as FLAC instead of\n  as WAV. If the specific format is important to you, please provide a\n  file format explicitly.\n  \"\"\"\n  # TODO(nickfelt): remove on-demand imports once dep situation is fixed.\n  import tensorflow.compat.v1 as tf\n\n  audio = np.array(audio)\n  if audio.ndim != 3:\n    raise ValueError('Shape %r must have rank 3' % (audio.shape,))\n  if encoding is None:\n    encoding = 'wav'\n\n  if encoding == 'wav':\n    encoding = metadata.Encoding.Value('WAV')\n    encoder = functools.partial(encoder_util.encode_wav,\n                                samples_per_second=sample_rate)\n  else:\n    raise ValueError('Unknown encoding: %r' % encoding)\n\n  limited_audio = audio[:max_outputs]\n  if labels is None:\n    limited_labels = [b''] * len(limited_audio)\n  else:\n    limited_labels = [tf.compat.as_bytes(label)\n                      for label in labels[:max_outputs]]\n\n  encoded_audio = [encoder(a) for a in limited_audio]\n  content = np.array([encoded_audio, limited_labels]).transpose()\n  tensor = tf.make_tensor_proto(content, dtype=tf.string)\n\n  if display_name is None:\n    display_name = name\n  summary_metadata = metadata.create_summary_metadata(\n      display_name=display_name,\n      description=description,\n      encoding=encoding)\n  tf_summary_metadata = tf.SummaryMetadata.FromString(\n      summary_metadata.SerializeToString())\n\n  summary = tf.Summary()\n  summary.value.add(tag='%s/audio_summary' % name,\n                    metadata=tf_summary_metadata,\n                    tensor=tensor)\n  return summary"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef op(\n    name,\n    labels,\n    predictions,\n    num_thresholds=None,\n    weights=None,\n    display_name=None,\n    description=None,\n    collections=None):\n  \"\"\"Create a PR curve summary op for a single binary classifier.\n\n  Computes true/false positive/negative values for the given `predictions`\n  against the ground truth `labels`, against a list of evenly distributed\n  threshold values in `[0, 1]` of length `num_thresholds`.\n\n  Each number in `predictions`, a float in `[0, 1]`, is compared with its\n  corresponding boolean label in `labels`, and counts as a single tp/fp/tn/fn\n  value at each threshold. This is then multiplied with `weights` which can be\n  used to reweight certain values, or more commonly used for masking values.\n\n  Args:\n    name: A tag attached to the summary. Used by TensorBoard for organization.\n    labels: The ground truth values. A Tensor of `bool` values with arbitrary\n        shape.\n    predictions: A float32 `Tensor` whose values are in the range `[0, 1]`.\n        Dimensions must match those of `labels`.\n    num_thresholds: Number of thresholds, evenly distributed in `[0, 1]`, to\n        compute PR metrics for. Should be `>= 2`. This value should be a\n        constant integer value, not a Tensor that stores an integer.\n    weights: Optional float32 `Tensor`. Individual counts are multiplied by this\n        value. This tensor must be either the same shape as or broadcastable to\n        the `labels` tensor.\n    display_name: Optional name for this summary in TensorBoard, as a\n        constant `str`. Defaults to `name`.\n    description: Optional long-form description for this summary, as a\n        constant `str`. Markdown is supported. Defaults to empty.\n    collections: Optional list of graph collections keys. The new\n        summary op is added to these collections. Defaults to\n        `[Graph Keys.SUMMARIES]`.\n\n  Returns:\n    A summary operation for use in a TensorFlow graph. The float32 tensor\n    produced by the summary operation is of dimension (6, num_thresholds). The\n    first dimension (of length 6) is of the order: true positives,\n    false positives, true negatives, false negatives, precision, recall.\n\n  \"\"\"\n  # TODO(nickfelt): remove on-demand imports once dep situation is fixed.\n  import tensorflow.compat.v1 as tf\n\n  if num_thresholds is None:\n    num_thresholds = _DEFAULT_NUM_THRESHOLDS\n\n  if weights is None:\n    weights = 1.0\n\n  dtype = predictions.dtype\n\n  with tf.name_scope(name, values=[labels, predictions, weights]):\n    tf.assert_type(labels, tf.bool)\n    # We cast to float to ensure we have 0.0 or 1.0.\n    f_labels = tf.cast(labels, dtype)\n    # Ensure predictions are all in range [0.0, 1.0].\n    predictions = tf.minimum(1.0, tf.maximum(0.0, predictions))\n    # Get weighted true/false labels.\n    true_labels = f_labels * weights\n    false_labels = (1.0 - f_labels) * weights\n\n    # Before we begin, flatten predictions.\n    predictions = tf.reshape(predictions, [-1])\n\n    # Shape the labels so they are broadcast-able for later multiplication.\n    true_labels = tf.reshape(true_labels, [-1, 1])\n    false_labels = tf.reshape(false_labels, [-1, 1])\n\n    # To compute TP/FP/TN/FN, we are measuring a binary classifier\n    #   C(t) = (predictions >= t)\n    # at each threshold 't'. So we have\n    #   TP(t) = sum( C(t) * true_labels )\n    #   FP(t) = sum( C(t) * false_labels )\n    #\n    # But, computing C(t) requires computation for each t. To make it fast,\n    # observe that C(t) is a cumulative integral, and so if we have\n    #   thresholds = [t_0, ..., t_{n-1}];  t_0 < ... < t_{n-1}\n    # where n = num_thresholds, and if we can compute the bucket function\n    #   B(i) = Sum( (predictions == t), t_i <= t < t{i+1} )\n    # then we get\n    #   C(t_i) = sum( B(j), j >= i )\n    # which is the reversed cumulative sum in tf.cumsum().\n    #\n    # We can compute B(i) efficiently by taking advantage of the fact that\n    # our thresholds are evenly distributed, in that\n    #   width = 1.0 / (num_thresholds - 1)\n    #   thresholds = [0.0, 1*width, 2*width, 3*width, ..., 1.0]\n    # Given a prediction value p, we can map it to its bucket by\n    #   bucket_index(p) = floor( p * (num_thresholds - 1) )\n    # so we can use tf.scatter_add() to update the buckets in one pass.\n\n    # Compute the bucket indices for each prediction value.\n    bucket_indices = tf.cast(\n        tf.floor(predictions * (num_thresholds - 1)), tf.int32)\n\n    # Bucket predictions.\n    tp_buckets = tf.reduce_sum(\n        input_tensor=tf.one_hot(bucket_indices, depth=num_thresholds) * true_labels,\n        axis=0)\n    fp_buckets = tf.reduce_sum(\n        input_tensor=tf.one_hot(bucket_indices, depth=num_thresholds) * false_labels,\n        axis=0)\n\n    # Set up the cumulative sums to compute the actual metrics.\n    tp = tf.cumsum(tp_buckets, reverse=True, name='tp')\n    fp = tf.cumsum(fp_buckets, reverse=True, name='fp')\n    # fn = sum(true_labels) - tp\n    #    = sum(tp_buckets) - tp\n    #    = tp[0] - tp\n    # Similarly,\n    # tn = fp[0] - fp\n    tn = fp[0] - fp\n    fn = tp[0] - tp\n\n    precision = tp / tf.maximum(_MINIMUM_COUNT, tp + fp)\n    recall = tp / tf.maximum(_MINIMUM_COUNT, tp + fn)\n\n    return _create_tensor_summary(\n        name,\n        tp,\n        fp,\n        tn,\n        fn,\n        precision,\n        recall,\n        num_thresholds,\n        display_name,\n        description,\n        collections)", "response": "Create a PR curve summary op for a single binary classifier."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pb(name,\n       labels,\n       predictions,\n       num_thresholds=None,\n       weights=None,\n       display_name=None,\n       description=None):\n  \"\"\"Create a PR curves summary protobuf.\n\n  Arguments:\n    name: A name for the generated node. Will also serve as a series name in\n        TensorBoard.\n    labels: The ground truth values. A bool numpy array.\n    predictions: A float32 numpy array whose values are in the range `[0, 1]`.\n        Dimensions must match those of `labels`.\n    num_thresholds: Optional number of thresholds, evenly distributed in\n        `[0, 1]`, to compute PR metrics for. When provided, should be an int of\n        value at least 2. Defaults to 201.\n    weights: Optional float or float32 numpy array. Individual counts are\n        multiplied by this value. This tensor must be either the same shape as\n        or broadcastable to the `labels` numpy array.\n    display_name: Optional name for this summary in TensorBoard, as a `str`.\n        Defaults to `name`.\n    description: Optional long-form description for this summary, as a `str`.\n        Markdown is supported. Defaults to empty.\n  \"\"\"\n  # TODO(nickfelt): remove on-demand imports once dep situation is fixed.\n  import tensorflow.compat.v1 as tf\n\n  if num_thresholds is None:\n    num_thresholds = _DEFAULT_NUM_THRESHOLDS\n\n  if weights is None:\n    weights = 1.0\n\n  # Compute bins of true positives and false positives.\n  bucket_indices = np.int32(np.floor(predictions * (num_thresholds - 1)))\n  float_labels = labels.astype(np.float)\n  histogram_range = (0, num_thresholds - 1)\n  tp_buckets, _ = np.histogram(\n      bucket_indices,\n      bins=num_thresholds,\n      range=histogram_range,\n      weights=float_labels * weights)\n  fp_buckets, _ = np.histogram(\n      bucket_indices,\n      bins=num_thresholds,\n      range=histogram_range,\n      weights=(1.0 - float_labels) * weights)\n\n  # Obtain the reverse cumulative sum.\n  tp = np.cumsum(tp_buckets[::-1])[::-1]\n  fp = np.cumsum(fp_buckets[::-1])[::-1]\n  tn = fp[0] - fp\n  fn = tp[0] - tp\n  precision = tp / np.maximum(_MINIMUM_COUNT, tp + fp)\n  recall = tp / np.maximum(_MINIMUM_COUNT, tp + fn)\n\n  return raw_data_pb(name,\n                     true_positive_counts=tp,\n                     false_positive_counts=fp,\n                     true_negative_counts=tn,\n                     false_negative_counts=fn,\n                     precision=precision,\n                     recall=recall,\n                     num_thresholds=num_thresholds,\n                     display_name=display_name,\n                     description=description)", "response": "Create a PR curves summary protobuf."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing a precision-recall curve summary across batches of data. This function is similar to op() above, but can be used to compute the PR curve across multiple batches of labels and predictions, in the same style as the metrics found in tf.metrics. This function creates multiple local variables for storing true positives, true negative, etc. accumulated over each batch of data, and uses these local variables for computing the final PR curve summary. These variables can be updated with the returned update_op. Args: name: A tag attached to the summary. Used by TensorBoard for organization. labels: The ground truth values, a `Tensor` whose dimensions must match `predictions`. Will be cast to `bool`. predictions: A floating point `Tensor` of arbitrary shape and whose values are in the range `[0, 1]`. num_thresholds: The number of evenly spaced thresholds to generate for computing the PR curve. Defaults to 201. weights: Optional `Tensor` whose rank is either 0, or the same rank as `labels`, and must be broadcastable to `labels` (i.e., all dimensions must be either `1`, or the same as the corresponding `labels` dimension). metrics_collections: An optional list of collections that `auc` should be added to. updates_collections: An optional list of collections that `update_op` should be added to. display_name: Optional name for this summary in TensorBoard, as a constant `str`. Defaults to `name`. description: Optional long-form description for this summary, as a constant `str`. Markdown is supported. Defaults to empty. Returns: pr_curve: A string `Tensor` containing a single value: the serialized PR curve Tensor summary. The summary contains a float32 `Tensor` of dimension (6, num_thresholds). The first dimension (of length 6) is of the order: true positives, false positives, true negatives, false negatives, precision, recall. update_op: An operation that updates the summary with the latest data.", "response": "def streaming_op(name,\n                 labels,\n                 predictions,\n                 num_thresholds=None,\n                 weights=None,\n                 metrics_collections=None,\n                 updates_collections=None,\n                 display_name=None,\n                 description=None):\n  \"\"\"Computes a precision-recall curve summary across batches of data.\n\n  This function is similar to op() above, but can be used to compute the PR\n  curve across multiple batches of labels and predictions, in the same style\n  as the metrics found in tf.metrics.\n\n  This function creates multiple local variables for storing true positives,\n  true negative, etc. accumulated over each batch of data, and uses these local\n  variables for computing the final PR curve summary. These variables can be\n  updated with the returned update_op.\n\n  Args:\n    name: A tag attached to the summary. Used by TensorBoard for organization.\n    labels: The ground truth values, a `Tensor` whose dimensions must match\n      `predictions`. Will be cast to `bool`.\n    predictions: A floating point `Tensor` of arbitrary shape and whose values\n      are in the range `[0, 1]`.\n    num_thresholds: The number of evenly spaced thresholds to generate for\n      computing the PR curve. Defaults to 201.\n    weights: Optional `Tensor` whose rank is either 0, or the same rank as\n      `labels`, and must be broadcastable to `labels` (i.e., all dimensions must\n      be either `1`, or the same as the corresponding `labels` dimension).\n    metrics_collections: An optional list of collections that `auc` should be\n      added to.\n    updates_collections: An optional list of collections that `update_op` should\n      be added to.\n    display_name: Optional name for this summary in TensorBoard, as a\n        constant `str`. Defaults to `name`.\n    description: Optional long-form description for this summary, as a\n        constant `str`. Markdown is supported. Defaults to empty.\n\n  Returns:\n    pr_curve: A string `Tensor` containing a single value: the\n      serialized PR curve Tensor summary. The summary contains a\n      float32 `Tensor` of dimension (6, num_thresholds). The first\n      dimension (of length 6) is of the order: true positives, false\n      positives, true negatives, false negatives, precision, recall.\n    update_op: An operation that updates the summary with the latest data.\n  \"\"\"\n  # TODO(nickfelt): remove on-demand imports once dep situation is fixed.\n  import tensorflow.compat.v1 as tf\n\n  if num_thresholds is None:\n    num_thresholds = _DEFAULT_NUM_THRESHOLDS\n\n  thresholds = [i / float(num_thresholds - 1)\n                for i in range(num_thresholds)]\n\n  with tf.name_scope(name, values=[labels, predictions, weights]):\n    tp, update_tp = tf.metrics.true_positives_at_thresholds(\n        labels=labels,\n        predictions=predictions,\n        thresholds=thresholds,\n        weights=weights)\n    fp, update_fp = tf.metrics.false_positives_at_thresholds(\n        labels=labels,\n        predictions=predictions,\n        thresholds=thresholds,\n        weights=weights)\n    tn, update_tn = tf.metrics.true_negatives_at_thresholds(\n        labels=labels,\n        predictions=predictions,\n        thresholds=thresholds,\n        weights=weights)\n    fn, update_fn = tf.metrics.false_negatives_at_thresholds(\n        labels=labels,\n        predictions=predictions,\n        thresholds=thresholds,\n        weights=weights)\n\n    def compute_summary(tp, fp, tn, fn, collections):\n      precision = tp / tf.maximum(_MINIMUM_COUNT, tp + fp)\n      recall = tp / tf.maximum(_MINIMUM_COUNT, tp + fn)\n\n      return _create_tensor_summary(\n          name,\n          tp,\n          fp,\n          tn,\n          fn,\n          precision,\n          recall,\n          num_thresholds,\n          display_name,\n          description,\n          collections)\n\n    pr_curve = compute_summary(tp, fp, tn, fn, metrics_collections)\n    update_op = tf.group(update_tp, update_fp, update_tn, update_fn)\n    if updates_collections:\n      for collection in updates_collections:\n        tf.add_to_collection(collection, update_op)\n\n    return pr_curve, update_op"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef raw_data_op(\n    name,\n    true_positive_counts,\n    false_positive_counts,\n    true_negative_counts,\n    false_negative_counts,\n    precision,\n    recall,\n    num_thresholds=None,\n    display_name=None,\n    description=None,\n    collections=None):\n  \"\"\"Create an op that collects data for visualizing PR curves.\n\n  Unlike the op above, this one avoids computing precision, recall, and the\n  intermediate counts. Instead, it accepts those tensors as arguments and\n  relies on the caller to ensure that the calculations are correct (and the\n  counts yield the provided precision and recall values).\n\n  This op is useful when a caller seeks to compute precision and recall\n  differently but still use the PR curves plugin.\n\n  Args:\n    name: A tag attached to the summary. Used by TensorBoard for organization.\n    true_positive_counts: A rank-1 tensor of true positive counts. Must contain\n        `num_thresholds` elements and be castable to float32. Values correspond\n        to thresholds that increase from left to right (from 0 to 1).\n    false_positive_counts: A rank-1 tensor of false positive counts. Must\n        contain `num_thresholds` elements and be castable to float32. Values\n        correspond to thresholds that increase from left to right (from 0 to 1).\n    true_negative_counts: A rank-1 tensor of true negative counts. Must contain\n        `num_thresholds` elements and be castable to float32. Values\n        correspond to thresholds that increase from left to right (from 0 to 1).\n    false_negative_counts: A rank-1 tensor of false negative counts. Must\n        contain `num_thresholds` elements and be castable to float32. Values\n        correspond to thresholds that increase from left to right (from 0 to 1).\n    precision: A rank-1 tensor of precision values. Must contain\n        `num_thresholds` elements and be castable to float32. Values correspond\n        to thresholds that increase from left to right (from 0 to 1).\n    recall: A rank-1 tensor of recall values. Must contain `num_thresholds`\n        elements and be castable to float32. Values correspond to thresholds\n        that increase from left to right (from 0 to 1).\n    num_thresholds: Number of thresholds, evenly distributed in `[0, 1]`, to\n        compute PR metrics for. Should be `>= 2`. This value should be a\n        constant integer value, not a Tensor that stores an integer.\n    display_name: Optional name for this summary in TensorBoard, as a\n        constant `str`. Defaults to `name`.\n    description: Optional long-form description for this summary, as a\n        constant `str`. Markdown is supported. Defaults to empty.\n    collections: Optional list of graph collections keys. The new\n        summary op is added to these collections. Defaults to\n        `[Graph Keys.SUMMARIES]`.\n\n  Returns:\n    A summary operation for use in a TensorFlow graph. See docs for the `op`\n    method for details on the float32 tensor produced by this summary.\n  \"\"\"\n  # TODO(nickfelt): remove on-demand imports once dep situation is fixed.\n  import tensorflow.compat.v1 as tf\n\n  with tf.name_scope(name, values=[\n      true_positive_counts,\n      false_positive_counts,\n      true_negative_counts,\n      false_negative_counts,\n      precision,\n      recall,\n  ]):\n    return _create_tensor_summary(\n        name,\n        true_positive_counts,\n        false_positive_counts,\n        true_negative_counts,\n        false_negative_counts,\n        precision,\n        recall,\n        num_thresholds,\n        display_name,\n        description,\n        collections)", "response": "Create an op that collects data for visualizing PR curves."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef raw_data_pb(\n    name,\n    true_positive_counts,\n    false_positive_counts,\n    true_negative_counts,\n    false_negative_counts,\n    precision,\n    recall,\n    num_thresholds=None,\n    display_name=None,\n    description=None):\n  \"\"\"Create a PR curves summary protobuf from raw data values.\n\n  Args:\n    name: A tag attached to the summary. Used by TensorBoard for organization.\n    true_positive_counts: A rank-1 numpy array of true positive counts. Must\n        contain `num_thresholds` elements and be castable to float32.\n    false_positive_counts: A rank-1 numpy array of false positive counts. Must\n        contain `num_thresholds` elements and be castable to float32.\n    true_negative_counts: A rank-1 numpy array of true negative counts. Must\n        contain `num_thresholds` elements and be castable to float32.\n    false_negative_counts: A rank-1 numpy array of false negative counts. Must\n        contain `num_thresholds` elements and be castable to float32.\n    precision: A rank-1 numpy array of precision values. Must contain\n        `num_thresholds` elements and be castable to float32.\n    recall: A rank-1 numpy array of recall values. Must contain `num_thresholds`\n        elements and be castable to float32.\n    num_thresholds: Number of thresholds, evenly distributed in `[0, 1]`, to\n        compute PR metrics for. Should be an int `>= 2`.\n    display_name: Optional name for this summary in TensorBoard, as a `str`.\n        Defaults to `name`.\n    description: Optional long-form description for this summary, as a `str`.\n        Markdown is supported. Defaults to empty.\n\n  Returns:\n    A summary operation for use in a TensorFlow graph. See docs for the `op`\n    method for details on the float32 tensor produced by this summary.\n  \"\"\"\n  # TODO(nickfelt): remove on-demand imports once dep situation is fixed.\n  import tensorflow.compat.v1 as tf\n\n  if display_name is None:\n    display_name = name\n  summary_metadata = metadata.create_summary_metadata(\n      display_name=display_name if display_name is not None else name,\n      description=description or '',\n      num_thresholds=num_thresholds)\n  tf_summary_metadata = tf.SummaryMetadata.FromString(\n      summary_metadata.SerializeToString())\n  summary = tf.Summary()\n  data = np.stack(\n      (true_positive_counts,\n       false_positive_counts,\n       true_negative_counts,\n       false_negative_counts,\n       precision,\n       recall))\n  tensor = tf.make_tensor_proto(np.float32(data), dtype=tf.float32)\n  summary.value.add(tag='%s/pr_curves' % name,\n                    metadata=tf_summary_metadata,\n                    tensor=tensor)\n  return summary", "response": "Create a protobuf for a raw data set."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _create_tensor_summary(\n    name,\n    true_positive_counts,\n    false_positive_counts,\n    true_negative_counts,\n    false_negative_counts,\n    precision,\n    recall,\n    num_thresholds=None,\n    display_name=None,\n    description=None,\n    collections=None):\n  \"\"\"A private helper method for generating a tensor summary.\n\n  We use a helper method instead of having `op` directly call `raw_data_op`\n  to prevent the scope of `raw_data_op` from being embedded within `op`.\n\n  Arguments are the same as for raw_data_op.\n\n  Returns:\n    A tensor summary that collects data for PR curves.\n  \"\"\"\n  # TODO(nickfelt): remove on-demand imports once dep situation is fixed.\n  import tensorflow.compat.v1 as tf\n\n  # Store the number of thresholds within the summary metadata because\n  # that value is constant for all pr curve summaries with the same tag.\n  summary_metadata = metadata.create_summary_metadata(\n      display_name=display_name if display_name is not None else name,\n      description=description or '',\n      num_thresholds=num_thresholds)\n\n  # Store values within a tensor. We store them in the order:\n  # true positives, false positives, true negatives, false\n  # negatives, precision, and recall.\n  combined_data = tf.stack([\n      tf.cast(true_positive_counts, tf.float32),\n      tf.cast(false_positive_counts, tf.float32),\n      tf.cast(true_negative_counts, tf.float32),\n      tf.cast(false_negative_counts, tf.float32),\n      tf.cast(precision, tf.float32),\n      tf.cast(recall, tf.float32)])\n\n  return tf.summary.tensor_summary(\n      name='pr_curves',\n      tensor=combined_data,\n      collections=collections,\n      summary_metadata=summary_metadata)", "response": "Private helper method for generating a tensor summary."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nexecuting the request. Returns the response body of the request.", "response": "def run(self):\n    \"\"\"Executes the request.\n\n    Returns:\n       An array of tuples representing the metric evaluations--each of the form\n       (<wall time in secs>, <training step>, <metric value>).\n    \"\"\"\n    run, tag = metrics.run_tag_from_session_and_metric(\n        self._request.session_name, self._request.metric_name)\n    body, _ = self._scalars_plugin_instance.scalars_impl(\n        tag, run, None, scalars_plugin.OutputFormat.JSON)\n    return body"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_active(self):\n    if self._db_connection_provider:\n      # The plugin is active if one relevant tag can be found in the database.\n      db = self._db_connection_provider()\n      cursor = db.execute('''\n        SELECT\n          1\n        FROM Tags\n        WHERE Tags.plugin_name = ?\n        LIMIT 1\n      ''', (metadata.PLUGIN_NAME,))\n      return bool(list(cursor))\n\n    return bool(self._multiplexer) and any(self.index_impl().values())", "response": "This plugin is active iff any run has at least one histograms tag."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a list of histogram values for a given run and tag.", "response": "def histograms_impl(self, tag, run, downsample_to=None):\n    \"\"\"Result of the form `(body, mime_type)`, or `ValueError`.\n\n    At most `downsample_to` events will be returned. If this value is\n    `None`, then no downsampling will be performed.\n    \"\"\"\n    if self._db_connection_provider:\n      # Serve data from the database.\n      db = self._db_connection_provider()\n      cursor = db.cursor()\n      # Prefetch the tag ID matching this run and tag.\n      cursor.execute(\n          '''\n          SELECT\n            tag_id\n          FROM Tags\n          JOIN Runs USING (run_id)\n          WHERE\n            Runs.run_name = :run\n            AND Tags.tag_name = :tag\n            AND Tags.plugin_name = :plugin\n          ''',\n          {'run': run, 'tag': tag, 'plugin': metadata.PLUGIN_NAME})\n      row = cursor.fetchone()\n      if not row:\n        raise ValueError('No histogram tag %r for run %r' % (tag, run))\n      (tag_id,) = row\n      # Fetch tensor values, optionally with linear-spaced sampling by step.\n      # For steps ranging from s_min to s_max and sample size k, this query\n      # divides the range into k - 1 equal-sized intervals and returns the\n      # lowest step at or above each of the k interval boundaries (which always\n      # includes s_min and s_max, and may be fewer than k results if there are\n      # intervals where no steps are present). For contiguous steps the results\n      # can be formally expressed as the following:\n      #   [s_min + math.ceil(i / k * (s_max - s_min)) for i in range(0, k + 1)]\n      cursor.execute(\n          '''\n          SELECT\n            MIN(step) AS step,\n            computed_time,\n            data,\n            dtype,\n            shape\n          FROM Tensors\n          INNER JOIN (\n            SELECT\n              MIN(step) AS min_step,\n              MAX(step) AS max_step\n            FROM Tensors\n            /* Filter out NULL so we can use TensorSeriesStepIndex. */\n            WHERE series = :tag_id AND step IS NOT NULL\n          )\n          /* Ensure we omit reserved rows, which have NULL step values. */\n          WHERE series = :tag_id AND step IS NOT NULL\n          /* Bucket rows into sample_size linearly spaced buckets, or do\n             no sampling if sample_size is NULL. */\n          GROUP BY\n            IFNULL(:sample_size - 1, max_step - min_step)\n            * (step - min_step) / (max_step - min_step)\n          ORDER BY step\n          ''',\n          {'tag_id': tag_id, 'sample_size': downsample_to})\n      events = [(computed_time, step, self._get_values(data, dtype, shape))\n                for step, computed_time, data, dtype, shape in cursor]\n    else:\n      # Serve data from events files.\n      try:\n        tensor_events = self._multiplexer.Tensors(run, tag)\n      except KeyError:\n        raise ValueError('No histogram tag %r for run %r' % (tag, run))\n      if downsample_to is not None and len(tensor_events) > downsample_to:\n        rand_indices = random.Random(0).sample(\n            six.moves.xrange(len(tensor_events)), downsample_to)\n        indices = sorted(rand_indices)\n        tensor_events = [tensor_events[i] for i in indices]\n      events = [[e.wall_time, e.step, tensor_util.make_ndarray(e.tensor_proto).tolist()]\n                for e in tensor_events]\n    return (events, 'application/json')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nobtain values for histogram data given blob and dtype enum.", "response": "def _get_values(self, data_blob, dtype_enum, shape_string):\n    \"\"\"Obtains values for histogram data given blob and dtype enum.\n    Args:\n      data_blob: The blob obtained from the database.\n      dtype_enum: The enum representing the dtype.\n      shape_string: A comma-separated string of numbers denoting shape.\n    Returns:\n      The histogram values as a list served to the frontend.\n    \"\"\"\n    buf = np.frombuffer(data_blob, dtype=tf.DType(dtype_enum).as_numpy_dtype)\n    return buf.reshape([int(i) for i in shape_string.split(',')]).tolist()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef histograms_route(self, request):\n    tag = request.args.get('tag')\n    run = request.args.get('run')\n    try:\n      (body, mime_type) = self.histograms_impl(\n          tag, run, downsample_to=self.SAMPLE_SIZE)\n      code = 200\n    except ValueError as e:\n      (body, mime_type) = (str(e), 'text/plain')\n      code = 400\n    return http_util.Respond(request, body, mime_type, code=code)", "response": "Given a tag and single run return array of histogram values."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninitialize the graph and session if this has not yet been done.", "response": "def _lazily_initialize(self):\n    \"\"\"Initialize the graph and session, if this has not yet been done.\"\"\"\n    # TODO(nickfelt): remove on-demand imports once dep situation is fixed.\n    import tensorflow.compat.v1 as tf\n    with self._initialization_lock:\n      if self._session:\n        return\n      graph = tf.Graph()\n      with graph.as_default():\n        self.initialize_graph()\n      # Don't reserve GPU because libpng can't run on GPU.\n      config = tf.ConfigProto(device_count={'GPU': 0})\n      self._session = tf.Session(graph=graph, config=config)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntrying to get the scalars plugin.", "response": "def _get_scalars_plugin(self):\n    \"\"\"Tries to get the scalars plugin.\n\n    Returns:\n      The scalars plugin. Or None if it is not yet registered.\n    \"\"\"\n    if scalars_metadata.PLUGIN_NAME in self._plugin_name_to_instance:\n      # The plugin is registered.\n      return self._plugin_name_to_instance[scalars_metadata.PLUGIN_NAME]\n    # The plugin is not yet registered.\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns True if the plugin is active if 2 conditions hold.", "response": "def is_active(self):\n    \"\"\"This plugin is active if 2 conditions hold.\n\n    1. The scalars plugin is registered and active.\n    2. There is a custom layout for the dashboard.\n\n    Returns: A boolean. Whether the plugin is active.\n    \"\"\"\n    if not self._multiplexer:\n      return False\n\n    scalars_plugin_instance = self._get_scalars_plugin()\n    if not (scalars_plugin_instance and\n            scalars_plugin_instance.is_active()):\n      return False\n\n    # This plugin is active if any run has a layout.\n    return bool(self._multiplexer.PluginRunToTagToContent(metadata.PLUGIN_NAME))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprovides a response for downloading scalars data for a data series.", "response": "def download_data_impl(self, run, tag, response_format):\n    \"\"\"Provides a response for downloading scalars data for a data series.\n\n    Args:\n      run: The run.\n      tag: The specific tag.\n      response_format: A string. One of the values of the OutputFormat enum of\n        the scalar plugin.\n\n    Raises:\n      ValueError: If the scalars plugin is not registered.\n\n    Returns:\n      2 entities:\n        - A JSON object response body.\n        - A mime type (string) for the response.\n    \"\"\"\n    scalars_plugin_instance = self._get_scalars_plugin()\n    if not scalars_plugin_instance:\n      raise ValueError(('Failed to respond to request for /download_data. '\n                        'The scalars plugin is oddly not registered.'))\n\n    body, mime_type = scalars_plugin_instance.scalars_impl(\n        tag, run, None, response_format)\n    return body, mime_type"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef scalars_route(self, request):\n    # TODO: return HTTP status code for malformed requests\n    tag_regex_string = request.args.get('tag')\n    run = request.args.get('run')\n    mime_type = 'application/json'\n\n    try:\n      body = self.scalars_impl(run, tag_regex_string)\n    except ValueError as e:\n      return http_util.Respond(\n          request=request,\n          content=str(e),\n          content_type='text/plain',\n          code=500)\n\n    # Produce the response.\n    return http_util.Respond(request, body, mime_type)", "response": "This route handles the scalar events request."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef scalars_impl(self, run, tag_regex_string):\n    if not tag_regex_string:\n      # The user provided no regex.\n      return {\n          _REGEX_VALID_PROPERTY: False,\n          _TAG_TO_EVENTS_PROPERTY: {},\n      }\n\n    # Construct the regex.\n    try:\n      regex = re.compile(tag_regex_string)\n    except re.error:\n      return {\n          _REGEX_VALID_PROPERTY: False,\n          _TAG_TO_EVENTS_PROPERTY: {},\n      }\n\n    # Fetch the tags for the run. Filter for tags that match the regex.\n    run_to_data = self._multiplexer.PluginRunToTagToContent(\n        scalars_metadata.PLUGIN_NAME)\n\n    tag_to_data = None\n    try:\n      tag_to_data = run_to_data[run]\n    except KeyError:\n      # The run could not be found. Perhaps a configuration specified a run that\n      # TensorBoard has not read from disk yet.\n      payload = {}\n\n    if tag_to_data:\n      scalars_plugin_instance = self._get_scalars_plugin()\n      if not scalars_plugin_instance:\n        raise ValueError(('Failed to respond to request for /scalars. '\n                          'The scalars plugin is oddly not registered.'))\n\n      form = scalars_plugin.OutputFormat.JSON\n      payload = {\n        tag: scalars_plugin_instance.scalars_impl(tag, run, None, form)[0]\n            for tag in tag_to_data.keys()\n            if regex.match(tag)\n      }\n\n    return {\n        _REGEX_VALID_PROPERTY: True,\n        _TAG_TO_EVENTS_PROPERTY: payload,\n    }", "response": "Given a tag regex and single run return a dictionary that is the JSON - able response."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a JSON object containing the custom layout specified by the config file in the logdir.", "response": "def layout_route(self, request):\n    r\"\"\"Fetches the custom layout specified by the config file in the logdir.\n\n    If more than 1 run contains a layout, this method merges the layouts by\n    merging charts within individual categories. If 2 categories with the same\n    name are found, the charts within are merged. The merging is based on the\n    order of the runs to which the layouts are written.\n\n    The response is a JSON object mirroring properties of the Layout proto if a\n    layout for any run is found.\n\n    The response is an empty object if no layout could be found.\n    \"\"\"\n    body = self.layout_impl()\n    return http_util.Respond(request, body, 'application/json')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef make_table_row(contents, tag='td'):\n  columns = ('<%s>%s</%s>\\n' % (tag, s, tag) for s in contents)\n  return '<tr>\\n' + ''.join(columns) + '</tr>\\n'", "response": "Given an iterable of string contents and a tag creates a table row."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngives a numpy array of strings concatenate them into a html table.", "response": "def make_table(contents, headers=None):\n  \"\"\"Given a numpy ndarray of strings, concatenate them into a html table.\n\n  Args:\n    contents: A np.ndarray of strings. May be 1d or 2d. In the 1d case, the\n      table is laid out vertically (i.e. row-major).\n    headers: A np.ndarray or list of string header names for the table.\n\n  Returns:\n    A string containing all of the content strings, organized into a table.\n\n  Raises:\n    ValueError: If contents is not a np.ndarray.\n    ValueError: If contents is not 1d or 2d.\n    ValueError: If contents is empty.\n    ValueError: If headers is present and not a list, tuple, or ndarray.\n    ValueError: If headers is not 1d.\n    ValueError: If number of elements in headers does not correspond to number\n      of columns in contents.\n  \"\"\"\n  if not isinstance(contents, np.ndarray):\n    raise ValueError('make_table contents must be a numpy ndarray')\n\n  if contents.ndim not in [1, 2]:\n    raise ValueError('make_table requires a 1d or 2d numpy array, was %dd' %\n                     contents.ndim)\n\n  if headers:\n    if isinstance(headers, (list, tuple)):\n      headers = np.array(headers)\n    if not isinstance(headers, np.ndarray):\n      raise ValueError('Could not convert headers %s into np.ndarray' % headers)\n    if headers.ndim != 1:\n      raise ValueError('Headers must be 1d, is %dd' % headers.ndim)\n    expected_n_columns = contents.shape[1] if contents.ndim == 2 else 1\n    if headers.shape[0] != expected_n_columns:\n      raise ValueError('Number of headers %d must match number of columns %d' %\n                       (headers.shape[0], expected_n_columns))\n    header = '<thead>\\n%s</thead>\\n' % make_table_row(headers, tag='th')\n  else:\n    header = ''\n\n  n_rows = contents.shape[0]\n  if contents.ndim == 1:\n    # If it's a vector, we need to wrap each element in a new list, otherwise\n    # we would turn the string itself into a row (see test code)\n    rows = (make_table_row([contents[i]]) for i in range(n_rows))\n  else:\n    rows = (make_table_row(contents[i, :]) for i in range(n_rows))\n\n  return '<table>\\n%s<tbody>\\n%s</tbody>\\n</table>' % (header, ''.join(rows))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngive a np. npdarray with nDims > 2 reduce it to 2d.", "response": "def reduce_to_2d(arr):\n  \"\"\"Given a np.npdarray with nDims > 2, reduce it to 2d.\n\n  It does this by selecting the zeroth coordinate for every dimension greater\n  than two.\n\n  Args:\n    arr: a numpy ndarray of dimension at least 2.\n\n  Returns:\n    A two-dimensional subarray from the input array.\n\n  Raises:\n    ValueError: If the argument is not a numpy ndarray, or the dimensionality\n      is too low.\n  \"\"\"\n  if not isinstance(arr, np.ndarray):\n    raise ValueError('reduce_to_2d requires a numpy.ndarray')\n\n  ndims = len(arr.shape)\n  if ndims < 2:\n    raise ValueError('reduce_to_2d requires an array of dimensionality >=2')\n  # slice(None) is equivalent to `:`, so we take arr[0,0,...0,:,:]\n  slices = ([0] * (ndims - 2)) + [slice(None), slice(None)]\n  return arr[slices]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef text_array_to_html(text_arr):\n  if not text_arr.shape:\n    # It is a scalar. No need to put it in a table, just apply markdown\n    return plugin_util.markdown_to_safe_html(np.asscalar(text_arr))\n  warning = ''\n  if len(text_arr.shape) > 2:\n    warning = plugin_util.markdown_to_safe_html(WARNING_TEMPLATE\n                                                % len(text_arr.shape))\n    text_arr = reduce_to_2d(text_arr)\n\n  html_arr = [plugin_util.markdown_to_safe_html(x)\n              for x in text_arr.reshape(-1)]\n  html_arr = np.array(html_arr).reshape(text_arr.shape)\n\n  return warning + make_table(html_arr)", "response": "Take a numpy. ndarray containing strings and convert it into html."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef process_string_tensor_event(event):\n  string_arr = tensor_util.make_ndarray(event.tensor_proto)\n  html = text_array_to_html(string_arr)\n  return {\n      'wall_time': event.wall_time,\n      'step': event.step,\n      'text': html,\n  }", "response": "Convert a TensorEvent into a JSON - compatible response."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndetermine whether this plugin is active.", "response": "def is_active(self):\n    \"\"\"Determines whether this plugin is active.\n\n    This plugin is only active if TensorBoard sampled any text summaries.\n\n    Returns:\n      Whether this plugin is active.\n    \"\"\"\n    if not self._multiplexer:\n      return False\n\n    if self._index_cached is not None:\n      # If we already have computed the index, use it to determine whether\n      # the plugin should be active, and if so, return immediately.\n      if any(self._index_cached.values()):\n        return True\n\n    if self._multiplexer.PluginRunToTagToContent(metadata.PLUGIN_NAME):\n      # Text data is present in the multiplexer. No need to further check for\n      # data stored via the outdated plugin assets method.\n      return True\n\n    # We haven't conclusively determined if the plugin should be active. Launch\n    # a thread to compute index_impl() and return False to avoid blocking.\n    self._maybe_launch_index_impl_thread()\n\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlaunching a thread to compute index_impl.", "response": "def _maybe_launch_index_impl_thread(self):\n    \"\"\"Attempts to launch a thread to compute index_impl().\n\n    This may not launch a new thread if one is already running to compute\n    index_impl(); in that case, this function is a no-op.\n    \"\"\"\n    # Try to acquire the lock for computing index_impl(), without blocking.\n    if self._index_impl_lock.acquire(False):\n      # We got the lock. Start the thread, which will unlock the lock when done.\n      self._index_impl_thread = threading.Thread(\n          target=self._async_index_impl,\n          name='TextPluginIndexImplThread')\n      self._index_impl_thread.start()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing index_impl asynchronously on a separate thread.", "response": "def _async_index_impl(self):\n    \"\"\"Computes index_impl() asynchronously on a separate thread.\"\"\"\n    start = time.time()\n    logger.info('TextPlugin computing index_impl() in a new thread')\n    self._index_cached = self.index_impl()\n    self._index_impl_thread = None\n    self._index_impl_lock.release()\n    elapsed = time.time() - start\n    logger.info(\n        'TextPlugin index_impl() thread ending after %0.3f sec', elapsed)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_summary_metadata(display_name, description, num_thresholds):\n  pr_curve_plugin_data = plugin_data_pb2.PrCurvePluginData(\n      version=PROTO_VERSION, num_thresholds=num_thresholds)\n  content = pr_curve_plugin_data.SerializeToString()\n  return summary_pb2.SummaryMetadata(\n      display_name=display_name,\n      summary_description=description,\n      plugin_data=summary_pb2.SummaryMetadata.PluginData(\n          plugin_name=PLUGIN_NAME,\n          content=content))", "response": "Create a summary_pb2. SummaryMetadata proto for PR curves plugin data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_plugin_metadata(content):\n  if not isinstance(content, bytes):\n    raise TypeError('Content type must be bytes')\n  result = plugin_data_pb2.PrCurvePluginData.FromString(content)\n  if result.version == 0:\n    return result\n  else:\n    logger.warn(\n        'Unknown metadata version: %s. The latest version known to '\n        'this build of TensorBoard is %s; perhaps a newer build is '\n        'available?', result.version, PROTO_VERSION)\n    return result", "response": "Parse summary metadata to a Python object."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a dict mapping keys in TRACKED_FIELDS to an Observation list.", "response": "def get_field_to_observations_map(generator, query_for_tag=''):\n  \"\"\"Return a field to `Observations` dict for the event generator.\n\n  Args:\n    generator: A generator over event protos.\n    query_for_tag: A string that if specified, only create observations for\n      events with this tag name.\n\n  Returns:\n    A dict mapping keys in `TRACKED_FIELDS` to an `Observation` list.\n  \"\"\"\n\n  def increment(stat, event, tag=''):\n    assert stat in TRACKED_FIELDS\n    field_to_obs[stat].append(Observation(step=event.step,\n                                          wall_time=event.wall_time,\n                                          tag=tag)._asdict())\n\n  field_to_obs = dict([(t, []) for t in TRACKED_FIELDS])\n\n  for event in generator:\n    ## Process the event\n    if event.HasField('graph_def') and (not query_for_tag):\n      increment('graph', event)\n    if event.HasField('session_log') and (not query_for_tag):\n      status = event.session_log.status\n      if status == event_pb2.SessionLog.START:\n        increment('sessionlog:start', event)\n      elif status == event_pb2.SessionLog.STOP:\n        increment('sessionlog:stop', event)\n      elif status == event_pb2.SessionLog.CHECKPOINT:\n        increment('sessionlog:checkpoint', event)\n    elif event.HasField('summary'):\n      for value in event.summary.value:\n        if query_for_tag and value.tag != query_for_tag:\n          continue\n\n        for proto_name, display_name in SUMMARY_TYPE_TO_FIELD.items():\n          if value.HasField(proto_name):\n            increment(display_name, event, value.tag)\n  return field_to_obs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_unique_tags(field_to_obs):\n  return {field: sorted(set([x.get('tag', '') for x in observations]))\n          for field, observations in field_to_obs.items()\n          if field in TAG_FIELDS}", "response": "Returns a dictionary of unique tags that a user could query over."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprints a shallow dict to console.", "response": "def print_dict(d, show_missing=True):\n  \"\"\"Prints a shallow dict to console.\n\n  Args:\n    d: Dict to print.\n    show_missing: Whether to show keys with empty values.\n  \"\"\"\n  for k, v in sorted(d.items()):\n    if (not v) and show_missing:\n      # No instances of the key, so print missing symbol.\n      print('{} -'.format(k))\n    elif isinstance(v, list):\n      # Value is a list, so print each item of the list.\n      print(k)\n      for item in v:\n        print('   {}'.format(item))\n    elif isinstance(v, dict):\n      # Value is a dict, so print each (key, value) pair of the dict.\n      print(k)\n      for kk, vv in sorted(v.items()):\n        print('   {:<20} {}'.format(kk, vv))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntransform the field - to - obs mapping into a printable dictionary.", "response": "def get_dict_to_print(field_to_obs):\n  \"\"\"Transform the field-to-obs mapping into a printable dictionary.\n\n  Args:\n    field_to_obs: Dict that maps string field to `Observation` list.\n\n  Returns:\n    A dict with the keys and values to print to console.\n  \"\"\"\n\n  def compressed_steps(steps):\n    return {'num_steps': len(set(steps)),\n            'min_step': min(steps),\n            'max_step': max(steps),\n            'last_step': steps[-1],\n            'first_step': steps[0],\n            'outoforder_steps': get_out_of_order(steps)}\n\n  def full_steps(steps):\n    return {'steps': steps, 'outoforder_steps': get_out_of_order(steps)}\n\n  output = {}\n  for field, observations in field_to_obs.items():\n    if not observations:\n      output[field] = None\n      continue\n\n    steps = [x['step'] for x in observations]\n    if field in SHORT_FIELDS:\n      output[field] = compressed_steps(steps)\n    if field in LONG_FIELDS:\n      output[field] = full_steps(steps)\n\n  return output"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns elements that break the monotonically non - decreasing trend.", "response": "def get_out_of_order(list_of_numbers):\n  \"\"\"Returns elements that break the monotonically non-decreasing trend.\n\n  This is used to find instances of global step values that are \"out-of-order\",\n  which may trigger TensorBoard event discarding logic.\n\n  Args:\n    list_of_numbers: A list of numbers.\n\n  Returns:\n    A list of tuples in which each tuple are two elements are adjacent, but the\n    second element is lower than the first.\n  \"\"\"\n  # TODO: Consider changing this to only check for out-of-order\n  # steps within a particular tag.\n  result = []\n  # pylint: disable=consider-using-enumerate\n  for i in range(len(list_of_numbers)):\n    if i == 0:\n      continue\n    if list_of_numbers[i] < list_of_numbers[i - 1]:\n      result.append((list_of_numbers[i - 1], list_of_numbers[i]))\n  return result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a list of event generators for each subdirectory with event files.", "response": "def generators_from_logdir(logdir):\n  \"\"\"Returns a list of event generators for subdirectories with event files.\n\n  The number of generators returned should equal the number of directories\n  within logdir that contain event files. If only logdir contains event files,\n  returns a list of length one.\n\n  Args:\n    logdir: A log directory that contains event files.\n\n  Returns:\n    List of event generators for each subdirectory with event files.\n  \"\"\"\n  subdirs = io_wrapper.GetLogdirSubdirectories(logdir)\n  generators = [\n      itertools.chain(*[\n          generator_from_event_file(os.path.join(subdir, f))\n          for f in tf.io.gfile.listdir(subdir)\n          if io_wrapper.IsTensorFlowEventsFile(os.path.join(subdir, f))\n      ]) for subdir in subdirs\n  ]\n  return generators"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_inspection_units(logdir='', event_file='', tag=''):\n  if logdir:\n    subdirs = io_wrapper.GetLogdirSubdirectories(logdir)\n    inspection_units = []\n    for subdir in subdirs:\n      generator = itertools.chain(*[\n          generator_from_event_file(os.path.join(subdir, f))\n          for f in tf.io.gfile.listdir(subdir)\n          if io_wrapper.IsTensorFlowEventsFile(os.path.join(subdir, f))\n      ])\n      inspection_units.append(InspectionUnit(\n          name=subdir,\n          generator=generator,\n          field_to_obs=get_field_to_observations_map(generator, tag)))\n    if inspection_units:\n      print('Found event files in:\\n{}\\n'.format('\\n'.join(\n          [u.name for u in inspection_units])))\n    elif io_wrapper.IsTensorFlowEventsFile(logdir):\n      print(\n          'It seems that {} may be an event file instead of a logdir. If this '\n          'is the case, use --event_file instead of --logdir to pass '\n          'it in.'.format(logdir))\n    else:\n      print('No event files found within logdir {}'.format(logdir))\n    return inspection_units\n  elif event_file:\n    generator = generator_from_event_file(event_file)\n    return [InspectionUnit(\n        name=event_file,\n        generator=generator,\n        field_to_obs=get_field_to_observations_map(generator, tag))]\n  return []", "response": "Returns a list of InspectionUnit objects given either logdir or event_file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding the CLI flags to the given parser.", "response": "def define_flags(self, parser):\n    \"\"\"Adds DebuggerPlugin CLI flags to parser.\"\"\"\n    group = parser.add_argument_group('debugger plugin')\n    group.add_argument(\n        '--debugger_data_server_grpc_port',\n        metavar='PORT',\n        type=int,\n        default=-1,\n        help='''\\\nThe port at which the non-interactive debugger data server should\nreceive debugging data via gRPC from one or more debugger-enabled\nTensorFlow runtimes. No debugger plugin or debugger data server will be\nstarted if this flag is not provided. This flag differs from the\n`--debugger_port` flag in that it starts a non-interactive mode. It is\nfor use with the \"health pills\" feature of the Graph Dashboard. This\nflag is mutually exclusive with `--debugger_port`.\\\n''')\n    group.add_argument(\n        '--debugger_port',\n        metavar='PORT',\n        type=int,\n        default=-1,\n        help='''\\\nThe port at which the interactive debugger data server (to be started by\nthe debugger plugin) should receive debugging data via gRPC from one or\nmore debugger-enabled TensorFlow runtimes. No debugger plugin or\ndebugger data server will be started if this flag is not provided. This\nflag differs from the `--debugger_data_server_grpc_port` flag in that it\nstarts an interactive mode that allows user to pause at selected nodes\ninside a TensorFlow Graph or between Session.runs. It is for use with\nthe interactive Debugger Dashboard. This flag is mutually exclusive with\n`--debugger_data_server_grpc_port`.\\\n''')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading the debugger plugin if possible.", "response": "def load(self, context):\n    \"\"\"Returns the debugger plugin, if possible.\n\n    Args:\n      context: The TBContext flags including `add_arguments`.\n\n    Returns:\n      A DebuggerPlugin instance or None if it couldn't be loaded.\n    \"\"\"\n    if not (context.flags.debugger_data_server_grpc_port > 0 or\n            context.flags.debugger_port > 0):\n      return None\n    flags = context.flags\n    try:\n      # pylint: disable=g-import-not-at-top,unused-import\n      import tensorflow\n    except ImportError:\n      raise ImportError(\n          'To use the debugger plugin, you need to have TensorFlow installed:\\n'\n          '  pip install tensorflow')\n    try:\n      # pylint: disable=line-too-long,g-import-not-at-top\n      from tensorboard.plugins.debugger import debugger_plugin as debugger_plugin_lib\n      from tensorboard.plugins.debugger import interactive_debugger_plugin as interactive_debugger_plugin_lib\n      # pylint: enable=line-too-long,g-import-not-at-top\n    except ImportError as e:\n      e_type, e_value, e_traceback = sys.exc_info()\n      message = e.msg if hasattr(e, 'msg') else e.message  # Handle py2 vs py3\n      if 'grpc' in message:\n        e_value = ImportError(\n            message +\n            '\\n\\nTo use the debugger plugin, you need to have '\n            'gRPC installed:\\n  pip install grpcio')\n      six.reraise(e_type, e_value, e_traceback)\n    if flags.debugger_port > 0:\n      interactive_plugin = (\n          interactive_debugger_plugin_lib.InteractiveDebuggerPlugin(context))\n      logger.info('Starting Interactive Debugger Plugin at gRPC port %d',\n                   flags.debugger_data_server_grpc_port)\n      interactive_plugin.listen(flags.debugger_port)\n      return interactive_plugin\n    elif flags.debugger_data_server_grpc_port > 0:\n      noninteractive_plugin = debugger_plugin_lib.DebuggerPlugin(context)\n      logger.info('Starting Non-interactive Debugger Plugin at gRPC port %d',\n                   flags.debugger_data_server_grpc_port)\n      noninteractive_plugin.listen(flags.debugger_data_server_grpc_port)\n      return noninteractive_plugin\n    raise AssertionError()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_summary_metadata(hparams_plugin_data_pb):\n  if not isinstance(hparams_plugin_data_pb, plugin_data_pb2.HParamsPluginData):\n    raise TypeError('Needed an instance of plugin_data_pb2.HParamsPluginData.'\n                    ' Got: %s' % type(hparams_plugin_data_pb))\n  content = plugin_data_pb2.HParamsPluginData()\n  content.CopyFrom(hparams_plugin_data_pb)\n  content.version = PLUGIN_DATA_VERSION\n  return tf.compat.v1.SummaryMetadata(\n      plugin_data=tf.compat.v1.SummaryMetadata.PluginData(\n          plugin_name=PLUGIN_NAME, content=content.SerializeToString()))", "response": "Creates a summary_pb2. SummaryMetadata for the HParams plugin."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _parse_plugin_data_as(content, data_oneof_field):\n  plugin_data = plugin_data_pb2.HParamsPluginData.FromString(content)\n  if plugin_data.version != PLUGIN_DATA_VERSION:\n    raise error.HParamsError(\n        'Only supports plugin_data version: %s; found: %s in: %s' %\n        (PLUGIN_DATA_VERSION, plugin_data.version, plugin_data))\n  if not plugin_data.HasField(data_oneof_field):\n    raise error.HParamsError(\n        'Expected plugin_data.%s to be set. Got: %s' %\n        (data_oneof_field, plugin_data))\n  return getattr(plugin_data, data_oneof_field)", "response": "Parses the plugin_data. content. crawled_content and returns the data oneof s field."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef write_event(self, event):\n    self._lock.acquire()\n    try:\n      self._events_writer.WriteEvent(event)\n      self._event_count += 1\n      if self._always_flush:\n        # We flush on every event within the integration test.\n        self._events_writer.Flush()\n\n      if self._event_count == self._check_this_often:\n        # Every so often, we check whether the size of the file is too big.\n        self._event_count = 0\n\n        # Flush to get an accurate size check.\n        self._events_writer.Flush()\n\n        file_path = os.path.join(self._events_directory,\n                                 self.get_current_file_name())\n        if not tf.io.gfile.exists(file_path):\n          # The events file does not exist. Perhaps the user had manually\n          # deleted it after training began. Create a new one.\n          self._events_writer.Close()\n          self._events_writer = self._create_events_writer(\n              self._events_directory)\n        elif tf.io.gfile.stat(file_path).length > self._single_file_size_cap_bytes:\n          # The current events file has gotten too big. Close the previous\n          # events writer. Make a new one.\n          self._events_writer.Close()\n          self._events_writer = self._create_events_writer(\n              self._events_directory)\n    except IOError as err:\n      logger.error(\n          \"Writing to %s failed: %s\", self.get_current_file_name(), err)\n    self._lock.release()", "response": "Writes an event to disk."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndisposes of this events writer manager.", "response": "def dispose(self):\n    \"\"\"Disposes of this events writer manager, making it no longer usable.\n\n    Call this method when this object is done being used in order to clean up\n    resources and handlers. This method should ever only be called once.\n    \"\"\"\n    self._lock.acquire()\n    self._events_writer.Close()\n    self._events_writer = None\n    self._lock.release()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _create_events_writer(self, directory):\n    total_size = 0\n    events_files = self._fetch_events_files_on_disk()\n    for file_name in events_files:\n      file_path = os.path.join(self._events_directory, file_name)\n      total_size += tf.io.gfile.stat(file_path).length\n\n    if total_size >= self.total_file_size_cap_bytes:\n      # The total size written to disk is too big. Delete events files until\n      # the size is below the cap.\n      for file_name in events_files:\n        if total_size < self.total_file_size_cap_bytes:\n          break\n\n        file_path = os.path.join(self._events_directory, file_name)\n        file_size = tf.io.gfile.stat(file_path).length\n        try:\n          tf.io.gfile.remove(file_path)\n          total_size -= file_size\n          logger.info(\n              \"Deleted %s because events files take up over %d bytes\",\n              file_path, self.total_file_size_cap_bytes)\n        except IOError as err:\n          logger.error(\"Deleting %s failed: %s\", file_path, err)\n\n    # We increment this index because each events writer must differ in prefix.\n    self._events_file_count += 1\n    file_path = \"%s.%d.%d\" % (\n        os.path.join(directory, DEBUGGER_EVENTS_FILE_STARTING_TEXT),\n        time.time(), self._events_file_count)\n    logger.info(\"Creating events file %s\", file_path)\n    return pywrap_tensorflow.EventsWriter(tf.compat.as_bytes(file_path))", "response": "Creates a new events writer."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nretrieves the names of the debugger - related events files within the directory.", "response": "def _fetch_events_files_on_disk(self):\n    \"\"\"Obtains the names of debugger-related events files within the directory.\n\n    Returns:\n      The names of the debugger-related events files written to disk. The names\n      are sorted in increasing events file index.\n    \"\"\"\n    all_files = tf.io.gfile.listdir(self._events_directory)\n    relevant_files = [\n        file_name for file_name in all_files\n        if _DEBUGGER_EVENTS_FILE_NAME_REGEX.match(file_name)\n    ]\n    return sorted(relevant_files, key=self._obtain_file_index)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nencodes image to PNG on thread_count threads in parallel.", "response": "def bench(image, thread_count):\n  \"\"\"Encode `image` to PNG on `thread_count` threads in parallel.\n\n  Returns:\n    A `float` representing number of seconds that it takes all threads\n    to finish encoding `image`.\n  \"\"\"\n  threads = [threading.Thread(target=lambda: encoder.encode_png(image))\n             for _ in xrange(thread_count)]\n  start_time = datetime.datetime.now()\n  for thread in threads:\n    thread.start()\n  for thread in threads:\n    thread.join()\n  end_time = datetime.datetime.now()\n  delta = (end_time - start_time).total_seconds()\n  return delta"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _image_of_size(image_size):\n  return np.random.uniform(0, 256, [image_size, image_size, 3]).astype(np.uint8)", "response": "Generate a square RGB test image of the given side length."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _format_line(headers, fields):\n  assert len(fields) == len(headers), (fields, headers)\n  fields = [\"%2.4f\" % field if isinstance(field, float) else str(field)\n            for field in fields]\n  return '  '.join(' ' * max(0, len(header) - len(field)) + field\n                   for (header, field) in zip(headers, fields))", "response": "Format a line of a table."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_gated_grpc_tensors(self, matching_debug_op=None):\n    with self._grpc_gated_lock:\n      matching_debug_op = matching_debug_op or 'DebugIdentity'\n      if matching_debug_op not in self._grpc_gated_tensors:\n        # First, construct a map from node name to op type.\n        node_name_to_op_type = dict(\n            (node.name, node.op) for node in self._graph_def.node)\n\n        # Second, populate the output list.\n        gated = []\n        for node in self._graph_def.node:\n          if node.op == matching_debug_op:\n            for attr_key in node.attr:\n              if attr_key == 'gated_grpc' and node.attr[attr_key].b:\n                node_name, output_slot, _, debug_op = (\n                    debug_graphs.parse_debug_node_name(node.name))\n                gated.append(\n                    (node_name, node_name_to_op_type[node_name], output_slot,\n                     debug_op))\n                break\n        self._grpc_gated_tensors[matching_debug_op] = gated\n\n      return self._grpc_gated_tensors[matching_debug_op]", "response": "Extract all nodes with gated - gRPC debug ops attached."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nexpands the base name if there are nested under the node.", "response": "def maybe_base_expanded_node_name(self, node_name):\n    \"\"\"Expand the base name if there are node names nested under the node.\n\n    For example, if there are two nodes in the graph, \"a\" and \"a/read\", then\n    calling this function on \"a\" will give \"a/(a)\", a form that points at\n    a leaf node in the nested TensorBoard graph. Calling this function on\n    \"a/read\" will just return \"a/read\", because there is no node nested under\n    it.\n\n    This method is thread-safe.\n\n    Args:\n      node_name: Name of the node.\n      graph_def: The `GraphDef` that the node is a part of.\n\n    Returns:\n      Possibly base-expanded node name.\n    \"\"\"\n    with self._node_name_lock:\n      # Lazily populate the map from original node name to base-expanded ones.\n      if self._maybe_base_expanded_node_names is None:\n        self._maybe_base_expanded_node_names = dict()\n        # Sort all the node names.\n        sorted_names = sorted(node.name for node in self._graph_def.node)\n        for i, name in enumerate(sorted_names):\n          j = i + 1\n          while j < len(sorted_names) and sorted_names[j].startswith(name):\n            if sorted_names[j].startswith(name + '/'):\n              self._maybe_base_expanded_node_names[name] = (\n                  name + '/(' + name.split('/')[-1] + ')')\n              break\n            j += 1\n      return self._maybe_base_expanded_node_names.get(node_name, node_name)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd runs from a directory.", "response": "def AddRunsFromDirectory(self, path, name=None):\n    \"\"\"Load runs from a directory; recursively walks subdirectories.\n\n    If path doesn't exist, no-op. This ensures that it is safe to call\n      `AddRunsFromDirectory` multiple times, even before the directory is made.\n\n    Args:\n      path: A string path to a directory to load runs from.\n      name: Optional, specifies a name for the experiment under which the\n        runs from this directory hierarchy will be imported. If omitted, the\n        path will be used as the name.\n\n    Raises:\n      ValueError: If the path exists and isn't a directory.\n    \"\"\"\n    logger.info('Starting AddRunsFromDirectory: %s (as %s)', path, name)\n    for subdir in io_wrapper.GetLogdirSubdirectories(path):\n      logger.info('Processing directory %s', subdir)\n      if subdir not in self._run_loaders:\n        logger.info('Creating DB loader for directory %s', subdir)\n        names = self._get_exp_and_run_names(path, subdir, name)\n        experiment_name, run_name = names\n        self._run_loaders[subdir] = _RunLoader(\n            subdir=subdir,\n            experiment_name=experiment_name,\n            run_name=run_name)\n    logger.info('Done with AddRunsFromDirectory: %s', path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads events from every detected run.", "response": "def Reload(self):\n    \"\"\"Load events from every detected run.\"\"\"\n    logger.info('Beginning DbImportMultiplexer.Reload()')\n    # Defer event sink creation until needed; this ensures it will only exist in\n    # the thread that calls Reload(), since DB connections must be thread-local.\n    if not self._event_sink:\n      self._event_sink = self._CreateEventSink()\n    # Use collections.deque() for speed when we don't need blocking since it\n    # also has thread-safe appends/pops.\n    loader_queue = collections.deque(six.itervalues(self._run_loaders))\n    loader_delete_queue = collections.deque()\n\n    def batch_generator():\n      while True:\n        try:\n          loader = loader_queue.popleft()\n        except IndexError:\n          return\n        try:\n          for batch in loader.load_batches():\n            yield batch\n        except directory_watcher.DirectoryDeletedError:\n          loader_delete_queue.append(loader)\n        except (OSError, IOError) as e:\n          logger.error('Unable to load run %r: %s', loader.subdir, e)\n\n    num_threads = min(self._max_reload_threads, len(self._run_loaders))\n    if num_threads <= 1:\n      logger.info('Importing runs serially on a single thread')\n      for batch in batch_generator():\n        self._event_sink.write_batch(batch)\n    else:\n      output_queue = queue.Queue()\n      sentinel = object()\n      def producer():\n        try:\n          for batch in batch_generator():\n            output_queue.put(batch)\n        finally:\n          output_queue.put(sentinel)\n      logger.info('Starting %d threads to import runs', num_threads)\n      for i in xrange(num_threads):\n        thread = threading.Thread(target=producer, name='Loader %d' % i)\n        thread.daemon = True\n        thread.start()\n      num_live_threads = num_threads\n      while num_live_threads > 0:\n        output = output_queue.get()\n        if output == sentinel:\n          num_live_threads -= 1\n          continue\n        self._event_sink.write_batch(output)\n    for loader in loader_delete_queue:\n      logger.warn('Deleting loader %r', loader.subdir)\n      del self._run_loaders[loader.subdir]\n    logger.info('Finished with DbImportMultiplexer.Reload()')"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a batched event iterator over the run directory event files.", "response": "def load_batches(self):\n    \"\"\"Returns a batched event iterator over the run directory event files.\"\"\"\n    event_iterator = self._directory_watcher.Load()\n    while True:\n      events = []\n      event_bytes = 0\n      start = time.time()\n      for event_proto in event_iterator:\n        events.append(event_proto)\n        event_bytes += len(event_proto)\n        if len(events) >= self._BATCH_COUNT or event_bytes >= self._BATCH_BYTES:\n          break\n      elapsed = time.time() - start\n      logger.debug('RunLoader.load_batch() yielded in %0.3f sec for %s',\n                       elapsed, self._subdir)\n      if not events:\n        return\n      yield _EventBatch(\n          events=events,\n          experiment_name=self._experiment_name,\n          run_name=self._run_name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _process_event(self, event, tagged_data):\n    event_type = event.WhichOneof('what')\n    # Handle the most common case first.\n    if event_type == 'summary':\n      for value in event.summary.value:\n        value = data_compat.migrate_value(value)\n        tag, metadata, values = tagged_data.get(value.tag, (None, None, []))\n        values.append((event.step, event.wall_time, value.tensor))\n        if tag is None:\n          # Store metadata only from the first event.\n          tagged_data[value.tag] = sqlite_writer.TagData(\n              value.tag, value.metadata, values)\n    elif event_type == 'file_version':\n      pass  # TODO: reject file version < 2 (at loader level)\n    elif event_type == 'session_log':\n      if event.session_log.status == event_pb2.SessionLog.START:\n        pass  # TODO: implement purging via sqlite writer truncation method\n    elif event_type in ('graph_def', 'meta_graph_def'):\n      pass  # TODO: support graphs\n    elif event_type == 'tagged_run_metadata':\n      pass", "response": "Processes a single tf. Event and records it in tagged_data."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _buckets(data, bucket_count=None):\n  # TODO(nickfelt): remove on-demand imports once dep situation is fixed.\n  import tensorflow.compat.v1 as tf\n  if bucket_count is None:\n    bucket_count = summary_v2.DEFAULT_BUCKET_COUNT\n  with tf.name_scope('buckets', values=[data, bucket_count]), \\\n       tf.control_dependencies([tf.assert_scalar(bucket_count),\n                                tf.assert_type(bucket_count, tf.int32)]):\n    data = tf.reshape(data, shape=[-1])  # flatten\n    data = tf.cast(data, tf.float64)\n    is_empty = tf.equal(tf.size(input=data), 0)\n\n    def when_empty():\n      return tf.constant([], shape=(0, 3), dtype=tf.float64)\n\n    def when_nonempty():\n      min_ = tf.reduce_min(input_tensor=data)\n      max_ = tf.reduce_max(input_tensor=data)\n      range_ = max_ - min_\n      is_singular = tf.equal(range_, 0)\n\n      def when_nonsingular():\n        bucket_width = range_ / tf.cast(bucket_count, tf.float64)\n        offsets = data - min_\n        bucket_indices = tf.cast(tf.floor(offsets / bucket_width),\n                                 dtype=tf.int32)\n        clamped_indices = tf.minimum(bucket_indices, bucket_count - 1)\n        one_hots = tf.one_hot(clamped_indices, depth=bucket_count)\n        bucket_counts = tf.cast(tf.reduce_sum(input_tensor=one_hots, axis=0),\n                                dtype=tf.float64)\n        edges = tf.linspace(min_, max_, bucket_count + 1)\n        left_edges = edges[:-1]\n        right_edges = edges[1:]\n        return tf.transpose(a=tf.stack(\n            [left_edges, right_edges, bucket_counts]))\n\n      def when_singular():\n        center = min_\n        bucket_starts = tf.stack([center - 0.5])\n        bucket_ends = tf.stack([center + 0.5])\n        bucket_counts = tf.stack([tf.cast(tf.size(input=data), tf.float64)])\n        return tf.transpose(\n            a=tf.stack([bucket_starts, bucket_ends, bucket_counts]))\n\n      return tf.cond(is_singular, when_singular, when_nonsingular)\n\n    return tf.cond(is_empty, when_empty, when_nonempty)", "response": "Create a TensorFlow op to group data into histogram buckets."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a legacy histogram summary op.", "response": "def op(name,\n       data,\n       bucket_count=None,\n       display_name=None,\n       description=None,\n       collections=None):\n  \"\"\"Create a legacy histogram summary op.\n\n  Arguments:\n    name: A unique name for the generated summary node.\n    data: A `Tensor` of any shape. Must be castable to `float64`.\n    bucket_count: Optional positive `int`. The output will have this\n      many buckets, except in two edge cases. If there is no data, then\n      there are no buckets. If there is data but all points have the\n      same value, then there is one bucket whose left and right\n      endpoints are the same.\n    display_name: Optional name for this summary in TensorBoard, as a\n      constant `str`. Defaults to `name`.\n    description: Optional long-form description for this summary, as a\n      constant `str`. Markdown is supported. Defaults to empty.\n    collections: Optional list of graph collections keys. The new\n      summary op is added to these collections. Defaults to\n      `[Graph Keys.SUMMARIES]`.\n\n  Returns:\n    A TensorFlow summary op.\n  \"\"\"\n  # TODO(nickfelt): remove on-demand imports once dep situation is fixed.\n  import tensorflow.compat.v1 as tf\n\n  if display_name is None:\n    display_name = name\n  summary_metadata = metadata.create_summary_metadata(\n      display_name=display_name, description=description)\n  with tf.name_scope(name):\n    tensor = _buckets(data, bucket_count=bucket_count)\n    return tf.summary.tensor_summary(name='histogram_summary',\n                                     tensor=tensor,\n                                     collections=collections,\n                                     summary_metadata=summary_metadata)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pb(name, data, bucket_count=None, display_name=None, description=None):\n  # TODO(nickfelt): remove on-demand imports once dep situation is fixed.\n  import tensorflow.compat.v1 as tf\n\n  if bucket_count is None:\n    bucket_count = summary_v2.DEFAULT_BUCKET_COUNT\n  data = np.array(data).flatten().astype(float)\n  if data.size == 0:\n    buckets = np.array([]).reshape((0, 3))\n  else:\n    min_ = np.min(data)\n    max_ = np.max(data)\n    range_ = max_ - min_\n    if range_ == 0:\n      center = min_\n      buckets = np.array([[center - 0.5, center + 0.5, float(data.size)]])\n    else:\n      bucket_width = range_ / bucket_count\n      offsets = data - min_\n      bucket_indices = np.floor(offsets / bucket_width).astype(int)\n      clamped_indices = np.minimum(bucket_indices, bucket_count - 1)\n      one_hots = (np.array([clamped_indices]).transpose()\n                  == np.arange(0, bucket_count))  # broadcast\n      assert one_hots.shape == (data.size, bucket_count), (\n          one_hots.shape, (data.size, bucket_count))\n      bucket_counts = np.sum(one_hots, axis=0)\n      edges = np.linspace(min_, max_, bucket_count + 1)\n      left_edges = edges[:-1]\n      right_edges = edges[1:]\n      buckets = np.array([left_edges, right_edges, bucket_counts]).transpose()\n  tensor = tf.make_tensor_proto(buckets, dtype=tf.float64)\n\n  if display_name is None:\n    display_name = name\n  summary_metadata = metadata.create_summary_metadata(\n      display_name=display_name, description=description)\n  tf_summary_metadata = tf.SummaryMetadata.FromString(\n      summary_metadata.SerializeToString())\n\n  summary = tf.Summary()\n  summary.value.add(tag='%s/histogram_summary' % name,\n                    metadata=tf_summary_metadata,\n                    tensor=tensor)\n  return summary", "response": "Create a legacy histogram summary protobuf."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a tensor to the watch store.", "response": "def add(self, value):\n    \"\"\"Add a tensor the watch store.\"\"\"\n    if self._disposed:\n      raise ValueError(\n          'Cannot add value: this _WatchStore instance is already disposed')\n    self._data.append(value)\n    if hasattr(value, 'nbytes'):\n      self._in_mem_bytes += value.nbytes\n      self._ensure_bytes_limits()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets number of values in memory.", "response": "def num_in_memory(self):\n    \"\"\"Get number of values in memory.\"\"\"\n    n = len(self._data) - 1\n    while n >= 0:\n      if isinstance(self._data[n], _TensorValueDiscarded):\n        break\n      n -= 1\n    return len(self._data) - 1 - n"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef num_discarded(self):\n    if not self._data:\n      return 0\n    n = 0\n    while n < len(self._data):\n      if not isinstance(self._data[n], _TensorValueDiscarded):\n        break\n      n += 1\n    return n", "response": "Get the number of values discarded due to exceeding both limits."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef query(self, time_indices):\n    if self._disposed:\n      raise ValueError(\n          'Cannot query: this _WatchStore instance is already disposed')\n    if not isinstance(time_indices, (tuple, list)):\n      time_indices = [time_indices]\n    output = []\n    for time_index in time_indices:\n      if isinstance(self._data[time_index], _TensorValueDiscarded):\n        output.append(None)\n      else:\n        data_item = self._data[time_index]\n        if (hasattr(data_item, 'dtype') and\n            tensor_helper.translate_dtype(data_item.dtype) == 'string'):\n          _, _, data_item = tensor_helper.array_view(data_item)\n          data_item = np.array(\n              tensor_helper.process_buffers_for_display(data_item),\n              dtype=np.object)\n        output.append(data_item)\n\n    return output", "response": "Query the values at given time indices."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add(self, watch_key, tensor_value):\n    if watch_key not in self._tensor_data:\n      self._tensor_data[watch_key] = _WatchStore(\n          watch_key,\n          mem_bytes_limit=self._watch_mem_bytes_limit)\n    self._tensor_data[watch_key].add(tensor_value)", "response": "Adds a tensor value to the internal watch store."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef query(self,\n            watch_key,\n            time_indices=None,\n            slicing=None,\n            mapping=None):\n    \"\"\"Query tensor store for a given watch_key.\n\n    Args:\n      watch_key: The watch key to query.\n      time_indices: A numpy-style slicing string for time indices. E.g.,\n        `-1`, `:-2`, `[::2]`. If not provided (`None`), will use -1.\n      slicing: A numpy-style slicing string for individual time steps.\n      mapping: An mapping string or a list of them. Supported mappings:\n        `{None, 'image/png', 'health-pill'}`.\n\n    Returns:\n      The potentially sliced values as a nested list of values or its mapped\n        format. A `list` of nested `list` of values.\n\n    Raises:\n      ValueError: If the shape of the sliced array is incompatible with mapping\n        mode. Or if the mapping type is invalid.\n    \"\"\"\n    if watch_key not in self._tensor_data:\n      raise KeyError(\"watch_key not found: %s\" % watch_key)\n\n    if time_indices is None:\n      time_indices = '-1'\n    time_slicing = tensor_helper.parse_time_indices(time_indices)\n    all_time_indices = list(range(self._tensor_data[watch_key].num_total()))\n    sliced_time_indices = all_time_indices[time_slicing]\n    if not isinstance(sliced_time_indices, list):\n      sliced_time_indices = [sliced_time_indices]\n\n    recombine_and_map = False\n    step_mapping = mapping\n    if len(sliced_time_indices) > 1 and mapping not in (None, ):\n      recombine_and_map = True\n      step_mapping = None\n\n    output = []\n    for index in sliced_time_indices:\n      value = self._tensor_data[watch_key].query(index)[0]\n      if (value is not None and\n          not isinstance(value, debug_data.InconvertibleTensorProto)):\n        output.append(tensor_helper.array_view(\n            value, slicing=slicing, mapping=step_mapping)[2])\n      else:\n        output.append(None)\n\n    if recombine_and_map:\n      if mapping == 'image/png':\n        output = tensor_helper.array_to_base64_png(output)\n      elif mapping and mapping != 'none':\n        logger.warn(\n            'Unsupported mapping mode after recomining time steps: %s',\n            mapping)\n    return output", "response": "Query the tensor store for a given watch key."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nstarting listening on the given gRPC port.", "response": "def listen(self, grpc_port):\n    \"\"\"Start listening on the given gRPC port.\n\n    This method of an instance of DebuggerPlugin can be invoked at most once.\n    This method is not thread safe.\n\n    Args:\n      grpc_port: port number to listen at.\n\n    Raises:\n      ValueError: If this instance is already listening at a gRPC port.\n    \"\"\"\n    if self._grpc_port:\n      raise ValueError(\n          \"This DebuggerPlugin instance is already listening at gRPC port %d\" %\n          self._grpc_port)\n    self._grpc_port = grpc_port\n\n    sys.stderr.write('Creating DebuggerDataServer at port %d and logdir %s\\n' %\n                     (self._grpc_port, self._logdir))\n    sys.stderr.flush()\n    self._debugger_data_server = debugger_server_lib.DebuggerDataServer(\n        self._grpc_port, self._logdir)\n\n    threading.Thread(target=self._debugger_data_server.\n                     start_the_debugger_data_receiving_server).start()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_active(self):\n    return bool(\n        self._grpc_port is not None and\n        self._event_multiplexer and\n        self._event_multiplexer.PluginRunToTagToContent(\n            constants.DEBUGGER_PLUGIN_NAME))", "response": "Determines whether this plugin is active."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving the health pills for a run sampled by the event multiplexer.", "response": "def _obtain_sampled_health_pills(self, run, node_names):\n    \"\"\"Obtains the health pills for a run sampled by the event multiplexer.\n\n    This is much faster than the alternative path of reading health pills from\n    disk.\n\n    Args:\n      run: The run to fetch health pills for.\n      node_names: A list of node names for which to retrieve health pills.\n\n    Returns:\n      A dictionary mapping from node name to a list of\n      event_accumulator.HealthPillEvents.\n    \"\"\"\n    runs_to_tags_to_content = self._event_multiplexer.PluginRunToTagToContent(\n        constants.DEBUGGER_PLUGIN_NAME)\n\n    if run not in runs_to_tags_to_content:\n      # The run lacks health pills.\n      return {}\n\n    # This is also a mapping between node name and plugin content because this\n    # plugin tags by node name.\n    tags_to_content = runs_to_tags_to_content[run]\n\n    mapping = {}\n    for node_name in node_names:\n      if node_name not in tags_to_content:\n        # This node lacks health pill data.\n        continue\n\n      health_pills = []\n      for tensor_event in self._event_multiplexer.Tensors(run, node_name):\n        json_string = tags_to_content[node_name]\n        try:\n          content_object = json.loads(tf.compat.as_text(json_string))\n          device_name = content_object['device']\n          output_slot = content_object['outputSlot']\n          health_pills.append(\n              self._tensor_proto_to_health_pill(tensor_event, node_name,\n                                                device_name, output_slot))\n        except (KeyError, ValueError) as e:\n          logger.error('Could not determine device from JSON string '\n                           '%r: %r', json_string, e)\n\n      mapping[node_name] = health_pills\n\n    return mapping"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts a TensorEvent to a HealthPillEvent.", "response": "def _tensor_proto_to_health_pill(self, tensor_event, node_name, device,\n                                   output_slot):\n    \"\"\"Converts an event_accumulator.TensorEvent to a HealthPillEvent.\n\n    Args:\n      tensor_event: The event_accumulator.TensorEvent to convert.\n      node_name: The name of the node (without the output slot).\n      device: The device.\n      output_slot: The integer output slot this health pill is relevant to.\n\n    Returns:\n      A HealthPillEvent.\n    \"\"\"\n    return self._process_health_pill_value(\n        wall_time=tensor_event.wall_time,\n        step=tensor_event.step,\n        device_name=device,\n        output_slot=output_slot,\n        node_name=node_name,\n        tensor_proto=tensor_event.tensor_proto)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _obtain_health_pills_at_step(self, events_directory, node_names, step):\n    # Obtain all files with debugger-related events.\n    pattern = os.path.join(events_directory, _DEBUGGER_EVENTS_GLOB_PATTERN)\n    file_paths = glob.glob(pattern)\n\n    if not file_paths:\n      raise IOError(\n          'No events files found that matches the pattern %r.' % pattern)\n\n    # Sort by name (and thus by timestamp).\n    file_paths.sort()\n\n    mapping = collections.defaultdict(list)\n    node_name_set = frozenset(node_names)\n\n    for file_path in file_paths:\n      should_stop = self._process_health_pill_event(\n          node_name_set, mapping, step, file_path)\n      if should_stop:\n        break\n\n    return mapping", "response": "Reads disk to obtain the health pills for a particular step."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _process_health_pill_event(self, node_name_set, mapping, target_step,\n                                 file_path):\n    \"\"\"Creates health pills out of data in an event.\n\n    Creates health pills out of the event and adds them to the mapping.\n\n    Args:\n      node_name_set: A set of node names that are relevant.\n      mapping: The mapping from node name to HealthPillEvents.\n          This object may be destructively modified.\n      target_step: The target step at which to obtain health pills.\n      file_path: The path to the file with health pill events.\n\n    Returns:\n      Whether we should stop reading events because future events are no longer\n      relevant.\n    \"\"\"\n    events_loader = event_file_loader.EventFileLoader(file_path)\n    for event in events_loader.Load():\n      if not event.HasField('summary'):\n        logger.warn(\n            'An event in a debugger events file lacks a summary.')\n        continue\n\n      if event.step < target_step:\n        # This event is not of the relevant step. We perform this check\n        # first because the majority of events will be eliminated from\n        # consideration by this check.\n        continue\n\n      if event.step > target_step:\n        # We have passed the relevant step. No need to read more events.\n        return True\n\n      for value in event.summary.value:\n        # Obtain the device name from the metadata.\n        summary_metadata = value.metadata\n        plugin_data = summary_metadata.plugin_data\n        if plugin_data.plugin_name == constants.DEBUGGER_PLUGIN_NAME:\n          try:\n            content = json.loads(\n                tf.compat.as_text(summary_metadata.plugin_data.content))\n          except ValueError as err:\n            logger.warn(\n                'Could not parse the JSON string containing data for '\n                'the debugger plugin: %r, %r', content, err)\n            continue\n          device_name = content['device']\n          output_slot = content['outputSlot']\n        else:\n          logger.error(\n              'No debugger plugin data found for event with tag %s and node '\n              'name %s.', value.tag, value.node_name)\n          continue\n\n        if not value.HasField('tensor'):\n          logger.warn(\n              'An event in a debugger events file lacks a tensor value.')\n          continue\n\n        match = re.match(r'^(.*):(\\d+):DebugNumericSummary$', value.node_name)\n        if not match:\n          logger.warn(\n              ('A event with a health pill has an invalid watch, (i.e., an '\n               'unexpected debug op): %r'), value.node_name)\n          return None\n\n        health_pill = self._process_health_pill_value(\n            wall_time=event.wall_time,\n            step=event.step,\n            device_name=device_name,\n            output_slot=output_slot,\n            node_name=match.group(1),\n            tensor_proto=value.tensor,\n            node_name_set=node_name_set)\n        if not health_pill:\n          continue\n        mapping[health_pill.node_name].append(health_pill)\n\n    # Keep reading events.\n    return False", "response": "Processes a health pill event."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprocess the health pill value.", "response": "def _process_health_pill_value(self,\n                                 wall_time,\n                                 step,\n                                 device_name,\n                                 output_slot,\n                                 node_name,\n                                 tensor_proto,\n                                 node_name_set=None):\n    \"\"\"Creates a HealthPillEvent containing various properties of a health pill.\n\n    Args:\n      wall_time: The wall time in seconds.\n      step: The session run step of the event.\n      device_name: The name of the node's device.\n      output_slot: The numeric output slot.\n      node_name: The name of the node (without the output slot).\n      tensor_proto: A tensor proto of data.\n      node_name_set: An optional set of node names that are relevant. If not\n        provided, no filtering by relevance occurs.\n\n    Returns:\n      An event_accumulator.HealthPillEvent. Or None if one could not be created.\n    \"\"\"\n    if node_name_set and node_name not in node_name_set:\n      # This event is not relevant.\n      return None\n\n    # Since we seek health pills for a specific step, this function\n    # returns 1 health pill per node per step. The wall time is the\n    # seconds since the epoch.\n    elements = list(tensor_util.make_ndarray(tensor_proto))\n    return HealthPillEvent(\n        wall_time=wall_time,\n        step=step,\n        device_name=device_name,\n        output_slot=output_slot,\n        node_name=node_name,\n        dtype=repr(tf.as_dtype(elements[12])),\n        shape=elements[14:],\n        value=elements)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _info_to_string(info):\n  for key in _TENSORBOARD_INFO_FIELDS:\n    field_type = _TENSORBOARD_INFO_FIELDS[key]\n    if not isinstance(getattr(info, key), field_type.runtime_type):\n      raise ValueError(\n          \"expected %r of type %s, but found: %r\" %\n          (key, field_type.runtime_type, getattr(info, key))\n      )\n  if info.version != version.VERSION:\n    raise ValueError(\n        \"expected 'version' to be %r, but found: %r\" %\n        (version.VERSION, info.version)\n    )\n  json_value = {\n      k: _TENSORBOARD_INFO_FIELDS[k].serialize(getattr(info, k))\n      for k in _TENSORBOARD_INFO_FIELDS\n  }\n  return json.dumps(json_value, sort_keys=True, indent=4)", "response": "Convert a TensorBoardInfo object to string form."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _info_from_string(info_string):\n\n  try:\n    json_value = json.loads(info_string)\n  except ValueError:\n    raise ValueError(\"invalid JSON: %r\" % (info_string,))\n  if not isinstance(json_value, dict):\n    raise ValueError(\"not a JSON object: %r\" % (json_value,))\n  if json_value.get(\"version\") != version.VERSION:\n    raise ValueError(\"incompatible version: %r\" % (json_value,))\n  expected_keys = frozenset(_TENSORBOARD_INFO_FIELDS)\n  actual_keys = frozenset(json_value)\n  if expected_keys != actual_keys:\n    raise ValueError(\n        \"bad keys on TensorBoardInfo (missing: %s; extraneous: %s)\"\n        % (expected_keys - actual_keys, actual_keys - expected_keys)\n    )\n\n  # Validate and deserialize fields.\n  for key in _TENSORBOARD_INFO_FIELDS:\n    field_type = _TENSORBOARD_INFO_FIELDS[key]\n    if not isinstance(json_value[key], field_type.serialized_type):\n      raise ValueError(\n          \"expected %r of type %s, but found: %r\" %\n          (key, field_type.serialized_type, json_value[key])\n      )\n    json_value[key] = field_type.deserialize(json_value[key])\n\n  return TensorBoardInfo(**json_value)", "response": "Parse a TensorBoardInfo object from its string representation."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cache_key(working_directory, arguments, configure_kwargs):\n  if not isinstance(arguments, (list, tuple)):\n    raise TypeError(\n        \"'arguments' should be a list of arguments, but found: %r \"\n        \"(use `shlex.split` if given a string)\"\n        % (arguments,)\n    )\n  datum = {\n      \"working_directory\": working_directory,\n      \"arguments\": arguments,\n      \"configure_kwargs\": configure_kwargs,\n  }\n  raw = base64.b64encode(\n      json.dumps(datum, sort_keys=True, separators=(\",\", \":\")).encode(\"utf-8\")\n  )\n  # `raw` is of type `bytes`, even though it only contains ASCII\n  # characters; we want it to be `str` in both Python 2 and 3.\n  return str(raw.decode(\"ascii\"))", "response": "Compute a TensorBoardInfo. cache_key field."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting path to directory where info files are stored.", "response": "def _get_info_dir():\n  \"\"\"Get path to directory in which to store info files.\n\n  The directory returned by this function is \"owned\" by this module. If\n  the contents of the directory are modified other than via the public\n  functions of this module, subsequent behavior is undefined.\n\n  The directory will be created if it does not exist.\n  \"\"\"\n  path = os.path.join(tempfile.gettempdir(), \".tensorboard-info\")\n  try:\n    os.makedirs(path)\n  except OSError as e:\n    if e.errno == errno.EEXIST and os.path.isdir(path):\n      pass\n    else:\n      raise\n  else:\n    os.chmod(path, 0o777)\n  return path"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrite TensorBoardInfo to the current process s info file.", "response": "def write_info_file(tensorboard_info):\n  \"\"\"Write TensorBoardInfo to the current process's info file.\n\n  This should be called by `main` once the server is ready. When the\n  server shuts down, `remove_info_file` should be called.\n\n  Args:\n    tensorboard_info: A valid `TensorBoardInfo` object.\n\n  Raises:\n    ValueError: If any field on `info` is not of the correct type.\n  \"\"\"\n  payload = \"%s\\n\" % _info_to_string(tensorboard_info)\n  with open(_get_info_file_path(), \"w\") as outfile:\n    outfile.write(payload)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef remove_info_file():\n  try:\n    os.unlink(_get_info_file_path())\n  except OSError as e:\n    if e.errno == errno.ENOENT:\n      # The user may have wiped their temporary directory or something.\n      # Not a problem: we're already in the state that we want to be in.\n      pass\n    else:\n      raise", "response": "Remove the current process s TensorBoardInfo file if it exists."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list of TensorBoardInfo objects for all running TensorBoard processes.", "response": "def get_all():\n  \"\"\"Return TensorBoardInfo values for running TensorBoard processes.\n\n  This function may not provide a perfect snapshot of the set of running\n  processes. Its result set may be incomplete if the user has cleaned\n  their /tmp/ directory while TensorBoard processes are running. It may\n  contain extraneous entries if TensorBoard processes exited uncleanly\n  (e.g., with SIGKILL or SIGQUIT).\n\n  Returns:\n    A fresh list of `TensorBoardInfo` objects.\n  \"\"\"\n  info_dir = _get_info_dir()\n  results = []\n  for filename in os.listdir(info_dir):\n    filepath = os.path.join(info_dir, filename)\n    try:\n      with open(filepath) as infile:\n        contents = infile.read()\n    except IOError as e:\n      if e.errno == errno.EACCES:\n        # May have been written by this module in a process whose\n        # `umask` includes some bits of 0o444.\n        continue\n      else:\n        raise\n    try:\n      info = _info_from_string(contents)\n    except ValueError:\n      tb_logging.get_logger().warning(\n          \"invalid info file: %r\",\n          filepath,\n          exc_info=True,\n      )\n    else:\n      results.append(info)\n  return results"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nstarting a new TensorBoard process or reuse a compatible one.", "response": "def start(arguments, timeout=datetime.timedelta(seconds=60)):\n  \"\"\"Start a new TensorBoard instance, or reuse a compatible one.\n\n  If the cache key determined by the provided arguments and the current\n  working directory (see `cache_key`) matches the cache key of a running\n  TensorBoard process (see `get_all`), that process will be reused.\n\n  Otherwise, a new TensorBoard process will be spawned with the provided\n  arguments, using the `tensorboard` binary from the system path.\n\n  Args:\n    arguments: List of strings to be passed as arguments to\n      `tensorboard`. (If you have a raw command-line string, see\n      `shlex.split`.)\n    timeout: `datetime.timedelta` object describing how long to wait for\n      the subprocess to initialize a TensorBoard server and write its\n      `TensorBoardInfo` file. If the info file is not written within\n      this time period, `start` will assume that the subprocess is stuck\n      in a bad state, and will give up on waiting for it and return a\n      `StartTimedOut` result. Note that in such a case the subprocess\n      will not be killed. Default value is 60 seconds.\n\n  Returns:\n    A `StartReused`, `StartLaunched`, `StartFailed`, or `StartTimedOut`\n    object.\n  \"\"\"\n  match = _find_matching_instance(\n      cache_key(\n          working_directory=os.getcwd(),\n          arguments=arguments,\n          configure_kwargs={},\n      ),\n  )\n  if match:\n    return StartReused(info=match)\n\n  (stdout_fd, stdout_path) = tempfile.mkstemp(prefix=\".tensorboard-stdout-\")\n  (stderr_fd, stderr_path) = tempfile.mkstemp(prefix=\".tensorboard-stderr-\")\n  start_time_seconds = time.time()\n  try:\n    p = subprocess.Popen(\n        [\"tensorboard\"] + arguments,\n        stdout=stdout_fd,\n        stderr=stderr_fd,\n    )\n  finally:\n    os.close(stdout_fd)\n    os.close(stderr_fd)\n\n  poll_interval_seconds = 0.5\n  end_time_seconds = start_time_seconds + timeout.total_seconds()\n  while time.time() < end_time_seconds:\n    time.sleep(poll_interval_seconds)\n    subprocess_result = p.poll()\n    if subprocess_result is not None:\n      return StartFailed(\n          exit_code=subprocess_result,\n          stdout=_maybe_read_file(stdout_path),\n          stderr=_maybe_read_file(stderr_path),\n      )\n    for info in get_all():\n      if info.pid == p.pid and info.start_time >= start_time_seconds:\n        return StartLaunched(info=info)\n  else:\n    return StartTimedOut(pid=p.pid)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _find_matching_instance(cache_key):\n  infos = get_all()\n  candidates = [info for info in infos if info.cache_key == cache_key]\n  for candidate in sorted(candidates, key=lambda x: x.port):\n    # TODO(@wchargin): Check here that the provided port is still live.\n    return candidate\n  return None", "response": "Find a running TensorBoard instance compatible with the cache key."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread the given file if it exists.", "response": "def _maybe_read_file(filename):\n  \"\"\"Read the given file, if it exists.\n\n  Args:\n    filename: A path to a file.\n\n  Returns:\n    A string containing the file contents, or `None` if the file does\n    not exist.\n  \"\"\"\n  try:\n    with open(filename) as infile:\n      return infile.read()\n  except IOError as e:\n    if e.errno == errno.ENOENT:\n      return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef process_raw_trace(raw_trace):\n  trace = trace_events_pb2.Trace()\n  trace.ParseFromString(raw_trace)\n  return ''.join(trace_events_json.TraceEventsJsonStream(trace))", "response": "Processes raw trace data and returns the UI data."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndetermines if this plugin is active and has any profile data to show.", "response": "def is_active(self):\n    \"\"\"Whether this plugin is active and has any profile data to show.\n\n    Detecting profile data is expensive, so this process runs asynchronously\n    and the value reported by this method is the cached value and may be stale.\n\n    Returns:\n      Whether any run has profile data.\n    \"\"\"\n    # If we are already active, we remain active and don't recompute this.\n    # Otherwise, try to acquire the lock without blocking; if we get it and\n    # we're still not active, launch a thread to check if we're active and\n    # release the lock once the computation is finished. Either way, this\n    # thread returns the current cached value to avoid blocking.\n    if not self._is_active and self._is_active_lock.acquire(False):\n      if self._is_active:\n        self._is_active_lock.release()\n      else:\n        def compute_is_active():\n          self._is_active = any(self.generate_run_to_tools())\n          self._is_active_lock.release()\n        new_thread = threading.Thread(\n            target=compute_is_active,\n            name='ProfilePluginIsActiveThread')\n        new_thread.start()\n    return self._is_active"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate a single run for each profile.", "response": "def generate_run_to_tools(self):\n    \"\"\"Generator for pairs of \"run name\" and a list of tools for that run.\n\n    The \"run name\" here is a \"frontend run name\" - see _run_dir() for the\n    definition of a \"frontend run name\" and how it maps to a directory of\n    profile data for a specific profile \"run\". The profile plugin concept of\n    \"run\" is different from the normal TensorBoard run; each run in this case\n    represents a single instance of profile data collection, more similar to a\n    \"step\" of data in typical TensorBoard semantics. These runs reside in\n    subdirectories of the plugins/profile directory within any regular\n    TensorBoard run directory (defined as a subdirectory of the logdir that\n    contains at least one tfevents file) or within the logdir root directory\n    itself (even if it contains no tfevents file and would thus not be\n    considered a normal TensorBoard run, for backwards compatibility).\n\n    Within those \"profile run directories\", there are files in the directory\n    that correspond to different profiling tools. The file that contains profile\n    for a specific tool \"x\" will have a suffix name TOOLS[\"x\"].\n\n    Example:\n      logs/\n        plugins/\n          profile/\n            run1/\n              hostA.trace\n        train/\n          events.out.tfevents.foo\n          plugins/\n            profile/\n              run1/\n                hostA.trace\n                hostB.trace\n              run2/\n                hostA.trace\n        validation/\n          events.out.tfevents.foo\n          plugins/\n            profile/\n              run1/\n                hostA.trace\n\n    Yields:\n      A sequence of tuples mapping \"frontend run names\" to lists of tool names\n      available for those runs. For the above example, this would be:\n\n          (\"run1\", [\"trace_viewer\"])\n          (\"train/run1\", [\"trace_viewer\"])\n          (\"train/run2\", [\"trace_viewer\"])\n          (\"validation/run1\", [\"trace_viewer\"])\n    \"\"\"\n    self.start_grpc_stub_if_necessary()\n\n    plugin_assets = self.multiplexer.PluginAssets(PLUGIN_NAME)\n    tb_run_names_to_dirs = self.multiplexer.RunPaths()\n\n    # Ensure that we also check the root logdir, even if it isn't a recognized\n    # TensorBoard run (i.e. has no tfevents file directly under it), to remain\n    # backwards compatible with previously profile plugin behavior. Note that we\n    # check if logdir is a directory to handle case where it's actually a\n    # multipart directory spec, which this plugin does not support.\n    if '.' not in plugin_assets and tf.io.gfile.isdir(self.logdir):\n      tb_run_names_to_dirs['.'] = self.logdir\n      plugin_assets['.'] = plugin_asset_util.ListAssets(\n          self.logdir, PLUGIN_NAME)\n\n    for tb_run_name, profile_runs in six.iteritems(plugin_assets):\n      tb_run_dir = tb_run_names_to_dirs[tb_run_name]\n      tb_plugin_dir = plugin_asset_util.PluginDirectory(\n          tb_run_dir, PLUGIN_NAME)\n      for profile_run in profile_runs:\n        # Remove trailing slash; some filesystem implementations emit this.\n        profile_run = profile_run.rstrip('/')\n        if tb_run_name == '.':\n          frontend_run = profile_run\n        else:\n          frontend_run = '/'.join([tb_run_name, profile_run])\n        profile_run_dir = os.path.join(tb_plugin_dir, profile_run)\n        if tf.io.gfile.isdir(profile_run_dir):\n          yield frontend_run, self._get_active_tools(profile_run_dir)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef host_impl(self, run, tool):\n    hosts = {}\n    run_dir = self._run_dir(run)\n    if not run_dir:\n      logger.warn(\"Cannot find asset directory for: %s\", run)\n      return hosts\n    tool_pattern = '*' + TOOLS[tool]\n    try:\n      files = tf.io.gfile.glob(os.path.join(run_dir, tool_pattern))\n      hosts = [os.path.basename(f).replace(TOOLS[tool], '') for f in files]\n    except tf.errors.OpError as e:\n      logger.warn(\"Cannot read asset directory: %s, OpError %s\",\n                      run_dir, e)\n    return hosts", "response": "Returns the list of hosts for the run and tool."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef data_impl(self, request):\n    run = request.args.get('run')\n    tool = request.args.get('tag')\n    host = request.args.get('host')\n    run_dir = self._run_dir(run)\n    # Profile plugin \"run\" is the last component of run dir.\n    profile_run = os.path.basename(run_dir)\n\n    if tool not in TOOLS:\n      return None\n\n    self.start_grpc_stub_if_necessary()\n    if tool == 'trace_viewer@' and self.stub is not None:\n      from tensorflow.contrib.tpu.profiler import tpu_profiler_analysis_pb2\n      grpc_request = tpu_profiler_analysis_pb2.ProfileSessionDataRequest()\n      grpc_request.repository_root = run_dir\n      grpc_request.session_id = profile_run[:-1]\n      grpc_request.tool_name = 'trace_viewer'\n      # Remove the trailing dot if present\n      grpc_request.host_name = host.rstrip('.')\n\n      grpc_request.parameters['resolution'] = request.args.get('resolution')\n      if request.args.get('start_time_ms') is not None:\n        grpc_request.parameters['start_time_ms'] = request.args.get(\n            'start_time_ms')\n      if request.args.get('end_time_ms') is not None:\n        grpc_request.parameters['end_time_ms'] = request.args.get('end_time_ms')\n      grpc_response = self.stub.GetSessionToolData(grpc_request)\n      return grpc_response.output\n\n    if tool not in TOOLS:\n      return None\n    tool_name = str(host) + TOOLS[tool]\n    asset_path = os.path.join(run_dir, tool_name)\n    raw_data = None\n    try:\n      with tf.io.gfile.GFile(asset_path, 'rb') as f:\n        raw_data = f.read()\n    except tf.errors.NotFoundError:\n      logger.warn('Asset path %s not found', asset_path)\n    except tf.errors.OpError as e:\n      logger.warn(\"Couldn't read asset path: %s, OpError %s\", asset_path, e)\n\n    if raw_data is None:\n      return None\n    if tool == 'trace_viewer':\n      return process_raw_trace(raw_data)\n    if tool in _RAW_DATA_TOOLS:\n      return raw_data\n    return None", "response": "Retrieves and processes the tool data for a run and a host."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrun a temperature simulation. This will simulate an object at temperature `initial_temperature` sitting at rest in a large room at temperature `ambient_temperature`. The object has some intrinsic `heat_coefficient`, which indicates how much thermal conductivity it has: for instance, metals have high thermal conductivity, while the thermal conductivity of water is low. Over time, the object's temperature will adjust to match the temperature of its environment. We'll track the object's temperature, how far it is from the room's temperature, and how much it changes at each time step. Arguments: logdir: the top-level directory into which to write summary data run_name: the name of this run; will be created as a subdirectory under logdir initial_temperature: float; the object's initial temperature ambient_temperature: float; the temperature of the enclosing room heat_coefficient: float; a measure of the object's thermal conductivity", "response": "def run(logdir, run_name,\n        initial_temperature, ambient_temperature, heat_coefficient):\n  \"\"\"Run a temperature simulation.\n\n  This will simulate an object at temperature `initial_temperature`\n  sitting at rest in a large room at temperature `ambient_temperature`.\n  The object has some intrinsic `heat_coefficient`, which indicates\n  how much thermal conductivity it has: for instance, metals have high\n  thermal conductivity, while the thermal conductivity of water is low.\n\n  Over time, the object's temperature will adjust to match the\n  temperature of its environment. We'll track the object's temperature,\n  how far it is from the room's temperature, and how much it changes at\n  each time step.\n\n  Arguments:\n    logdir: the top-level directory into which to write summary data\n    run_name: the name of this run; will be created as a subdirectory\n      under logdir\n    initial_temperature: float; the object's initial temperature\n    ambient_temperature: float; the temperature of the enclosing room\n    heat_coefficient: float; a measure of the object's thermal\n      conductivity\n  \"\"\"\n  tf.compat.v1.reset_default_graph()\n  tf.compat.v1.set_random_seed(0)\n\n  with tf.name_scope('temperature'):\n    # Create a mutable variable to hold the object's temperature, and\n    # create a scalar summary to track its value over time. The name of\n    # the summary will appear as \"temperature/current\" due to the\n    # name-scope above.\n    temperature = tf.Variable(tf.constant(initial_temperature),\n                              name='temperature')\n    summary.op('current', temperature,\n               display_name='Temperature',\n               description='The temperature of the object under '\n               'simulation, in Kelvins.')\n\n    # Compute how much the object's temperature differs from that of its\n    # environment, and track this, too: likewise, as\n    # \"temperature/difference_to_ambient\".\n    ambient_difference = temperature - ambient_temperature\n    summary.op('difference_to_ambient', ambient_difference,\n               display_name='Difference to ambient temperature',\n               description='The difference between the ambient '\n                           'temperature and the temperature of the '\n                           'object under simulation, in Kelvins.')\n\n  # Newton suggested that the rate of change of the temperature of an\n  # object is directly proportional to this `ambient_difference` above,\n  # where the proportionality constant is what we called the heat\n  # coefficient. But in real life, not everything is quite so clean, so\n  # we'll add in some noise. (The value of 50 is arbitrary, chosen to\n  # make the data look somewhat interesting. :-) )\n  noise = 50 * tf.random.normal([])\n  delta = -heat_coefficient * (ambient_difference + noise)\n  summary.op('delta', delta,\n             description='The change in temperature from the previous '\n                         'step, in Kelvins.')\n\n  # Collect all the scalars that we want to keep track of.\n  summ = tf.compat.v1.summary.merge_all()\n\n  # Now, augment the current temperature by this delta that we computed,\n  # blocking the assignment on summary collection to avoid race conditions\n  # and ensure that the summary always reports the pre-update value.\n  with tf.control_dependencies([summ]):\n    update_step = temperature.assign_add(delta)\n\n  sess = tf.compat.v1.Session()\n  writer = tf.summary.FileWriter(os.path.join(logdir, run_name))\n  writer.add_graph(sess.graph)\n  sess.run(tf.compat.v1.global_variables_initializer())\n  for step in xrange(STEPS):\n    # By asking TensorFlow to compute the update step, we force it to\n    # change the value of the temperature variable. We don't actually\n    # care about this value, so we discard it; instead, we grab the\n    # summary data computed along the way.\n    (s, _) = sess.run([summ, update_step])\n    writer.add_summary(s, global_step=step)\n  writer.close()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run_all(logdir, verbose=False):\n  for initial_temperature in [270.0, 310.0, 350.0]:\n    for final_temperature in [270.0, 310.0, 350.0]:\n      for heat_coefficient in [0.001, 0.005]:\n        run_name = 'temperature:t0=%g,tA=%g,kH=%g' % (\n            initial_temperature, final_temperature, heat_coefficient)\n        if verbose:\n          print('--- Running: %s' % run_name)\n        run(logdir, run_name,\n            initial_temperature, final_temperature, heat_coefficient)", "response": "Runs all the simulations on a reasonable set of parameters."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef Cleanse(obj, encoding='utf-8'):\n  if isinstance(obj, int):\n    return obj\n  elif isinstance(obj, float):\n    if obj == _INFINITY:\n      return 'Infinity'\n    elif obj == _NEGATIVE_INFINITY:\n      return '-Infinity'\n    elif math.isnan(obj):\n      return 'NaN'\n    else:\n      return obj\n  elif isinstance(obj, bytes):\n    return tf.compat.as_text(obj, encoding)\n  elif isinstance(obj, (list, tuple)):\n    return [Cleanse(i, encoding) for i in obj]\n  elif isinstance(obj, set):\n    return [Cleanse(i, encoding) for i in sorted(obj)]\n  elif isinstance(obj, dict):\n    return {Cleanse(k, encoding): Cleanse(v, encoding) for k, v in obj.items()}\n  else:\n    return obj", "response": "Makes Python object appropriate for JSON serialization."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a legacy text summary op.", "response": "def op(name,\n       data,\n       display_name=None,\n       description=None,\n       collections=None):\n  \"\"\"Create a legacy text summary op.\n\n  Text data summarized via this plugin will be visible in the Text Dashboard\n  in TensorBoard. The standard TensorBoard Text Dashboard will render markdown\n  in the strings, and will automatically organize 1D and 2D tensors into tables.\n  If a tensor with more than 2 dimensions is provided, a 2D subarray will be\n  displayed along with a warning message. (Note that this behavior is not\n  intrinsic to the text summary API, but rather to the default TensorBoard text\n  plugin.)\n\n  Args:\n    name: A name for the generated node. Will also serve as a series name in\n      TensorBoard.\n    data: A string-type Tensor to summarize. The text must be encoded in UTF-8.\n    display_name: Optional name for this summary in TensorBoard, as a\n      constant `str`. Defaults to `name`.\n    description: Optional long-form description for this summary, as a\n      constant `str`. Markdown is supported. Defaults to empty.\n    collections: Optional list of ops.GraphKeys. The collections to which to add\n      the summary. Defaults to [Graph Keys.SUMMARIES].\n\n  Returns:\n    A TensorSummary op that is configured so that TensorBoard will recognize\n    that it contains textual data. The TensorSummary is a scalar `Tensor` of\n    type `string` which contains `Summary` protobufs.\n\n  Raises:\n    ValueError: If tensor has the wrong type.\n  \"\"\"\n  # TODO(nickfelt): remove on-demand imports once dep situation is fixed.\n  import tensorflow.compat.v1 as tf\n\n  if display_name is None:\n    display_name = name\n  summary_metadata = metadata.create_summary_metadata(\n      display_name=display_name, description=description)\n  with tf.name_scope(name):\n    with tf.control_dependencies([tf.assert_type(data, tf.string)]):\n      return tf.summary.tensor_summary(name='text_summary',\n                                       tensor=data,\n                                       collections=collections,\n                                       summary_metadata=summary_metadata)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a legacy text summary protobuf.", "response": "def pb(name, data, display_name=None, description=None):\n  \"\"\"Create a legacy text summary protobuf.\n\n  Arguments:\n    name: A name for the generated node. Will also serve as a series name in\n      TensorBoard.\n    data: A Python bytestring (of type bytes), or Unicode string. Or a numpy\n      data array of those types.\n    display_name: Optional name for this summary in TensorBoard, as a\n      `str`. Defaults to `name`.\n    description: Optional long-form description for this summary, as a\n      `str`. Markdown is supported. Defaults to empty.\n\n  Raises:\n    ValueError: If the type of the data is unsupported.\n\n  Returns:\n    A `tf.Summary` protobuf object.\n  \"\"\"\n  # TODO(nickfelt): remove on-demand imports once dep situation is fixed.\n  import tensorflow.compat.v1 as tf\n\n  try:\n    tensor = tf.make_tensor_proto(data, dtype=tf.string)\n  except TypeError as e:\n    raise ValueError(e)\n\n  if display_name is None:\n    display_name = name\n  summary_metadata = metadata.create_summary_metadata(\n      display_name=display_name, description=description)\n  tf_summary_metadata = tf.SummaryMetadata.FromString(\n      summary_metadata.SerializeToString())\n  summary = tf.Summary()\n  summary.value.add(tag='%s/text_summary' % name,\n                    metadata=tf_summary_metadata,\n                    tensor=tensor)\n  return summary"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _GetPurgeMessage(most_recent_step, most_recent_wall_time, event_step,\n                     event_wall_time, num_expired_scalars, num_expired_histos,\n                     num_expired_comp_histos, num_expired_images,\n                     num_expired_audio):\n  \"\"\"Return the string message associated with TensorBoard purges.\"\"\"\n  return ('Detected out of order event.step likely caused by '\n          'a TensorFlow restart. Purging expired events from Tensorboard'\n          ' display between the previous step: {} (timestamp: {}) and '\n          'current step: {} (timestamp: {}). Removing {} scalars, {} '\n          'histograms, {} compressed histograms, {} images, '\n          'and {} audio.').format(most_recent_step, most_recent_wall_time,\n                                  event_step, event_wall_time,\n                                  num_expired_scalars, num_expired_histos,\n                                  num_expired_comp_histos, num_expired_images,\n                                  num_expired_audio)", "response": "Return a string message associated with TensorBoard purges."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates an event generator for file or directory at given path string.", "response": "def _GeneratorFromPath(path):\n  \"\"\"Create an event generator for file or directory at given path string.\"\"\"\n  if not path:\n    raise ValueError('path must be a valid string')\n  if io_wrapper.IsTensorFlowEventsFile(path):\n    return event_file_loader.EventFileLoader(path)\n  else:\n    return directory_watcher.DirectoryWatcher(\n        path,\n        event_file_loader.EventFileLoader,\n        io_wrapper.IsTensorFlowEventsFile)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _ParseFileVersion(file_version):\n  tokens = file_version.split('brain.Event:')\n  try:\n    return float(tokens[-1])\n  except ValueError:\n    ## This should never happen according to the definition of file_version\n    ## specified in event.proto.\n    logger.warn(\n        ('Invalid event.proto file_version. Defaulting to use of '\n         'out-of-order event.step logic for purging expired events.'))\n    return -1", "response": "Parses the string file_version in event. proto into a float."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads all events added since the last call to Reload.", "response": "def Reload(self):\n    \"\"\"Loads all events added since the last call to `Reload`.\n\n    If `Reload` was never called, loads all events in the file.\n\n    Returns:\n      The `EventAccumulator`.\n    \"\"\"\n    with self._generator_mutex:\n      for event in self._generator.Load():\n        self._ProcessEvent(event)\n    return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nretrieving the contents of a given plugin asset.", "response": "def RetrievePluginAsset(self, plugin_name, asset_name):\n    \"\"\"Return the contents of a given plugin asset.\n\n    Args:\n      plugin_name: The string name of a plugin.\n      asset_name: The string name of an asset.\n\n    Returns:\n      The string contents of the plugin asset.\n\n    Raises:\n      KeyError: If the asset is not available.\n    \"\"\"\n    return plugin_asset_util.RetrieveAsset(self.path, plugin_name, asset_name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the timestamp in seconds of the first event in the sequence.", "response": "def FirstEventTimestamp(self):\n    \"\"\"Returns the timestamp in seconds of the first event.\n\n    If the first event has been loaded (either by this method or by `Reload`,\n    this returns immediately. Otherwise, it will load in the first event. Note\n    that this means that calling `Reload` will cause this to block until\n    `Reload` has finished.\n\n    Returns:\n      The timestamp in seconds of the first event that was loaded.\n\n    Raises:\n      ValueError: If no events have been loaded and there were no events found\n      on disk.\n    \"\"\"\n    if self._first_event_timestamp is not None:\n      return self._first_event_timestamp\n    with self._generator_mutex:\n      try:\n        event = next(self._generator.Load())\n        self._ProcessEvent(event)\n        return self._first_event_timestamp\n\n      except StopIteration:\n        raise ValueError('No event timestamp could be found')"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a dict mapping tags to content specific to a plugin.", "response": "def PluginTagToContent(self, plugin_name):\n    \"\"\"Returns a dict mapping tags to content specific to that plugin.\n\n    Args:\n      plugin_name: The name of the plugin for which to fetch plugin-specific\n        content.\n\n    Raises:\n      KeyError: if the plugin name is not found.\n\n    Returns:\n      A dict mapping tags to plugin-specific content (which are always strings).\n      Those strings are often serialized protos.\n    \"\"\"\n    if plugin_name not in self._plugin_to_tag_to_content:\n      raise KeyError('Plugin %r could not be found.' % plugin_name)\n    return self._plugin_to_tag_to_content[plugin_name]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns all tags found in the value stream.", "response": "def Tags(self):\n    \"\"\"Return all tags found in the value stream.\n\n    Returns:\n      A `{tagType: ['list', 'of', 'tags']}` dictionary.\n    \"\"\"\n    return {\n        IMAGES: self.images.Keys(),\n        AUDIO: self.audios.Keys(),\n        HISTOGRAMS: self.histograms.Keys(),\n        SCALARS: self.scalars.Keys(),\n        COMPRESSED_HISTOGRAMS: self.compressed_histograms.Keys(),\n        TENSORS: self.tensors.Keys(),\n        # Use a heuristic: if the metagraph is available, but\n        # graph is not, then we assume the metagraph contains the graph.\n        GRAPH: self._graph is not None,\n        META_GRAPH: self._meta_graph is not None,\n        RUN_METADATA: list(self._tagged_metadata.keys())\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef Graph(self):\n    graph = graph_pb2.GraphDef()\n    if self._graph is not None:\n      graph.ParseFromString(self._graph)\n      return graph\n    raise ValueError('There is no graph in this EventAccumulator')", "response": "Returns the graph definition if there is one."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef MetaGraph(self):\n    if self._meta_graph is None:\n      raise ValueError('There is no metagraph in this EventAccumulator')\n    meta_graph = meta_graph_pb2.MetaGraphDef()\n    meta_graph.ParseFromString(self._meta_graph)\n    return meta_graph", "response": "Returns the metagraph definition if there is one."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef RunMetadata(self, tag):\n    if tag not in self._tagged_metadata:\n      raise ValueError('There is no run metadata with this tag name')\n\n    run_metadata = config_pb2.RunMetadata()\n    run_metadata.ParseFromString(self._tagged_metadata[tag])\n    return run_metadata", "response": "Given a tag return the associated session. run() metadata."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _MaybePurgeOrphanedData(self, event):\n    if not self.purge_orphaned_data:\n      return\n    ## Check if the event happened after a crash, and purge expired tags.\n    if self.file_version and self.file_version >= 2:\n      ## If the file_version is recent enough, use the SessionLog enum\n      ## to check for restarts.\n      self._CheckForRestartAndMaybePurge(event)\n    else:\n      ## If there is no file version, default to old logic of checking for\n      ## out of order steps.\n      self._CheckForOutOfOrderStepAndMaybePurge(event)", "response": "Maybe purge orphaned data due to a TensorFlow crash."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _CheckForRestartAndMaybePurge(self, event):\n    if event.HasField(\n        'session_log') and event.session_log.status == event_pb2.SessionLog.START:\n      self._Purge(event, by_tags=False)", "response": "Checks if the event is a SessionLog. START event and if so deletes all previously seen events."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _CheckForOutOfOrderStepAndMaybePurge(self, event):\n    if event.step < self.most_recent_step and event.HasField('summary'):\n      self._Purge(event, by_tags=True)\n    else:\n      self.most_recent_step = event.step\n      self.most_recent_wall_time = event.wall_time", "response": "Checks if the event is out - of - order and discards expired events for tags."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprocess a proto histogram by adding it to accumulated state.", "response": "def _ProcessHistogram(self, tag, wall_time, step, histo):\n    \"\"\"Processes a proto histogram by adding it to accumulated state.\"\"\"\n    histo = self._ConvertHistogramProtoToTuple(histo)\n    histo_ev = HistogramEvent(wall_time, step, histo)\n    self.histograms.AddItem(tag, histo_ev)\n    self.compressed_histograms.AddItem(tag, histo_ev, self._CompressHistogram)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprocess an image by adding it to accumulated state.", "response": "def _ProcessImage(self, tag, wall_time, step, image):\n    \"\"\"Processes an image by adding it to accumulated state.\"\"\"\n    event = ImageEvent(wall_time=wall_time,\n                       step=step,\n                       encoded_image_string=image.encoded_image_string,\n                       width=image.width,\n                       height=image.height)\n    self.images.AddItem(tag, event)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _ProcessAudio(self, tag, wall_time, step, audio):\n    event = AudioEvent(wall_time=wall_time,\n                       step=step,\n                       encoded_audio_string=audio.encoded_audio_string,\n                       content_type=audio.content_type,\n                       sample_rate=audio.sample_rate,\n                       length_frames=audio.length_frames)\n    self.audios.AddItem(tag, event)", "response": "Processes a audio by adding it to accumulated state."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _ProcessScalar(self, tag, wall_time, step, scalar):\n    sv = ScalarEvent(wall_time=wall_time, step=step, value=scalar)\n    self.scalars.AddItem(tag, sv)", "response": "Processes a simple value by adding it to accumulated state."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _Purge(self, event, by_tags):\n    ## Keep data in reservoirs that has a step less than event.step\n    _NotExpired = lambda x: x.step < event.step\n\n    if by_tags:\n      def _ExpiredPerTag(value):\n        return [getattr(self, x).FilterItems(_NotExpired, value.tag)\n                for x in self.accumulated_attrs]\n\n      expired_per_tags = [_ExpiredPerTag(value)\n                          for value in event.summary.value]\n      expired_per_type = [sum(x) for x in zip(*expired_per_tags)]\n    else:\n      expired_per_type = [getattr(self, x).FilterItems(_NotExpired)\n                          for x in self.accumulated_attrs]\n\n    if sum(expired_per_type) > 0:\n      purge_msg = _GetPurgeMessage(self.most_recent_step,\n                                   self.most_recent_wall_time, event.step,\n                                   event.wall_time, *expired_per_type)\n      logger.warn(purge_msg)", "response": "Purge all events that have occurred after the given event. step."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef Load(self):\n    logger.debug('Loading events from %s', self._file_path)\n\n    # GetNext() expects a status argument on TF <= 1.7.\n    get_next_args = inspect.getargspec(self._reader.GetNext).args  # pylint: disable=deprecated-method\n    # First argument is self\n    legacy_get_next = (len(get_next_args) > 1)\n\n    while True:\n      try:\n        if legacy_get_next:\n          with tf.compat.v1.errors.raise_exception_on_not_ok_status() as status:\n            self._reader.GetNext(status)\n        else:\n          self._reader.GetNext()\n      except (tf.errors.DataLossError, tf.errors.OutOfRangeError) as e:\n        logger.debug('Cannot read more events: %s', e)\n        # We ignore partial read exceptions, because a record may be truncated.\n        # PyRecordReader holds the offset prior to the failed read, so retrying\n        # will succeed.\n        break\n      yield self._reader.record()\n    logger.debug('No more events in %s', self._file_path)", "response": "Loads all new events from disk as raw serialized proto bytestrings."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload all new events from disk.", "response": "def Load(self):\n    \"\"\"Loads all new events from disk.\n\n    Calling Load multiple times in a row will not 'drop' events as long as the\n    return value is not iterated over.\n\n    Yields:\n      All events in the file that have not been yielded yet.\n    \"\"\"\n    for record in super(EventFileLoader, self).Load():\n      yield event_pb2.Event.FromString(record)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninput unscaled sections. returns: sections scaled to [0, 255]", "response": "def scale_sections(sections, scaling_scope):\n  '''\n  input: unscaled sections.\n  returns: sections scaled to [0, 255]\n  '''\n  new_sections = []\n\n  if scaling_scope == 'layer':\n    for section in sections:\n      new_sections.append(scale_image_for_display(section))\n\n  elif scaling_scope == 'network':\n    global_min, global_max = global_extrema(sections)\n\n    for section in sections:\n      new_sections.append(scale_image_for_display(section,\n                                                  global_min,\n                                                  global_max))\n  return new_sections"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrecording the summary values based on an updated message from the debugger.", "response": "def on_value_event(self, event):\n    \"\"\"Records the summary values based on an updated message from the debugger.\n\n    Logs an error message if writing the event to disk fails.\n\n    Args:\n      event: The Event proto to be processed.\n    \"\"\"\n    if not event.summary.value:\n      logger.warn(\"The summary of the event lacks a value.\")\n      return\n\n    # The node name property is actually a watch key, which is a concatenation\n    # of several pieces of data.\n    watch_key = event.summary.value[0].node_name\n    if not watch_key.endswith(constants.DEBUG_NUMERIC_SUMMARY_SUFFIX):\n      # Ignore events that lack a DebugNumericSummary.\n      # NOTE(@chihuahua): We may later handle other types of debug ops.\n      return\n\n    # We remove the constants.DEBUG_NUMERIC_SUMMARY_SUFFIX from the end of the\n    # watch name because it is not distinguishing: every health pill entry ends\n    # with it.\n    node_name_and_output_slot = watch_key[\n        :-len(constants.DEBUG_NUMERIC_SUMMARY_SUFFIX)]\n\n    shape = tensor_util.make_ndarray(event.summary.value[0].tensor).shape\n    if (len(shape) != 1 or\n        shape[0] < constants.MIN_DEBUG_NUMERIC_SUMMARY_TENSOR_LENGTH):\n      logger.warn(\"Health-pill tensor either lacks a dimension or is \"\n                         \"shaped incorrectly: %s\" % shape)\n      return\n\n    match = re.match(r\"^(.*):(\\d+)$\", node_name_and_output_slot)\n    if not match:\n      logger.warn(\n          (\"A event with a health pill has an invalid node name and output \"\n           \"slot combination, (i.e., an unexpected debug op): %r\"),\n          node_name_and_output_slot)\n      return\n\n    if self._session_run_index >= 0:\n      event.step = self._session_run_index\n    else:\n      # Data from parameter servers (or any graphs without a master) do not\n      # contain core metadata. So the session run count is missing. Set its\n      # value to a microsecond epoch timestamp.\n      event.step = int(time.time() * 1e6)\n\n    # Write this event to the events file designated for data from the\n    # debugger.\n    self._events_writer_manager.write_event(event)\n\n    alert = numerics_alert.extract_numerics_alert(event)\n    if self._numerics_alert_callback and alert:\n      self._numerics_alert_callback(alert)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing the session_run_index value from the event proto.", "response": "def _parse_session_run_index(self, event):\n    \"\"\"Parses the session_run_index value from the event proto.\n\n    Args:\n      event: The event with metadata that contains the session_run_index.\n\n    Returns:\n      The int session_run_index value. Or\n      constants.SENTINEL_FOR_UNDETERMINED_STEP if it could not be determined.\n    \"\"\"\n    metadata_string = event.log_message.message\n    try:\n      metadata = json.loads(metadata_string)\n    except ValueError as e:\n      logger.error(\n          \"Could not decode metadata string '%s' for step value: %s\",\n          metadata_string, e)\n      return constants.SENTINEL_FOR_UNDETERMINED_STEP\n\n    try:\n      return metadata[\"session_run_index\"]\n    except KeyError:\n      logger.error(\n          \"The session_run_index is missing from the metadata: %s\",\n          metadata_string)\n      return constants.SENTINEL_FOR_UNDETERMINED_STEP"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates fixed size histogram by adding compression to accumulated state. This routine transforms a histogram at a particular step by interpolating its variable number of buckets to represent their cumulative weight at a constant number of compression points. This significantly reduces the size of the histogram and makes it suitable for a two-dimensional area plot where the output of this routine constitutes the ranges for a single x coordinate. Args: histo: A HistogramProto object. bps: Compression points represented in basis points, 1/100ths of a percent. Defaults to normal distribution. Returns: List of values for each basis point.", "response": "def compress_histogram_proto(histo, bps=NORMAL_HISTOGRAM_BPS):\n  \"\"\"Creates fixed size histogram by adding compression to accumulated state.\n\n  This routine transforms a histogram at a particular step by interpolating its\n  variable number of buckets to represent their cumulative weight at a constant\n  number of compression points. This significantly reduces the size of the\n  histogram and makes it suitable for a two-dimensional area plot where the\n  output of this routine constitutes the ranges for a single x coordinate.\n\n  Args:\n    histo: A HistogramProto object.\n    bps: Compression points represented in basis points, 1/100ths of a percent.\n        Defaults to normal distribution.\n\n  Returns:\n    List of values for each basis point.\n  \"\"\"\n  # See also: Histogram::Percentile() in core/lib/histogram/histogram.cc\n  if not histo.num:\n    return [CompressedHistogramValue(b, 0.0) for b in bps]\n  bucket = np.array(histo.bucket)\n  bucket_limit = list(histo.bucket_limit)\n  weights = (bucket * bps[-1] / (bucket.sum() or 1.0)).cumsum()\n  values = []\n  j = 0\n  while j < len(bps):\n    i = np.searchsorted(weights, bps[j], side='right')\n    while i < len(weights):\n      cumsum = weights[i]\n      cumsum_prev = weights[i - 1] if i > 0 else 0.0\n      if cumsum == cumsum_prev:  # prevent lerp divide by zero\n        i += 1\n        continue\n      if not i or not cumsum_prev:\n        lhs = histo.min\n      else:\n        lhs = max(bucket_limit[i - 1], histo.min)\n      rhs = min(bucket_limit[i], histo.max)\n      weight = _lerp(bps[j], cumsum_prev, cumsum, lhs, rhs)\n      values.append(CompressedHistogramValue(bps[j], weight))\n      j += 1\n      break\n    else:\n      break\n  while j < len(bps):\n    values.append(CompressedHistogramValue(bps[j], histo.max))\n    j += 1\n  return values"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef compress_histogram(buckets, bps=NORMAL_HISTOGRAM_BPS):\n  # See also: Histogram::Percentile() in core/lib/histogram/histogram.cc\n  buckets = np.array(buckets)\n  if not buckets.size:\n    return [CompressedHistogramValue(b, 0.0) for b in bps]\n  (minmin, maxmax) = (buckets[0][0], buckets[-1][1])\n  counts = buckets[:, 2]\n  right_edges = list(buckets[:, 1])\n  weights = (counts * bps[-1] / (counts.sum() or 1.0)).cumsum()\n\n  result = []\n  bp_index = 0\n  while bp_index < len(bps):\n    i = np.searchsorted(weights, bps[bp_index], side='right')\n    while i < len(weights):\n      cumsum = weights[i]\n      cumsum_prev = weights[i - 1] if i > 0 else 0.0\n      if cumsum == cumsum_prev:  # prevent division-by-zero in `_lerp`\n        i += 1\n        continue\n      if not i or not cumsum_prev:\n        lhs = minmin\n      else:\n        lhs = max(right_edges[i - 1], minmin)\n      rhs = min(right_edges[i], maxmax)\n      weight = _lerp(bps[bp_index], cumsum_prev, cumsum, lhs, rhs)\n      result.append(CompressedHistogramValue(bps[bp_index], weight))\n      bp_index += 1\n      break\n    else:\n      break\n  while bp_index < len(bps):\n    result.append(CompressedHistogramValue(bps[bp_index], maxmax))\n    bp_index += 1\n  return result", "response": "This routine creates a fixed size histogram by adding compression to accumulated state."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _lerp(x, x0, x1, y0, y1):\n  return y0 + (x - x0) * float(y1 - y0) / (x1 - x0)", "response": "Affinely map from x0 x1 onto y0 y1"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_active(self):\n    if self._db_connection_provider:\n      # The plugin is active if one relevant tag can be found in the database.\n      db = self._db_connection_provider()\n      cursor = db.execute(\n          '''\n          SELECT 1\n          FROM Tags\n          WHERE Tags.plugin_name = ?\n          LIMIT 1\n          ''',\n          (metadata.PLUGIN_NAME,))\n      return bool(list(cursor))\n    if not self._multiplexer:\n      return False\n    return bool(self._multiplexer.PluginRunToTagToContent(metadata.PLUGIN_NAME))", "response": "The images plugin is active iff any run has at least one relevant tag."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _serve_image_metadata(self, request):\n    tag = request.args.get('tag')\n    run = request.args.get('run')\n    sample = int(request.args.get('sample', 0))\n    response = self._image_response_for_run(run, tag, sample)\n    return http_util.Respond(request, response, 'application/json')", "response": "This function is used to serve a list of images for a given tag and list of runs."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _image_response_for_run(self, run, tag, sample):\n    if self._db_connection_provider:\n      db = self._db_connection_provider()\n      cursor = db.execute(\n          '''\n          SELECT\n            computed_time,\n            step,\n            CAST (T0.data AS INT) AS width,\n            CAST (T1.data AS INT) AS height\n          FROM Tensors\n          JOIN TensorStrings AS T0\n            ON Tensors.rowid = T0.tensor_rowid\n          JOIN TensorStrings AS T1\n            ON Tensors.rowid = T1.tensor_rowid\n          WHERE\n            series = (\n              SELECT tag_id\n              FROM Runs\n              CROSS JOIN Tags USING (run_id)\n              WHERE Runs.run_name = :run AND Tags.tag_name = :tag)\n            AND step IS NOT NULL\n            AND dtype = :dtype\n            /* Should be n-vector, n >= 3: [width, height, samples...] */\n            AND (NOT INSTR(shape, ',') AND CAST (shape AS INT) >= 3)\n            AND T0.idx = 0\n            AND T1.idx = 1\n          ORDER BY step\n          ''',\n          {'run': run, 'tag': tag, 'dtype': tf.string.as_datatype_enum})\n      return [{\n          'wall_time': computed_time,\n          'step': step,\n          'width': width,\n          'height': height,\n          'query': self._query_for_individual_image(run, tag, sample, index)\n      } for index, (computed_time, step, width, height) in enumerate(cursor)]\n    response = []\n    index = 0\n    tensor_events = self._multiplexer.Tensors(run, tag)\n    filtered_events = self._filter_by_sample(tensor_events, sample)\n    for (index, tensor_event) in enumerate(filtered_events):\n      (width, height) = tensor_event.tensor_proto.string_val[:2]\n      response.append({\n          'wall_time': tensor_event.wall_time,\n          'step': tensor_event.step,\n          # We include the size so that the frontend can add that to the <img>\n          # tag so that the page layout doesn't change when the image loads.\n          'width': int(width),\n          'height': int(height),\n          'query': self._query_for_individual_image(run, tag, sample, index)\n      })\n    return response", "response": "Builds a JSON - serializable object with information about images for a given run."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_individual_image(self, run, tag, index, sample):\n    if self._db_connection_provider:\n      db = self._db_connection_provider()\n      cursor = db.execute(\n          '''\n          SELECT data\n          FROM TensorStrings\n          WHERE\n            /* Skip first 2 elements which are width and height. */\n            idx = 2 + :sample\n            AND tensor_rowid = (\n              SELECT rowid\n              FROM Tensors\n              WHERE\n                series = (\n                   SELECT tag_id\n                   FROM Runs\n                   CROSS JOIN Tags USING (run_id)\n                   WHERE\n                     Runs.run_name = :run\n                     AND Tags.tag_name = :tag)\n                AND step IS NOT NULL\n                AND dtype = :dtype\n                /* Should be n-vector, n >= 3: [width, height, samples...] */\n                AND (NOT INSTR(shape, ',') AND CAST (shape AS INT) >= 3)\n              ORDER BY step\n              LIMIT 1\n              OFFSET :index)\n          ''',\n          {'run': run,\n           'tag': tag,\n           'sample': sample,\n           'index': index,\n           'dtype': tf.string.as_datatype_enum})\n      (data,) = cursor.fetchone()\n      return six.binary_type(data)\n\n    events = self._filter_by_sample(self._multiplexer.Tensors(run, tag), sample)\n    images = events[index].tensor_proto.string_val[2:]  # skip width, height\n    return images[sample]", "response": "Returns the actual image bytes for a given image."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _serve_individual_image(self, request):\n    run = request.args.get('run')\n    tag = request.args.get('tag')\n    index = int(request.args.get('index'))\n    sample = int(request.args.get('sample', 0))\n    data = self._get_individual_image(run, tag, index, sample)\n    image_type = imghdr.what(None, data)\n    content_type = _IMGHDR_TO_MIMETYPE.get(image_type, _DEFAULT_IMAGE_MIMETYPE)\n    return http_util.Respond(request, data, content_type)", "response": "Serves an individual image."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef start_runs(\n    logdir,\n    steps,\n    run_name,\n    thresholds,\n    mask_every_other_prediction=False):\n  \"\"\"Generate a PR curve with precision and recall evenly weighted.\n\n  Arguments:\n    logdir: The directory into which to store all the runs' data.\n    steps: The number of steps to run for.\n    run_name: The name of the run.\n    thresholds: The number of thresholds to use for PR curves.\n    mask_every_other_prediction: Whether to mask every other prediction by\n      alternating weights between 0 and 1.\n  \"\"\"\n  tf.compat.v1.reset_default_graph()\n  tf.compat.v1.set_random_seed(42)\n\n  # Create a normal distribution layer used to generate true color labels.\n  distribution = tf.compat.v1.distributions.Normal(loc=0., scale=142.)\n\n  # Sample the distribution to generate colors. Lets generate different numbers\n  # of each color. The first dimension is the count of examples.\n\n  # The calls to sample() are given fixed random seed values that are \"magic\"\n  # in that they correspond to the default seeds for those ops when the PR\n  # curve test (which depends on this code) was written. We've pinned these\n  # instead of continuing to use the defaults since the defaults are based on\n  # node IDs from the sequence of nodes added to the graph, which can silently\n  # change when this code or any TF op implementations it uses are modified.\n\n  # TODO(nickfelt): redo the PR curve test to avoid reliance on random seeds.\n\n  # Generate reds.\n  number_of_reds = 100\n  true_reds = tf.clip_by_value(\n      tf.concat([\n          255 - tf.abs(distribution.sample([number_of_reds, 1], seed=11)),\n          tf.abs(distribution.sample([number_of_reds, 2], seed=34))\n      ], axis=1),\n      0, 255)\n\n  # Generate greens.\n  number_of_greens = 200\n  true_greens = tf.clip_by_value(\n      tf.concat([\n          tf.abs(distribution.sample([number_of_greens, 1], seed=61)),\n          255 - tf.abs(distribution.sample([number_of_greens, 1], seed=82)),\n          tf.abs(distribution.sample([number_of_greens, 1], seed=105))\n      ], axis=1),\n      0, 255)\n\n  # Generate blues.\n  number_of_blues = 150\n  true_blues = tf.clip_by_value(\n      tf.concat([\n          tf.abs(distribution.sample([number_of_blues, 2], seed=132)),\n          255 - tf.abs(distribution.sample([number_of_blues, 1], seed=153))\n      ], axis=1),\n      0, 255)\n\n  # Assign each color a vector of 3 booleans based on its true label.\n  labels = tf.concat([\n      tf.tile(tf.constant([[True, False, False]]), (number_of_reds, 1)),\n      tf.tile(tf.constant([[False, True, False]]), (number_of_greens, 1)),\n      tf.tile(tf.constant([[False, False, True]]), (number_of_blues, 1)),\n  ], axis=0)\n\n  # We introduce 3 normal distributions. They are used to predict whether a\n  # color falls under a certain class (based on distances from corners of the\n  # color triangle). The distributions vary per color. We have the distributions\n  # narrow over time.\n  initial_standard_deviations = [v + FLAGS.steps for v in (158, 200, 242)]\n  iteration = tf.compat.v1.placeholder(tf.int32, shape=[])\n  red_predictor = tf.compat.v1.distributions.Normal(\n      loc=0.,\n      scale=tf.cast(\n          initial_standard_deviations[0] - iteration,\n          dtype=tf.float32))\n  green_predictor = tf.compat.v1.distributions.Normal(\n      loc=0.,\n      scale=tf.cast(\n          initial_standard_deviations[1] - iteration,\n          dtype=tf.float32))\n  blue_predictor = tf.compat.v1.distributions.Normal(\n      loc=0.,\n      scale=tf.cast(\n          initial_standard_deviations[2] - iteration,\n          dtype=tf.float32))\n\n  # Make predictions (assign 3 probabilities to each color based on each color's\n  # distance to each of the 3 corners). We seek double the area in the right\n  # tail of the normal distribution.\n  examples = tf.concat([true_reds, true_greens, true_blues], axis=0)\n  probabilities_colors_are_red = (1 - red_predictor.cdf(\n      tf.norm(tensor=examples - tf.constant([255., 0, 0]), axis=1))) * 2\n  probabilities_colors_are_green = (1 - green_predictor.cdf(\n      tf.norm(tensor=examples - tf.constant([0, 255., 0]), axis=1))) * 2\n  probabilities_colors_are_blue = (1 - blue_predictor.cdf(\n      tf.norm(tensor=examples - tf.constant([0, 0, 255.]), axis=1))) * 2\n\n  predictions = (\n      probabilities_colors_are_red,\n      probabilities_colors_are_green,\n      probabilities_colors_are_blue\n  )\n\n  # This is the crucial piece. We write data required for generating PR curves.\n  # We create 1 summary per class because we create 1 PR curve per class.\n  for i, color in enumerate(('red', 'green', 'blue')):\n    description = ('The probabilities used to create this PR curve are '\n                   'generated from a normal distribution. Its standard '\n                   'deviation is initially %0.0f and decreases over time.' %\n                   initial_standard_deviations[i])\n\n    weights = None\n    if mask_every_other_prediction:\n      # Assign a weight of 0 to every even-indexed prediction. Odd-indexed\n      # predictions are assigned a default weight of 1.\n      consecutive_indices = tf.reshape(\n          tf.range(tf.size(input=predictions[i])), tf.shape(input=predictions[i]))\n      weights = tf.cast(consecutive_indices % 2, dtype=tf.float32)\n\n    summary.op(\n        name=color,\n        labels=labels[:, i],\n        predictions=predictions[i],\n        num_thresholds=thresholds,\n        weights=weights,\n        display_name='classifying %s' % color,\n        description=description)\n  merged_summary_op = tf.compat.v1.summary.merge_all()\n  events_directory = os.path.join(logdir, run_name)\n  sess = tf.compat.v1.Session()\n  writer = tf.compat.v1.summary.FileWriter(events_directory, sess.graph)\n\n  for step in xrange(steps):\n    feed_dict = {\n        iteration: step,\n    }\n    merged_summary = sess.run(merged_summary_op, feed_dict=feed_dict)\n    writer.add_summary(merged_summary, step)\n\n  writer.close()", "response": "Generates a PR curve with precision and recall evenly weighted."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates summaries for all the runs in the order of the given steps.", "response": "def run_all(logdir, steps, thresholds, verbose=False):\n  \"\"\"Generate PR curve summaries.\n\n  Arguments:\n    logdir: The directory into which to store all the runs' data.\n    steps: The number of steps to run for.\n    verbose: Whether to print the names of runs into stdout during execution.\n    thresholds: The number of thresholds to use for PR curves.\n  \"\"\"\n  # First, we generate data for a PR curve that assigns even weights for\n  # predictions of all classes.\n  run_name = 'colors'\n  if verbose:\n    print('--- Running: %s' % run_name)\n  start_runs(\n      logdir=logdir,\n      steps=steps,\n      run_name=run_name,\n      thresholds=thresholds)\n\n  # Next, we generate data for a PR curve that assigns arbitrary weights to\n  # predictions.\n  run_name = 'mask_every_other_prediction'\n  if verbose:\n    print('--- Running: %s' % run_name)\n  start_runs(\n      logdir=logdir,\n      steps=steps,\n      run_name=run_name,\n      thresholds=thresholds,\n      mask_every_other_prediction=True)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef image(name,\n          data,\n          step=None,\n          max_outputs=3,\n          description=None):\n  \"\"\"Write an image summary.\n\n  Arguments:\n    name: A name for this summary. The summary tag used for TensorBoard will\n      be this name prefixed by any active name scopes.\n    data: A `Tensor` representing pixel data with shape `[k, h, w, c]`,\n      where `k` is the number of images, `h` and `w` are the height and\n      width of the images, and `c` is the number of channels, which\n      should be 1, 2, 3, or 4 (grayscale, grayscale with alpha, RGB, RGBA).\n      Any of the dimensions may be statically unknown (i.e., `None`).\n      Floating point data will be clipped to the range [0,1).\n    step: Explicit `int64`-castable monotonic step value for this summary. If\n      omitted, this defaults to `tf.summary.experimental.get_step()`, which must\n      not be None.\n    max_outputs: Optional `int` or rank-0 integer `Tensor`. At most this\n      many images will be emitted at each step. When more than\n      `max_outputs` many images are provided, the first `max_outputs` many\n      images will be used and the rest silently discarded.\n    description: Optional long-form description for this summary, as a\n      constant `str`. Markdown is supported. Defaults to empty.\n\n  Returns:\n    True on success, or false if no summary was emitted because no default\n    summary writer was available.\n\n  Raises:\n    ValueError: if a default writer exists, but no step was provided and\n      `tf.summary.experimental.get_step()` is None.\n  \"\"\"\n  summary_metadata = metadata.create_summary_metadata(\n      display_name=None, description=description)\n  # TODO(https://github.com/tensorflow/tensorboard/issues/2109): remove fallback\n  summary_scope = (\n      getattr(tf.summary.experimental, 'summary_scope', None) or\n      tf.summary.summary_scope)\n  with summary_scope(\n      name, 'image_summary', values=[data, max_outputs, step]) as (tag, _):\n    tf.debugging.assert_rank(data, 4)\n    tf.debugging.assert_non_negative(max_outputs)\n    images = tf.image.convert_image_dtype(data, tf.uint8, saturate=True)\n    limited_images = images[:max_outputs]\n    encoded_images = tf.map_fn(tf.image.encode_png, limited_images,\n                               dtype=tf.string,\n                               name='encode_each_image')\n    # Workaround for map_fn returning float dtype for an empty elems input.\n    encoded_images = tf.cond(\n        tf.shape(input=encoded_images)[0] > 0,\n        lambda: encoded_images, lambda: tf.constant([], tf.string))\n    image_shape = tf.shape(input=images)\n    dimensions = tf.stack([tf.as_string(image_shape[2], name='width'),\n                           tf.as_string(image_shape[1], name='height')],\n                          name='dimensions')\n    tensor = tf.concat([dimensions, encoded_images], axis=0)\n    return tf.summary.write(\n        tag=tag, tensor=tensor, step=step, metadata=summary_metadata)", "response": "Writes an image summary."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the examples to be displayed in WIT.", "response": "def set_examples(self, examples):\n    \"\"\"Sets the examples to be displayed in WIT.\n\n    Args:\n      examples: List of example protos.\n\n    Returns:\n      self, in order to enabled method chaining.\n    \"\"\"\n    self.store('examples', examples)\n    if len(examples) > 0:\n      self.store('are_sequence_examples',\n                 isinstance(examples[0], tf.train.SequenceExample))\n    return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the model for inference as a TF Estimator.", "response": "def set_estimator_and_feature_spec(self, estimator, feature_spec):\n    \"\"\"Sets the model for inference as a TF Estimator.\n\n    Instead of using TF Serving to host a model for WIT to query, WIT can\n    directly use a TF Estimator object as the model to query. In order to\n    accomplish this, a feature_spec must also be provided to parse the\n    example protos for input into the estimator.\n\n    Args:\n      estimator: The TF Estimator which will be used for model inference.\n      feature_spec: The feature_spec object which will be used for example\n      parsing.\n\n    Returns:\n      self, in order to enabled method chaining.\n    \"\"\"\n    # If custom function is set, remove it before setting estimator\n    self.delete('custom_predict_fn')\n\n    self.store('estimator_and_spec', {\n      'estimator': estimator, 'feature_spec': feature_spec})\n    self.set_inference_address('estimator')\n    # If no model name has been set, give a default\n    if not self.has_model_name():\n      self.set_model_name('1')\n    return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_compare_estimator_and_feature_spec(self, estimator, feature_spec):\n    # If custom function is set, remove it before setting estimator\n    self.delete('compare_custom_predict_fn')\n\n    self.store('compare_estimator_and_spec', {\n      'estimator': estimator, 'feature_spec': feature_spec})\n    self.set_compare_inference_address('estimator')\n    # If no model name has been set, give a default\n    if not self.has_compare_model_name():\n      self.set_compare_model_name('2')\n    return self", "response": "Sets a second model for inference as a TF Estimator and a feature_spec object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_custom_predict_fn(self, predict_fn):\n    # If estimator is set, remove it before setting predict_fn\n    self.delete('estimator_and_spec')\n\n    self.store('custom_predict_fn', predict_fn)\n    self.set_inference_address('custom_predict_fn')\n    # If no model name has been set, give a default\n    if not self.has_model_name():\n      self.set_model_name('1')\n    return self", "response": "Sets a custom python function for inference."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset a custom python function for inference.", "response": "def set_compare_custom_predict_fn(self, predict_fn):\n    \"\"\"Sets a second custom function for inference.\n\n    If you wish to compare the results of two models in WIT, use this method\n    to setup the details of the second model.\n\n    Instead of using TF Serving to host a model for WIT to query, WIT can\n    directly use a custom function as the model to query. In this case, the\n    provided function should accept example protos and return:\n      - For classification: A 2D list of numbers. The first dimension is for\n        each example being predicted. The second dimension are the probabilities\n        for each class ID in the prediction.\n      - For regression: A 1D list of numbers, with a regression score for each\n        example being predicted.\n\n    Args:\n      predict_fn: The custom python function which will be used for model\n      inference.\n\n    Returns:\n      self, in order to enabled method chaining.\n    \"\"\"\n    # If estimator is set, remove it before setting predict_fn\n    self.delete('compare_estimator_and_spec')\n\n    self.store('compare_custom_predict_fn', predict_fn)\n    self.set_compare_inference_address('custom_predict_fn')\n    # If no model name has been set, give a default\n    if not self.has_compare_model_name():\n      self.set_compare_model_name('2')\n    return self"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _reshape_irregular_array(self, array, section_height, image_width):\n    '''Reshapes arrays of ranks not in {1, 2, 4}\n    '''\n    section_area = section_height * image_width\n    flattened_array = np.ravel(array)\n\n    if not self.config['show_all']:\n      flattened_array = flattened_array[:int(section_area/MIN_SQUARE_SIZE)]\n\n    cell_count = np.prod(flattened_array.shape)\n    cell_area = section_area / cell_count\n\n    cell_side_length = max(1, floor(sqrt(cell_area)))\n    row_count = max(1, int(section_height / cell_side_length))\n    col_count = int(cell_count / row_count)\n\n    # Reshape the truncated array so that it has the same aspect ratio as\n    # the section.\n\n    # Truncate whatever remaining values there are that don't fit. Hopefully\n    # it doesn't matter that the last few (< section count) aren't there.\n    section = np.reshape(flattened_array[:row_count * col_count],\n                         (row_count, col_count))\n\n    return section", "response": "Reshapes an irregular array of ranks not in { 1 2 4 }."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _arrays_to_sections(self, arrays):\n    '''\n    input: unprocessed numpy arrays.\n    returns: columns of the size that they will appear in the image, not scaled\n             for display. That needs to wait until after variance is computed.\n    '''\n    sections = []\n    sections_to_resize_later = {}\n    show_all = self.config['show_all']\n    image_width = self._determine_image_width(arrays, show_all)\n\n    for array_number, array in enumerate(arrays):\n      rank = len(array.shape)\n      section_height = self._determine_section_height(array, show_all)\n\n      if rank == 1:\n        section = np.atleast_2d(array)\n      elif rank == 2:\n        section = array\n      elif rank == 4:\n        section = self._reshape_conv_array(array, section_height, image_width)\n      else:\n        section = self._reshape_irregular_array(array,\n                                                section_height,\n                                                image_width)\n      # Only calculate variance for what we have to. In some cases (biases),\n      # the section is larger than the array, so we don't want to calculate\n      # variance for the same value over and over - better to resize later.\n      # About a 6-7x speedup for a big network with a big variance window.\n      section_size = section_height * image_width\n      array_size = np.prod(array.shape)\n\n      if section_size > array_size:\n        sections.append(section)\n        sections_to_resize_later[array_number] = section_height\n      else:\n        sections.append(im_util.resize(section, section_height, image_width))\n\n    self.sections_over_time.append(sections)\n\n    if self.config['mode'] == 'variance':\n      sections = self._sections_to_variance_sections(self.sections_over_time)\n\n    for array_number, height in sections_to_resize_later.items():\n      sections[array_number] = im_util.resize(sections[array_number],\n                                              height,\n                                              image_width)\n    return sections", "response": "This function takes a list of numpy arrays and returns a list of the sections that are needed to display the image."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes the variance of corresponding sections over time.", "response": "def _sections_to_variance_sections(self, sections_over_time):\n    '''Computes the variance of corresponding sections over time.\n\n    Returns:\n      a list of np arrays.\n    '''\n    variance_sections = []\n\n    for i in range(len(sections_over_time[0])):\n      time_sections = [sections[i] for sections in sections_over_time]\n      variance = np.var(time_sections, axis=0)\n      variance_sections.append(variance)\n\n    return variance_sections"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nclearing the deque if certain parts of the config have changed.", "response": "def _maybe_clear_deque(self):\n    '''Clears the deque if certain parts of the config have changed.'''\n\n    for config_item in ['values', 'mode', 'show_all']:\n      if self.config[config_item] != self.old_config[config_item]:\n        self.sections_over_time.clear()\n        break\n\n    self.old_config = self.config\n\n    window_size = self.config['window_size']\n    if window_size != self.sections_over_time.maxlen:\n      self.sections_over_time = deque(self.sections_over_time, window_size)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprovide the root module of a TF - like API for use within TensorBoard.", "response": "def tf():\n  \"\"\"Provide the root module of a TF-like API for use within TensorBoard.\n\n  By default this is equivalent to `import tensorflow as tf`, but it can be used\n  in combination with //tensorboard/compat:tensorflow (to fall back to a stub TF\n  API implementation if the real one is not available) or with\n  //tensorboard/compat:no_tensorflow (to force unconditional use of the stub).\n\n  Returns:\n    The root module of a TF-like API, if available.\n\n  Raises:\n    ImportError: if a TF-like API is not available.\n  \"\"\"\n  try:\n    from tensorboard.compat import notf  # pylint: disable=g-import-not-at-top\n  except ImportError:\n    try:\n      import tensorflow  # pylint: disable=g-import-not-at-top\n      return tensorflow\n    except ImportError:\n      pass\n  from tensorboard.compat import tensorflow_stub  # pylint: disable=g-import-not-at-top\n  return tensorflow_stub"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef tf2():\n  # Import the `tf` compat API from this file and check if it's already TF 2.0.\n  if tf.__version__.startswith('2.'):\n    return tf\n  elif hasattr(tf, 'compat') and hasattr(tf.compat, 'v2'):\n    # As a fallback, try `tensorflow.compat.v2` if it's defined.\n    return tf.compat.v2\n  raise ImportError('cannot import tensorflow 2.0 API')", "response": "Provide the root module of a TF - 2. 0 API for use within TensorBoard.\n"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _pywrap_tensorflow():\n  try:\n    from tensorboard.compat import notf  # pylint: disable=g-import-not-at-top\n  except ImportError:\n    try:\n      from tensorflow.python import pywrap_tensorflow  # pylint: disable=g-import-not-at-top\n      return pywrap_tensorflow\n    except ImportError:\n      pass\n  from tensorboard.compat.tensorflow_stub import pywrap_tensorflow  # pylint: disable=g-import-not-at-top\n  return pywrap_tensorflow", "response": "Provide pywrap_tensorflow access in TensorBoard."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_experiment_summary():\n\n  # Convert TEMPERATURE_LIST to google.protobuf.ListValue\n  temperature_list = struct_pb2.ListValue()\n  temperature_list.extend(TEMPERATURE_LIST)\n  materials = struct_pb2.ListValue()\n  materials.extend(HEAT_COEFFICIENTS.keys())\n  return summary.experiment_pb(\n      hparam_infos=[\n          api_pb2.HParamInfo(name='initial_temperature',\n                             display_name='Initial temperature',\n                             type=api_pb2.DATA_TYPE_FLOAT64,\n                             domain_discrete=temperature_list),\n          api_pb2.HParamInfo(name='ambient_temperature',\n                             display_name='Ambient temperature',\n                             type=api_pb2.DATA_TYPE_FLOAT64,\n                             domain_discrete=temperature_list),\n          api_pb2.HParamInfo(name='material',\n                             display_name='Material',\n                             type=api_pb2.DATA_TYPE_STRING,\n                             domain_discrete=materials)\n      ],\n      metric_infos=[\n          api_pb2.MetricInfo(\n              name=api_pb2.MetricName(\n                  tag='temperature/current/scalar_summary'),\n              display_name='Current Temp.'),\n          api_pb2.MetricInfo(\n              name=api_pb2.MetricName(\n                  tag='temperature/difference_to_ambient/scalar_summary'),\n              display_name='Difference To Ambient Temp.'),\n          api_pb2.MetricInfo(\n              name=api_pb2.MetricName(\n                  tag='delta/scalar_summary'),\n              display_name='Delta T')\n      ]\n  )", "response": "Creates a summary proto buffer holding this experiment."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrunning a temperature simulation.", "response": "def run(logdir, session_id, hparams, group_name):\n  \"\"\"Runs a temperature simulation.\n\n  This will simulate an object at temperature `initial_temperature`\n  sitting at rest in a large room at temperature `ambient_temperature`.\n  The object has some intrinsic `heat_coefficient`, which indicates\n  how much thermal conductivity it has: for instance, metals have high\n  thermal conductivity, while the thermal conductivity of water is low.\n\n  Over time, the object's temperature will adjust to match the\n  temperature of its environment. We'll track the object's temperature,\n  how far it is from the room's temperature, and how much it changes at\n  each time step.\n\n  Arguments:\n    logdir: the top-level directory into which to write summary data\n    session_id: an id for the session.\n    hparams: A dictionary mapping a hyperparameter name to its value.\n    group_name: an id for the session group this session belongs to.\n  \"\"\"\n  tf.reset_default_graph()\n  tf.set_random_seed(0)\n\n  initial_temperature = hparams['initial_temperature']\n  ambient_temperature = hparams['ambient_temperature']\n  heat_coefficient = HEAT_COEFFICIENTS[hparams['material']]\n  session_dir = os.path.join(logdir, session_id)\n  writer = tf.summary.FileWriter(session_dir)\n  writer.add_summary(summary.session_start_pb(hparams=hparams,\n                                              group_name=group_name))\n  writer.flush()\n  with tf.name_scope('temperature'):\n    # Create a mutable variable to hold the object's temperature, and\n    # create a scalar summary to track its value over time. The name of\n    # the summary will appear as 'temperature/current' due to the\n    # name-scope above.\n    temperature = tf.Variable(\n        tf.constant(initial_temperature),\n        name='temperature')\n    scalar_summary.op('current', temperature,\n                      display_name='Temperature',\n                      description='The temperature of the object under '\n                                  'simulation, in Kelvins.')\n\n    # Compute how much the object's temperature differs from that of its\n    # environment, and track this, too: likewise, as\n    # 'temperature/difference_to_ambient'.\n    ambient_difference = temperature - ambient_temperature\n    scalar_summary.op('difference_to_ambient', ambient_difference,\n                      display_name='Difference to ambient temperature',\n                      description=('The difference between the ambient '\n                                   'temperature and the temperature of the '\n                                   'object under simulation, in Kelvins.'))\n\n  # Newton suggested that the rate of change of the temperature of an\n  # object is directly proportional to this `ambient_difference` above,\n  # where the proportionality constant is what we called the heat\n  # coefficient. But in real life, not everything is quite so clean, so\n  # we'll add in some noise. (The value of 50 is arbitrary, chosen to\n  # make the data look somewhat interesting. :-) )\n  noise = 50 * tf.random.normal([])\n  delta = -heat_coefficient * (ambient_difference + noise)\n  scalar_summary.op('delta', delta,\n                    description='The change in temperature from the previous '\n                                'step, in Kelvins.')\n\n  # Collect all the scalars that we want to keep track of.\n  summ = tf.summary.merge_all()\n\n  # Now, augment the current temperature by this delta that we computed,\n  # blocking the assignment on summary collection to avoid race conditions\n  # and ensure that the summary always reports the pre-update value.\n  with tf.control_dependencies([summ]):\n    update_step = temperature.assign_add(delta)\n\n  sess = tf.Session()\n  sess.run(tf.global_variables_initializer())\n  for step in xrange(FLAGS.num_steps):\n    # By asking TensorFlow to compute the update step, we force it to\n    # change the value of the temperature variable. We don't actually\n    # care about this value, so we discard it; instead, we grab the\n    # summary data computed along the way.\n    (s, _) = sess.run([summ, update_step])\n    if (step % FLAGS.summary_freq) == 0:\n      writer.add_summary(s, global_step=step)\n  writer.add_summary(summary.session_end_pb(api_pb2.STATUS_SUCCESS))\n  writer.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nruns all the training simulations on a reasonable set of parameters.", "response": "def run_all(logdir, verbose=False):\n  \"\"\"Run simulations on a reasonable set of parameters.\n\n  Arguments:\n    logdir: the directory into which to store all the runs' data\n    verbose: if true, print out each run's name as it begins.\n  \"\"\"\n  writer = tf.summary.FileWriter(logdir)\n  writer.add_summary(create_experiment_summary())\n  writer.close()\n  session_num = 0\n  num_sessions = (len(TEMPERATURE_LIST)*len(TEMPERATURE_LIST)*\n                  len(HEAT_COEFFICIENTS)*2)\n  for initial_temperature in TEMPERATURE_LIST:\n    for ambient_temperature in TEMPERATURE_LIST:\n      for material in HEAT_COEFFICIENTS:\n        hparams = {u'initial_temperature': initial_temperature,\n                   u'ambient_temperature': ambient_temperature,\n                   u'material': material}\n        hparam_str = str(hparams)\n        group_name = fingerprint(hparam_str)\n        for repeat_idx in xrange(2):\n          session_id = str(session_num)\n          if verbose:\n            print('--- Running training session %d/%d' % (session_num + 1,\n                                                          num_sessions))\n            print(hparam_str)\n            print('--- repeat #: %d' % (repeat_idx+1))\n          run(logdir, session_id, hparams, group_name)\n          session_num += 1"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the registered filesystem for the given file.", "response": "def get_filesystem(filename):\n    \"\"\"Return the registered filesystem for the given file.\"\"\"\n    filename = compat.as_str_any(filename)\n    prefix = \"\"\n    index = filename.find(\"://\")\n    if index >= 0:\n        prefix = filename[:index]\n    fs = _REGISTERED_FILESYSTEMS.get(prefix, None)\n    if fs is None:\n        raise ValueError(\"No recognized filesystem for prefix %s\" % prefix)\n    return fs"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef walk(top, topdown=True, onerror=None):\n    top = compat.as_str_any(top)\n    fs = get_filesystem(top)\n    try:\n        listing = listdir(top)\n    except errors.NotFoundError as err:\n        if onerror:\n            onerror(err)\n        else:\n            return\n\n    files = []\n    subdirs = []\n    for item in listing:\n        full_path = fs.join(top, compat.as_str_any(item))\n        if isdir(full_path):\n            subdirs.append(item)\n        else:\n            files.append(item)\n\n    here = (top, subdirs, files)\n\n    if topdown:\n        yield here\n\n    for subdir in subdirs:\n        joined_subdir = fs.join(top, compat.as_str_any(subdir))\n        for subitem in walk(joined_subdir, topdown, onerror=onerror):\n            yield subitem\n\n    if not topdown:\n        yield here", "response": "Recursive directory tree generator for directories."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read(self, filename, binary_mode=False, size=None, offset=None):\n        mode = \"rb\" if binary_mode else \"r\"\n        with io.open(filename, mode) as f:\n            if offset is not None:\n                f.seek(offset)\n            if size is not None:\n                return f.read(size)\n            else:\n                return f.read()", "response": "Reads contents of a file into a string."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of files that match the given pattern ( s ).", "response": "def glob(self, filename):\n        \"\"\"Returns a list of files that match the given pattern(s).\"\"\"\n        if isinstance(filename, six.string_types):\n            return [\n                # Convert the filenames to string from bytes.\n                compat.as_str_any(matching_filename)\n                for matching_filename in py_glob.glob(\n                    compat.as_bytes(filename))\n            ]\n        else:\n            return [\n                # Convert the filenames to string from bytes.\n                compat.as_str_any(matching_filename)\n                for single_filename in filename\n                for matching_filename in py_glob.glob(\n                    compat.as_bytes(single_filename))\n            ]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of entries contained within a directory.", "response": "def listdir(self, dirname):\n        \"\"\"Returns a list of entries contained within a directory.\"\"\"\n        if not self.isdir(dirname):\n            raise errors.NotFoundError(None, None, \"Could not find directory\")\n\n        entries = os.listdir(compat.as_str_any(dirname))\n        entries = [compat.as_str_any(item) for item in entries]\n        return entries"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef stat(self, filename):\n        # NOTE: Size of the file is given by .st_size as returned from\n        # os.stat(), but we convert to .length\n        try:\n            len = os.stat(compat.as_bytes(filename)).st_size\n        except OSError:\n            raise errors.NotFoundError(None, None, \"Could not find file\")\n        return StatData(len)", "response": "Returns file statistics for a given path."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef bucket_and_path(self, url):\n        url = compat.as_str_any(url)\n        if url.startswith(\"s3://\"):\n            url = url[len(\"s3://\"):]\n        idx = url.index(\"/\")\n        bucket = url[:idx]\n        path = url[(idx + 1):]\n        return bucket, path", "response": "Split an S3 - prefixed URL into bucket and path."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndetermine whether a path exists or not.", "response": "def exists(self, filename):\n        \"\"\"Determines whether a path exists or not.\"\"\"\n        client = boto3.client(\"s3\")\n        bucket, path = self.bucket_and_path(filename)\n        r = client.list_objects(Bucket=bucket, Prefix=path, Delimiter=\"/\")\n        if r.get(\"Contents\") or r.get(\"CommonPrefixes\"):\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef read(self, filename, binary_mode=False, size=None, offset=None):\n        s3 = boto3.resource(\"s3\")\n        bucket, path = self.bucket_and_path(filename)\n        args = {}\n        endpoint = 0\n        if size is not None or offset is not None:\n            if offset is None:\n                offset = 0\n            endpoint = '' if size is None else (offset + size)\n            args['Range'] = 'bytes={}-{}'.format(offset, endpoint)\n        try:\n            stream = s3.Object(bucket, path).get(**args)['Body'].read()\n        except botocore.exceptions.ClientError as exc:\n            if exc.response['Error']['Code'] == '416':\n                if size is not None:\n                    # Asked for too much, so request just to the end. Do this\n                    # in a second request so we don't check length in all cases.\n                    client = boto3.client(\"s3\")\n                    obj = client.head_object(Bucket=bucket, Key=path)\n                    len = obj['ContentLength']\n                    endpoint = min(len, offset + size)\n                if offset == endpoint:\n                    # Asked for no bytes, so just return empty\n                    stream = b''\n                else:\n                    args['Range'] = 'bytes={}-{}'.format(offset, endpoint)\n                    stream = s3.Object(bucket, path).get(**args)['Body'].read()\n            else:\n                raise\n        if binary_mode:\n            return bytes(stream)\n        else:\n            return stream.decode('utf-8')", "response": "Reads contents of a file into a string or bytes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a list of files that match the given pattern ( s ).", "response": "def glob(self, filename):\n        \"\"\"Returns a list of files that match the given pattern(s).\"\"\"\n        # Only support prefix with * at the end and no ? in the string\n        star_i = filename.find('*')\n        quest_i = filename.find('?')\n        if quest_i >= 0:\n            raise NotImplementedError(\n                \"{} not supported by compat glob\".format(filename))\n        if star_i != len(filename) - 1:\n            # Just return empty so we can use glob from directory watcher\n            #\n            # TODO: Remove and instead handle in GetLogdirSubdirectories.\n            # However, we would need to handle it for all non-local registered\n            # filesystems in some way.\n            return []\n        filename = filename[:-1]\n        client = boto3.client(\"s3\")\n        bucket, path = self.bucket_and_path(filename)\n        p = client.get_paginator(\"list_objects\")\n        keys = []\n        for r in p.paginate(Bucket=bucket, Prefix=path):\n            for o in r.get(\"Contents\", []):\n                key = o[\"Key\"][len(path):]\n                if key:  # Skip the base dir, which would add an empty string\n                    keys.append(filename + key)\n        return keys"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef isdir(self, dirname):\n        client = boto3.client(\"s3\")\n        bucket, path = self.bucket_and_path(dirname)\n        if not path.endswith(\"/\"):\n            path += \"/\"  # This will now only retrieve subdir content\n        r = client.list_objects(Bucket=bucket, Prefix=path, Delimiter=\"/\")\n        if r.get(\"Contents\") or r.get(\"CommonPrefixes\"):\n            return True\n        return False", "response": "Returns whether the path is a directory or not."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of entries contained within a directory.", "response": "def listdir(self, dirname):\n        \"\"\"Returns a list of entries contained within a directory.\"\"\"\n        client = boto3.client(\"s3\")\n        bucket, path = self.bucket_and_path(dirname)\n        p = client.get_paginator(\"list_objects\")\n        if not path.endswith(\"/\"):\n            path += \"/\"  # This will now only retrieve subdir content\n        keys = []\n        for r in p.paginate(Bucket=bucket, Prefix=path, Delimiter=\"/\"):\n            keys.extend(o[\"Prefix\"][len(path):-1] for o in r.get(\"CommonPrefixes\", []))\n            for o in r.get(\"Contents\", []):\n                key = o[\"Key\"][len(path):]\n                if key:  # Skip the base dir, which would add an empty string\n                    keys.append(key)\n        return keys"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning file statistics for a given path.", "response": "def stat(self, filename):\n        \"\"\"Returns file statistics for a given path.\"\"\"\n        # NOTE: Size of the file is given by ContentLength from S3,\n        # but we convert to .length\n        client = boto3.client(\"s3\")\n        bucket, path = self.bucket_and_path(filename)\n        try:\n            obj = client.head_object(Bucket=bucket, Key=path)\n            return StatData(obj['ContentLength'])\n        except botocore.exceptions.ClientError as exc:\n            if exc.response['Error']['Code'] == '404':\n                raise errors.NotFoundError(None, None, \"Could not find file\")\n            else:\n                raise"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndetermines the most specific context that we re in.", "response": "def _get_context():\n  \"\"\"Determine the most specific context that we're in.\n\n  Returns:\n    _CONTEXT_COLAB: If in Colab with an IPython notebook context.\n    _CONTEXT_IPYTHON: If not in Colab, but we are in an IPython notebook\n      context (e.g., from running `jupyter notebook` at the command\n      line).\n    _CONTEXT_NONE: Otherwise (e.g., by running a Python script at the\n      command-line or using the `ipython` interactive shell).\n  \"\"\"\n  # In Colab, the `google.colab` module is available, but the shell\n  # returned by `IPython.get_ipython` does not have a `get_trait`\n  # method.\n  try:\n    import google.colab\n    import IPython\n  except ImportError:\n    pass\n  else:\n    if IPython.get_ipython() is not None:\n      # We'll assume that we're in a Colab notebook context.\n      return _CONTEXT_COLAB\n\n  # In an IPython command line shell or Jupyter notebook, we can\n  # directly query whether we're in a notebook context.\n  try:\n    import IPython\n  except ImportError:\n    pass\n  else:\n    ipython = IPython.get_ipython()\n    if ipython is not None and ipython.has_trait(\"kernel\"):\n      return _CONTEXT_IPYTHON\n\n  # Otherwise, we're not in a known notebook context.\n  return _CONTEXT_NONE"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlaunch and display a TensorBoard instance as if at the command line.", "response": "def start(args_string):\n  \"\"\"Launch and display a TensorBoard instance as if at the command line.\n\n  Args:\n    args_string: Command-line arguments to TensorBoard, to be\n      interpreted by `shlex.split`: e.g., \"--logdir ./logs --port 0\".\n      Shell metacharacters are not supported: e.g., \"--logdir 2>&1\" will\n      point the logdir at the literal directory named \"2>&1\".\n  \"\"\"\n  context = _get_context()\n  try:\n    import IPython\n    import IPython.display\n  except ImportError:\n    IPython = None\n\n  if context == _CONTEXT_NONE:\n    handle = None\n    print(\"Launching TensorBoard...\")\n  else:\n    handle = IPython.display.display(\n        IPython.display.Pretty(\"Launching TensorBoard...\"),\n        display_id=True,\n    )\n\n  def print_or_update(message):\n    if handle is None:\n      print(message)\n    else:\n      handle.update(IPython.display.Pretty(message))\n\n  parsed_args = shlex.split(args_string, comments=True, posix=True)\n  start_result = manager.start(parsed_args)\n\n  if isinstance(start_result, manager.StartLaunched):\n    _display(\n        port=start_result.info.port,\n        print_message=False,\n        display_handle=handle,\n    )\n\n  elif isinstance(start_result, manager.StartReused):\n    template = (\n        \"Reusing TensorBoard on port {port} (pid {pid}), started {delta} ago. \"\n        \"(Use '!kill {pid}' to kill it.)\"\n    )\n    message = template.format(\n        port=start_result.info.port,\n        pid=start_result.info.pid,\n        delta=_time_delta_from_info(start_result.info),\n    )\n    print_or_update(message)\n    _display(\n        port=start_result.info.port,\n        print_message=False,\n        display_handle=None,\n    )\n\n  elif isinstance(start_result, manager.StartFailed):\n    def format_stream(name, value):\n      if value == \"\":\n        return \"\"\n      elif value is None:\n        return \"\\n<could not read %s>\" % name\n      else:\n        return \"\\nContents of %s:\\n%s\" % (name, value.strip())\n    message = (\n        \"ERROR: Failed to launch TensorBoard (exited with %d).%s%s\" %\n        (\n            start_result.exit_code,\n            format_stream(\"stderr\", start_result.stderr),\n            format_stream(\"stdout\", start_result.stdout),\n        )\n    )\n    print_or_update(message)\n\n  elif isinstance(start_result, manager.StartTimedOut):\n    message = (\n        \"ERROR: Timed out waiting for TensorBoard to start. \"\n        \"It may still be running as pid %d.\"\n        % start_result.pid\n    )\n    print_or_update(message)\n\n  else:\n    raise TypeError(\n        \"Unexpected result from `manager.start`: %r.\\n\"\n        \"This is a TensorBoard bug; please report it.\"\n        % start_result\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _time_delta_from_info(info):\n  delta_seconds = int(time.time()) - info.start_time\n  return str(datetime.timedelta(seconds=delta_seconds))", "response": "Format the elapsed time for the given TensorBoardInfo."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef display(port=None, height=None):\n  _display(port=port, height=height, print_message=True, display_handle=None)", "response": "Display a TensorBoard instance already running on this machine."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndisplay a TensorBoard instance in a Colab output frame.", "response": "def _display_colab(port, height, display_handle):\n  \"\"\"Display a TensorBoard instance in a Colab output frame.\n\n  The Colab VM is not directly exposed to the network, so the Colab\n  runtime provides a service worker tunnel to proxy requests from the\n  end user's browser through to servers running on the Colab VM: the\n  output frame may issue requests to https://localhost:<port> (HTTPS\n  only), which will be forwarded to the specified port on the VM.\n\n  It does not suffice to create an `iframe` and let the service worker\n  redirect its traffic (`<iframe src=\"https://localhost:6006\">`),\n  because for security reasons service workers cannot intercept iframe\n  traffic. Instead, we manually fetch the TensorBoard index page with an\n  XHR in the output frame, and inject the raw HTML into `document.body`.\n\n  By default, the TensorBoard web app requests resources against\n  relative paths, like `./data/logdir`. Within the output frame, these\n  requests must instead hit `https://localhost:<port>/data/logdir`. To\n  redirect them, we change the document base URI, which transparently\n  affects all requests (XHRs and resources alike).\n  \"\"\"\n  import IPython.display\n  shell = \"\"\"\n    <div id=\"root\"></div>\n    <script>\n      (function() {\n        window.TENSORBOARD_ENV = window.TENSORBOARD_ENV || {};\n        window.TENSORBOARD_ENV[\"IN_COLAB\"] = true;\n        document.querySelector(\"base\").href = \"https://localhost:%PORT%\";\n        function fixUpTensorboard(root) {\n          const tftb = root.querySelector(\"tf-tensorboard\");\n          // Disable the fragment manipulation behavior in Colab. Not\n          // only is the behavior not useful (as the iframe's location\n          // is not visible to the user), it causes TensorBoard's usage\n          // of `window.replace` to navigate away from the page and to\n          // the `localhost:<port>` URL specified by the base URI, which\n          // in turn causes the frame to (likely) crash.\n          tftb.removeAttribute(\"use-hash\");\n        }\n        function executeAllScripts(root) {\n          // When `script` elements are inserted into the DOM by\n          // assigning to an element's `innerHTML`, the scripts are not\n          // executed. Thus, we manually re-insert these scripts so that\n          // TensorBoard can initialize itself.\n          for (const script of root.querySelectorAll(\"script\")) {\n            const newScript = document.createElement(\"script\");\n            newScript.type = script.type;\n            newScript.textContent = script.textContent;\n            root.appendChild(newScript);\n            script.remove();\n          }\n        }\n        function setHeight(root, height) {\n          // We set the height dynamically after the TensorBoard UI has\n          // been initialized. This avoids an intermediate state in\n          // which the container plus the UI become taller than the\n          // final width and cause the Colab output frame to be\n          // permanently resized, eventually leading to an empty\n          // vertical gap below the TensorBoard UI. It's not clear\n          // exactly what causes this problematic intermediate state,\n          // but setting the height late seems to fix it.\n          root.style.height = `${height}px`;\n        }\n        const root = document.getElementById(\"root\");\n        fetch(\".\")\n          .then((x) => x.text())\n          .then((html) => void (root.innerHTML = html))\n          .then(() => fixUpTensorboard(root))\n          .then(() => executeAllScripts(root))\n          .then(() => setHeight(root, %HEIGHT%));\n      })();\n    </script>\n  \"\"\".replace(\"%PORT%\", \"%d\" % port).replace(\"%HEIGHT%\", \"%d\" % height)\n  html = IPython.display.HTML(shell)\n  if display_handle:\n    display_handle.update(html)\n  else:\n    IPython.display.display(html)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef list():\n  infos = manager.get_all()\n  if not infos:\n    print(\"No known TensorBoard instances running.\")\n    return\n\n  print(\"Known TensorBoard instances:\")\n  for info in infos:\n    template = \"  - port {port}: {data_source} (started {delta} ago; pid {pid})\"\n    print(template.format(\n        port=info.port,\n        data_source=manager.data_source_from_info(info),\n        delta=_time_delta_from_info(info),\n        pid=info.pid,\n    ))", "response": "Print a listing of known TensorBoard instances."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks the path name to see if it is probably a TF Events file.", "response": "def IsTensorFlowEventsFile(path):\n  \"\"\"Check the path name to see if it is probably a TF Events file.\n\n  Args:\n    path: A file path to check if it is an event file.\n\n  Raises:\n    ValueError: If the path is an empty string.\n\n  Returns:\n    If path is formatted like a TensorFlowEventsFile.\n  \"\"\"\n  if not path:\n    raise ValueError('Path must be a nonempty string')\n  return 'tfevents' in tf.compat.as_str_any(os.path.basename(path))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nyields all files in the given directory. The paths are absolute.", "response": "def ListDirectoryAbsolute(directory):\n  \"\"\"Yields all files in the given directory. The paths are absolute.\"\"\"\n  return (os.path.join(directory, path)\n          for path in tf.io.gfile.listdir(directory))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _EscapeGlobCharacters(path):\n  drive, path = os.path.splitdrive(path)\n  return '%s%s' % (drive, _ESCAPE_GLOB_CHARACTERS_REGEX.sub(r'[\\1]', path))", "response": "Escapes the glob characters in a path."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ListRecursivelyViaGlobbing(top):\n  current_glob_string = os.path.join(_EscapeGlobCharacters(top), '*')\n  level = 0\n\n  while True:\n    logger.info('GlobAndListFiles: Starting to glob level %d', level)\n    glob = tf.io.gfile.glob(current_glob_string)\n    logger.info(\n        'GlobAndListFiles: %d files glob-ed at level %d', len(glob), level)\n\n    if not glob:\n      # This subdirectory level lacks files. Terminate.\n      return\n\n    # Map subdirectory to a list of files.\n    pairs = collections.defaultdict(list)\n    for file_path in glob:\n      pairs[os.path.dirname(file_path)].append(file_path)\n    for dir_name, file_paths in six.iteritems(pairs):\n      yield (dir_name, tuple(file_paths))\n\n    if len(pairs) == 1:\n      # If at any point the glob returns files that are all in a single\n      # directory, replace the current globbing path with that directory as the\n      # literal prefix. This should improve efficiency in cases where a single\n      # subdir is significantly deeper than the rest of the sudirs.\n      current_glob_string = os.path.join(list(pairs.keys())[0], '*')\n\n    # Iterate to the next level of subdirectories.\n    current_glob_string = os.path.join(current_glob_string, '*')\n    level += 1", "response": "Recursively lists all files within a directory."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwalking a directory tree yielding tuples containing the path to each file and the path to each of its subdirectories.", "response": "def ListRecursivelyViaWalking(top):\n  \"\"\"Walks a directory tree, yielding (dir_path, file_paths) tuples.\n\n  For each of `top` and its subdirectories, yields a tuple containing the path\n  to the directory and the path to each of the contained files.  Note that\n  unlike os.Walk()/tf.io.gfile.walk()/ListRecursivelyViaGlobbing, this does not\n  list subdirectories. The file paths are all absolute. If the directory does\n  not exist, this yields nothing.\n\n  Walking may be incredibly slow on certain file systems.\n\n  Args:\n    top: A path to a directory.\n\n  Yields:\n    A (dir_path, file_paths) tuple for each directory/subdirectory.\n  \"\"\"\n  for dir_path, _, filenames in tf.io.gfile.walk(top, topdown=True):\n    yield (dir_path, (os.path.join(dir_path, filename)\n                      for filename in filenames))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef GetLogdirSubdirectories(path):\n  if not tf.io.gfile.exists(path):\n    # No directory to traverse.\n    return ()\n\n  if not tf.io.gfile.isdir(path):\n    raise ValueError('GetLogdirSubdirectories: path exists and is not a '\n                     'directory, %s' % path)\n\n  if IsCloudPath(path):\n    # Glob-ing for files can be significantly faster than recursively\n    # walking through directories for some file systems.\n    logger.info(\n        'GetLogdirSubdirectories: Starting to list directories via glob-ing.')\n    traversal_method = ListRecursivelyViaGlobbing\n  else:\n    # For other file systems, the glob-ing based method might be slower because\n    # each call to glob could involve performing a recursive walk.\n    logger.info(\n        'GetLogdirSubdirectories: Starting to list directories via walking.')\n    traversal_method = ListRecursivelyViaWalking\n\n  return (\n      subdir\n      for (subdir, files) in traversal_method(path)\n      if any(IsTensorFlowEventsFile(f) for f in files)\n  )", "response": "Returns all subdirectories with events files in the given path."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef audio(name,\n          data,\n          sample_rate,\n          step=None,\n          max_outputs=3,\n          encoding=None,\n          description=None):\n  \"\"\"Write an audio summary.\n\n  Arguments:\n    name: A name for this summary. The summary tag used for TensorBoard will\n      be this name prefixed by any active name scopes.\n    data: A `Tensor` representing audio data with shape `[k, t, c]`,\n      where `k` is the number of audio clips, `t` is the number of\n      frames, and `c` is the number of channels. Elements should be\n      floating-point values in `[-1.0, 1.0]`. Any of the dimensions may\n      be statically unknown (i.e., `None`).\n    sample_rate: An `int` or rank-0 `int32` `Tensor` that represents the\n      sample rate, in Hz. Must be positive.\n    step: Explicit `int64`-castable monotonic step value for this summary. If\n      omitted, this defaults to `tf.summary.experimental.get_step()`, which must\n      not be None.\n    max_outputs: Optional `int` or rank-0 integer `Tensor`. At most this\n      many audio clips will be emitted at each step. When more than\n      `max_outputs` many clips are provided, the first `max_outputs`\n      many clips will be used and the rest silently discarded.\n    encoding: Optional constant `str` for the desired encoding. Only \"wav\"\n      is currently supported, but this is not guaranteed to remain the\n      default, so if you want \"wav\" in particular, set this explicitly.\n    description: Optional long-form description for this summary, as a\n      constant `str`. Markdown is supported. Defaults to empty.\n\n  Returns:\n    True on success, or false if no summary was emitted because no default\n    summary writer was available.\n\n  Raises:\n    ValueError: if a default writer exists, but no step was provided and\n      `tf.summary.experimental.get_step()` is None.\n  \"\"\"\n  audio_ops = getattr(tf, 'audio', None)\n  if audio_ops is None:\n    # Fallback for older versions of TF without tf.audio.\n    from tensorflow.python.ops import gen_audio_ops as audio_ops\n\n  if encoding is None:\n    encoding = 'wav'\n  if encoding != 'wav':\n    raise ValueError('Unknown encoding: %r' % encoding)\n  summary_metadata = metadata.create_summary_metadata(\n      display_name=None,\n      description=description,\n      encoding=metadata.Encoding.Value('WAV'))\n  inputs = [data, sample_rate, max_outputs, step]\n  # TODO(https://github.com/tensorflow/tensorboard/issues/2109): remove fallback\n  summary_scope = (\n      getattr(tf.summary.experimental, 'summary_scope', None) or\n      tf.summary.summary_scope)\n  with summary_scope(\n      name, 'audio_summary', values=inputs) as (tag, _):\n    tf.debugging.assert_rank(data, 3)\n    tf.debugging.assert_non_negative(max_outputs)\n    limited_audio = data[:max_outputs]\n    encode_fn = functools.partial(audio_ops.encode_wav,\n                                  sample_rate=sample_rate)\n    encoded_audio = tf.map_fn(encode_fn, limited_audio,\n                              dtype=tf.string,\n                              name='encode_each_audio')\n    # Workaround for map_fn returning float dtype for an empty elems input.\n    encoded_audio = tf.cond(\n        tf.shape(input=encoded_audio)[0] > 0,\n        lambda: encoded_audio, lambda: tf.constant([], tf.string))\n    limited_labels = tf.tile([''], tf.shape(input=limited_audio)[:1])\n    tensor = tf.transpose(a=tf.stack([encoded_audio, limited_labels]))\n    return tf.summary.write(\n        tag=tag, tensor=tensor, step=step, metadata=summary_metadata)", "response": "Write an audio summary."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndetermine whether a health pill event contains bad values.", "response": "def extract_numerics_alert(event):\n  \"\"\"Determines whether a health pill event contains bad values.\n\n  A bad value is one of NaN, -Inf, or +Inf.\n\n  Args:\n    event: (`Event`) A `tensorflow.Event` proto from `DebugNumericSummary`\n      ops.\n\n  Returns:\n    An instance of `NumericsAlert`, if bad values are found.\n    `None`, if no bad values are found.\n\n  Raises:\n    ValueError: if the event does not have the expected tag prefix or the\n      debug op name is not the expected debug op name suffix.\n  \"\"\"\n  value = event.summary.value[0]\n  debugger_plugin_metadata_content = None\n  if value.HasField(\"metadata\"):\n    plugin_data = value.metadata.plugin_data\n    if plugin_data.plugin_name == constants.DEBUGGER_PLUGIN_NAME:\n      debugger_plugin_metadata_content = plugin_data.content\n\n  if not debugger_plugin_metadata_content:\n    raise ValueError(\"Event proto input lacks debugger plugin SummaryMetadata.\")\n\n  debugger_plugin_metadata_content = tf.compat.as_text(\n      debugger_plugin_metadata_content)\n  try:\n    content_object = json.loads(debugger_plugin_metadata_content)\n    device_name = content_object[\"device\"]\n  except (KeyError, ValueError) as e:\n    raise ValueError(\"Could not determine device from JSON string %r, %r\" %\n                     (debugger_plugin_metadata_content, e))\n\n  debug_op_suffix = \":DebugNumericSummary\"\n  if not value.node_name.endswith(debug_op_suffix):\n    raise ValueError(\n        \"Event proto input does not have the expected debug op suffix %s\" %\n        debug_op_suffix)\n  tensor_name = value.node_name[:-len(debug_op_suffix)]\n\n  elements = tf_debug.load_tensor_from_event(event)\n  nan_count = elements[constants.NAN_NUMERIC_SUMMARY_OP_INDEX]\n  neg_inf_count = elements[constants.NEG_INF_NUMERIC_SUMMARY_OP_INDEX]\n  pos_inf_count = elements[constants.POS_INF_NUMERIC_SUMMARY_OP_INDEX]\n  if nan_count > 0 or neg_inf_count > 0 or pos_inf_count > 0:\n    return NumericsAlert(\n        device_name, tensor_name, event.wall_time, nan_count, neg_inf_count,\n        pos_inf_count)\n  return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the first timestamp of the given event type.", "response": "def first_timestamp(self, event_key=None):\n    \"\"\"Obtain the first timestamp.\n\n    Args:\n      event_key: the type key of the sought events (e.g., constants.NAN_KEY).\n      If None, includes all event type keys.\n\n    Returns:\n      First (earliest) timestamp of all the events of the given type (or all\n        event types if event_key is None).\n    \"\"\"\n    if event_key is None:\n      timestamps = [self._trackers[key].first_timestamp\n                    for key in self._trackers]\n      return min(timestamp for timestamp in timestamps if timestamp >= 0)\n    else:\n      return self._trackers[event_key].first_timestamp"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef last_timestamp(self, event_key=None):\n    if event_key is None:\n      timestamps = [self._trackers[key].first_timestamp\n                    for key in self._trackers]\n      return max(timestamp for timestamp in timestamps if timestamp >= 0)\n    else:\n      return self._trackers[event_key].last_timestamp", "response": "Obtain the last timestamp of the given event."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a JSON - able representation of this object.", "response": "def create_jsonable_history(self):\n    \"\"\"Creates a JSON-able representation of this object.\n\n    Returns:\n      A dictionary mapping key to EventTrackerDescription (which can be used to\n      create event trackers).\n    \"\"\"\n    return {value_category_key: tracker.get_description()\n            for (value_category_key, tracker) in self._trackers.items()}"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nregisters an alerting numeric event.", "response": "def register(self, numerics_alert):\n    \"\"\"Register an alerting numeric event.\n\n    Args:\n      numerics_alert: An instance of `NumericsAlert`.\n    \"\"\"\n    key = (numerics_alert.device_name, numerics_alert.tensor_name)\n    if key in self._data:\n      self._data[key].add(numerics_alert)\n    else:\n      if len(self._data) < self._capacity:\n        history = NumericsAlertHistory()\n        history.add(numerics_alert)\n        self._data[key] = history"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef report(self, device_name_filter=None, tensor_name_filter=None):\n    report = []\n    for key in self._data:\n      device_name, tensor_name = key\n      history = self._data[key]\n      report.append(\n          NumericsAlertReportRow(\n              device_name=device_name,\n              tensor_name=tensor_name,\n              first_timestamp=history.first_timestamp(),\n              nan_event_count=history.event_count(constants.NAN_KEY),\n              neg_inf_event_count=history.event_count(constants.NEG_INF_KEY),\n              pos_inf_event_count=history.event_count(constants.POS_INF_KEY)))\n\n    if device_name_filter:\n      device_name_pattern = re.compile(device_name_filter)\n      report = [item for item in report\n                if device_name_pattern.match(item.device_name)]\n    if tensor_name_filter:\n      tensor_name_pattern = re.compile(tensor_name_filter)\n      report = [item for item in report\n                if tensor_name_pattern.match(item.tensor_name)]\n    # Sort results chronologically.\n    return sorted(report, key=lambda x: x.first_timestamp)", "response": "Get a report of offending device and tensor names."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a JSON - able representation of this object.", "response": "def create_jsonable_registry(self):\n    \"\"\"Creates a JSON-able representation of this object.\n\n    Returns:\n      A dictionary mapping (device, tensor name) to JSON-able object\n      representations of NumericsAlertHistory.\n    \"\"\"\n    # JSON does not support tuples as keys. Only strings. Therefore, we store\n    # the device name, tensor name, and dictionary data within a 3-item list.\n    return [HistoryTriplet(pair[0], pair[1], history.create_jsonable_history())\n            for (pair, history) in self._data.items()]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run(logdir, run_name, wave_name, wave_constructor):\n  tf.compat.v1.reset_default_graph()\n  tf.compat.v1.set_random_seed(0)\n\n  # On each step `i`, we'll set this placeholder to `i`. This allows us\n  # to know \"what time it is\" at each step.\n  step_placeholder = tf.compat.v1.placeholder(tf.float32, shape=[])\n\n  # We want to linearly interpolate a frequency between A4 (440 Hz) and\n  # A5 (880 Hz).\n  with tf.name_scope('compute_frequency'):\n    f_min = 440.0\n    f_max = 880.0\n    t = step_placeholder / (FLAGS.steps - 1)\n    frequency = f_min * (1.0 - t) + f_max * t\n\n  # Let's log this frequency, just so that we can make sure that it's as\n  # expected.\n  tf.compat.v1.summary.scalar('frequency', frequency)\n\n  # Now, we pass this to the wave constructor to get our waveform. Doing\n  # so within a name scope means that any summaries that the wave\n  # constructor produces will be namespaced.\n  with tf.name_scope(wave_name):\n    waveform = wave_constructor(frequency)\n\n  # We also have the opportunity to annotate each audio clip with a\n  # label. This is a good place to include the frequency, because it'll\n  # be visible immediately next to the audio clip.\n  with tf.name_scope('compute_labels'):\n    samples = tf.shape(input=waveform)[0]\n    wave_types = tf.tile([\"*Wave type:* `%s`.\" % wave_name], [samples])\n    frequencies = tf.strings.join([\n        \"*Frequency:* \",\n        tf.tile([tf.as_string(frequency, precision=2)], [samples]),\n        \" Hz.\",\n    ])\n    samples = tf.strings.join([\n        \"*Sample:* \", tf.as_string(tf.range(samples) + 1),\n        \" of \", tf.as_string(samples), \".\",\n    ])\n    labels = tf.strings.join([wave_types, frequencies, samples], separator=\" \")\n\n  # We can place a description next to the summary in TensorBoard. This\n  # is a good place to explain what the summary represents, methodology\n  # for creating it, etc. Let's include the source code of the function\n  # that generated the wave.\n  source = '\\n'.join('    %s' % line.rstrip()\n                     for line in inspect.getsourcelines(wave_constructor)[0])\n  description = (\"A wave of type `%r`, generated via:\\n\\n%s\"\n                 % (wave_name, source))\n\n  # Here's the crucial piece: we interpret this result as audio.\n  summary.op('waveform', waveform, FLAGS.sample_rate,\n             labels=labels,\n             display_name=wave_name,\n             description=description)\n\n  # Now, we can collect up all the summaries and begin the run.\n  summ = tf.compat.v1.summary.merge_all()\n\n  sess = tf.compat.v1.Session()\n  writer = tf.summary.FileWriter(os.path.join(logdir, run_name))\n  writer.add_graph(sess.graph)\n  sess.run(tf.compat.v1.global_variables_initializer())\n  for step in xrange(FLAGS.steps):\n    s = sess.run(summ, feed_dict={step_placeholder: float(step)})\n    writer.add_summary(s, global_step=step)\n  writer.close()", "response": "Generate the wave data of the given form."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nemits a sine wave at the given frequency.", "response": "def sine_wave(frequency):\n  \"\"\"Emit a sine wave at the given frequency.\"\"\"\n  xs = tf.reshape(tf.range(_samples(), dtype=tf.float32), [1, _samples(), 1])\n  ts = xs / FLAGS.sample_rate\n  return tf.sin(2 * math.pi * frequency * ts)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nemit a triangle wave at the given frequency.", "response": "def triangle_wave(frequency):\n  \"\"\"Emit a triangle wave at the given frequency.\"\"\"\n  xs = tf.reshape(tf.range(_samples(), dtype=tf.float32), [1, _samples(), 1])\n  ts = xs / FLAGS.sample_rate\n  #\n  # A triangle wave looks like this:\n  #\n  #      /\\      /\\\n  #     /  \\    /  \\\n  #         \\  /    \\  /\n  #          \\/      \\/\n  #\n  # If we look at just half a period (the first four slashes in the\n  # diagram above), we can see that it looks like a transformed absolute\n  # value function.\n  #\n  # Let's start by computing the times relative to the start of each\n  # half-wave pulse (each individual \"mountain\" or \"valley\", of which\n  # there are four in the above diagram).\n  half_pulse_index = ts * (frequency * 2)\n  half_pulse_angle = half_pulse_index % 1.0  # in [0, 1]\n  #\n  # Now, we can see that each positive half-pulse (\"mountain\") has\n  # amplitude given by A(z) = 0.5 - abs(z - 0.5), and then normalized:\n  absolute_amplitude = (0.5 - tf.abs(half_pulse_angle - 0.5)) / 0.5\n  #\n  # But every other half-pulse is negative, so we should invert these.\n  half_pulse_parity = tf.sign(1 - (half_pulse_index % 2.0))\n  amplitude = half_pulse_parity * absolute_amplitude\n  #\n  # This is precisely the desired result, so we're done!\n  return amplitude"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nemit two sine waves in stereo at different octaves.", "response": "def bisine_wave(frequency):\n  \"\"\"Emit two sine waves, in stereo at different octaves.\"\"\"\n  #\n  # We can first our existing sine generator to generate two different\n  # waves.\n  f_hi = frequency\n  f_lo = frequency / 2.0\n  with tf.name_scope('hi'):\n    sine_hi = sine_wave(f_hi)\n  with tf.name_scope('lo'):\n    sine_lo = sine_wave(f_lo)\n  #\n  # Now, we have two tensors of shape [1, _samples(), 1]. By concatenating\n  # them along axis 2, we get a tensor of shape [1, _samples(), 2]---a\n  # stereo waveform.\n  return tf.concat([sine_lo, sine_hi], axis=2)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nemits two sine waves with balance oscillating left and right.", "response": "def bisine_wahwah_wave(frequency):\n  \"\"\"Emit two sine waves with balance oscillating left and right.\"\"\"\n  #\n  # This is clearly intended to build on the bisine wave defined above,\n  # so we can start by generating that.\n  waves_a = bisine_wave(frequency)\n  #\n  # Then, by reversing axis 2, we swap the stereo channels. By mixing\n  # this with `waves_a`, we'll be able to create the desired effect.\n  waves_b = tf.reverse(waves_a, axis=[2])\n  #\n  # Let's have the balance oscillate from left to right four times.\n  iterations = 4\n  #\n  # Now, we compute the balance for each sample: `ts` has values\n  # in [0, 1] that indicate how much we should use `waves_a`.\n  xs = tf.reshape(tf.range(_samples(), dtype=tf.float32), [1, _samples(), 1])\n  thetas = xs / _samples() * iterations\n  ts = (tf.sin(math.pi * 2 * thetas) + 1) / 2\n  #\n  # Finally, we can mix the two together, and we're done.\n  wave = ts * waves_a + (1.0 - ts) * waves_b\n  #\n  # Alternately, we can make the effect more pronounced by exaggerating\n  # the sample data. Let's emit both variations.\n  exaggerated_wave = wave ** 3.0\n  return tf.concat([wave, exaggerated_wave], axis=0)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates waves of the shapes defined above.", "response": "def run_all(logdir, verbose=False):\n  \"\"\"Generate waves of the shapes defined above.\n\n  Arguments:\n    logdir: the directory into which to store all the runs' data\n    verbose: if true, print out each run's name as it begins\n  \"\"\"\n  waves = [sine_wave, square_wave, triangle_wave,\n           bisine_wave, bisine_wahwah_wave]\n  for (i, wave_constructor) in enumerate(waves):\n    wave_name = wave_constructor.__name__\n    run_name = 'wave:%02d,%s' % (i + 1, wave_name)\n    if verbose:\n      print('--- Running: %s' % run_name)\n    run(logdir, run_name, wave_name, wave_constructor)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef prepare_graph_for_ui(graph, limit_attr_size=1024,\n                         large_attrs_key='_too_large_attrs'):\n  \"\"\"Prepares (modifies in-place) the graph to be served to the front-end.\n\n  For now, it supports filtering out attributes that are\n  too large to be shown in the graph UI.\n\n  Args:\n    graph: The GraphDef proto message.\n    limit_attr_size: Maximum allowed size in bytes, before the attribute\n        is considered large. Default is 1024 (1KB). Must be > 0 or None.\n        If None, there will be no filtering.\n    large_attrs_key: The attribute key that will be used for storing attributes\n        that are too large. Default is '_too_large_attrs'. Must be != None if\n        `limit_attr_size` is != None.\n\n  Raises:\n    ValueError: If `large_attrs_key is None` while `limit_attr_size != None`.\n    ValueError: If `limit_attr_size` is defined, but <= 0.\n  \"\"\"\n  # Check input for validity.\n  if limit_attr_size is not None:\n    if large_attrs_key is None:\n      raise ValueError('large_attrs_key must be != None when limit_attr_size'\n                       '!= None.')\n\n    if limit_attr_size <= 0:\n      raise ValueError('limit_attr_size must be > 0, but is %d' %\n                       limit_attr_size)\n\n  # Filter only if a limit size is defined.\n  if limit_attr_size is not None:\n    for node in graph.node:\n      # Go through all the attributes and filter out ones bigger than the\n      # limit.\n      keys = list(node.attr.keys())\n      for key in keys:\n        size = node.attr[key].ByteSize()\n        if size > limit_attr_size or size < 0:\n          del node.attr[key]\n          # Add the attribute key to the list of \"too large\" attributes.\n          # This is used in the info card in the graph UI to show the user\n          # that some attributes are too large to be shown.\n          node.attr[large_attrs_key].list.s.append(tf.compat.as_bytes(key))", "response": "Prepares the graph to be served to the front - end UI."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef info_impl(self):\n    result = {}\n    def add_row_item(run, tag=None):\n      run_item = result.setdefault(run, {\n          'run': run,\n          'tags': {},\n          # A run-wide GraphDef of ops.\n          'run_graph': False})\n\n      tag_item = None\n      if tag:\n        tag_item = run_item.get('tags').setdefault(tag, {\n            'tag': tag,\n            'conceptual_graph': False,\n            # A tagged GraphDef of ops.\n            'op_graph': False,\n            'profile': False})\n      return (run_item, tag_item)\n\n    mapping = self._multiplexer.PluginRunToTagToContent(\n        _PLUGIN_NAME_RUN_METADATA_WITH_GRAPH)\n    for run_name, tag_to_content in six.iteritems(mapping):\n      for (tag, content) in six.iteritems(tag_to_content):\n        # The Summary op is defined in TensorFlow and does not use a stringified proto\n        # as a content of plugin data. It contains single string that denotes a version.\n        # https://github.com/tensorflow/tensorflow/blob/11f4ecb54708865ec757ca64e4805957b05d7570/tensorflow/python/ops/summary_ops_v2.py#L789-L790\n        if content != b'1':\n          logger.warn('Ignoring unrecognizable version of RunMetadata.')\n          continue\n        (_, tag_item) = add_row_item(run_name, tag)\n        tag_item['op_graph'] = True\n\n    # Tensors associated with plugin name _PLUGIN_NAME_RUN_METADATA contain\n    # both op graph and profile information.\n    mapping = self._multiplexer.PluginRunToTagToContent(\n        _PLUGIN_NAME_RUN_METADATA)\n    for run_name, tag_to_content in six.iteritems(mapping):\n      for (tag, content) in six.iteritems(tag_to_content):\n        if content != b'1':\n          logger.warn('Ignoring unrecognizable version of RunMetadata.')\n          continue\n        (_, tag_item) = add_row_item(run_name, tag)\n        tag_item['profile'] = True\n        tag_item['op_graph'] = True\n\n    # Tensors associated with plugin name _PLUGIN_NAME_KERAS_MODEL contain\n    # serialized Keras model in JSON format.\n    mapping = self._multiplexer.PluginRunToTagToContent(\n        _PLUGIN_NAME_KERAS_MODEL)\n    for run_name, tag_to_content in six.iteritems(mapping):\n      for (tag, content) in six.iteritems(tag_to_content):\n        if content != b'1':\n          logger.warn('Ignoring unrecognizable version of RunMetadata.')\n          continue\n        (_, tag_item) = add_row_item(run_name, tag)\n        tag_item['conceptual_graph'] = True\n\n    for (run_name, run_data) in six.iteritems(self._multiplexer.Runs()):\n      if run_data.get(event_accumulator.GRAPH):\n        (run_item, _) = add_row_item(run_name, None)\n        run_item['run_graph'] = True\n\n    for (run_name, run_data) in six.iteritems(self._multiplexer.Runs()):\n      if event_accumulator.RUN_METADATA in run_data:\n        for tag in run_data[event_accumulator.RUN_METADATA]:\n          (_, tag_item) = add_row_item(run_name, tag)\n          tag_item['profile'] = True\n\n    return result", "response": "Returns a dict of all runs and tags and their data availabilities."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the graph for the given run and tag.", "response": "def graph_impl(self, run, tag, is_conceptual, limit_attr_size=None, large_attrs_key=None):\n    \"\"\"Result of the form `(body, mime_type)`, or `None` if no graph exists.\"\"\"\n    if is_conceptual:\n      tensor_events = self._multiplexer.Tensors(run, tag)\n      # Take the first event if there are multiple events written from different\n      # steps.\n      keras_model_config = json.loads(tensor_events[0].tensor_proto.string_val[0])\n      graph = keras_util.keras_model_to_graph_def(keras_model_config)\n    elif tag:\n      tensor_events = self._multiplexer.Tensors(run, tag)\n      # Take the first event if there are multiple events written from different\n      # steps.\n      run_metadata = config_pb2.RunMetadata.FromString(\n          tensor_events[0].tensor_proto.string_val[0])\n      graph = graph_pb2.GraphDef()\n\n      for func_graph in run_metadata.function_graphs:\n        graph_util.combine_graph_defs(graph, func_graph.pre_optimization_graph)\n    else:\n      graph = self._multiplexer.Graph(run)\n\n    # This next line might raise a ValueError if the limit parameters\n    # are invalid (size is negative, size present but key absent, etc.).\n    process_graph.prepare_graph_for_ui(graph, limit_attr_size, large_attrs_key)\n    return (str(graph), 'text/x-protobuf')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the result of the form body mime_type or None if no data exists.", "response": "def run_metadata_impl(self, run, tag):\n    \"\"\"Result of the form `(body, mime_type)`, or `None` if no data exists.\"\"\"\n    try:\n      run_metadata = self._multiplexer.RunMetadata(run, tag)\n    except ValueError:\n      # TODO(stephanwlee): Should include whether FE is fetching for v1 or v2 RunMetadata\n      # so we can remove this try/except.\n      tensor_events = self._multiplexer.Tensors(run, tag)\n      if tensor_events is None:\n        return None\n      # Take the first event if there are multiple events written from different\n      # steps.\n      run_metadata = config_pb2.RunMetadata.FromString(\n          tensor_events[0].tensor_proto.string_val[0])\n    if run_metadata is None:\n      return None\n    return (str(run_metadata), 'text/x-protobuf')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef graph_route(self, request):\n    run = request.args.get('run')\n    tag = request.args.get('tag', '')\n    conceptual_arg = request.args.get('conceptual', False)\n    is_conceptual = True if conceptual_arg == 'true' else False\n\n    if run is None:\n      return http_util.Respond(\n          request, 'query parameter \"run\" is required', 'text/plain', 400)\n\n    limit_attr_size = request.args.get('limit_attr_size', None)\n    if limit_attr_size is not None:\n      try:\n        limit_attr_size = int(limit_attr_size)\n      except ValueError:\n        return http_util.Respond(\n            request, 'query parameter `limit_attr_size` must be an integer',\n            'text/plain', 400)\n\n    large_attrs_key = request.args.get('large_attrs_key', None)\n\n    try:\n      result = self.graph_impl(run, tag, is_conceptual, limit_attr_size, large_attrs_key)\n    except ValueError as e:\n      return http_util.Respond(request, e.message, 'text/plain', code=400)\n    else:\n      if result is not None:\n        (body, mime_type) = result  # pylint: disable=unpacking-non-sequence\n        return http_util.Respond(request, body, mime_type)\n      else:\n        return http_util.Respond(request, '404 Not Found', 'text/plain',\n                                 code=404)", "response": "Given a single run return the graph definition in protobuf format."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngive a tag and a run return the session. run() metadata.", "response": "def run_metadata_route(self, request):\n    \"\"\"Given a tag and a run, return the session.run() metadata.\"\"\"\n    tag = request.args.get('tag')\n    run = request.args.get('run')\n    if tag is None:\n      return http_util.Respond(\n          request, 'query parameter \"tag\" is required', 'text/plain', 400)\n    if run is None:\n      return http_util.Respond(\n          request, 'query parameter \"run\" is required', 'text/plain', 400)\n    result = self.run_metadata_impl(run, tag)\n    if result is not None:\n      (body, mime_type) = result  # pylint: disable=unpacking-non-sequence\n      return http_util.Respond(request, body, mime_type)\n    else:\n      return http_util.Respond(request, '404 Not Found', 'text/plain',\n                               code=404)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the plugin if possible.", "response": "def load(self, context):\n    \"\"\"Returns the plugin, if possible.\n\n    Args:\n      context: The TBContext flags.\n\n    Returns:\n      A ProfilePlugin instance or None if it couldn't be loaded.\n    \"\"\"\n    try:\n      # pylint: disable=g-import-not-at-top,unused-import\n      import tensorflow\n      # Available in TensorFlow 1.14 or later, so do import check\n      # pylint: disable=g-import-not-at-top,unused-import\n      from tensorflow.python.eager import profiler_client\n    except ImportError:\n      return\n    # pylint: disable=g-import-not-at-top\n    from tensorboard.plugins.profile.profile_plugin import ProfilePlugin\n    return ProfilePlugin(context)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a Keras model with the given hyperparameters.", "response": "def model_fn(hparams, seed):\n  \"\"\"Create a Keras model with the given hyperparameters.\n\n  Args:\n    hparams: A dict mapping hyperparameters in `HPARAMS` to values.\n    seed: A hashable object to be used as a random seed (e.g., to\n      construct dropout layers in the model).\n\n  Returns:\n    A compiled Keras model.\n  \"\"\"\n  rng = random.Random(seed)\n\n  model = tf.keras.models.Sequential()\n  model.add(tf.keras.layers.Input(INPUT_SHAPE))\n  model.add(tf.keras.layers.Reshape(INPUT_SHAPE + (1,)))  # grayscale channel\n\n  # Add convolutional layers.\n  conv_filters = 8\n  for _ in xrange(hparams[HP_CONV_LAYERS]):\n    model.add(tf.keras.layers.Conv2D(\n        filters=conv_filters,\n        kernel_size=hparams[HP_CONV_KERNEL_SIZE],\n        padding=\"same\",\n        activation=\"relu\",\n    ))\n    model.add(tf.keras.layers.MaxPool2D(pool_size=2, padding=\"same\"))\n    conv_filters *= 2\n\n  model.add(tf.keras.layers.Flatten())\n  model.add(tf.keras.layers.Dropout(hparams[HP_DROPOUT], seed=rng.random()))\n\n  # Add fully connected layers.\n  dense_neurons = 32\n  for _ in xrange(hparams[HP_DENSE_LAYERS]):\n    model.add(tf.keras.layers.Dense(dense_neurons, activation=\"relu\"))\n    dense_neurons *= 2\n\n  # Add the final output layer.\n  model.add(tf.keras.layers.Dense(OUTPUT_CLASSES, activation=\"softmax\"))\n\n  model.compile(\n      loss=\"sparse_categorical_crossentropy\",\n      optimizer=hparams[HP_OPTIMIZER],\n      metrics=[\"accuracy\"],\n  )\n  return model"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run(data, base_logdir, session_id, group_id, hparams):\n  model = model_fn(hparams=hparams, seed=session_id)\n  logdir = os.path.join(base_logdir, session_id)\n\n  callback = tf.keras.callbacks.TensorBoard(\n      logdir,\n      update_freq=flags.FLAGS.summary_freq,\n      profile_batch=0,  # workaround for issue #2084\n  )\n  hparams_callback = hp.KerasCallback(logdir, hparams, group_name=group_id)\n  ((x_train, y_train), (x_test, y_test)) = data\n  result = model.fit(\n      x=x_train,\n      y=y_train,\n      epochs=flags.FLAGS.num_epochs,\n      shuffle=False,\n      validation_data=(x_test, y_test),\n      callbacks=[callback, hparams_callback],\n  )", "response": "Run a training or validation session."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef prepare_data():\n  ((x_train, y_train), (x_test, y_test)) = DATASET.load_data()\n  x_train = x_train.astype(\"float32\")\n  x_test = x_test.astype(\"float32\")\n  x_train /= 255.0\n  x_test /= 255.0\n  return ((x_train, y_train), (x_test, y_test))", "response": "Load and normalize data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nperforming random search over the hyperparameter space.", "response": "def run_all(logdir, verbose=False):\n  \"\"\"Perform random search over the hyperparameter space.\n\n  Arguments:\n    logdir: The top-level directory into which to write data. This\n      directory should be empty or nonexistent.\n    verbose: If true, print out each run's name as it begins.\n  \"\"\"\n  data = prepare_data()\n  rng = random.Random(0)\n\n  base_writer = tf.summary.create_file_writer(logdir)\n  with base_writer.as_default():\n    experiment = hp.Experiment(hparams=HPARAMS, metrics=METRICS)\n    experiment_string = experiment.summary_pb().SerializeToString()\n    tf.summary.experimental.write_raw_pb(experiment_string, step=0)\n    base_writer.flush()\n  base_writer.close()\n\n  sessions_per_group = 2\n  num_sessions = flags.FLAGS.num_session_groups * sessions_per_group\n  session_index = 0  # across all session groups\n  for group_index in xrange(flags.FLAGS.num_session_groups):\n    hparams = {h: sample_uniform(h.domain, rng) for h in HPARAMS}\n    hparams_string = str(hparams)\n    group_id = hashlib.sha256(hparams_string.encode(\"utf-8\")).hexdigest()\n    for repeat_index in xrange(sessions_per_group):\n      session_id = str(session_index)\n      session_index += 1\n      if verbose:\n        print(\n            \"--- Running training session %d/%d\"\n            % (session_index, num_sessions)\n        )\n        print(hparams_string)\n        print(\"--- repeat #: %d\" % (repeat_index + 1))\n      run(\n          data=data,\n          base_logdir=logdir,\n          session_id=session_id,\n          group_id=group_id,\n          hparams=hparams,\n      )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsampling a value uniformly from a domain.", "response": "def sample_uniform(domain, rng):\n  \"\"\"Sample a value uniformly from a domain.\n\n  Args:\n    domain: An `IntInterval`, `RealInterval`, or `Discrete` domain.\n    rng: A `random.Random` object; defaults to the `random` module.\n\n  Raises:\n    TypeError: If `domain` is not a known kind of domain.\n    IndexError: If the domain is empty.\n  \"\"\"\n  if isinstance(domain, hp.IntInterval):\n    return rng.randint(domain.min_value, domain.max_value)\n  elif isinstance(domain, hp.RealInterval):\n    return rng.uniform(domain.min_value, domain.max_value)\n  elif isinstance(domain, hp.Discrete):\n    return rng.choice(domain.values)\n  else:\n    raise TypeError(\"unknown domain type: %r\" % (domain,))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pr_curves_route(self, request):\n    runs = request.args.getlist('run')\n    if not runs:\n      return http_util.Respond(\n          request, 'No runs provided when fetching PR curve data', 400)\n\n    tag = request.args.get('tag')\n    if not tag:\n      return http_util.Respond(\n          request, 'No tag provided when fetching PR curve data', 400)\n\n    try:\n      response = http_util.Respond(\n          request, self.pr_curves_impl(runs, tag), 'application/json')\n    except ValueError as e:\n      return http_util.Respond(request, str(e), 'text/plain', 400)\n\n    return response", "response": "A route that returns a JSON mapping between runs and PR curves data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef pr_curves_impl(self, runs, tag):\n    if self._db_connection_provider:\n      # Serve data from the database.\n      db = self._db_connection_provider()\n\n      # We select for steps greater than -1 because the writer inserts\n      # placeholder rows en masse. The check for step filters out those rows.\n      cursor = db.execute('''\n        SELECT\n          Runs.run_name,\n          Tensors.step,\n          Tensors.computed_time,\n          Tensors.data,\n          Tensors.dtype,\n          Tensors.shape,\n          Tags.plugin_data\n        FROM Tensors\n        JOIN Tags\n          ON Tensors.series = Tags.tag_id\n        JOIN Runs\n          ON Tags.run_id = Runs.run_id\n        WHERE\n          Runs.run_name IN (%s)\n          AND Tags.tag_name = ?\n          AND Tags.plugin_name = ?\n          AND Tensors.step > -1\n        ORDER BY Tensors.step\n      ''' % ','.join(['?'] * len(runs)), runs + [tag, metadata.PLUGIN_NAME])\n      response_mapping = {}\n      for (run, step, wall_time, data, dtype, shape, plugin_data) in cursor:\n        if run not in response_mapping:\n          response_mapping[run] = []\n        buf = np.frombuffer(data, dtype=tf.DType(dtype).as_numpy_dtype)\n        data_array = buf.reshape([int(i) for i in shape.split(',')])\n        plugin_data_proto = plugin_data_pb2.PrCurvePluginData()\n        string_buffer = np.frombuffer(plugin_data, dtype=np.dtype('b'))\n        plugin_data_proto.ParseFromString(tf.compat.as_bytes(\n            string_buffer.tostring()))\n        thresholds = self._compute_thresholds(plugin_data_proto.num_thresholds)\n        entry = self._make_pr_entry(step, wall_time, data_array, thresholds)\n        response_mapping[run].append(entry)\n    else:\n      # Serve data from events files.\n      response_mapping = {}\n      for run in runs:\n        try:\n          tensor_events = self._multiplexer.Tensors(run, tag)\n        except KeyError:\n          raise ValueError(\n              'No PR curves could be found for run %r and tag %r' % (run, tag))\n\n        content = self._multiplexer.SummaryMetadata(\n            run, tag).plugin_data.content\n        pr_curve_data = metadata.parse_plugin_metadata(content)\n        thresholds = self._compute_thresholds(pr_curve_data.num_thresholds)\n        response_mapping[run] = [\n            self._process_tensor_event(e, thresholds) for e in tensor_events]\n    return response_mapping", "response": "Creates the JSON object for the PR curves route response for a given run and tag."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef tags_impl(self):\n    if self._db_connection_provider:\n      # Read tags from the database.\n      db = self._db_connection_provider()\n      cursor = db.execute('''\n        SELECT\n          Tags.tag_name,\n          Tags.display_name,\n          Runs.run_name\n        FROM Tags\n        JOIN Runs\n          ON Tags.run_id = Runs.run_id\n        WHERE\n          Tags.plugin_name = ?\n      ''', (metadata.PLUGIN_NAME,))\n      result = {}\n      for (tag_name, display_name, run_name) in cursor:\n        if run_name not in result:\n          result[run_name] = {}\n        result[run_name][tag_name] = {\n            'displayName': display_name,\n            # TODO(chihuahua): Populate the description. Currently, the tags\n            # table does not link with the description table.\n            'description': '',\n        }\n    else:\n      # Read tags from events files.\n      runs = self._multiplexer.Runs()\n      result = {run: {} for run in runs}\n\n      mapping = self._multiplexer.PluginRunToTagToContent(metadata.PLUGIN_NAME)\n      for (run, tag_to_content) in six.iteritems(mapping):\n        for (tag, _) in six.iteritems(tag_to_content):\n          summary_metadata = self._multiplexer.SummaryMetadata(run, tag)\n          result[run][tag] = {'displayName': summary_metadata.display_name,\n                              'description': plugin_util.markdown_to_safe_html(\n                                  summary_metadata.summary_description)}\n\n    return result", "response": "Creates the JSON object for the tags route response."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef available_time_entries_impl(self):\n    result = {}\n    if self._db_connection_provider:\n      db = self._db_connection_provider()\n      # For each run, pick a tag.\n      cursor = db.execute(\n          '''\n          SELECT\n            TagPickingTable.run_name,\n            Tensors.step,\n            Tensors.computed_time\n          FROM (/* For each run, pick any tag. */\n            SELECT\n              Runs.run_id AS run_id,\n              Runs.run_name AS run_name,\n              Tags.tag_id AS tag_id\n            FROM Runs\n            JOIN Tags\n              ON Tags.run_id = Runs.run_id\n            WHERE\n              Tags.plugin_name = ?\n            GROUP BY Runs.run_id) AS TagPickingTable\n          JOIN Tensors\n            ON Tensors.series = TagPickingTable.tag_id\n          WHERE Tensors.step IS NOT NULL\n          ORDER BY Tensors.step\n          ''', (metadata.PLUGIN_NAME,))\n      for (run, step, wall_time) in cursor:\n        if run not in result:\n          result[run] = []\n        result[run].append(self._create_time_entry(step, wall_time))\n    else:\n      # Read data from disk.\n      all_runs = self._multiplexer.PluginRunToTagToContent(\n          metadata.PLUGIN_NAME)\n      for run, tag_to_content in all_runs.items():\n        if not tag_to_content:\n          # This run lacks data for this plugin.\n          continue\n        # Just use the list of tensor events for any of the tags to determine\n        # the steps to list for the run. The steps are often the same across\n        # tags for each run, albeit the user may elect to sample certain tags\n        # differently within the same run. If the latter occurs, TensorBoard\n        # will show the actual step of each tag atop the card for the tag.\n        tensor_events = self._multiplexer.Tensors(\n            run, min(six.iterkeys(tag_to_content)))\n        result[run] = [self._create_time_entry(e.step, e.wall_time)\n                       for e in tensor_events]\n    return result", "response": "Creates the JSON object for the available time entries route response."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndetermining whether this plugin is active.", "response": "def is_active(self):\n    \"\"\"Determines whether this plugin is active.\n\n    This plugin is active only if PR curve summary data is read by TensorBoard.\n\n    Returns:\n      Whether this plugin is active.\n    \"\"\"\n    if self._db_connection_provider:\n      # The plugin is active if one relevant tag can be found in the database.\n      db = self._db_connection_provider()\n      cursor = db.execute(\n          '''\n          SELECT 1\n          FROM Tags\n          WHERE Tags.plugin_name = ?\n          LIMIT 1\n          ''',\n          (metadata.PLUGIN_NAME,))\n      return bool(list(cursor))\n\n    if not self._multiplexer:\n      return False\n\n    all_runs = self._multiplexer.PluginRunToTagToContent(metadata.PLUGIN_NAME)\n\n    # The plugin is active if any of the runs has a tag relevant to the plugin.\n    return any(six.itervalues(all_runs))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _process_tensor_event(self, event, thresholds):\n    return self._make_pr_entry(\n        event.step,\n        event.wall_time,\n        tensor_util.make_ndarray(event.tensor_proto),\n        thresholds)", "response": "Converts a TensorEvent into a dictionary that encapsulates information on it."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating an entry for PR curve data.", "response": "def _make_pr_entry(self, step, wall_time, data_array, thresholds):\n    \"\"\"Creates an entry for PR curve data. Each entry corresponds to 1 step.\n\n    Args:\n      step: The step.\n      wall_time: The wall time.\n      data_array: A numpy array of PR curve data stored in the summary format.\n      thresholds: An array of floating point thresholds.\n\n    Returns:\n      A PR curve entry.\n    \"\"\"\n    # Trim entries for which TP + FP = 0 (precision is undefined) at the tail of\n    # the data.\n    true_positives = [int(v) for v in data_array[metadata.TRUE_POSITIVES_INDEX]]\n    false_positives = [\n        int(v) for v in data_array[metadata.FALSE_POSITIVES_INDEX]]\n    tp_index = metadata.TRUE_POSITIVES_INDEX\n    fp_index = metadata.FALSE_POSITIVES_INDEX\n    positives = data_array[[tp_index, fp_index], :].astype(int).sum(axis=0)\n    end_index_inclusive = len(positives) - 1\n    while end_index_inclusive > 0 and positives[end_index_inclusive] == 0:\n      end_index_inclusive -= 1\n    end_index = end_index_inclusive + 1\n\n    return {\n        'wall_time': wall_time,\n        'step': step,\n        'precision': data_array[metadata.PRECISION_INDEX, :end_index].tolist(),\n        'recall': data_array[metadata.RECALL_INDEX, :end_index].tolist(),\n        'true_positives': true_positives[:end_index],\n        'false_positives': false_positives[:end_index],\n        'true_negatives':\n            [int(v) for v in\n             data_array[metadata.TRUE_NEGATIVES_INDEX][:end_index]],\n        'false_negatives':\n            [int(v) for v in\n             data_array[metadata.FALSE_NEGATIVES_INDEX][:end_index]],\n        'thresholds': thresholds[:end_index],\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nnormalize a dict keyed by HParam s and raw strings.", "response": "def _normalize_hparams(hparams):\n  \"\"\"Normalize a dict keyed by `HParam`s and/or raw strings.\n\n  Args:\n    hparams: A `dict` whose keys are `HParam` objects and/or strings\n      representing hyperparameter names, and whose values are\n      hyperparameter values. No two keys may have the same name.\n\n  Returns:\n    A `dict` whose keys are hyperparameter names (as strings) and whose\n    values are the corresponding hyperparameter values.\n\n  Raises:\n    ValueError: If two entries in `hparams` share the same\n      hyperparameter name.\n  \"\"\"\n  result = {}\n  for (k, v) in six.iteritems(hparams):\n    if isinstance(k, HParam):\n      k = k.name\n    if k in result:\n      raise ValueError(\"multiple values specified for hparam %r\" % (k,))\n    result[k] = v\n  return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef summary_pb(self):\n    hparam_infos = []\n    for hparam in self._hparams:\n      info = api_pb2.HParamInfo(\n          name=hparam.name,\n          description=hparam.description,\n          display_name=hparam.display_name,\n      )\n      domain = hparam.domain\n      if domain is not None:\n        domain.update_hparam_info(info)\n      hparam_infos.append(info)\n    metric_infos = [metric.as_proto() for metric in self._metrics]\n    return summary.experiment_pb(\n        hparam_infos=hparam_infos,\n        metric_infos=metric_infos,\n        user=self._user,\n        description=self._description,\n        time_created_secs=self._time_created_secs,\n    )", "response": "Create a top - level experiment summary describing this experiment."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate a bunch of histogram data and write it to logdir.", "response": "def run_all(logdir, verbose=False, num_summaries=400):\n  \"\"\"Generate a bunch of histogram data, and write it to logdir.\"\"\"\n  del verbose\n\n  tf.compat.v1.set_random_seed(0)\n\n  k = tf.compat.v1.placeholder(tf.float32)\n\n  # Make a normal distribution, with a shifting mean\n  mean_moving_normal = tf.random.normal(shape=[1000], mean=(5*k), stddev=1)\n  # Record that distribution into a histogram summary\n  histogram_summary.op(\"normal/moving_mean\",\n                       mean_moving_normal,\n                       description=\"A normal distribution whose mean changes \"\n                                   \"over time.\")\n\n  # Make a normal distribution with shrinking variance\n  shrinking_normal = tf.random.normal(shape=[1000], mean=0, stddev=1-(k))\n  # Record that distribution too\n  histogram_summary.op(\"normal/shrinking_variance\", shrinking_normal,\n                       description=\"A normal distribution whose variance \"\n                                   \"shrinks over time.\")\n\n  # Let's combine both of those distributions into one dataset\n  normal_combined = tf.concat([mean_moving_normal, shrinking_normal], 0)\n  # We add another histogram summary to record the combined distribution\n  histogram_summary.op(\"normal/bimodal\", normal_combined,\n                       description=\"A combination of two normal distributions, \"\n                                   \"one with a moving mean and one with  \"\n                                   \"shrinking variance. The result is a \"\n                                   \"distribution that starts as unimodal and \"\n                                   \"becomes more and more bimodal over time.\")\n\n  # Add a gamma distribution\n  gamma = tf.random.gamma(shape=[1000], alpha=k)\n  histogram_summary.op(\"gamma\", gamma,\n                       description=\"A gamma distribution whose shape \"\n                                   \"parameter, \u03b1, changes over time.\")\n\n  # And a poisson distribution\n  poisson = tf.compat.v1.random_poisson(shape=[1000], lam=k)\n  histogram_summary.op(\"poisson\", poisson,\n                       description=\"A Poisson distribution, which only \"\n                                   \"takes on integer values.\")\n\n  # And a uniform distribution\n  uniform = tf.random.uniform(shape=[1000], maxval=k*10)\n  histogram_summary.op(\"uniform\", uniform,\n                       description=\"A simple uniform distribution.\")\n\n  # Finally, combine everything together!\n  all_distributions = [mean_moving_normal, shrinking_normal,\n                       gamma, poisson, uniform]\n  all_combined = tf.concat(all_distributions, 0)\n  histogram_summary.op(\"all_combined\", all_combined,\n                       description=\"An amalgamation of five distributions: a \"\n                                   \"uniform distribution, a gamma \"\n                                   \"distribution, a Poisson distribution, and \"\n                                   \"two normal distributions.\")\n\n  summaries = tf.compat.v1.summary.merge_all()\n\n  # Setup a session and summary writer\n  sess = tf.compat.v1.Session()\n  writer = tf.summary.FileWriter(logdir)\n\n  # Setup a loop and write the summaries to disk\n  N = num_summaries\n  for step in xrange(N):\n    k_val = step/float(N)\n    summ = sess.run(summaries, feed_dict={k: k_val})\n    writer.add_summary(summ, global_step=step)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses and asserts a positive integer query parameter.", "response": "def _parse_positive_int_param(request, param_name):\n  \"\"\"Parses and asserts a positive (>0) integer query parameter.\n\n  Args:\n    request: The Werkzeug Request object\n    param_name: Name of the parameter.\n\n  Returns:\n    Param, or None, or -1 if parameter is not a positive integer.\n  \"\"\"\n  param = request.args.get(param_name)\n  if not param:\n    return None\n  try:\n    param = int(param)\n    if param <= 0:\n      raise ValueError()\n    return param\n  except ValueError:\n    return -1"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndetermines whether this projector plugin is active.", "response": "def is_active(self):\n    \"\"\"Determines whether this plugin is active.\n\n    This plugin is only active if any run has an embedding.\n\n    Returns:\n      Whether any run has embedding data to show in the projector.\n    \"\"\"\n    if not self.multiplexer:\n      return False\n\n    if self._is_active:\n      # We have already determined that the projector plugin should be active.\n      # Do not re-compute that. We have no reason to later set this plugin to be\n      # inactive.\n      return True\n\n    if self._thread_for_determining_is_active:\n      # We are currently determining whether the plugin is active. Do not start\n      # a separate thread.\n      return self._is_active\n\n    # The plugin is currently not active. The frontend might check again later.\n    # For now, spin off a separate thread to determine whether the plugin is\n    # active.\n    new_thread = threading.Thread(\n        target=self._determine_is_active,\n        name='ProjectorPluginIsActiveThread')\n    self._thread_for_determining_is_active = new_thread\n    new_thread.start()\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef configs(self):\n    run_path_pairs = list(self.run_paths.items())\n    self._append_plugin_asset_directories(run_path_pairs)\n    # If there are no summary event files, the projector should still work,\n    # treating the `logdir` as the model checkpoint directory.\n    if not run_path_pairs:\n      run_path_pairs.append(('.', self.logdir))\n    if (self._run_paths_changed() or\n        _latest_checkpoints_changed(self._configs, run_path_pairs)):\n      self.readers = {}\n      self._configs, self.config_fpaths = self._read_latest_config_files(\n          run_path_pairs)\n      self._augment_configs_with_checkpoint_info()\n    return self._configs", "response": "Returns a map of run paths to ProjectorConfig protos."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef Reload(self):\n    logger.info('Beginning EventMultiplexer.Reload()')\n    self._reload_called = True\n    # Build a list so we're safe even if the list of accumulators is modified\n    # even while we're reloading.\n    with self._accumulators_mutex:\n      items = list(self._accumulators.items())\n\n    names_to_delete = set()\n    for name, accumulator in items:\n      try:\n        accumulator.Reload()\n      except (OSError, IOError) as e:\n        logger.error(\"Unable to reload accumulator '%s': %s\", name, e)\n      except directory_watcher.DirectoryDeletedError:\n        names_to_delete.add(name)\n\n    with self._accumulators_mutex:\n      for name in names_to_delete:\n        logger.warn(\"Deleting accumulator '%s'\", name)\n        del self._accumulators[name]\n    logger.info('Finished with EventMultiplexer.Reload()')\n    return self", "response": "Call Reload on every EventAccumulator."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef Histograms(self, run, tag):\n    accumulator = self.GetAccumulator(run)\n    return accumulator.Histograms(tag)", "response": "Retrieves the histogram events associated with a run and tag."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nretrieve the compressed histogram events associated with a run and tag.", "response": "def CompressedHistograms(self, run, tag):\n    \"\"\"Retrieve the compressed histogram events associated with a run and tag.\n\n    Args:\n      run: A string name of the run for which values are retrieved.\n      tag: A string name of the tag for which values are retrieved.\n\n    Raises:\n      KeyError: If the run is not found, or the tag is not available for\n        the given run.\n\n    Returns:\n      An array of `event_accumulator.CompressedHistogramEvents`.\n    \"\"\"\n    accumulator = self.GetAccumulator(run)\n    return accumulator.CompressedHistograms(tag)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nretrieves the image events associated with a run and tag.", "response": "def Images(self, run, tag):\n    \"\"\"Retrieve the image events associated with a run and tag.\n\n    Args:\n      run: A string name of the run for which values are retrieved.\n      tag: A string name of the tag for which values are retrieved.\n\n    Raises:\n      KeyError: If the run is not found, or the tag is not available for\n        the given run.\n\n    Returns:\n      An array of `event_accumulator.ImageEvents`.\n    \"\"\"\n    accumulator = self.GetAccumulator(run)\n    return accumulator.Images(tag)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwrites a histogram summary.", "response": "def histogram(name, data, step=None, buckets=None, description=None):\n  \"\"\"Write a histogram summary.\n\n  Arguments:\n    name: A name for this summary. The summary tag used for TensorBoard will\n      be this name prefixed by any active name scopes.\n    data: A `Tensor` of any shape. Must be castable to `float64`.\n    step: Explicit `int64`-castable monotonic step value for this summary. If\n      omitted, this defaults to `tf.summary.experimental.get_step()`, which must\n      not be None.\n    buckets: Optional positive `int`. The output will have this\n      many buckets, except in two edge cases. If there is no data, then\n      there are no buckets. If there is data but all points have the\n      same value, then there is one bucket whose left and right\n      endpoints are the same.\n    description: Optional long-form description for this summary, as a\n      constant `str`. Markdown is supported. Defaults to empty.\n\n  Returns:\n    True on success, or false if no summary was emitted because no default\n    summary writer was available.\n\n  Raises:\n    ValueError: if a default writer exists, but no step was provided and\n      `tf.summary.experimental.get_step()` is None.\n  \"\"\"\n  summary_metadata = metadata.create_summary_metadata(\n      display_name=None, description=description)\n  # TODO(https://github.com/tensorflow/tensorboard/issues/2109): remove fallback\n  summary_scope = (\n      getattr(tf.summary.experimental, 'summary_scope', None) or\n      tf.summary.summary_scope)\n  with summary_scope(\n      name, 'histogram_summary', values=[data, buckets, step]) as (tag, _):\n    tensor = _buckets(data, bucket_count=buckets)\n    return tf.summary.write(\n        tag=tag, tensor=tensor, step=step, metadata=summary_metadata)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef histogram_pb(tag, data, buckets=None, description=None):\n  bucket_count = DEFAULT_BUCKET_COUNT if buckets is None else buckets\n  data = np.array(data).flatten().astype(float)\n  if data.size == 0:\n    buckets = np.array([]).reshape((0, 3))\n  else:\n    min_ = np.min(data)\n    max_ = np.max(data)\n    range_ = max_ - min_\n    if range_ == 0:\n      center = min_\n      buckets = np.array([[center - 0.5, center + 0.5, float(data.size)]])\n    else:\n      bucket_width = range_ / bucket_count\n      offsets = data - min_\n      bucket_indices = np.floor(offsets / bucket_width).astype(int)\n      clamped_indices = np.minimum(bucket_indices, bucket_count - 1)\n      one_hots = (np.array([clamped_indices]).transpose()\n                  == np.arange(0, bucket_count))  # broadcast\n      assert one_hots.shape == (data.size, bucket_count), (\n          one_hots.shape, (data.size, bucket_count))\n      bucket_counts = np.sum(one_hots, axis=0)\n      edges = np.linspace(min_, max_, bucket_count + 1)\n      left_edges = edges[:-1]\n      right_edges = edges[1:]\n      buckets = np.array([left_edges, right_edges, bucket_counts]).transpose()\n  tensor = tensor_util.make_tensor_proto(buckets, dtype=np.float64)\n\n  summary_metadata = metadata.create_summary_metadata(\n      display_name=None, description=description)\n  summary = summary_pb2.Summary()\n  summary.value.add(tag=tag,\n                    metadata=summary_metadata,\n                    tensor=tensor)\n  return summary", "response": "Create a histogram summary protobuf."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmake recommended modifications to the environment.", "response": "def setup_environment():\n  \"\"\"Makes recommended modifications to the environment.\n\n  This functions changes global state in the Python process. Calling\n  this function is a good idea, but it can't appropriately be called\n  from library routines.\n  \"\"\"\n  absl.logging.set_verbosity(absl.logging.WARNING)\n\n  # The default is HTTP/1.0 for some strange reason. If we don't use\n  # HTTP/1.1 then a new TCP socket and Python thread is created for\n  # each HTTP request. The tradeoff is we must always specify the\n  # Content-Length header, or do chunked encoding for streaming.\n  serving.WSGIRequestHandler.protocol_version = 'HTTP/1.1'"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a function that returns a newly opened file handle to zip file containing static assets for stock TensorBoard.", "response": "def get_default_assets_zip_provider():\n  \"\"\"Opens stock TensorBoard web assets collection.\n\n  Returns:\n    Returns function that returns a newly opened file handle to zip file\n    containing static assets for stock TensorBoard, or None if webfiles.zip\n    could not be found. The value the callback returns must be closed. The\n    paths inside the zip file are considered absolute paths on the web server.\n  \"\"\"\n  path = os.path.join(os.path.dirname(inspect.getfile(sys._getframe(1))),\n                      'webfiles.zip')\n  if not os.path.exists(path):\n    logger.warning('webfiles.zip static assets not found: %s', path)\n    return None\n  return lambda: open(path, 'rb')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a server factory that performs port scanning.", "response": "def with_port_scanning(cls):\n  \"\"\"Create a server factory that performs port scanning.\n\n  This function returns a callable whose signature matches the\n  specification of `TensorBoardServer.__init__`, using `cls` as an\n  underlying implementation. It passes through `flags` unchanged except\n  in the case that `flags.port is None`, in which case it repeatedly\n  instantiates the underlying server with new port suggestions.\n\n  Args:\n    cls: A valid implementation of `TensorBoardServer`. This class's\n      initializer should raise a `TensorBoardPortInUseError` upon\n      failing to bind to a port when it is expected that binding to\n      another nearby port might succeed.\n\n      The initializer for `cls` will only ever be invoked with `flags`\n      such that `flags.port is not None`.\n\n  Returns:\n    A function that implements the `__init__` contract of\n    `TensorBoardServer`.\n  \"\"\"\n\n  def init(wsgi_app, flags):\n    # base_port: what's the first port to which we should try to bind?\n    # should_scan: if that fails, shall we try additional ports?\n    # max_attempts: how many ports shall we try?\n    should_scan = flags.port is None\n    base_port = core_plugin.DEFAULT_PORT if flags.port is None else flags.port\n    max_attempts = 10 if should_scan else 1\n\n    if base_port > 0xFFFF:\n      raise TensorBoardServerException(\n          'TensorBoard cannot bind to port %d > %d' % (base_port, 0xFFFF)\n      )\n    max_attempts = 10 if should_scan else 1\n    base_port = min(base_port + max_attempts, 0x10000) - max_attempts\n\n    for port in xrange(base_port, base_port + max_attempts):\n      subflags = argparse.Namespace(**vars(flags))\n      subflags.port = port\n      try:\n        return cls(wsgi_app=wsgi_app, flags=subflags)\n      except TensorBoardPortInUseError:\n        if not should_scan:\n          raise\n    # All attempts failed to bind.\n    raise TensorBoardServerException(\n        'TensorBoard could not bind to any port around %s '\n        '(tried %d times)'\n        % (base_port, max_attempts))\n\n  return init"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef configure(self, argv=('',), **kwargs):\n    parser = argparse_flags.ArgumentParser(\n        prog='tensorboard',\n        description=('TensorBoard is a suite of web applications for '\n                     'inspecting and understanding your TensorFlow runs '\n                     'and graphs. https://github.com/tensorflow/tensorboard '))\n    for loader in self.plugin_loaders:\n      loader.define_flags(parser)\n    arg0 = argv[0] if argv else ''\n    flags = parser.parse_args(argv[1:])  # Strip binary name from argv.\n    self.cache_key = manager.cache_key(\n        working_directory=os.getcwd(),\n        arguments=argv[1:],\n        configure_kwargs=kwargs,\n    )\n    if absl_flags and arg0:\n      # Only expose main module Abseil flags as TensorBoard native flags.\n      # This is the same logic Abseil's ArgumentParser uses for determining\n      # which Abseil flags to include in the short helpstring.\n      for flag in set(absl_flags.FLAGS.get_key_flags_for_module(arg0)):\n        if hasattr(flags, flag.name):\n          raise ValueError('Conflicting Abseil flag: %s' % flag.name)\n        setattr(flags, flag.name, flag.value)\n    for k, v in kwargs.items():\n      if not hasattr(flags, k):\n        raise ValueError('Unknown TensorBoard flag: %s' % k)\n      setattr(flags, k, v)\n    for loader in self.plugin_loaders:\n      loader.fix_flags(flags)\n    self.flags = flags\n    return [arg0]", "response": "Configures TensorBoard behavior via flags."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nblocks main function for TensorBoard.", "response": "def main(self, ignored_argv=('',)):\n    \"\"\"Blocking main function for TensorBoard.\n\n    This method is called by `tensorboard.main.run_main`, which is the\n    standard entrypoint for the tensorboard command line program. The\n    configure() method must be called first.\n\n    Args:\n      ignored_argv: Do not pass. Required for Abseil compatibility.\n\n    Returns:\n      Process exit code, i.e. 0 if successful or non-zero on failure. In\n      practice, an exception will most likely be raised instead of\n      returning non-zero.\n\n    :rtype: int\n    \"\"\"\n    self._install_signal_handler(signal.SIGTERM, \"SIGTERM\")\n    if self.flags.inspect:\n      logger.info('Not bringing up TensorBoard, but inspecting event files.')\n      event_file = os.path.expanduser(self.flags.event_file)\n      efi.inspect(self.flags.logdir, event_file, self.flags.tag)\n      return 0\n    if self.flags.version_tb:\n      print(version.VERSION)\n      return 0\n    try:\n      server = self._make_server()\n      sys.stderr.write('TensorBoard %s at %s (Press CTRL+C to quit)\\n' %\n                       (version.VERSION, server.get_url()))\n      sys.stderr.flush()\n      self._register_info(server)\n      server.serve_forever()\n      return 0\n    except TensorBoardServerException as e:\n      logger.error(e.msg)\n      sys.stderr.write('ERROR: %s\\n' % e.msg)\n      sys.stderr.flush()\n      return -1"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef launch(self):\n    # Make it easy to run TensorBoard inside other programs, e.g. Colab.\n    server = self._make_server()\n    thread = threading.Thread(target=server.serve_forever, name='TensorBoard')\n    thread.daemon = True\n    thread.start()\n    return server.get_url()", "response": "This method is used to launch a TensorBoard web server. It is intended to be used by other programs to run TensorBoard inside other programs."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _register_info(self, server):\n    server_url = urllib.parse.urlparse(server.get_url())\n    info = manager.TensorBoardInfo(\n        version=version.VERSION,\n        start_time=int(time.time()),\n        port=server_url.port,\n        pid=os.getpid(),\n        path_prefix=self.flags.path_prefix,\n        logdir=self.flags.logdir,\n        db=self.flags.db,\n        cache_key=self.cache_key,\n    )\n    atexit.register(manager.remove_info_file)\n    manager.write_info_file(info)", "response": "Write a TensorBoardInfo file and arrange for its cleanup."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _install_signal_handler(self, signal_number, signal_name):\n    old_signal_handler = None  # set below\n    def handler(handled_signal_number, frame):\n      # In case we catch this signal again while running atexit\n      # handlers, take the hint and actually die.\n      signal.signal(signal_number, signal.SIG_DFL)\n      sys.stderr.write(\"TensorBoard caught %s; exiting...\\n\" % signal_name)\n      # The main thread is the only non-daemon thread, so it suffices to\n      # exit hence.\n      if old_signal_handler not in (signal.SIG_IGN, signal.SIG_DFL):\n        old_signal_handler(handled_signal_number, frame)\n      sys.exit(0)\n    old_signal_handler = signal.signal(signal_number, handler)", "response": "Installs a signal handler to gracefully exit on the given signal."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconstruct the TensorBoard WSGI app and instantiates the server.", "response": "def _make_server(self):\n    \"\"\"Constructs the TensorBoard WSGI app and instantiates the server.\"\"\"\n    app = application.standard_tensorboard_wsgi(self.flags,\n                                                self.plugin_loaders,\n                                                self.assets_zip_provider)\n    return self.server_class(app, self.flags)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_wildcard_address(self, port):\n    fallback_address = '::' if socket.has_ipv6 else '0.0.0.0'\n    if hasattr(socket, 'AI_PASSIVE'):\n      try:\n        addrinfos = socket.getaddrinfo(None, port, socket.AF_UNSPEC,\n                                       socket.SOCK_STREAM, socket.IPPROTO_TCP,\n                                       socket.AI_PASSIVE)\n      except socket.gaierror as e:\n        logger.warn('Failed to auto-detect wildcard address, assuming %s: %s',\n                    fallback_address, str(e))\n        return fallback_address\n      addrs_by_family = defaultdict(list)\n      for family, _, _, _, sockaddr in addrinfos:\n        # Format of the \"sockaddr\" socket address varies by address family,\n        # but [0] is always the IP address portion.\n        addrs_by_family[family].append(sockaddr[0])\n      if hasattr(socket, 'AF_INET6') and addrs_by_family[socket.AF_INET6]:\n        return addrs_by_family[socket.AF_INET6][0]\n      if hasattr(socket, 'AF_INET') and addrs_by_family[socket.AF_INET]:\n        return addrs_by_family[socket.AF_INET][0]\n    logger.warn('Failed to auto-detect wildcard address, assuming %s',\n                fallback_address)\n    return fallback_address", "response": "Returns a wildcard address for the given port."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef server_bind(self):\n    socket_is_v6 = (\n        hasattr(socket, 'AF_INET6') and self.socket.family == socket.AF_INET6)\n    has_v6only_option = (\n        hasattr(socket, 'IPPROTO_IPV6') and hasattr(socket, 'IPV6_V6ONLY'))\n    if self._auto_wildcard and socket_is_v6 and has_v6only_option:\n      try:\n        self.socket.setsockopt(socket.IPPROTO_IPV6, socket.IPV6_V6ONLY, 0)\n      except socket.error as e:\n        # Log a warning on failure to dual-bind, except for EAFNOSUPPORT\n        # since that's expected if IPv4 isn't supported at all (IPv6-only).\n        if hasattr(errno, 'EAFNOSUPPORT') and e.errno != errno.EAFNOSUPPORT:\n          logger.warn('Failed to dual-bind to IPv4 wildcard: %s', str(e))\n    super(WerkzeugServer, self).server_bind()", "response": "Override to enable IPV4 mapping for IPV6 sockets when desired."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef handle_error(self, request, client_address):\n    del request  # unused\n    # Kludge to override a SocketServer.py method so we can get rid of noisy\n    # EPIPE errors. They're kind of a red herring as far as errors go. For\n    # example, `curl -N http://localhost:6006/ | head` will cause an EPIPE.\n    exc_info = sys.exc_info()\n    e = exc_info[1]\n    if isinstance(e, IOError) and e.errno == errno.EPIPE:\n      logger.warn('EPIPE caused by %s in HTTP serving' % str(client_address))\n    else:\n      logger.error('HTTP serving error', exc_info=exc_info)", "response": "Override to get rid of noisy EPIPE errors."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _event(self, event):\n    result = dict(\n        pid=event.device_id,\n        tid=event.resource_id,\n        name=event.name,\n        ts=event.timestamp_ps / 1000000.0)\n    if event.duration_ps:\n      result['ph'] = _TYPE_COMPLETE\n      result['dur'] = event.duration_ps / 1000000.0\n    else:\n      result['ph'] = _TYPE_INSTANT\n      result['s'] = _SCOPE_THREAD\n    for key in dict(event.args):\n      if 'args' not in result:\n        result['args'] = {}\n      result['args'][key] = event.args[key]\n    return result", "response": "Converts a TraceEvent proto into a catapult trace event python value."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a legacy scalar summary op.", "response": "def op(name,\n       data,\n       display_name=None,\n       description=None,\n       collections=None):\n  \"\"\"Create a legacy scalar summary op.\n\n  Arguments:\n    name: A unique name for the generated summary node.\n    data: A real numeric rank-0 `Tensor`. Must have `dtype` castable\n      to `float32`.\n    display_name: Optional name for this summary in TensorBoard, as a\n      constant `str`. Defaults to `name`.\n    description: Optional long-form description for this summary, as a\n      constant `str`. Markdown is supported. Defaults to empty.\n    collections: Optional list of graph collections keys. The new\n      summary op is added to these collections. Defaults to\n      `[Graph Keys.SUMMARIES]`.\n\n  Returns:\n    A TensorFlow summary op.\n  \"\"\"\n  # TODO(nickfelt): remove on-demand imports once dep situation is fixed.\n  import tensorflow.compat.v1 as tf\n\n  if display_name is None:\n    display_name = name\n  summary_metadata = metadata.create_summary_metadata(\n      display_name=display_name, description=description)\n  with tf.name_scope(name):\n    with tf.control_dependencies([tf.assert_scalar(data)]):\n      return tf.summary.tensor_summary(name='scalar_summary',\n                                       tensor=tf.cast(data, tf.float32),\n                                       collections=collections,\n                                       summary_metadata=summary_metadata)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef pb(name, data, display_name=None, description=None):\n  # TODO(nickfelt): remove on-demand imports once dep situation is fixed.\n  import tensorflow.compat.v1 as tf\n\n  data = np.array(data)\n  if data.shape != ():\n    raise ValueError('Expected scalar shape for data, saw shape: %s.'\n                     % data.shape)\n  if data.dtype.kind not in ('b', 'i', 'u', 'f'):  # bool, int, uint, float\n    raise ValueError('Cast %s to float is not supported' % data.dtype.name)\n  tensor = tf.make_tensor_proto(data.astype(np.float32))\n\n  if display_name is None:\n    display_name = name\n  summary_metadata = metadata.create_summary_metadata(\n      display_name=display_name, description=description)\n  tf_summary_metadata = tf.SummaryMetadata.FromString(\n      summary_metadata.SerializeToString())\n  summary = tf.Summary()\n  summary.value.add(tag='%s/scalar_summary' % name,\n                    metadata=tf_summary_metadata,\n                    tensor=tensor)\n  return summary", "response": "Create a legacy scalar summary protobuf."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating temp symlink tree runs program and copies back outputs.", "response": "def run(inputs, program, outputs):\n  \"\"\"Creates temp symlink tree, runs program, and copies back outputs.\n\n  Args:\n    inputs: List of fake paths to real paths, which are used for symlink tree.\n    program: List containing real path of program and its arguments. The\n        execroot directory will be appended as the last argument.\n    outputs: List of fake outputted paths to copy back to real paths.\n  Returns:\n    0 if succeeded or nonzero if failed.\n  \"\"\"\n  root = tempfile.mkdtemp()\n  try:\n    cwd = os.getcwd()\n    for fake, real in inputs:\n      parent = os.path.join(root, os.path.dirname(fake))\n      if not os.path.exists(parent):\n        os.makedirs(parent)\n      # Use symlink if possible and not on Windows, since on Windows 10\n      # symlinks exist but they require administrator privileges to use.\n      if hasattr(os, 'symlink') and not os.name == 'nt':\n        os.symlink(os.path.join(cwd, real), os.path.join(root, fake))\n      else:\n        shutil.copyfile(os.path.join(cwd, real), os.path.join(root, fake))\n    if subprocess.call(program + [root]) != 0:\n      return 1\n    for fake, real in outputs:\n      shutil.copyfile(os.path.join(root, fake), real)\n    return 0\n  finally:\n    try:\n      shutil.rmtree(root)\n    except EnvironmentError:\n      # Ignore \"file in use\" errors on Windows; ok since it's just a tmpdir.\n      pass"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninvoke run function using a JSON file config. Args: args: CLI args, which can be a JSON file containing an object whose attributes are the parameters to the run function. If multiple JSON files are passed, their contents are concatenated. Returns: 0 if succeeded or nonzero if failed. Raises: Exception: If input data is missing.", "response": "def main(args):\n  \"\"\"Invokes run function using a JSON file config.\n\n  Args:\n    args: CLI args, which can be a JSON file containing an object whose\n        attributes are the parameters to the run function. If multiple JSON\n        files are passed, their contents are concatenated.\n  Returns:\n    0 if succeeded or nonzero if failed.\n  Raises:\n    Exception: If input data is missing.\n  \"\"\"\n  if not args:\n    raise Exception('Please specify at least one JSON config path')\n  inputs = []\n  program = []\n  outputs = []\n  for arg in args:\n    with open(arg) as fd:\n      config = json.load(fd)\n    inputs.extend(config.get('inputs', []))\n    program.extend(config.get('program', []))\n    outputs.extend(config.get('outputs', []))\n  if not program:\n    raise Exception('Please specify a program')\n  return run(inputs, program, outputs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef initialize_schema(connection):\n  cursor = connection.cursor()\n  cursor.execute(\"PRAGMA application_id={}\".format(_TENSORBOARD_APPLICATION_ID))\n  cursor.execute(\"PRAGMA user_version={}\".format(_TENSORBOARD_USER_VERSION))\n  with connection:\n    for statement in _SCHEMA_STATEMENTS:\n      lines = statement.strip('\\n').split('\\n')\n      message = lines[0] + ('...' if len(lines) > 1 else '')\n      logger.debug('Running DB init statement: %s', message)\n      cursor.execute(statement)", "response": "Initializes TensorBoard sqlite schema using the given connection."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a freshly created DB - wide unique ID.", "response": "def _create_id(self):\n    \"\"\"Returns a freshly created DB-wide unique ID.\"\"\"\n    cursor = self._db.cursor()\n    cursor.execute('INSERT INTO Ids DEFAULT VALUES')\n    return cursor.lastrowid"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the ID for the current user creating the row if needed.", "response": "def _maybe_init_user(self):\n    \"\"\"Returns the ID for the current user, creating the row if needed.\"\"\"\n    user_name = os.environ.get('USER', '') or os.environ.get('USERNAME', '')\n    cursor = self._db.cursor()\n    cursor.execute('SELECT user_id FROM Users WHERE user_name = ?',\n                   (user_name,))\n    row = cursor.fetchone()\n    if row:\n      return row[0]\n    user_id = self._create_id()\n    cursor.execute(\n        \"\"\"\n        INSERT INTO USERS (user_id, user_name, inserted_time)\n        VALUES (?, ?, ?)\n        \"\"\",\n        (user_id, user_name, time.time()))\n    return user_id"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the ID for the given experiment creating the row if needed.", "response": "def _maybe_init_experiment(self, experiment_name):\n    \"\"\"Returns the ID for the given experiment, creating the row if needed.\n\n    Args:\n      experiment_name: name of experiment.\n    \"\"\"\n    user_id = self._maybe_init_user()\n    cursor = self._db.cursor()\n    cursor.execute(\n        \"\"\"\n        SELECT experiment_id FROM Experiments\n        WHERE user_id = ? AND experiment_name = ?\n        \"\"\",\n        (user_id, experiment_name))\n    row = cursor.fetchone()\n    if row:\n      return row[0]\n    experiment_id = self._create_id()\n    # TODO: track computed time from run start times\n    computed_time = 0\n    cursor.execute(\n        \"\"\"\n        INSERT INTO Experiments (\n          user_id, experiment_id, experiment_name,\n          inserted_time, started_time, is_watching\n        ) VALUES (?, ?, ?, ?, ?, ?)\n        \"\"\",\n        (user_id, experiment_id, experiment_name, time.time(), computed_time,\n         False))\n    return experiment_id"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _maybe_init_run(self, experiment_name, run_name):\n    experiment_id = self._maybe_init_experiment(experiment_name)\n    cursor = self._db.cursor()\n    cursor.execute(\n        \"\"\"\n        SELECT run_id FROM Runs\n        WHERE experiment_id = ? AND run_name = ?\n        \"\"\",\n        (experiment_id, run_name))\n    row = cursor.fetchone()\n    if row:\n      return row[0]\n    run_id = self._create_id()\n    # TODO: track actual run start times\n    started_time = 0\n    cursor.execute(\n        \"\"\"\n        INSERT INTO Runs (\n          experiment_id, run_id, run_name, inserted_time, started_time\n        ) VALUES (?, ?, ?, ?, ?)\n        \"\"\",\n        (experiment_id, run_id, run_name, time.time(), started_time))\n    return run_id", "response": "Returns the ID for the given run creating the row if needed."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _maybe_init_tags(self, run_id, tag_to_metadata):\n    cursor = self._db.cursor()\n    # TODO: for huge numbers of tags (e.g. 1000+), this is slower than just\n    # querying for the known tag names explicitly; find a better tradeoff.\n    cursor.execute('SELECT tag_name, tag_id FROM Tags WHERE run_id = ?',\n                   (run_id,))\n    tag_to_id = {row[0]: row[1] for row in cursor.fetchall()\n                 if row[0] in tag_to_metadata}\n    new_tag_data = []\n    for tag, metadata in six.iteritems(tag_to_metadata):\n      if tag not in tag_to_id:\n        tag_id = self._create_id()\n        tag_to_id[tag] = tag_id\n        new_tag_data.append((run_id, tag_id, tag, time.time(),\n                             metadata.display_name,\n                             metadata.plugin_data.plugin_name,\n                             self._make_blob(metadata.plugin_data.content)))\n    cursor.executemany(\n        \"\"\"\n        INSERT INTO Tags (\n          run_id, tag_id, tag_name, inserted_time, display_name, plugin_name,\n          plugin_data\n        ) VALUES (?, ?, ?, ?, ?, ?, ?)\n        \"\"\",\n        new_tag_data)\n    return tag_to_id", "response": "Returns a tag - to - ID map for the given tags creating rows if needed."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the raw encoded image data downloading it if necessary.", "response": "def image_data(verbose=False):\n  \"\"\"Get the raw encoded image data, downloading it if necessary.\"\"\"\n  # This is a principled use of the `global` statement; don't lint me.\n  global _IMAGE_DATA  # pylint: disable=global-statement\n  if _IMAGE_DATA is None:\n    if verbose:\n      logger.info(\"--- Downloading image.\")\n    with contextlib.closing(urllib.request.urlopen(IMAGE_URL)) as infile:\n      _IMAGE_DATA = infile.read()\n  return _IMAGE_DATA"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nperforms a 2D pixel convolution on the given image.", "response": "def convolve(image, pixel_filter, channels=3, name=None):\n  \"\"\"Perform a 2D pixel convolution on the given image.\n\n  Arguments:\n    image: A 3D `float32` `Tensor` of shape `[height, width, channels]`,\n      where `channels` is the third argument to this function and the\n      first two dimensions are arbitrary.\n    pixel_filter: A 2D `Tensor`, representing pixel weightings for the\n      kernel. This will be used to create a 4D kernel---the extra two\n      dimensions are for channels (see `tf.nn.conv2d` documentation),\n      and the kernel will be constructed so that the channels are\n      independent: each channel only observes the data from neighboring\n      pixels of the same channel.\n    channels: An integer representing the number of channels in the\n      image (e.g., 3 for RGB).\n\n  Returns:\n    A 3D `float32` `Tensor` of the same shape as the input.\n  \"\"\"\n  with tf.name_scope(name, 'convolve'):\n    tf.compat.v1.assert_type(image, tf.float32)\n    channel_filter = tf.eye(channels)\n    filter_ = (tf.expand_dims(tf.expand_dims(pixel_filter, -1), -1) *\n               tf.expand_dims(tf.expand_dims(channel_filter, 0), 0))\n    result_batch = tf.nn.conv2d(tf.stack([image]),  # batch\n                                filter=filter_,\n                                strides=[1, 1, 1, 1],\n                                padding='SAME')\n    return result_batch[0]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_image(verbose=False):\n  base_data = tf.constant(image_data(verbose=verbose))\n  base_image = tf.image.decode_image(base_data, channels=3)\n  base_image.set_shape((IMAGE_HEIGHT, IMAGE_WIDTH, 3))\n  parsed_image = tf.Variable(base_image, name='image', dtype=tf.uint8)\n  return parsed_image", "response": "Get the image as a TensorFlow variable."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrunning a box - to - Gaussian - blur demonstration.", "response": "def run_box_to_gaussian(logdir, verbose=False):\n  \"\"\"Run a box-blur-to-Gaussian-blur demonstration.\n\n  See the summary description for more details.\n\n  Arguments:\n    logdir: Directory into which to write event logs.\n    verbose: Boolean; whether to log any output.\n  \"\"\"\n  if verbose:\n    logger.info('--- Starting run: box_to_gaussian')\n\n  tf.compat.v1.reset_default_graph()\n  tf.compat.v1.set_random_seed(0)\n\n  image = get_image(verbose=verbose)\n  blur_radius = tf.compat.v1.placeholder(shape=(), dtype=tf.int32)\n  with tf.name_scope('filter'):\n    blur_side_length = blur_radius * 2 + 1\n    pixel_filter = tf.ones((blur_side_length, blur_side_length))\n    pixel_filter = (pixel_filter\n                    / tf.cast(tf.size(input=pixel_filter), tf.float32))  # normalize\n\n  iterations = 4\n  images = [tf.cast(image, tf.float32) / 255.0]\n  for _ in xrange(iterations):\n    images.append(convolve(images[-1], pixel_filter))\n  with tf.name_scope('convert_to_uint8'):\n    images = tf.stack(\n        [tf.cast(255 * tf.clip_by_value(image_, 0.0, 1.0), tf.uint8)\n         for image_ in images])\n\n  summ = image_summary.op(\n      'box_to_gaussian', images, max_outputs=iterations,\n      display_name='Gaussian blur as a limit process of box blurs',\n      description=('Demonstration of forming a Gaussian blur by '\n                   'composing box blurs, each of which can be expressed '\n                   'as a 2D convolution.\\n\\n'\n                   'A Gaussian blur is formed by convolving a Gaussian '\n                   'kernel over an image. But a Gaussian kernel is '\n                   'itself the limit of convolving a constant kernel '\n                   'with itself many times. Thus, while applying '\n                   'a box-filter convolution just once produces '\n                   'results that are noticeably different from those '\n                   'of a Gaussian blur, repeating the same convolution '\n                   'just a few times causes the result to rapidly '\n                   'converge to an actual Gaussian blur.\\n\\n'\n                   'Here, the step value controls the blur radius, '\n                   'and the image sample controls the number of times '\n                   'that the convolution is applied (plus one). '\n                   'So, when *sample*=1, the original image is shown; '\n                   '*sample*=2 shows a box blur; and a hypothetical '\n                   '*sample*=&infin; would show a true Gaussian blur.\\n\\n'\n                   'This is one ingredient in a recipe to compute very '\n                   'fast Gaussian blurs. The other pieces require '\n                   'special treatment for the box blurs themselves '\n                   '(decomposition to dual one-dimensional box blurs, '\n                   'each of which is computed with a sliding window); '\n                   'we don&rsquo;t perform those optimizations here.\\n\\n'\n                   '[Here are some slides describing the full process.]'\n                   '(%s)\\n\\n'\n                   '%s'\n                   % ('http://elynxsdk.free.fr/ext-docs/Blur/Fast_box_blur.pdf',\n                      IMAGE_CREDIT)))\n\n  with tf.compat.v1.Session() as sess:\n    sess.run(image.initializer)\n    writer = tf.summary.FileWriter(os.path.join(logdir, 'box_to_gaussian'))\n    writer.add_graph(sess.graph)\n    for step in xrange(8):\n      if verbose:\n        logger.info('--- box_to_gaussian: step: %s' % step)\n        feed_dict = {blur_radius: step}\n      run_options = tf.compat.v1.RunOptions(trace_level=tf.compat.v1.RunOptions.FULL_TRACE)\n      run_metadata = config_pb2.RunMetadata()\n      s = sess.run(summ, feed_dict=feed_dict,\n                   options=run_options, run_metadata=run_metadata)\n      writer.add_summary(s, global_step=step)\n      writer.add_run_metadata(run_metadata, 'step_%04d' % step)\n    writer.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run_sobel(logdir, verbose=False):\n  if verbose:\n    logger.info('--- Starting run: sobel')\n\n  tf.compat.v1.reset_default_graph()\n  tf.compat.v1.set_random_seed(0)\n\n  image = get_image(verbose=verbose)\n  kernel_radius = tf.compat.v1.placeholder(shape=(), dtype=tf.int32)\n\n  with tf.name_scope('horizontal_kernel'):\n    kernel_side_length = kernel_radius * 2 + 1\n    # Drop off influence for pixels further away from the center.\n    weighting_kernel = (\n        1.0 - tf.abs(tf.linspace(-1.0, 1.0, num=kernel_side_length)))\n    differentiation_kernel = tf.linspace(-1.0, 1.0, num=kernel_side_length)\n    horizontal_kernel = tf.matmul(tf.expand_dims(weighting_kernel, 1),\n                                  tf.expand_dims(differentiation_kernel, 0))\n\n  with tf.name_scope('vertical_kernel'):\n    vertical_kernel = tf.transpose(a=horizontal_kernel)\n\n  float_image = tf.cast(image, tf.float32)\n  dx = convolve(float_image, horizontal_kernel, name='convolve_dx')\n  dy = convolve(float_image, vertical_kernel, name='convolve_dy')\n  gradient_magnitude = tf.norm(tensor=[dx, dy], axis=0, name='gradient_magnitude')\n  with tf.name_scope('normalized_gradient'):\n    normalized_gradient = gradient_magnitude / tf.reduce_max(input_tensor=gradient_magnitude)\n  with tf.name_scope('output_image'):\n    output_image = tf.cast(255 * normalized_gradient, tf.uint8)\n\n  summ = image_summary.op(\n      'sobel', tf.stack([output_image]),\n      display_name='Sobel edge detection',\n      description=(u'Demonstration of [Sobel edge detection]. The step '\n                   'parameter adjusts the radius of the kernel. '\n                   'The kernel can be of arbitrary size, and considers '\n                   u'nearby pixels with \\u2113\\u2082-linear falloff.\\n\\n'\n                   # (that says ``$\\ell_2$-linear falloff'')\n                   'Edge detection is done on a per-channel basis, so '\n                   'you can observe which edges are &ldquo;mostly red '\n                   'edges,&rdquo; for instance.\\n\\n'\n                   'For practical edge detection, a small kernel '\n                   '(usually not more than more than *r*=2) is best.\\n\\n'\n                   '[Sobel edge detection]: %s\\n\\n'\n                   \"%s\"\n                   % ('https://en.wikipedia.org/wiki/Sobel_operator',\n                      IMAGE_CREDIT)))\n\n  with tf.compat.v1.Session() as sess:\n    sess.run(image.initializer)\n    writer = tf.summary.FileWriter(os.path.join(logdir, 'sobel'))\n    writer.add_graph(sess.graph)\n    for step in xrange(8):\n      if verbose:\n        logger.info(\"--- sobel: step: %s\" % step)\n        feed_dict = {kernel_radius: step}\n      run_options = tf.compat.v1.RunOptions(trace_level=tf.compat.v1.RunOptions.FULL_TRACE)\n      run_metadata = config_pb2.RunMetadata()\n      s = sess.run(summ, feed_dict=feed_dict,\n                   options=run_options, run_metadata=run_metadata)\n      writer.add_summary(s, global_step=step)\n      writer.add_run_metadata(run_metadata, 'step_%04d' % step)\n    writer.close()", "response": "A Sobel edge detection demonstration."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run_all(logdir, verbose=False):\n  run_box_to_gaussian(logdir, verbose=verbose)\n  run_sobel(logdir, verbose=verbose)", "response": "Run all the runs in the specified directory."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the value of a feature from Example regardless of feature type.", "response": "def proto_value_for_feature(example, feature_name):\n  \"\"\"Get the value of a feature from Example regardless of feature type.\"\"\"\n  feature = get_example_features(example)[feature_name]\n  if feature is None:\n    raise ValueError('Feature {} is not on example proto.'.format(feature_name))\n  feature_type = feature.WhichOneof('kind')\n  if feature_type is None:\n    raise ValueError('Feature {} on example proto has no declared type.'.format(\n        feature_name))\n  return getattr(feature, feature_type).value"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns an OriginalFeatureList object representing the original feature.", "response": "def parse_original_feature_from_example(example, feature_name):\n  \"\"\"Returns an `OriginalFeatureList` for the specified feature_name.\n\n  Args:\n    example: An example.\n    feature_name: A string feature name.\n\n  Returns:\n    A filled in `OriginalFeatureList` object representing the feature.\n  \"\"\"\n  feature = get_example_features(example)[feature_name]\n  feature_type = feature.WhichOneof('kind')\n  original_value = proto_value_for_feature(example, feature_name)\n\n  return OriginalFeatureList(feature_name, original_value, feature_type)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrap the provided InferenceResult proto into a new InferenceResult proto.", "response": "def wrap_inference_results(inference_result_proto):\n  \"\"\"Returns packaged inference results from the provided proto.\n\n  Args:\n    inference_result_proto: The classification or regression response proto.\n\n  Returns:\n    An InferenceResult proto with the result from the response.\n  \"\"\"\n  inference_proto = inference_pb2.InferenceResult()\n  if isinstance(inference_result_proto,\n                classification_pb2.ClassificationResponse):\n    inference_proto.classification_result.CopyFrom(\n        inference_result_proto.result)\n  elif isinstance(inference_result_proto, regression_pb2.RegressionResponse):\n    inference_proto.regression_result.CopyFrom(inference_result_proto.result)\n  return inference_proto"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of feature names for float and int64 type features.", "response": "def get_numeric_feature_names(example):\n  \"\"\"Returns a list of feature names for float and int64 type features.\n\n  Args:\n    example: An example.\n\n  Returns:\n    A list of strings of the names of numeric features.\n  \"\"\"\n  numeric_features = ('float_list', 'int64_list')\n  features = get_example_features(example)\n  return sorted([\n      feature_name for feature_name in features\n      if features[feature_name].WhichOneof('kind') in numeric_features\n  ])"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a list of categorical feature names for byte type features.", "response": "def get_categorical_feature_names(example):\n  \"\"\"Returns a list of feature names for byte type features.\n\n  Args:\n    example: An example.\n\n  Returns:\n    A list of categorical feature names (e.g. ['education', 'marital_status'] )\n  \"\"\"\n  features = get_example_features(example)\n  return sorted([\n      feature_name for feature_name in features\n      if features[feature_name].WhichOneof('kind') == 'bytes_list'\n  ])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a dict mapping each numerical feature name to observed range.", "response": "def get_numeric_features_to_observed_range(examples):\n  \"\"\"Returns numerical features and their observed ranges.\n\n  Args:\n    examples: Examples to read to get ranges.\n\n  Returns:\n    A dict mapping feature_name -> {'observedMin': 'observedMax': } dicts,\n    with a key for each numerical feature.\n  \"\"\"\n  observed_features = collections.defaultdict(list)  # name -> [value, ]\n  for example in examples:\n    for feature_name in get_numeric_feature_names(example):\n      original_feature = parse_original_feature_from_example(\n          example, feature_name)\n      observed_features[feature_name].extend(original_feature.original_value)\n  return {\n      feature_name: {\n          'observedMin': min(feature_values),\n          'observedMax': max(feature_values),\n      }\n      for feature_name, feature_values in iteritems(observed_features)\n  }"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn categorical features and a sampling of their most - common values.", "response": "def get_categorical_features_to_sampling(examples, top_k):\n  \"\"\"Returns categorical features and a sampling of their most-common values.\n\n  The results of this slow function are used by the visualization repeatedly,\n  so the results are cached.\n\n  Args:\n    examples: Examples to read to get feature samples.\n    top_k: Max number of samples to return per feature.\n\n  Returns:\n    A dict of feature_name -> {'samples': ['Married-civ-spouse',\n      'Never-married', 'Divorced']}.\n\n    There is one key for each categorical feature.\n\n    Currently, the inner dict just has one key, but this structure leaves room\n    for further expansion, and mirrors the structure used by\n    `get_numeric_features_to_observed_range`.\n  \"\"\"\n  observed_features = collections.defaultdict(list)  # name -> [value, ]\n  for example in examples:\n    for feature_name in get_categorical_feature_names(example):\n      original_feature = parse_original_feature_from_example(\n          example, feature_name)\n      observed_features[feature_name].extend(original_feature.original_value)\n\n  result = {}\n  for feature_name, feature_values in sorted(iteritems(observed_features)):\n    samples = [\n        word\n        for word, count in collections.Counter(feature_values).most_common(\n            top_k) if count > 1\n    ]\n    if samples:\n      result[feature_name] = {'samples': samples}\n  return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a list of MutantFeatureValue s that are variants of original.", "response": "def make_mutant_features(original_feature, index_to_mutate, viz_params):\n  \"\"\"Return a list of `MutantFeatureValue`s that are variants of original.\"\"\"\n  lower = viz_params.x_min\n  upper = viz_params.x_max\n  examples = viz_params.examples\n  num_mutants = viz_params.num_mutants\n\n  if original_feature.feature_type == 'float_list':\n    return [\n        MutantFeatureValue(original_feature, index_to_mutate, value)\n        for value in np.linspace(lower, upper, num_mutants)\n    ]\n  elif original_feature.feature_type == 'int64_list':\n    mutant_values = np.linspace(int(lower), int(upper),\n                                num_mutants).astype(int).tolist()\n    # Remove duplicates that can occur due to integer constraint.\n    mutant_values = sorted(set(mutant_values))\n    return [\n        MutantFeatureValue(original_feature, index_to_mutate, value)\n        for value in mutant_values\n    ]\n  elif original_feature.feature_type == 'bytes_list':\n    feature_to_samples = get_categorical_features_to_sampling(\n        examples, num_mutants)\n\n    # `mutant_values` looks like:\n    # [['Married-civ-spouse'], ['Never-married'], ['Divorced'], ['Separated']]\n    mutant_values = feature_to_samples[original_feature.feature_name]['samples']\n    return [\n        MutantFeatureValue(original_feature, None, value)\n        for value in mutant_values\n    ]\n  else:\n    raise ValueError('Malformed original feature had type of: ' +\n                     original_feature.feature_type)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a list of tuples that are lists of MutantFeatureValue s and a list of MutantExamples.", "response": "def make_mutant_tuples(example_protos, original_feature, index_to_mutate,\n                       viz_params):\n  \"\"\"Return a list of `MutantFeatureValue`s and a list of mutant Examples.\n\n  Args:\n    example_protos: The examples to mutate.\n    original_feature: A `OriginalFeatureList` that encapsulates the feature to\n      mutate.\n    index_to_mutate: The index of the int64_list or float_list to mutate.\n    viz_params: A `VizParams` object that contains the UI state of the request.\n\n  Returns:\n    A list of `MutantFeatureValue`s and a list of mutant examples.\n  \"\"\"\n  mutant_features = make_mutant_features(original_feature, index_to_mutate,\n                                         viz_params)\n  mutant_examples = []\n  for example_proto in example_protos:\n    for mutant_feature in mutant_features:\n      copied_example = copy.deepcopy(example_proto)\n      feature_name = mutant_feature.original_feature.feature_name\n\n      try:\n        feature_list = proto_value_for_feature(copied_example, feature_name)\n        if index_to_mutate is None:\n          new_values = mutant_feature.mutant_value\n        else:\n          new_values = list(feature_list)\n          new_values[index_to_mutate] = mutant_feature.mutant_value\n\n        del feature_list[:]\n        feature_list.extend(new_values)\n        mutant_examples.append(copied_example)\n      except (ValueError, IndexError):\n        # If the mutant value can't be set, still add the example to the\n        # mutant_example even though no change was made. This is necessary to\n        # allow for computation of global PD plots when not all examples have\n        # the same number of feature values for a feature.\n        mutant_examples.append(copied_example)\n\n  return mutant_features, mutant_examples"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a JSON - formatted representation of the mutant charts for a single feature.", "response": "def mutant_charts_for_feature(example_protos, feature_name, serving_bundles,\n                              viz_params):\n  \"\"\"Returns JSON formatted for rendering all charts for a feature.\n\n  Args:\n    example_proto: The example protos to mutate.\n    feature_name: The string feature name to mutate.\n    serving_bundles: One `ServingBundle` object per model, that contains the\n      information to make the serving request.\n    viz_params: A `VizParams` object that contains the UI state of the request.\n\n  Raises:\n    InvalidUserInputError if `viz_params.feature_index_pattern` requests out of\n    range indices for `feature_name` within `example_proto`.\n\n  Returns:\n    A JSON-able dict for rendering a single mutant chart.  parsed in\n    `tf-inference-dashboard.html`.\n    {\n      'chartType': 'numeric', # oneof('numeric', 'categorical')\n      'data': [A list of data] # parseable by vz-line-chart or vz-bar-chart\n    }\n  \"\"\"\n\n  def chart_for_index(index_to_mutate):\n    mutant_features, mutant_examples = make_mutant_tuples(\n        example_protos, original_feature, index_to_mutate, viz_params)\n\n    charts = []\n    for serving_bundle in serving_bundles:\n      inference_result_proto = run_inference(mutant_examples, serving_bundle)\n      charts.append(make_json_formatted_for_single_chart(\n        mutant_features, inference_result_proto, index_to_mutate))\n    return charts\n  try:\n    original_feature = parse_original_feature_from_example(\n        example_protos[0], feature_name)\n  except ValueError as e:\n    return {\n        'chartType': 'categorical',\n        'data': []\n    }\n\n  indices_to_mutate = viz_params.feature_indices or range(\n      original_feature.length)\n  chart_type = ('categorical' if original_feature.feature_type == 'bytes_list'\n                else 'numeric')\n\n  try:\n    return {\n        'chartType': chart_type,\n        'data': [\n            chart_for_index(index_to_mutate)\n            for index_to_mutate in indices_to_mutate\n        ]\n    }\n  except IndexError as e:\n    raise common_utils.InvalidUserInputError(e)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a JSON - formatted representation of a single chart.", "response": "def make_json_formatted_for_single_chart(mutant_features,\n                                         inference_result_proto,\n                                         index_to_mutate):\n  \"\"\"Returns JSON formatted for a single mutant chart.\n\n  Args:\n    mutant_features: An iterable of `MutantFeatureValue`s representing the\n      X-axis.\n    inference_result_proto: A ClassificationResponse or RegressionResponse\n      returned by Servo, representing the Y-axis.\n      It contains one 'classification' or 'regression' for every Example that\n      was sent for inference. The length of that field should be the same length\n      of mutant_features.\n    index_to_mutate: The index of the feature being mutated for this chart.\n\n  Returns:\n    A JSON-able dict for rendering a single mutant chart, parseable by\n    `vz-line-chart` or `vz-bar-chart`.\n  \"\"\"\n  x_label = 'step'\n  y_label = 'scalar'\n\n  if isinstance(inference_result_proto,\n                classification_pb2.ClassificationResponse):\n    # classification_label -> [{x_label: y_label:}]\n    series = {}\n\n    # ClassificationResponse has a separate probability for each label\n    for idx, classification in enumerate(\n        inference_result_proto.result.classifications):\n      # For each example to use for mutant inference, we create a copied example\n      # with the feature in question changed to each possible mutant value. So\n      # when we get the inferences back, we get num_examples*num_mutants\n      # results. So, modding by len(mutant_features) allows us to correctly\n      # lookup the mutant value for each inference.\n      mutant_feature = mutant_features[idx % len(mutant_features)]\n      for class_index, classification_class in enumerate(\n        classification.classes):\n        # Fill in class index when labels are missing\n        if classification_class.label == '':\n          classification_class.label = str(class_index)\n        # Special case to not include the \"0\" class in binary classification.\n        # Since that just results in a chart that is symmetric around 0.5.\n        if len(\n            classification.classes) == 2 and classification_class.label == '0':\n          continue\n        key = classification_class.label\n        if index_to_mutate:\n          key += ' (index %d)' % index_to_mutate\n        if not key in series:\n          series[key] = {}\n        if not mutant_feature.mutant_value in series[key]:\n          series[key][mutant_feature.mutant_value] = []\n        series[key][mutant_feature.mutant_value].append(\n          classification_class.score)\n\n    # Post-process points to have separate list for each class\n    return_series = collections.defaultdict(list)\n    for key, mutant_values in iteritems(series):\n      for value, y_list in iteritems(mutant_values):\n        return_series[key].append({\n          x_label: value,\n          y_label: sum(y_list) / float(len(y_list))\n        })\n      return_series[key].sort(key=lambda p: p[x_label])\n    return return_series\n\n  elif isinstance(inference_result_proto, regression_pb2.RegressionResponse):\n    points = {}\n\n    for idx, regression in enumerate(inference_result_proto.result.regressions):\n      # For each example to use for mutant inference, we create a copied example\n      # with the feature in question changed to each possible mutant value. So\n      # when we get the inferences back, we get num_examples*num_mutants\n      # results. So, modding by len(mutant_features) allows us to correctly\n      # lookup the mutant value for each inference.\n      mutant_feature = mutant_features[idx % len(mutant_features)]\n      if not mutant_feature.mutant_value in points:\n        points[mutant_feature.mutant_value] = []\n      points[mutant_feature.mutant_value].append(regression.value)\n    key = 'value'\n    if (index_to_mutate != 0):\n      key += ' (index %d)' % index_to_mutate\n    list_of_points = []\n    for value, y_list in iteritems(points):\n      list_of_points.append({\n        x_label: value,\n        y_label: sum(y_list) / float(len(y_list))\n      })\n    list_of_points.sort(key=lambda p: p[x_label])\n    return {key: list_of_points}\n\n  else:\n    raise NotImplementedError('Only classification and regression implemented.')"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the non - sequence features from the provided example.", "response": "def get_example_features(example):\n  \"\"\"Returns the non-sequence features from the provided example.\"\"\"\n  return (example.features.feature if isinstance(example, tf.train.Example)\n          else example.context.feature)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run_inference_for_inference_results(examples, serving_bundle):\n  inference_result_proto = run_inference(examples, serving_bundle)\n  inferences = wrap_inference_results(inference_result_proto)\n  infer_json = json_format.MessageToJson(\n    inferences, including_default_value_fields=True)\n  return json.loads(infer_json)", "response": "Calls servo and wraps the inference results."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_eligible_features(examples, num_mutants):\n  features_dict = (\n      get_numeric_features_to_observed_range(\n          examples))\n\n  features_dict.update(\n      get_categorical_features_to_sampling(\n          examples, num_mutants))\n\n  # Massage the features_dict into a sorted list before returning because\n  # Polymer dom-repeat needs a list.\n  features_list = []\n  for k, v in sorted(features_dict.items()):\n    v['name'] = k\n    features_list.append(v)\n  return features_list", "response": "Returns a list of JSON objects for each feature in the examples."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a list of label strings loaded from the provided path.", "response": "def get_label_vocab(vocab_path):\n  \"\"\"Returns a list of label strings loaded from the provided path.\"\"\"\n  if vocab_path:\n    try:\n      with tf.io.gfile.GFile(vocab_path, 'r') as f:\n        return [line.rstrip('\\n') for line in f]\n    except tf.errors.NotFoundError as err:\n      tf.logging.error('error reading vocab file: %s', err)\n  return []"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_sprite_image(examples):\n\n    def generate_image_from_thubnails(thumbnails, thumbnail_dims):\n      \"\"\"Generates a sprite atlas image from a set of thumbnails.\"\"\"\n      num_thumbnails = tf.shape(thumbnails)[0].eval()\n      images_per_row = int(math.ceil(math.sqrt(num_thumbnails)))\n      thumb_height = thumbnail_dims[0]\n      thumb_width = thumbnail_dims[1]\n      master_height = images_per_row * thumb_height\n      master_width = images_per_row * thumb_width\n      num_channels = 3\n      master = np.zeros([master_height, master_width, num_channels])\n      for idx, image in enumerate(thumbnails.eval()):\n        left_idx = idx % images_per_row\n        top_idx = int(math.floor(idx / images_per_row))\n        left_start = left_idx * thumb_width\n        left_end = left_start + thumb_width\n        top_start = top_idx * thumb_height\n        top_end = top_start + thumb_height\n        master[top_start:top_end, left_start:left_end, :] = image\n      return tf.image.encode_png(master)\n\n    image_feature_name = 'image/encoded'\n    sprite_thumbnail_dim_px = 32\n    with tf.compat.v1.Session():\n      keys_to_features = {\n          image_feature_name:\n              tf.FixedLenFeature((), tf.string, default_value=''),\n      }\n      parsed = tf.parse_example(examples, keys_to_features)\n      images = tf.zeros([1, 1, 1, 1], tf.float32)\n      i = tf.constant(0)\n      thumbnail_dims = (sprite_thumbnail_dim_px,\n                        sprite_thumbnail_dim_px)\n      num_examples = tf.constant(len(examples))\n      encoded_images = parsed[image_feature_name]\n\n      # Loop over all examples, decoding the image feature value, resizing\n      # and appending to a list of all images.\n      def loop_body(i, encoded_images, images):\n        encoded_image = encoded_images[i]\n        image = tf.image.decode_jpeg(encoded_image, channels=3)\n        resized_image = tf.image.resize(image, thumbnail_dims)\n        expanded_image = tf.expand_dims(resized_image, 0)\n        images = tf.cond(\n            tf.equal(i, 0), lambda: expanded_image,\n            lambda: tf.concat([images, expanded_image], 0))\n        return i + 1, encoded_images, images\n\n      loop_out = tf.while_loop(\n          lambda i, encoded_images, images: tf.less(i, num_examples),\n          loop_body, [i, encoded_images, images],\n          shape_invariants=[\n              i.get_shape(),\n              encoded_images.get_shape(),\n              tf.TensorShape(None)\n          ])\n\n      # Create the single sprite atlas image from these thumbnails.\n      sprite = generate_image_from_thubnails(loop_out[2], thumbnail_dims)\n      return sprite.eval()", "response": "Creates a sprite image for use in Facets Dive."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrun inference on examples given model information.", "response": "def run_inference(examples, serving_bundle):\n  \"\"\"Run inference on examples given model information\n\n  Args:\n    examples: A list of examples that matches the model spec.\n    serving_bundle: A `ServingBundle` object that contains the information to\n      make the inference request.\n\n  Returns:\n    A ClassificationResponse or RegressionResponse proto.\n  \"\"\"\n  batch_size = 64\n  if serving_bundle.estimator and serving_bundle.feature_spec:\n    # If provided an estimator and feature spec then run inference locally.\n    preds = serving_bundle.estimator.predict(\n      lambda: tf.data.Dataset.from_tensor_slices(\n        tf.parse_example([ex.SerializeToString() for ex in examples],\n        serving_bundle.feature_spec)).batch(batch_size))\n\n    if serving_bundle.use_predict:\n      preds_key = serving_bundle.predict_output_tensor\n    elif serving_bundle.model_type == 'regression':\n      preds_key = 'predictions'\n    else:\n      preds_key = 'probabilities'\n\n    values = []\n    for pred in preds:\n      values.append(pred[preds_key])\n    return common_utils.convert_prediction_values(values, serving_bundle)\n  elif serving_bundle.custom_predict_fn:\n    # If custom_predict_fn is provided, pass examples directly for local\n    # inference.\n    values = serving_bundle.custom_predict_fn(examples)\n    return common_utils.convert_prediction_values(values, serving_bundle)\n  else:\n    return platform_utils.call_servo(examples, serving_bundle)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef Items(self, key):\n    with self._mutex:\n      if key not in self._buckets:\n        raise KeyError('Key %s was not found in Reservoir' % key)\n      bucket = self._buckets[key]\n    return bucket.Items()", "response": "Returns a list of items associated with a given key."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a new item to the Reservoir with the given tag.", "response": "def AddItem(self, key, item, f=lambda x: x):\n    \"\"\"Add a new item to the Reservoir with the given tag.\n\n    If the reservoir has not yet reached full size, the new item is guaranteed\n    to be added. If the reservoir is full, then behavior depends on the\n    always_keep_last boolean.\n\n    If always_keep_last was set to true, the new item is guaranteed to be added\n    to the reservoir, and either the previous last item will be replaced, or\n    (with low probability) an older item will be replaced.\n\n    If always_keep_last was set to false, then the new item will replace an\n    old item with low probability.\n\n    If f is provided, it will be applied to transform item (lazily, iff item is\n      going to be included in the reservoir).\n\n    Args:\n      key: The key to store the item under.\n      item: The item to add to the reservoir.\n      f: An optional function to transform the item prior to addition.\n    \"\"\"\n    with self._mutex:\n      bucket = self._buckets[key]\n    bucket.AddItem(item, f)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef FilterItems(self, filterFn, key=None):\n    with self._mutex:\n      if key:\n        if key in self._buckets:\n          return self._buckets[key].FilterItems(filterFn)\n        else:\n          return 0\n      else:\n        return sum(bucket.FilterItems(filterFn)\n                   for bucket in self._buckets.values())", "response": "Filter items within a Reservoir using a filtering function."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef AddItem(self, item, f=lambda x: x):\n    with self._mutex:\n      if len(self.items) < self._max_size or self._max_size == 0:\n        self.items.append(f(item))\n      else:\n        r = self._random.randint(0, self._num_items_seen)\n        if r < self._max_size:\n          self.items.pop(r)\n          self.items.append(f(item))\n        elif self.always_keep_last:\n          self.items[-1] = f(item)\n      self._num_items_seen += 1", "response": "Adds an item to the ReservoirBucket."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef FilterItems(self, filterFn):\n    with self._mutex:\n      size_before = len(self.items)\n      self.items = list(filter(filterFn, self.items))\n      size_diff = size_before - len(self.items)\n\n      # Estimate a correction the number of items seen\n      prop_remaining = len(self.items) / float(\n          size_before) if size_before > 0 else 0\n      self._num_items_seen = int(round(self._num_items_seen * prop_remaining))\n      return size_diff", "response": "Filters items in a ReservoirBucket using a filtering function."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _GetDenseDimensions(list_of_lists):\n    if not isinstance(list_of_lists, (list, tuple)):\n        return []\n    elif not list_of_lists:\n        return [0]\n    else:\n        return [len(list_of_lists)] + _GetDenseDimensions(list_of_lists[0])", "response": "Returns the inferred dense dimensions of a list of lists."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a TensorProto from values dtype and shape.", "response": "def make_tensor_proto(values, dtype=None, shape=None, verify_shape=False):\n    \"\"\"Create a TensorProto.\n\n  Args:\n    values:         Values to put in the TensorProto.\n    dtype:          Optional tensor_pb2 DataType value.\n    shape:          List of integers representing the dimensions of tensor.\n    verify_shape:   Boolean that enables verification of a shape of values.\n\n  Returns:\n    A `TensorProto`. Depending on the type, it may contain data in the\n    \"tensor_content\" attribute, which is not directly useful to Python programs.\n    To access the values you should convert the proto back to a numpy ndarray\n    with `tensor_util.MakeNdarray(proto)`.\n\n    If `values` is a `TensorProto`, it is immediately returned; `dtype` and\n    `shape` are ignored.\n\n  Raises:\n    TypeError:  if unsupported types are provided.\n    ValueError: if arguments have inappropriate values or if verify_shape is\n     True and shape of values is not equals to a shape from the argument.\n\n  make_tensor_proto accepts \"values\" of a python scalar, a python list, a\n  numpy ndarray, or a numpy scalar.\n\n  If \"values\" is a python scalar or a python list, make_tensor_proto\n  first convert it to numpy ndarray. If dtype is None, the\n  conversion tries its best to infer the right numpy data\n  type. Otherwise, the resulting numpy array has a convertible data\n  type with the given dtype.\n\n  In either case above, the numpy ndarray (either the caller provided\n  or the auto converted) must have the convertible type with dtype.\n\n  make_tensor_proto then converts the numpy array to a tensor proto.\n\n  If \"shape\" is None, the resulting tensor proto represents the numpy\n  array precisely.\n\n  Otherwise, \"shape\" specifies the tensor's shape and the numpy array\n  can not have more elements than what \"shape\" specifies.\n  \"\"\"\n    if isinstance(values, tensor_pb2.TensorProto):\n        return values\n\n    if dtype:\n        dtype = dtypes.as_dtype(dtype)\n\n    is_quantized = dtype in [\n        dtypes.qint8,\n        dtypes.quint8,\n        dtypes.qint16,\n        dtypes.quint16,\n        dtypes.qint32,\n    ]\n\n    # We first convert value to a numpy array or scalar.\n    if isinstance(values, (np.ndarray, np.generic)):\n        if dtype:\n            nparray = values.astype(dtype.as_numpy_dtype)\n        else:\n            nparray = values\n    elif callable(getattr(values, \"__array__\", None)) or isinstance(\n        getattr(values, \"__array_interface__\", None), dict\n    ):\n        # If a class has the __array__ method, or __array_interface__ dict, then it\n        # is possible to convert to numpy array.\n        nparray = np.asarray(values, dtype=dtype)\n\n        # This is the preferred way to create an array from the object, so replace\n        # the `values` with the array so that _FlattenToStrings is not run.\n        values = nparray\n    else:\n        if values is None:\n            raise ValueError(\"None values not supported.\")\n        # if dtype is provided, forces numpy array to be the type\n        # provided if possible.\n        if dtype and dtype.is_numpy_compatible:\n            np_dt = dtype.as_numpy_dtype\n        else:\n            np_dt = None\n        # If shape is None, numpy.prod returns None when dtype is not set, but raises\n        # exception when dtype is set to np.int64\n        if shape is not None and np.prod(shape, dtype=np.int64) == 0:\n            nparray = np.empty(shape, dtype=np_dt)\n        else:\n            _Assertconvertible(values, dtype)\n            nparray = np.array(values, dtype=np_dt)\n            # check to them.\n            # We need to pass in quantized values as tuples, so don't apply the shape\n            if list(nparray.shape) != _GetDenseDimensions(values) and not is_quantized:\n                raise ValueError(\n                    \"\"\"Argument must be a dense tensor: %s\"\"\"\n                    \"\"\" - got shape %s, but wanted %s.\"\"\"\n                    % (values, list(nparray.shape), _GetDenseDimensions(values))\n                )\n\n        # python/numpy default float type is float64. We prefer float32 instead.\n        if (nparray.dtype == np.float64) and dtype is None:\n            nparray = nparray.astype(np.float32)\n        # python/numpy default int type is int64. We prefer int32 instead.\n        elif (nparray.dtype == np.int64) and dtype is None:\n            downcasted_array = nparray.astype(np.int32)\n            # Do not down cast if it leads to precision loss.\n            if np.array_equal(downcasted_array, nparray):\n                nparray = downcasted_array\n\n    # if dtype is provided, it must be convertible with what numpy\n    # conversion says.\n    numpy_dtype = dtypes.as_dtype(nparray.dtype)\n    if numpy_dtype is None:\n        raise TypeError(\"Unrecognized data type: %s\" % nparray.dtype)\n\n    # If dtype was specified and is a quantized type, we convert\n    # numpy_dtype back into the quantized version.\n    if is_quantized:\n        numpy_dtype = dtype\n\n    if dtype is not None and (\n        not hasattr(dtype, \"base_dtype\") or dtype.base_dtype != numpy_dtype.base_dtype\n    ):\n        raise TypeError(\n            \"Inconvertible types: %s vs. %s. Value is %s\"\n            % (dtype, nparray.dtype, values)\n        )\n\n    # If shape is not given, get the shape from the numpy array.\n    if shape is None:\n        shape = nparray.shape\n        is_same_size = True\n        shape_size = nparray.size\n    else:\n        shape = [int(dim) for dim in shape]\n        shape_size = np.prod(shape, dtype=np.int64)\n        is_same_size = shape_size == nparray.size\n\n        if verify_shape:\n            if not nparray.shape == tuple(shape):\n                raise TypeError(\n                    \"Expected Tensor's shape: %s, got %s.\"\n                    % (tuple(shape), nparray.shape)\n                )\n\n        if nparray.size > shape_size:\n            raise ValueError(\n                \"Too many elements provided. Needed at most %d, but received %d\"\n                % (shape_size, nparray.size)\n            )\n\n    tensor_proto = tensor_pb2.TensorProto(\n        dtype=numpy_dtype.as_datatype_enum,\n        tensor_shape=tensor_shape.as_shape(shape).as_proto(),\n    )\n\n    if is_same_size and numpy_dtype in _TENSOR_CONTENT_TYPES and shape_size > 1:\n        if nparray.size * nparray.itemsize >= (1 << 31):\n            raise ValueError(\n                \"Cannot create a tensor proto whose content is larger than 2GB.\"\n            )\n        tensor_proto.tensor_content = nparray.tostring()\n        return tensor_proto\n\n    # If we were not given values as a numpy array, compute the proto_values\n    # from the given values directly, to avoid numpy trimming nulls from the\n    # strings. Since values could be a list of strings, or a multi-dimensional\n    # list of lists that might or might not correspond to the given shape,\n    # we flatten it conservatively.\n    if numpy_dtype == dtypes.string and not isinstance(values, np.ndarray):\n        proto_values = _FlattenToStrings(values)\n\n        # At this point, values may be a list of objects that we could not\n        # identify a common type for (hence it was inferred as\n        # np.object/dtypes.string).  If we are unable to convert it to a\n        # string, we raise a more helpful error message.\n        #\n        # Ideally, we'd be able to convert the elements of the list to a\n        # common type, but this type inference requires some thinking and\n        # so we defer it for now.\n        try:\n            str_values = [compat.as_bytes(x) for x in proto_values]\n        except TypeError:\n            raise TypeError(\n                \"Failed to convert object of type %s to Tensor. \"\n                \"Contents: %s. Consider casting elements to a \"\n                \"supported type.\" % (type(values), values)\n            )\n        tensor_proto.string_val.extend(str_values)\n        return tensor_proto\n\n    # TensorFlow expects C order (a.k.a., eigen row major).\n    proto_values = nparray.ravel()\n\n    append_fn = GetNumpyAppendFn(proto_values.dtype)\n    if append_fn is None:\n        raise TypeError(\n            \"Element type not supported in TensorProto: %s\" % numpy_dtype.name\n        )\n    append_fn(tensor_proto, proto_values)\n\n    return tensor_proto"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a numpy ndarray from a tensor.", "response": "def make_ndarray(tensor):\n    \"\"\"Create a numpy ndarray from a tensor.\n\n  Create a numpy ndarray with the same shape and data as the tensor.\n\n  Args:\n    tensor: A TensorProto.\n\n  Returns:\n    A numpy array with the tensor contents.\n\n  Raises:\n    TypeError: if tensor has unsupported type.\n\n  \"\"\"\n    shape = [d.size for d in tensor.tensor_shape.dim]\n    num_elements = np.prod(shape, dtype=np.int64)\n    tensor_dtype = dtypes.as_dtype(tensor.dtype)\n    dtype = tensor_dtype.as_numpy_dtype\n\n    if tensor.tensor_content:\n        return np.frombuffer(tensor.tensor_content, dtype=dtype).copy().reshape(shape)\n    elif tensor_dtype == dtypes.float16 or tensor_dtype == dtypes.bfloat16:\n        # the half_val field of the TensorProto stores the binary representation\n        # of the fp16: we need to reinterpret this as a proper float16\n        if len(tensor.half_val) == 1:\n            tmp = np.array(tensor.half_val[0], dtype=np.uint16)\n            tmp.dtype = tensor_dtype.as_numpy_dtype\n            return np.repeat(tmp, num_elements).reshape(shape)\n        else:\n            tmp = np.fromiter(tensor.half_val, dtype=np.uint16)\n            tmp.dtype = tensor_dtype.as_numpy_dtype\n            return tmp.reshape(shape)\n    elif tensor_dtype == dtypes.float32:\n        if len(tensor.float_val) == 1:\n            return np.repeat(\n                np.array(tensor.float_val[0], dtype=dtype), num_elements\n            ).reshape(shape)\n        else:\n            return np.fromiter(tensor.float_val, dtype=dtype).reshape(shape)\n    elif tensor_dtype == dtypes.float64:\n        if len(tensor.double_val) == 1:\n            return np.repeat(\n                np.array(tensor.double_val[0], dtype=dtype), num_elements\n            ).reshape(shape)\n        else:\n            return np.fromiter(tensor.double_val, dtype=dtype).reshape(shape)\n    elif tensor_dtype in [\n        dtypes.int32,\n        dtypes.uint8,\n        dtypes.uint16,\n        dtypes.int16,\n        dtypes.int8,\n        dtypes.qint32,\n        dtypes.quint8,\n        dtypes.qint8,\n        dtypes.qint16,\n        dtypes.quint16,\n    ]:\n        if len(tensor.int_val) == 1:\n            return np.repeat(\n                np.array(tensor.int_val[0], dtype=dtype), num_elements\n            ).reshape(shape)\n        else:\n            return np.fromiter(tensor.int_val, dtype=dtype).reshape(shape)\n    elif tensor_dtype == dtypes.int64:\n        if len(tensor.int64_val) == 1:\n            return np.repeat(\n                np.array(tensor.int64_val[0], dtype=dtype), num_elements\n            ).reshape(shape)\n        else:\n            return np.fromiter(tensor.int64_val, dtype=dtype).reshape(shape)\n    elif tensor_dtype == dtypes.string:\n        if len(tensor.string_val) == 1:\n            return np.repeat(\n                np.array(tensor.string_val[0], dtype=dtype), num_elements\n            ).reshape(shape)\n        else:\n            return np.array([x for x in tensor.string_val], dtype=dtype).reshape(shape)\n    elif tensor_dtype == dtypes.complex64:\n        it = iter(tensor.scomplex_val)\n        if len(tensor.scomplex_val) == 2:\n            return np.repeat(\n                np.array(\n                    complex(tensor.scomplex_val[0], tensor.scomplex_val[1]), dtype=dtype\n                ),\n                num_elements,\n            ).reshape(shape)\n        else:\n            return np.array(\n                [complex(x[0], x[1]) for x in zip(it, it)], dtype=dtype\n            ).reshape(shape)\n    elif tensor_dtype == dtypes.complex128:\n        it = iter(tensor.dcomplex_val)\n        if len(tensor.dcomplex_val) == 2:\n            return np.repeat(\n                np.array(\n                    complex(tensor.dcomplex_val[0], tensor.dcomplex_val[1]), dtype=dtype\n                ),\n                num_elements,\n            ).reshape(shape)\n        else:\n            return np.array(\n                [complex(x[0], x[1]) for x in zip(it, it)], dtype=dtype\n            ).reshape(shape)\n    elif tensor_dtype == dtypes.bool:\n        if len(tensor.bool_val) == 1:\n            return np.repeat(\n                np.array(tensor.bool_val[0], dtype=dtype), num_elements\n            ).reshape(shape)\n        else:\n            return np.fromiter(tensor.bool_val, dtype=dtype).reshape(shape)\n    else:\n        raise TypeError(\"Unsupported tensor type: %s\" % tensor.dtype)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a tensor summary op that writes the layout to disk.", "response": "def op(scalars_layout, collections=None):\n  \"\"\"Creates a summary that contains a layout.\n\n  When users navigate to the custom scalars dashboard, they will see a layout\n  based on the proto provided to this function.\n\n  Args:\n    scalars_layout: The scalars_layout_pb2.Layout proto that specifies the\n        layout.\n    collections: Optional list of graph collections keys. The new\n        summary op is added to these collections. Defaults to\n        `[Graph Keys.SUMMARIES]`.\n\n  Returns:\n    A tensor summary op that writes the layout to disk.\n  \"\"\"\n  # TODO(nickfelt): remove on-demand imports once dep situation is fixed.\n  import tensorflow.compat.v1 as tf\n\n  assert isinstance(scalars_layout, layout_pb2.Layout)\n  summary_metadata = metadata.create_summary_metadata()\n  return tf.summary.tensor_summary(name=metadata.CONFIG_SUMMARY_TAG,\n                                   tensor=tf.constant(\n                                       scalars_layout.SerializeToString(),\n                                       dtype=tf.string),\n                                   collections=collections,\n                                   summary_metadata=summary_metadata)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a summary that contains a layout.", "response": "def pb(scalars_layout):\n  \"\"\"Creates a summary that contains a layout.\n\n  When users navigate to the custom scalars dashboard, they will see a layout\n  based on the proto provided to this function.\n\n  Args:\n    scalars_layout: The scalars_layout_pb2.Layout proto that specifies the\n        layout.\n\n  Returns:\n    A summary proto containing the layout.\n  \"\"\"\n  # TODO(nickfelt): remove on-demand imports once dep situation is fixed.\n  import tensorflow.compat.v1 as tf\n\n  assert isinstance(scalars_layout, layout_pb2.Layout)\n  tensor = tf.make_tensor_proto(\n      scalars_layout.SerializeToString(), dtype=tf.string)\n  tf_summary_metadata = tf.SummaryMetadata.FromString(\n      metadata.create_summary_metadata().SerializeToString())\n  summary = tf.Summary()\n  summary.value.add(tag=metadata.CONFIG_SUMMARY_TAG,\n                    metadata=tf_summary_metadata,\n                    tensor=tensor)\n  return summary"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_convertible_with(self, other):\n        other = as_dimension(other)\n        return self._value is None or other.value is None or self._value == other.value", "response": "Returns true if this Dimension is convertible with other."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a Dimension that combines the information in self and other.", "response": "def merge_with(self, other):\n        \"\"\"Returns a Dimension that combines the information in `self` and `other`.\n\n        Dimensions are combined as follows:\n\n        ```python\n        tf.Dimension(n)   .merge_with(tf.Dimension(n))    == tf.Dimension(n)\n        tf.Dimension(n)   .merge_with(tf.Dimension(None)) == tf.Dimension(n)\n        tf.Dimension(None).merge_with(tf.Dimension(n))    == tf.Dimension(n)\n        tf.Dimension(None).merge_with(tf.Dimension(None)) == tf.Dimension(None)\n        tf.Dimension(n)   .merge_with(tf.Dimension(m))  # raises ValueError for n != m\n        ```\n\n        Args:\n          other: Another Dimension.\n\n        Returns:\n          A Dimension containing the combined information of `self` and\n          `other`.\n\n        Raises:\n          ValueError: If `self` and `other` are not convertible (see\n            is_convertible_with).\n        \"\"\"\n        other = as_dimension(other)\n        self.assert_is_convertible_with(other)\n        if self._value is None:\n            return Dimension(other.value)\n        else:\n            return Dimension(self._value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ndims(self):\n        if self._dims is None:\n            return None\n        else:\n            if self._ndims is None:\n                self._ndims = len(self._dims)\n            return self._ndims", "response": "Returns the rank of this shape or None if it is unspecified."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the total number of elements for incomplete shapes or None for incomplete shapes.", "response": "def num_elements(self):\n        \"\"\"Returns the total number of elements, or none for incomplete shapes.\"\"\"\n        if self.is_fully_defined():\n            size = 1\n            for dim in self._dims:\n                size *= dim.value\n            return size\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef merge_with(self, other):\n        other = as_shape(other)\n        if self._dims is None:\n            return other\n        else:\n            try:\n                self.assert_same_rank(other)\n                new_dims = []\n                for i, dim in enumerate(self._dims):\n                    new_dims.append(dim.merge_with(other[i]))\n                return TensorShape(new_dims)\n            except ValueError:\n                raise ValueError(\"Shapes %s and %s are not convertible\" % (self, other))", "response": "Returns a TensorShape containing the combined information in self and other."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the concatenation of the dimension in self and other.", "response": "def concatenate(self, other):\n        \"\"\"Returns the concatenation of the dimension in `self` and `other`.\n\n        *N.B.* If either `self` or `other` is completely unknown,\n        concatenation will discard information about the other shape. In\n        future, we might support concatenation that preserves this\n        information for use with slicing.\n\n        Args:\n          other: Another `TensorShape`.\n\n        Returns:\n          A `TensorShape` whose dimensions are the concatenation of the\n          dimensions in `self` and `other`.\n        \"\"\"\n        # TODO(mrry): Handle the case where we concatenate a known shape with a\n        # completely unknown shape, so that we can use the partial information.\n        other = as_shape(other)\n        if self._dims is None or other.dims is None:\n            return unknown_shape()\n        else:\n            return TensorShape(self._dims + other.dims)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef assert_same_rank(self, other):\n        other = as_shape(other)\n        if self.ndims is not None and other.ndims is not None:\n            if self.ndims != other.ndims:\n                raise ValueError(\n                    \"Shapes %s and %s must have the same rank\" % (self, other)\n                )", "response": "Raises an exception if self and other do not have convertible ranks."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a new shape based on self with the given rank.", "response": "def with_rank(self, rank):\n        \"\"\"Returns a shape based on `self` with the given rank.\n\n        This method promotes a completely unknown shape to one with a\n        known rank.\n\n        Args:\n          rank: An integer.\n\n        Returns:\n          A shape that is at least as specific as `self` with the given rank.\n\n        Raises:\n          ValueError: If `self` does not represent a shape with the given `rank`.\n        \"\"\"\n        try:\n            return self.merge_with(unknown_shape(ndims=rank))\n        except ValueError:\n            raise ValueError(\"Shape %s must have rank %d\" % (self, rank))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a new shape based on self with at least the given rank.", "response": "def with_rank_at_least(self, rank):\n        \"\"\"Returns a shape based on `self` with at least the given rank.\n\n        Args:\n          rank: An integer.\n\n        Returns:\n          A shape that is at least as specific as `self` with at least the given\n          rank.\n\n        Raises:\n          ValueError: If `self` does not represent a shape with at least the given\n            `rank`.\n        \"\"\"\n        if self.ndims is not None and self.ndims < rank:\n            raise ValueError(\"Shape %s must have rank at least %d\" % (self, rank))\n        else:\n            return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef with_rank_at_most(self, rank):\n        if self.ndims is not None and self.ndims > rank:\n            raise ValueError(\"Shape %s must have rank at most %d\" % (self, rank))\n        else:\n            return self", "response": "Returns a new shape based on self with at most the given rank."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns True iff self is convertible with other.", "response": "def is_convertible_with(self, other):\n        \"\"\"Returns True iff `self` is convertible with `other`.\n\n        Two possibly-partially-defined shapes are convertible if there\n        exists a fully-defined shape that both shapes can represent. Thus,\n        convertibility allows the shape inference code to reason about\n        partially-defined shapes. For example:\n\n        * TensorShape(None) is convertible with all shapes.\n\n        * TensorShape([None, None]) is convertible with all two-dimensional\n          shapes, such as TensorShape([32, 784]), and also TensorShape(None). It is\n          not convertible with, for example, TensorShape([None]) or\n          TensorShape([None, None, None]).\n\n        * TensorShape([32, None]) is convertible with all two-dimensional shapes\n          with size 32 in the 0th dimension, and also TensorShape([None, None])\n          and TensorShape(None). It is not convertible with, for example,\n          TensorShape([32]), TensorShape([32, None, 1]) or TensorShape([64, None]).\n\n        * TensorShape([32, 784]) is convertible with itself, and also\n          TensorShape([32, None]), TensorShape([None, 784]), TensorShape([None,\n          None]) and TensorShape(None). It is not convertible with, for example,\n          TensorShape([32, 1, 784]) or TensorShape([None]).\n\n        The convertibility relation is reflexive and symmetric, but not\n        transitive. For example, TensorShape([32, 784]) is convertible with\n        TensorShape(None), and TensorShape(None) is convertible with\n        TensorShape([4, 4]), but TensorShape([32, 784]) is not convertible with\n        TensorShape([4, 4]).\n\n        Args:\n          other: Another TensorShape.\n\n        Returns:\n          True iff `self` is convertible with `other`.\n\n        \"\"\"\n        other = as_shape(other)\n        if self._dims is not None and other.dims is not None:\n            if self.ndims != other.ndims:\n                return False\n            for x_dim, y_dim in zip(self._dims, other.dims):\n                if not x_dim.is_convertible_with(y_dim):\n                    return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the most specific TensorShape convertible with self and other.", "response": "def most_specific_convertible_shape(self, other):\n        \"\"\"Returns the most specific TensorShape convertible with `self` and `other`.\n\n        * TensorShape([None, 1]) is the most specific TensorShape convertible with\n          both TensorShape([2, 1]) and TensorShape([5, 1]). Note that\n          TensorShape(None) is also convertible with above mentioned TensorShapes.\n\n        * TensorShape([1, 2, 3]) is the most specific TensorShape convertible with\n          both TensorShape([1, 2, 3]) and TensorShape([1, 2, 3]). There are more\n          less specific TensorShapes convertible with above mentioned TensorShapes,\n          e.g. TensorShape([1, 2, None]), TensorShape(None).\n\n        Args:\n          other: Another `TensorShape`.\n\n        Returns:\n          A `TensorShape` which is the most specific convertible shape of `self`\n          and `other`.\n        \"\"\"\n\n        other = as_shape(other)\n        if self._dims is None or other.dims is None or self.ndims != other.ndims:\n            return unknown_shape()\n\n        dims = [(Dimension(None))] * self.ndims\n        for i, (d1, d2) in enumerate(zip(self._dims, other.dims)):\n            if d1 is not None and d2 is not None and d1 == d2:\n                dims[i] = d1\n        return TensorShape(dims)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns True iff self is fully defined in every dimension.", "response": "def is_fully_defined(self):\n        \"\"\"Returns True iff `self` is fully defined in every dimension.\"\"\"\n        return self._dims is not None and all(\n            dim.value is not None for dim in self._dims\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of integers or None for each dimension.", "response": "def as_list(self):\n        \"\"\"Returns a list of integers or `None` for each dimension.\n\n        Returns:\n          A list of integers or `None` for each dimension.\n\n        Raises:\n          ValueError: If `self` is an unknown shape with an unknown rank.\n        \"\"\"\n        if self._dims is None:\n            raise ValueError(\"as_list() is not defined on an unknown TensorShape.\")\n        return [dim.value for dim in self._dims]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn this shape as a TensorShapeProto.", "response": "def as_proto(self):\n        \"\"\"Returns this shape as a `TensorShapeProto`.\"\"\"\n        if self._dims is None:\n            return tensor_shape_pb2.TensorShapeProto(unknown_rank=True)\n        else:\n            return tensor_shape_pb2.TensorShapeProto(\n                dim=[\n                    tensor_shape_pb2.TensorShapeProto.Dim(\n                        size=-1 if d.value is None else d.value\n                    )\n                    for d in self._dims\n                ]\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts a PredictResponse to ClassificationResponse or RegressionResponse.", "response": "def convert_predict_response(pred, serving_bundle):\n  \"\"\"Converts a PredictResponse to ClassificationResponse or RegressionResponse.\n\n  Args:\n    pred: PredictResponse to convert.\n    serving_bundle: A `ServingBundle` object that contains the information about\n      the serving request that the response was generated by.\n\n  Returns:\n    A ClassificationResponse or RegressionResponse.\n  \"\"\"\n  output = pred.outputs[serving_bundle.predict_output_tensor]\n  raw_output = output.float_val\n  if serving_bundle.model_type == 'classification':\n    values = []\n    for example_index in range(output.tensor_shape.dim[0].size):\n      start = example_index * output.tensor_shape.dim[1].size\n      values.append(raw_output[start:start + output.tensor_shape.dim[1].size])\n  else:\n    values = raw_output\n  return convert_prediction_values(values, serving_bundle, pred.model_spec)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef convert_prediction_values(values, serving_bundle, model_spec=None):\n  if serving_bundle.model_type == 'classification':\n    response = classification_pb2.ClassificationResponse()\n    for example_index in range(len(values)):\n      classification = response.result.classifications.add()\n      for class_index in range(len(values[example_index])):\n        class_score = classification.classes.add()\n        class_score.score = values[example_index][class_index]\n        class_score.label = str(class_index)\n  else:\n    response = regression_pb2.RegressionResponse()\n    for example_index in range(len(values)):\n      regression = response.result.regressions.add()\n      regression.value = values[example_index]\n  if model_spec:\n    response.model_spec.CopyFrom(model_spec)\n  return response", "response": "Converts tensor values into ClassificationResponse or RegressionResponse."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _GetPurgeMessage(most_recent_step, most_recent_wall_time, event_step,\n                     event_wall_time, num_expired):\n  \"\"\"Return the string message associated with TensorBoard purges.\"\"\"\n  return ('Detected out of order event.step likely caused by a TensorFlow '\n          'restart. Purging {} expired tensor events from Tensorboard display '\n          'between the previous step: {} (timestamp: {}) and current step: {} '\n          '(timestamp: {}).'\n         ).format(num_expired, most_recent_step, most_recent_wall_time,\n                  event_step, event_wall_time)", "response": "Return the string message associated with TensorBoard purges."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a dict mapping tags to content specific to a plugin.", "response": "def PluginTagToContent(self, plugin_name):\n    \"\"\"Returns a dict mapping tags to content specific to that plugin.\n\n    Args:\n      plugin_name: The name of the plugin for which to fetch plugin-specific\n        content.\n\n    Raises:\n      KeyError: if the plugin name is not found.\n\n    Returns:\n      A dict mapping tags to plugin-specific content (which are always strings).\n      Those strings are often serialized protos.\n    \"\"\"\n    if plugin_name not in self._plugin_to_tag_to_content:\n      raise KeyError('Plugin %r could not be found.' % plugin_name)\n    with self._plugin_tag_locks[plugin_name]:\n      # Return a snapshot to avoid concurrent mutation and iteration issues.\n      return dict(self._plugin_to_tag_to_content[plugin_name])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncall whenever an event is loaded.", "response": "def _ProcessEvent(self, event):\n    \"\"\"Called whenever an event is loaded.\"\"\"\n    if self._first_event_timestamp is None:\n      self._first_event_timestamp = event.wall_time\n\n    if event.HasField('file_version'):\n      new_file_version = _ParseFileVersion(event.file_version)\n      if self.file_version and self.file_version != new_file_version:\n        ## This should not happen.\n        logger.warn(('Found new file_version for event.proto. This will '\n                         'affect purging logic for TensorFlow restarts. '\n                         'Old: {0} New: {1}').format(self.file_version,\n                                                     new_file_version))\n      self.file_version = new_file_version\n\n    self._MaybePurgeOrphanedData(event)\n\n    ## Process the event.\n    # GraphDef and MetaGraphDef are handled in a special way:\n    # If no graph_def Event is available, but a meta_graph_def is, and it\n    # contains a graph_def, then use the meta_graph_def.graph_def as our graph.\n    # If a graph_def Event is available, always prefer it to the graph_def\n    # inside the meta_graph_def.\n    if event.HasField('graph_def'):\n      if self._graph is not None:\n        logger.warn(\n            ('Found more than one graph event per run, or there was '\n             'a metagraph containing a graph_def, as well as one or '\n             'more graph events.  Overwriting the graph with the '\n             'newest event.'))\n      self._graph = event.graph_def\n      self._graph_from_metagraph = False\n    elif event.HasField('meta_graph_def'):\n      if self._meta_graph is not None:\n        logger.warn(('Found more than one metagraph event per run. '\n                         'Overwriting the metagraph with the newest event.'))\n      self._meta_graph = event.meta_graph_def\n      if self._graph is None or self._graph_from_metagraph:\n        # We may have a graph_def in the metagraph.  If so, and no\n        # graph_def is directly available, use this one instead.\n        meta_graph = meta_graph_pb2.MetaGraphDef()\n        meta_graph.ParseFromString(self._meta_graph)\n        if meta_graph.graph_def:\n          if self._graph is not None:\n            logger.warn(\n                ('Found multiple metagraphs containing graph_defs,'\n                 'but did not find any graph events.  Overwriting the '\n                 'graph with the newest metagraph version.'))\n          self._graph_from_metagraph = True\n          self._graph = meta_graph.graph_def.SerializeToString()\n    elif event.HasField('tagged_run_metadata'):\n      tag = event.tagged_run_metadata.tag\n      if tag in self._tagged_metadata:\n        logger.warn('Found more than one \"run metadata\" event with tag ' +\n                        tag + '. Overwriting it with the newest event.')\n      self._tagged_metadata[tag] = event.tagged_run_metadata.run_metadata\n    elif event.HasField('summary'):\n      for value in event.summary.value:\n        value = data_compat.migrate_value(value)\n\n        if value.HasField('metadata'):\n          tag = value.tag\n          # We only store the first instance of the metadata. This check\n          # is important: the `FileWriter` does strip metadata from all\n          # values except the first one per each tag, but a new\n          # `FileWriter` is created every time a training job stops and\n          # restarts. Hence, we must also ignore non-initial metadata in\n          # this logic.\n          if tag not in self.summary_metadata:\n            self.summary_metadata[tag] = value.metadata\n            plugin_data = value.metadata.plugin_data\n            if plugin_data.plugin_name:\n              with self._plugin_tag_locks[plugin_data.plugin_name]:\n                self._plugin_to_tag_to_content[plugin_data.plugin_name][tag] = (\n                    plugin_data.content)\n            else:\n              logger.warn(\n                  ('This summary with tag %r is oddly not associated with a '\n                   'plugin.'), tag)\n\n        for summary_type, summary_func in SUMMARY_TYPES.items():\n          if value.HasField(summary_type):\n            datum = getattr(value, summary_type)\n            tag = value.tag\n            if summary_type == 'tensor' and not tag:\n              # This tensor summary was created using the old method that used\n              # plugin assets. We must still continue to support it.\n              tag = value.node_name\n            getattr(self, summary_func)(tag, event.wall_time, event.step, datum)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef Tags(self):\n    return {\n        TENSORS: list(self.tensors_by_tag.keys()),\n        # Use a heuristic: if the metagraph is available, but\n        # graph is not, then we assume the metagraph contains the graph.\n        GRAPH: self._graph is not None,\n        META_GRAPH: self._meta_graph is not None,\n        RUN_METADATA: list(self._tagged_metadata.keys())\n    }", "response": "Return all tags found in the value stream."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _CheckForOutOfOrderStepAndMaybePurge(self, event):\n    if event.step < self.most_recent_step and event.HasField('summary'):\n      self._Purge(event, by_tags=True)", "response": "Checks if the event is out - of - order and discards expired events for tags."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _Purge(self, event, by_tags):\n    ## Keep data in reservoirs that has a step less than event.step\n    _NotExpired = lambda x: x.step < event.step\n\n    num_expired = 0\n    if by_tags:\n      for value in event.summary.value:\n        if value.tag in self.tensors_by_tag:\n          tag_reservoir = self.tensors_by_tag[value.tag]\n          num_expired += tag_reservoir.FilterItems(\n              _NotExpired, _TENSOR_RESERVOIR_KEY)\n    else:\n      for tag_reservoir in six.itervalues(self.tensors_by_tag):\n        num_expired += tag_reservoir.FilterItems(\n            _NotExpired, _TENSOR_RESERVOIR_KEY)\n    if num_expired > 0:\n      purge_msg = _GetPurgeMessage(self.most_recent_step,\n                                   self.most_recent_wall_time, event.step,\n                                   event.wall_time, num_expired)\n      logger.warn(purge_msg)", "response": "Purge all events that have occurred after the given event. step."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load(self, context):\n    try:\n      # pylint: disable=g-import-not-at-top,unused-import\n      import tensorflow\n    except ImportError:\n      return\n    # pylint: disable=g-import-not-at-top\n    from tensorboard.plugins.beholder.beholder_plugin import BeholderPlugin\n    return BeholderPlugin(context)", "response": "Returns the plugin if possible."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwalk the nested keras layer configuration in preorder.", "response": "def _walk_layers(keras_layer):\n  \"\"\"Walks the nested keras layer configuration in preorder.\n  Args:\n    keras_layer: Keras configuration from model.to_json.\n\n  Yields:\n    A tuple of (name_scope, layer_config).\n    name_scope: a string representing a scope name, similar to that of tf.name_scope.\n    layer_config: a dict representing a Keras layer configuration.\n  \"\"\"\n  yield ('', keras_layer)\n  if keras_layer.get('config').get('layers'):\n    name_scope = keras_layer.get('config').get('name')\n    for layer in keras_layer.get('config').get('layers'):\n      for (sub_name_scope, sublayer) in _walk_layers(layer):\n        sub_name_scope = '%s/%s' % (\n            name_scope, sub_name_scope) if sub_name_scope else name_scope\n        yield (sub_name_scope, sublayer)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _update_dicts(name_scope,\n                  model_layer,\n                  input_to_in_layer,\n                  model_name_to_output,\n                  prev_node_name):\n  \"\"\"Updates input_to_in_layer, model_name_to_output, and prev_node_name\n  based on the model_layer.\n\n  Args:\n    name_scope: a string representing a scope name, similar to that of tf.name_scope.\n    model_layer: a dict representing a Keras model configuration.\n    input_to_in_layer: a dict mapping Keras.layers.Input to inbound layer.\n    model_name_to_output: a dict mapping Keras Model name to output layer of the model.\n    prev_node_name: a string representing a previous, in sequential model layout,\n                    node name.\n\n  Returns:\n    A tuple of (input_to_in_layer, model_name_to_output, prev_node_name).\n    input_to_in_layer: a dict mapping Keras.layers.Input to inbound layer.\n    model_name_to_output: a dict mapping Keras Model name to output layer of the model.\n    prev_node_name: a string representing a previous, in sequential model layout,\n                    node name.\n  \"\"\"\n  layer_config = model_layer.get('config')\n  if not layer_config.get('layers'):\n    raise ValueError('layer is not a model.')\n\n  node_name = _scoped_name(name_scope, layer_config.get('name'))\n  input_layers = layer_config.get('input_layers')\n  output_layers = layer_config.get('output_layers')\n  inbound_nodes = model_layer.get('inbound_nodes')\n\n  is_functional_model = bool(input_layers and output_layers)\n  # In case of [1] and the parent model is functional, current layer\n  # will have the 'inbound_nodes' property.\n  is_parent_functional_model = bool(inbound_nodes)\n\n  if is_parent_functional_model and is_functional_model:\n    for (input_layer, inbound_node) in zip(input_layers, inbound_nodes):\n      input_layer_name = _scoped_name(node_name, input_layer)\n      inbound_node_name = _scoped_name(name_scope, inbound_node[0])\n      input_to_in_layer[input_layer_name] = inbound_node_name\n  elif is_parent_functional_model and not is_functional_model:\n    # Sequential model can take only one input. Make sure inbound to the\n    # model is linked to the first layer in the Sequential model.\n    prev_node_name = _scoped_name(name_scope, inbound_nodes[0][0][0])\n  elif not is_parent_functional_model and prev_node_name and is_functional_model:\n    assert len(input_layers) == 1, (\n        'Cannot have multi-input Functional model when parent model '\n        'is not Functional. Number of input layers: %d' % len(input_layer))\n    input_layer = input_layers[0]\n    input_layer_name = _scoped_name(node_name, input_layer)\n    input_to_in_layer[input_layer_name] = prev_node_name\n\n  if is_functional_model and output_layers:\n    layers = _norm_to_list_of_layers(output_layers)\n    layer_names = [_scoped_name(node_name, layer[0]) for layer in layers]\n    model_name_to_output[node_name] = layer_names\n  else:\n    last_layer = layer_config.get('layers')[-1]\n    last_layer_name = last_layer.get('config').get('name')\n    output_node = _scoped_name(node_name, last_layer_name)\n    model_name_to_output[node_name] = [output_node]\n  return (input_to_in_layer, model_name_to_output, prev_node_name)", "response": "Updates input_to_in_layer model_name_to_output and prev_node_name_to_output based on the model_layer."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert a Keras model to a GraphDef representation of the Keras model.", "response": "def keras_model_to_graph_def(keras_layer):\n  \"\"\"Returns a GraphDef representation of the Keras model in a dict form.\n\n  Note that it only supports models that implemented to_json().\n\n  Args:\n    keras_layer: A dict from Keras model.to_json().\n\n  Returns:\n    A GraphDef representation of the layers in the model.\n  \"\"\"\n  input_to_layer = {}\n  model_name_to_output = {}\n  g = GraphDef()\n\n  # Sequential model layers do not have a field \"inbound_nodes\" but\n  # instead are defined implicitly via order of layers.\n  prev_node_name = None\n\n  for (name_scope, layer) in _walk_layers(keras_layer):\n    if _is_model(layer):\n      (input_to_layer, model_name_to_output, prev_node_name) = _update_dicts(\n          name_scope, layer, input_to_layer, model_name_to_output, prev_node_name)\n      continue\n\n    layer_config = layer.get('config')\n    node_name = _scoped_name(name_scope, layer_config.get('name'))\n\n    node_def = g.node.add()\n    node_def.name = node_name\n\n    if layer.get('class_name') is not None:\n      keras_cls_name = layer.get('class_name').encode('ascii')\n      node_def.attr['keras_class'].s = keras_cls_name\n\n    if layer_config.get('dtype') is not None:\n      tf_dtype = dtypes.as_dtype(layer_config.get('dtype'))\n      node_def.attr['dtype'].type = tf_dtype.as_datatype_enum\n\n    if layer.get('inbound_nodes') is not None:\n      for maybe_inbound_node in layer.get('inbound_nodes'):\n        inbound_nodes = _norm_to_list_of_layers(maybe_inbound_node)\n        for [name, size, index, _] in inbound_nodes:\n          inbound_name = _scoped_name(name_scope, name)\n          # An input to a layer can be output from a model. In that case, the name\n          # of inbound_nodes to a layer is a name of a model. Remap the name of the\n          # model to output layer of the model. Also, since there can be multiple\n          # outputs in a model, make sure we pick the right output_layer from the model.\n          inbound_node_names = model_name_to_output.get(\n              inbound_name, [inbound_name])\n          node_def.input.append(inbound_node_names[index])\n    elif prev_node_name is not None:\n      node_def.input.append(prev_node_name)\n\n    if node_name in input_to_layer:\n      node_def.input.append(input_to_layer.get(node_name))\n\n    prev_node_name = node_def.name\n\n  return g"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load(self, context):\n    try:\n      # pylint: disable=g-import-not-at-top,unused-import\n      import tensorflow\n    except ImportError:\n      return\n    # pylint: disable=g-import-not-at-top\n    from tensorboard.plugins.hparams.hparams_plugin import HParamsPlugin\n    return HParamsPlugin(context)", "response": "Returns the HParamsPlugin instance if possible."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts Markdown to HTML that s safe to splice into the DOM.", "response": "def markdown_to_safe_html(markdown_string):\n  \"\"\"Convert Markdown to HTML that's safe to splice into the DOM.\n\n  Arguments:\n    markdown_string: A Unicode string or UTF-8--encoded bytestring\n      containing Markdown source. Markdown tables are supported.\n\n  Returns:\n    A string containing safe HTML.\n  \"\"\"\n  warning = ''\n  # Convert to utf-8 whenever we have a binary input.\n  if isinstance(markdown_string, six.binary_type):\n    markdown_string_decoded = markdown_string.decode('utf-8')\n    # Remove null bytes and warn if there were any, since it probably means\n    # we were given a bad encoding.\n    markdown_string = markdown_string_decoded.replace(u'\\x00', u'')\n    num_null_bytes = len(markdown_string_decoded) - len(markdown_string)\n    if num_null_bytes:\n      warning = ('<!-- WARNING: discarded %d null bytes in markdown string '\n                 'after UTF-8 decoding -->\\n') % num_null_bytes\n\n  string_html = markdown.markdown(\n      markdown_string, extensions=['markdown.extensions.tables'])\n  string_sanitized = bleach.clean(\n      string_html, tags=_ALLOWED_TAGS, attributes=_ALLOWED_ATTRIBUTES)\n  return warning + string_sanitized"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef as_dtype(type_value):\n    if isinstance(type_value, DType):\n        return type_value\n\n    try:\n        return _INTERN_TABLE[type_value]\n    except KeyError:\n        pass\n\n    try:\n        return _STRING_TO_TF[type_value]\n    except KeyError:\n        pass\n\n    try:\n        return _PYTHON_TO_TF[type_value]\n    except KeyError:\n        pass\n\n    if isinstance(type_value, np.dtype):\n        # The numpy dtype for strings is variable length. We can not compare\n        # dtype with a single constant (np.string does not exist) to decide\n        # dtype is a \"string\" type. We need to compare the dtype.type to be\n        # sure it's a string type.\n        if type_value.type == np.string_ or type_value.type == np.unicode_:\n            return string\n\n    if isinstance(type_value, (type, np.dtype)):\n        for key, val in _NP_TO_TF:\n            try:\n                if key == type_value:\n                    return val\n            except TypeError as e:\n                raise TypeError(\n                    \"Cannot convert {} to a dtype. {}\".format(type_value, e)\n                )\n\n    raise TypeError(\"Cannot convert value %r to a TensorFlow DType.\" % type_value)", "response": "Converts the given type_value to a TensorFlow DType object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef real_dtype(self):\n        base = self.base_dtype\n        if base == complex64:\n            return float32\n        elif base == complex128:\n            return float64\n        else:\n            return self", "response": "Returns the dtype correspond to this dtype s real part."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn whether this is a non - quantized integer type.", "response": "def is_integer(self):\n        \"\"\"Returns whether this is a (non-quantized) integer type.\"\"\"\n        return (\n            self.is_numpy_compatible\n            and not self.is_quantized\n            and np.issubdtype(self.as_numpy_dtype, np.integer)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_floating(self):\n        return (\n            self.is_numpy_compatible and np.issubdtype(self.as_numpy_dtype, np.floating)\n        ) or self.base_dtype == bfloat16", "response": "Returns whether this is a non - quantized floating point type."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the minimum representable value in this data type.", "response": "def min(self):\n        \"\"\"Returns the minimum representable value in this data type.\n\n        Raises:\n          TypeError: if this is a non-numeric, unordered, or quantized type.\n\n        \"\"\"\n        if self.is_quantized or self.base_dtype in (\n            bool,\n            string,\n            complex64,\n            complex128,\n        ):\n            raise TypeError(\"Cannot find minimum value of %s.\" % self)\n\n        # there is no simple way to get the min value of a dtype, we have to check\n        # float and int types separately\n        try:\n            return np.finfo(self.as_numpy_dtype()).min\n        except:  # bare except as possible raises by finfo not documented\n            try:\n                return np.iinfo(self.as_numpy_dtype()).min\n            except:\n                if self.base_dtype == bfloat16:\n                    return _np_bfloat16(float.fromhex(\"-0x1.FEp127\"))\n                raise TypeError(\"Cannot find minimum value of %s.\" % self)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef limits(self, clip_negative=True):\n        min, max = dtype_range[self.as_numpy_dtype]  # pylint: disable=redefined-builtin\n        if clip_negative:\n            min = 0  # pylint: disable=redefined-builtin\n        return min, max", "response": "Return intensity limits of the image dtype."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn True if the other DType will be implicitly converted to this DType.", "response": "def is_compatible_with(self, other):\n        \"\"\"Returns True if the `other` DType will be converted to this DType.\n\n        The conversion rules are as follows:\n\n        ```python\n        DType(T)       .is_compatible_with(DType(T))        == True\n        DType(T)       .is_compatible_with(DType(T).as_ref) == True\n        DType(T).as_ref.is_compatible_with(DType(T))        == False\n        DType(T).as_ref.is_compatible_with(DType(T).as_ref) == True\n        ```\n\n        Args:\n          other: A `DType` (or object that may be converted to a `DType`).\n\n        Returns:\n          True if a Tensor of the `other` `DType` will be implicitly converted to\n          this `DType`.\n        \"\"\"\n        other = as_dtype(other)\n        return self._type_enum in (\n            other.as_datatype_enum,\n            other.base_dtype.as_datatype_enum,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstarting listening on the given gRPC port.", "response": "def listen(self, grpc_port):\n    \"\"\"Start listening on the given gRPC port.\n\n    This method of an instance of InteractiveDebuggerPlugin can be invoked at\n    most once. This method is not thread safe.\n\n    Args:\n      grpc_port: port number to listen at.\n\n    Raises:\n      ValueError: If this instance is already listening at a gRPC port.\n    \"\"\"\n    if self._grpc_port:\n      raise ValueError(\n          'This InteractiveDebuggerPlugin instance is already listening at '\n          'gRPC port %d' % self._grpc_port)\n    self._grpc_port = grpc_port\n\n    sys.stderr.write('Creating InteractiveDebuggerPlugin at port %d\\n' %\n                     self._grpc_port)\n    sys.stderr.flush()\n    self._debugger_data_server = (\n        interactive_debugger_server_lib.InteractiveDebuggerDataServer(\n            self._grpc_port))\n\n    self._server_thread = threading.Thread(\n        target=self._debugger_data_server.run_server)\n    self._server_thread.start()\n\n    signal.signal(signal.SIGINT, self.signal_handler)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_plugin_apps(self):\n    return {\n        _ACK_ROUTE: self._serve_ack,\n        _COMM_ROUTE: self._serve_comm,\n        _DEBUGGER_GRPC_HOST_PORT_ROUTE: self._serve_debugger_grpc_host_port,\n        _DEBUGGER_GRAPH_ROUTE: self._serve_debugger_graph,\n        _GATED_GRPC_ROUTE: self._serve_gated_grpc,\n        _TENSOR_DATA_ROUTE: self._serve_tensor_data,\n        _SOURCE_CODE_ROUTE: self._serve_source_code,\n    }", "response": "Gets a mapping between routes and handlers."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_active(self):\n    if not self._multiplexer:\n      return False\n    return bool(self._multiplexer.PluginRunToTagToContent(metadata.PLUGIN_NAME))", "response": "The audio plugin is active iff any run has at least one relevant tag."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns information about the tags in each run.", "response": "def _index_impl(self):\n    \"\"\"Return information about the tags in each run.\n\n    Result is a dictionary of the form\n\n        {\n          \"runName1\": {\n            \"tagName1\": {\n              \"displayName\": \"The first tag\",\n              \"description\": \"<p>Long ago there was just one tag...</p>\",\n              \"samples\": 3\n            },\n            \"tagName2\": ...,\n            ...\n          },\n          \"runName2\": ...,\n          ...\n        }\n\n    For each tag, `samples` is the greatest number of audio clips that\n    appear at any particular step. (It's not related to \"samples of a\n    waveform.\") For example, if for tag `minibatch_input` there are\n    five audio clips at step 0 and ten audio clips at step 1, then the\n    dictionary for `\"minibatch_input\"` will contain `\"samples\": 10`.\n    \"\"\"\n    runs = self._multiplexer.Runs()\n    result = {run: {} for run in runs}\n\n    mapping = self._multiplexer.PluginRunToTagToContent(metadata.PLUGIN_NAME)\n    for (run, tag_to_content) in six.iteritems(mapping):\n      for tag in tag_to_content:\n        summary_metadata = self._multiplexer.SummaryMetadata(run, tag)\n        tensor_events = self._multiplexer.Tensors(run, tag)\n        samples = max([self._number_of_samples(event.tensor_proto)\n                       for event in tensor_events] + [0])\n        result[run][tag] = {'displayName': summary_metadata.display_name,\n                            'description': plugin_util.markdown_to_safe_html(\n                                summary_metadata.summary_description),\n                            'samples': samples}\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _serve_audio_metadata(self, request):\n    tag = request.args.get('tag')\n    run = request.args.get('run')\n    sample = int(request.args.get('sample', 0))\n\n    events = self._multiplexer.Tensors(run, tag)\n    response = self._audio_response_for_run(events, run, tag, sample)\n    return http_util.Respond(request, response, 'application/json')", "response": "This function serves a list of audio metadata for a single run."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _audio_response_for_run(self, tensor_events, run, tag, sample):\n    response = []\n    index = 0\n    filtered_events = self._filter_by_sample(tensor_events, sample)\n    content_type = self._get_mime_type(run, tag)\n    for (index, tensor_event) in enumerate(filtered_events):\n      data = tensor_util.make_ndarray(tensor_event.tensor_proto)\n      label = data[sample, 1]\n      response.append({\n          'wall_time': tensor_event.wall_time,\n          'step': tensor_event.step,\n          'label': plugin_util.markdown_to_safe_html(label),\n          'contentType': content_type,\n          'query': self._query_for_individual_audio(run, tag, sample, index)\n      })\n    return response", "response": "Builds a JSON - serializable object with information about audio."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nbuilds a URL for accessing the specified audio entry in the given run with the given tag and sample and index.", "response": "def _query_for_individual_audio(self, run, tag, sample, index):\n    \"\"\"Builds a URL for accessing the specified audio.\n\n    This should be kept in sync with _serve_audio_metadata. Note that the URL is\n    *not* guaranteed to always return the same audio, since audio may be\n    unloaded from the reservoir as new audio entries come in.\n\n    Args:\n      run: The name of the run.\n      tag: The tag.\n      index: The index of the audio entry. Negative values are OK.\n\n    Returns:\n      A string representation of a URL that will load the index-th sampled audio\n      in the given run with the given tag.\n    \"\"\"\n    query_string = urllib.parse.urlencode({\n        'run': run,\n        'tag': tag,\n        'sample': sample,\n        'index': index,\n    })\n    return query_string"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _serve_individual_audio(self, request):\n    tag = request.args.get('tag')\n    run = request.args.get('run')\n    index = int(request.args.get('index'))\n    sample = int(request.args.get('sample', 0))\n    events = self._filter_by_sample(self._multiplexer.Tensors(run, tag), sample)\n    data = tensor_util.make_ndarray(events[index].tensor_proto)[sample, 0]\n    mime_type = self._get_mime_type(run, tag)\n    return http_util.Respond(request, data, mime_type)", "response": "Serve encoded audio data."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwrites the usage text to stdout with some help text.", "response": "def _usage(shorthelp):\n    \"\"\"Writes __main__'s docstring to stdout with some help text.\n\n    Args:\n      shorthelp: bool, if True, prints only flags from the main module,\n          rather than all flags.\n    \"\"\"\n    doc = _sys.modules['__main__'].__doc__\n    if not doc:\n        doc = '\\nUSAGE: %s [flags]\\n' % _sys.argv[0]\n        doc = flags.text_wrap(doc, indent='       ', firstline_indent='')\n    else:\n        # Replace all '%s' with sys.argv[0], and all '%%' with '%'.\n        num_specifiers = doc.count('%') - 2 * doc.count('%%')\n        try:\n            doc %= (_sys.argv[0],) * num_specifiers\n        except (OverflowError, TypeError, ValueError):\n            # Just display the docstring as-is.\n            pass\n    if shorthelp:\n        flag_str = flags.FLAGS.main_module_help()\n    else:\n        flag_str = str(flags.FLAGS)\n    try:\n        _sys.stdout.write(doc)\n        if flag_str:\n            _sys.stdout.write('\\nflags:\\n')\n            _sys.stdout.write(flag_str)\n        _sys.stdout.write('\\n')\n    except IOError as e:\n        # We avoid printing a huge backtrace if we get EPIPE, because\n        # \"foo.par --help | less\" is a frequent use case.\n        if e.errno != _errno.EPIPE:\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrunning the program with an optional main function and argv list.", "response": "def run(main=None, argv=None):\n    \"\"\"Runs the program with an optional 'main' function and 'argv' list.\"\"\"\n\n    # Define help flags.\n    _define_help_flags()\n\n    # Parse known flags.\n    argv = flags.FLAGS(_sys.argv if argv is None else argv, known_only=True)\n\n    main = main or _sys.modules['__main__'].main\n\n    # Call the main function, passing through any arguments\n    # to the final program.\n    _sys.exit(main(argv))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a legacy image summary op for use in a TensorFlow graph.", "response": "def op(name,\n       images,\n       max_outputs=3,\n       display_name=None,\n       description=None,\n       collections=None):\n  \"\"\"Create a legacy image summary op for use in a TensorFlow graph.\n\n  Arguments:\n    name: A unique name for the generated summary node.\n    images: A `Tensor` representing pixel data with shape `[k, h, w, c]`,\n      where `k` is the number of images, `h` and `w` are the height and\n      width of the images, and `c` is the number of channels, which\n      should be 1, 3, or 4. Any of the dimensions may be statically\n      unknown (i.e., `None`).\n    max_outputs: Optional `int` or rank-0 integer `Tensor`. At most this\n      many images will be emitted at each step. When more than\n      `max_outputs` many images are provided, the first `max_outputs` many\n      images will be used and the rest silently discarded.\n    display_name: Optional name for this summary in TensorBoard, as a\n      constant `str`. Defaults to `name`.\n    description: Optional long-form description for this summary, as a\n      constant `str`. Markdown is supported. Defaults to empty.\n    collections: Optional list of graph collections keys. The new\n      summary op is added to these collections. Defaults to\n      `[Graph Keys.SUMMARIES]`.\n\n  Returns:\n    A TensorFlow summary op.\n  \"\"\"\n  # TODO(nickfelt): remove on-demand imports once dep situation is fixed.\n  import tensorflow.compat.v1 as tf\n\n  if display_name is None:\n    display_name = name\n  summary_metadata = metadata.create_summary_metadata(\n      display_name=display_name, description=description)\n  with tf.name_scope(name), \\\n       tf.control_dependencies([tf.assert_rank(images, 4),\n                                tf.assert_type(images, tf.uint8),\n                                tf.assert_non_negative(max_outputs)]):\n    limited_images = images[:max_outputs]\n    encoded_images = tf.map_fn(tf.image.encode_png, limited_images,\n                               dtype=tf.string,\n                               name='encode_each_image')\n    image_shape = tf.shape(input=images)\n    dimensions = tf.stack([tf.as_string(image_shape[2], name='width'),\n                           tf.as_string(image_shape[1], name='height')],\n                          name='dimensions')\n    tensor = tf.concat([dimensions, encoded_images], axis=0)\n    return tf.summary.tensor_summary(name='image_summary',\n                                     tensor=tensor,\n                                     collections=collections,\n                                     summary_metadata=summary_metadata)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a legacy image summary protobuf.", "response": "def pb(name, images, max_outputs=3, display_name=None, description=None):\n  \"\"\"Create a legacy image summary protobuf.\n\n  This behaves as if you were to create an `op` with the same arguments\n  (wrapped with constant tensors where appropriate) and then execute\n  that summary op in a TensorFlow session.\n\n  Arguments:\n    name: A unique name for the generated summary, including any desired\n      name scopes.\n    images: An `np.array` representing pixel data with shape\n      `[k, h, w, c]`, where `k` is the number of images, `w` and `h` are\n      the width and height of the images, and `c` is the number of\n      channels, which should be 1, 3, or 4.\n    max_outputs: Optional `int`. At most this many images will be\n      emitted. If more than this many images are provided, the first\n      `max_outputs` many images will be used and the rest silently\n      discarded.\n    display_name: Optional name for this summary in TensorBoard, as a\n      `str`. Defaults to `name`.\n    description: Optional long-form description for this summary, as a\n      `str`. Markdown is supported. Defaults to empty.\n\n  Returns:\n    A `tf.Summary` protobuf object.\n  \"\"\"\n  # TODO(nickfelt): remove on-demand imports once dep situation is fixed.\n  import tensorflow.compat.v1 as tf\n\n  images = np.array(images).astype(np.uint8)\n  if images.ndim != 4:\n    raise ValueError('Shape %r must have rank 4' % (images.shape, ))\n\n  limited_images = images[:max_outputs]\n  encoded_images = [encoder.encode_png(image) for image in limited_images]\n  (width, height) = (images.shape[2], images.shape[1])\n  content = [str(width), str(height)] + encoded_images\n  tensor = tf.make_tensor_proto(content, dtype=tf.string)\n\n  if display_name is None:\n    display_name = name\n  summary_metadata = metadata.create_summary_metadata(\n      display_name=display_name, description=description)\n  tf_summary_metadata = tf.SummaryMetadata.FromString(\n      summary_metadata.SerializeToString())\n\n  summary = tf.Summary()\n  summary.value.add(tag='%s/image_summary' % name,\n                    metadata=tf_summary_metadata,\n                    tensor=tensor)\n  return summary"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\napplies user per - summary size guidance overrides.", "response": "def tensor_size_guidance_from_flags(flags):\n  \"\"\"Apply user per-summary size guidance overrides.\"\"\"\n\n  tensor_size_guidance = dict(DEFAULT_TENSOR_SIZE_GUIDANCE)\n  if not flags or not flags.samples_per_plugin:\n    return tensor_size_guidance\n\n  for token in flags.samples_per_plugin.split(','):\n    k, v = token.strip().split('=')\n    tensor_size_guidance[k] = int(v)\n\n  return tensor_size_guidance"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef standard_tensorboard_wsgi(flags, plugin_loaders, assets_zip_provider):\n  multiplexer = event_multiplexer.EventMultiplexer(\n      size_guidance=DEFAULT_SIZE_GUIDANCE,\n      tensor_size_guidance=tensor_size_guidance_from_flags(flags),\n      purge_orphaned_data=flags.purge_orphaned_data,\n      max_reload_threads=flags.max_reload_threads)\n  loading_multiplexer = multiplexer\n  reload_interval = flags.reload_interval\n  # For db import op mode, prefer reloading in a child process. See\n  # https://github.com/tensorflow/tensorboard/issues/1467\n  reload_task = flags.reload_task\n  if reload_task == 'auto' and flags.db_import and flags.db_import_use_op:\n    reload_task == 'process'\n  db_uri = flags.db\n  # For DB import mode, create a DB file if we weren't given one.\n  if flags.db_import and not flags.db:\n    tmpdir = tempfile.mkdtemp(prefix='tbimport')\n    atexit.register(shutil.rmtree, tmpdir)\n    db_uri = 'sqlite:%s/tmp.sqlite' % tmpdir\n  db_module, db_connection_provider = get_database_info(db_uri)\n  if flags.db_import:\n    # DB import mode.\n    if db_module != sqlite3:\n      raise base_plugin.FlagsError('--db_import is only compatible with sqlite DBs')\n    logger.info('Importing logdir into DB at %s', db_uri)\n    loading_multiplexer = db_import_multiplexer.DbImportMultiplexer(\n        db_connection_provider=db_connection_provider,\n        purge_orphaned_data=flags.purge_orphaned_data,\n        max_reload_threads=flags.max_reload_threads,\n        use_import_op=flags.db_import_use_op)\n  elif flags.db:\n    # DB read-only mode, never load event logs.\n    reload_interval = -1\n  plugin_name_to_instance = {}\n  context = base_plugin.TBContext(\n      db_module=db_module,\n      db_connection_provider=db_connection_provider,\n      db_uri=db_uri,\n      flags=flags,\n      logdir=flags.logdir,\n      multiplexer=multiplexer,\n      assets_zip_provider=assets_zip_provider,\n      plugin_name_to_instance=plugin_name_to_instance,\n      window_title=flags.window_title)\n  plugins = []\n  for loader in plugin_loaders:\n    plugin = loader.load(context)\n    if plugin is None:\n      continue\n    plugins.append(plugin)\n    plugin_name_to_instance[plugin.plugin_name] = plugin\n  return TensorBoardWSGIApp(flags.logdir, plugins, loading_multiplexer,\n                            reload_interval, flags.path_prefix,\n                            reload_task)", "response": "Construct a TensorBoard WSGI application with standard plugins and multiplexer."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef TensorBoardWSGIApp(logdir, plugins, multiplexer, reload_interval,\n                       path_prefix='', reload_task='auto'):\n  \"\"\"Constructs the TensorBoard application.\n\n  Args:\n    logdir: the logdir spec that describes where data will be loaded.\n      may be a directory, or comma,separated list of directories, or colons\n      can be used to provide named directories\n    plugins: A list of base_plugin.TBPlugin subclass instances.\n    multiplexer: The EventMultiplexer with TensorBoard data to serve\n    reload_interval: How often (in seconds) to reload the Multiplexer.\n      Zero means reload just once at startup; negative means never load.\n    path_prefix: A prefix of the path when app isn't served from root.\n    reload_task: Indicates the type of background task to reload with.\n\n  Returns:\n    A WSGI application that implements the TensorBoard backend.\n\n  Raises:\n    ValueError: If something is wrong with the plugin configuration.\n\n  :type plugins: list[base_plugin.TBPlugin]\n  :rtype: TensorBoardWSGI\n  \"\"\"\n  path_to_run = parse_event_files_spec(logdir)\n  if reload_interval >= 0:\n    # We either reload the multiplexer once when TensorBoard starts up, or we\n    # continuously reload the multiplexer.\n    start_reloading_multiplexer(multiplexer, path_to_run, reload_interval,\n                                reload_task)\n  return TensorBoardWSGI(plugins, path_prefix)", "response": "Constructs a TensorBoard WSGI application."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses the logdir into a dict mapping directory paths to run group names.", "response": "def parse_event_files_spec(logdir):\n  \"\"\"Parses `logdir` into a map from paths to run group names.\n\n  The events files flag format is a comma-separated list of path specifications.\n  A path specification either looks like 'group_name:/path/to/directory' or\n  '/path/to/directory'; in the latter case, the group is unnamed. Group names\n  cannot start with a forward slash: /foo:bar/baz will be interpreted as a\n  spec with no name and path '/foo:bar/baz'.\n\n  Globs are not supported.\n\n  Args:\n    logdir: A comma-separated list of run specifications.\n  Returns:\n    A dict mapping directory paths to names like {'/path/to/directory': 'name'}.\n    Groups without an explicit name are named after their path. If logdir is\n    None, returns an empty dict, which is helpful for testing things that don't\n    require any valid runs.\n  \"\"\"\n  files = {}\n  if logdir is None:\n    return files\n  # Make sure keeping consistent with ParseURI in core/lib/io/path.cc\n  uri_pattern = re.compile('[a-zA-Z][0-9a-zA-Z.]*://.*')\n  for specification in logdir.split(','):\n    # Check if the spec contains group. A spec start with xyz:// is regarded as\n    # URI path spec instead of group spec. If the spec looks like /foo:bar/baz,\n    # then we assume it's a path with a colon. If the spec looks like\n    # [a-zA-z]:\\foo then we assume its a Windows path and not a single letter\n    # group\n    if (uri_pattern.match(specification) is None and ':' in specification and\n        specification[0] != '/' and not os.path.splitdrive(specification)[0]):\n      # We split at most once so run_name:/path:with/a/colon will work.\n      run_name, _, path = specification.partition(':')\n    else:\n      run_name = None\n      path = specification\n    if uri_pattern.match(path) is None:\n      path = os.path.realpath(os.path.expanduser(path))\n    files[path] = run_name\n  return files"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef start_reloading_multiplexer(multiplexer, path_to_run, load_interval,\n                                reload_task):\n  \"\"\"Starts automatically reloading the given multiplexer.\n\n  If `load_interval` is positive, the thread will reload the multiplexer\n  by calling `ReloadMultiplexer` every `load_interval` seconds, starting\n  immediately. Otherwise, reloads the multiplexer once and never again.\n\n  Args:\n    multiplexer: The `EventMultiplexer` to add runs to and reload.\n    path_to_run: A dict mapping from paths to run names, where `None` as the run\n      name is interpreted as a run name equal to the path.\n    load_interval: An integer greater than or equal to 0. If positive, how many\n      seconds to wait after one load before starting the next load. Otherwise,\n      reloads the multiplexer once and never again (no continuous reloading).\n    reload_task: Indicates the type of background task to reload with.\n\n  Raises:\n    ValueError: If `load_interval` is negative.\n  \"\"\"\n  if load_interval < 0:\n    raise ValueError('load_interval is negative: %d' % load_interval)\n\n  def _reload():\n    while True:\n      start = time.time()\n      logger.info('TensorBoard reload process beginning')\n      for path, name in six.iteritems(path_to_run):\n        multiplexer.AddRunsFromDirectory(path, name)\n      logger.info('TensorBoard reload process: Reload the whole Multiplexer')\n      multiplexer.Reload()\n      duration = time.time() - start\n      logger.info('TensorBoard done reloading. Load took %0.3f secs', duration)\n      if load_interval == 0:\n        # Only load the multiplexer once. Do not continuously reload.\n        break\n      time.sleep(load_interval)\n\n  if reload_task == 'process':\n    logger.info('Launching reload in a child process')\n    import multiprocessing\n    process = multiprocessing.Process(target=_reload, name='Reloader')\n    # Best-effort cleanup; on exit, the main TB parent process will attempt to\n    # kill all its daemonic children.\n    process.daemon = True\n    process.start()\n  elif reload_task in ('thread', 'auto'):\n    logger.info('Launching reload in a daemon thread')\n    thread = threading.Thread(target=_reload, name='Reloader')\n    # Make this a daemon thread, which won't block TB from exiting.\n    thread.daemon = True\n    thread.start()\n  elif reload_task == 'blocking':\n    if load_interval != 0:\n      raise ValueError('blocking reload only allowed with load_interval=0')\n    _reload()\n  else:\n    raise ValueError('unrecognized reload_task: %s' % reload_task)", "response": "Starts automatically reloading the given multiplexer."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the database module and connection provider TBContext fields relating to SQL database.", "response": "def get_database_info(db_uri):\n  \"\"\"Returns TBContext fields relating to SQL database.\n\n  Args:\n    db_uri: A string URI expressing the DB file, e.g. \"sqlite:~/tb.db\".\n\n  Returns:\n    A tuple with the db_module and db_connection_provider TBContext fields. If\n    db_uri was empty, then (None, None) is returned.\n\n  Raises:\n    ValueError: If db_uri scheme is not supported.\n  \"\"\"\n  if not db_uri:\n    return None, None\n  scheme = urlparse.urlparse(db_uri).scheme\n  if scheme == 'sqlite':\n    return sqlite3, create_sqlite_connection_provider(db_uri)\n  else:\n    raise ValueError('Only sqlite DB URIs are supported now: ' + db_uri)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_sqlite_connection_provider(db_uri):\n  uri = urlparse.urlparse(db_uri)\n  if uri.scheme != 'sqlite':\n    raise ValueError('Scheme is not sqlite: ' + db_uri)\n  if uri.netloc:\n    raise ValueError('Can not connect to SQLite over network: ' + db_uri)\n  if uri.path == ':memory:':\n    raise ValueError('Memory mode SQLite not supported: ' + db_uri)\n  path = os.path.expanduser(uri.path)\n  params = _get_connect_params(uri.query)\n  # TODO(@jart): Add thread-local pooling.\n  return lambda: sqlite3.connect(path, **params)", "response": "Creates a function that returns a new SQLite Connection object."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nserves an object mapping plugin name to whether it is enabled.", "response": "def _serve_plugins_listing(self, request):\n    \"\"\"Serves an object mapping plugin name to whether it is enabled.\n\n    Args:\n      request: The werkzeug.Request object.\n\n    Returns:\n      A werkzeug.Response object.\n    \"\"\"\n    response = {}\n    for plugin in self._plugins:\n      start = time.time()\n      response[plugin.plugin_name] = plugin.is_active()\n      elapsed = time.time() - start\n      logger.info(\n          'Plugin listing: is_active() for %s took %0.3f seconds',\n          plugin.plugin_name, elapsed)\n    return http_util.Respond(request, response, 'application/json')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_time_indices(s):\n  if not s.startswith('['):\n    s = '[' + s + ']'\n  parsed = command_parser._parse_slices(s)\n  if len(parsed) != 1:\n    raise ValueError(\n        'Invalid number of slicing objects in time indices (%d)' % len(parsed))\n  else:\n    return parsed[0]", "response": "Parse a string as time indices."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef process_buffers_for_display(s, limit=40):\n  if isinstance(s, (list, tuple)):\n    return [process_buffers_for_display(elem, limit=limit) for elem in s]\n  else:\n    length = len(s)\n    if length > limit:\n      return (binascii.b2a_qp(s[:limit]) +\n              b' (length-%d truncated at %d bytes)' % (length, limit))\n    else:\n      return binascii.b2a_qp(s)", "response": "Process a buffer for human - readable display."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef array_view(array, slicing=None, mapping=None):\n\n  dtype = translate_dtype(array.dtype)\n  sliced_array = (array[command_parser._parse_slices(slicing)] if slicing\n                  else array)\n\n  if np.isscalar(sliced_array) and str(dtype) == 'string':\n    # When a string Tensor (for which dtype is 'object') is sliced down to only\n    # one element, it becomes a string, instead of an numpy array.\n    # We preserve the dimensionality of original array in the returned shape\n    # and slice.\n    ndims = len(array.shape)\n    slice_shape = []\n    for _ in range(ndims):\n      sliced_array = [sliced_array]\n      slice_shape.append(1)\n    return dtype, tuple(slice_shape), sliced_array\n  else:\n    shape = sliced_array.shape\n    if mapping == \"image/png\":\n      if len(sliced_array.shape) == 2:\n        return dtype, shape, array_to_base64_png(sliced_array)\n      elif len(sliced_array.shape) == 3:\n        raise NotImplementedError(\n            \"image/png mapping for 3D array has not been implemented\")\n      else:\n        raise ValueError(\"Invalid rank for image/png mapping: %d\" %\n                         len(sliced_array.shape))\n    elif mapping == 'health-pill':\n      health_pill = health_pill_calc.calc_health_pill(array)\n      return dtype, shape, health_pill\n    elif mapping is None or mapping == '' or  mapping.lower() == 'none':\n      return dtype, shape, sliced_array.tolist()\n    else:\n      raise ValueError(\"Invalid mapping: %s\" % mapping)", "response": "View a nested list of arrays."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts an array into base64 - noded PNG image.", "response": "def array_to_base64_png(array):\n  \"\"\"Convert an array into base64-enoded PNG image.\n\n  Args:\n    array: A 2D np.ndarray or nested list of items.\n\n  Returns:\n    A base64-encoded string the image. The image is grayscale if the array is\n    2D. The image is RGB color if the image is 3D with lsat dimension equal to\n    3.\n\n  Raises:\n    ValueError: If the input `array` is not rank-2, or if the rank-2 `array` is\n      empty.\n  \"\"\"\n  # TODO(cais): Deal with 3D case.\n  # TODO(cais): If there are None values in here, replace them with all NaNs.\n  array = np.array(array, dtype=np.float32)\n  if len(array.shape) != 2:\n    raise ValueError(\n        \"Expected rank-2 array; received rank-%d array.\" % len(array.shape))\n  if not np.size(array):\n    raise ValueError(\n        \"Cannot encode an empty array (size: %s) as image.\" % (array.shape,))\n\n  is_infinity = np.isinf(array)\n  is_positive = array > 0.0\n  is_positive_infinity = np.logical_and(is_infinity, is_positive)\n  is_negative_infinity = np.logical_and(is_infinity,\n                                        np.logical_not(is_positive))\n  is_nan = np.isnan(array)\n  finite_indices = np.where(np.logical_and(np.logical_not(is_infinity),\n                                           np.logical_not(is_nan)))\n  if np.size(finite_indices):\n    # Finite subset is not empty.\n    minval = np.min(array[finite_indices])\n    maxval = np.max(array[finite_indices])\n    scaled = np.array((array - minval) / (maxval - minval) * 255,\n                      dtype=np.uint8)\n    rgb = np.repeat(np.expand_dims(scaled, -1), IMAGE_COLOR_CHANNELS, axis=-1)\n  else:\n    rgb = np.zeros(array.shape + (IMAGE_COLOR_CHANNELS,), dtype=np.uint8)\n\n  # Color-code pixels that correspond to infinities and nans.\n  rgb[is_positive_infinity] = POSITIVE_INFINITY_RGB\n  rgb[is_negative_infinity] = NEGATIVE_INFINITY_RGB\n  rgb[is_nan] = NAN_RGB\n\n  image_encoded = base64.b64encode(encoder.encode_png(rgb))\n  return image_encoded"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef combine_graph_defs(to_proto, from_proto):\n  if from_proto.version != to_proto.version:\n    raise ValueError('Cannot combine GraphDefs of different versions.')\n\n  try:\n    _safe_copy_proto_list_values(\n        to_proto.node,\n        from_proto.node,\n        lambda n: n.name)\n  except _ProtoListDuplicateKeyError as exc:\n    raise ValueError('A GraphDef contains non-unique node names: %s' % exc)\n  except _SameKeyDiffContentError as exc:\n    raise ValueError(\n      ('Cannot combine GraphDefs because nodes share a name '\n       'but contents are different: %s') % exc)\n  try:\n    _safe_copy_proto_list_values(\n        to_proto.library.function,\n        from_proto.library.function,\n        lambda n: n.signature.name)\n  except _ProtoListDuplicateKeyError as exc:\n    raise ValueError('A GraphDef contains non-unique function names: %s' % exc)\n  except _SameKeyDiffContentError as exc:\n    raise ValueError(\n      ('Cannot combine GraphDefs because functions share a name '\n       'but are different: %s') % exc)\n\n  try:\n    _safe_copy_proto_list_values(\n        to_proto.library.gradient,\n        from_proto.library.gradient,\n        lambda g: g.gradient_func)\n  except _ProtoListDuplicateKeyError as exc:\n    raise ValueError(\n      'A GraphDef contains non-unique gradient function names: %s' % exc)\n  except _SameKeyDiffContentError as exc:\n    raise ValueError(\n      ('Cannot combine GraphDefs because gradients share a gradient_func name '\n       'but map to different functions: %s') % exc)\n\n  return to_proto", "response": "Combines two TensorBoard GraphDefs by adding nodes from from_proto into to_proto."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrite a scalar summary.", "response": "def scalar(name, data, step=None, description=None):\n  \"\"\"Write a scalar summary.\n\n  Arguments:\n    name: A name for this summary. The summary tag used for TensorBoard will\n      be this name prefixed by any active name scopes.\n    data: A real numeric scalar value, convertible to a `float32` Tensor.\n    step: Explicit `int64`-castable monotonic step value for this summary. If\n      omitted, this defaults to `tf.summary.experimental.get_step()`, which must\n      not be None.\n    description: Optional long-form description for this summary, as a\n      constant `str`. Markdown is supported. Defaults to empty.\n\n  Returns:\n    True on success, or false if no summary was written because no default\n    summary writer was available.\n\n  Raises:\n    ValueError: if a default writer exists, but no step was provided and\n      `tf.summary.experimental.get_step()` is None.\n  \"\"\"\n  summary_metadata = metadata.create_summary_metadata(\n      display_name=None, description=description)\n  # TODO(https://github.com/tensorflow/tensorboard/issues/2109): remove fallback\n  summary_scope = (\n      getattr(tf.summary.experimental, 'summary_scope', None) or\n      tf.summary.summary_scope)\n  with summary_scope(\n      name, 'scalar_summary', values=[data, step]) as (tag, _):\n    tf.debugging.assert_scalar(data)\n    return tf.summary.write(tag=tag,\n                            tensor=tf.cast(data, tf.float32),\n                            step=step,\n                            metadata=summary_metadata)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a scalar summary_pb2. Summary protobuf.", "response": "def scalar_pb(tag, data, description=None):\n  \"\"\"Create a scalar summary_pb2.Summary protobuf.\n\n  Arguments:\n    tag: String tag for the summary.\n    data: A 0-dimensional `np.array` or a compatible python number type.\n    description: Optional long-form description for this summary, as a\n      `str`. Markdown is supported. Defaults to empty.\n\n  Raises:\n    ValueError: If the type or shape of the data is unsupported.\n\n  Returns:\n    A `summary_pb2.Summary` protobuf object.\n  \"\"\"\n  arr = np.array(data)\n  if arr.shape != ():\n    raise ValueError('Expected scalar shape for tensor, got shape: %s.'\n                     % arr.shape)\n  if arr.dtype.kind not in ('b', 'i', 'u', 'f'):  # bool, int, uint, float\n    raise ValueError('Cast %s to float is not supported' % arr.dtype.name)\n  tensor_proto = tensor_util.make_tensor_proto(arr.astype(np.float32))\n  summary_metadata = metadata.create_summary_metadata(\n      display_name=None, description=description)\n  summary = summary_pb2.Summary()\n  summary.value.add(tag=tag,\n                    metadata=summary_metadata,\n                    tensor=tensor_proto)\n  return summary"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef dump_data(logdir):\n  # Create a tfevents file in the logdir so it is detected as a run.\n  write_empty_event_file(logdir)\n\n  plugin_logdir = plugin_asset_util.PluginDirectory(\n      logdir, profile_plugin.ProfilePlugin.plugin_name)\n  _maybe_create_directory(plugin_logdir)\n\n  for run in profile_demo_data.RUNS:\n    run_dir = os.path.join(plugin_logdir, run)\n    _maybe_create_directory(run_dir)\n    if run in profile_demo_data.TRACES:\n      with open(os.path.join(run_dir, 'trace'), 'w') as f:\n        proto = trace_events_pb2.Trace()\n        text_format.Merge(profile_demo_data.TRACES[run], proto)\n        f.write(proto.SerializeToString())\n\n    if run not in profile_demo_data.TRACE_ONLY:\n      shutil.copyfile('tensorboard/plugins/profile/profile_demo.op_profile.json',\n                      os.path.join(run_dir, 'op_profile.json'))\n      shutil.copyfile(\n          'tensorboard/plugins/profile/profile_demo.memory_viewer.json',\n          os.path.join(run_dir, 'memory_viewer.json'))\n      shutil.copyfile(\n          'tensorboard/plugins/profile/profile_demo.pod_viewer.json',\n          os.path.join(run_dir, 'pod_viewer.json'))\n      shutil.copyfile(\n          'tensorboard/plugins/profile/profile_demo.google_chart_demo.json',\n          os.path.join(run_dir, 'google_chart_demo.json'))\n\n  # Unsupported tool data should not be displayed.\n  run_dir = os.path.join(plugin_logdir, 'empty')\n  _maybe_create_directory(run_dir)\n  with open(os.path.join(run_dir, 'unsupported'), 'w') as f:\n    f.write('unsupported data')", "response": "Dumps plugin data to the log directory."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate the health pill of a tensor.", "response": "def calc_health_pill(tensor):\n  \"\"\"Calculate health pill of a tensor.\n\n  Args:\n    tensor: An instance of `np.array` (for initialized tensors) or\n      `tensorflow.python.debug.lib.debug_data.InconvertibleTensorProto`\n      (for unininitialized tensors).\n\n  Returns:\n    If `tensor` is an initialized tensor of numeric or boolean types:\n      the calculated health pill, as a `list` of `float`s.\n    Else if `tensor` is an initialized tensor with `string`, `resource` or any\n      other non-numeric types:\n      `None`.\n    Else (i.e., if `tensor` is uninitialized): An all-zero `list`, with the\n      first element signifying that the tensor is uninitialized.\n  \"\"\"\n  health_pill = [0.0] * 14\n\n  # TODO(cais): Add unit test for this method that compares results with\n  #   DebugNumericSummary output.\n\n  # Is tensor initialized.\n  if not isinstance(tensor, np.ndarray):\n    return health_pill\n  health_pill[0] = 1.0\n\n  if not (np.issubdtype(tensor.dtype, np.float) or\n          np.issubdtype(tensor.dtype, np.complex) or\n          np.issubdtype(tensor.dtype, np.integer) or\n          tensor.dtype == np.bool):\n    return None\n\n  # Total number of elements.\n  health_pill[1] = float(np.size(tensor))\n\n  # TODO(cais): Further performance optimization?\n  nan_mask = np.isnan(tensor)\n  inf_mask = np.isinf(tensor)\n  # Number of NaN elements.\n  health_pill[2] = float(np.sum(nan_mask))\n  # Number of -Inf elements.\n  health_pill[3] = float(np.sum(tensor == -np.inf))\n  # Number of finite negative elements.\n  health_pill[4] = float(np.sum(\n      np.logical_and(np.logical_not(inf_mask), tensor < 0.0)))\n  # Number of zero elements.\n  health_pill[5] = float(np.sum(tensor == 0.0))\n  # Number finite positive elements.\n  health_pill[6] = float(np.sum(\n      np.logical_and(np.logical_not(inf_mask), tensor > 0.0)))\n  # Number of +Inf elements.\n  health_pill[7] = float(np.sum(tensor == np.inf))\n\n  finite_subset = tensor[\n      np.logical_and(np.logical_not(nan_mask), np.logical_not(inf_mask))]\n  if np.size(finite_subset):\n    # Finite subset is not empty.\n    # Minimum of the non-NaN non-Inf elements.\n    health_pill[8] = float(np.min(finite_subset))\n    # Maximum of the non-NaN non-Inf elements.\n    health_pill[9] = float(np.max(finite_subset))\n    # Mean of the non-NaN non-Inf elements.\n    health_pill[10] = float(np.mean(finite_subset))\n    # Variance of the non-NaN non-Inf elements.\n    health_pill[11] = float(np.var(finite_subset))\n  else:\n    # If no finite element exists:\n    # Set minimum to +inf.\n    health_pill[8] = np.inf\n    # Set maximum to -inf.\n    health_pill[9] = -np.inf\n    # Set mean to NaN.\n    health_pill[10] = np.nan\n    # Set variance to NaN.\n    health_pill[11] = np.nan\n\n  # DType encoded as a number.\n  # TODO(cais): Convert numpy dtype to corresponding tensorflow dtype enum.\n  health_pill[12] = -1.0\n  # ndims.\n  health_pill[13] = float(len(tensor.shape))\n  # Size of the dimensions.\n  health_pill.extend([float(x) for x in tensor.shape])\n  return health_pill"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread the config file from disk or creates a new one.", "response": "def _get_config(self):\n    '''Reads the config file from disk or creates a new one.'''\n    filename = '{}/{}'.format(self.PLUGIN_LOGDIR, CONFIG_FILENAME)\n    modified_time = os.path.getmtime(filename)\n\n    if modified_time != self.config_last_modified_time:\n      config = read_pickle(filename, default=self.previous_config)\n      self.previous_config = config\n    else:\n      config = self.previous_config\n\n    self.config_last_modified_time = modified_time\n    return config"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwrites the frame to disk as a tensor summary.", "response": "def _write_summary(self, session, frame):\n    '''Writes the frame to disk as a tensor summary.'''\n    summary = session.run(self.summary_op, feed_dict={\n        self.frame_placeholder: frame\n    })\n    path = '{}/{}'.format(self.PLUGIN_LOGDIR, SUMMARY_FILENAME)\n    write_file(summary, path)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _enough_time_has_passed(self, FPS):\n    '''For limiting how often frames are computed.'''\n    if FPS == 0:\n      return False\n    else:\n      earliest_time = self.last_update_time + (1.0 / FPS)\n      return time.time() >= earliest_time", "response": "For limiting how often frames are computed."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _update_recording(self, frame, config):\n    '''Adds a frame to the current video output.'''\n    # pylint: disable=redefined-variable-type\n    should_record = config['is_recording']\n\n    if should_record:\n      if not self.is_recording:\n        self.is_recording = True\n        logger.info(\n            'Starting recording using %s',\n            self.video_writer.current_output().name())\n      self.video_writer.write_frame(frame)\n    elif self.is_recording:\n      self.is_recording = False\n      self.video_writer.finish()\n      logger.info('Finished recording')", "response": "Adds a frame to the current video output."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update(self, session, arrays=None, frame=None):\n    '''Creates a frame and writes it to disk.\n\n    Args:\n      arrays: a list of np arrays. Use the \"custom\" option in the client.\n      frame: a 2D np array. This way the plugin can be used for video of any\n             kind, not just the visualization that comes with the plugin.\n\n             frame can also be a function, which only is evaluated when the\n             \"frame\" option is selected by the client.\n    '''\n    new_config = self._get_config()\n\n    if self._enough_time_has_passed(self.previous_config['FPS']):\n      self.visualizer.update(new_config)\n      self.last_update_time = time.time()\n      final_image = self._update_frame(session, arrays, frame, new_config)\n      self._update_recording(final_image, new_config)", "response": "Creates a frame and writes it to disk."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef gradient_helper(optimizer, loss, var_list=None):\n    '''A helper to get the gradients out at each step.\n\n    Args:\n      optimizer: the optimizer op.\n      loss: the op that computes your loss value.\n\n    Returns: the gradient tensors and the train_step op.\n    '''\n    if var_list is None:\n      var_list = tf.compat.v1.trainable_variables()\n\n    grads_and_vars = optimizer.compute_gradients(loss, var_list=var_list)\n    grads = [pair[0] for pair in grads_and_vars]\n\n    return grads, optimizer.apply_gradients(grads_and_vars)", "response": "A helper to get the gradients out at each step."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a key_func to be used in list. sort.", "response": "def _create_key_func(extractor, none_is_largest):\n  \"\"\"Returns a key_func to be used in list.sort().\n\n  Returns a key_func to be used in list.sort() that sorts session groups\n  by the value extracted by extractor. 'None' extracted values will either\n  be considered largest or smallest as specified by the \"none_is_largest\"\n  boolean parameter.\n\n  Args:\n    extractor: An extractor function that extract the key from the session\n      group.\n    none_is_largest: bool. If true treats 'None's as largest; otherwise\n      smallest.\n\n  \"\"\"\n  if none_is_largest:\n    def key_func_none_is_largest(session_group):\n      value = extractor(session_group)\n      return (value is None, value)\n    return key_func_none_is_largest\n  def key_func_none_is_smallest(session_group):\n    value = extractor(session_group)\n    return (value is not None, value)\n  return key_func_none_is_smallest"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _create_extractors(col_params):\n  result = []\n  for col_param in col_params:\n    result.append(_create_extractor(col_param))\n  return result", "response": "Creates extractors to extract properties corresponding to col_params."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a function that extracts a metric from a session group or a session.", "response": "def _create_metric_extractor(metric_name):\n  \"\"\"Returns function that extracts a metric from a session group or a session.\n\n  Args:\n    metric_name: tensorboard.hparams.MetricName protobuffer. Identifies the\n    metric to extract from the session group.\n  Returns:\n    A function that takes a tensorboard.hparams.SessionGroup or\n    tensorborad.hparams.Session protobuffer and returns the value of the metric\n    identified by 'metric_name' or None if the value doesn't exist.\n  \"\"\"\n  def extractor_fn(session_or_group):\n    metric_value = _find_metric_value(session_or_group,\n                                      metric_name)\n    return metric_value.value if metric_value else None\n\n  return extractor_fn"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the metric_value for a given metric in a given session or session group.", "response": "def _find_metric_value(session_or_group, metric_name):\n  \"\"\"Returns the metric_value for a given metric in a session or session group.\n\n  Args:\n    session_or_group: A Session protobuffer or SessionGroup protobuffer.\n    metric_name: A MetricName protobuffer. The metric to search for.\n  Returns:\n    A MetricValue protobuffer representing the value of the given metric or\n    None if no such metric was found in session_or_group.\n  \"\"\"\n  # Note: We can speed this up by converting the metric_values field\n  # to a dictionary on initialization, to avoid a linear search here. We'll\n  # need to wrap the SessionGroup and Session protos in a python object for\n  # that.\n  for metric_value in session_or_group.metric_values:\n    if (metric_value.name.tag == metric_name.tag and\n        metric_value.name.group == metric_name.group):\n      return metric_value"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _create_hparam_extractor(hparam_name):\n  def extractor_fn(session_group):\n    if hparam_name in session_group.hparams:\n      return _value_to_python(session_group.hparams[hparam_name])\n    return None\n\n  return extractor_fn", "response": "Creates an extractor function that extracts an hparam from a tensorboard. hparams. SessionGroup protobuffer and\n    returns the value as a native Python object of the hparam identified by hparam_name."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates filters for the given col_params.", "response": "def _create_filters(col_params, extractors):\n  \"\"\"Creates filters for the given col_params.\n\n  Args:\n    col_params: List of ListSessionGroupsRequest.ColParam protobufs.\n    extractors: list of extractor functions of the same length as col_params.\n      Each element should extract the column described by the corresponding\n      element of col_params.\n  Returns:\n    A list of filter functions. Each corresponding to a single\n    col_params.filter oneof field of _request\n  \"\"\"\n  result = []\n  for col_param, extractor in zip(col_params, extractors):\n    a_filter = _create_filter(col_param, extractor)\n    if a_filter:\n      result.append(a_filter)\n  return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a filter for the given col_param and extractor.", "response": "def _create_filter(col_param, extractor):\n  \"\"\"Creates a filter for the given col_param and extractor.\n\n  Args:\n    col_param: A tensorboard.hparams.ColParams object identifying the column\n      and describing the filter to apply.\n    extractor: A function that extract the column value identified by\n      'col_param' from a tensorboard.hparams.SessionGroup protobuffer.\n  Returns:\n    A boolean function taking a tensorboard.hparams.SessionGroup protobuffer\n    returning True if the session group passes the filter described by\n    'col_param'. If col_param does not specify a filter (i.e. any session\n    group passes) returns None.\n  \"\"\"\n  include_missing_values = not col_param.exclude_missing_values\n  if col_param.HasField('filter_regexp'):\n    value_filter_fn = _create_regexp_filter(col_param.filter_regexp)\n  elif col_param.HasField('filter_interval'):\n    value_filter_fn = _create_interval_filter(col_param.filter_interval)\n  elif col_param.HasField('filter_discrete'):\n    value_filter_fn = _create_discrete_set_filter(col_param.filter_discrete)\n  elif include_missing_values:\n    # No 'filter' field and include_missing_values is True.\n    # Thus, the resulting filter always returns True, so to optimize for this\n    # common case we do not include it in the list of filters to check.\n    return None\n  else:\n    value_filter_fn = lambda _: True\n\n  def filter_fn(session_group):\n    value = extractor(session_group)\n    if value is None:\n      return include_missing_values\n    return value_filter_fn(value)\n\n  return filter_fn"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _create_regexp_filter(regex):\n  # Warning: Note that python's regex library allows inputs that take\n  # exponential time. Time-limiting it is difficult. When we move to\n  # a true multi-tenant tensorboard server, the regexp implementation here\n  # would need to be replaced by something more secure.\n  compiled_regex = re.compile(regex)\n  def filter_fn(value):\n    if not isinstance(value, six.string_types):\n      raise error.HParamsError(\n          'Cannot use a regexp filter for a value of type %s. Value: %s' %\n          (type(value), value))\n    return re.search(compiled_regex, value) is not None\n\n  return filter_fn", "response": "Returns a function that filters strings based on a regular exp."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a function that checks whether a number belongs to an interval.", "response": "def _create_interval_filter(interval):\n  \"\"\"Returns a function that checkes whether a number belongs to an interval.\n\n  Args:\n    interval: A tensorboard.hparams.Interval protobuf describing the interval.\n  Returns:\n    A function taking a number (a float or an object of a type in\n    six.integer_types) that returns True if the number belongs to (the closed)\n    'interval'.\n  \"\"\"\n  def filter_fn(value):\n    if (not isinstance(value, six.integer_types) and\n        not isinstance(value, float)):\n      raise error.HParamsError(\n          'Cannot use an interval filter for a value of type: %s, Value: %s' %\n          (type(value), value))\n    return interval.min_value <= value and value <= interval.max_value\n\n  return filter_fn"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting a google. protobuf. Value to a native Python object.", "response": "def _value_to_python(value):\n  \"\"\"Converts a google.protobuf.Value to a native Python object.\"\"\"\n\n  assert isinstance(value, struct_pb2.Value)\n  field = value.WhichOneof('kind')\n  if field == 'number_value':\n    return value.number_value\n  elif field == 'string_value':\n    return value.string_value\n  elif field == 'bool_value':\n    return value.bool_value\n  else:\n    raise ValueError('Unknown struct_pb2.Value oneof field set: %s' % field)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the metrics for the group to be the average of its sessions.", "response": "def _set_avg_session_metrics(session_group):\n  \"\"\"Sets the metrics for the group to be the average of its sessions.\n\n  The resulting session group metrics consist of the union of metrics across\n  the group's sessions. The value of each session group metric is the average\n  of that metric values across the sessions in the group. The 'step' and\n  'wall_time_secs' fields of the resulting MetricValue field in the session\n  group are populated with the corresponding averages (truncated for 'step')\n  as well.\n\n  Args:\n    session_group: A SessionGroup protobuffer.\n  \"\"\"\n  assert session_group.sessions, 'SessionGroup cannot be empty.'\n  # Algorithm: Iterate over all (session, metric) pairs and maintain a\n  # dict from _MetricIdentifier to _MetricStats objects.\n  # Then use the final dict state to compute the average for each metric.\n  metric_stats = collections.defaultdict(_MetricStats)\n  for session in session_group.sessions:\n    for metric_value in session.metric_values:\n      metric_name = _MetricIdentifier(group=metric_value.name.group,\n                                      tag=metric_value.name.tag)\n      stats = metric_stats[metric_name]\n      stats.total += metric_value.value\n      stats.count += 1\n      stats.total_step += metric_value.training_step\n      stats.total_wall_time_secs += metric_value.wall_time_secs\n\n  del session_group.metric_values[:]\n  for (metric_name, stats) in six.iteritems(metric_stats):\n    session_group.metric_values.add(\n        name=api_pb2.MetricName(group=metric_name.group, tag=metric_name.tag),\n        value=float(stats.total)/float(stats.count),\n        training_step=stats.total_step // stats.count,\n        wall_time_secs=stats.total_wall_time_secs / stats.count)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _set_median_session_metrics(session_group, aggregation_metric):\n  measurements = sorted(_measurements(session_group, aggregation_metric),\n                        key=operator.attrgetter('metric_value.value'))\n  median_session = measurements[(len(measurements) - 1) // 2].session_index\n  del session_group.metric_values[:]\n  session_group.metric_values.MergeFrom(\n      session_group.sessions[median_session].metric_values)", "response": "Sets the metrics for the session_group to those of its median session."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the metrics for the extremum session in the given session_group to those of its extremum session.", "response": "def _set_extremum_session_metrics(session_group, aggregation_metric,\n                                  extremum_fn):\n  \"\"\"Sets the metrics for session_group to those of its \"extremum session\".\n\n  The extremum session is the session in session_group with the extremum value\n  of the metric given by 'aggregation_metric'. The extremum is taken over the\n  subset of sessions in the group whose 'aggregation_metric' was measured\n  at the largest training step among the sessions in the group.\n\n  Args:\n    session_group: A SessionGroup protobuffer.\n    aggregation_metric: A MetricName protobuffer.\n    extremum_fn: callable. Must be either 'min' or 'max'. Determines the type of\n      extremum to compute.\n  \"\"\"\n  measurements = _measurements(session_group, aggregation_metric)\n  ext_session = extremum_fn(\n      measurements,\n      key=operator.attrgetter('metric_value.value')).session_index\n  del session_group.metric_values[:]\n  session_group.metric_values.MergeFrom(\n      session_group.sessions[ext_session].metric_values)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run(self):\n    session_groups = self._build_session_groups()\n    session_groups = self._filter(session_groups)\n    self._sort(session_groups)\n    return self._create_response(session_groups)", "response": "Handles the request specified on construction."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nbuilds a list of SessionGroups protobuffers from the summary data.", "response": "def _build_session_groups(self):\n    \"\"\"Returns a list of SessionGroups protobuffers from the summary data.\"\"\"\n\n    # Algorithm: We keep a dict 'groups_by_name' mapping a SessionGroup name\n    # (str) to a SessionGroup protobuffer. We traverse the runs associated with\n    # the plugin--each representing a single session. We form a Session\n    # protobuffer from each run and add it to the relevant SessionGroup object\n    # in the 'groups_by_name' dict. We create the SessionGroup object, if this\n    # is the first session of that group we encounter.\n    groups_by_name = {}\n    run_to_tag_to_content = self._context.multiplexer.PluginRunToTagToContent(\n        metadata.PLUGIN_NAME)\n    for (run, tag_to_content) in six.iteritems(run_to_tag_to_content):\n      if metadata.SESSION_START_INFO_TAG not in tag_to_content:\n        continue\n      start_info = metadata.parse_session_start_info_plugin_data(\n          tag_to_content[metadata.SESSION_START_INFO_TAG])\n      end_info = None\n      if metadata.SESSION_END_INFO_TAG in tag_to_content:\n        end_info = metadata.parse_session_end_info_plugin_data(\n            tag_to_content[metadata.SESSION_END_INFO_TAG])\n      session = self._build_session(run, start_info, end_info)\n      if session.status in self._request.allowed_statuses:\n        self._add_session(session, start_info, groups_by_name)\n\n    # Compute the session group's aggregated metrics for each group.\n    groups = groups_by_name.values()\n    for group in groups:\n      # We sort the sessions in a group so that the order is deterministic.\n      group.sessions.sort(key=operator.attrgetter('name'))\n      self._aggregate_metrics(group)\n    return groups"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _add_session(self, session, start_info, groups_by_name):\n    # If the group_name is empty, this session's group contains only\n    # this session. Use the session name for the group name since session\n    # names are unique.\n    group_name = start_info.group_name or session.name\n    if group_name in groups_by_name:\n      groups_by_name[group_name].sessions.extend([session])\n    else:\n      # Create the group and add the session as the first one.\n      group = api_pb2.SessionGroup(\n          name=group_name,\n          sessions=[session],\n          monitor_url=start_info.monitor_url)\n      # Copy hparams from the first session (all sessions should have the same\n      # hyperparameter values) into result.\n      # There doesn't seem to be a way to initialize a protobuffer map in the\n      # constructor.\n      for (key, value) in six.iteritems(start_info.hparams):\n        group.hparams[key].CopyFrom(value)\n      groups_by_name[group_name] = group", "response": "Adds a new Session protobuffer to the groups_by_name dict."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbuilding a session object.", "response": "def _build_session(self, name, start_info, end_info):\n    \"\"\"Builds a session object.\"\"\"\n\n    assert start_info is not None\n    result = api_pb2.Session(\n        name=name,\n        start_time_secs=start_info.start_time_secs,\n        model_uri=start_info.model_uri,\n        metric_values=self._build_session_metric_values(name),\n        monitor_url=start_info.monitor_url)\n    if end_info is not None:\n      result.status = end_info.status\n      result.end_time_secs = end_info.end_time_secs\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _build_session_metric_values(self, session_name):\n\n    # result is a list of api_pb2.MetricValue instances.\n    result = []\n    metric_infos = self._experiment.metric_infos\n    for metric_info in metric_infos:\n      metric_name = metric_info.name\n      try:\n        metric_eval = metrics.last_metric_eval(\n            self._context.multiplexer,\n            session_name,\n            metric_name)\n      except KeyError:\n        # It's ok if we don't find the metric in the session.\n        # We skip it here. For filtering and sorting purposes its value is None.\n        continue\n\n      # metric_eval is a 3-tuple of the form [wall_time, step, value]\n      result.append(api_pb2.MetricValue(name=metric_name,\n                                        wall_time_secs=metric_eval[0],\n                                        training_step=metric_eval[1],\n                                        value=metric_eval[2]))\n    return result", "response": "Builds the session metric values."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the metrics of the group based on aggregation_type.", "response": "def _aggregate_metrics(self, session_group):\n    \"\"\"Sets the metrics of the group based on aggregation_type.\"\"\"\n\n    if (self._request.aggregation_type == api_pb2.AGGREGATION_AVG or\n        self._request.aggregation_type == api_pb2.AGGREGATION_UNSET):\n      _set_avg_session_metrics(session_group)\n    elif self._request.aggregation_type == api_pb2.AGGREGATION_MEDIAN:\n      _set_median_session_metrics(session_group,\n                                  self._request.aggregation_metric)\n    elif self._request.aggregation_type == api_pb2.AGGREGATION_MIN:\n      _set_extremum_session_metrics(session_group,\n                                    self._request.aggregation_metric,\n                                    min)\n    elif self._request.aggregation_type == api_pb2.AGGREGATION_MAX:\n      _set_extremum_session_metrics(session_group,\n                                    self._request.aggregation_metric,\n                                    max)\n    else:\n      raise error.HParamsError('Unknown aggregation_type in request: %s' %\n                               self._request.aggregation_type)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _sort(self, session_groups):\n\n    # Sort by session_group name so we have a deterministic order.\n    session_groups.sort(key=operator.attrgetter('name'))\n    # Sort by lexicographical order of the _request.col_params whose order\n    # is not ORDER_UNSPECIFIED. The first such column is the primary sorting\n    # key, the second is the secondary sorting key, etc. To achieve that we\n    # need to iterate on these columns in reverse order (thus the primary key\n    # is the key used in the last sort).\n    for col_param, extractor in reversed(list(zip(self._request.col_params,\n                                                  self._extractors))):\n      if col_param.order == api_pb2.ORDER_UNSPECIFIED:\n        continue\n      if col_param.order == api_pb2.ORDER_ASC:\n        session_groups.sort(\n            key=_create_key_func(\n                extractor,\n                none_is_largest=not col_param.missing_values_first))\n      elif col_param.order == api_pb2.ORDER_DESC:\n        session_groups.sort(\n            key=_create_key_func(\n                extractor,\n                none_is_largest=col_param.missing_values_first),\n            reverse=True)\n      else:\n        raise error.HParamsError('Unknown col_param.order given: %s' %\n                                 col_param)", "response": "Sorts the session_groups in place according to _request. col_params."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef recordAnalyzeAudio(duration, outputWavFile, midTermBufferSizeSec, modelName, modelType):\n\t'''\n\trecordAnalyzeAudio(duration, outputWavFile, midTermBufferSizeSec, modelName, modelType)\n\n\tThis function is used to record and analyze audio segments, in a fix window basis.\n\n\tARGUMENTS: \n\t- duration\t\t\ttotal recording duration\n\t- outputWavFile\t\t\tpath of the output WAV file\n\t- midTermBufferSizeSec\t\t(fix)segment length in seconds\n\t- modelName\t\t\tclassification model name\n\t- modelType\t\t\tclassification model type\n\n\t'''\n\n\tif modelType=='svm':\n\t\t[Classifier, MEAN, STD, classNames, mtWin, mtStep, stWin, stStep, compute_beat] = aT.load_model(modelName)\n\telif modelType=='knn':\n\t\t[Classifier, MEAN, STD, classNames, mtWin, mtStep, stWin, stStep, compute_beat] = aT.load_model_knn(modelName)\n\telse:\n\t\tClassifier = None\n\n\tinp = alsaaudio.PCM(alsaaudio.PCM_CAPTURE, alsaaudio.PCM_NONBLOCK)\n\tinp.setchannels(1)\n\tinp.setrate(Fs)\n\tinp.setformat(alsaaudio.PCM_FORMAT_S16_LE)\n\tinp.setperiodsize(512)\n\tmidTermBufferSize = int(midTermBufferSizeSec * Fs)\n\tallData = []\n\tmidTermBuffer = []\n\tcurWindow = []\n\tcount = 0\n\n\twhile len(allData)<duration*Fs:\n\t\t# Read data from device\n\t\tl,data = inp.read()\n\t    \tif l:\n\t\t\tfor i in range(l):\n\t\t\t\tcurWindow.append(audioop.getsample(data, 2, i))\t\t\n\t\t\tif (len(curWindow)+len(midTermBuffer)>midTermBufferSize):\n\t\t\t\tsamplesToCopyToMidBuffer = midTermBufferSize - len(midTermBuffer)\n\t\t\telse:\n\t\t\t\tsamplesToCopyToMidBuffer = len(curWindow)\n\t\t\tmidTermBuffer = midTermBuffer + curWindow[0:samplesToCopyToMidBuffer];\n\t\t\tdel(curWindow[0:samplesToCopyToMidBuffer])\n\t\tif len(midTermBuffer) == midTermBufferSize:\n\t\t\tcount += 1\t\t\t\t\t\t\n\t\t\tif Classifier!=None:\n\t\t\t\t[mtFeatures, stFeatures, _] = aF.mtFeatureExtraction(midTermBuffer, Fs, 2.0*Fs, 2.0*Fs, 0.020*Fs, 0.020*Fs)\n\t\t\t\tcurFV = (mtFeatures[:,0] - MEAN) / STD;\n\t\t\t\t[result, P] = aT.classifierWrapper(Classifier, modelType, curFV)\n\t\t\t\tprint classNames[int(result)]\n\t\t\tallData = allData + midTermBuffer\n\n\t\t\tplt.clf()\n\t\t\tplt.plot(midTermBuffer)\n\t\t\tplt.show(block = False)\n\t\t\tplt.draw()\n\n\n\t\t\tmidTermBuffer = []\n\n\tallDataArray = numpy.int16(allData)\n\twavfile.write(outputWavFile, Fs, allDataArray)", "response": "This function is used to record and analyze audio segments in a fix window basis."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nbreaks an audio stream to segments of interest by a csv file", "response": "def annotation2files(wavFile, csvFile):\n    '''\n        Break an audio stream to segments of interest, \n        defined by a csv file\n        \n        - wavFile:    path to input wavfile\n        - csvFile:    path to csvFile of segment limits\n        \n        Input CSV file must be of the format <T1>\\t<T2>\\t<Label>\n    '''    \n    \n    [Fs, x] = audioBasicIO.readAudioFile(wavFile)\n    with open(csvFile, 'rb') as csvfile:\n        reader = csv.reader(csvfile, delimiter='\\t', quotechar='|')\n        for j, row in enumerate(reader):\n            T1 = float(row[0].replace(\",\",\".\"))\n            T2 = float(row[1].replace(\",\",\".\"))            \n            label = \"%s_%s_%.2f_%.2f.wav\" % (wavFile, row[2], T1, T2)\n            label = label.replace(\" \", \"_\")\n            xtemp = x[int(round(T1*Fs)):int(round(T2*Fs))]            \n            print T1, T2, label, xtemp.shape\n            wavfile.write(label, Fs, xtemp)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef convertDirMP3ToWav(dirName, Fs, nC, useMp3TagsAsName = False):\n    '''\n    This function converts the MP3 files stored in a folder to WAV. If required, the output names of the WAV files are based on MP3 tags, otherwise the same names are used.\n    ARGUMENTS:\n     - dirName:     the path of the folder where the MP3s are stored\n     - Fs:          the sampling rate of the generated WAV files\n     - nC:          the number of channels of the generated WAV files\n     - useMp3TagsAsName:    True if the WAV filename is generated on MP3 tags\n    '''\n\n    types = (dirName+os.sep+'*.mp3',) # the tuple of file types\n    filesToProcess = [] \n\n    for files in types:\n        filesToProcess.extend(glob.glob(files))     \n\n    for f in filesToProcess:\n        #tag.link(f)\n        audioFile = eyed3.load(f)               \n        if useMp3TagsAsName and audioFile.tag != None:          \n            artist = audioFile.tag.artist\n            title = audioFile.tag.title\n            if artist!=None and title!=None:\n                if len(title)>0 and len(artist)>0:\n                    wavFileName = ntpath.split(f)[0] + os.sep + artist.replace(\",\",\" \") + \" --- \" + title.replace(\",\",\" \") + \".wav\"\n                else:\n                    wavFileName = f.replace(\".mp3\",\".wav\")  \n            else:\n                wavFileName = f.replace(\".mp3\",\".wav\")                      \n        else:\n            wavFileName = f.replace(\".mp3\",\".wav\")      \n        command = \"avconv -i \\\"\" + f + \"\\\" -ar \" +str(Fs) + \" -ac \" + str(nC) + \" \\\"\" + wavFileName + \"\\\"\";\n        print(command)\n        os.system(command.decode('unicode_escape').encode('ascii','ignore').replace(\"\\0\",\"\"))", "response": "This function converts the MP3 files stored in a folder to WAV files."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef readAudioFile(path):\n    '''\n    This function returns a numpy array that stores the audio samples of a specified WAV of AIFF file\n    '''\n    extension = os.path.splitext(path)[1]\n\n    try:\n        #if extension.lower() == '.wav':\n            #[Fs, x] = wavfile.read(path)\n        if extension.lower() == '.aif' or extension.lower() == '.aiff':\n            s = aifc.open(path, 'r')\n            nframes = s.getnframes()\n            strsig = s.readframes(nframes)\n            x = numpy.fromstring(strsig, numpy.short).byteswap()\n            Fs = s.getframerate()\n        elif extension.lower() == '.mp3' or extension.lower() == '.wav' or extension.lower() == '.au' or extension.lower() == '.ogg':            \n            try:\n                audiofile = AudioSegment.from_file(path)\n            #except pydub.exceptions.CouldntDecodeError:\n            except:\n                print(\"Error: file not found or other I/O error. \"\n                      \"(DECODING FAILED)\")\n                return (-1,-1)                \n\n            if audiofile.sample_width==2:                \n                data = numpy.fromstring(audiofile._data, numpy.int16)\n            elif audiofile.sample_width==4:\n                data = numpy.fromstring(audiofile._data, numpy.int32)\n            else:\n                return (-1, -1)\n            Fs = audiofile.frame_rate\n            x = []\n            for chn in list(range(audiofile.channels)):\n                x.append(data[chn::audiofile.channels])\n            x = numpy.array(x).T\n        else:\n            print(\"Error in readAudioFile(): Unknown file type!\")\n            return (-1,-1)\n    except IOError: \n        print(\"Error: file not found or other I/O error.\")\n        return (-1,-1)\n\n    if x.ndim==2:\n        if x.shape[1]==1:\n            x = x.flatten()\n\n    return (Fs, x)", "response": "This function returns a numpy array that stores the audio samples of a specified WAV of AIFF file and returns the audio samples of the AIFF file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef stereo2mono(x):\n    '''\n    This function converts the input signal\n    (stored in a numpy array) to MONO (if it is STEREO)\n    '''\n    if isinstance(x, int):\n        return -1\n    if x.ndim==1:\n        return x\n    elif x.ndim==2:\n        if x.shape[1]==1:\n            return x.flatten()\n        else:\n            if x.shape[1]==2:\n                return ( (x[:,1] / 2) + (x[:,0] / 2) )\n            else:\n                return -1", "response": "This function converts the input signal to MONO"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef selfSimilarityMatrix(featureVectors):\n    '''\n    This function computes the self-similarity matrix for a sequence\n    of feature vectors.\n    ARGUMENTS:\n     - featureVectors:     a numpy matrix (nDims x nVectors) whose i-th column\n                           corresponds to the i-th feature vector\n\n    RETURNS:\n     - S:                  the self-similarity matrix (nVectors x nVectors)\n    '''\n\n    [nDims, nVectors] = featureVectors.shape\n    [featureVectors2, MEAN, STD] = aT.normalizeFeatures([featureVectors.T])\n    featureVectors2 = featureVectors2[0].T\n    S = 1.0 - distance.squareform(distance.pdist(featureVectors2.T, 'cosine'))\n    return S", "response": "This function computes the self - similarity matrix for a sequence of feature vectors."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert a sequence of class flags per time window into a sequence of class segs and classes", "response": "def flags2segs(flags, window):\n    '''\n    ARGUMENTS:\n     - flags:      a sequence of class flags (per time window)\n     - window:     window duration (in seconds)\n\n    RETURNS:\n     - segs:       a sequence of segment's limits: segs[i,0] is start and\n                   segs[i,1] are start and end point of segment i\n     - classes:    a sequence of class flags: class[i] is the class ID of\n                   the i-th segment\n    '''\n\n    preFlag = 0\n    cur_flag = 0\n    n_segs = 0\n\n    cur_val = flags[cur_flag]\n    segsList = []\n    classes = []\n    while (cur_flag < len(flags) - 1):\n        stop = 0\n        preFlag = cur_flag\n        preVal = cur_val\n        while (stop == 0):\n            cur_flag = cur_flag + 1\n            tempVal = flags[cur_flag]\n            if ((tempVal != cur_val) | (cur_flag == len(flags) - 1)):  # stop\n                n_segs = n_segs + 1\n                stop = 1\n                cur_seg = cur_val\n                cur_val = flags[cur_flag]\n                segsList.append((cur_flag * window))\n                classes.append(preVal)\n    segs = numpy.zeros((len(segsList), 2))\n\n    for i in range(len(segsList)):\n        if i > 0:\n            segs[i, 0] = segsList[i-1]\n        segs[i, 1] = segsList[i]\n    return (segs, classes)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef segs2flags(seg_start, seg_end, seg_label, win_size):\n    '''\n    This function converts segment endpoints and respective segment\n    labels to fix-sized class labels.\n    ARGUMENTS:\n     - seg_start:    segment start points (in seconds)\n     - seg_end:    segment endpoints (in seconds)\n     - seg_label:    segment labels\n      - win_size:    fix-sized window (in seconds)\n    RETURNS:\n     - flags:    numpy array of class indices\n     - class_names:    list of classnames (strings)\n    '''\n    flags = []\n    class_names = list(set(seg_label))\n    curPos = win_size / 2.0\n    while curPos < seg_end[-1]:\n        for i in range(len(seg_start)):\n            if curPos > seg_start[i] and curPos <= seg_end[i]:\n                break\n        flags.append(class_names.index(seg_label[i]))\n        curPos += win_size\n    return numpy.array(flags), class_names", "response": "This function converts segment endpoints and respective segment\n    labels to fix - sized class indices."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef plotSegmentationResults(flags_ind, flags_ind_gt, class_names, mt_step, ONLY_EVALUATE=False):\n    '''\n    This function plots statistics on the classification-segmentation results produced either by the fix-sized supervised method or the HMM method.\n    It also computes the overall accuracy achieved by the respective method if ground-truth is available.\n    '''    \n    flags = [class_names[int(f)] for f in flags_ind]\n    (segs, classes) = flags2segs(flags, mt_step)    \n    min_len = min(flags_ind.shape[0], flags_ind_gt.shape[0])    \n    if min_len > 0:\n        accuracy = numpy.sum(flags_ind[0:min_len] == \n                             flags_ind_gt[0:min_len]) / float(min_len)\n    else:\n        accuracy = -1\n\n    if not ONLY_EVALUATE:\n        duration = segs[-1, 1]\n        s_percentages = numpy.zeros((len(class_names), 1))\n        percentages = numpy.zeros((len(class_names), 1))\n        av_durations = numpy.zeros((len(class_names), 1))\n\n        for iSeg in range(segs.shape[0]):\n            s_percentages[class_names.index(classes[iSeg])] += \\\n                (segs[iSeg, 1]-segs[iSeg, 0])\n\n        for i in range(s_percentages.shape[0]):\n            percentages[i] = 100.0 * s_percentages[i] / duration\n            S = sum(1 for c in classes if c == class_names[i])\n            if S > 0:\n                av_durations[i] = s_percentages[i] / S\n            else:\n                av_durations[i] = 0.0\n\n        for i in range(percentages.shape[0]):\n            print(class_names[i], percentages[i], av_durations[i])\n\n        font = {'size': 10}\n        plt.rc('font', **font)\n\n        fig = plt.figure()\n        ax1 = fig.add_subplot(211)\n        ax1.set_yticks(numpy.array(range(len(class_names))))\n        ax1.axis((0, duration, -1, len(class_names)))\n        ax1.set_yticklabels(class_names)\n        ax1.plot(numpy.array(range(len(flags_ind))) * mt_step +\n                 mt_step / 2.0, flags_ind)\n        if flags_ind_gt.shape[0] > 0:\n            ax1.plot(numpy.array(range(len(flags_ind_gt))) * mt_step +\n                     mt_step / 2.0, flags_ind_gt + 0.05, '--r')\n        plt.xlabel(\"time (seconds)\")\n        if accuracy >= 0:\n            plt.title('Accuracy = {0:.1f}%'.format(100.0 * accuracy))\n\n        ax2 = fig.add_subplot(223)\n        plt.title(\"Classes percentage durations\")\n        ax2.axis((0, len(class_names) + 1, 0, 100))\n        ax2.set_xticks(numpy.array(range(len(class_names) + 1)))\n        ax2.set_xticklabels([\" \"] + class_names)\n        ax2.bar(numpy.array(range(len(class_names))) + 0.5, percentages)\n\n        ax3 = fig.add_subplot(224)\n        plt.title(\"Segment average duration per class\")\n        ax3.axis((0, len(class_names)+1, 0, av_durations.max()))\n        ax3.set_xticks(numpy.array(range(len(class_names) + 1)))\n        ax3.set_xticklabels([\" \"] + class_names)\n        ax3.bar(numpy.array(range(len(class_names))) + 0.5, av_durations)\n        fig.tight_layout()\n        plt.show()\n    return accuracy", "response": "This function plots statistics on the classification - segmentation results produced by the fix - solved supervised method or the HMM method."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef trainHMM_computeStatistics(features, labels):\n    '''\n    This function computes the statistics used to train an HMM joint segmentation-classification model\n    using a sequence of sequential features and respective labels\n\n    ARGUMENTS:\n     - features:    a numpy matrix of feature vectors (numOfDimensions x n_wins)\n     - labels:    a numpy array of class indices (n_wins x 1)\n    RETURNS:\n     - start_prob:    matrix of prior class probabilities (n_classes x 1)\n     - transmat:    transition matrix (n_classes x n_classes)\n     - means:    means matrix (numOfDimensions x 1)\n     - cov:        deviation matrix (numOfDimensions x 1)\n    '''\n    u_labels = numpy.unique(labels)\n    n_comps = len(u_labels)\n\n    n_feats = features.shape[0]\n\n    if features.shape[1] < labels.shape[0]:\n        print(\"trainHMM warning: number of short-term feature vectors \"\n              \"must be greater or equal to the labels length!\")\n        labels = labels[0:features.shape[1]]\n\n    # compute prior probabilities:\n    start_prob = numpy.zeros((n_comps,))\n    for i, u in enumerate(u_labels):\n        start_prob[i] = numpy.count_nonzero(labels == u)\n    # normalize prior probabilities\n    start_prob = start_prob / start_prob.sum()\n\n    # compute transition matrix:\n    transmat = numpy.zeros((n_comps, n_comps))\n    for i in range(labels.shape[0]-1):\n        transmat[int(labels[i]), int(labels[i + 1])] += 1\n    # normalize rows of transition matrix:\n    for i in range(n_comps):\n        transmat[i, :] /= transmat[i, :].sum()\n\n    means = numpy.zeros((n_comps, n_feats))\n    for i in range(n_comps):\n        means[i, :] = numpy.matrix(features[:, \n                                   numpy.nonzero(labels == \n                                                 u_labels[i])[0]].mean(axis=1))\n\n    cov = numpy.zeros((n_comps, n_feats))\n    for i in range(n_comps):\n        #cov[i,:,:] = numpy.cov(features[:,numpy.nonzero(labels==u_labels[i])[0]])  # use this lines if HMM using full gaussian distributions are to be used!\n        cov[i, :] = numpy.std(features[:, numpy.nonzero(labels == \n                                                        u_labels[i])[0]], \n                              axis=1)\n\n    return start_prob, transmat, means, cov", "response": "This function computes the statistics used to train an HMM joint segmentation - classification model using a sequence of sequential features and their corresponding labels."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef trainHMM_fromFile(wav_file, gt_file, hmm_model_name, mt_win, mt_step):\n    '''\n    This function trains a HMM model for segmentation-classification using a single annotated audio file\n    ARGUMENTS:\n     - wav_file:        the path of the audio filename\n     - gt_file:         the path of the ground truth filename\n                       (a csv file of the form <segment start in seconds>,<segment end in seconds>,<segment label> in each row\n     - hmm_model_name:   the name of the HMM model to be stored\n     - mt_win:          mid-term window size\n     - mt_step:         mid-term window step\n    RETURNS:\n     - hmm:            an object to the resulting HMM\n     - class_names:     a list of class_names\n\n    After training, hmm, class_names, along with the mt_win and mt_step values are stored in the hmm_model_name file\n    '''\n\n    [seg_start, seg_end, seg_labs] = readSegmentGT(gt_file)\n    flags, class_names = segs2flags(seg_start, seg_end, seg_labs, mt_step)\n    [fs, x] = audioBasicIO.readAudioFile(wav_file)\n    [F, _, _] = aF.mtFeatureExtraction(x, fs, mt_win * fs, mt_step * fs,\n                                       round(fs * 0.050), round(fs * 0.050))\n    start_prob, transmat, means, cov = trainHMM_computeStatistics(F, flags)\n    hmm = hmmlearn.hmm.GaussianHMM(start_prob.shape[0], \"diag\")\n\n    hmm.startprob_ = start_prob\n    hmm.transmat_ = transmat    \n    hmm.means_ = means\n    hmm.covars_ = cov\n    \n    fo = open(hmm_model_name, \"wb\")\n    cPickle.dump(hmm, fo, protocol=cPickle.HIGHEST_PROTOCOL)\n    cPickle.dump(class_names, fo, protocol=cPickle.HIGHEST_PROTOCOL)\n    cPickle.dump(mt_win, fo, protocol=cPickle.HIGHEST_PROTOCOL)\n    cPickle.dump(mt_step, fo, protocol=cPickle.HIGHEST_PROTOCOL)\n    fo.close()\n\n    return hmm, class_names", "response": "This function trains a HMM model for segmentation - classification using a single annotated audio file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef trainHMM_fromDir(dirPath, hmm_model_name, mt_win, mt_step):\n    '''\n    This function trains a HMM model for segmentation-classification using\n    a where WAV files and .segment (ground-truth files) are stored\n    ARGUMENTS:\n     - dirPath:        the path of the data diretory\n     - hmm_model_name:    the name of the HMM model to be stored\n     - mt_win:        mid-term window size\n     - mt_step:        mid-term window step\n    RETURNS:\n     - hmm:            an object to the resulting HMM\n     - class_names:        a list of class_names\n\n    After training, hmm, class_names, along with the mt_win\n    and mt_step values are stored in the hmm_model_name file\n    '''\n\n    flags_all = numpy.array([])\n    classes_all = []\n    for i, f in enumerate(glob.glob(dirPath + os.sep + '*.wav')):\n        # for each WAV file\n        wav_file = f\n        gt_file = f.replace('.wav', '.segments')\n        if not os.path.isfile(gt_file):\n            continue\n        [seg_start, seg_end, seg_labs] = readSegmentGT(gt_file)\n        flags, class_names = segs2flags(seg_start, seg_end, seg_labs, mt_step)\n        for c in class_names:\n            # update class names:\n            if c not in classes_all:\n                classes_all.append(c)\n        [fs, x] = audioBasicIO.readAudioFile(wav_file)\n        [F, _, _] = aF.mtFeatureExtraction(x, fs, mt_win * fs,\n                                           mt_step * fs, round(fs * 0.050),\n                                           round(fs * 0.050))\n\n        lenF = F.shape[1]\n        lenL = len(flags)\n        min_sm = min(lenF, lenL)\n        F = F[:, 0:min_sm]\n        flags = flags[0:min_sm]\n\n        flagsNew = []\n        for j, fl in enumerate(flags):      # append features and labels\n            flagsNew.append(classes_all.index(class_names[flags[j]]))\n\n        flags_all = numpy.append(flags_all, numpy.array(flagsNew))\n\n        if i == 0:\n            f_all = F\n        else:\n            f_all = numpy.concatenate((f_all, F), axis=1)\n    start_prob, transmat, means, cov = trainHMM_computeStatistics(f_all, flags_all)        # compute HMM statistics\n    hmm = hmmlearn.hmm.GaussianHMM(start_prob.shape[0], \"diag\")      # train HMM\n    hmm.startprob_ = start_prob\n    hmm.transmat_ = transmat        \n    hmm.means_ = means\n    hmm.covars_ = cov\n\n    fo = open(hmm_model_name, \"wb\")   # save HMM model\n    cPickle.dump(hmm, fo, protocol=cPickle.HIGHEST_PROTOCOL)\n    cPickle.dump(classes_all, fo, protocol=cPickle.HIGHEST_PROTOCOL)\n    cPickle.dump(mt_win, fo, protocol=cPickle.HIGHEST_PROTOCOL)\n    cPickle.dump(mt_step, fo, protocol=cPickle.HIGHEST_PROTOCOL)\n    fo.close()\n\n    return hmm, classes_all", "response": "This function trains a HMM model from a directory containing WAV files and ground - truth files."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef mtFileClassification(input_file, model_name, model_type,\n                         plot_results=False, gt_file=\"\"):\n    '''\n    This function performs mid-term classification of an audio stream.\n    Towards this end, supervised knowledge is used, i.e. a pre-trained classifier.\n    ARGUMENTS:\n        - input_file:        path of the input WAV file\n        - model_name:        name of the classification model\n        - model_type:        svm or knn depending on the classifier type\n        - plot_results:      True if results are to be plotted using\n                             matplotlib along with a set of statistics\n\n    RETURNS:\n          - segs:           a sequence of segment's endpoints: segs[i] is the\n                            endpoint of the i-th segment (in seconds)\n          - classes:        a sequence of class flags: class[i] is the\n                            class ID of the i-th segment\n    '''\n\n    if not os.path.isfile(model_name):\n        print(\"mtFileClassificationError: input model_type not found!\")\n        return (-1, -1, -1, -1)\n    # Load classifier:\n    if model_type == \"knn\":\n        [classifier, MEAN, STD, class_names, mt_win, mt_step, st_win, st_step, compute_beat] = \\\n            aT.load_model_knn(model_name)\n    else:\n        [classifier, MEAN, STD, class_names, mt_win, mt_step, st_win, st_step,\n         compute_beat] = aT.load_model(model_name)\n\n    if compute_beat:\n        print(\"Model \" + model_name + \" contains long-term music features \"\n                                     \"(beat etc) and cannot be used in \"\n                                     \"segmentation\")\n        return (-1, -1, -1, -1)\n    [fs, x] = audioBasicIO.readAudioFile(input_file)       # load input file\n    if fs == -1:                                           # could not read file\n        return (-1, -1, -1, -1)\n    x = audioBasicIO.stereo2mono(x)                        # convert stereo (if) to mono\n    duration = len(x) / fs\n    # mid-term feature extraction:\n    [mt_feats, _, _] = aF.mtFeatureExtraction(x, fs, mt_win * fs,\n                                                     mt_step * fs,\n                                                     round(fs * st_win),\n                                                     round(fs * st_step))\n    flags = []\n    Ps = []\n    flags_ind = []\n    for i in range(mt_feats.shape[1]):              # for each feature vector (i.e. for each fix-sized segment):\n        cur_fv = (mt_feats[:, i] - MEAN) / STD       # normalize current feature vector\n        [res, P] = aT.classifierWrapper(classifier, model_type, cur_fv)    # classify vector\n        flags_ind.append(res)\n        flags.append(class_names[int(res)])              # update class label matrix\n        Ps.append(numpy.max(P))                          # update probability matrix\n    flags_ind = numpy.array(flags_ind)\n\n    # 1-window smoothing\n    for i in range(1, len(flags_ind) - 1):\n        if flags_ind[i-1] == flags_ind[i + 1]:\n            flags_ind[i] = flags_ind[i + 1]\n    # convert fix-sized flags to segments and classes\n    (segs, classes) = flags2segs(flags, mt_step)\n    segs[-1] = len(x) / float(fs)\n\n    # Load grount-truth:        \n    if os.path.isfile(gt_file):\n        [seg_start_gt, seg_end_gt, seg_l_gt] = readSegmentGT(gt_file)\n        flags_gt, class_names_gt = segs2flags(seg_start_gt, seg_end_gt, seg_l_gt, mt_step)\n        flags_ind_gt = []\n        for j, fl in enumerate(flags_gt):\n            # \"align\" labels with GT\n            if class_names_gt[flags_gt[j]] in class_names:\n                flags_ind_gt.append(class_names.index(class_names_gt[flags_gt[j]]))\n            else:\n                flags_ind_gt.append(-1)\n        flags_ind_gt = numpy.array(flags_ind_gt)        \n        cm = numpy.zeros((len(class_names_gt), len(class_names_gt)))\n        for i in range(min(flags_ind.shape[0], flags_ind_gt.shape[0])):\n            cm[int(flags_ind_gt[i]),int(flags_ind[i])] += 1        \n    else:\n        cm = []\n        flags_ind_gt = numpy.array([])\n    acc = plotSegmentationResults(flags_ind, flags_ind_gt,\n                                  class_names, mt_step, not plot_results)\n    if acc >= 0:\n        print(\"Overall Accuracy: {0:.3f}\".format(acc)  )\n        return (flags_ind, class_names_gt, acc, cm)\n    else:\n        return (flags_ind, class_names, acc, cm)", "response": "This function performs the MIDI classification of an audio file."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfunctions that is used to silence removal of a signal in a short - term sequence.", "response": "def silenceRemoval(x, fs, st_win, st_step, smoothWindow=0.5, weight=0.5, plot=False):\n    '''\n    Event Detection (silence removal)\n    ARGUMENTS:\n         - x:                the input audio signal\n         - fs:               sampling freq\n         - st_win, st_step:    window size and step in seconds\n         - smoothWindow:     (optinal) smooth window (in seconds)\n         - weight:           (optinal) weight factor (0 < weight < 1) the higher, the more strict\n         - plot:             (optinal) True if results are to be plotted\n    RETURNS:\n         - seg_limits:    list of segment limits in seconds (e.g [[0.1, 0.9], [1.4, 3.0]] means that\n                    the resulting segments are (0.1 - 0.9) seconds and (1.4, 3.0) seconds\n    '''\n\n    if weight >= 1:\n        weight = 0.99\n    if weight <= 0:\n        weight = 0.01\n\n    # Step 1: feature extraction\n    x = audioBasicIO.stereo2mono(x)\n    st_feats, _ = aF.stFeatureExtraction(x, fs, st_win * fs, \n                                                  st_step * fs)\n\n    # Step 2: train binary svm classifier of low vs high energy frames\n    # keep only the energy short-term sequence (2nd feature)\n    st_energy = st_feats[1, :]\n    en = numpy.sort(st_energy)\n    # number of 10% of the total short-term windows\n    l1 = int(len(en) / 10)\n    # compute \"lower\" 10% energy threshold\n    t1 = numpy.mean(en[0:l1]) + 0.000000000000001\n    # compute \"higher\" 10% energy threshold\n    t2 = numpy.mean(en[-l1:-1]) + 0.000000000000001\n    # get all features that correspond to low energy\n    class1 = st_feats[:, numpy.where(st_energy <= t1)[0]]\n    # get all features that correspond to high energy\n    class2 = st_feats[:, numpy.where(st_energy >= t2)[0]]\n    # form the binary classification task and ...\n    faets_s = [class1.T, class2.T]\n    # normalize and train the respective svm probabilistic model\n    # (ONSET vs SILENCE)\n    [faets_s_norm, means_s, stds_s] = aT.normalizeFeatures(faets_s)\n    svm = aT.trainSVM(faets_s_norm, 1.0)\n\n    # Step 3: compute onset probability based on the trained svm\n    prob_on_set = []\n    for i in range(st_feats.shape[1]):\n        # for each frame\n        cur_fv = (st_feats[:, i] - means_s) / stds_s\n        # get svm probability (that it belongs to the ONSET class)\n        prob_on_set.append(svm.predict_proba(cur_fv.reshape(1,-1))[0][1])\n    prob_on_set = numpy.array(prob_on_set)\n    # smooth probability:\n    prob_on_set = smoothMovingAvg(prob_on_set, smoothWindow / st_step)\n\n    # Step 4A: detect onset frame indices:\n    prog_on_set_sort = numpy.sort(prob_on_set)\n    # find probability Threshold as a weighted average\n    # of top 10% and lower 10% of the values\n    Nt = int(prog_on_set_sort.shape[0] / 10)\n    T = (numpy.mean((1 - weight) * prog_on_set_sort[0:Nt]) +\n         weight * numpy.mean(prog_on_set_sort[-Nt::]))\n\n    max_idx = numpy.where(prob_on_set > T)[0]\n    # get the indices of the frames that satisfy the thresholding\n    i = 0\n    time_clusters = []\n    seg_limits = []\n\n    # Step 4B: group frame indices to onset segments\n    while i < len(max_idx):\n        # for each of the detected onset indices\n        cur_cluster = [max_idx[i]]\n        if i == len(max_idx)-1:\n            break\n        while max_idx[i+1] - cur_cluster[-1] <= 2:\n            cur_cluster.append(max_idx[i+1])\n            i += 1\n            if i == len(max_idx)-1:\n                break\n        i += 1\n        time_clusters.append(cur_cluster)\n        seg_limits.append([cur_cluster[0] * st_step,\n                           cur_cluster[-1] * st_step])\n\n    # Step 5: Post process: remove very small segments:\n    min_dur = 0.2\n    seg_limits_2 = []\n    for s in seg_limits:\n        if s[1] - s[0] > min_dur:\n            seg_limits_2.append(s)\n    seg_limits = seg_limits_2\n\n    if plot:\n        timeX = numpy.arange(0, x.shape[0] / float(fs), 1.0 / fs)\n\n        plt.subplot(2, 1, 1)\n        plt.plot(timeX, x)\n        for s in seg_limits:\n            plt.axvline(x=s[0])\n            plt.axvline(x=s[1])\n        plt.subplot(2, 1, 2)\n        plt.plot(numpy.arange(0, prob_on_set.shape[0] * st_step, st_step), \n                 prob_on_set)\n        plt.title('Signal')\n        for s in seg_limits:\n            plt.axvline(x=s[0])\n            plt.axvline(x=s[1])\n        plt.title('svm Probability')\n        plt.show()\n\n    return seg_limits"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef levenshtein(str1, s2):\n    '''\n    Distance between two strings\n    '''\n    N1 = len(str1)\n    N2 = len(s2)\n\n    stringRange = [range(N1 + 1)] * (N2 + 1)\n    for i in range(N2 + 1):\n        stringRange[i] = range(i,i + N1 + 1)\n    for i in range(0,N2):\n        for j in range(0,N1):\n            if str1[j] == s2[i]:\n                stringRange[i+1][j+1] = min(stringRange[i+1][j] + 1,\n                                            stringRange[i][j+1] + 1,\n                                            stringRange[i][j])\n            else:\n                stringRange[i+1][j+1] = min(stringRange[i+1][j] + 1,\n                                            stringRange[i][j+1] + 1,\n                                            stringRange[i][j] + 1)\n    return stringRange[N2][N1]", "response": "Returns the Levenshtein distance between two strings"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates a list of colors based on a list of strings. Similar strings correspond to similar colors.", "response": "def text_list_to_colors(names):\n    '''\n    Generates a list of colors based on a list of names (strings). Similar strings correspond to similar colors.\n    '''\n    # STEP A: compute strings distance between all combnations of strings\n    Dnames = np.zeros( (len(names), len(names)) )\n    for i in range(len(names)):\n        for j in range(len(names)):\n            Dnames[i,j] = 1 - 2.0 * levenshtein(names[i], names[j]) / float(len(names[i]+names[j]))\n\n    # STEP B: pca dimanesionality reduction to a single-dimension (from the distance space)\n    pca = sklearn.decomposition.PCA(n_components = 1)\n    pca.fit(Dnames)    \n    \n    # STEP C: mapping of 1-dimensional values to colors in a jet-colormap\n    textToColor = pca.transform(Dnames)\n    textToColor = 255 * (textToColor - textToColor.min()) / (textToColor.max() - textToColor.min())\n    textmaps = generateColorMap();\n    colors = [textmaps[int(c)] for c in textToColor]\n    return colors"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef text_list_to_colors_simple(names):\n    '''\n    Generates a list of colors based on a list of names (strings). Similar strings correspond to similar colors. \n    '''\n    uNames = list(set(names))\n    uNames.sort()\n    textToColor = [ uNames.index(n) for n in names ]\n    textToColor = np.array(textToColor)\n    textToColor = 255 * (textToColor - textToColor.min()) / \\\n                  (textToColor.max() - textToColor.min())\n    textmaps = generateColorMap();\n    colors = [textmaps[int(c)] for c in textToColor]\n    return colors", "response": "Generates a list of colors based on a list of strings. Similar strings correspond to similar colors. \n   "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating a d3js chordial diagram that illustrates similarites", "response": "def chordialDiagram(fileStr, SM, Threshold, names, namesCategories):\n    '''\n    Generates a d3js chordial diagram that illustrates similarites\n    '''\n    colors = text_list_to_colors_simple(namesCategories)\n    SM2 = SM.copy()\n    SM2 = (SM2 + SM2.T) / 2.0\n    for i in range(SM2.shape[0]):\n        M = Threshold\n#        a = np.sort(SM2[i,:])[::-1]\n#        M = np.mean(a[0:int(SM2.shape[1]/3+1)])\n        SM2[i, SM2[i, :] < M] = 0;\n    dirChordial = fileStr + \"_Chordial\"\n    if not os.path.isdir(dirChordial):\n        os.mkdir(dirChordial)\n    jsonPath         = dirChordial + os.sep + \"matrix.json\"\n    namesPath        = dirChordial + os.sep + \"Names.csv\"\n \n    jsonSMMatrix = simplejson.dumps(SM2.tolist())\n    f = open(jsonPath,'w'); f.write(jsonSMMatrix);  f.close()\n    f = open(namesPath,'w'); f.write(\"name,color\\n\"); \n    for i, n in enumerate(names):\n        f.write(\"{0:s},{1:s}\\n\".format(n,\"#\"+str(colors[i])))\n    f.close()\n\n    shutil.copyfile(os.path.join(os.path.dirname(os.path.realpath(__file__)),\n                                 \"data\", \"similarities.html\"),\n                    dirChordial+os.sep+\"similarities.html\")\n    shutil.copyfile(os.path.join(os.path.dirname(os.path.realpath(__file__)),\n                                 \"data\",\n                                 \"style.css\"),\n                    dirChordial+os.sep+\"style.css\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef visualizeFeaturesFolder(folder, dimReductionMethod, priorKnowledge = \"none\"):\n    '''\n    This function generates a chordial visualization for the recordings of the provided path.\n    ARGUMENTS:\n        - folder:        path of the folder that contains the WAV files to be processed\n        - dimReductionMethod:    method used to reduce the dimension of the initial feature space before computing the similarity.\n        - priorKnowledge:    if this is set equal to \"artist\"\n    '''\n    if dimReductionMethod==\"pca\":\n        allMtFeatures, wavFilesList, _ = aF.dirWavFeatureExtraction(folder, 30.0, 30.0, 0.050, 0.050, compute_beat = True)\n        if allMtFeatures.shape[0]==0:\n            print(\"Error: No data found! Check input folder\")\n            return\n        \n        namesCategoryToVisualize = [ntpath.basename(w).replace('.wav','').split(\" --- \")[0] for w in wavFilesList]; \n        namesToVisualize       = [ntpath.basename(w).replace('.wav','') for w in wavFilesList]; \n\n        (F, MEAN, STD) = aT.normalizeFeatures([allMtFeatures])\n        F = np.concatenate(F)\n        \n        # check that the new PCA dimension is at most equal to the number of samples\n        K1 = 2\n        K2 = 10\n        if K1 > F.shape[0]:\n            K1 = F.shape[0]\n        if K2 > F.shape[0]:\n            K2 = F.shape[0]\n        pca1 = sklearn.decomposition.PCA(n_components = K1)\n        pca1.fit(F)        \n        pca2 = sklearn.decomposition.PCA(n_components = K2)\n        pca2.fit(F)        \n\n        finalDims = pca1.transform(F)\n        finalDims2 = pca2.transform(F)\n    else:    \n        allMtFeatures, Ys, wavFilesList = aF.dirWavFeatureExtractionNoAveraging(folder, 20.0, 5.0, 0.040, 0.040) # long-term statistics cannot be applied in this context (LDA needs mid-term features)\n        if allMtFeatures.shape[0]==0:\n            print(\"Error: No data found! Check input folder\")\n            return\n        \n        namesCategoryToVisualize = [ntpath.basename(w).replace('.wav','').split(\" --- \")[0] for w in wavFilesList]; \n        namesToVisualize       = [ntpath.basename(w).replace('.wav','') for w in wavFilesList]; \n\n        ldaLabels = Ys\n        if priorKnowledge==\"artist\":\n            uNamesCategoryToVisualize = list(set(namesCategoryToVisualize))\n            YsNew = np.zeros( Ys.shape )\n            for i, uname in enumerate(uNamesCategoryToVisualize):        # for each unique artist name:\n                indicesUCategories = [j for j, x in enumerate(namesCategoryToVisualize) if x == uname]\n                for j in indicesUCategories:\n                    indices = np.nonzero(Ys==j)\n                    YsNew[indices] = i\n            ldaLabels = YsNew\n\n        (F, MEAN, STD) = aT.normalizeFeatures([allMtFeatures])\n        F = np.array(F[0])\n\n        clf = sklearn.discriminant_analysis.LinearDiscriminantAnalysis(n_components=10)\n        clf.fit(F, ldaLabels)    \n        reducedDims =  clf.transform(F)\n\n        pca = sklearn.decomposition.PCA(n_components = 2)\n        pca.fit(reducedDims)\n        reducedDims = pca.transform(reducedDims)\n\n        # TODO: CHECK THIS ... SHOULD LDA USED IN SEMI-SUPERVISED ONLY????\n\n        uLabels = np.sort(np.unique((Ys)))        # uLabels must have as many labels as the number of wavFilesList elements\n        reducedDimsAvg = np.zeros( (uLabels.shape[0], reducedDims.shape[1] ) )\n        finalDims = np.zeros( (uLabels.shape[0], 2) ) \n        for i, u in enumerate(uLabels):\n            indices = [j for j, x in enumerate(Ys) if x == u]\n            f = reducedDims[indices, :]\n            finalDims[i, :] = f.mean(axis=0)\n        finalDims2 = reducedDims\n\n    for i in range(finalDims.shape[0]):            \n        plt.text(finalDims[i,0], finalDims[i,1], ntpath.basename(wavFilesList[i].replace('.wav','')), horizontalalignment='center', verticalalignment='center', fontsize=10)\n        plt.plot(finalDims[i,0], finalDims[i,1], '*r')\n    plt.xlim([1.2*finalDims[:,0].min(), 1.2*finalDims[:,0].max()])\n    plt.ylim([1.2*finalDims[:,1].min(), 1.2*finalDims[:,1].max()])            \n    plt.show()\n\n    SM = 1.0 - distance.squareform(distance.pdist(finalDims2, 'cosine'))\n    for i in range(SM.shape[0]):\n        SM[i,i] = 0.0;\n\n\n    chordialDiagram(\"visualization\", SM, 0.50, namesToVisualize, namesCategoryToVisualize)\n\n    SM = 1.0 - distance.squareform(distance.pdist(F, 'cosine'))\n    for i in range(SM.shape[0]):\n        SM[i,i] = 0.0;\n    chordialDiagram(\"visualizationInitial\", SM, 0.50, namesToVisualize, namesCategoryToVisualize)\n\n    # plot super-categories (i.e. artistname\n    uNamesCategoryToVisualize = sort(list(set(namesCategoryToVisualize)))\n    finalDimsGroup = np.zeros( (len(uNamesCategoryToVisualize), finalDims2.shape[1] ) )\n    for i, uname in enumerate(uNamesCategoryToVisualize):\n        indices = [j for j, x in enumerate(namesCategoryToVisualize) if x == uname]\n        f = finalDims2[indices, :]\n        finalDimsGroup[i, :] = f.mean(axis=0)\n\n    SMgroup = 1.0 - distance.squareform(distance.pdist(finalDimsGroup, 'cosine'))\n    for i in range(SMgroup.shape[0]):\n        SMgroup[i,i] = 0.0;\n    chordialDiagram(\"visualizationGroup\", SMgroup, 0.50, uNamesCategoryToVisualize, uNamesCategoryToVisualize)", "response": "This function generates a chordial visualization for the recordings of the folder."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute zero crossing rate of frame", "response": "def stZCR(frame):\n    \"\"\"Computes zero crossing rate of frame\"\"\"\n    count = len(frame)\n    countZ = numpy.sum(numpy.abs(numpy.diff(numpy.sign(frame)))) / 2\n    return (numpy.float64(countZ) / numpy.float64(count-1.0))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef stEnergyEntropy(frame, n_short_blocks=10):\n    Eol = numpy.sum(frame ** 2)    # total frame energy\n    L = len(frame)\n    sub_win_len = int(numpy.floor(L / n_short_blocks))\n    if L != sub_win_len * n_short_blocks:\n            frame = frame[0:sub_win_len * n_short_blocks]\n    # sub_wins is of size [n_short_blocks x L]\n    sub_wins = frame.reshape(sub_win_len, n_short_blocks, order='F').copy()\n\n    # Compute normalized sub-frame energies:\n    s = numpy.sum(sub_wins ** 2, axis=0) / (Eol + eps)\n\n    # Compute entropy of the normalized sub-frame energies:\n    Entropy = -numpy.sum(s * numpy.log2(s + eps))\n    return Entropy", "response": "Computes the entropy of the energy of a frame."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef stSpectralCentroidAndSpread(X, fs):\n    ind = (numpy.arange(1, len(X) + 1)) * (fs/(2.0 * len(X)))\n\n    Xt = X.copy()\n    Xt = Xt / Xt.max()\n    NUM = numpy.sum(ind * Xt)\n    DEN = numpy.sum(Xt) + eps\n\n    # Centroid:\n    C = (NUM / DEN)\n\n    # Spread:\n    S = numpy.sqrt(numpy.sum(((ind - C) ** 2) * Xt) / DEN)\n\n    # Normalize:\n    C = C / (fs / 2.0)\n    S = S / (fs / 2.0)\n\n    return (C, S)", "response": "Computes the spectral centroid of frame X given abs ( FFT )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute the spectral entropy of a single frame.", "response": "def stSpectralEntropy(X, n_short_blocks=10):\n    \"\"\"Computes the spectral entropy\"\"\"\n    L = len(X)                         # number of frame samples\n    Eol = numpy.sum(X ** 2)            # total spectral energy\n\n    sub_win_len = int(numpy.floor(L / n_short_blocks))   # length of sub-frame\n    if L != sub_win_len * n_short_blocks:\n        X = X[0:sub_win_len * n_short_blocks]\n\n    sub_wins = X.reshape(sub_win_len, n_short_blocks, order='F').copy()  # define sub-frames (using matrix reshape)\n    s = numpy.sum(sub_wins ** 2, axis=0) / (Eol + eps)                      # compute spectral sub-energies\n    En = -numpy.sum(s*numpy.log2(s + eps))                                    # compute spectral entropy\n\n    return En"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes the spectral flux feature of the current frame", "response": "def stSpectralFlux(X, X_prev):\n    \"\"\"\n    Computes the spectral flux feature of the current frame\n    ARGUMENTS:\n        X:            the abs(fft) of the current frame\n        X_prev:        the abs(fft) of the previous frame\n    \"\"\"\n    # compute the spectral flux as the sum of square distances:\n    sumX = numpy.sum(X + eps)\n    sumPrevX = numpy.sum(X_prev + eps)\n    F = numpy.sum((X / sumX - X_prev/sumPrevX) ** 2)\n\n    return F"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing the spectral roll - off of a frequency set", "response": "def stSpectralRollOff(X, c, fs):\n    \"\"\"Computes spectral roll-off\"\"\"\n    totalEnergy = numpy.sum(X ** 2)\n    fftLength = len(X)\n    Thres = c*totalEnergy\n    # Ffind the spectral rolloff as the frequency position \n    # where the respective spectral energy is equal to c*totalEnergy\n    CumSum = numpy.cumsum(X ** 2) + eps\n    [a, ] = numpy.nonzero(CumSum > Thres)\n    if len(a) > 0:\n        mC = numpy.float64(a[0]) / (float(fftLength))\n    else:\n        mC = 0.0\n    return (mC)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef stHarmonic(frame, fs):\n    M = numpy.round(0.016 * fs) - 1\n    R = numpy.correlate(frame, frame, mode='full')\n\n    g = R[len(frame)-1]\n    R = R[len(frame):-1]\n\n    # estimate m0 (as the first zero crossing of R)\n    [a, ] = numpy.nonzero(numpy.diff(numpy.sign(R)))\n\n    if len(a) == 0:\n        m0 = len(R)-1\n    else:\n        m0 = a[0]\n    if M > len(R):\n        M = len(R) - 1\n\n    Gamma = numpy.zeros((M), dtype=numpy.float64)\n    CSum = numpy.cumsum(frame ** 2)\n    Gamma[m0:M] = R[m0:M] / (numpy.sqrt((g * CSum[M:m0:-1])) + eps)\n\n    ZCR = stZCR(Gamma)\n\n    if ZCR > 0.15:\n        HR = 0.0\n        f0 = 0.0\n    else:\n        if len(Gamma) == 0:\n            HR = 1.0\n            blag = 0.0\n            Gamma = numpy.zeros((M), dtype=numpy.float64)\n        else:\n            HR = numpy.max(Gamma)\n            blag = numpy.argmax(Gamma)\n\n        # Get fundamental frequency:\n        f0 = fs / (blag + eps)\n        if f0 > 5000:\n            f0 = 0.0\n        if HR < 0.1:\n            f0 = 0.0\n\n    return (HR, f0)", "response": "Computes harmonic ratio and pitch of a single frame."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing the triangular filterbank for MFCC computation (used in the stFeatureExtraction function before the stMFCC function call) This function is taken from the scikits.talkbox library (MIT Licence): https://pypi.python.org/pypi/scikits.talkbox", "response": "def mfccInitFilterBanks(fs, nfft):\n    \"\"\"\n    Computes the triangular filterbank for MFCC computation \n    (used in the stFeatureExtraction function before the stMFCC function call)\n    This function is taken from the scikits.talkbox library (MIT Licence):\n    https://pypi.python.org/pypi/scikits.talkbox\n    \"\"\"\n\n    # filter bank params:\n    lowfreq = 133.33\n    linsc = 200/3.\n    logsc = 1.0711703\n    numLinFiltTotal = 13\n    numLogFilt = 27\n\n    if fs < 8000:\n        nlogfil = 5\n\n    # Total number of filters\n    nFiltTotal = numLinFiltTotal + numLogFilt\n\n    # Compute frequency points of the triangle:\n    freqs = numpy.zeros(nFiltTotal+2)\n    freqs[:numLinFiltTotal] = lowfreq + numpy.arange(numLinFiltTotal) * linsc\n    freqs[numLinFiltTotal:] = freqs[numLinFiltTotal-1] * logsc ** numpy.arange(1, numLogFilt + 3)\n    heights = 2./(freqs[2:] - freqs[0:-2])\n\n    # Compute filterbank coeff (in fft domain, in bins)\n    fbank = numpy.zeros((nFiltTotal, nfft))\n    nfreqs = numpy.arange(nfft) / (1. * nfft) * fs\n\n    for i in range(nFiltTotal):\n        lowTrFreq = freqs[i]\n        cenTrFreq = freqs[i+1]\n        highTrFreq = freqs[i+2]\n\n        lid = numpy.arange(numpy.floor(lowTrFreq * nfft / fs) + 1, \n                           numpy.floor(cenTrFreq * nfft / fs) + 1,  \n                                       dtype=numpy.int)\n        lslope = heights[i] / (cenTrFreq - lowTrFreq)\n        rid = numpy.arange(numpy.floor(cenTrFreq * nfft / fs) + 1, \n                                       numpy.floor(highTrFreq * nfft / fs) + 1, \n                                       dtype=numpy.int)\n        rslope = heights[i] / (highTrFreq - cenTrFreq)\n        fbank[i][lid] = lslope * (nfreqs[lid] - lowTrFreq)\n        fbank[i][rid] = rslope * (highTrFreq - nfreqs[rid])\n\n    return fbank, freqs"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef stMFCC(X, fbank, n_mfcc_feats):\n\n    mspec = numpy.log10(numpy.dot(X, fbank.T)+eps)\n    ceps = dct(mspec, type=2, norm='ortho', axis=-1)[:n_mfcc_feats]\n    return ceps", "response": "Computes the MFCCs of a frame given the fft magnitude X and filter bank."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef stChromaFeaturesInit(nfft, fs):\n    freqs = numpy.array([((f + 1) * fs) / (2 * nfft) for f in range(nfft)])    \n    Cp = 27.50    \n    nChroma = numpy.round(12.0 * numpy.log2(freqs / Cp)).astype(int)\n\n    nFreqsPerChroma = numpy.zeros((nChroma.shape[0], ))\n\n    uChroma = numpy.unique(nChroma)\n    for u in uChroma:\n        idx = numpy.nonzero(nChroma == u)\n        nFreqsPerChroma[idx] = idx[0].shape\n    \n    return nChroma, nFreqsPerChroma", "response": "This function initializes the chroma matrices used in the calculation of the chroma features"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef stChromagram(signal, fs, win, step, PLOT=False):\n    win = int(win)\n    step = int(step)\n    signal = numpy.double(signal)\n    signal = signal / (2.0 ** 15)\n    DC = signal.mean()\n    MAX = (numpy.abs(signal)).max()\n    signal = (signal - DC) / (MAX - DC)\n\n    N = len(signal)        # total number of signals\n    cur_p = 0\n    count_fr = 0\n    nfft = int(win / 2)\n    nChroma, nFreqsPerChroma = stChromaFeaturesInit(nfft, fs)\n    chromaGram = numpy.array([], dtype=numpy.float64)\n\n    while (cur_p + win - 1 < N):\n        count_fr += 1\n        x = signal[cur_p:cur_p + win]\n        cur_p = cur_p + step\n        X = abs(fft(x))\n        X = X[0:nfft]\n        X = X / len(X)\n        chromaNames, C = stChromaFeatures(X, fs, nChroma, nFreqsPerChroma)\n        C = C[:, 0]\n        if count_fr == 1:\n            chromaGram = C.T\n        else:\n            chromaGram = numpy.vstack((chromaGram, C.T))\n    FreqAxis = chromaNames\n    TimeAxis = [(t * step) / fs for t in range(chromaGram.shape[0])]\n\n    if (PLOT):\n        fig, ax = plt.subplots()\n        chromaGramToPlot = chromaGram.transpose()[::-1, :]\n        Ratio = int(chromaGramToPlot.shape[1] / (3*chromaGramToPlot.shape[0]))\n        if Ratio < 1:\n            Ratio = 1\n        chromaGramToPlot = numpy.repeat(chromaGramToPlot, Ratio, axis=0)\n        imgplot = plt.imshow(chromaGramToPlot)\n        fstep = int(nfft / 5.0)\n#        FreqTicks = range(0, int(nfft) + fstep, fstep)\n#        FreqTicksLabels = [str(fs/2-int((f*fs) / (2*nfft))) for f in FreqTicks]\n        ax.set_yticks(range(int(Ratio / 2), len(FreqAxis) * Ratio, Ratio))\n        ax.set_yticklabels(FreqAxis[::-1])\n        TStep = int(count_fr / 3)\n        TimeTicks = range(0, count_fr, TStep)\n        TimeTicksLabels = ['%.2f' % (float(t * step) / fs) for t in TimeTicks]\n        ax.set_xticks(TimeTicks)\n        ax.set_xticklabels(TimeTicksLabels)\n        ax.set_xlabel('time (secs)')\n        imgplot.set_cmap('jet')\n        plt.colorbar()\n        plt.show()\n\n    return (chromaGram, TimeAxis, FreqAxis)", "response": "Function that calculates the short - term FFT mag for spectogram estimation"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef stSpectogram(signal, fs, win, step, PLOT=False):\n    win = int(win)\n    step = int(step)\n    signal = numpy.double(signal)\n    signal = signal / (2.0 ** 15)\n    DC = signal.mean()\n    MAX = (numpy.abs(signal)).max()\n    signal = (signal - DC) / (MAX - DC)\n\n    N = len(signal)        # total number of signals\n    cur_p = 0\n    count_fr = 0\n    nfft = int(win / 2)\n    specgram = numpy.array([], dtype=numpy.float64)\n\n    while (cur_p + win - 1 < N):\n        count_fr += 1\n        x = signal[cur_p:cur_p+win]\n        cur_p = cur_p + step\n        X = abs(fft(x))\n        X = X[0:nfft]\n        X = X / len(X)\n\n        if count_fr == 1:\n            specgram = X ** 2\n        else:\n            specgram = numpy.vstack((specgram, X))\n\n    FreqAxis = [float((f + 1) * fs) / (2 * nfft) for f in range(specgram.shape[1])]\n    TimeAxis = [float(t * step) / fs for t in range(specgram.shape[0])]\n\n    if (PLOT):\n        fig, ax = plt.subplots()\n        imgplot = plt.imshow(specgram.transpose()[::-1, :])\n        fstep = int(nfft / 5.0)\n        FreqTicks = range(0, int(nfft) + fstep, fstep)\n        FreqTicksLabels = [str(fs / 2 - int((f * fs) / (2 * nfft))) for f in FreqTicks]\n        ax.set_yticks(FreqTicks)\n        ax.set_yticklabels(FreqTicksLabels)\n        TStep = int(count_fr/3)\n        TimeTicks = range(0, count_fr, TStep)\n        TimeTicksLabels = ['%.2f' % (float(t * step) / fs) for t in TimeTicks]\n        ax.set_xticks(TimeTicks)\n        ax.set_xticklabels(TimeTicksLabels)\n        ax.set_xlabel('time (secs)')\n        ax.set_ylabel('freq (Hz)')\n        imgplot.set_cmap('jet')\n        plt.colorbar()\n        plt.show()\n\n    return (specgram, TimeAxis, FreqAxis)", "response": "This function calculates the short - term FFT for spectogram estimation"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef stFeatureExtraction(signal, fs, win, step):\n\n    win = int(win)\n    step = int(step)\n\n    # Signal normalization\n    signal = numpy.double(signal)\n\n    signal = signal / (2.0 ** 15)\n    DC = signal.mean()\n    MAX = (numpy.abs(signal)).max()\n    signal = (signal - DC) / (MAX + 0.0000000001)\n\n    N = len(signal)                                # total number of samples\n    cur_p = 0\n    count_fr = 0\n    nFFT = int(win / 2)\n\n    [fbank, freqs] = mfccInitFilterBanks(fs, nFFT)                # compute the triangular filter banks used in the mfcc calculation\n    nChroma, nFreqsPerChroma = stChromaFeaturesInit(nFFT, fs)\n\n    n_time_spectral_feats = 8\n    n_harmonic_feats = 0\n    n_mfcc_feats = 13\n    n_chroma_feats = 13\n    n_total_feats = n_time_spectral_feats + n_mfcc_feats + n_harmonic_feats + n_chroma_feats\n#    n_total_feats = n_time_spectral_feats + n_mfcc_feats + n_harmonic_feats\n    feature_names = []\n    feature_names.append(\"zcr\")\n    feature_names.append(\"energy\")\n    feature_names.append(\"energy_entropy\")\n    feature_names += [\"spectral_centroid\", \"spectral_spread\"]\n    feature_names.append(\"spectral_entropy\")\n    feature_names.append(\"spectral_flux\")\n    feature_names.append(\"spectral_rolloff\")\n    feature_names += [\"mfcc_{0:d}\".format(mfcc_i) \n                      for mfcc_i in range(1, n_mfcc_feats+1)]\n    feature_names += [\"chroma_{0:d}\".format(chroma_i) \n                      for chroma_i in range(1, n_chroma_feats)]\n    feature_names.append(\"chroma_std\")\n    st_features = []\n    while (cur_p + win - 1 < N):                        # for each short-term window until the end of signal\n        count_fr += 1\n        x = signal[cur_p:cur_p+win]                    # get current window\n        cur_p = cur_p + step                           # update window position\n        X = abs(fft(x))                                  # get fft magnitude\n        X = X[0:nFFT]                                    # normalize fft\n        X = X / len(X)\n        if count_fr == 1:\n            X_prev = X.copy()                             # keep previous fft mag (used in spectral flux)\n        curFV = numpy.zeros((n_total_feats, 1))\n        curFV[0] = stZCR(x)                              # zero crossing rate\n        curFV[1] = stEnergy(x)                           # short-term energy\n        curFV[2] = stEnergyEntropy(x)                    # short-term entropy of energy\n        [curFV[3], curFV[4]] = stSpectralCentroidAndSpread(X, fs)    # spectral centroid and spread\n        curFV[5] = stSpectralEntropy(X)                  # spectral entropy\n        curFV[6] = stSpectralFlux(X, X_prev)              # spectral flux\n        curFV[7] = stSpectralRollOff(X, 0.90, fs)        # spectral rolloff\n        curFV[n_time_spectral_feats:n_time_spectral_feats+n_mfcc_feats, 0] = \\\n            stMFCC(X, fbank, n_mfcc_feats).copy()    # MFCCs\n        chromaNames, chromaF = stChromaFeatures(X, fs, nChroma, nFreqsPerChroma)\n        curFV[n_time_spectral_feats + n_mfcc_feats:\n              n_time_spectral_feats + n_mfcc_feats + n_chroma_feats - 1] = \\\n            chromaF\n        curFV[n_time_spectral_feats + n_mfcc_feats + n_chroma_feats - 1] = \\\n            chromaF.std()\n        st_features.append(curFV)\n        # delta features\n        '''\n        if count_fr>1:\n            delta = curFV - prevFV\n            curFVFinal = numpy.concatenate((curFV, delta))            \n        else:\n            curFVFinal = numpy.concatenate((curFV, curFV))\n        prevFV = curFV\n        st_features.append(curFVFinal)        \n        '''\n        # end of delta\n        X_prev = X.copy()\n\n    st_features = numpy.concatenate(st_features, 1)\n    return st_features, feature_names", "response": "This function extracts the features from a signal using the short - term windowing process."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef mtFeatureExtraction(signal, fs, mt_win, mt_step, st_win, st_step):\n\n    mt_win_ratio = int(round(mt_win / st_step))\n    mt_step_ratio = int(round(mt_step / st_step))\n\n    mt_features = []\n\n    st_features, f_names = stFeatureExtraction(signal, fs, st_win, st_step)\n    n_feats = len(st_features)\n    n_stats = 2\n\n    mt_features, mid_feature_names = [], []\n    #for i in range(n_stats * n_feats + 1):\n    for i in range(n_stats * n_feats):\n        mt_features.append([])\n        mid_feature_names.append(\"\")\n\n    for i in range(n_feats):        # for each of the short-term features:\n        cur_p = 0\n        N = len(st_features[i])\n        mid_feature_names[i] = f_names[i] + \"_\" + \"mean\"\n        mid_feature_names[i + n_feats] = f_names[i] + \"_\" + \"std\"\n\n        while (cur_p < N):\n            N1 = cur_p\n            N2 = cur_p + mt_win_ratio\n            if N2 > N:\n                N2 = N\n            cur_st_feats = st_features[i][N1:N2]\n\n            mt_features[i].append(numpy.mean(cur_st_feats))\n            mt_features[i + n_feats].append(numpy.std(cur_st_feats))\n            #mt_features[i+2*n_feats].append(numpy.std(cur_st_feats) / (numpy.mean(cur_st_feats)+0.00000010))\n            cur_p += mt_step_ratio\n    return numpy.array(mt_features), st_features, mid_feature_names", "response": "Mid - term feature extraction"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dirsWavFeatureExtraction(dirNames, mt_win, mt_step, st_win, st_step, compute_beat=False):\n    '''\n    Same as dirWavFeatureExtraction, but instead of a single dir it\n    takes a list of paths as input and returns a list of feature matrices.\n    EXAMPLE:\n    [features, classNames] =\n           a.dirsWavFeatureExtraction(['audioData/classSegmentsRec/noise','audioData/classSegmentsRec/speech',\n                                       'audioData/classSegmentsRec/brush-teeth','audioData/classSegmentsRec/shower'], 1, 1, 0.02, 0.02);\n\n    It can be used during the training process of a classification model ,\n    in order to get feature matrices from various audio classes (each stored in a seperate path)\n    '''\n\n    # feature extraction for each class:\n    features = []\n    classNames = []\n    fileNames = []\n    for i, d in enumerate(dirNames):\n        [f, fn, feature_names] = dirWavFeatureExtraction(d, mt_win, mt_step,\n                                                         st_win, st_step,\n                                                         compute_beat=compute_beat)\n        if f.shape[0] > 0:\n            # if at least one audio file has been found in the provided folder:\n            features.append(f)\n            fileNames.append(fn)\n            if d[-1] == os.sep:\n                classNames.append(d.split(os.sep)[-2])\n            else:\n                classNames.append(d.split(os.sep)[-1])\n    return features, classNames, fileNames", "response": "This function takes a list of files and directories and returns a list of feature matrices."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dirWavFeatureExtractionNoAveraging(dirName, mt_win, mt_step, st_win, st_step):\n\n    all_mt_feats = numpy.array([])\n    signal_idx = numpy.array([])\n    process_times = []\n\n    types = ('*.wav', '*.aif',  '*.aiff', '*.ogg')\n    wav_file_list = []\n    for files in types:\n        wav_file_list.extend(glob.glob(os.path.join(dirName, files)))\n\n    wav_file_list = sorted(wav_file_list)\n\n    for i, wavFile in enumerate(wav_file_list):\n        [fs, x] = audioBasicIO.readAudioFile(wavFile)\n        if isinstance(x, int):\n            continue        \n        \n        x = audioBasicIO.stereo2mono(x)\n        [mt_term_feats, _, _] = mtFeatureExtraction(x, fs, round(mt_win * fs),\n                                                    round(mt_step * fs),\n                                                    round(fs * st_win),\n                                                    round(fs * st_step))\n\n        mt_term_feats = numpy.transpose(mt_term_feats)\n        if len(all_mt_feats) == 0:                # append feature vector\n            all_mt_feats = mt_term_feats\n            signal_idx = numpy.zeros((mt_term_feats.shape[0], ))\n        else:\n            all_mt_feats = numpy.vstack((all_mt_feats, mt_term_feats))\n            signal_idx = numpy.append(signal_idx, i * numpy.ones((mt_term_feats.shape[0], )))\n\n    return (all_mt_feats, signal_idx, wav_file_list)", "response": "This function extracts the mid - term features of the WAVE files of a particular folder without averaging each file."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the market value of all positions", "response": "def market_value(self):\n        \"\"\"\n        [float] \u5e02\u503c\n        \"\"\"\n        return sum(position.market_value for position in six.itervalues(self._positions))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets total transaction cost", "response": "def transaction_cost(self):\n        \"\"\"\n        [float] \u603b\u8d39\u7528\n        \"\"\"\n        return sum(position.transaction_cost for position in six.itervalues(self._positions))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef buy_close(id_or_ins, amount, price=None, style=None, close_today=False):\n    position_effect = POSITION_EFFECT.CLOSE_TODAY if close_today else POSITION_EFFECT.CLOSE\n    return order(id_or_ins, amount, SIDE.BUY, position_effect, cal_style(price, style))", "response": "\u5e73\u5356\u4ed3\n\n    :param id_or_ins: \u4e0b\u5355\u6807\u7684\u7269\n    :type id_or_ins: :class:`~Instrument` object | `str` | List[:class:`~Instrument`] | List[`str`]\n\n    :param int amount: \u4e0b\u5355\u624b\u6570\n\n    :param float price: \u4e0b\u5355\u4ef7\u683c\uff0c\u9ed8\u8ba4\u4e3aNone\uff0c\u8868\u793a :class:`~MarketOrder`, \u6b64\u53c2\u6570\u4e3b\u8981\u7528\u4e8e\u7b80\u5316 `style` \u53c2\u6570\u3002\n\n    :param style: \u4e0b\u5355\u7c7b\u578b, \u9ed8\u8ba4\u662f\u5e02\u4ef7\u5355\u3002\u76ee\u524d\u652f\u6301\u7684\u8ba2\u5355\u7c7b\u578b\u6709 :class:`~LimitOrder` \u548c :class:`~MarketOrder`\n    :type style: `OrderStyle` object\n\n    :param bool close_today: \u662f\u5426\u6307\u5b9a\u53d1\u5e73\u4eca\u4ed3\u5355\uff0c\u9ed8\u8ba4\u4e3aFalse\uff0c\u53d1\u9001\u5e73\u4ed3\u5355\n\n    :return: :class:`~Order` object | list[:class:`~Order`] | None\n\n    :example:\n\n    .. code-block:: python\n\n        #\u5e02\u4ef7\u5355\u5c06\u73b0\u6709IF1603\u7a7a\u4ed3\u4e70\u5165\u5e73\u4ed32\u5f20\uff1a\n        buy_close('IF1603', 2)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sell_open(id_or_ins, amount, price=None, style=None):\n    return order(id_or_ins, amount, SIDE.SELL, POSITION_EFFECT.OPEN, cal_style(price, style))", "response": "Order to sell open."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sell_close(id_or_ins, amount, price=None, style=None, close_today=False):\n    position_effect = POSITION_EFFECT.CLOSE_TODAY if close_today else POSITION_EFFECT.CLOSE\n    return order(id_or_ins, amount, SIDE.SELL, position_effect, cal_style(price, style))", "response": "Order to sell close a market."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_future_contracts(underlying_symbol):\n    env = Environment.get_instance()\n    return env.data_proxy.get_future_contracts(underlying_symbol, env.trading_dt)", "response": "get_future_contracts - Get all future contracts"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting quantity of the order", "response": "def quantity(self):\n        \"\"\"\n        [int] \u8ba2\u5355\u6570\u91cf\n        \"\"\"\n        if np.isnan(self._quantity):\n            raise RuntimeError(\"Quantity of order {} is not supposed to be nan.\".format(self.order_id))\n        return self._quantity"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef filled_quantity(self):\n        if np.isnan(self._filled_quantity):\n            raise RuntimeError(\"Filled quantity of order {} is not supposed to be nan.\".format(self.order_id))\n        return self._filled_quantity", "response": "Returns the filled quantity of the order."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the frozen price of the order.", "response": "def frozen_price(self):\n        \"\"\"\n        [float] \u51bb\u7ed3\u4ef7\u683c\n        \"\"\"\n        if np.isnan(self._frozen_price):\n            raise RuntimeError(\"Frozen price of order {} is not supposed to be nan.\".format(self.order_id))\n        return self._frozen_price"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef value_percent(self):\n        accounts = Environment.get_instance().portfolio.accounts\n        if DEFAULT_ACCOUNT_TYPE.STOCK.name not in accounts:\n            return 0\n        total_value = accounts[DEFAULT_ACCOUNT_TYPE.STOCK.name].total_value\n        return 0 if total_value == 0 else self.market_value / total_value", "response": "get the value of the current asset"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns True if the order book is marked as de - listed.", "response": "def is_de_listed(self):\n        \"\"\"\n        \u5224\u65ad\u5408\u7ea6\u662f\u5426\u8fc7\u671f\n        \"\"\"\n        env = Environment.get_instance()\n        instrument = env.get_instrument(self._order_book_id)\n        current_date = env.trading_dt\n\n        if instrument.de_listed_date is not None:\n            if instrument.de_listed_date.date() > env.config.base.end_date:\n                return False\n            if current_date >= env.data_proxy.get_previous_trading_date(instrument.de_listed_date):\n                return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef bought_value(self):\n        user_system_log.warn(_(u\"[abandon] {} is no longer valid.\").format('stock_position.bought_value'))\n        return self._quantity * self._avg_price", "response": "get the bought value"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the price of the last trade", "response": "def trading_pnl(self):\n        \"\"\"\n        [float] \u4ea4\u6613\u76c8\u4e8f\uff0c\u7b56\u7565\u5728\u5f53\u524d\u4ea4\u6613\u65e5\u4ea7\u751f\u7684\u76c8\u4e8f\u4e2d\u6765\u6e90\u4e8e\u5f53\u65e5\u6210\u4ea4\u7684\u90e8\u5206\n        \"\"\"\n        last_price = self._data_proxy.get_last_price(self._order_book_id)\n        return self._contract_multiplier * (self._trade_quantity * last_price - self._trade_cost)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the PNL of the order book.", "response": "def position_pnl(self):\n        \"\"\"\n        [float] \u6628\u4ed3\u76c8\u4e8f\uff0c\u7b56\u7565\u5728\u5f53\u524d\u4ea4\u6613\u65e5\u4ea7\u751f\u7684\u76c8\u4e8f\u4e2d\u6765\u6e90\u4e8e\u6628\u4ed3\u7684\u90e8\u5206\n        \"\"\"\n        last_price = self._data_proxy.get_last_price(self._order_book_id)\n        if self._direction == POSITION_DIRECTION.LONG:\n            price_spread = last_price - self._last_price\n        else:\n            price_spread = self._last_price - last_price\n\n        return self._logical_old_quantity * self._contract_multiplier * price_spread"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nregisters the event bus.", "response": "def register_event(self):\n        \"\"\"\n        \u6ce8\u518c\u4e8b\u4ef6\n        \"\"\"\n        event_bus = Environment.get_instance().event_bus\n        event_bus.prepend_listener(EVENT.PRE_BEFORE_TRADING, self._pre_before_trading)\n        event_bus.prepend_listener(EVENT.POST_SETTLEMENT, self._post_settlement)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef unit_net_value(self):\n        if self._units == 0:\n            return np.nan\n        return self.total_value / self._units", "response": "Returns the unit net value of the current time series."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the daily returns of the current time series.", "response": "def daily_returns(self):\n        \"\"\"\n        [float] \u5f53\u524d\u6700\u65b0\u4e00\u5929\u7684\u65e5\u6536\u76ca\n        \"\"\"\n        if self._static_unit_net_value == 0:\n            return np.nan\n        return 0 if self._static_unit_net_value == 0 else self.unit_net_value / self._static_unit_net_value - 1"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget total value of all the items in the cache", "response": "def total_value(self):\n        \"\"\"\n        [float]\u603b\u6743\u76ca\n        \"\"\"\n        return sum(account.total_value for account in six.itervalues(self._accounts))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the set of mixed positions", "response": "def positions(self):\n        \"\"\"\n        [dict] \u6301\u4ed3\n        \"\"\"\n        if self._mixed_positions is None:\n            self._mixed_positions = MixedPositions(self._accounts)\n        return self._mixed_positions"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cash(self):\n        return sum(account.cash for account in six.itervalues(self._accounts))", "response": "get the cash of all the accounts"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the market value of all accounts", "response": "def market_value(self):\n        \"\"\"\n        [float] \u5e02\u503c\n        \"\"\"\n        return sum(account.market_value for account in six.itervalues(self._accounts))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets holding price in ppnl", "response": "def buy_holding_pnl(self):\n        \"\"\"\n        [float] \u4e70\u65b9\u5411\u5f53\u65e5\u6301\u4ed3\u76c8\u4e8f\n        \"\"\"\n        return (self.last_price - self.buy_avg_holding_price) * self.buy_quantity * self.contract_multiplier"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sell_holding_pnl(self):\n        return (self.sell_avg_holding_price - self.last_price) * self.sell_quantity * self.contract_multiplier", "response": "get sell holding price"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting buy price in ppnl", "response": "def buy_pnl(self):\n        \"\"\"\n        [float] \u4e70\u65b9\u5411\u7d2f\u8ba1\u76c8\u4e8f\n        \"\"\"\n        return (self.last_price - self._buy_avg_open_price) * self.buy_quantity * self.contract_multiplier"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget buy open order quantity", "response": "def buy_open_order_quantity(self):\n        \"\"\"\n        [int] \u4e70\u65b9\u5411\u6302\u5355\u91cf\n        \"\"\"\n        return sum(order.unfilled_quantity for order in self.open_orders if\n                   order.side == SIDE.BUY and order.position_effect == POSITION_EFFECT.OPEN)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting sell open order quantity", "response": "def sell_open_order_quantity(self):\n        \"\"\"\n        [int] \u5356\u65b9\u5411\u6302\u5355\u91cf\n        \"\"\"\n        return sum(order.unfilled_quantity for order in self.open_orders if\n                   order.side == SIDE.SELL and order.position_effect == POSITION_EFFECT.OPEN)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget buy close order quantity", "response": "def buy_close_order_quantity(self):\n        \"\"\"\n        [int] \u4e70\u65b9\u5411\u6302\u5355\u91cf\n        \"\"\"\n        return sum(order.unfilled_quantity for order in self.open_orders if order.side == SIDE.BUY and\n                   order.position_effect in [POSITION_EFFECT.CLOSE, POSITION_EFFECT.CLOSE_TODAY])"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef buy_avg_holding_price(self):\n        return 0 if self.buy_quantity == 0 else self._buy_holding_cost / self.buy_quantity / self.contract_multiplier", "response": "get avg holding price of buy"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the avg holding price of sell", "response": "def sell_avg_holding_price(self):\n        \"\"\"\n        [float] \u5356\u65b9\u5411\u6301\u4ed3\u5747\u4ef7\n        \"\"\"\n        return 0 if self.sell_quantity == 0 else self._sell_holding_cost / self.sell_quantity / self.contract_multiplier"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_de_listed(self):\n        instrument = Environment.get_instance().get_instrument(self._order_book_id)\n        current_date = Environment.get_instance().trading_dt\n        if instrument.de_listed_date is not None and current_date >= instrument.de_listed_date:\n            return True\n        return False", "response": "Returns True if the order book is marked as de - listed."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\napply a trade to the current state.", "response": "def apply_trade(self, trade):\n        \"\"\"\n        \u5e94\u7528\u6210\u4ea4\uff0c\u5e76\u8ba1\u7b97\u4ea4\u6613\u4ea7\u751f\u7684\u73b0\u91d1\u53d8\u52a8\u3002\n\n        \u5f00\u4ed3\uff1a\n        delta_cash\n        = -1 * margin\n        = -1 * quantity * contract_multiplier * price * margin_rate\n\n        \u5e73\u4ed3\uff1a\n        delta_cash\n        = old_margin - margin + delta_realized_pnl\n        = (sum of (cost_price * quantity) of closed trade) * contract_multiplier * margin_rate + delta_realized_pnl\n\n        :param trade: rqalpha.model.trade.Trade\n        :return: float\n        \"\"\"\n        # close_trade: delta_cash = old_margin - margin + delta_realized_pnl\n        trade_quantity = trade.last_quantity\n        if trade.side == SIDE.BUY:\n            if trade.position_effect == POSITION_EFFECT.OPEN:\n                self._buy_avg_open_price = (self._buy_avg_open_price * self.buy_quantity +\n                                            trade_quantity * trade.last_price) / (self.buy_quantity + trade_quantity)\n                self._buy_transaction_cost += trade.transaction_cost\n                self._buy_today_holding_list.insert(0, (trade.last_price, trade_quantity))\n                return -1 * self._margin_of(trade_quantity, trade.last_price)\n            else:\n                old_margin = self.margin\n                self._sell_transaction_cost += trade.transaction_cost\n                delta_realized_pnl = self._close_holding(trade)\n                self._sell_realized_pnl += delta_realized_pnl\n                return old_margin - self.margin + delta_realized_pnl\n        else:\n            if trade.position_effect == POSITION_EFFECT.OPEN:\n                self._sell_avg_open_price = (self._sell_avg_open_price * self.sell_quantity +\n                                             trade_quantity * trade.last_price) / (self.sell_quantity + trade_quantity)\n                self._sell_transaction_cost += trade.transaction_cost\n                self._sell_today_holding_list.insert(0, (trade.last_price, trade_quantity))\n                return -1 * self._margin_of(trade_quantity, trade.last_price)\n            else:\n                old_margin = self.margin\n                self._buy_transaction_cost += trade.transaction_cost\n                delta_realized_pnl = self._close_holding(trade)\n                self._buy_realized_pnl += delta_realized_pnl\n                return old_margin - self.margin + delta_realized_pnl"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _close_holding(self, trade):\n        left_quantity = trade.last_quantity\n        delta = 0\n        if trade.side == SIDE.BUY:\n            # \u5148\u5e73\u6628\u4ed3\n            if trade.position_effect == POSITION_EFFECT.CLOSE and len(self._sell_old_holding_list) != 0:\n                old_price, old_quantity = self._sell_old_holding_list.pop()\n\n                if old_quantity > left_quantity:\n                    consumed_quantity = left_quantity\n                    self._sell_old_holding_list = [(old_price, old_quantity - left_quantity)]\n                else:\n                    consumed_quantity = old_quantity\n                left_quantity -= consumed_quantity\n                delta += self._cal_realized_pnl(old_price, trade.last_price, trade.side, consumed_quantity)\n            # \u518d\u5e73\u4eca\u4ed3\n            while True:\n                if left_quantity <= 0:\n                    break\n                oldest_price, oldest_quantity = self._sell_today_holding_list.pop()\n                if oldest_quantity > left_quantity:\n                    consumed_quantity = left_quantity\n                    self._sell_today_holding_list.append((oldest_price, oldest_quantity - left_quantity))\n                else:\n                    consumed_quantity = oldest_quantity\n                left_quantity -= consumed_quantity\n                delta += self._cal_realized_pnl(oldest_price, trade.last_price, trade.side, consumed_quantity)\n        else:\n            # \u5148\u5e73\u6628\u4ed3\n            if trade.position_effect == POSITION_EFFECT.CLOSE and len(self._buy_old_holding_list) != 0:\n                old_price, old_quantity = self._buy_old_holding_list.pop()\n                if old_quantity > left_quantity:\n                    consumed_quantity = left_quantity\n                    self._buy_old_holding_list = [(old_price, old_quantity - left_quantity)]\n                else:\n                    consumed_quantity = old_quantity\n                left_quantity -= consumed_quantity\n                delta += self._cal_realized_pnl(old_price, trade.last_price, trade.side, consumed_quantity)\n            # \u518d\u5e73\u4eca\u4ed3\n            while True:\n                if left_quantity <= 0:\n                    break\n                oldest_price, oldest_quantity = self._buy_today_holding_list.pop()\n                if oldest_quantity > left_quantity:\n                    consumed_quantity = left_quantity\n                    self._buy_today_holding_list.append((oldest_price, oldest_quantity - left_quantity))\n                    left_quantity = 0\n                else:\n                    consumed_quantity = oldest_quantity\n                left_quantity -= consumed_quantity\n                delta += self._cal_realized_pnl(oldest_price, trade.last_price, trade.side, consumed_quantity)\n        return delta", "response": "Private method that returns the amount of close holding trades."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sector_code(self):\n        try:\n            return self.__dict__[\"sector_code\"]\n        except (KeyError, ValueError):\n            raise AttributeError(\n                \"Instrument(order_book_id={}) has no attribute 'sector_code' \".format(self.order_book_id)\n            )", "response": "get the sector code of the instrument"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the name of the sector code", "response": "def sector_code_name(self):\n        \"\"\"\n        [str] \u4ee5\u5f53\u5730\u8bed\u8a00\u4e3a\u6807\u51c6\u7684\u677f\u5757\u4ee3\u7801\u540d\uff08\u80a1\u7968\u4e13\u7528\uff09\n        \"\"\"\n        try:\n            return self.__dict__[\"sector_code_name\"]\n        except (KeyError, ValueError):\n            raise AttributeError(\n                \"Instrument(order_book_id={}) has no attribute 'sector_code_name' \".format(self.order_book_id)\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef industry_code(self):\n        try:\n            return self.__dict__[\"industry_code\"]\n        except (KeyError, ValueError):\n            raise AttributeError(\n                \"Instrument(order_book_id={}) has no attribute 'industry_code' \".format(self.order_book_id)\n            )", "response": "Return the Industry code for this instrument."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef industry_name(self):\n        try:\n            return self.__dict__[\"industry_name\"]\n        except (KeyError, ValueError):\n            raise AttributeError(\n                \"Instrument(order_book_id={}) has no attribute 'industry_name' \".format(self.order_book_id)\n            )", "response": "return industry_name of this instrument"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef concept_names(self):\n        try:\n            return self.__dict__[\"concept_names\"]\n        except (KeyError, ValueError):\n            raise AttributeError(\n                \"Instrument(order_book_id={}) has no attribute 'concept_names' \".format(self.order_book_id)\n            )", "response": "return a list of concept names"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef board_type(self):\n        try:\n            return self.__dict__[\"board_type\"]\n        except (KeyError, ValueError):\n            raise AttributeError(\n                \"Instrument(order_book_id={}) has no attribute 'board_type' \".format(self.order_book_id)\n            )", "response": "Returns the board type of the instrument."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the status of the instrument.", "response": "def status(self):\n        \"\"\"\n        [str] \u5408\u7ea6\u72b6\u6001\u3002\u2019Active\u2019 - \u6b63\u5e38\u4e0a\u5e02, \u2018Delisted\u2019 - \u7ec8\u6b62\u4e0a\u5e02, \u2018TemporarySuspended\u2019 - \u6682\u505c\u4e0a\u5e02,\n        \u2018PreIPO\u2019 - \u53d1\u884c\u914d\u552e\u671f\u95f4, \u2018FailIPO\u2019 - \u53d1\u884c\u5931\u8d25\uff08\u80a1\u7968\u4e13\u7528\uff09\n        \"\"\"\n        try:\n            return self.__dict__[\"status\"]\n        except (KeyError, ValueError):\n            raise AttributeError(\n                \"Instrument(order_book_id={}) has no attribute 'status' \".format(self.order_book_id)\n            )"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the special type of the instrument.", "response": "def special_type(self):\n        \"\"\"\n        [str] \u7279\u522b\u5904\u7406\u72b6\u6001\u3002\u2019Normal\u2019 - \u6b63\u5e38\u4e0a\u5e02, \u2018ST\u2019 - ST\u5904\u7406, \u2018StarST\u2019 - *ST\u4ee3\u8868\u8be5\u80a1\u7968\u6b63\u5728\u63a5\u53d7\u9000\u5e02\u8b66\u544a,\n        \u2018PT\u2019 - \u4ee3\u8868\u8be5\u80a1\u7968\u8fde\u7eed3\u5e74\u6536\u5165\u4e3a\u8d1f\uff0c\u5c06\u88ab\u6682\u505c\u4ea4\u6613, \u2018Other\u2019 - \u5176\u4ed6\uff08\u80a1\u7968\u4e13\u7528\uff09\n        \"\"\"\n        try:\n            return self.__dict__[\"special_type\"]\n        except (KeyError, ValueError):\n            raise AttributeError(\n                \"Instrument(order_book_id={}) has no attribute 'special_type' \".format(self.order_book_id)\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef contract_multiplier(self):\n        try:\n            return self.__dict__[\"contract_multiplier\"]\n        except (KeyError, ValueError):\n            raise AttributeError(\n                \"Instrument(order_book_id={}) has no attribute 'contract_multiplier' \".format(self.order_book_id)\n            )", "response": "get the contract multiplier"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the margin rate of the instrument", "response": "def margin_rate(self):\n        \"\"\"\n        [float] \u5408\u7ea6\u6700\u4f4e\u4fdd\u8bc1\u91d1\u7387\uff08\u671f\u8d27\u4e13\u7528\uff09\n        \"\"\"\n        try:\n            return self.__dict__[\"margin_rate\"]\n        except (KeyError, ValueError):\n            raise AttributeError(\n                \"Instrument(order_book_id={}) has no attribute 'margin_rate' \".format(self.order_book_id)\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef underlying_order_book_id(self):\n        try:\n            return self.__dict__[\"underlying_order_book_id\"]\n        except (KeyError, ValueError):\n            raise AttributeError(\n                \"Instrument(order_book_id={}) has no attribute 'underlying_order_book_id' \".format(self.order_book_id)\n            )", "response": "Return the order book ID of the current instance."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef underlying_symbol(self):\n        try:\n            return self.__dict__[\"underlying_symbol\"]\n        except (KeyError, ValueError):\n            raise AttributeError(\n                \"Instrument(order_book_id={}) has no attribute 'underlying_symbol' \".format(self.order_book_id)\n            )", "response": "Return the underlying symbol of the instrument."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef maturity_date(self):\n        try:\n            return self.__dict__[\"maturity_date\"]\n        except (KeyError, ValueError):\n            raise AttributeError(\n                \"Instrument(order_book_id={}) has no attribute 'maturity_date' \".format(self.order_book_id)\n            )", "response": "Return the date of the instrument that maturity was performed on."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef settlement_method(self):\n        try:\n            return self.__dict__[\"settlement_method\"]\n        except (KeyError, ValueError):\n            raise AttributeError(\n                \"Instrument(order_book_id={}) has no attribute 'settlement_method' \".format(self.order_book_id)\n            )", "response": "Return the settlement method of the current instrument."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef listing(self):\n\n        now = Environment.get_instance().calendar_dt\n        return self.listed_date <= now <= self.de_listed_date", "response": "Returns True if the user is listing the user s entries."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_trade_commission(self, trade):\n        order_id = trade.order_id\n        if self.env.data_proxy.instruments(trade.order_book_id).type == 'PublicFund':\n            return self._get_public_fund_commission(trade.order_book_id, trade.side, trade.last_price * trade.last_quantity)\n        commission = self.commission_map[order_id]\n        cost_commission = trade.last_price * trade.last_quantity * self.commission_rate * self.commission_multiplier\n        if cost_commission > commission:\n            if commission == self.min_commission:\n                self.commission_map[order_id] = 0\n                return cost_commission\n            else:\n                self.commission_map[order_id] = 0\n                return cost_commission - commission\n        else:\n            if commission == self.min_commission:\n                self.commission_map[order_id] -= cost_commission\n                return commission\n            else:\n                self.commission_map[order_id] -= cost_commission\n                return 0", "response": "get_trade_commission - Returns the commission of the given trade"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_tax(self, order_book_id, _, cost_money):\n        instrument = Environment.get_instance().get_instrument(order_book_id)\n        if instrument.type != 'CS':\n            return 0\n        tax = cost_money * self.tax_rate\n        if tax < 1:\n            tax = 1\n        else:\n            tax = round(tax, 2)\n        return tax + 5", "response": "Get the tax for a given orderbook."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _bar_status(self):\n        if self.isnan or np.isnan(self.limit_up):\n            return BAR_STATUS.ERROR\n        if self.close >= self.limit_up:\n            return BAR_STATUS.LIMIT_UP\n        if self.close <= self.limit_down:\n            return BAR_STATUS.LIMIT_DOWN\n        return BAR_STATUS.NORMAL", "response": "WARNING: \u83b7\u53d6 bar_status \u6bd4\u8f83\u8017\u8d39\u6027\u80fd\uff0c\u800c\u4e14\u662flazy_compute\uff0c\u56e0\u6b64\u4e0d\u8981\u591a\u6b21\u8c03\u7528\uff01\uff01\uff01\uff01"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef prev_settlement(self):\n        try:\n            return self._data['prev_settlement']\n        except (ValueError, KeyError):\n            pass\n\n        if self._prev_settlement is None:\n            trading_dt = Environment.get_instance().trading_dt\n            data_proxy = Environment.get_instance().data_proxy\n            self._prev_settlement = data_proxy.get_prev_settlement(self._instrument.order_book_id, trading_dt)\n        return self._prev_settlement", "response": "get the previous settlement"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef submit_order(id_or_ins, amount, side, price=None, position_effect=None):\n    order_book_id = assure_order_book_id(id_or_ins)\n    env = Environment.get_instance()\n    if (\n        env.config.base.run_type != RUN_TYPE.BACKTEST\n        and env.get_instrument(order_book_id).type == \"Future\"\n    ):\n        if \"88\" in order_book_id:\n            raise RQInvalidArgument(\n                _(u\"Main Future contracts[88] are not supported in paper trading.\")\n            )\n        if \"99\" in order_book_id:\n            raise RQInvalidArgument(\n                _(u\"Index Future contracts[99] are not supported in paper trading.\")\n            )\n    style = cal_style(price, None)\n    market_price = env.get_last_price(order_book_id)\n    if not is_valid_price(market_price):\n        user_system_log.warn(\n            _(u\"Order Creation Failed: [{order_book_id}] No market data\").format(\n                order_book_id=order_book_id\n            )\n        )\n        return\n\n    amount = int(amount)\n\n    order = Order.__from_create__(\n        order_book_id=order_book_id,\n        quantity=amount,\n        side=side,\n        style=style,\n        position_effect=position_effect,\n    )\n\n    if order.type == ORDER_TYPE.MARKET:\n        order.set_frozen_price(market_price)\n    if env.can_submit_order(order):\n        env.broker.submit_order(order)\n        return order", "response": "Submit order to the order_list."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncanceling order if it is available", "response": "def cancel_order(order):\n    \"\"\"\n    \u64a4\u5355\n\n    :param order: \u9700\u8981\u64a4\u9500\u7684order\u5bf9\u8c61\n    :type order: :class:`~Order` object\n    \"\"\"\n    env = Environment.get_instance()\n    if env.can_cancel_order(order):\n        env.broker.cancel_order(order)\n    return order"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update_universe(id_or_symbols):\n    if isinstance(id_or_symbols, (six.string_types, Instrument)):\n        id_or_symbols = [id_or_symbols]\n    order_book_ids = set(\n        assure_order_book_id(order_book_id) for order_book_id in id_or_symbols\n    )\n    if order_book_ids != Environment.get_instance().get_universe():\n        Environment.get_instance().update_universe(order_book_ids)", "response": "Update the universe of the given id or symbols."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef subscribe(id_or_symbols):\n    current_universe = Environment.get_instance().get_universe()\n    if isinstance(id_or_symbols, six.string_types):\n        order_book_id = instruments(id_or_symbols).order_book_id\n        current_universe.add(order_book_id)\n    elif isinstance(id_or_symbols, Instrument):\n        current_universe.add(id_or_symbols.order_book_id)\n    elif isinstance(id_or_symbols, Iterable):\n        for item in id_or_symbols:\n            current_universe.add(assure_order_book_id(item))\n    else:\n        raise RQInvalidArgument(_(u\"unsupported order_book_id type\"))\n    verify_that(\"id_or_symbols\")._are_valid_instruments(\"subscribe\", id_or_symbols)\n    Environment.get_instance().update_universe(current_universe)", "response": "subscribe to an order_book"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef unsubscribe(id_or_symbols):\n    current_universe = Environment.get_instance().get_universe()\n    if isinstance(id_or_symbols, six.string_types):\n        order_book_id = instruments(id_or_symbols).order_book_id\n        current_universe.discard(order_book_id)\n    elif isinstance(id_or_symbols, Instrument):\n        current_universe.discard(id_or_symbols.order_book_id)\n    elif isinstance(id_or_symbols, Iterable):\n        for item in id_or_symbols:\n            i = assure_order_book_id(item)\n            current_universe.discard(i)\n    else:\n        raise RQInvalidArgument(_(u\"unsupported order_book_id type\"))\n\n    Environment.get_instance().update_universe(current_universe)", "response": "Unsubscribe from an order_book."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef history_bars(\n    order_book_id,\n    bar_count,\n    frequency,\n    fields=None,\n    skip_suspended=True,\n    include_now=False,\n    adjust_type=\"pre\",\n):\n    \"\"\"\n    \u83b7\u53d6\u6307\u5b9a\u5408\u7ea6\u7684\u5386\u53f2\u884c\u60c5\uff0c\u540c\u65f6\u652f\u6301\u65e5\u4ee5\u53ca\u5206\u949f\u5386\u53f2\u6570\u636e\u3002\u4e0d\u80fd\u5728init\u4e2d\u8c03\u7528\u3002 \u6ce8\u610f\uff0c\u8be5API\u4f1a\u81ea\u52a8\u8df3\u8fc7\u505c\u724c\u6570\u636e\u3002\n\n    \u65e5\u56de\u6d4b\u83b7\u53d6\u5206\u949f\u5386\u53f2\u6570\u636e\uff1a\u4e0d\u652f\u6301\n\n    \u65e5\u56de\u6d4b\u83b7\u53d6\u65e5\u5386\u53f2\u6570\u636e\n\n    =========================   ===================================================\n    \u8c03\u7528\u65f6\u95f4                      \u8fd4\u56de\u6570\u636e\n    =========================   ===================================================\n    T\u65e5before_trading            T-1\u65e5day bar\n    T\u65e5handle_bar                T\u65e5day bar\n    =========================   ===================================================\n\n    \u5206\u949f\u56de\u6d4b\u83b7\u53d6\u65e5\u5386\u53f2\u6570\u636e\n\n    =========================   ===================================================\n    \u8c03\u7528\u65f6\u95f4                      \u8fd4\u56de\u6570\u636e\n    =========================   ===================================================\n    T\u65e5before_trading            T-1\u65e5day bar\n    T\u65e5handle_bar                T-1\u65e5day bar\n    =========================   ===================================================\n\n    \u5206\u949f\u56de\u6d4b\u83b7\u53d6\u5206\u949f\u5386\u53f2\u6570\u636e\n\n    =========================   ===================================================\n    \u8c03\u7528\u65f6\u95f4                      \u8fd4\u56de\u6570\u636e\n    =========================   ===================================================\n    T\u65e5before_trading            T-1\u65e5\u6700\u540e\u4e00\u4e2aminute bar\n    T\u65e5handle_bar                T\u65e5\u5f53\u524dminute bar\n    =========================   ===================================================\n\n    :param order_book_id: \u5408\u7ea6\u4ee3\u7801\n    :type order_book_id: `str`\n\n    :param int bar_count: \u83b7\u53d6\u7684\u5386\u53f2\u6570\u636e\u6570\u91cf\uff0c\u5fc5\u586b\u9879\n    :param str frequency: \u83b7\u53d6\u6570\u636e\u4ec0\u4e48\u6837\u7684\u9891\u7387\u8fdb\u884c\u3002'1d'\u6216'1m'\u5206\u522b\u8868\u793a\u6bcf\u65e5\u548c\u6bcf\u5206\u949f\uff0c\u5fc5\u586b\u9879\n    :param str fields: \u8fd4\u56de\u6570\u636e\u5b57\u6bb5\u3002\u5fc5\u586b\u9879\u3002\u89c1\u4e0b\u65b9\u5217\u8868\u3002\n\n    =========================   ===================================================\n    fields                      \u5b57\u6bb5\u540d\n    =========================   ===================================================\n    datetime                    \u65f6\u95f4\u6233\n    open                        \u5f00\u76d8\u4ef7\n    high                        \u6700\u9ad8\u4ef7\n    low                         \u6700\u4f4e\u4ef7\n    close                       \u6536\u76d8\u4ef7\n    volume                      \u6210\u4ea4\u91cf\n    total_turnover              \u6210\u4ea4\u989d\n    open_interest               \u6301\u4ed3\u91cf\uff08\u671f\u8d27\u4e13\u7528\uff09\n    basis_spread                \u671f\u73b0\u5dee\uff08\u80a1\u6307\u671f\u8d27\u4e13\u7528\uff09\n    settlement                  \u7ed3\u7b97\u4ef7\uff08\u671f\u8d27\u65e5\u7ebf\u4e13\u7528\uff09\n    prev_settlement             \u7ed3\u7b97\u4ef7\uff08\u671f\u8d27\u65e5\u7ebf\u4e13\u7528\uff09\n    =========================   ===================================================\n\n    :param bool skip_suspended: \u662f\u5426\u8df3\u8fc7\u505c\u724c\u6570\u636e\n    :param bool include_now: \u662f\u5426\u5305\u542b\u5f53\u524d\u6570\u636e\n    :param str adjust_type: \u590d\u6743\u7c7b\u578b\uff0c\u9ed8\u8ba4\u4e3a\u524d\u590d\u6743 pre\uff1b\u53ef\u9009 pre, none, post\n\n    :return: `ndarray`, \u65b9\u4fbf\u76f4\u63a5\u4e0etalib\u7b49\u8ba1\u7b97\u5e93\u5bf9\u63a5\uff0c\u6548\u7387\u8f83history\u8fd4\u56de\u7684DataFrame\u66f4\u9ad8\u3002\n\n    :example:\n\n    \u83b7\u53d6\u6700\u8fd15\u5929\u7684\u65e5\u7ebf\u6536\u76d8\u4ef7\u5e8f\u5217\uff08\u7b56\u7565\u5f53\u524d\u65e5\u671f\u4e3a20160706\uff09:\n\n    ..  code-block:: python3\n        :linenos:\n\n        [In]\n        logger.info(history_bars('000002.XSHE', 5, '1d', 'close'))\n        [Out]\n        [ 8.69  8.7   8.71  8.81  8.81]\n    \"\"\"\n    order_book_id = assure_order_book_id(order_book_id)\n    env = Environment.get_instance()\n    dt = env.calendar_dt\n\n    if frequency[-1] not in {\"m\", \"d\"}:\n        raise RQInvalidArgument(\"invalid frequency {}\".format(frequency))\n\n    if frequency[-1] == \"m\" and env.config.base.frequency == \"1d\":\n        raise RQInvalidArgument(\"can not get minute history in day back test\")\n\n    if frequency[-1] == \"d\" and frequency != \"1d\":\n        raise RQInvalidArgument(\"invalid frequency\")\n\n    if adjust_type not in {\"pre\", \"post\", \"none\"}:\n        raise RuntimeError(\"invalid adjust_type\")\n\n    if frequency == \"1d\":\n        sys_frequency = Environment.get_instance().config.base.frequency\n        if (\n            sys_frequency in [\"1m\", \"tick\"]\n            and not include_now\n            and ExecutionContext.phase() != EXECUTION_PHASE.AFTER_TRADING\n        ) or (ExecutionContext.phase() == EXECUTION_PHASE.BEFORE_TRADING):\n            dt = env.data_proxy.get_previous_trading_date(env.trading_dt.date())\n            # \u5f53 EXECUTION_PHASE.BEFORE_TRADING \u7684\u65f6\u5019\uff0c\u5f3a\u5236 include_now \u4e3a False\n            include_now = False\n        if sys_frequency == \"1d\":\n            # \u65e5\u56de\u6d4b\u4e0d\u652f\u6301 include_now\n            include_now = False\n\n    if fields is None:\n        fields = [\"datetime\", \"open\", \"high\", \"low\", \"close\", \"volume\"]\n\n    return env.data_proxy.history_bars(\n        order_book_id,\n        bar_count,\n        frequency,\n        fields,\n        dt,\n        skip_suspended=skip_suspended,\n        include_now=include_now,\n        adjust_type=adjust_type,\n        adjust_orig=env.trading_dt,\n    )", "response": "A history bar is a sequence of two - dimensional lists."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef all_instruments(type=None, date=None):\n    env = Environment.get_instance()\n    if date is None:\n        dt = env.trading_dt\n    else:\n        dt = pd.Timestamp(date).to_pydatetime()\n        dt = min(dt, env.trading_dt)\n\n    if type is not None:\n        if isinstance(type, six.string_types):\n            type = [type]\n\n        types = set()\n        for t in type:\n            if t == \"Stock\":\n                types.add(\"CS\")\n            elif t == \"Fund\":\n                types.update([\"ETF\", \"LOF\", \"SF\", \"FenjiA\", \"FenjiB\", \"FenjiMu\"])\n            else:\n                types.add(t)\n    else:\n        types = None\n\n    result = env.data_proxy.all_instruments(types, dt)\n    if types is not None and len(types) == 1:\n        return pd.DataFrame([i.__dict__ for i in result])\n\n    return pd.DataFrame(\n        [\n            [i.order_book_id, i.symbol, i.type, i.listed_date, i.de_listed_date]\n            for i in result\n        ],\n        columns=[\"order_book_id\", \"symbol\", \"type\", \"listed_date\", \"de_listed_date\"],\n    )", "response": "\u83b7\u53d6\u67d0\u4e2a\u56fd\u5bb6\u5e02\u573a\u7684\u6240\u6709\u5408\u7ea6\u4fe1\u606f\u3002\u4f7f\u7528\u8005\u53ef\u4ee5\u901a\u8fc7\u8fd9\u4e00\u65b9\u6cd5\u5f88\u5feb\u5730\u5bf9\u5408\u7ea6\u4fe1\u606f\u6709\u4e00\u4e2a\u5feb\u901f\u4e86\u89e3\uff0c\u76ee\u524d\u4ec5\u652f\u6301\u4e2d\u56fd\u5e02\u573a\u3002\n\n    :param str type: \u9700\u8981\u67e5\u8be2\u5408\u7ea6\u7c7b\u578b\uff0c\u4f8b\u5982\uff1atype='CS'\u4ee3\u8868\u80a1\u7968\u3002\u9ed8\u8ba4\u662f\u6240\u6709\u7c7b\u578b\n\n    :param date: \u67e5\u8be2\u65f6\u95f4\u70b9\n    :type date: `str` | `datetime` | `date`\n\n\n    :return: `pandas DataFrame` \u6240\u6709\u5408\u7ea6\u7684\u57fa\u672c\u4fe1\u606f\u3002\n\n    \u5176\u4e2dtype\u53c2\u6570\u4f20\u5165\u7684\u5408\u7ea6\u7c7b\u578b\u548c\u5bf9\u5e94\u7684\u89e3\u91ca\u5982\u4e0b\uff1a\n\n    =========================   ===================================================\n    \u5408\u7ea6\u7c7b\u578b                      \u8bf4\u660e\n    =========================   ===================================================\n    CS                          Common Stock, \u5373\u80a1\u7968\n    ETF                         Exchange Traded Fund, \u5373\u4ea4\u6613\u6240\u4ea4\u6613\u57fa\u91d1\n    LOF                         Listed Open-Ended Fund\uff0c\u5373\u4e0a\u5e02\u578b\u5f00\u653e\u5f0f\u57fa\u91d1\n    FenjiMu                     Fenji Mu Fund, \u5373\u5206\u7ea7\u6bcd\u57fa\u91d1\n    FenjiA                      Fenji A Fund, \u5373\u5206\u7ea7A\u7c7b\u57fa\u91d1\n    FenjiB                      Fenji B Funds, \u5373\u5206\u7ea7B\u7c7b\u57fa\u91d1\n    INDX                        Index, \u5373\u6307\u6570\n    Future                      Futures\uff0c\u5373\u671f\u8d27\uff0c\u5305\u542b\u80a1\u6307\u3001\u56fd\u503a\u548c\u5546\u54c1\u671f\u8d27\n    =========================   ===================================================\n\n    :example:\n\n    \u83b7\u53d6\u4e2d\u56fd\u5e02\u573a\u6240\u6709\u5206\u7ea7\u57fa\u91d1\u7684\u57fa\u7840\u4fe1\u606f:\n\n    ..  code-block:: python3\n        :linenos:\n\n        [In]all_instruments('FenjiA')\n        [Out]\n            abbrev_symbol    order_book_id    product    sector_code  symbol\n        0    CYGA    150303.XSHE    null    null    \u534e\u5b89\u521b\u4e1a\u677f50A\n        1    JY500A    150088.XSHE    null    null    \u91d1\u9e70500A\n        2    TD500A    150053.XSHE    null    null    \u6cf0\u8fbe\u7a33\u5065\n        3    HS500A    150110.XSHE    null    null    \u534e\u5546500A\n        4    QSAJ    150235.XSHE    null    null    \u9e4f\u534e\u8bc1\u5238A\n        ..."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef current_snapshot(id_or_symbol):\n    env = Environment.get_instance()\n    frequency = env.config.base.frequency\n    order_book_id = assure_order_book_id(id_or_symbol)\n\n    dt = env.calendar_dt\n\n    if env.config.base.run_type == RUN_TYPE.BACKTEST:\n        if ExecutionContext.phase() == EXECUTION_PHASE.BEFORE_TRADING:\n            dt = env.data_proxy.get_previous_trading_date(env.trading_dt.date())\n            return env.data_proxy.current_snapshot(order_book_id, \"1d\", dt)\n        elif ExecutionContext.phase() == EXECUTION_PHASE.AFTER_TRADING:\n            return env.data_proxy.current_snapshot(order_book_id, \"1d\", dt)\n\n    # PT\u3001\u5b9e\u76d8\u76f4\u63a5\u53d6\u6700\u65b0\u5feb\u7167\uff0c\u5ffd\u7565 frequency, dt \u53c2\u6570\n    return env.data_proxy.current_snapshot(order_book_id, frequency, dt)", "response": "Return the current snapshot of the given id or symbol."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dividend_receivable(self):\n        return sum(d['quantity'] * d['dividend_per_share'] for d in six.itervalues(self._dividend_receivable))", "response": "get the sum of the dividend receivable"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef history_bars(self, instrument, bar_count, frequency, fields, dt, skip_suspended=True,\n                     include_now=False, adjust_type='pre', adjust_orig=None):\n        \"\"\"\n        \u83b7\u53d6\u5386\u53f2\u6570\u636e\n\n        :param instrument: \u5408\u7ea6\u5bf9\u8c61\n        :type instrument: :class:`~Instrument`\n\n        :param int bar_count: \u83b7\u53d6\u7684\u5386\u53f2\u6570\u636e\u6570\u91cf\n        :param str frequency: \u5468\u671f\u9891\u7387\uff0c`1d` \u8868\u793a\u65e5\u5468\u671f, `1m` \u8868\u793a\u5206\u949f\u5468\u671f\n        :param str fields: \u8fd4\u56de\u6570\u636e\u5b57\u6bb5\n\n        =========================   ===================================================\n        fields                      \u5b57\u6bb5\u540d\n        =========================   ===================================================\n        datetime                    \u65f6\u95f4\u6233\n        open                        \u5f00\u76d8\u4ef7\n        high                        \u6700\u9ad8\u4ef7\n        low                         \u6700\u4f4e\u4ef7\n        close                       \u6536\u76d8\u4ef7\n        volume                      \u6210\u4ea4\u91cf\n        total_turnover              \u6210\u4ea4\u989d\n        datetime                    int\u7c7b\u578b\u65f6\u95f4\u6233\n        open_interest               \u6301\u4ed3\u91cf\uff08\u671f\u8d27\u4e13\u7528\uff09\n        basis_spread                \u671f\u73b0\u5dee\uff08\u80a1\u6307\u671f\u8d27\u4e13\u7528\uff09\n        settlement                  \u7ed3\u7b97\u4ef7\uff08\u671f\u8d27\u65e5\u7ebf\u4e13\u7528\uff09\n        prev_settlement             \u7ed3\u7b97\u4ef7\uff08\u671f\u8d27\u65e5\u7ebf\u4e13\u7528\uff09\n        =========================   ===================================================\n\n        :param datetime.datetime dt: \u65f6\u95f4\n        :param bool skip_suspended: \u662f\u5426\u8df3\u8fc7\u505c\u724c\u65e5\n        :param bool include_now: \u662f\u5426\u5305\u542b\u5f53\u5929\u6700\u65b0\u6570\u636e\n        :param str adjust_type: \u590d\u6743\u7c7b\u578b\uff0c'pre', 'none', 'post'\n        :param datetime.datetime adjust_orig: \u590d\u6743\u8d77\u70b9\uff1b\n\n        :return: `numpy.ndarray`\n\n        \"\"\"\n        raise NotImplementedError", "response": "This method returns a list of bar_count bar_count_close and bar_count_prev."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef order_lots(id_or_ins, amount, price=None, style=None):\n    order_book_id = assure_stock_order_book_id(id_or_ins)\n\n    round_lot = int(Environment.get_instance().get_instrument(order_book_id).round_lot)\n\n    style = cal_style(price, style)\n\n    return order_shares(id_or_ins, amount * round_lot, style=style)", "response": "\u6307\u5b9a\u624b\u6570\u53d1\u9001\u4e70/\u5356\u5355\u3002\u5982\u6709\u9700\u8981\u843d\u5355\u7c7b\u578b\u5f53\u505a\u4e00\u4e2a\u53c2\u91cf\u4f20\u5165\uff0c\u5982\u679c\u5ffd\u7565\u6389\u843d\u5355\u7c7b\u578b\uff0c\u90a3\u4e48\u9ed8\u8ba4\u662f\u5e02\u4ef7\u5355\uff08market order\uff09\u3002\n\n    :param id_or_ins: \u4e0b\u5355\u6807\u7684\u7269\n    :type id_or_ins: :class:`~Instrument` object | `str`\n\n    :param int amount: \u4e0b\u5355\u91cf, \u6b63\u6570\u4ee3\u8868\u4e70\u5165\uff0c\u8d1f\u6570\u4ee3\u8868\u5356\u51fa\u3002\u5c06\u4f1a\u6839\u636e\u4e00\u624bxx\u80a1\u6765\u5411\u4e0b\u8c03\u6574\u5230\u4e00\u624b\u7684\u500d\u6570\uff0c\u6bd4\u5982\u4e2d\u56fdA\u80a1\u5c31\u662f\u8c03\u6574\u6210100\u80a1\u7684\u500d\u6570\u3002\n\n    :param float price: \u4e0b\u5355\u4ef7\u683c\uff0c\u9ed8\u8ba4\u4e3aNone\uff0c\u8868\u793a :class:`~MarketOrder`, \u6b64\u53c2\u6570\u4e3b\u8981\u7528\u4e8e\u7b80\u5316 `style` \u53c2\u6570\u3002\n\n    :param style: \u4e0b\u5355\u7c7b\u578b, \u9ed8\u8ba4\u662f\u5e02\u4ef7\u5355\u3002\u76ee\u524d\u652f\u6301\u7684\u8ba2\u5355\u7c7b\u578b\u6709 :class:`~LimitOrder` \u548c :class:`~MarketOrder`\n    :type style: `OrderStyle` object\n\n    :return: :class:`~Order` object | None\n\n    :example:\n\n    .. code-block:: python\n\n        #\u4e70\u516520\u624b\u7684\u5e73\u5b89\u94f6\u884c\u80a1\u7968\uff0c\u5e76\u4e14\u53d1\u9001\u5e02\u4ef7\u5355\uff1a\n        order_lots('000001.XSHE', 20)\n        #\u4e70\u516510\u624b\u5e73\u5b89\u94f6\u884c\u80a1\u7968\uff0c\u5e76\u4e14\u53d1\u9001\u9650\u4ef7\u5355\uff0c\u4ef7\u683c\u4e3a\uffe510\uff1a\n        order_lots('000001.XSHE', 10, style=LimitOrder(10))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef order_value(id_or_ins, cash_amount, price=None, style=None):\n\n    style = cal_style(price, style)\n\n    if isinstance(style, LimitOrder):\n        if style.get_limit_price() <= 0:\n            raise RQInvalidArgument(_(u\"Limit order price should be positive\"))\n\n    order_book_id = assure_stock_order_book_id(id_or_ins)\n    env = Environment.get_instance()\n\n    price = env.get_last_price(order_book_id)\n    if not is_valid_price(price):\n        user_system_log.warn(\n            _(u\"Order Creation Failed: [{order_book_id}] No market data\").format(order_book_id=order_book_id))\n        return\n\n    account = env.portfolio.accounts[DEFAULT_ACCOUNT_TYPE.STOCK.name]\n\n    if cash_amount > 0:\n        cash_amount = min(cash_amount, account.cash)\n\n    price = price if isinstance(style, MarketOrder) else style.get_limit_price()\n    amount = int(Decimal(cash_amount) / Decimal(price))\n\n    if cash_amount > 0:\n        round_lot = int(env.get_instrument(order_book_id).round_lot)\n\n        # FIXME: logic duplicate with order_shares\n        amount = int(Decimal(amount) / Decimal(round_lot)) * round_lot\n\n        while amount > 0:\n            dummy_order = Order.__from_create__(order_book_id, amount, SIDE.BUY, style, POSITION_EFFECT.OPEN)\n            expected_transaction_cost = env.get_order_transaction_cost(DEFAULT_ACCOUNT_TYPE.STOCK, dummy_order)\n            if amount * price + expected_transaction_cost <= cash_amount:\n                break\n            amount -= round_lot\n        else:\n            user_system_log.warn(_(u\"Order Creation Failed: 0 order quantity\"))\n            return\n\n    # if the cash_amount is larger than you current security\u2019s position,\n    # then it will sell all shares of this security.\n\n    position = account.positions[order_book_id]\n    amount = downsize_amount(amount, position)\n\n    return order_shares(order_book_id, amount, style=style)", "response": "\u4f7f\u7528\u60f3\u8981\u82b1\u8d39\u7684\u91d1\u94b1\u4e70\u5165/\u5356\u51fa\u80a1\u7968\uff0c\u800c\u4e0d\u662f\u4e70\u5165/\u5356\u51fa\u60f3\u8981\u7684\u80a1\u6570\uff0c\u6b63\u6570\u4ee3\u8868\u4e70\u5165\uff0c\u8d1f\u6570\u4ee3\u8868\u5356\u51fa\u3002\u80a1\u7968\u7684\u80a1\u6570\u603b\u662f\u4f1a\u88ab\u8c03\u6574\u6210\u5bf9\u5e94\u7684100\u7684\u500d\u6570\uff08\u5728A\u4e2d\u56fdA\u80a1\u5e02\u573a1\u624b\u662f100\u80a1\uff09\u3002\u5982\u679c\u8d44\u91d1\u4e0d\u8db3\uff0c\u8be5API\u5c06\u4e0d\u4f1a\u521b\u5efa\u53d1\u9001\u8ba2\u5355\u3002\n\n    \u9700\u8981\u6ce8\u610f\uff1a\n    \u5f53\u60a8\u63d0\u4ea4\u4e00\u4e2a\u4e70\u5355\u65f6\uff0ccash_amount \u4ee3\u8868\u7684\u542b\u4e49\u662f\u60a8\u5e0c\u671b\u4e70\u5165\u80a1\u7968\u6d88\u8017\u7684\u91d1\u989d\uff08\u5305\u542b\u7a0e\u8d39\uff09\uff0c\u6700\u7ec8\u4e70\u5165\u7684\u80a1\u6570\u4e0d\u4ec5\u548c\u53d1\u5355\u7684\u4ef7\u683c\u6709\u5173\uff0c\u8fd8\u548c\u7a0e\u8d39\u76f8\u5173\u7684\u53c2\u6570\u8bbe\u7f6e\u6709\u5173\u3002\n    \u5f53\u60a8\u63d0\u4ea4\u4e00\u4e2a\u5356\u5355\u65f6\uff0ccash_amount \u4ee3\u8868\u7684\u610f\u4e49\u662f\u60a8\u5e0c\u671b\u5356\u51fa\u80a1\u7968\u7684\u603b\u4ef7\u503c\u3002\u5982\u679c\u91d1\u989d\u8d85\u51fa\u4e86\u60a8\u6240\u6301\u6709\u80a1\u7968\u7684\u4ef7\u503c\uff0c\u90a3\u4e48\u60a8\u5c06\u5356\u51fa\u6240\u6709\u80a1\u7968\u3002\n\n    :param id_or_ins: \u4e0b\u5355\u6807\u7684\u7269\n    :type id_or_ins: :class:`~Instrument` object | `str`\n\n    :param float cash_amount: \u9700\u8981\u82b1\u8d39\u73b0\u91d1\u8d2d\u4e70/\u5356\u51fa\u8bc1\u5238\u7684\u6570\u76ee\u3002\u6b63\u6570\u4ee3\u8868\u4e70\u5165\uff0c\u8d1f\u6570\u4ee3\u8868\u5356\u51fa\u3002\n\n    :param float price: \u4e0b\u5355\u4ef7\u683c\uff0c\u9ed8\u8ba4\u4e3aNone\uff0c\u8868\u793a :class:`~MarketOrder`, \u6b64\u53c2\u6570\u4e3b\u8981\u7528\u4e8e\u7b80\u5316 `style` \u53c2\u6570\u3002\n\n    :param style: \u4e0b\u5355\u7c7b\u578b, \u9ed8\u8ba4\u662f\u5e02\u4ef7\u5355\u3002\u76ee\u524d\u652f\u6301\u7684\u8ba2\u5355\u7c7b\u578b\u6709 :class:`~LimitOrder` \u548c :class:`~MarketOrder`\n    :type style: `OrderStyle` object\n\n    :return: :class:`~Order` object | None\n\n    :example:\n\n    .. code-block:: python\n\n        #\u82b1\u8d39\u6700\u591a\uffe510000\u4e70\u5165\u5e73\u5b89\u94f6\u884c\u80a1\u7968\uff0c\u5e76\u4ee5\u5e02\u4ef7\u5355\u53d1\u9001\u3002\u5177\u4f53\u4e0b\u5355\u7684\u6570\u91cf\u4e0e\u60a8\u7b56\u7565\u7a0e\u8d39\u76f8\u5173\u7684\u914d\u7f6e\u6709\u5173\u3002\n        order_value('000001.XSHE', 10000)\n        #\u5356\u51fa\u4ef7\u503c\uffe510000\u7684\u73b0\u5728\u6301\u6709\u7684\u5e73\u5b89\u94f6\u884c\uff1a\n        order_value('000001.XSHE', -10000)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\norder a single item in a random order.", "response": "def order_percent(id_or_ins, percent, price=None, style=None):\n    \"\"\"\n    \u53d1\u9001\u4e00\u4e2a\u82b1\u8d39\u4ef7\u503c\u7b49\u4e8e\u76ee\u524d\u6295\u8d44\u7ec4\u5408\uff08\u5e02\u573a\u4ef7\u503c\u548c\u76ee\u524d\u73b0\u91d1\u7684\u603b\u548c\uff09\u4e00\u5b9a\u767e\u5206\u6bd4\u73b0\u91d1\u7684\u4e70/\u5356\u5355\uff0c\u6b63\u6570\u4ee3\u8868\u4e70\uff0c\u8d1f\u6570\u4ee3\u8868\u5356\u3002\u80a1\u7968\u7684\u80a1\u6570\u603b\u662f\u4f1a\u88ab\u8c03\u6574\u6210\u5bf9\u5e94\u7684\u4e00\u624b\u7684\u80a1\u7968\u6570\u7684\u500d\u6570\uff081\u624b\u662f100\u80a1\uff09\u3002\u767e\u5206\u6bd4\u662f\u4e00\u4e2a\u5c0f\u6570\uff0c\u5e76\u4e14\u5c0f\u4e8e\u6216\u7b49\u4e8e1\uff08<=100%\uff09\uff0c0.5\u8868\u793a\u7684\u662f50%.\u9700\u8981\u6ce8\u610f\uff0c\u5982\u679c\u8d44\u91d1\u4e0d\u8db3\uff0c\u8be5API\u5c06\u4e0d\u4f1a\u521b\u5efa\u53d1\u9001\u8ba2\u5355\u3002\n\n    \u9700\u8981\u6ce8\u610f\uff1a\n    \u53d1\u9001\u4e70\u5355\u65f6\uff0cpercent \u4ee3\u8868\u7684\u662f\u671f\u671b\u4e70\u5165\u80a1\u7968\u6d88\u8017\u7684\u91d1\u989d\uff08\u5305\u542b\u7a0e\u8d39\uff09\u5360\u6295\u8d44\u7ec4\u5408\u603b\u6743\u76ca\u7684\u6bd4\u4f8b\u3002\n    \u53d1\u9001\u5356\u5355\u65f6\uff0cpercent \u4ee3\u8868\u7684\u662f\u671f\u671b\u5356\u51fa\u7684\u80a1\u7968\u603b\u4ef7\u503c\u5360\u6295\u8d44\u7ec4\u5408\u603b\u6743\u76ca\u7684\u6bd4\u4f8b\u3002\n\n    :param id_or_ins: \u4e0b\u5355\u6807\u7684\u7269\n    :type id_or_ins: :class:`~Instrument` object | `str`\n\n    :param float percent: \u5360\u6709\u73b0\u6709\u7684\u6295\u8d44\u7ec4\u5408\u4ef7\u503c\u7684\u767e\u5206\u6bd4\u3002\u6b63\u6570\u8868\u793a\u4e70\u5165\uff0c\u8d1f\u6570\u8868\u793a\u5356\u51fa\u3002\n\n    :param float price: \u4e0b\u5355\u4ef7\u683c\uff0c\u9ed8\u8ba4\u4e3aNone\uff0c\u8868\u793a :class:`~MarketOrder`, \u6b64\u53c2\u6570\u4e3b\u8981\u7528\u4e8e\u7b80\u5316 `style` \u53c2\u6570\u3002\n\n    :param style: \u4e0b\u5355\u7c7b\u578b, \u9ed8\u8ba4\u662f\u5e02\u4ef7\u5355\u3002\u76ee\u524d\u652f\u6301\u7684\u8ba2\u5355\u7c7b\u578b\u6709 :class:`~LimitOrder` \u548c :class:`~MarketOrder`\n    :type style: `OrderStyle` object\n\n    :return: :class:`~Order` object | None\n\n    :example:\n\n    .. code-block:: python\n\n        #\u82b1\u8d39\u7b49\u4e8e\u73b0\u6709\u6295\u8d44\u7ec4\u540850%\u4ef7\u503c\u7684\u73b0\u91d1\u4e70\u5165\u5e73\u5b89\u94f6\u884c\u80a1\u7968\uff1a\n        order_percent('000001.XSHG', 0.5)\n    \"\"\"\n    if percent < -1 or percent > 1:\n        raise RQInvalidArgument(_(u\"percent should between -1 and 1\"))\n\n    style = cal_style(price, style)\n    account = Environment.get_instance().portfolio.accounts[DEFAULT_ACCOUNT_TYPE.STOCK.name]\n    return order_value(id_or_ins, account.total_value * percent, style=style)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget order target value.", "response": "def order_target_value(id_or_ins, cash_amount, price=None, style=None):\n    \"\"\"\n    \u4e70\u5165/\u5356\u51fa\u5e76\u4e14\u81ea\u52a8\u8c03\u6574\u8be5\u8bc1\u5238\u7684\u4ed3\u4f4d\u5230\u4e00\u4e2a\u76ee\u6807\u4ef7\u503c\u3002\n    \u52a0\u4ed3\u65f6\uff0ccash_amount \u4ee3\u8868\u73b0\u6709\u6301\u4ed3\u7684\u4ef7\u503c\u52a0\u4e0a\u5373\u5c06\u82b1\u8d39\uff08\u5305\u542b\u7a0e\u8d39\uff09\u7684\u73b0\u91d1\u7684\u603b\u4ef7\u503c\u3002\n    \u51cf\u4ed3\u65f6\uff0ccash_amount \u4ee3\u8868\u8c03\u6574\u4ed3\u4f4d\u7684\u76ee\u6807\u4ef7\u81f3\u3002\n\n    \u9700\u8981\u6ce8\u610f\uff0c\u5982\u679c\u8d44\u91d1\u4e0d\u8db3\uff0c\u8be5API\u5c06\u4e0d\u4f1a\u521b\u5efa\u53d1\u9001\u8ba2\u5355\u3002\n\n    :param id_or_ins: \u4e0b\u5355\u6807\u7684\u7269\n    :type id_or_ins: :class:`~Instrument` object | `str` | List[:class:`~Instrument`] | List[`str`]\n\n    :param float cash_amount: \u6700\u7ec8\u7684\u8be5\u8bc1\u5238\u7684\u4ed3\u4f4d\u76ee\u6807\u4ef7\u503c\u3002\n\n    :param float price: \u4e0b\u5355\u4ef7\u683c\uff0c\u9ed8\u8ba4\u4e3aNone\uff0c\u8868\u793a :class:`~MarketOrder`, \u6b64\u53c2\u6570\u4e3b\u8981\u7528\u4e8e\u7b80\u5316 `style` \u53c2\u6570\u3002\n\n    :param style: \u4e0b\u5355\u7c7b\u578b, \u9ed8\u8ba4\u662f\u5e02\u4ef7\u5355\u3002\u76ee\u524d\u652f\u6301\u7684\u8ba2\u5355\u7c7b\u578b\u6709 :class:`~LimitOrder` \u548c :class:`~MarketOrder`\n    :type style: `OrderStyle` object\n\n    :return: :class:`~Order` object | None\n\n    :example:\n\n    .. code-block:: python\n\n        #\u5982\u679c\u73b0\u5728\u7684\u6295\u8d44\u7ec4\u5408\u4e2d\u6301\u6709\u4ef7\u503c\uffe53000\u7684\u5e73\u5b89\u94f6\u884c\u80a1\u7968\u7684\u4ed3\u4f4d\uff0c\u4ee5\u4e0b\u4ee3\u7801\u8303\u4f8b\u4f1a\u53d1\u9001\u82b1\u8d39 \uffe57000 \u73b0\u91d1\u7684\u5e73\u5b89\u94f6\u884c\u4e70\u5355\u5230\u5e02\u573a\u3002\uff08\u5411\u4e0b\u8c03\u6574\u5230\u6700\u63a5\u8fd1\u6bcf\u624b\u80a1\u6570\u5373100\u7684\u500d\u6570\u7684\u80a1\u6570\uff09\uff1a\n        order_target_value('000001.XSHE', 10000)\n    \"\"\"\n    order_book_id = assure_stock_order_book_id(id_or_ins)\n    account = Environment.get_instance().portfolio.accounts[DEFAULT_ACCOUNT_TYPE.STOCK.name]\n    position = account.positions[order_book_id]\n\n    style = cal_style(price, style)\n    if cash_amount == 0:\n        return _sell_all_stock(order_book_id, position.sellable, style)\n\n    try:\n        market_value = position.market_value\n    except RuntimeError:\n        order_result = order_value(order_book_id, np.nan, style=style)\n        if order_result:\n            raise\n    else:\n        return order_value(order_book_id, cash_amount - market_value, style=style)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\norder a single target by a given percentage.", "response": "def order_target_percent(id_or_ins, percent, price=None, style=None):\n    \"\"\"\n    \u4e70\u5165/\u5356\u51fa\u8bc1\u5238\u4ee5\u81ea\u52a8\u8c03\u6574\u8be5\u8bc1\u5238\u7684\u4ed3\u4f4d\u5230\u5360\u6709\u4e00\u4e2a\u76ee\u6807\u4ef7\u503c\u3002\n\n    \u52a0\u4ed3\u65f6\uff0cpercent \u4ee3\u8868\u8bc1\u5238\u5df2\u6709\u6301\u4ed3\u7684\u4ef7\u503c\u52a0\u4e0a\u5373\u5c06\u82b1\u8d39\u7684\u73b0\u91d1\uff08\u5305\u542b\u7a0e\u8d39\uff09\u7684\u603b\u503c\u5360\u5f53\u524d\u6295\u8d44\u7ec4\u5408\u603b\u4ef7\u503c\u7684\u6bd4\u4f8b\u3002\n    \u51cf\u4ed3\u65f6\uff0cpercent \u4ee3\u8868\u8bc1\u5238\u5c06\u88ab\u8c03\u6574\u5230\u7684\u76ee\u6807\u4ef7\u81f3\u5360\u5f53\u524d\u6295\u8d44\u7ec4\u5408\u603b\u4ef7\u503c\u7684\u6bd4\u4f8b\u3002\n\n    \u5176\u5b9e\u6211\u4eec\u9700\u8981\u8ba1\u7b97\u4e00\u4e2aposition_to_adjust (\u5373\u5e94\u8be5\u8c03\u6574\u7684\u4ed3\u4f4d)\n\n    `position_to_adjust = target_position - current_position`\n\n    \u6295\u8d44\u7ec4\u5408\u4ef7\u503c\u7b49\u4e8e\u6240\u6709\u5df2\u6709\u4ed3\u4f4d\u7684\u4ef7\u503c\u548c\u5269\u4f59\u73b0\u91d1\u7684\u603b\u548c\u3002\u4e70/\u5356\u5355\u4f1a\u88ab\u4e0b\u820d\u5165\u4e00\u624b\u80a1\u6570\uff08A\u80a1\u662f100\u7684\u500d\u6570\uff09\u7684\u500d\u6570\u3002\u76ee\u6807\u767e\u5206\u6bd4\u5e94\u8be5\u662f\u4e00\u4e2a\u5c0f\u6570\uff0c\u5e76\u4e14\u6700\u5927\u503c\u5e94\u8be5<=1\uff0c\u6bd4\u59820.5\u8868\u793a50%\u3002\n\n    \u5982\u679cposition_to_adjust \u8ba1\u7b97\u4e4b\u540e\u662f\u6b63\u7684\uff0c\u90a3\u4e48\u4f1a\u4e70\u5165\u8be5\u8bc1\u5238\uff0c\u5426\u5219\u4f1a\u5356\u51fa\u8be5\u8bc1\u5238\u3002 \u9700\u8981\u6ce8\u610f\uff0c\u5982\u679c\u8d44\u91d1\u4e0d\u8db3\uff0c\u8be5API\u5c06\u4e0d\u4f1a\u521b\u5efa\u53d1\u9001\u8ba2\u5355\u3002\n\n    :param id_or_ins: \u4e0b\u5355\u6807\u7684\u7269\n    :type id_or_ins: :class:`~Instrument` object | `str` | List[:class:`~Instrument`] | List[`str`]\n\n    :param float percent: \u4ed3\u4f4d\u6700\u7ec8\u6240\u5360\u6295\u8d44\u7ec4\u5408\u603b\u4ef7\u503c\u7684\u76ee\u6807\u767e\u5206\u6bd4\u3002\n\n    :param float price: \u4e0b\u5355\u4ef7\u683c\uff0c\u9ed8\u8ba4\u4e3aNone\uff0c\u8868\u793a :class:`~MarketOrder`, \u6b64\u53c2\u6570\u4e3b\u8981\u7528\u4e8e\u7b80\u5316 `style` \u53c2\u6570\u3002\n\n    :param style: \u4e0b\u5355\u7c7b\u578b, \u9ed8\u8ba4\u662f\u5e02\u4ef7\u5355\u3002\u76ee\u524d\u652f\u6301\u7684\u8ba2\u5355\u7c7b\u578b\u6709 :class:`~LimitOrder` \u548c :class:`~MarketOrder`\n    :type style: `OrderStyle` object\n\n    :return: :class:`~Order` object | None\n\n    :example:\n\n    .. code-block:: python\n\n        #\u5982\u679c\u6295\u8d44\u7ec4\u5408\u4e2d\u5df2\u7ecf\u6709\u4e86\u5e73\u5b89\u94f6\u884c\u80a1\u7968\u7684\u4ed3\u4f4d\uff0c\u5e76\u4e14\u5360\u636e\u76ee\u524d\u6295\u8d44\u7ec4\u5408\u768410%\u7684\u4ef7\u503c\uff0c\u90a3\u4e48\u4ee5\u4e0b\u4ee3\u7801\u4f1a\u6d88\u8017\u76f8\u5f53\u4e8e\u5f53\u524d\u6295\u8d44\u7ec4\u5408\u4ef7\u503c5%\u7684\u73b0\u91d1\u4e70\u5165\u5e73\u5b89\u94f6\u884c\u80a1\u7968\uff1a\n        order_target_percent('000001.XSHE', 0.15)\n    \"\"\"\n    if percent < 0 or percent > 1:\n        raise RQInvalidArgument(_(u\"percent should between 0 and 1\"))\n    order_book_id = assure_stock_order_book_id(id_or_ins)\n\n    style = cal_style(price, style)\n\n    account = Environment.get_instance().portfolio.accounts[DEFAULT_ACCOUNT_TYPE.STOCK.name]\n    position = account.positions[order_book_id]\n\n    if percent == 0:\n        return _sell_all_stock(order_book_id, position.sellable, style)\n\n    try:\n        market_value = position.market_value\n    except RuntimeError:\n        order_result = order_value(order_book_id, np.nan, style=style)\n        if order_result:\n            raise\n    else:\n        return order_value(order_book_id, account.total_value * percent - market_value, style=style)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_suspended(order_book_id, count=1):\n    dt = Environment.get_instance().calendar_dt.date()\n    order_book_id = assure_stock_order_book_id(order_book_id)\n    return Environment.get_instance().data_proxy.is_suspended(order_book_id, dt, count)", "response": "\u5224\u65ad\u67d0\u53ea\u80a1\u7968\u662f\u5426\u5168\u5929\u505c\u724c\u3002\n\n    :param str order_book_id: \u67d0\u53ea\u80a1\u7968\u7684\u4ee3\u7801\u6216\u80a1\u7968\u4ee3\u7801\uff0c\u53ef\u4f20\u5165\u5355\u53ea\u80a1\u7968\u7684order_book_id, symbol\n\n    :param int count: \u56de\u6eaf\u83b7\u53d6\u7684\u6570\u636e\u4e2a\u6570\u3002\u9ed8\u8ba4\u4e3a\u5f53\u524d\u80fd\u591f\u83b7\u53d6\u5230\u7684\u6700\u8fd1\u7684\u6570\u636e\n\n    :return: count\u4e3a1\u65f6 `bool`; count>1\u65f6 `pandas.DataFrame`"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_bundle(data_bundle_path, locale):\n    import rqalpha.utils.bundle_helper\n    rqalpha.utils.bundle_helper.update_bundle(data_bundle_path, locale)", "response": "Update the data bundle with the given locale."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run(**kwargs):\n    config_path = kwargs.get('config_path', None)\n    if config_path is not None:\n        config_path = os.path.abspath(config_path)\n        kwargs.pop('config_path')\n    if not kwargs.get('base__securities', None):\n        kwargs.pop('base__securities', None)\n\n    from rqalpha import main\n    source_code = kwargs.get(\"base__source_code\")\n    cfg = parse_config(kwargs, config_path=config_path, click_type=True, source_code=source_code)\n    source_code = cfg.base.source_code\n    results = main.run(cfg, source_code=source_code)\n\n    # store results into ipython when running in ipython\n    from rqalpha.utils import is_run_from_ipython\n    if results is not None and is_run_from_ipython():\n        import IPython\n        from rqalpha.utils import RqAttrDict\n        ipy = IPython.get_ipython()\n        report = results.get(\"sys_analyser\", {})\n        ipy.user_global_ns[\"results\"] = results\n        ipy.user_global_ns[\"report\"] = RqAttrDict(report)\n\n    if results is None:\n        sys.exit(1)", "response": "Start to run a strategy"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate example strategies to target folder", "response": "def examples(directory):\n    \"\"\"\n    Generate example strategies to target folder\n    \"\"\"\n    source_dir = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"examples\")\n\n    try:\n        shutil.copytree(source_dir, os.path.join(directory, \"examples\"))\n    except OSError as e:\n        if e.errno == errno.EEXIST:\n            six.print_(\"Folder examples is exists.\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef generate_config(directory):\n    default_config = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"config.yml\")\n    target_config_path = os.path.abspath(os.path.join(directory, 'config.yml'))\n    shutil.copy(default_config, target_config_path)\n    six.print_(\"Config file has been generated in\", target_config_path)", "response": "Generate default config file"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef mod(cmd, params):\n    def list(params):\n        \"\"\"\n        List all mod configuration\n        \"\"\"\n        from tabulate import tabulate\n        from rqalpha.utils.config import get_mod_conf\n\n        mod_config = get_mod_conf()\n        table = []\n\n        for mod_name, mod in six.iteritems(mod_config['mod']):\n            table.append([\n                mod_name,\n                (\"enabled\" if mod['enabled'] else \"disabled\")\n            ])\n\n        headers = [\n            \"name\",\n            \"status\"\n        ]\n\n        six.print_(tabulate(table, headers=headers, tablefmt=\"psql\"))\n        six.print_(\"You can use `rqalpha mod list/install/uninstall/enable/disable` to manage your mods\")\n\n    def install(params):\n        \"\"\"\n        Install third-party Mod\n        \"\"\"\n        try:\n            from pip._internal import main as pip_main\n            from pip._internal.commands.install import InstallCommand\n        except ImportError:\n            from pip import main as pip_main\n            from pip.commands.install import InstallCommand\n\n        params = [param for param in params]\n\n        options, mod_list = InstallCommand().parse_args(params)\n        mod_list = [mod_name for mod_name in mod_list if mod_name != \".\"]\n\n        params = [\"install\"] + params\n\n        for mod_name in mod_list:\n            mod_name_index = params.index(mod_name)\n            if mod_name.startswith(\"rqalpha_mod_sys_\"):\n                six.print_('System Mod can not be installed or uninstalled')\n                return\n            if \"rqalpha_mod_\" in mod_name:\n                lib_name = mod_name\n            else:\n                lib_name = \"rqalpha_mod_\" + mod_name\n            params[mod_name_index] = lib_name\n\n        # Install Mod\n        installed_result = pip_main(params)\n\n        # Export config\n        from rqalpha.utils.config import load_yaml, user_mod_conf_path\n        user_conf = load_yaml(user_mod_conf_path()) if os.path.exists(user_mod_conf_path()) else {'mod': {}}\n\n        if installed_result == 0:\n            # \u5982\u679c\u4e3a0\uff0c\u5219\u8bf4\u660e\u5b89\u88c5\u6210\u529f\n            if len(mod_list) == 0:\n                \"\"\"\n                \u4e3b\u8981\u662f\u65b9\u4fbf `pip install -e .` \u8fd9\u79cd\u65b9\u5f0f \u672c\u5730\u8c03\u8bd5 Mod \u4f7f\u7528\uff0c\u9700\u8981\u6ee1\u8db3\u4ee5\u4e0b\u6761\u4ef6:\n                1.  `rqalpha mod install -e .` \u547d\u4ee4\u662f\u5728\u5bf9\u5e94 \u81ea\u5b9a\u4e49 Mod \u7684\u6839\u76ee\u5f55\u4e0b\n                2.  \u8be5 Mod \u5fc5\u987b\u5305\u542b `setup.py` \u6587\u4ef6\uff08\u5426\u5219\u4e5f\u4e0d\u80fd\u6b63\u5e38\u7684 `pip install -e .` \u6765\u5b89\u88c5\uff09\n                3.  \u8be5 Mod \u5305\u540d\u5fc5\u987b\u6309\u7167 RQAlpha \u7684\u89c4\u8303\u6765\u547d\u540d\uff0c\u5177\u4f53\u89c4\u5219\u5982\u4e0b\n                    *   \u5fc5\u987b\u4ee5 `rqalpha-mod-` \u6765\u5f00\u5934\uff0c\u6bd4\u5982 `rqalpha-mod-xxx-yyy`\n                    *   \u5bf9\u5e94import\u7684\u5e93\u540d\u5fc5\u987b\u8981 `rqalpha_mod_` \u6765\u5f00\u5934\uff0c\u5e76\u4e14\u9700\u8981\u548c\u5305\u540d\u540e\u534a\u90e8\u5206\u4e00\u81f4\uff0c\u4f46\u662f `-` \u9700\u8981\u66ff\u6362\u4e3a `_`, \u6bd4\u5982 `rqalpha_mod_xxx_yyy`\n                \"\"\"\n                mod_name = _detect_package_name_from_dir(params)\n                mod_name = mod_name.replace(\"-\", \"_\").replace(\"rqalpha_mod_\", \"\")\n                mod_list.append(mod_name)\n\n            for mod_name in mod_list:\n                if \"rqalpha_mod_\" in mod_name:\n                    mod_name = mod_name.replace(\"rqalpha_mod_\", \"\")\n                if \"==\" in mod_name:\n                    mod_name = mod_name.split('==')[0]\n                user_conf['mod'][mod_name] = {}\n                user_conf['mod'][mod_name]['enabled'] = False\n\n            dump_config(user_mod_conf_path(), user_conf)\n\n        return installed_result\n\n    def uninstall(params):\n        \"\"\"\n        Uninstall third-party Mod\n        \"\"\"\n\n        try:\n            from pip._internal import main as pip_main\n            from pip._internal.commands.uninstall import UninstallCommand\n        except ImportError:\n            # be compatible with pip < 10.0\n            from pip import main as pip_main\n            from pip.commands.uninstall import UninstallCommand\n\n        params = [param for param in params]\n\n        options, mod_list = UninstallCommand().parse_args(params)\n\n        params = [\"uninstall\"] + params\n\n        for mod_name in mod_list:\n            mod_name_index = params.index(mod_name)\n            if mod_name.startswith(\"rqalpha_mod_sys_\"):\n                six.print_('System Mod can not be installed or uninstalled')\n                return\n            if \"rqalpha_mod_\" in mod_name:\n                lib_name = mod_name\n            else:\n                lib_name = \"rqalpha_mod_\" + mod_name\n            params[mod_name_index] = lib_name\n\n        # Uninstall Mod\n        uninstalled_result = pip_main(params)\n        # Remove Mod Config\n        from rqalpha.utils.config import user_mod_conf_path, load_yaml\n        user_conf = load_yaml(user_mod_conf_path()) if os.path.exists(user_mod_conf_path()) else {'mod': {}}\n\n        for mod_name in mod_list:\n            if \"rqalpha_mod_\" in mod_name:\n                mod_name = mod_name.replace(\"rqalpha_mod_\", \"\")\n\n            del user_conf['mod'][mod_name]\n\n        dump_config(user_mod_conf_path(), user_conf)\n        return uninstalled_result\n\n    def enable(params):\n        \"\"\"\n        enable mod\n        \"\"\"\n        mod_name = params[0]\n        if \"rqalpha_mod_\" in mod_name:\n            mod_name = mod_name.replace(\"rqalpha_mod_\", \"\")\n\n        # check whether is installed\n        module_name = \"rqalpha_mod_\" + mod_name\n        if module_name.startswith(\"rqalpha_mod_sys_\"):\n            module_name = \"rqalpha.mod.\" + module_name\n        try:\n            import_module(module_name)\n        except ImportError:\n            installed_result = install([module_name])\n            if installed_result != 0:\n                return\n\n        from rqalpha.utils.config import user_mod_conf_path, load_yaml\n        user_conf = load_yaml(user_mod_conf_path()) if os.path.exists(user_mod_conf_path()) else {'mod': {}}\n\n        try:\n            user_conf['mod'][mod_name]['enabled'] = True\n        except KeyError:\n            user_conf['mod'][mod_name] = {'enabled': True}\n\n        dump_config(user_mod_conf_path(), user_conf)\n\n    def disable(params):\n        \"\"\"\n        disable mod\n        \"\"\"\n        mod_name = params[0]\n\n        if \"rqalpha_mod_\" in mod_name:\n            mod_name = mod_name.replace(\"rqalpha_mod_\", \"\")\n\n        from rqalpha.utils.config import user_mod_conf_path, load_yaml\n        user_conf = load_yaml(user_mod_conf_path()) if os.path.exists(user_mod_conf_path()) else {'mod': {}}\n\n        try:\n            user_conf['mod'][mod_name]['enabled'] = False\n        except KeyError:\n            user_conf['mod'][mod_name] = {'enabled': False}\n\n        dump_config(user_mod_conf_path(), user_conf)\n\n    locals()[cmd](params)", "response": "A function that returns a list of mod modules that can be installed or uninstalled."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef report(result_pickle_file_path, target_report_csv_path):\n    import pandas as pd\n    result_dict = pd.read_pickle(result_pickle_file_path)\n\n    from .report import generate_report\n    generate_report(result_dict, target_report_csv_path)", "response": "Generate report from backtest output file"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets total margin of all positions", "response": "def margin(self):\n        \"\"\"\n        [float] \u603b\u4fdd\u8bc1\u91d1\n        \"\"\"\n        return sum(position.margin for position in six.itervalues(self._positions))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget total amount of buy margin", "response": "def buy_margin(self):\n        \"\"\"\n        [float] \u4e70\u65b9\u5411\u4fdd\u8bc1\u91d1\n        \"\"\"\n        return sum(position.buy_margin for position in six.itervalues(self._positions))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting total amount of sell margin", "response": "def sell_margin(self):\n        \"\"\"\n        [float] \u5356\u65b9\u5411\u4fdd\u8bc1\u91d1\n        \"\"\"\n        return sum(position.sell_margin for position in six.itervalues(self._positions))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the holding pnl of all positions", "response": "def holding_pnl(self):\n        \"\"\"\n        [float] \u6d6e\u52a8\u76c8\u4e8f\n        \"\"\"\n        return sum(position.holding_pnl for position in six.itervalues(self._positions))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the realized PNL of the current object.", "response": "def realized_pnl(self):\n        \"\"\"\n        [float] \u5e73\u4ed3\u76c8\u4e8f\n        \"\"\"\n        return sum(position.realized_pnl for position in six.itervalues(self._positions))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef order(order_book_id, quantity, price=None, style=None):\n    style = cal_style(price, style)\n    orders = Environment.get_instance().portfolio.order(order_book_id, quantity, style)\n\n    if isinstance(orders, Order):\n        return [orders]\n    return orders", "response": "MarketOrder requires a order_book_id to be passed in."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nexecuting a query on the specified record set.", "response": "def query(self, body, params=None):\n        \"\"\"\n        `<Execute SQL>`_\n\n        :arg body: Use the `query` element to start a query. Use the `cursor`\n            element to continue a query.\n        :arg format: a short version of the Accept header, e.g. json, yaml\n        \"\"\"\n        if body in SKIP_IN_PATH:\n            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n        return self.transport.perform_request(\"POST\", \"/_sql\", params=params, body=body)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef translate(self, body, params=None):\n        if body in SKIP_IN_PATH:\n            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n        return self.transport.perform_request(\n            \"POST\", \"/_sql/translate\", params=params, body=body\n        )", "response": "This method is used to translate the specified resource into Elasticsearch queries."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef log_request_success(self, method, full_url, path, body, status_code, response, duration):\n        #  TODO: optionally pass in params instead of full_url and do urlencode only when needed\n\n        # body has already been serialized to utf-8, deserialize it for logging\n        # TODO: find a better way to avoid (de)encoding the body back and forth\n        if body:\n            try:\n                body = body.decode('utf-8', 'ignore')\n            except AttributeError:\n                pass\n\n        logger.info(\n            '%s %s [status:%s request:%.3fs]', method, full_url,\n            status_code, duration\n        )\n        logger.debug('> %s', body)\n        logger.debug('< %s', response)\n\n        self._log_trace(method, path, body, status_code, response, duration)", "response": "Log a successful API call."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nlogs an unsuccessful API call.", "response": "def log_request_fail(self, method, full_url, path, body, duration, status_code=None, response=None, exception=None):\n        \"\"\" Log an unsuccessful API call.  \"\"\"\n        # do not log 404s on HEAD requests\n        if method == 'HEAD' and status_code == 404:\n            return\n        logger.warning(\n            '%s %s [status:%s request:%.3fs]', method, full_url,\n            status_code or 'N/A', duration, exc_info=exception is not None\n        )\n\n        # body has already been serialized to utf-8, deserialize it for logging\n        # TODO: find a better way to avoid (de)encoding the body back and forth\n        if body:\n            try:\n                body = body.decode('utf-8', 'ignore')\n            except AttributeError:\n                pass\n\n        logger.debug('> %s', body)\n\n        self._log_trace(method, path, body, status_code, response, duration)\n\n        if response is not None:\n            logger.debug('< %s', response)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nlocating appropriate exception and raise it.", "response": "def _raise_error(self, status_code, raw_data):\n        \"\"\" Locate appropriate exception and raise it. \"\"\"\n        error_message = raw_data\n        additional_info = None\n        try:\n            if raw_data:\n                additional_info = json.loads(raw_data)\n                error_message = additional_info.get('error', error_message)\n                if isinstance(error_message, dict) and 'type' in error_message:\n                    error_message = error_message['type']\n        except (ValueError, TypeError) as err:\n            logger.warning('Undecodable raw error response from server: %s', err)\n\n        raise HTTP_EXCEPTIONS.get(status_code, TransportError)(status_code, error_message, additional_info)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ping(self, params=None):\n        try:\n            return self.transport.perform_request(\"HEAD\", \"/\", params=params)\n        except TransportError:\n            return False", "response": "Ping the elastic elastic"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nperform a multi get operation on an index type and ids.", "response": "def mget(self, body, doc_type=None, index=None, params=None):\n        \"\"\"\n        Get multiple documents based on an index, type (optional) and ids.\n        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/docs-multi-get.html>`_\n\n        :arg body: Document identifiers; can be either `docs` (containing full\n            document information) or `ids` (when index and type is provided in\n            the URL.\n        :arg index: The name of the index\n        :arg _source: True or false to return the _source field or not, or a\n            list of fields to return\n        :arg _source_exclude: A list of fields to exclude from the returned\n            _source field\n        :arg _source_include: A list of fields to extract and return from the\n            _source field\n        :arg preference: Specify the node or shard the operation should be\n            performed on (default: random)\n        :arg realtime: Specify whether to perform the operation in realtime or\n            search mode\n        :arg refresh: Refresh the shard containing the document before\n            performing the operation\n        :arg routing: Specific routing value\n        :arg stored_fields: A comma-separated list of stored fields to return in\n            the response\n        \"\"\"\n        if body in SKIP_IN_PATH:\n            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n        return self.transport.perform_request(\n            \"GET\", _make_path(index, doc_type, \"_mget\"), params=params, body=body\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update(self, index, id, doc_type=\"_doc\", body=None, params=None):\n        for param in (index, id):\n            if param in SKIP_IN_PATH:\n                raise ValueError(\"Empty value passed for a required argument.\")\n        return self.transport.perform_request(\n            \"POST\", _make_path(index, doc_type, id, \"_update\"), params=params, body=body\n        )", "response": "Update a document based on a script or partial data provided."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nexecutes a search query and get back search hits", "response": "def search(self, index=None, body=None, params=None):\n        \"\"\"\n        Execute a search query and get back search hits that match the query.\n        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/search-search.html>`_\n\n        :arg index: A list of index names to search, or a string containing a\n            comma-separated list of index names to search; use `_all`\n            or empty string to perform the operation on all indices\n        :arg body: The search definition using the Query DSL\n        :arg _source: True or false to return the _source field or not, or a\n            list of fields to return\n        :arg _source_exclude: A list of fields to exclude from the returned\n            _source field\n        :arg _source_include: A list of fields to extract and return from the\n            _source field\n        :arg allow_no_indices: Whether to ignore if a wildcard indices\n            expression resolves into no concrete indices. (This includes `_all`\n            string or when no indices have been specified)\n        :arg allow_partial_search_results: Set to false to return an overall\n            failure if the request would produce partial results. Defaults to\n            True, which will allow partial results in the case of timeouts or\n            partial failures\n        :arg analyze_wildcard: Specify whether wildcard and prefix queries\n            should be analyzed (default: false)\n        :arg analyzer: The analyzer to use for the query string\n        :arg batched_reduce_size: The number of shard results that should be\n            reduced at once on the coordinating node. This value should be used\n            as a protection mechanism to reduce the memory overhead per search\n            request if the potential number of shards in the request can be\n            large., default 512\n        :arg default_operator: The default operator for query string query (AND\n            or OR), default 'OR', valid choices are: 'AND', 'OR'\n        :arg df: The field to use as default where no field prefix is given in\n            the query string\n        :arg docvalue_fields: A comma-separated list of fields to return as the\n            docvalue representation of a field for each hit\n        :arg expand_wildcards: Whether to expand wildcard expression to concrete\n            indices that are open, closed or both., default 'open', valid\n            choices are: 'open', 'closed', 'none', 'all'\n        :arg explain: Specify whether to return detailed information about score\n            computation as part of a hit\n        :arg from\\\\_: Starting offset (default: 0)\n        :arg ignore_unavailable: Whether specified concrete indices should be\n            ignored when unavailable (missing or closed)\n        :arg lenient: Specify whether format-based query failures (such as\n            providing text to a numeric field) should be ignored\n        :arg max_concurrent_shard_requests: The number of concurrent shard\n            requests this search executes concurrently. This value should be\n            used to limit the impact of the search on the cluster in order to\n            limit the number of concurrent shard requests, default 'The default\n            grows with the number of nodes in the cluster but is at most 256.'\n        :arg pre_filter_shard_size: A threshold that enforces a pre-filter\n            roundtrip to prefilter search shards based on query rewriting if\n            the number of shards the search request expands to exceeds the\n            threshold. This filter roundtrip can limit the number of shards\n            significantly if for instance a shard can not match any documents\n            based on it's rewrite method ie. if date filters are mandatory to\n            match but the shard bounds and the query are disjoint., default 128\n        :arg preference: Specify the node or shard the operation should be\n            performed on (default: random)\n        :arg q: Query in the Lucene query string syntax\n        :arg rest_total_hits_as_int: This parameter is used to restore the total hits as a number\n            in the response. This param is added version 6.x to handle mixed cluster queries where nodes\n            are in multiple versions (7.0 and 6.latest)\n        :arg request_cache: Specify if request cache should be used for this\n            request or not, defaults to index level setting\n        :arg routing: A comma-separated list of specific routing values\n        :arg scroll: Specify how long a consistent view of the index should be\n            maintained for scrolled search\n        :arg search_type: Search operation type, valid choices are:\n            'query_then_fetch', 'dfs_query_then_fetch'\n        :arg size: Number of hits to return (default: 10)\n        :arg sort: A comma-separated list of <field>:<direction> pairs\n        :arg stats: Specific 'tag' of the request for logging and statistical\n            purposes\n        :arg stored_fields: A comma-separated list of stored fields to return as\n            part of a hit\n        :arg suggest_field: Specify which field to use for suggestions\n        :arg suggest_mode: Specify suggest mode, default 'missing', valid\n            choices are: 'missing', 'popular', 'always'\n        :arg suggest_size: How many suggestions to return in response\n        :arg suggest_text: The source text for which the suggestions should be\n            returned\n        :arg terminate_after: The maximum number of documents to collect for\n            each shard, upon reaching which the query execution will terminate\n            early.\n        :arg timeout: Explicit operation timeout\n        :arg track_scores: Whether to calculate and return scores even if they\n            are not used for sorting\n        :arg track_total_hits: Indicate if the number of documents that match\n            the query should be tracked\n        :arg typed_keys: Specify whether aggregation and suggester names should\n            be prefixed by their respective types in the response\n        :arg version: Specify whether to return document version as part of a\n            hit\n        \"\"\"\n        # from is a reserved word so it cannot be used, use from_ instead\n        if \"from_\" in params:\n            params[\"from\"] = params.pop(\"from_\")\n\n        if not index:\n            index = \"_all\"\n        return self.transport.perform_request(\n            \"GET\", _make_path(index, \"_search\"), params=params, body=body\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nchange the value of ``requests_per_second`` of a running ``reindex`` task. `<https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-reindex.html>`_ :arg task_id: The task id to rethrottle :arg requests_per_second: The throttle to set on this request in floating sub-requests per second. -1 means set no throttle.", "response": "def reindex_rethrottle(self, task_id=None, params=None):\n        \"\"\"\n        Change the value of ``requests_per_second`` of a running ``reindex`` task.\n        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-reindex.html>`_\n\n        :arg task_id: The task id to rethrottle\n        :arg requests_per_second: The throttle to set on this request in\n            floating sub-requests per second. -1 means set no throttle.\n        \"\"\"\n        return self.transport.perform_request(\n            \"POST\", _make_path(\"_reindex\", task_id, \"_rethrottle\"), params=params\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef search_shards(self, index=None, params=None):\n        return self.transport.perform_request(\n            \"GET\", _make_path(index, \"_search_shards\"), params=params\n        )", "response": "The search_shards api returns the indices and shards that a search\n            request would be executed against."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef search_template(self, index=None, body=None, params=None):\n        return self.transport.perform_request(\n            \"GET\", _make_path(index, \"_search\", \"template\"), params=params, body=body\n        )", "response": "A query that searches for the score of a specific resource in a specific index."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nscrolls a search request created by specifying the scroll parameter.", "response": "def scroll(self, scroll_id=None, body=None, params=None):\n        \"\"\"\n        Scroll a search request created by specifying the scroll parameter.\n        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-scroll.html>`_\n\n        :arg scroll_id: The scroll ID\n        :arg body: The scroll ID if not passed by URL or query parameter.\n        :arg scroll: Specify how long a consistent view of the index should be\n            maintained for scrolled search\n        :arg rest_total_hits_as_int: This parameter is used to restore the total hits as a number\n            in the response. This param is added version 6.x to handle mixed cluster queries where nodes\n            are in multiple versions (7.0 and 6.latest)\n        \"\"\"\n        if scroll_id in SKIP_IN_PATH and body in SKIP_IN_PATH:\n            raise ValueError(\"You need to supply scroll_id or body.\")\n        elif scroll_id and not body:\n            body = {\"scroll_id\": scroll_id}\n        elif scroll_id:\n            params[\"scroll_id\"] = scroll_id\n\n        return self.transport.perform_request(\n            \"GET\", \"/_search/scroll\", params=params, body=body\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef put_script(self, id, body, context=None, params=None):\n        for param in (id, body):\n            if param in SKIP_IN_PATH:\n                raise ValueError(\"Empty value passed for a required argument.\")\n        return self.transport.perform_request(\n            \"PUT\", _make_path(\"_scripts\", id, context), params=params, body=body\n        )", "response": "Create a script in given language with specified ID."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef render_search_template(self, id=None, body=None, params=None):\n        return self.transport.perform_request(\n            \"GET\", _make_path(\"_render\", \"template\", id), params=params, body=body\n        )", "response": "Render the stored search definition template."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef msearch_template(self, body, index=None, params=None):\n        if body in SKIP_IN_PATH:\n            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n        return self.transport.perform_request(\n            \"GET\",\n            _make_path(index, \"_msearch\", \"template\"),\n            params=params,\n            body=self._bulk_body(body),\n            headers={\"content-type\": \"application/x-ndjson\"},\n        )", "response": "The _msearch template endpoint allows to use the mustache language to render search requests before they are executed and fill existing metadata - search request templates with template parameters."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nbulking operation for the specified resource.", "response": "def bulk(self, body, doc_type=None, params=None):\n        \"\"\"\n        `<http://www.elastic.co/guide/en/monitoring/current/appendix-api-bulk.html>`_\n\n        :arg body: The operation definition and data (action-data pairs),\n            separated by newlines\n        :arg doc_type: Default document type for items which don't provide one\n        :arg interval: Collection interval (e.g., '10s' or '10000ms') of the\n            payload\n        :arg system_api_version: API Version of the monitored system\n        :arg system_id: Identifier of the monitored system\n        \"\"\"\n        if body in SKIP_IN_PATH:\n            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n        return self.transport.perform_request(\n            \"POST\",\n            _make_path(\"_monitoring\", doc_type, \"bulk\"),\n            params=params,\n            body=self._bulk_body(body),\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_connection(self, host):\n        self.hosts.append(host)\n        self.set_connections(self.hosts)", "response": "Add a new connection to the pool."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sniff_hosts(self, initial=False):\n        node_info = self._get_sniff_data(initial)\n\n        hosts = list(filter(None, (self._get_host_info(n) for n in node_info)))\n\n        # we weren't able to get any nodes or host_info_callback blocked all -\n        # raise error.\n        if not hosts:\n            raise TransportError(\"N/A\", \"Unable to sniff hosts - no viable hosts found.\")\n\n        self.set_connections(hosts)", "response": "Obtain a list of nodes from the cluster and create a connection pool using the information retrieved."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmarking a connection as dead in the connection pool.", "response": "def mark_dead(self, connection):\n        \"\"\"\n        Mark a connection as dead (failed) in the connection pool. If sniffing\n        on failure is enabled this will initiate the sniffing process.\n\n        :arg connection: instance of :class:`~elasticsearch.Connection` that failed\n        \"\"\"\n        # mark as dead even when sniffing to avoid hitting this host during the sniff process\n        self.connection_pool.mark_dead(connection)\n        if self.sniff_on_connection_fail:\n            self.sniff_hosts()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef perform_request(self, method, url, headers=None, params=None, body=None):\n        if body is not None:\n            body = self.serializer.dumps(body)\n\n            # some clients or environments don't support sending GET with body\n            if method in ('HEAD', 'GET') and self.send_get_body_as != 'GET':\n                # send it as post instead\n                if self.send_get_body_as == 'POST':\n                    method = 'POST'\n\n                # or as source parameter\n                elif self.send_get_body_as == 'source':\n                    if params is None:\n                        params = {}\n                    params['source'] = body\n                    body = None\n\n        if body is not None:\n            try:\n                body = body.encode('utf-8', 'surrogatepass')\n            except (UnicodeDecodeError, AttributeError):\n                # bytes/str - no need to re-encode\n                pass\n\n        ignore = ()\n        timeout = None\n        if params:\n            timeout = params.pop('request_timeout', None)\n            ignore = params.pop('ignore', ())\n            if isinstance(ignore, int):\n                ignore = (ignore, )\n\n        for attempt in range(self.max_retries + 1):\n            connection = self.get_connection()\n\n            try:\n                # add a delay before attempting the next retry\n                # 0, 1, 3, 7, etc...\n                delay = 2**attempt - 1\n                time.sleep(delay)\n                status, headers_response, data = connection.perform_request(method, url, params, body, headers=headers, ignore=ignore, timeout=timeout)\n\n            except TransportError as e:\n                if method == 'HEAD' and e.status_code == 404:\n                    return False\n\n                retry = False\n                if isinstance(e, ConnectionTimeout):\n                    retry = self.retry_on_timeout\n                elif isinstance(e, ConnectionError):\n                    retry = True\n                elif e.status_code in self.retry_on_status:\n                    retry = True\n\n                if retry:\n                    # only mark as dead if we are retrying\n                    self.mark_dead(connection)\n                    # raise exception on last retry\n                    if attempt == self.max_retries:\n                        raise\n                else:\n                    raise\n\n            else:\n                # connection didn't fail, confirm it's live status\n                self.connection_pool.mark_live(connection)\n\n                if method == 'HEAD':\n                    return 200 <= status < 300\n\n                if data:\n                    data = self.deserializer.loads(data, headers_response.get('content-type'))\n                return data", "response": "Perform a request to the connection pool and return the response."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchanging the password for a user.", "response": "def change_password(self, body, username=None, params=None):\n        \"\"\"\n        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-change-password.html>`_\n\n        :arg body: the new password for the user\n        :arg username: The username of the user to change the password for\n        :arg refresh: If `true` (the default) then refresh the affected shards\n            to make this operation visible to search, if `wait_for` then wait\n            for a refresh to make this operation visible to search, if `false`\n            then do nothing with refreshes., valid choices are: 'true', 'false',\n            'wait_for'\n        \"\"\"\n        if body in SKIP_IN_PATH:\n            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n        return self.transport.perform_request(\n            \"PUT\",\n            _make_path(\"_security\", \"user\", username, \"_password\"),\n            params=params,\n            body=body,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nclears the cache for the specified realms.", "response": "def clear_cached_realms(self, realms, params=None):\n        \"\"\"\n        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-clear-cache.html>`_\n\n        :arg realms: Comma-separated list of realms to clear\n        :arg usernames: Comma-separated list of usernames to clear from the\n            cache\n        \"\"\"\n        if realms in SKIP_IN_PATH:\n            raise ValueError(\"Empty value passed for a required argument 'realms'.\")\n        return self.transport.perform_request(\n            \"POST\",\n            _make_path(\"_security\", \"realm\", realms, \"_clear_cache\"),\n            params=params,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef clear_cached_roles(self, name, params=None):\n        if name in SKIP_IN_PATH:\n            raise ValueError(\"Empty value passed for a required argument 'name'.\")\n        return self.transport.perform_request(\n            \"POST\", _make_path(\"_security\", \"role\", name, \"_clear_cache\"), params=params\n        )", "response": "Clears the cached roles for the specified role."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_api_key(self, body, params=None):\n        if body in SKIP_IN_PATH:\n            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n        return self.transport.perform_request(\n            \"PUT\", \"/_security/api_key\", params=params, body=body\n        )", "response": "Creates an API key for the specified resource."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef delete_privileges(self, application, name, params=None):\n        for param in (application, name):\n            if param in SKIP_IN_PATH:\n                raise ValueError(\"Empty value passed for a required argument.\")\n        return self.transport.perform_request(\n            \"DELETE\",\n            _make_path(\"_security\", \"privilege\", application, name),\n            params=params,\n        )", "response": "Delete privilege for a specific user."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef delete_user(self, username, params=None):\n        if username in SKIP_IN_PATH:\n            raise ValueError(\"Empty value passed for a required argument 'username'.\")\n        return self.transport.perform_request(\n            \"DELETE\", _make_path(\"_security\", \"user\", username), params=params\n        )", "response": "Delete a user from the elastic elastic"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_privileges(self, application=None, name=None, params=None):\n        return self.transport.perform_request(\n            \"GET\",\n            _make_path(\"_security\", \"privilege\", application, name),\n            params=params,\n        )", "response": "Get the privilege of a user."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting a role from the elastic search.", "response": "def get_role(self, name=None, params=None):\n        \"\"\"\n        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-get-role.html>`_\n\n        :arg name: Role name\n        \"\"\"\n        return self.transport.perform_request(\n            \"GET\", _make_path(\"_security\", \"role\", name), params=params\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_role_mapping(self, name=None, params=None):\n        return self.transport.perform_request(\n            \"GET\", _make_path(\"_security\", \"role_mapping\", name), params=params\n        )", "response": "Get the role mapping for a specific user."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_token(self, body, params=None):\n        if body in SKIP_IN_PATH:\n            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n        return self.transport.perform_request(\n            \"POST\", \"/_security/oauth2/token\", params=params, body=body\n        )", "response": "This method returns the token for the user."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef has_privileges(self, body, user=None, params=None):\n        if body in SKIP_IN_PATH:\n            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n        return self.transport.perform_request(\n            \"GET\",\n            _make_path(\"_security\", \"user\", user, \"_has_privileges\"),\n            params=params,\n            body=body,\n        )", "response": "This method checks if the user has the specified privileges."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninvalidate the token for the user.", "response": "def invalidate_token(self, body, params=None):\n        \"\"\"\n        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-invalidate-token.html>`_\n\n        :arg body: The token to invalidate\n        \"\"\"\n        if body in SKIP_IN_PATH:\n            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n        return self.transport.perform_request(\n            \"DELETE\", \"/_security/oauth2/token\", params=params, body=body\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef put_user(self, username, body, params=None):\n        for param in (username, body):\n            if param in SKIP_IN_PATH:\n                raise ValueError(\"Empty value passed for a required argument.\")\n        return self.transport.perform_request(\n            \"PUT\", _make_path(\"_security\", \"user\", username), params=params, body=body\n        )", "response": "This method allows you to add a user to the user s cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef delete_lifecycle(self, policy=None, params=None):\n        return self.transport.perform_request(\n            \"DELETE\", _make_path(\"_ilm\", \"policy\", policy), params=params\n        )", "response": "Delete the index lifecycle of the specified resource."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nexplaining the life of an index.", "response": "def explain_lifecycle(self, index=None, params=None):\n        \"\"\"\n        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ilm-explain-lifecycle.html>`_\n\n        :arg index: The name of the index to explain\n        \"\"\"\n        return self.transport.perform_request(\n            \"GET\", _make_path(index, \"_ilm\", \"explain\"), params=params\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the current index lifecycle", "response": "def get_lifecycle(self, policy=None, params=None):\n        \"\"\"\n        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ilm-get-lifecycle.html>`_\n\n        :arg policy: The name of the index lifecycle policy\n        \"\"\"\n        return self.transport.perform_request(\n            \"GET\", _make_path(\"_ilm\", \"policy\", policy), params=params\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmoving the current index to a new lifecycle step.", "response": "def move_to_step(self, index=None, body=None, params=None):\n        \"\"\"\n        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ilm-move-to-step.html>`_\n\n        :arg index: The name of the index whose lifecycle step is to change\n        :arg body: The new lifecycle step to move to\n        \"\"\"\n        return self.transport.perform_request(\n            \"POST\", _make_path(\"_ilm\", \"move\", index), params=params, body=body\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef remove_policy(self, index=None, params=None):\n        return self.transport.perform_request(\n            \"POST\", _make_path(index, \"_ilm\", \"remove\"), params=params\n        )", "response": "Remove the current policy from an index."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretries the failed tier entry.", "response": "def retry(self, index=None, params=None):\n        \"\"\"\n        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ilm-retry-policy.html>`_\n\n        :arg index: The name of the indices (comma-separated) whose failed\n            lifecycle step is to be retry\n        \"\"\"\n        return self.transport.perform_request(\n            \"POST\", _make_path(index, \"_ilm\", \"retry\"), params=params\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing the commits in the git repository log and generate a list of dicts per commit.", "response": "def parse_commits(head, name):\n    \"\"\"\n    Go through the git repository log and generate a document per commit\n    containing all the metadata.\n    \"\"\"\n    for commit in head.traverse():\n        yield {\n            '_id': commit.hexsha,\n            'repository': name,\n            'committed_date': datetime.fromtimestamp(commit.committed_date),\n            'committer': {\n                'name': commit.committer.name,\n                'email': commit.committer.email,\n            },\n            'authored_date': datetime.fromtimestamp(commit.authored_date),\n            'author': {\n                'name': commit.author.name,\n                'email': commit.author.email,\n            },\n            'description': commit.message,\n            'parent_shas': [p.hexsha for p in commit.parents],\n            # we only care about the filenames, not the per-file stats\n            'files': list(commit.stats.files),\n            'stats': commit.stats.total,\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load_repo(client, path=None, index='git'):\n    path = dirname(dirname(abspath(__file__))) if path is None else path\n    repo_name = basename(path)\n    repo = git.Repo(path)\n\n    create_git_index(client, index)\n\n    # we let the streaming bulk continuously process the commits as they come\n    # in - since the `parse_commits` function is a generator this will avoid\n    # loading all the commits into memory\n    for ok, result in streaming_bulk(\n            client,\n            parse_commits(repo.refs.master.commit, repo_name),\n            index=index,\n            doc_type='doc',\n            chunk_size=50 # keep the batch sizes small for appearances only\n        ):\n        action, result = result.popitem()\n        doc_id = '/%s/doc/%s' % (index, result['_id'])\n        # process the information from ES whether the document has been\n        # successfully indexed\n        if not ok:\n            print('Failed to %s document %s: %r' % (action, doc_id, result))\n        else:\n            print(doc_id)", "response": "Parse a git repository with all it s commits and load it into elasticsearch\n    using client."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget all jobs in the current tenant.", "response": "def get_jobs(self, id=None, params=None):\n        \"\"\"\n        `<>`_\n\n        :arg id: The ID of the job(s) to fetch. Accepts glob patterns, or left\n            blank for all jobs\n        \"\"\"\n        return self.transport.perform_request(\n            \"GET\", _make_path(\"_rollup\", \"job\", id), params=params\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_rollup_caps(self, id=None, params=None):\n        return self.transport.perform_request(\n            \"GET\", _make_path(\"_rollup\", \"data\", id), params=params\n        )", "response": "Get the rollup capabilities of a specific item."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a new job in the database.", "response": "def put_job(self, id, body, params=None):\n        \"\"\"\n        `<>`_\n\n        :arg id: The ID of the job to create\n        :arg body: The job configuration\n        \"\"\"\n        for param in (id, body):\n            if param in SKIP_IN_PATH:\n                raise ValueError(\"Empty value passed for a required argument.\")\n        return self.transport.perform_request(\n            \"PUT\", _make_path(\"_rollup\", \"job\", id), params=params, body=body\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the current migration information.", "response": "def info(self, index=None, params=None):\n        \"\"\"\n        `<http://www.elastic.co/guide/en/migration/current/migration-api-deprecation.html>`_\n\n        :arg index: Index pattern\n        \"\"\"\n        return self.transport.perform_request(\n            \"GET\",\n            _make_path(index, \"_xpack\", \"migration\", \"deprecations\"),\n            params=params,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef info(self, node_id=None, metric=None, params=None):\n        return self.transport.perform_request(\n            \"GET\", _make_path(\"_nodes\", node_id, metric), params=params\n        )", "response": "Get information about the cluster nodes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef stats(self, node_id=None, metric=None, index_metric=None, params=None):\n        return self.transport.perform_request(\n            \"GET\",\n            _make_path(\"_nodes\", node_id, \"stats\", metric, index_metric),\n            params=params,\n        )", "response": "This method returns the statistics for the specified node."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the current hot threads for a node.", "response": "def hot_threads(self, node_id=None, params=None):\n        \"\"\"\n        An API allowing to get the current hot threads on each node in the cluster.\n        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-nodes-hot-threads.html>`_\n\n        :arg node_id: A comma-separated list of node IDs or names to limit the\n            returned information; use `_local` to return information from the\n            node you're connecting to, leave empty to get information from all\n            nodes\n        :arg type: The type to sample (default: cpu), valid choices are:\n            'cpu', 'wait', 'block'\n        :arg ignore_idle_threads: Don't show threads that are in known-idle\n            places, such as waiting on a socket select or pulling from an empty\n            task queue (default: true)\n        :arg interval: The interval for the second sampling of threads\n        :arg snapshots: Number of samples of thread stacktrace (default: 10)\n        :arg threads: Specify the number of threads to provide information for\n            (default: 3)\n        :arg timeout: Explicit operation timeout\n        \"\"\"\n        # avoid python reserved words\n        if params and \"type_\" in params:\n            params[\"type\"] = params.pop(\"type_\")\n        return self.transport.perform_request(\n            \"GET\", _make_path(\"_cluster\", \"nodes\", node_id, \"hotthreads\"), params=params\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef usage(self, node_id=None, metric=None, params=None):\n        return self.transport.perform_request(\n            \"GET\", _make_path(\"_nodes\", node_id, \"usage\", metric), params=params\n        )", "response": "This method returns the current usage of the cluster nodes."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsplits the list of actions into chunks by number or size or start a new one.", "response": "def _chunk_actions(actions, chunk_size, max_chunk_bytes, serializer):\n    \"\"\"\n    Split actions into chunks by number or size, serialize them into strings in\n    the process.\n    \"\"\"\n    bulk_actions, bulk_data = [], []\n    size, action_count = 0, 0\n    for action, data in actions:\n        raw_data, raw_action = data, action\n        action = serializer.dumps(action)\n        # +1 to account for the trailing new line character\n        cur_size = len(action.encode(\"utf-8\")) + 1\n\n        if data is not None:\n            data = serializer.dumps(data)\n            cur_size += len(data.encode(\"utf-8\")) + 1\n\n        # full chunk, send it and start a new one\n        if bulk_actions and (\n            size + cur_size > max_chunk_bytes or action_count == chunk_size\n        ):\n            yield bulk_data, bulk_actions\n            bulk_actions, bulk_data = [], []\n            size, action_count = 0, 0\n\n        bulk_actions.append(action)\n        if data is not None:\n            bulk_actions.append(data)\n            bulk_data.append((raw_action, raw_data))\n        else:\n            bulk_data.append((raw_action,))\n\n        size += cur_size\n        action_count += 1\n\n    if bulk_actions:\n        yield bulk_data, bulk_actions"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _process_bulk_chunk(\n    client,\n    bulk_actions,\n    bulk_data,\n    raise_on_exception=True,\n    raise_on_error=True,\n    *args,\n    **kwargs\n):\n    \"\"\"\n    Send a bulk request to elasticsearch and process the output.\n    \"\"\"\n    # if raise on error is set, we need to collect errors per chunk before raising them\n    errors = []\n\n    try:\n        # send the actual request\n        resp = client.bulk(\"\\n\".join(bulk_actions) + \"\\n\", *args, **kwargs)\n    except TransportError as e:\n        # default behavior - just propagate exception\n        if raise_on_exception:\n            raise e\n\n        # if we are not propagating, mark all actions in current chunk as failed\n        err_message = str(e)\n        exc_errors = []\n\n        for data in bulk_data:\n            # collect all the information about failed actions\n            op_type, action = data[0].copy().popitem()\n            info = {\"error\": err_message, \"status\": e.status_code, \"exception\": e}\n            if op_type != \"delete\":\n                info[\"data\"] = data[1]\n            info.update(action)\n            exc_errors.append({op_type: info})\n\n        # emulate standard behavior for failed actions\n        if raise_on_error:\n            raise BulkIndexError(\n                \"%i document(s) failed to index.\" % len(exc_errors), exc_errors\n            )\n        else:\n            for err in exc_errors:\n                yield False, err\n            return\n\n    # go through request-response pairs and detect failures\n    for data, (op_type, item) in zip(\n        bulk_data, map(methodcaller(\"popitem\"), resp[\"items\"])\n    ):\n        ok = 200 <= item.get(\"status\", 500) < 300\n        if not ok and raise_on_error:\n            # include original document source\n            if len(data) > 1:\n                item[\"data\"] = data[1]\n            errors.append({op_type: item})\n\n        if ok or not errors:\n            # if we are not just recording all errors to be able to raise\n            # them all at once, yield items individually\n            yield ok, {op_type: item}\n\n    if errors:\n        raise BulkIndexError(\"%i document(s) failed to index.\" % len(errors), errors)", "response": "Process a bulk request and return a list of items."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstreaming bulk consumes actions from the iterable passed in and yields results per action. For non-streaming usecases use :func:`~elasticsearch.helpers.bulk` which is a wrapper around streaming bulk that returns summary information about the bulk operation once the entire input is consumed and sent. If you specify ``max_retries`` it will also retry any documents that were rejected with a ``429`` status code. To do this it will wait (**by calling time.sleep which will block**) for ``initial_backoff`` seconds and then, every subsequent rejection for the same chunk, for double the time every time up to ``max_backoff`` seconds. :arg client: instance of :class:`~elasticsearch.Elasticsearch` to use :arg actions: iterable containing the actions to be executed :arg chunk_size: number of docs in one chunk sent to es (default: 500) :arg max_chunk_bytes: the maximum size of the request in bytes (default: 100MB) :arg raise_on_error: raise ``BulkIndexError`` containing errors (as `.errors`) from the execution of the last chunk when some occur. By default we raise. :arg raise_on_exception: if ``False`` then don't propagate exceptions from call to ``bulk`` and just report the items that failed as failed. :arg expand_action_callback: callback executed on each action passed in, should return a tuple containing the action line and the data line (`None` if data line should be omitted). :arg max_retries: maximum number of times a document will be retried when ``429`` is received, set to 0 (default) for no retries on ``429`` :arg initial_backoff: number of seconds we should wait before the first retry. Any subsequent retries will be powers of ``initial_backoff * 2**retry_number`` :arg max_backoff: maximum number of seconds a retry will wait :arg yield_ok: if set to False will skip successful documents in the output", "response": "def streaming_bulk(\n    client,\n    actions,\n    chunk_size=500,\n    max_chunk_bytes=100 * 1024 * 1024,\n    raise_on_error=True,\n    expand_action_callback=expand_action,\n    raise_on_exception=True,\n    max_retries=0,\n    initial_backoff=2,\n    max_backoff=600,\n    yield_ok=True,\n    *args,\n    **kwargs\n):\n\n    \"\"\"\n    Streaming bulk consumes actions from the iterable passed in and yields\n    results per action. For non-streaming usecases use\n    :func:`~elasticsearch.helpers.bulk` which is a wrapper around streaming\n    bulk that returns summary information about the bulk operation once the\n    entire input is consumed and sent.\n\n    If you specify ``max_retries`` it will also retry any documents that were\n    rejected with a ``429`` status code. To do this it will wait (**by calling\n    time.sleep which will block**) for ``initial_backoff`` seconds and then,\n    every subsequent rejection for the same chunk, for double the time every\n    time up to ``max_backoff`` seconds.\n\n    :arg client: instance of :class:`~elasticsearch.Elasticsearch` to use\n    :arg actions: iterable containing the actions to be executed\n    :arg chunk_size: number of docs in one chunk sent to es (default: 500)\n    :arg max_chunk_bytes: the maximum size of the request in bytes (default: 100MB)\n    :arg raise_on_error: raise ``BulkIndexError`` containing errors (as `.errors`)\n        from the execution of the last chunk when some occur. By default we raise.\n    :arg raise_on_exception: if ``False`` then don't propagate exceptions from\n        call to ``bulk`` and just report the items that failed as failed.\n    :arg expand_action_callback: callback executed on each action passed in,\n        should return a tuple containing the action line and the data line\n        (`None` if data line should be omitted).\n    :arg max_retries: maximum number of times a document will be retried when\n        ``429`` is received, set to 0 (default) for no retries on ``429``\n    :arg initial_backoff: number of seconds we should wait before the first\n        retry. Any subsequent retries will be powers of ``initial_backoff *\n        2**retry_number``\n    :arg max_backoff: maximum number of seconds a retry will wait\n    :arg yield_ok: if set to False will skip successful documents in the output\n    \"\"\"\n    actions = map(expand_action_callback, actions)\n\n    for bulk_data, bulk_actions in _chunk_actions(\n        actions, chunk_size, max_chunk_bytes, client.transport.serializer\n    ):\n\n        for attempt in range(max_retries + 1):\n            to_retry, to_retry_data = [], []\n            if attempt:\n                time.sleep(min(max_backoff, initial_backoff * 2 ** (attempt - 1)))\n\n            try:\n                for data, (ok, info) in zip(\n                    bulk_data,\n                    _process_bulk_chunk(\n                        client,\n                        bulk_actions,\n                        bulk_data,\n                        raise_on_exception,\n                        raise_on_error,\n                        *args,\n                        **kwargs\n                    ),\n                ):\n\n                    if not ok:\n                        action, info = info.popitem()\n                        # retry if retries enabled, we get 429, and we are not\n                        # in the last attempt\n                        if (\n                            max_retries\n                            and info[\"status\"] == 429\n                            and (attempt + 1) <= max_retries\n                        ):\n                            # _process_bulk_chunk expects strings so we need to\n                            # re-serialize the data\n                            to_retry.extend(\n                                map(client.transport.serializer.dumps, data)\n                            )\n                            to_retry_data.append(data)\n                        else:\n                            yield ok, {action: info}\n                    elif yield_ok:\n                        yield ok, info\n\n            except TransportError as e:\n                # suppress 429 errors since we will retry them\n                if attempt == max_retries or e.status_code != 429:\n                    raise\n            else:\n                if not to_retry:\n                    break\n                # retry only subset of documents that didn't succeed\n                bulk_actions, bulk_data = to_retry, to_retry_data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparalleling version of the bulk helper run in multiple threads at once. :arg client: instance of :class:`~elasticsearch.Elasticsearch` to use :arg actions: iterator containing the actions :arg thread_count: size of the threadpool to use for the bulk requests :arg chunk_size: number of docs in one chunk sent to es (default: 500) :arg max_chunk_bytes: the maximum size of the request in bytes (default: 100MB) :arg raise_on_error: raise ``BulkIndexError`` containing errors (as `.errors`) from the execution of the last chunk when some occur. By default we raise. :arg raise_on_exception: if ``False`` then don't propagate exceptions from call to ``bulk`` and just report the items that failed as failed. :arg expand_action_callback: callback executed on each action passed in, should return a tuple containing the action line and the data line (`None` if data line should be omitted). :arg queue_size: size of the task queue between the main thread (producing chunks to send) and the processing threads.", "response": "def parallel_bulk(\n    client,\n    actions,\n    thread_count=4,\n    chunk_size=500,\n    max_chunk_bytes=100 * 1024 * 1024,\n    queue_size=4,\n    expand_action_callback=expand_action,\n    *args,\n    **kwargs\n):\n    \"\"\"\n    Parallel version of the bulk helper run in multiple threads at once.\n\n    :arg client: instance of :class:`~elasticsearch.Elasticsearch` to use\n    :arg actions: iterator containing the actions\n    :arg thread_count: size of the threadpool to use for the bulk requests\n    :arg chunk_size: number of docs in one chunk sent to es (default: 500)\n    :arg max_chunk_bytes: the maximum size of the request in bytes (default: 100MB)\n    :arg raise_on_error: raise ``BulkIndexError`` containing errors (as `.errors`)\n        from the execution of the last chunk when some occur. By default we raise.\n    :arg raise_on_exception: if ``False`` then don't propagate exceptions from\n        call to ``bulk`` and just report the items that failed as failed.\n    :arg expand_action_callback: callback executed on each action passed in,\n        should return a tuple containing the action line and the data line\n        (`None` if data line should be omitted).\n    :arg queue_size: size of the task queue between the main thread (producing\n        chunks to send) and the processing threads.\n    \"\"\"\n    # Avoid importing multiprocessing unless parallel_bulk is used\n    # to avoid exceptions on restricted environments like App Engine\n    from multiprocessing.pool import ThreadPool\n\n    actions = map(expand_action_callback, actions)\n\n    class BlockingPool(ThreadPool):\n        def _setup_queues(self):\n            super(BlockingPool, self)._setup_queues()\n            # The queue must be at least the size of the number of threads to \n            # prevent hanging when inserting sentinel values during teardown.\n            self._inqueue = Queue(max(queue_size, thread_count))\n            self._quick_put = self._inqueue.put\n\n    pool = BlockingPool(thread_count)\n\n    try:\n        for result in pool.imap(\n            lambda bulk_chunk: list(\n                _process_bulk_chunk(\n                    client, bulk_chunk[1], bulk_chunk[0], *args, **kwargs\n                )\n            ),\n            _chunk_actions(\n                actions, chunk_size, max_chunk_bytes, client.transport.serializer\n            ),\n        ):\n            for item in result:\n                yield item\n\n    finally:\n        pool.close()\n        pool.join()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nscans the index for the given search parameters.", "response": "def scan(\n    client,\n    query=None,\n    scroll=\"5m\",\n    raise_on_error=True,\n    preserve_order=False,\n    size=1000,\n    request_timeout=None,\n    clear_scroll=True,\n    scroll_kwargs=None,\n    **kwargs\n):\n    \"\"\"\n    Simple abstraction on top of the\n    :meth:`~elasticsearch.Elasticsearch.scroll` api - a simple iterator that\n    yields all hits as returned by underlining scroll requests.\n\n    By default scan does not return results in any pre-determined order. To\n    have a standard order in the returned documents (either by score or\n    explicit sort definition) when scrolling, use ``preserve_order=True``. This\n    may be an expensive operation and will negate the performance benefits of\n    using ``scan``.\n\n    :arg client: instance of :class:`~elasticsearch.Elasticsearch` to use\n    :arg query: body for the :meth:`~elasticsearch.Elasticsearch.search` api\n    :arg scroll: Specify how long a consistent view of the index should be\n        maintained for scrolled search\n    :arg raise_on_error: raises an exception (``ScanError``) if an error is\n        encountered (some shards fail to execute). By default we raise.\n    :arg preserve_order: don't set the ``search_type`` to ``scan`` - this will\n        cause the scroll to paginate with preserving the order. Note that this\n        can be an extremely expensive operation and can easily lead to\n        unpredictable results, use with caution.\n    :arg size: size (per shard) of the batch send at each iteration.\n    :arg request_timeout: explicit timeout for each call to ``scan``\n    :arg clear_scroll: explicitly calls delete on the scroll id via the clear\n        scroll API at the end of the method on completion or error, defaults\n        to true.\n    :arg scroll_kwargs: additional kwargs to be passed to\n        :meth:`~elasticsearch.Elasticsearch.scroll`\n\n    Any additional keyword arguments will be passed to the initial\n    :meth:`~elasticsearch.Elasticsearch.search` call::\n\n        scan(es,\n            query={\"query\": {\"match\": {\"title\": \"python\"}}},\n            index=\"orders-*\",\n            doc_type=\"books\"\n        )\n\n    \"\"\"\n    scroll_kwargs = scroll_kwargs or {}\n\n    if not preserve_order:\n        query = query.copy() if query else {}\n        query[\"sort\"] = \"_doc\"\n\n    # initial search\n    resp = client.search(\n        body=query, scroll=scroll, size=size, request_timeout=request_timeout, **kwargs\n    )\n    scroll_id = resp.get(\"_scroll_id\")\n\n    try:\n        while scroll_id and resp['hits']['hits']:\n            for hit in resp[\"hits\"][\"hits\"]:\n                yield hit\n\n            # check if we have any errors\n            if resp[\"_shards\"][\"successful\"] < resp[\"_shards\"][\"total\"]:\n                logger.warning(\n                    \"Scroll request has only succeeded on %d shards out of %d.\",\n                    resp[\"_shards\"][\"successful\"],\n                    resp[\"_shards\"][\"total\"],\n                )\n                if raise_on_error:\n                    raise ScanError(\n                        scroll_id,\n                        \"Scroll request has only succeeded on %d shards out of %d.\"\n                        % (resp[\"_shards\"][\"successful\"], resp[\"_shards\"][\"total\"]),\n                    )\n\n            resp = client.scroll(\n                scroll_id,\n                scroll=scroll,\n                request_timeout=request_timeout,\n                **scroll_kwargs\n            )\n            scroll_id = resp.get(\"_scroll_id\")\n\n    finally:\n        if scroll_id and clear_scroll:\n            client.clear_scroll(body={\"scroll_id\": [scroll_id]}, ignore=(404,))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef reindex(\n    client,\n    source_index,\n    target_index,\n    query=None,\n    target_client=None,\n    chunk_size=500,\n    scroll=\"5m\",\n    scan_kwargs={},\n    bulk_kwargs={},\n):\n\n    \"\"\"\n    Reindex all documents from one index that satisfy a given query\n    to another, potentially (if `target_client` is specified) on a different cluster.\n    If you don't specify the query you will reindex all the documents.\n\n    Since ``2.3`` a :meth:`~elasticsearch.Elasticsearch.reindex` api is\n    available as part of elasticsearch itself. It is recommended to use the api\n    instead of this helper wherever possible. The helper is here mostly for\n    backwards compatibility and for situations where more flexibility is\n    needed.\n\n    .. note::\n\n        This helper doesn't transfer mappings, just the data.\n\n    :arg client: instance of :class:`~elasticsearch.Elasticsearch` to use (for\n        read if `target_client` is specified as well)\n    :arg source_index: index (or list of indices) to read documents from\n    :arg target_index: name of the index in the target cluster to populate\n    :arg query: body for the :meth:`~elasticsearch.Elasticsearch.search` api\n    :arg target_client: optional, is specified will be used for writing (thus\n        enabling reindex between clusters)\n    :arg chunk_size: number of docs in one chunk sent to es (default: 500)\n    :arg scroll: Specify how long a consistent view of the index should be\n        maintained for scrolled search\n    :arg scan_kwargs: additional kwargs to be passed to\n        :func:`~elasticsearch.helpers.scan`\n    :arg bulk_kwargs: additional kwargs to be passed to\n        :func:`~elasticsearch.helpers.bulk`\n    \"\"\"\n    target_client = client if target_client is None else target_client\n    docs = scan(client, query=query, index=source_index, scroll=scroll, **scan_kwargs)\n\n    def _change_doc_index(hits, index):\n        for h in hits:\n            h[\"_index\"] = index\n            if \"fields\" in h:\n                h.update(h.pop(\"fields\"))\n            yield h\n\n    kwargs = {\"stats_only\": True}\n    kwargs.update(bulk_kwargs)\n    return bulk(\n        target_client,\n        _change_doc_index(docs, target_index),\n        chunk_size=chunk_size,\n        **kwargs\n    )", "response": "Reindex all documents from one index to another."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the data frame transform configuration for a specific locale.", "response": "def get_data_frame_transform(self, transform_id=None, params=None):\n        \"\"\"\n        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/get-data-frame-transform.html>`_\n\n        :arg transform_id: The id or comma delimited list of id expressions of\n            the transforms to get, '_all' or '*' implies get all transforms\n        :arg from_: skips a number of transform configs, defaults to 0\n        :arg size: specifies a max number of transforms to get, defaults to 100\n        \"\"\"\n        return self.transport.perform_request(\n            \"GET\", _make_path(\"_data_frame\", \"transforms\", transform_id), params=params\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_data_frame_transform_stats(self, transform_id=None, params=None):\n        return self.transport.perform_request(\n            \"GET\",\n            _make_path(\"_data_frame\", \"transforms\", transform_id, \"_stats\"),\n            params=params,\n        )", "response": "Get the statistics for a specific transform."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef preview_data_frame_transform(self, body, params=None):\n        if body in SKIP_IN_PATH:\n            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n        return self.transport.perform_request(\n            \"POST\", \"/_data_frame/transforms/_preview\", params=params, body=body\n        )", "response": "Preview a data frame transform."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstopping a data frame transform.", "response": "def stop_data_frame_transform(self, transform_id, params=None):\n        \"\"\"\n        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/stop-data-frame-transform.html>`_\n\n        :arg transform_id: The id of the transform to stop\n        :arg timeout: Controls the time to wait until the transform has stopped.\n            Default to 30 seconds\n        :arg wait_for_completion: Whether to wait for the transform to fully\n            stop before returning or not. Default to false\n        \"\"\"\n        if transform_id in SKIP_IN_PATH:\n            raise ValueError(\n                \"Empty value passed for a required argument 'transform_id'.\"\n            )\n        return self.transport.perform_request(\n            \"POST\",\n            _make_path(\"_data_frame\", \"transforms\", transform_id, \"_stop\"),\n            params=params,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the state of the allocation of the node.", "response": "def allocation(self, node_id=None, params=None):\n        \"\"\"\n        Allocation provides a snapshot of how shards have located around the\n        cluster and the state of disk usage.\n        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/cat-allocation.html>`_\n\n        :arg node_id: A comma-separated list of node IDs or names to limit the\n            returned information\n        :arg bytes: The unit in which to display byte values, valid choices are:\n            'b', 'k', 'kb', 'm', 'mb', 'g', 'gb', 't', 'tb', 'p', 'pb'\n        :arg format: a short version of the Accept header, e.g. json, yaml\n        :arg h: Comma-separated list of column names to display\n        :arg help: Return help information, default False\n        :arg local: Return local information, do not retrieve the state from\n            master node (default: false)\n        :arg master_timeout: Explicit operation timeout for connection to master\n            node\n        :arg s: Comma-separated list of column names or column aliases to sort\n            by\n        :arg v: Verbose mode. Display column headers, default False\n        \"\"\"\n        return self.transport.perform_request('GET', _make_path('_cat',\n            'allocation', node_id), params=params)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncount the number of documents in a specific index.", "response": "def count(self, index=None, params=None):\n        \"\"\"\n        Count provides quick access to the document count of the entire cluster,\n        or individual indices.\n        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/cat-count.html>`_\n\n        :arg index: A comma-separated list of index names to limit the returned\n            information\n        :arg format: a short version of the Accept header, e.g. json, yaml\n        :arg h: Comma-separated list of column names to display\n        :arg help: Return help information, default False\n        :arg local: Return local information, do not retrieve the state from\n            master node (default: false)\n        :arg master_timeout: Explicit operation timeout for connection to master\n            node\n        :arg s: Comma-separated list of column names or column aliases to sort\n            by\n        :arg v: Verbose mode. Display column headers, default False\n        \"\"\"\n        return self.transport.perform_request('GET', _make_path('_cat', 'count',\n            index), params=params)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef shards(self, index=None, params=None):\n        return self.transport.perform_request('GET', _make_path('_cat',\n            'shards', index), params=params)", "response": "The shards command is the detailed view of what shards are in the master node."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef thread_pool(self, thread_pool_patterns=None, params=None):\n        return self.transport.perform_request('GET', _make_path('_cat',\n            'thread_pool', thread_pool_patterns), params=params)", "response": "Get information about thread pools."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the fielddata of currently loaded fielddata on a per - node basis.", "response": "def fielddata(self, fields=None, params=None):\n        \"\"\"\n        Shows information about currently loaded fielddata on a per-node basis.\n        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/cat-fielddata.html>`_\n\n        :arg fields: A comma-separated list of fields to return the fielddata\n            size\n        :arg bytes: The unit in which to display byte values, valid choices are:\n            'b', 'k', 'kb', 'm', 'mb', 'g', 'gb', 't', 'tb', 'p', 'pb'\n        :arg format: a short version of the Accept header, e.g. json, yaml\n        :arg h: Comma-separated list of column names to display\n        :arg help: Return help information, default False\n        :arg local: Return local information, do not retrieve the state from\n            master node (default: false)\n        :arg master_timeout: Explicit operation timeout for connection to master\n            node\n        :arg s: Comma-separated list of column names or column aliases to sort\n            by\n        :arg v: Verbose mode. Display column headers, default False\n        \"\"\"\n        return self.transport.perform_request('GET', _make_path('_cat',\n            'fielddata', fields), params=params)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the templates of a specific node.", "response": "def templates(self, name=None, params=None):\n        \"\"\"\n        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/cat-templates.html>`_\n\n        :arg name: A pattern that returned template names must match\n        :arg format: a short version of the Accept header, e.g. json, yaml\n        :arg h: Comma-separated list of column names to display\n        :arg help: Return help information, default False\n        :arg local: Return local information, do not retrieve the state from\n            master node (default: false)\n        :arg master_timeout: Explicit operation timeout for connection to master\n            node\n        :arg s: Comma-separated list of column names or column aliases to sort\n            by\n        :arg v: Verbose mode. Display column headers, default False\n        \"\"\"\n        return self.transport.perform_request('GET', _make_path('_cat',\n            'templates', name), params=params)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ack_watch(self, watch_id, action_id=None, params=None):\n        if watch_id in SKIP_IN_PATH:\n            raise ValueError(\"Empty value passed for a required argument 'watch_id'.\")\n        return self.transport.perform_request(\n            \"PUT\",\n            _make_path(\"_watcher\", \"watch\", watch_id, \"_ack\", action_id),\n            params=params,\n        )", "response": "Acknowledge the action of a specific watch."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef execute_watch(self, id=None, body=None, params=None):\n        return self.transport.perform_request(\n            \"PUT\",\n            _make_path(\"_watcher\", \"watch\", id, \"_execute\"),\n            params=params,\n            body=body,\n        )", "response": "`<http://www.elastic.co/guide/en/elasticsearch/reference/current/watcher-api-execute-watch.html>`_\n\n        :arg id: Watch ID\n        :arg body: Execution control\n        :arg debug: indicates whether the watch should execute in debug mode"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nupdating the watch with the specified ID.", "response": "def put_watch(self, id, body=None, params=None):\n        \"\"\"\n        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/watcher-api-put-watch.html>`_\n\n        :arg id: Watch ID\n        :arg body: The watch\n        :arg active: Specify whether the watch is in/active by default\n        :arg if_primary_term: only update the watch if the last operation that\n            has changed the watch has the specified primary term\n        :arg if_seq_no: only update the watch if the last operation that has\n            changed the watch has the specified sequence number\n        :arg version: Explicit version number for concurrency control\n        \"\"\"\n        if id in SKIP_IN_PATH:\n            raise ValueError(\"Empty value passed for a required argument 'id'.\")\n        return self.transport.perform_request(\n            \"PUT\", _make_path(\"_watcher\", \"watch\", id), params=params, body=body\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting a very simple status on the health of the cluster.", "response": "def health(self, index=None, params=None):\n        \"\"\"\n        Get a very simple status on the health of the cluster.\n        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-health.html>`_\n\n        :arg index: Limit the information returned to a specific index\n        :arg level: Specify the level of detail for returned information,\n            default 'cluster', valid choices are: 'cluster', 'indices', 'shards'\n        :arg local: Return local information, do not retrieve the state from\n            master node (default: false)\n        :arg master_timeout: Explicit operation timeout for connection to master\n            node\n        :arg timeout: Explicit operation timeout\n        :arg wait_for_active_shards: Wait until the specified number of shards\n            is active\n        :arg wait_for_events: Wait until all currently queued events with the\n            given priority are processed, valid choices are: 'immediate',\n            'urgent', 'high', 'normal', 'low', 'languid'\n        :arg wait_for_no_relocating_shards: Whether to wait until there are no\n            relocating shards in the cluster\n        :arg wait_for_nodes: Wait until the specified number of nodes is\n            available\n        :arg wait_for_status: Wait until cluster is in a specific state, default\n            None, valid choices are: 'green', 'yellow', 'red'\n        \"\"\"\n        return self.transport.perform_request('GET', _make_path('_cluster',\n            'health', index), params=params)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget a comprehensive state of the whole cluster.", "response": "def state(self, metric=None, index=None, params=None):\n        \"\"\"\n        Get a comprehensive state information of the whole cluster.\n        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-state.html>`_\n\n        :arg metric: Limit the information returned to the specified metrics\n        :arg index: A comma-separated list of index names; use `_all` or empty\n            string to perform the operation on all indices\n        :arg allow_no_indices: Whether to ignore if a wildcard indices\n            expression resolves into no concrete indices. (This includes `_all`\n            string or when no indices have been specified)\n        :arg expand_wildcards: Whether to expand wildcard expression to concrete\n            indices that are open, closed or both., default 'open', valid\n            choices are: 'open', 'closed', 'none', 'all'\n        :arg flat_settings: Return settings in flat format (default: false)\n        :arg ignore_unavailable: Whether specified concrete indices should be\n            ignored when unavailable (missing or closed)\n        :arg local: Return local information, do not retrieve the state from\n            master node (default: false)\n        :arg master_timeout: Specify timeout for connection to master\n        \"\"\"\n        if index and not metric:\n            metric = '_all'\n        return self.transport.perform_request('GET', _make_path('_cluster',\n            'state', metric, index), params=params)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nexecuting a cluster reroute allocation command.", "response": "def reroute(self, body=None, params=None):\n        \"\"\"\n        Explicitly execute a cluster reroute allocation command including specific commands.\n        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-reroute.html>`_\n\n        :arg body: The definition of `commands` to perform (`move`, `cancel`,\n            `allocate`)\n        :arg dry_run: Simulate the operation only and return the resulting state\n        :arg explain: Return an explanation of why the commands can or cannot be\n            executed\n        :arg master_timeout: Explicit operation timeout for connection to master\n            node\n        :arg metric: Limit the information returned to the specified metrics.\n            Defaults to all but metadata, valid choices are: '_all', 'blocks',\n            'metadata', 'nodes', 'routing_table', 'master_node', 'version'\n        :arg retry_failed: Retries allocation of shards that are blocked due to\n            too many subsequent allocation failures\n        :arg timeout: Explicit operation timeout\n        \"\"\"\n        return self.transport.perform_request('POST', '/_cluster/reroute',\n            params=params, body=body)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nupdates the cluster wide specific settings.", "response": "def put_settings(self, body=None, params=None):\n        \"\"\"\n        Update cluster wide specific settings.\n        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-update-settings.html>`_\n\n        :arg body: The settings to be updated. Can be either `transient` or\n            `persistent` (survives cluster restart).\n        :arg flat_settings: Return settings in flat format (default: false)\n        :arg master_timeout: Explicit operation timeout for connection to master\n            node\n        :arg timeout: Explicit operation timeout\n        \"\"\"\n        return self.transport.perform_request('PUT', '/_cluster/settings',\n            params=params, body=body)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nexplain the cluster allocation.", "response": "def allocation_explain(self, body=None, params=None):\n        \"\"\"\n        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-allocation-explain.html>`_\n\n        :arg body: The index, shard, and primary flag to explain. Empty means\n            'explain the first unassigned shard'\n        :arg include_disk_info: Return information about disk usage and shard\n            sizes (default: false)\n        :arg include_yes_decisions: Return 'YES' decisions in explanation\n            (default: false)\n        \"\"\"\n        return self.transport.perform_request('GET',\n            '/_cluster/allocation/explain', params=params, body=body)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the deprecations of a specific migration.", "response": "def deprecations(self, index=None, params=None):\n        \"\"\"\n        `<http://www.elastic.co/guide/en/migration/current/migration-api-deprecation.html>`_\n\n        :arg index: Index pattern\n        \"\"\"\n        return self.transport.perform_request(\n            \"GET\", _make_path(index, \"_migration\", \"deprecations\"), params=params\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a connection instance and its current fail count.", "response": "def get_connection(self):\n        \"\"\"\n        Return a connection from the pool using the `ConnectionSelector`\n        instance.\n\n        It tries to resurrect eligible connections, forces a resurrection when\n        no connections are availible and passes the list of live connections to\n        the selector instance to choose from.\n\n        Returns a connection instance and it's current fail count.\n        \"\"\"\n        self.resurrect()\n        connections = self.connections[:]\n\n        # no live nodes, resurrect one by force and return it\n        if not connections:\n            return self.resurrect(True)\n\n        # only call selector if we have a selection\n        if len(connections) > 1:\n            return self.selector.select(connections)\n\n        # only one connection, no need for a selector\n        return connections[0]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef follow(self, index, body, params=None):\n        for param in (index, body):\n            if param in SKIP_IN_PATH:\n                raise ValueError(\"Empty value passed for a required argument.\")\n        return self.transport.perform_request(\n            \"PUT\", _make_path(index, \"_ccr\", \"follow\"), params=params, body=body\n        )", "response": "Follow the leader index with the given body."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef follow_info(self, index=None, params=None):\n        return self.transport.perform_request(\n            \"GET\", _make_path(index, \"_ccr\", \"info\"), params=params\n        )", "response": "Perform a GET request on the info endpoint of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the auto follow pattern.", "response": "def get_auto_follow_pattern(self, name=None, params=None):\n        \"\"\"\n        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ccr-get-auto-follow-pattern.html>`_\n\n        :arg name: The name of the auto follow pattern.\n        \"\"\"\n        return self.transport.perform_request(\n            \"GET\", _make_path(\"_ccr\", \"auto_follow\", name), params=params\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef put_auto_follow_pattern(self, name, body, params=None):\n        for param in (name, body):\n            if param in SKIP_IN_PATH:\n                raise ValueError(\"Empty value passed for a required argument.\")\n        return self.transport.perform_request(\n            \"PUT\", _make_path(\"_ccr\", \"auto_follow\", name), params=params, body=body\n        )", "response": "This method allows you to update the auto follow pattern."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nresumes the follow of a specific ccr.", "response": "def resume_follow(self, index, body=None, params=None):\n        \"\"\"\n        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ccr-post-resume-follow.html>`_\n\n        :arg index: The name of the follow index to resume following.\n        :arg body: The name of the leader index and other optional ccr related\n            parameters\n        \"\"\"\n        if index in SKIP_IN_PATH:\n            raise ValueError(\"Empty value passed for a required argument 'index'.\")\n        return self.transport.perform_request(\n            \"POST\", _make_path(index, \"_ccr\", \"resume_follow\"), params=params, body=body\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_pipeline(self, id=None, params=None):\n        return self.transport.perform_request('GET', _make_path('_ingest',\n            'pipeline', id), params=params)", "response": "Get the pipeline with the given id."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndeletes a calendar event.", "response": "def delete_calendar_event(self, calendar_id, event_id, params=None):\n        \"\"\"\n        `<>`_\n\n        :arg calendar_id: The ID of the calendar to modify\n        :arg event_id: The ID of the event to remove from the calendar\n        \"\"\"\n        for param in (calendar_id, event_id):\n            if param in SKIP_IN_PATH:\n                raise ValueError(\"Empty value passed for a required argument.\")\n        return self.transport.perform_request(\n            \"DELETE\",\n            _make_path(\"_ml\", \"calendars\", calendar_id, \"events\", event_id),\n            params=params,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef delete_filter(self, filter_id, params=None):\n        if filter_id in SKIP_IN_PATH:\n            raise ValueError(\"Empty value passed for a required argument 'filter_id'.\")\n        return self.transport.perform_request(\n            \"DELETE\", _make_path(\"_ml\", \"filters\", filter_id), params=params\n        )", "response": "This method deletes a filter from the database."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndelete the specified forecast from the specified job.", "response": "def delete_forecast(self, job_id, forecast_id=None, params=None):\n        \"\"\"\n        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/ml-delete-forecast.html>`_\n\n        :arg job_id: The ID of the job from which to delete forecasts\n        :arg forecast_id: The ID of the forecast to delete, can be comma\n            delimited list. Leaving blank implies `_all`\n        :arg allow_no_forecasts: Whether to ignore if `_all` matches no\n            forecasts\n        :arg timeout: Controls the time to wait until the forecast(s) are\n            deleted. Default to 30 seconds\n        \"\"\"\n        if job_id in SKIP_IN_PATH:\n            raise ValueError(\"Empty value passed for a required argument 'job_id'.\")\n        return self.transport.perform_request(\n            \"DELETE\",\n            _make_path(\"_ml\", \"anomaly_detectors\", job_id, \"_forecast\", forecast_id),\n            params=params,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndelete a job from the cluster.", "response": "def delete_job(self, job_id, params=None):\n        \"\"\"\n        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/ml-delete-job.html>`_\n\n        :arg job_id: The ID of the job to delete\n        :arg force: True if the job should be forcefully deleted, default False\n        :arg wait_for_completion: Should this request wait until the operation\n            has completed before returning, default True\n        \"\"\"\n        if job_id in SKIP_IN_PATH:\n            raise ValueError(\"Empty value passed for a required argument 'job_id'.\")\n        return self.transport.perform_request(\n            \"DELETE\", _make_path(\"_ml\", \"anomaly_detectors\", job_id), params=params\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef delete_model_snapshot(self, job_id, snapshot_id, params=None):\n        for param in (job_id, snapshot_id):\n            if param in SKIP_IN_PATH:\n                raise ValueError(\"Empty value passed for a required argument.\")\n        return self.transport.perform_request(\n            \"DELETE\",\n            _make_path(\n                \"_ml\", \"anomaly_detectors\", job_id, \"model_snapshots\", snapshot_id\n            ),\n            params=params,\n        )", "response": "Delete a model snapshot."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef flush_job(self, job_id, body=None, params=None):\n        if job_id in SKIP_IN_PATH:\n            raise ValueError(\"Empty value passed for a required argument 'job_id'.\")\n        return self.transport.perform_request(\n            \"POST\",\n            _make_path(\"_ml\", \"anomaly_detectors\", job_id, \"_flush\"),\n            params=params,\n            body=body,\n        )", "response": "Flushes the specified anomaly detector job."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_buckets(self, job_id, timestamp=None, body=None, params=None):\n        if job_id in SKIP_IN_PATH:\n            raise ValueError(\"Empty value passed for a required argument 'job_id'.\")\n        return self.transport.perform_request(\n            \"GET\",\n            _make_path(\n                \"_ml\", \"anomaly_detectors\", job_id, \"results\", \"buckets\", timestamp\n            ),\n            params=params,\n            body=body,\n        )", "response": "Get the single bucket results from the specified job."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_calendar_events(self, calendar_id, params=None):\n        if calendar_id in SKIP_IN_PATH:\n            raise ValueError(\n                \"Empty value passed for a required argument 'calendar_id'.\"\n            )\n        return self.transport.perform_request(\n            \"GET\", _make_path(\"_ml\", \"calendars\", calendar_id, \"events\"), params=params\n        )", "response": "Get the events for a specific calendar."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_calendars(self, calendar_id=None, body=None, params=None):\n        return self.transport.perform_request(\n            \"GET\", _make_path(\"_ml\", \"calendars\", calendar_id), params=params, body=body\n        )", "response": "Fetch the calendars for a specific calendar."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the categories of an anomaly detector.", "response": "def get_categories(self, job_id, category_id=None, body=None, params=None):\n        \"\"\"\n        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-category.html>`_\n\n        :arg job_id: The name of the job\n        :arg category_id: The identifier of the category definition of interest\n        :arg body: Category selection details if not provided in URI\n        :arg from_: skips a number of categories\n        :arg size: specifies a max number of categories to get\n        \"\"\"\n        if job_id in SKIP_IN_PATH:\n            raise ValueError(\"Empty value passed for a required argument 'job_id'.\")\n        return self.transport.perform_request(\n            \"GET\",\n            _make_path(\n                \"_ml\", \"anomaly_detectors\", job_id, \"results\", \"categories\", category_id\n            ),\n            params=params,\n            body=body,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_datafeed_stats(self, datafeed_id=None, params=None):\n        return self.transport.perform_request(\n            \"GET\", _make_path(\"_ml\", \"datafeeds\", datafeed_id, \"_stats\"), params=params\n        )", "response": "Fetch the statistics for the specified datafeed."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_datafeeds(self, datafeed_id=None, params=None):\n        return self.transport.perform_request(\n            \"GET\", _make_path(\"_ml\", \"datafeeds\", datafeed_id), params=params\n        )", "response": "Fetch the datafeeds for the specified datafeed_id."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfetches the filters for a specific resource.", "response": "def get_filters(self, filter_id=None, params=None):\n        \"\"\"\n        `<>`_\n\n        :arg filter_id: The ID of the filter to fetch\n        :arg from_: skips a number of filters\n        :arg size: specifies a max number of filters to get\n        \"\"\"\n        return self.transport.perform_request(\n            \"GET\", _make_path(\"_ml\", \"filters\", filter_id), params=params\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfetch the statistics for the specified job.", "response": "def get_job_stats(self, job_id=None, params=None):\n        \"\"\"\n        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-job-stats.html>`_\n\n        :arg job_id: The ID of the jobs stats to fetch\n        :arg allow_no_jobs: Whether to ignore if a wildcard expression matches\n            no jobs. (This includes `_all` string or when no jobs have been\n            specified)\n        \"\"\"\n        return self.transport.perform_request(\n            \"GET\",\n            _make_path(\"_ml\", \"anomaly_detectors\", job_id, \"_stats\"),\n            params=params,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_jobs(self, job_id=None, params=None):\n        return self.transport.perform_request(\n            \"GET\", _make_path(\"_ml\", \"anomaly_detectors\", job_id), params=params\n        )", "response": "Fetch the jobs from the specified anomaly detector."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_model_snapshots(self, job_id, snapshot_id=None, body=None, params=None):\n        if job_id in SKIP_IN_PATH:\n            raise ValueError(\"Empty value passed for a required argument 'job_id'.\")\n        return self.transport.perform_request(\n            \"GET\",\n            _make_path(\n                \"_ml\", \"anomaly_detectors\", job_id, \"model_snapshots\", snapshot_id\n            ),\n            params=params,\n            body=body,\n        )", "response": "Retrieves the model snapshots for the specified anomaly detector."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef post_calendar_events(self, calendar_id, body, params=None):\n        for param in (calendar_id, body):\n            if param in SKIP_IN_PATH:\n                raise ValueError(\"Empty value passed for a required argument.\")\n        return self.transport.perform_request(\n            \"POST\",\n            _make_path(\"_ml\", \"calendars\", calendar_id, \"events\"),\n            params=params,\n            body=body,\n        )", "response": "Post a list of events to a calendar."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nposting data to anomaly detector.", "response": "def post_data(self, job_id, body, params=None):\n        \"\"\"\n        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/ml-post-data.html>`_\n\n        :arg job_id: The name of the job receiving the data\n        :arg body: The data to process\n        :arg reset_end: Optional parameter to specify the end of the bucket\n            resetting range\n        :arg reset_start: Optional parameter to specify the start of the bucket\n            resetting range\n        \"\"\"\n        for param in (job_id, body):\n            if param in SKIP_IN_PATH:\n                raise ValueError(\"Empty value passed for a required argument.\")\n        return self.transport.perform_request(\n            \"POST\",\n            _make_path(\"_ml\", \"anomaly_detectors\", job_id, \"_data\"),\n            params=params,\n            body=self._bulk_body(body),\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a new calendar in the specified calendar.", "response": "def put_calendar(self, calendar_id, body=None, params=None):\n        \"\"\"\n        `<>`_\n\n        :arg calendar_id: The ID of the calendar to create\n        :arg body: The calendar details\n        \"\"\"\n        if calendar_id in SKIP_IN_PATH:\n            raise ValueError(\n                \"Empty value passed for a required argument 'calendar_id'.\"\n            )\n        return self.transport.perform_request(\n            \"PUT\", _make_path(\"_ml\", \"calendars\", calendar_id), params=params, body=body\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef put_calendar_job(self, calendar_id, job_id, params=None):\n        for param in (calendar_id, job_id):\n            if param in SKIP_IN_PATH:\n                raise ValueError(\"Empty value passed for a required argument.\")\n        return self.transport.perform_request(\n            \"PUT\",\n            _make_path(\"_ml\", \"calendars\", calendar_id, \"jobs\", job_id),\n            params=params,\n        )", "response": "This method allows you to update a calendar job in a specific calendar."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef start_datafeed(self, datafeed_id, body=None, params=None):\n        if datafeed_id in SKIP_IN_PATH:\n            raise ValueError(\n                \"Empty value passed for a required argument 'datafeed_id'.\"\n            )\n        return self.transport.perform_request(\n            \"POST\",\n            _make_path(\"_ml\", \"datafeeds\", datafeed_id, \"_start\"),\n            params=params,\n            body=body,\n        )", "response": "`<http://www.elastic.co/guide/en/elasticsearch/reference/current/ml-start-datafeed.html>`_\n\n        :arg datafeed_id: The ID of the datafeed to start\n        :arg body: The start datafeed parameters\n        :arg end: The end time when the datafeed should stop. When not set, the\n            datafeed continues in real time\n        :arg start: The start time from where the datafeed should begin\n        :arg timeout: Controls the time to wait until a datafeed has started.\n            Default to 20 seconds"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update_datafeed(self, datafeed_id, body, params=None):\n        for param in (datafeed_id, body):\n            if param in SKIP_IN_PATH:\n                raise ValueError(\"Empty value passed for a required argument.\")\n        return self.transport.perform_request(\n            \"POST\",\n            _make_path(\"_ml\", \"datafeeds\", datafeed_id, \"_update\"),\n            params=params,\n            body=body,\n        )", "response": "Updates the settings of a specific datafeed."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating the properties of a filter.", "response": "def update_filter(self, filter_id, body, params=None):\n        \"\"\"\n        `<>`_\n\n        :arg filter_id: The ID of the filter to update\n        :arg body: The filter update\n        \"\"\"\n        for param in (filter_id, body):\n            if param in SKIP_IN_PATH:\n                raise ValueError(\"Empty value passed for a required argument.\")\n        return self.transport.perform_request(\n            \"POST\",\n            _make_path(\"_ml\", \"filters\", filter_id, \"_update\"),\n            params=params,\n            body=body,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update_model_snapshot(self, job_id, snapshot_id, body, params=None):\n        for param in (job_id, snapshot_id, body):\n            if param in SKIP_IN_PATH:\n                raise ValueError(\"Empty value passed for a required argument.\")\n        return self.transport.perform_request(\n            \"POST\",\n            _make_path(\n                \"_ml\",\n                \"anomaly_detectors\",\n                job_id,\n                \"model_snapshots\",\n                snapshot_id,\n                \"_update\",\n            ),\n            params=params,\n            body=body,\n        )", "response": "Updates the properties of a specific model snapshot."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef validate(self, body, params=None):\n        if body in SKIP_IN_PATH:\n            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n        return self.transport.perform_request(\n            \"POST\", \"/_ml/anomaly_detectors/_validate\", params=params, body=body\n        )", "response": "This method validates the configuration of an anomaly in a single resource."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef validate_detector(self, body, params=None):\n        if body in SKIP_IN_PATH:\n            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n        return self.transport.perform_request(\n            \"POST\",\n            \"/_ml/anomaly_detectors/_validate/detector\",\n            params=params,\n            body=body,\n        )", "response": "`<>`_\n\n        :arg body: The detector"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef analyze(self, index=None, body=None, params=None):\n        return self.transport.perform_request(\n            \"GET\", _make_path(index, \"_analyze\"), params=params, body=body\n        )", "response": "Perform the analysis process on a text and return the tokens breakdown of the text."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nrefreshes one or more index.", "response": "def refresh(self, index=None, params=None):\n        \"\"\"\n        Explicitly refresh one or more index, making all operations performed\n        since the last refresh available for search.\n        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/indices-refresh.html>`_\n\n        :arg index: A comma-separated list of index names; use `_all` or empty\n            string to perform the operation on all indices\n        :arg allow_no_indices: Whether to ignore if a wildcard indices\n            expression resolves into no concrete indices. (This includes `_all`\n            string or when no indices have been specified)\n        :arg expand_wildcards: Whether to expand wildcard expression to concrete\n            indices that are open, closed or both., default 'open', valid\n            choices are: 'open', 'closed', 'none', 'all'\n        :arg ignore_unavailable: Whether specified concrete indices should be\n            ignored when unavailable (missing or closed)\n        \"\"\"\n        return self.transport.perform_request(\n            \"POST\", _make_path(index, \"_refresh\"), params=params\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nflushing the current set of not available log entries to the specified index.", "response": "def flush(self, index=None, params=None):\n        \"\"\"\n        Explicitly flush one or more indices.\n        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/indices-flush.html>`_\n\n        :arg index: A comma-separated list of index names; use `_all` or empty\n            string for all indices\n        :arg allow_no_indices: Whether to ignore if a wildcard indices\n            expression resolves into no concrete indices. (This includes `_all`\n            string or when no indices have been specified)\n        :arg expand_wildcards: Whether to expand wildcard expression to concrete\n            indices that are open, closed or both., default 'open', valid\n            choices are: 'open', 'closed', 'none', 'all'\n        :arg force: Whether a flush should be forced even if it is not\n            necessarily needed ie. if no changes will be committed to the index.\n            This is useful if transaction log IDs should be incremented even if\n            no uncommitted changes are present. (This setting can be considered\n            as internal)\n        :arg ignore_unavailable: Whether specified concrete indices should be\n            ignored when unavailable (missing or closed)\n        :arg wait_if_ongoing: If set to true the flush operation will block\n            until the flush can be executed if another flush operation is\n            already executing. The default is true. If set to false the flush\n            will be skipped iff if another flush operation is already running.\n        \"\"\"\n        return self.transport.perform_request(\n            \"POST\", _make_path(index, \"_flush\"), params=params\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef exists_type(self, index, doc_type, params=None):\n        for param in (index, doc_type):\n            if param in SKIP_IN_PATH:\n                raise ValueError(\"Empty value passed for a required argument.\")\n        return self.transport.perform_request(\n            \"HEAD\", _make_path(index, \"_mapping\", doc_type), params=params\n        )", "response": "Check if a type exists in an index or a master node."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nretrieves mapping definition of index or type.", "response": "def get_mapping(self, index=None, doc_type=None, params=None):\n        \"\"\"\n        Retrieve mapping definition of index or index/type.\n        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/indices-get-mapping.html>`_\n\n        :arg index: A comma-separated list of index names\n        :arg doc_type: A comma-separated list of document types\n        :arg allow_no_indices: Whether to ignore if a wildcard indices\n            expression resolves into no concrete indices. (This includes `_all`\n            string or when no indices have been specified)\n        :arg expand_wildcards: Whether to expand wildcard expression to concrete\n            indices that are open, closed or both., default 'open', valid\n            choices are: 'open', 'closed', 'none', 'all'\n        :arg ignore_unavailable: Whether specified concrete indices should be\n            ignored when unavailable (missing or closed)\n        :arg local: Return local information, do not retrieve the state from\n            master node (default: false)\n        :arg include_type_name: Specify whether requests and responses should include a\n            type name (default: depends on Elasticsearch version).\n        \"\"\"\n        return self.transport.perform_request(\n            \"GET\", _make_path(index, \"_mapping\", doc_type), params=params\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nretrieving mapping definition of a specific field.", "response": "def get_field_mapping(self, fields, index=None, doc_type=None, params=None):\n        \"\"\"\n        Retrieve mapping definition of a specific field.\n        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/indices-get-field-mapping.html>`_\n\n        :arg fields: A comma-separated list of fields\n        :arg index: A comma-separated list of index names\n        :arg doc_type: A comma-separated list of document types\n        :arg allow_no_indices: Whether to ignore if a wildcard indices\n            expression resolves into no concrete indices. (This includes `_all`\n            string or when no indices have been specified)\n        :arg expand_wildcards: Whether to expand wildcard expression to concrete\n            indices that are open, closed or both., default 'open', valid\n            choices are: 'open', 'closed', 'none', 'all'\n        :arg ignore_unavailable: Whether specified concrete indices should be\n            ignored when unavailable (missing or closed)\n        :arg include_defaults: Whether the default mapping values should be\n            returned as well\n        :arg local: Return local information, do not retrieve the state from\n            master node (default: false)\n        :arg include_type_name: Specify whether requests and responses should include a\n            type name (default: depends on Elasticsearch version).\n        \"\"\"\n        if fields in SKIP_IN_PATH:\n            raise ValueError(\"Empty value passed for a required argument 'fields'.\")\n        return self.transport.perform_request(\n            \"GET\",\n            _make_path(index, \"_mapping\", doc_type, \"field\", fields),\n            params=params,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates an alias for a specific index.", "response": "def put_alias(self, index, name, body=None, params=None):\n        \"\"\"\n        Create an alias for a specific index/indices.\n        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/indices-aliases.html>`_\n\n        :arg index: A comma-separated list of index names the alias should point\n            to (supports wildcards); use `_all` to perform the operation on all\n            indices.\n        :arg name: The name of the alias to be created or updated\n        :arg body: The settings for the alias, such as `routing` or `filter`\n        :arg master_timeout: Specify timeout for connection to master\n        :arg request_timeout: Explicit timeout for the operation\n        \"\"\"\n        for param in (index, name):\n            if param in SKIP_IN_PATH:\n                raise ValueError(\"Empty value passed for a required argument.\")\n        return self.transport.perform_request(\n            \"PUT\", _make_path(index, \"_alias\", name), params=params, body=body\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef exists_alias(self, index=None, name=None, params=None):\n        return self.transport.perform_request(\n            \"HEAD\", _make_path(index, \"_alias\", name), params=params\n        )", "response": "Return a boolean indicating whether given alias exists."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_alias(self, index=None, name=None, params=None):\n        return self.transport.perform_request(\n            \"GET\", _make_path(index, \"_alias\", name), params=params\n        )", "response": "Retrieve a specified alias."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef update_aliases(self, body, params=None):\n        if body in SKIP_IN_PATH:\n            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n        return self.transport.perform_request(\n            \"POST\", \"/_aliases\", params=params, body=body\n        )", "response": "Update specified aliases.\n        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/indices-aliases.html>`_\n\n        :arg body: The definition of `actions` to perform\n        :arg master_timeout: Specify timeout for connection to master\n        :arg request_timeout: Request timeout"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef delete_alias(self, index, name, params=None):\n        for param in (index, name):\n            if param in SKIP_IN_PATH:\n                raise ValueError(\"Empty value passed for a required argument.\")\n        return self.transport.perform_request(\n            \"DELETE\", _make_path(index, \"_alias\", name), params=params\n        )", "response": "Delete specific alias.\n        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/indices-aliases.html>`_\n\n        :arg index: A comma-separated list of index names (supports wildcards);\n            use `_all` for all indices\n        :arg name: A comma-separated list of aliases to delete (supports\n            wildcards); use `_all` to delete all aliases for the specified\n            indices.\n        :arg master_timeout: Specify timeout for connection to master\n        :arg request_timeout: Explicit timeout for the operation"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_template(self, name=None, params=None):\n        return self.transport.perform_request(\n            \"GET\", _make_path(\"_template\", name), params=params\n        )", "response": "Retrieve an index template by its name."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_settings(self, index=None, name=None, params=None):\n        return self.transport.perform_request(\n            \"GET\", _make_path(index, \"_settings\", name), params=params\n        )", "response": "Retrieve settings for one or more indices."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef stats(self, index=None, metric=None, params=None):\n        return self.transport.perform_request(\n            \"GET\", _make_path(index, \"_stats\", metric), params=params\n        )", "response": "Retrieve statistics on different operations happening on an index."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nclearing the cache associated with one ore more indices.", "response": "def clear_cache(self, index=None, params=None):\n        \"\"\"\n        Clear either all caches or specific cached associated with one ore more indices.\n        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/indices-clearcache.html>`_\n\n        :arg index: A comma-separated list of index name to limit the operation\n        :arg allow_no_indices: Whether to ignore if a wildcard indices\n            expression resolves into no concrete indices. (This includes `_all`\n            string or when no indices have been specified)\n        :arg expand_wildcards: Whether to expand wildcard expression to concrete\n            indices that are open, closed or both., default 'open', valid\n            choices are: 'open', 'closed', 'none', 'all'\n        :arg field_data: Clear field data\n        :arg fielddata: Clear field data\n        :arg fields: A comma-separated list of fields to clear when using the\n            `field_data` parameter (default: all)\n        :arg ignore_unavailable: Whether specified concrete indices should be\n            ignored when unavailable (missing or closed)\n        :arg query: Clear query caches\n        :arg recycler: Clear the recycler cache\n        :arg request: Clear request cache\n        :arg request_cache: Clear request cache\n        \"\"\"\n        return self.transport.perform_request(\n            \"POST\", _make_path(index, \"_cache\", \"clear\"), params=params\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef upgrade(self, index=None, params=None):\n        return self.transport.perform_request(\n            \"POST\", _make_path(index, \"_upgrade\"), params=params\n        )", "response": "Upgrade one or more indices to the latest format through an API."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_upgrade(self, index=None, params=None):\n        return self.transport.perform_request(\n            \"GET\", _make_path(index, \"_upgrade\"), params=params\n        )", "response": "Get the upgrade status of one or more indices."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nperforms a normal flush then add a generated unique marker to all shards.", "response": "def flush_synced(self, index=None, params=None):\n        \"\"\"\n        Perform a normal flush, then add a generated unique marker (sync_id) to all shards.\n        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/indices-synced-flush.html>`_\n\n        :arg index: A comma-separated list of index names; use `_all` or empty\n            string for all indices\n        :arg allow_no_indices: Whether to ignore if a wildcard indices\n            expression resolves into no concrete indices. (This includes `_all`\n            string or when no indices have been specified)\n        :arg expand_wildcards: Whether to expand wildcard expression to concrete\n            indices that are open, closed or both., default 'open', valid\n            choices are: 'open', 'closed', 'none', 'all'\n        :arg ignore_unavailable: Whether specified concrete indices should be\n            ignored when unavailable (missing or closed)\n        \"\"\"\n        return self.transport.perform_request(\n            \"POST\", _make_path(index, \"_flush\", \"synced\"), params=params\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef shard_stores(self, index=None, params=None):\n        return self.transport.perform_request(\n            \"GET\", _make_path(index, \"_shard_stores\"), params=params\n        )", "response": "Provides store information for shards with the given index."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef forcemerge(self, index=None, params=None):\n        return self.transport.perform_request(\n            \"POST\", _make_path(index, \"_forcemerge\"), params=params\n        )", "response": "Force merge a set of Lucene items into one or more indices."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nretrieve information about a snapshot.", "response": "def get(self, repository, snapshot, params=None):\n        \"\"\"\n        Retrieve information about a snapshot.\n        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html>`_\n\n        :arg repository: A repository name\n        :arg snapshot: A comma-separated list of snapshot names\n        :arg ignore_unavailable: Whether to ignore unavailable snapshots,\n            defaults to false which means a NotFoundError `snapshot_missing_exception` is thrown\n        :arg master_timeout: Explicit operation timeout for connection to master\n            node\n        :arg verbose: Whether to show verbose snapshot info or only show the\n            basic info found in the repository index blob\n        \"\"\"\n        for param in (repository, snapshot):\n            if param in SKIP_IN_PATH:\n                raise ValueError(\"Empty value passed for a required argument.\")\n        return self.transport.perform_request('GET', _make_path('_snapshot',\n            repository, snapshot), params=params)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove a shared file system repository.", "response": "def delete_repository(self, repository, params=None):\n        \"\"\"\n        Removes a shared file system repository.\n        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html>`_\n\n        :arg repository: A comma-separated list of repository names\n        :arg master_timeout: Explicit operation timeout for connection to master\n            node\n        :arg timeout: Explicit operation timeout\n        \"\"\"\n        if repository in SKIP_IN_PATH:\n            raise ValueError(\"Empty value passed for a required argument 'repository'.\")\n        return self.transport.perform_request('DELETE',\n            _make_path('_snapshot', repository), params=params)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_repository(self, repository=None, params=None):\n        return self.transport.perform_request('GET', _make_path('_snapshot',\n            repository), params=params)", "response": "Get information about registered repositories."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a shared file system repository.", "response": "def create_repository(self, repository, body, params=None):\n        \"\"\"\n        Registers a shared file system repository.\n        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html>`_\n\n        :arg repository: A repository name\n        :arg body: The repository definition\n        :arg master_timeout: Explicit operation timeout for connection to master\n            node\n        :arg timeout: Explicit operation timeout\n        :arg verify: Whether to verify the repository after creation\n        \"\"\"\n        for param in (repository, body):\n            if param in SKIP_IN_PATH:\n                raise ValueError(\"Empty value passed for a required argument.\")\n        return self.transport.perform_request('PUT', _make_path('_snapshot',\n            repository), params=params, body=body)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nrestores a snapshot. `<http://www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html>`_ :arg repository: A repository name :arg snapshot: A snapshot name :arg body: Details of what to restore :arg master_timeout: Explicit operation timeout for connection to master node :arg wait_for_completion: Should this request wait until the operation has completed before returning, default False", "response": "def restore(self, repository, snapshot, body=None, params=None):\n        \"\"\"\n        Restore a snapshot.\n        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html>`_\n\n        :arg repository: A repository name\n        :arg snapshot: A snapshot name\n        :arg body: Details of what to restore\n        :arg master_timeout: Explicit operation timeout for connection to master\n            node\n        :arg wait_for_completion: Should this request wait until the operation\n            has completed before returning, default False\n        \"\"\"\n        for param in (repository, snapshot):\n            if param in SKIP_IN_PATH:\n                raise ValueError(\"Empty value passed for a required argument.\")\n        return self.transport.perform_request('POST', _make_path('_snapshot',\n            repository, snapshot, '_restore'), params=params, body=body)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef status(self, repository=None, snapshot=None, params=None):\n        return self.transport.perform_request('GET', _make_path('_snapshot',\n            repository, snapshot, '_status'), params=params)", "response": "Return information about all currently running snapshots."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _escape(value):\n\n    # make sequences into comma-separated stings\n    if isinstance(value, (list, tuple)):\n        value = \",\".join(value)\n\n    # dates and datetimes into isoformat\n    elif isinstance(value, (date, datetime)):\n        value = value.isoformat()\n\n    # make bools into true/false strings\n    elif isinstance(value, bool):\n        value = str(value).lower()\n\n    # don't decode bytestrings\n    elif isinstance(value, bytes):\n        return value\n\n    # encode strings to utf-8\n    if isinstance(value, string_types):\n        if PY2 and isinstance(value, unicode):\n            return value.encode(\"utf-8\")\n        if not PY2 and isinstance(value, str):\n            return value.encode(\"utf-8\")\n\n    return str(value)", "response": "Escape a single value of a URL string or a query parameter."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a URL string from parts omit all None values and empty strings.", "response": "def _make_path(*parts):\n    \"\"\"\n    Create a URL string from parts, omit all `None` values and empty strings.\n    Convert lists and tuples to comma separated values.\n    \"\"\"\n    # TODO: maybe only allow some parts to be lists/tuples ?\n    return \"/\" + \"/\".join(\n        # preserve ',' and '*' in url for nicer URLs in logs\n        quote_plus(_escape(p), b\",*\")\n        for p in parts\n        if p not in SKIP_IN_PATH\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef query_params(*es_query_params):\n\n    def _wrapper(func):\n        @wraps(func)\n        def _wrapped(*args, **kwargs):\n            params = {}\n            if \"params\" in kwargs:\n                params = kwargs.pop(\"params\").copy()\n            for p in es_query_params + GLOBAL_PARAMS:\n                if p in kwargs:\n                    v = kwargs.pop(p)\n                    if v is not None:\n                        params[p] = _escape(v)\n\n            # don't treat ignore and request_timeout as other params to avoid escaping\n            for p in (\"ignore\", \"request_timeout\"):\n                if p in kwargs:\n                    params[p] = kwargs.pop(p)\n            return func(*args, params=params, **kwargs)\n\n        return _wrapped\n\n    return _wrapper", "response": "Decorator that returns a function that takes all accepted parameters from the method s kwargs and puts them in the params argument."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get(self, task_id=None, params=None):\n        return self.transport.perform_request('GET', _make_path('_tasks',\n            task_id), params=params)", "response": "Retrieve information for a particular task."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef print_hits(results):\n    \" Simple utility function to print results of a search query. \"\n    print_search_stats(results)\n    for hit in results['hits']['hits']:\n        # get created date for a repo and fallback to authored_date for a commit\n        created_at = parse_date(hit['_source'].get('created_at', hit['_source']['authored_date']))\n        print('/%s/%s/%s (%s): %s' % (\n                hit['_index'], hit['_type'], hit['_id'],\n                created_at.strftime('%Y-%m-%d'),\n                hit['_source']['description'].split('\\n')[0]))\n\n    print('=' * 80)\n    print()", "response": "Simple utility function to print results of a search query."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef post(self):\n        body = tornado.escape.json_decode(self.request.body)\n\n        try:\n            self._bo.register(\n                params=body[\"params\"],\n                target=body[\"target\"],\n            )\n            print(\"BO has registered: {} points.\".format(len(self._bo.space)), end=\"\\n\\n\")\n        except KeyError:\n            pass\n        finally:\n            suggested_params = self._bo.suggest(self._uf)\n\n        self.write(json.dumps(suggested_params))", "response": "Deal with incoming requests."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef register(self, params, target):\n        self._space.register(params, target)\n        self.dispatch(Events.OPTMIZATION_STEP)", "response": "Expect observation with known target"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef probe(self, params, lazy=True):\n        if lazy:\n            self._queue.add(params)\n        else:\n            self._space.probe(params)\n            self.dispatch(Events.OPTMIZATION_STEP)", "response": "Probe target of x"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef suggest(self, utility_function):\n        if len(self._space) == 0:\n            return self._space.array_to_params(self._space.random_sample())\n\n        # Sklearn's GP throws a large number of warnings at times, but\n        # we don't really need to see them here.\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            self._gp.fit(self._space.params, self._space.target)\n\n        # Finding argmax of the acquisition function.\n        suggestion = acq_max(\n            ac=utility_function.utility,\n            gp=self._gp,\n            y_max=self._space.target.max(),\n            bounds=self._space.bounds,\n            random_state=self._random_state\n        )\n\n        return self._space.array_to_params(suggestion)", "response": "Suggests the most promissing point to probe next."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _prime_queue(self, init_points):\n        if self._queue.empty and self._space.empty:\n            init_points = max(init_points, 1)\n\n        for _ in range(init_points):\n            self._queue.add(self._space.random_sample())", "response": "Make sure there s something in the queue at the very beginning."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef maximize(self,\n                 init_points=5,\n                 n_iter=25,\n                 acq='ucb',\n                 kappa=2.576,\n                 xi=0.0,\n                 **gp_params):\n        \"\"\"Mazimize your function\"\"\"\n        self._prime_subscriptions()\n        self.dispatch(Events.OPTMIZATION_START)\n        self._prime_queue(init_points)\n        self.set_gp_params(**gp_params)\n\n        util = UtilityFunction(kind=acq, kappa=kappa, xi=xi)\n        iteration = 0\n        while not self._queue.empty or iteration < n_iter:\n            try:\n                x_probe = next(self._queue)\n            except StopIteration:\n                x_probe = self.suggest(util)\n                iteration += 1\n\n            self.probe(x_probe, lazy=False)\n\n        self.dispatch(Events.OPTMIZATION_END)", "response": "Maximize the current state of the user."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef register(self, params, target):\n        x = self._as_array(params)\n        if x in self:\n            raise KeyError('Data point {} is not unique'.format(x))\n\n        # Insert data into unique dictionary\n        self._cache[_hashable(x.ravel())] = target\n\n        self._params = np.concatenate([self._params, x.reshape(1, -1)])\n        self._target = np.concatenate([self._target, [target]])", "response": "Add a point and its target value to the known data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef probe(self, params):\n        x = self._as_array(params)\n\n        try:\n            target = self._cache[_hashable(x)]\n        except KeyError:\n            params = dict(zip(self._keys, x))\n            target = self.target_func(**params)\n            self.register(x, target)\n        return target", "response": "Probe a single point x and return the value y."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate random points within the bounds of the space.", "response": "def random_sample(self):\n        \"\"\"\n        Creates random points within the bounds of the space.\n\n        Returns\n        ----------\n        data: ndarray\n            [num x dim] array points with dimensions corresponding to `self._keys`\n\n        Example\n        -------\n        >>> target_func = lambda p1, p2: p1 + p2\n        >>> pbounds = {'p1': (0, 1), 'p2': (1, 100)}\n        >>> space = TargetSpace(target_func, pbounds, random_state=0)\n        >>> space.random_points(1)\n        array([[ 55.33253689,   0.54488318]])\n        \"\"\"\n        # TODO: support integer, category, and basic scipy.optimize constraints\n        data = np.empty((1, self.dim))\n        for col, (lower, upper) in enumerate(self._bounds):\n            data.T[col] = self.random_state.uniform(lower, upper, size=1)\n        return data.ravel()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef max(self):\n        try:\n            res = {\n                'target': self.target.max(),\n                'params': dict(\n                    zip(self.keys, self.params[self.target.argmax()])\n                )\n            }\n        except ValueError:\n            res = {}\n        return res", "response": "Get maximum target value found and corresponding parametes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef res(self):\n        params = [dict(zip(self.keys, p)) for p in self.params]\n\n        return [\n            {\"target\": target, \"params\": param}\n            for target, param in zip(self.target, params)\n        ]", "response": "Get all target values found and corresponding parametes."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_bounds(self, new_bounds):\n        for row, key in enumerate(self.keys):\n            if key in new_bounds:\n                self._bounds[row] = new_bounds[key]", "response": "A method that allows changing the lower and upper search bounds of the items in the log file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef rfc_cv(n_estimators, min_samples_split, max_features, data, targets):\n    estimator = RFC(\n        n_estimators=n_estimators,\n        min_samples_split=min_samples_split,\n        max_features=max_features,\n        random_state=2\n    )\n    cval = cross_val_score(estimator, data, targets,\n                           scoring='neg_log_loss', cv=4)\n    return cval.mean()", "response": "This function is used to create a random forest classifier that cross - validation is performed on the data and targets."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef optimize_svc(data, targets):\n    def svc_crossval(expC, expGamma):\n        \"\"\"Wrapper of SVC cross validation.\n\n        Notice how we transform between regular and log scale. While this\n        is not technically necessary, it greatly improves the performance\n        of the optimizer.\n        \"\"\"\n        C = 10 ** expC\n        gamma = 10 ** expGamma\n        return svc_cv(C=C, gamma=gamma, data=data, targets=targets)\n\n    optimizer = BayesianOptimization(\n        f=svc_crossval,\n        pbounds={\"expC\": (-3, 2), \"expGamma\": (-4, -1)},\n        random_state=1234,\n        verbose=2\n    )\n    optimizer.maximize(n_iter=10)\n\n    print(\"Final result:\", optimizer.max)", "response": "Apply Bayesian Optimization to SVC parameters."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\napplies Bayesian Optimization to Random Forest parameters.", "response": "def optimize_rfc(data, targets):\n    \"\"\"Apply Bayesian Optimization to Random Forest parameters.\"\"\"\n    def rfc_crossval(n_estimators, min_samples_split, max_features):\n        \"\"\"Wrapper of RandomForest cross validation.\n\n        Notice how we ensure n_estimators and min_samples_split are casted\n        to integer before we pass them along. Moreover, to avoid max_features\n        taking values outside the (0, 1) range, we also ensure it is capped\n        accordingly.\n        \"\"\"\n        return rfc_cv(\n            n_estimators=int(n_estimators),\n            min_samples_split=int(min_samples_split),\n            max_features=max(min(max_features, 0.999), 1e-3),\n            data=data,\n            targets=targets,\n        )\n\n    optimizer = BayesianOptimization(\n        f=rfc_crossval,\n        pbounds={\n            \"n_estimators\": (10, 250),\n            \"min_samples_split\": (2, 25),\n            \"max_features\": (0.1, 0.999),\n        },\n        random_state=1234,\n        verbose=2\n    )\n    optimizer.maximize(n_iter=10)\n\n    print(\"Final result:\", optimizer.max)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef acq_max(ac, gp, y_max, bounds, random_state, n_warmup=100000, n_iter=250):\n\n    # Warm up with random points\n    x_tries = random_state.uniform(bounds[:, 0], bounds[:, 1],\n                                   size=(n_warmup, bounds.shape[0]))\n    ys = ac(x_tries, gp=gp, y_max=y_max)\n    x_max = x_tries[ys.argmax()]\n    max_acq = ys.max()\n\n    # Explore the parameter space more throughly\n    x_seeds = random_state.uniform(bounds[:, 0], bounds[:, 1],\n                                   size=(n_iter, bounds.shape[0]))\n    for x_try in x_seeds:\n        # Find the minimum of minus the acquisition function\n        res = minimize(lambda x: -ac(x.reshape(1, -1), gp=gp, y_max=y_max),\n                       x_try.reshape(1, -1),\n                       bounds=bounds,\n                       method=\"L-BFGS-B\")\n\n        # See if success\n        if not res.success:\n            continue\n\n        # Store it if better than previous minimum(maximum).\n        if max_acq is None or -res.fun[0] >= max_acq:\n            x_max = res.x\n            max_acq = -res.fun[0]\n\n    # Clip output to make sure it lies within the bounds. Due to floating\n    # point technicalities this is not always the case.\n    return np.clip(x_max, bounds[:, 0], bounds[:, 1])", "response": "A function to find the maximum of the acquisition function."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ensure_rng(random_state=None):\n    if random_state is None:\n        random_state = np.random.RandomState()\n    elif isinstance(random_state, int):\n        random_state = np.random.RandomState(random_state)\n    else:\n        assert isinstance(random_state, np.random.RandomState)\n    return random_state", "response": "Ensures that a random number generator is generated based on an optional seed."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef expand_abbreviations(template, abbreviations):\n    if template in abbreviations:\n        return abbreviations[template]\n\n    # Split on colon. If there is no colon, rest will be empty\n    # and prefix will be the whole template\n    prefix, sep, rest = template.partition(':')\n    if prefix in abbreviations:\n        return abbreviations[prefix].format(rest)\n\n    return template", "response": "Expand abbreviations in a project template name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndetermine if the repository directory contains a cookiecutter. json file.", "response": "def repository_has_cookiecutter_json(repo_directory):\n    \"\"\"Determine if `repo_directory` contains a `cookiecutter.json` file.\n\n    :param repo_directory: The candidate repository directory.\n    :return: True if the `repo_directory` is valid, else False.\n    \"\"\"\n    repo_directory_exists = os.path.isdir(repo_directory)\n\n    repo_config_exists = os.path.isfile(\n        os.path.join(repo_directory, 'cookiecutter.json')\n    )\n    return repo_directory_exists and repo_config_exists"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef determine_repo_dir(template, abbreviations, clone_to_dir, checkout,\n                       no_input, password=None):\n    \"\"\"\n    Locate the repository directory from a template reference.\n\n    Applies repository abbreviations to the template reference.\n    If the template refers to a repository URL, clone it.\n    If the template is a path to a local repository, use it.\n\n    :param template: A directory containing a project template directory,\n        or a URL to a git repository.\n    :param abbreviations: A dictionary of repository abbreviation\n        definitions.\n    :param clone_to_dir: The directory to clone the repository into.\n    :param checkout: The branch, tag or commit ID to checkout after clone.\n    :param no_input: Prompt the user at command line for manual configuration?\n    :param password: The password to use when extracting the repository.\n    :return: A tuple containing the cookiecutter template directory, and\n        a boolean descriving whether that directory should be cleaned up\n        after the template has been instantiated.\n    :raises: `RepositoryNotFound` if a repository directory could not be found.\n    \"\"\"\n    template = expand_abbreviations(template, abbreviations)\n\n    if is_zip_file(template):\n        unzipped_dir = unzip(\n            zip_uri=template,\n            is_url=is_repo_url(template),\n            clone_to_dir=clone_to_dir,\n            no_input=no_input,\n            password=password\n        )\n        repository_candidates = [unzipped_dir]\n        cleanup = True\n    elif is_repo_url(template):\n        cloned_repo = clone(\n            repo_url=template,\n            checkout=checkout,\n            clone_to_dir=clone_to_dir,\n            no_input=no_input,\n        )\n        repository_candidates = [cloned_repo]\n        cleanup = False\n    else:\n        repository_candidates = [\n            template,\n            os.path.join(clone_to_dir, template)\n        ]\n        cleanup = False\n\n    for repo_candidate in repository_candidates:\n        if repository_has_cookiecutter_json(repo_candidate):\n            return repo_candidate, cleanup\n\n    raise RepositoryNotFound(\n        'A valid repository for \"{}\" could not be found in the following '\n        'locations:\\n{}'.format(\n            template,\n            '\\n'.join(repository_candidates)\n        )\n    )", "response": "Locate the repository directory from a template reference."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfinding the project template in the given repo_dir.", "response": "def find_template(repo_dir):\n    \"\"\"Determine which child directory of `repo_dir` is the project template.\n\n    :param repo_dir: Local directory of newly cloned repo.\n    :returns project_template: Relative path to project template.\n    \"\"\"\n    logger.debug('Searching {} for the project template.'.format(repo_dir))\n\n    repo_dir_contents = os.listdir(repo_dir)\n\n    project_template = None\n    for item in repo_dir_contents:\n        if 'cookiecutter' in item and '{{' in item and '}}' in item:\n            project_template = item\n            break\n\n    if project_template:\n        project_template = os.path.join(repo_dir, project_template)\n        logger.debug(\n            'The project template appears to be {}'.format(project_template)\n        )\n        return project_template\n    else:\n        raise NonTemplatedInputDirException"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck whether the given path should only be copied and not rendered.", "response": "def is_copy_only_path(path, context):\n    \"\"\"Check whether the given `path` should only be copied and not rendered.\n\n    Returns True if `path` matches a pattern in the given `context` dict,\n    otherwise False.\n\n    :param path: A file-system path referring to a file or dir that\n        should be rendered or just copied.\n    :param context: cookiecutter context.\n    \"\"\"\n    try:\n        for dont_render in context['cookiecutter']['_copy_without_render']:\n            if fnmatch.fnmatch(path, dont_render):\n                return True\n    except KeyError:\n        return False\n\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmodifies the given context in place based on the overwrite_context.", "response": "def apply_overwrites_to_context(context, overwrite_context):\n    \"\"\"Modify the given context in place based on the overwrite_context.\"\"\"\n    for variable, overwrite in overwrite_context.items():\n        if variable not in context:\n            # Do not include variables which are not used in the template\n            continue\n\n        context_value = context[variable]\n\n        if isinstance(context_value, list):\n            # We are dealing with a choice variable\n            if overwrite in context_value:\n                # This overwrite is actually valid for the given context\n                # Let's set it as default (by definition first item in list)\n                # see ``cookiecutter.prompt.prompt_choice_for_config``\n                context_value.remove(overwrite)\n                context_value.insert(0, overwrite)\n        else:\n            # Simply overwrite the value for this variable\n            context[variable] = overwrite"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates a Cookiecutter project template.", "response": "def generate_context(context_file='cookiecutter.json', default_context=None,\n                     extra_context=None):\n    \"\"\"Generate the context for a Cookiecutter project template.\n\n    Loads the JSON file as a Python object, with key being the JSON filename.\n\n    :param context_file: JSON file containing key/value pairs for populating\n        the cookiecutter's variables.\n    :param default_context: Dictionary containing config to take into account.\n    :param extra_context: Dictionary containing configuration overrides\n    \"\"\"\n    context = OrderedDict([])\n\n    try:\n        with open(context_file) as file_handle:\n            obj = json.load(file_handle, object_pairs_hook=OrderedDict)\n    except ValueError as e:\n        # JSON decoding error.  Let's throw a new exception that is more\n        # friendly for the developer or user.\n        full_fpath = os.path.abspath(context_file)\n        json_exc_message = str(e)\n        our_exc_message = (\n            'JSON decoding error while loading \"{0}\".  Decoding'\n            ' error details: \"{1}\"'.format(full_fpath, json_exc_message))\n        raise ContextDecodingException(our_exc_message)\n\n    # Add the Python object to the context dictionary\n    file_name = os.path.split(context_file)[1]\n    file_stem = file_name.split('.')[0]\n    context[file_stem] = obj\n\n    # Overwrite context variable defaults with the default context from the\n    # user's global config, if available\n    if default_context:\n        apply_overwrites_to_context(obj, default_context)\n    if extra_context:\n        apply_overwrites_to_context(obj, extra_context)\n\n    logger.debug('Context generated is {}'.format(context))\n    return context"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates a file from the given infile and context.", "response": "def generate_file(project_dir, infile, context, env):\n    \"\"\"Render filename of infile as name of outfile, handle infile correctly.\n\n    Dealing with infile appropriately:\n\n        a. If infile is a binary file, copy it over without rendering.\n        b. If infile is a text file, render its contents and write the\n           rendered infile to outfile.\n\n    Precondition:\n\n        When calling `generate_file()`, the root template dir must be the\n        current working directory. Using `utils.work_in()` is the recommended\n        way to perform this directory change.\n\n    :param project_dir: Absolute path to the resulting generated project.\n    :param infile: Input file to generate the file from. Relative to the root\n        template dir.\n    :param context: Dict for populating the cookiecutter's variables.\n    :param env: Jinja2 template execution environment.\n    \"\"\"\n    logger.debug('Processing file {}'.format(infile))\n\n    # Render the path to the output file (not including the root project dir)\n    outfile_tmpl = env.from_string(infile)\n\n    outfile = os.path.join(project_dir, outfile_tmpl.render(**context))\n    file_name_is_empty = os.path.isdir(outfile)\n    if file_name_is_empty:\n        logger.debug('The resulting file name is empty: {0}'.format(outfile))\n        return\n\n    logger.debug('Created file at {0}'.format(outfile))\n\n    # Just copy over binary files. Don't render.\n    logger.debug(\"Check {} to see if it's a binary\".format(infile))\n    if is_binary(infile):\n        logger.debug(\n            'Copying binary {} to {} without rendering'\n            ''.format(infile, outfile)\n        )\n        shutil.copyfile(infile, outfile)\n    else:\n        # Force fwd slashes on Windows for get_template\n        # This is a by-design Jinja issue\n        infile_fwd_slashes = infile.replace(os.path.sep, '/')\n\n        # Render the file\n        try:\n            tmpl = env.get_template(infile_fwd_slashes)\n        except TemplateSyntaxError as exception:\n            # Disable translated so that printed exception contains verbose\n            # information about syntax error location\n            exception.translated = False\n            raise\n        rendered_file = tmpl.render(**context)\n\n        logger.debug('Writing contents to file {}'.format(outfile))\n\n        with io.open(outfile, 'w', encoding='utf-8') as fh:\n            fh.write(rendered_file)\n\n    # Apply file permissions to output file\n    shutil.copymode(infile, outfile)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef render_and_create_dir(dirname, context, output_dir, environment,\n                          overwrite_if_exists=False):\n    \"\"\"Render name of a directory, create the directory, return its path.\"\"\"\n    name_tmpl = environment.from_string(dirname)\n    rendered_dirname = name_tmpl.render(**context)\n\n    dir_to_create = os.path.normpath(\n        os.path.join(output_dir, rendered_dirname)\n    )\n\n    logger.debug('Rendered dir {} must exist in output_dir {}'.format(\n        dir_to_create,\n        output_dir\n    ))\n\n    output_dir_exists = os.path.exists(dir_to_create)\n\n    if output_dir_exists:\n        if overwrite_if_exists:\n            logger.debug(\n                'Output directory {} already exists,'\n                'overwriting it'.format(dir_to_create)\n            )\n        else:\n            msg = 'Error: \"{}\" directory already exists'.format(dir_to_create)\n            raise OutputDirExistsException(msg)\n    else:\n        make_sure_path_exists(dir_to_create)\n\n    return dir_to_create, not output_dir_exists", "response": "Render name of a directory create the directory and return its path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _run_hook_from_repo_dir(repo_dir, hook_name, project_dir, context,\n                            delete_project_on_failure):\n    \"\"\"Run hook from repo directory, clean project directory if hook fails.\n\n    :param repo_dir: Project template input directory.\n    :param hook_name: The hook to execute.\n    :param project_dir: The directory to execute the script from.\n    :param context: Cookiecutter project context.\n    :param delete_project_on_failure: Delete the project directory on hook\n        failure?\n    \"\"\"\n    with work_in(repo_dir):\n        try:\n            run_hook(hook_name, project_dir, context)\n        except FailedHookException:\n            if delete_project_on_failure:\n                rmtree(project_dir)\n            logger.error(\n                \"Stopping generation because {} hook \"\n                \"script didn't exit successfully\".format(hook_name)\n            )\n            raise", "response": "Run a hook from a repo directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef generate_files(repo_dir, context=None, output_dir='.',\n                   overwrite_if_exists=False):\n    \"\"\"Render the templates and saves them to files.\n\n    :param repo_dir: Project template input directory.\n    :param context: Dict for populating the template's variables.\n    :param output_dir: Where to output the generated project dir into.\n    :param overwrite_if_exists: Overwrite the contents of the output directory\n        if it exists.\n    \"\"\"\n    template_dir = find_template(repo_dir)\n    logger.debug('Generating project from {}...'.format(template_dir))\n    context = context or OrderedDict([])\n\n    unrendered_dir = os.path.split(template_dir)[1]\n    ensure_dir_is_templated(unrendered_dir)\n    env = StrictEnvironment(\n        context=context,\n        keep_trailing_newline=True,\n    )\n    try:\n        project_dir, output_directory_created = render_and_create_dir(\n            unrendered_dir,\n            context,\n            output_dir,\n            env,\n            overwrite_if_exists\n        )\n    except UndefinedError as err:\n        msg = \"Unable to create project directory '{}'\".format(unrendered_dir)\n        raise UndefinedVariableInTemplate(msg, err, context)\n\n    # We want the Jinja path and the OS paths to match. Consequently, we'll:\n    #   + CD to the template folder\n    #   + Set Jinja's path to '.'\n    #\n    #  In order to build our files to the correct folder(s), we'll use an\n    # absolute path for the target folder (project_dir)\n\n    project_dir = os.path.abspath(project_dir)\n    logger.debug('Project directory is {}'.format(project_dir))\n\n    # if we created the output directory, then it's ok to remove it\n    # if rendering fails\n    delete_project_on_failure = output_directory_created\n\n    _run_hook_from_repo_dir(\n        repo_dir,\n        'pre_gen_project',\n        project_dir,\n        context,\n        delete_project_on_failure\n    )\n\n    with work_in(template_dir):\n        env.loader = FileSystemLoader('.')\n\n        for root, dirs, files in os.walk('.'):\n            # We must separate the two types of dirs into different lists.\n            # The reason is that we don't want ``os.walk`` to go through the\n            # unrendered directories, since they will just be copied.\n            copy_dirs = []\n            render_dirs = []\n\n            for d in dirs:\n                d_ = os.path.normpath(os.path.join(root, d))\n                # We check the full path, because that's how it can be\n                # specified in the ``_copy_without_render`` setting, but\n                # we store just the dir name\n                if is_copy_only_path(d_, context):\n                    copy_dirs.append(d)\n                else:\n                    render_dirs.append(d)\n\n            for copy_dir in copy_dirs:\n                indir = os.path.normpath(os.path.join(root, copy_dir))\n                outdir = os.path.normpath(os.path.join(project_dir, indir))\n                logger.debug(\n                    'Copying dir {} to {} without rendering'\n                    ''.format(indir, outdir)\n                )\n                shutil.copytree(indir, outdir)\n\n            # We mutate ``dirs``, because we only want to go through these dirs\n            # recursively\n            dirs[:] = render_dirs\n            for d in dirs:\n                unrendered_dir = os.path.join(project_dir, root, d)\n                try:\n                    render_and_create_dir(\n                        unrendered_dir,\n                        context,\n                        output_dir,\n                        env,\n                        overwrite_if_exists\n                    )\n                except UndefinedError as err:\n                    if delete_project_on_failure:\n                        rmtree(project_dir)\n                    _dir = os.path.relpath(unrendered_dir, output_dir)\n                    msg = \"Unable to create directory '{}'\".format(_dir)\n                    raise UndefinedVariableInTemplate(msg, err, context)\n\n            for f in files:\n                infile = os.path.normpath(os.path.join(root, f))\n                if is_copy_only_path(infile, context):\n                    outfile_tmpl = env.from_string(infile)\n                    outfile_rendered = outfile_tmpl.render(**context)\n                    outfile = os.path.join(project_dir, outfile_rendered)\n                    logger.debug(\n                        'Copying file {} to {} without rendering'\n                        ''.format(infile, outfile)\n                    )\n                    shutil.copyfile(infile, outfile)\n                    shutil.copymode(infile, outfile)\n                    continue\n                try:\n                    generate_file(project_dir, infile, context, env)\n                except UndefinedError as err:\n                    if delete_project_on_failure:\n                        rmtree(project_dir)\n                    msg = \"Unable to create file '{}'\".format(infile)\n                    raise UndefinedVariableInTemplate(msg, err, context)\n\n    _run_hook_from_repo_dir(\n        repo_dir,\n        'post_gen_project',\n        project_dir,\n        context,\n        delete_project_on_failure\n    )\n\n    return project_dir", "response": "Generate the files for the current project."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nexpands both environment variables and user home in the given path.", "response": "def _expand_path(path):\n    \"\"\"Expand both environment variables and user home in the given path.\"\"\"\n    path = os.path.expandvars(path)\n    path = os.path.expanduser(path)\n    return path"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nretrieve the config from the specified path returning a config dict.", "response": "def get_config(config_path):\n    \"\"\"Retrieve the config from the specified path, returning a config dict.\"\"\"\n    if not os.path.exists(config_path):\n        raise ConfigDoesNotExistException\n\n    logger.debug('config_path is {0}'.format(config_path))\n    with io.open(config_path, encoding='utf-8') as file_handle:\n        try:\n            yaml_dict = poyo.parse_string(file_handle.read())\n        except poyo.exceptions.PoyoException as e:\n            raise InvalidConfiguration(\n                'Unable to parse YAML file {}. Error: {}'\n                ''.format(config_path, e)\n            )\n\n    config_dict = merge_configs(DEFAULT_CONFIG, yaml_dict)\n\n    raw_replay_dir = config_dict['replay_dir']\n    config_dict['replay_dir'] = _expand_path(raw_replay_dir)\n\n    raw_cookies_dir = config_dict['cookiecutters_dir']\n    config_dict['cookiecutters_dir'] = _expand_path(raw_cookies_dir)\n\n    return config_dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_user_config(config_file=None, default_config=False):\n    # Do NOT load a config. Return defaults instead.\n    if default_config:\n        return copy.copy(DEFAULT_CONFIG)\n\n    # Load the given config file\n    if config_file and config_file is not USER_CONFIG_PATH:\n        return get_config(config_file)\n\n    try:\n        # Does the user set up a config environment variable?\n        env_config_file = os.environ['COOKIECUTTER_CONFIG']\n    except KeyError:\n        # Load an optional user config if it exists\n        # otherwise return the defaults\n        if os.path.exists(USER_CONFIG_PATH):\n            return get_config(USER_CONFIG_PATH)\n        else:\n            return copy.copy(DEFAULT_CONFIG)\n    else:\n        # There is a config environment variable. Try to load it.\n        # Do not check for existence, so invalid file paths raise an error.\n        return get_config(env_config_file)", "response": "Return the user config as a dict."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nforce deletion of a file.", "response": "def force_delete(func, path, exc_info):\n    \"\"\"Error handler for `shutil.rmtree()` equivalent to `rm -rf`.\n\n    Usage: `shutil.rmtree(path, onerror=force_delete)`\n    From stackoverflow.com/questions/1889597\n    \"\"\"\n    os.chmod(path, stat.S_IWRITE)\n    func(path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef make_sure_path_exists(path):\n    logger.debug('Making sure path exists: {}'.format(path))\n    try:\n        os.makedirs(path)\n        logger.debug('Created directory at: {}'.format(path))\n    except OSError as exception:\n        if exception.errno != errno.EEXIST:\n            return False\n    return True", "response": "Ensures that a directory exists."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmakes the script_path executable.", "response": "def make_executable(script_path):\n    \"\"\"Make `script_path` executable.\n\n    :param script_path: The file to change\n    \"\"\"\n    status = os.stat(script_path)\n    os.chmod(script_path, status.st_mode | stat.S_IEXEC)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprompts user if the file or directory is okay to delete and re - download it.", "response": "def prompt_and_delete(path, no_input=False):\n    \"\"\"\n    Ask user if it's okay to delete the previously-downloaded file/directory.\n\n    If yes, delete it. If no, checks to see if the old version should be\n    reused. If yes, it's reused; otherwise, Cookiecutter exits.\n\n    :param path: Previously downloaded zipfile.\n    :param no_input: Suppress prompt to delete repo and just delete it.\n    :return: True if the content was deleted\n    \"\"\"\n    # Suppress prompt if called via API\n    if no_input:\n        ok_to_delete = True\n    else:\n        question = (\n            \"You've downloaded {} before. \"\n            \"Is it okay to delete and re-download it?\"\n        ).format(path)\n\n        ok_to_delete = read_user_yes_no(question, 'yes')\n\n    if ok_to_delete:\n        if os.path.isdir(path):\n            rmtree(path)\n        else:\n            os.remove(path)\n        return True\n    else:\n        ok_to_reuse = read_user_yes_no(\n            \"Do you want to re-use the existing version?\", 'yes'\n        )\n\n        if ok_to_reuse:\n            return False\n\n        sys.exit()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndownloads and unpack a zipfile at a given URI.", "response": "def unzip(zip_uri, is_url, clone_to_dir='.', no_input=False, password=None):\n    \"\"\"Download and unpack a zipfile at a given URI.\n\n    This will download the zipfile to the cookiecutter repository,\n    and unpack into a temporary directory.\n\n    :param zip_uri: The URI for the zipfile.\n    :param is_url: Is the zip URI a URL or a file?\n    :param clone_to_dir: The cookiecutter repository directory\n        to put the archive into.\n    :param no_input: Supress any prompts\n    :param password: The password to use when unpacking the repository.\n    \"\"\"\n    # Ensure that clone_to_dir exists\n    clone_to_dir = os.path.expanduser(clone_to_dir)\n    make_sure_path_exists(clone_to_dir)\n\n    if is_url:\n        # Build the name of the cached zipfile,\n        # and prompt to delete if it already exists.\n        identifier = zip_uri.rsplit('/', 1)[1]\n        zip_path = os.path.join(clone_to_dir, identifier)\n\n        if os.path.exists(zip_path):\n            download = prompt_and_delete(zip_path, no_input=no_input)\n        else:\n            download = True\n\n        if download:\n            # (Re) download the zipfile\n            r = requests.get(zip_uri, stream=True)\n            with open(zip_path, 'wb') as f:\n                for chunk in r.iter_content(chunk_size=1024):\n                    if chunk:  # filter out keep-alive new chunks\n                        f.write(chunk)\n    else:\n        # Just use the local zipfile as-is.\n        zip_path = os.path.abspath(zip_uri)\n\n    # Now unpack the repository. The zipfile will be unpacked\n    # into a temporary directory\n    try:\n        zip_file = ZipFile(zip_path)\n\n        if len(zip_file.namelist()) == 0:\n            raise InvalidZipRepository(\n                'Zip repository {} is empty'.format(zip_uri)\n            )\n\n        # The first record in the zipfile should be the directory entry for\n        # the archive. If it isn't a directory, there's a problem.\n        first_filename = zip_file.namelist()[0]\n        if not first_filename.endswith('/'):\n            raise InvalidZipRepository(\n                'Zip repository {} does not include '\n                'a top-level directory'.format(zip_uri)\n            )\n\n        # Construct the final target directory\n        project_name = first_filename[:-1]\n        unzip_base = tempfile.mkdtemp()\n        unzip_path = os.path.join(unzip_base, project_name)\n\n        # Extract the zip file into the temporary directory\n        try:\n            zip_file.extractall(path=unzip_base)\n        except RuntimeError:\n            # File is password protected; try to get a password from the\n            # environment; if that doesn't work, ask the user.\n            if password is not None:\n                try:\n                    zip_file.extractall(\n                        path=unzip_base,\n                        pwd=password.encode('utf-8')\n                    )\n                except RuntimeError:\n                    raise InvalidZipRepository(\n                        'Invalid password provided for protected repository'\n                    )\n            elif no_input:\n                raise InvalidZipRepository(\n                    'Unable to unlock password protected repository'\n                )\n            else:\n                retry = 0\n                while retry is not None:\n                    try:\n                        password = read_repo_password('Repo password')\n                        zip_file.extractall(\n                            path=unzip_base,\n                            pwd=password.encode('utf-8')\n                        )\n                        retry = None\n                    except RuntimeError:\n                        retry += 1\n                        if retry == 3:\n                            raise InvalidZipRepository(\n                                'Invalid password provided '\n                                'for protected repository'\n                            )\n\n    except BadZipFile:\n        raise InvalidZipRepository(\n            'Zip repository {} is not a valid zip archive:'.format(zip_uri)\n        )\n\n    return unzip_path"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cookiecutter(\n        template, checkout=None, no_input=False, extra_context=None,\n        replay=False, overwrite_if_exists=False, output_dir='.',\n        config_file=None, default_config=False, password=None):\n    \"\"\"\n    Run Cookiecutter just as if using it from the command line.\n\n    :param template: A directory containing a project template directory,\n        or a URL to a git repository.\n    :param checkout: The branch, tag or commit ID to checkout after clone.\n    :param no_input: Prompt the user at command line for manual configuration?\n    :param extra_context: A dictionary of context that overrides default\n        and user configuration.\n    :param: overwrite_if_exists: Overwrite the contents of output directory\n        if it exists\n    :param output_dir: Where to output the generated project dir into.\n    :param config_file: User configuration file path.\n    :param default_config: Use default values rather than a config file.\n    :param password: The password to use when extracting the repository.\n    \"\"\"\n    if replay and ((no_input is not False) or (extra_context is not None)):\n        err_msg = (\n            \"You can not use both replay and no_input or extra_context \"\n            \"at the same time.\"\n        )\n        raise InvalidModeException(err_msg)\n\n    config_dict = get_user_config(\n        config_file=config_file,\n        default_config=default_config,\n    )\n\n    repo_dir, cleanup = determine_repo_dir(\n        template=template,\n        abbreviations=config_dict['abbreviations'],\n        clone_to_dir=config_dict['cookiecutters_dir'],\n        checkout=checkout,\n        no_input=no_input,\n        password=password\n    )\n\n    template_name = os.path.basename(os.path.abspath(repo_dir))\n\n    if replay:\n        context = load(config_dict['replay_dir'], template_name)\n    else:\n        context_file = os.path.join(repo_dir, 'cookiecutter.json')\n        logger.debug('context_file is {}'.format(context_file))\n\n        context = generate_context(\n            context_file=context_file,\n            default_context=config_dict['default_context'],\n            extra_context=extra_context,\n        )\n\n        # prompt the user to manually configure at the command line.\n        # except when 'no-input' flag is set\n        context['cookiecutter'] = prompt_for_config(context, no_input)\n\n        # include template dir or url in the context dict\n        context['cookiecutter']['_template'] = template\n\n        dump(config_dict['replay_dir'], template_name, context)\n\n    # Create project from local context and project template.\n    result = generate_files(\n        repo_dir=repo_dir,\n        context=context,\n        overwrite_if_exists=overwrite_if_exists,\n        output_dir=output_dir\n    )\n\n    # Cleanup (if required)\n    if cleanup:\n        rmtree(repo_dir)\n\n    return result", "response": "Run Cookiecutter for a single project."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef read_user_yes_no(question, default_value):\n    # Please see http://click.pocoo.org/4/api/#click.prompt\n    return click.prompt(\n        question,\n        default=default_value,\n        type=click.BOOL\n    )", "response": "Prompt the user to reply with yes or no."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprompt the user to choose from several options for the given variable.", "response": "def read_user_choice(var_name, options):\n    \"\"\"Prompt the user to choose from several options for the given variable.\n\n    The first item will be returned if no input happens.\n\n    :param str var_name: Variable as specified in the context\n    :param list options: Sequence of options that are available to select from\n    :return: Exactly one item of ``options`` that has been chosen by the user\n    \"\"\"\n    # Please see http://click.pocoo.org/4/api/#click.prompt\n    if not isinstance(options, list):\n        raise TypeError\n\n    if not options:\n        raise ValueError\n\n    choice_map = OrderedDict(\n        (u'{}'.format(i), value) for i, value in enumerate(options, 1)\n    )\n    choices = choice_map.keys()\n    default = u'1'\n\n    choice_lines = [u'{} - {}'.format(*c) for c in choice_map.items()]\n    prompt = u'\\n'.join((\n        u'Select {}:'.format(var_name),\n        u'\\n'.join(choice_lines),\n        u'Choose from {}'.format(u', '.join(choices))\n    ))\n\n    user_choice = click.prompt(\n        prompt, type=click.Choice(choices), default=default\n    )\n    return choice_map[user_choice]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nprompting the user to provide a dictionary of data.", "response": "def read_user_dict(var_name, default_value):\n    \"\"\"Prompt the user to provide a dictionary of data.\n\n    :param str var_name: Variable as specified in the context\n    :param default_value: Value that will be returned if no input is provided\n    :return: A Python dictionary to use in the context.\n    \"\"\"\n    # Please see http://click.pocoo.org/4/api/#click.prompt\n    if not isinstance(default_value, dict):\n        raise TypeError\n\n    default_display = 'default'\n\n    user_value = click.prompt(\n        var_name,\n        default=default_display,\n        type=click.STRING,\n        value_proc=process_json,\n    )\n\n    if user_value == default_display:\n        # Return the given default w/o any processing\n        return default_value\n    return user_value"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nrenders the next variable in the cookiecutter. json file.", "response": "def render_variable(env, raw, cookiecutter_dict):\n    \"\"\"Inside the prompting taken from the cookiecutter.json file, this renders\n    the next variable. For example, if a project_name is \"Peanut Butter\n    Cookie\", the repo_name could be be rendered with:\n\n        `{{ cookiecutter.project_name.replace(\" \", \"_\") }}`.\n\n    This is then presented to the user as the default.\n\n    :param Environment env: A Jinja2 Environment object.\n    :param str raw: The next value to be prompted for by the user.\n    :param dict cookiecutter_dict: The current context as it's gradually\n        being populated with variables.\n    :return: The rendered value for the default variable.\n    \"\"\"\n    if raw is None:\n        return None\n    elif isinstance(raw, dict):\n        return {\n            render_variable(env, k, cookiecutter_dict):\n                render_variable(env, v, cookiecutter_dict)\n            for k, v in raw.items()\n        }\n    elif isinstance(raw, list):\n        return [\n            render_variable(env, v, cookiecutter_dict)\n            for v in raw\n        ]\n    elif not isinstance(raw, basestring):\n        raw = str(raw)\n\n    template = env.from_string(raw)\n\n    rendered_template = template.render(cookiecutter=cookiecutter_dict)\n    return rendered_template"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef prompt_choice_for_config(cookiecutter_dict, env, key, options, no_input):\n    rendered_options = [\n        render_variable(env, raw, cookiecutter_dict) for raw in options\n    ]\n\n    if no_input:\n        return rendered_options[0]\n    return read_user_choice(key, rendered_options)", "response": "Prompt the user which option to choose from the given config."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprompt the user to enter a new config using the context as a source for the order of the fields and sample values.", "response": "def prompt_for_config(context, no_input=False):\n    \"\"\"\n    Prompts the user to enter new config, using context as a source for the\n    field names and sample values.\n\n    :param no_input: Prompt the user at command line for manual configuration?\n    \"\"\"\n    cookiecutter_dict = OrderedDict([])\n    env = StrictEnvironment(context=context)\n\n    # First pass: Handle simple and raw variables, plus choices.\n    # These must be done first because the dictionaries keys and\n    # values might refer to them.\n    for key, raw in iteritems(context[u'cookiecutter']):\n        if key.startswith(u'_'):\n            cookiecutter_dict[key] = raw\n            continue\n\n        try:\n            if isinstance(raw, list):\n                # We are dealing with a choice variable\n                val = prompt_choice_for_config(\n                    cookiecutter_dict, env, key, raw, no_input\n                )\n                cookiecutter_dict[key] = val\n            elif not isinstance(raw, dict):\n                # We are dealing with a regular variable\n                val = render_variable(env, raw, cookiecutter_dict)\n\n                if not no_input:\n                    val = read_user_variable(key, val)\n\n                cookiecutter_dict[key] = val\n        except UndefinedError as err:\n            msg = \"Unable to render variable '{}'\".format(key)\n            raise UndefinedVariableInTemplate(msg, err, context)\n\n    # Second pass; handle the dictionaries.\n    for key, raw in iteritems(context[u'cookiecutter']):\n\n        try:\n            if isinstance(raw, dict):\n                # We are dealing with a dict variable\n                val = render_variable(env, raw, cookiecutter_dict)\n\n                if not no_input:\n                    val = read_user_dict(key, val)\n\n                cookiecutter_dict[key] = val\n        except UndefinedError as err:\n            msg = \"Unable to render variable '{}'\".format(key)\n            raise UndefinedVariableInTemplate(msg, err, context)\n\n    return cookiecutter_dict"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _read_extensions(self, context):\n        try:\n            extensions = context['cookiecutter']['_extensions']\n        except KeyError:\n            return []\n        else:\n            return [str(ext) for ext in extensions]", "response": "Return list of extensions as str to be passed on to the Jinja2 env."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef configure_logger(stream_level='DEBUG', debug_file=None):\n    # Set up 'cookiecutter' logger\n    logger = logging.getLogger('cookiecutter')\n    logger.setLevel(logging.DEBUG)\n\n    # Remove all attached handlers, in case there was\n    # a logger with using the name 'cookiecutter'\n    del logger.handlers[:]\n\n    # Create a file handler if a log file is provided\n    if debug_file is not None:\n        debug_formatter = logging.Formatter(LOG_FORMATS['DEBUG'])\n        file_handler = logging.FileHandler(debug_file)\n        file_handler.setLevel(LOG_LEVELS['DEBUG'])\n        file_handler.setFormatter(debug_formatter)\n        logger.addHandler(file_handler)\n\n    # Get settings based on the given stream_level\n    log_formatter = logging.Formatter(LOG_FORMATS[stream_level])\n    log_level = LOG_LEVELS[stream_level]\n\n    # Create a stream handler\n    stream_handler = logging.StreamHandler(stream=sys.stdout)\n    stream_handler.setLevel(log_level)\n    stream_handler.setFormatter(log_formatter)\n    logger.addHandler(stream_handler)\n\n    return logger", "response": "Configure logging for cookiecutter."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef identify_repo(repo_url):\n    repo_url_values = repo_url.split('+')\n    if len(repo_url_values) == 2:\n        repo_type = repo_url_values[0]\n        if repo_type in [\"git\", \"hg\"]:\n            return repo_type, repo_url_values[1]\n        else:\n            raise UnknownRepoType\n    else:\n        if 'git' in repo_url:\n            return 'git', repo_url\n        elif 'bitbucket' in repo_url:\n            return 'hg', repo_url\n        else:\n            raise UnknownRepoType", "response": "Identify a git or hg repo."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef clone(repo_url, checkout=None, clone_to_dir='.', no_input=False):\n    # Ensure that clone_to_dir exists\n    clone_to_dir = os.path.expanduser(clone_to_dir)\n    make_sure_path_exists(clone_to_dir)\n\n    # identify the repo_type\n    repo_type, repo_url = identify_repo(repo_url)\n\n    # check that the appropriate VCS for the repo_type is installed\n    if not is_vcs_installed(repo_type):\n        msg = \"'{0}' is not installed.\".format(repo_type)\n        raise VCSNotInstalled(msg)\n\n    repo_url = repo_url.rstrip('/')\n    tail = os.path.split(repo_url)[1]\n    if repo_type == 'git':\n        repo_dir = os.path.normpath(os.path.join(clone_to_dir,\n                                                 tail.rsplit('.git')[0]))\n    elif repo_type == 'hg':\n        repo_dir = os.path.normpath(os.path.join(clone_to_dir, tail))\n    logger.debug('repo_dir is {0}'.format(repo_dir))\n\n    if os.path.isdir(repo_dir):\n        clone = prompt_and_delete(repo_dir, no_input=no_input)\n    else:\n        clone = True\n\n    if clone:\n        try:\n            subprocess.check_output(\n                [repo_type, 'clone', repo_url],\n                cwd=clone_to_dir,\n                stderr=subprocess.STDOUT,\n            )\n            if checkout is not None:\n                subprocess.check_output(\n                    [repo_type, 'checkout', checkout],\n                    cwd=repo_dir,\n                    stderr=subprocess.STDOUT,\n                )\n        except subprocess.CalledProcessError as clone_error:\n            output = clone_error.output.decode('utf-8')\n            if 'not found' in output.lower():\n                raise RepositoryNotFound(\n                    'The repository {} could not be found, '\n                    'have you made a typo?'.format(repo_url)\n                )\n            if any(error in output for error in BRANCH_ERRORS):\n                raise RepositoryCloneFailed(\n                    'The {} branch of repository {} could not found, '\n                    'have you made a typo?'.format(checkout, repo_url)\n                )\n            raise\n\n    return repo_dir", "response": "Clone a VCS repository to a new directory."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndetermining if a hook file is valid.", "response": "def valid_hook(hook_file, hook_name):\n    \"\"\"Determine if a hook file is valid.\n\n    :param hook_file: The hook file to consider for validity\n    :param hook_name: The hook to find\n    :return: The hook file validity\n    \"\"\"\n    filename = os.path.basename(hook_file)\n    basename = os.path.splitext(filename)[0]\n\n    matching_hook = basename == hook_name\n    supported_hook = basename in _HOOKS\n    backup_file = filename.endswith('~')\n\n    return matching_hook and supported_hook and not backup_file"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_hook(hook_name, hooks_dir='hooks'):\n    logger.debug('hooks_dir is {}'.format(os.path.abspath(hooks_dir)))\n\n    if not os.path.isdir(hooks_dir):\n        logger.debug('No hooks/ dir in template_dir')\n        return None\n\n    for hook_file in os.listdir(hooks_dir):\n        if valid_hook(hook_file, hook_name):\n            return os.path.abspath(os.path.join(hooks_dir, hook_file))\n\n    return None", "response": "Find the absolute path to the hook script."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run_script(script_path, cwd='.'):\n    run_thru_shell = sys.platform.startswith('win')\n    if script_path.endswith('.py'):\n        script_command = [sys.executable, script_path]\n    else:\n        script_command = [script_path]\n\n    utils.make_executable(script_path)\n\n    try:\n        proc = subprocess.Popen(\n            script_command,\n            shell=run_thru_shell,\n            cwd=cwd\n        )\n        exit_status = proc.wait()\n        if exit_status != EXIT_SUCCESS:\n            raise FailedHookException(\n                'Hook script failed (exit status: {})'.format(exit_status)\n            )\n    except OSError as os_error:\n        if os_error.errno == errno.ENOEXEC:\n            raise FailedHookException(\n                'Hook script failed, might be an '\n                'empty file or missing a shebang'\n            )\n        raise FailedHookException(\n            'Hook script failed (error: {})'.format(os_error)\n        )", "response": "Execute a script from a working directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run_script_with_context(script_path, cwd, context):\n    _, extension = os.path.splitext(script_path)\n\n    contents = io.open(script_path, 'r', encoding='utf-8').read()\n\n    with tempfile.NamedTemporaryFile(\n        delete=False,\n        mode='wb',\n        suffix=extension\n    ) as temp:\n        env = StrictEnvironment(\n            context=context,\n            keep_trailing_newline=True,\n        )\n        template = env.from_string(contents)\n        output = template.render(**context)\n        temp.write(output.encode('utf-8'))\n\n    run_script(temp.name, cwd)", "response": "Execute a script in a Cookiecutter project template with the given context."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nruns a hook from the specified project directory.", "response": "def run_hook(hook_name, project_dir, context):\n    \"\"\"\n    Try to find and execute a hook from the specified project directory.\n\n    :param hook_name: The hook to execute.\n    :param project_dir: The directory to execute the script from.\n    :param context: Cookiecutter project context.\n    \"\"\"\n    script = find_hook(hook_name)\n    if script is None:\n        logger.debug('No {} hook found'.format(hook_name))\n        return\n    logger.debug('Running hook {}'.format(hook_name))\n    run_script_with_context(script, project_dir, context)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the Cookiecutter version location and Python powering it.", "response": "def version_msg():\n    \"\"\"Return the Cookiecutter version, location and Python powering it.\"\"\"\n    python_version = sys.version[:3]\n    location = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n    message = u'Cookiecutter %(version)s from {} (Python {})'\n    return message.format(location, python_version)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef main(\n        template, extra_context, no_input, checkout, verbose,\n        replay, overwrite_if_exists, output_dir, config_file,\n        default_config, debug_file):\n    \"\"\"Create a project from a Cookiecutter project template (TEMPLATE).\n\n    Cookiecutter is free and open source software, developed and managed by\n    volunteers. If you would like to help out or fund the project, please get\n    in touch at https://github.com/audreyr/cookiecutter.\n    \"\"\"\n    # If you _need_ to support a local template in a directory\n    # called 'help', use a qualified path to the directory.\n    if template == u'help':\n        click.echo(click.get_current_context().get_help())\n        sys.exit(0)\n\n    configure_logger(\n        stream_level='DEBUG' if verbose else 'INFO',\n        debug_file=debug_file,\n    )\n\n    try:\n        cookiecutter(\n            template, checkout, no_input,\n            extra_context=extra_context,\n            replay=replay,\n            overwrite_if_exists=overwrite_if_exists,\n            output_dir=output_dir,\n            config_file=config_file,\n            default_config=default_config,\n            password=os.environ.get('COOKIECUTTER_REPO_PASSWORD')\n        )\n    except (OutputDirExistsException,\n            InvalidModeException,\n            FailedHookException,\n            UnknownExtension,\n            InvalidZipRepository,\n            RepositoryNotFound,\n            RepositoryCloneFailed) as e:\n        click.echo(e)\n        sys.exit(1)\n    except UndefinedVariableInTemplate as undefined_err:\n        click.echo('{}'.format(undefined_err.message))\n        click.echo('Error message: {}'.format(undefined_err.error.message))\n\n        context_str = json.dumps(\n            undefined_err.context,\n            indent=4,\n            sort_keys=True\n        )\n        click.echo('Context: {}'.format(context_str))\n        sys.exit(1)", "response": "Create a new Cookiecutter project from a template."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmount the specified object to the current UnmountedType instance.", "response": "def mounted(cls, unmounted):  # noqa: N802\n        \"\"\"\n        Mount the UnmountedType instance\n        \"\"\"\n        assert isinstance(unmounted, UnmountedType), (\"{} can't mount {}\").format(\n            cls.__name__, repr(unmounted)\n        )\n\n        return cls(\n            unmounted.get_type(),\n            *unmounted.args,\n            _creation_counter=unmounted.creation_counter,\n            **unmounted.kwargs\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a customized copy of the Parameter.", "response": "def replace(\n        self,\n        name=_void,\n        kind=_void,\n        annotation=_void,\n        default=_void,\n        _partial_kwarg=_void,\n    ):\n        \"\"\"Creates a customized copy of the Parameter.\"\"\"\n\n        if name is _void:\n            name = self._name\n\n        if kind is _void:\n            kind = self._kind\n\n        if annotation is _void:\n            annotation = self._annotation\n\n        if default is _void:\n            default = self._default\n\n        if _partial_kwarg is _void:\n            _partial_kwarg = self._partial_kwarg\n\n        return type(self)(\n            name,\n            kind,\n            default=default,\n            annotation=annotation,\n            _partial_kwarg=_partial_kwarg,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef from_function(cls, func):\n\n        if not isinstance(func, types.FunctionType):\n            raise TypeError(\"{!r} is not a Python function\".format(func))\n\n        Parameter = cls._parameter_cls\n\n        # Parameter information.\n        func_code = func.__code__\n        pos_count = func_code.co_argcount\n        arg_names = func_code.co_varnames\n        positional = tuple(arg_names[:pos_count])\n        keyword_only_count = getattr(func_code, \"co_kwonlyargcount\", 0)\n        keyword_only = arg_names[pos_count : (pos_count + keyword_only_count)]\n        annotations = getattr(func, \"__annotations__\", {})\n        defaults = func.__defaults__\n        kwdefaults = getattr(func, \"__kwdefaults__\", None)\n\n        if defaults:\n            pos_default_count = len(defaults)\n        else:\n            pos_default_count = 0\n\n        parameters = []\n\n        # Non-keyword-only parameters w/o defaults.\n        non_default_count = pos_count - pos_default_count\n        for name in positional[:non_default_count]:\n            annotation = annotations.get(name, _empty)\n            parameters.append(\n                Parameter(name, annotation=annotation, kind=_POSITIONAL_OR_KEYWORD)\n            )\n\n        # ... w/ defaults.\n        for offset, name in enumerate(positional[non_default_count:]):\n            annotation = annotations.get(name, _empty)\n            parameters.append(\n                Parameter(\n                    name,\n                    annotation=annotation,\n                    kind=_POSITIONAL_OR_KEYWORD,\n                    default=defaults[offset],\n                )\n            )\n\n        # *args\n        if func_code.co_flags & 0x04:\n            name = arg_names[pos_count + keyword_only_count]\n            annotation = annotations.get(name, _empty)\n            parameters.append(\n                Parameter(name, annotation=annotation, kind=_VAR_POSITIONAL)\n            )\n\n        # Keyword-only parameters.\n        for name in keyword_only:\n            default = _empty\n            if kwdefaults is not None:\n                default = kwdefaults.get(name, _empty)\n\n            annotation = annotations.get(name, _empty)\n            parameters.append(\n                Parameter(\n                    name, annotation=annotation, kind=_KEYWORD_ONLY, default=default\n                )\n            )\n        # **kwargs\n        if func_code.co_flags & 0x08:\n            index = pos_count + keyword_only_count\n            if func_code.co_flags & 0x04:\n                index += 1\n\n            name = arg_names[index]\n            annotation = annotations.get(name, _empty)\n            parameters.append(Parameter(name, annotation=annotation, kind=_VAR_KEYWORD))\n\n        return cls(\n            parameters,\n            return_annotation=annotations.get(\"return\", _empty),\n            __validate_parameters__=False,\n        )", "response": "Constructs a signature for the given python function."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef replace(self, parameters=_void, return_annotation=_void):\n\n        if parameters is _void:\n            parameters = self.parameters.values()\n\n        if return_annotation is _void:\n            return_annotation = self._return_annotation\n\n        return type(self)(parameters, return_annotation=return_annotation)", "response": "Returns a new signature with the specified parameters and return_annotation."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _bind(self, args, kwargs, partial=False):\n\n        arguments = OrderedDict()\n\n        parameters = iter(self.parameters.values())\n        parameters_ex = ()\n        arg_vals = iter(args)\n\n        if partial:\n            # Support for binding arguments to 'functools.partial' objects.\n            # See 'functools.partial' case in 'signature()' implementation\n            # for details.\n            for param_name, param in self.parameters.items():\n                if param._partial_kwarg and param_name not in kwargs:\n                    # Simulating 'functools.partial' behavior\n                    kwargs[param_name] = param.default\n\n        while True:\n            # Let's iterate through the positional arguments and corresponding\n            # parameters\n            try:\n                arg_val = next(arg_vals)\n            except StopIteration:\n                # No more positional arguments\n                try:\n                    param = next(parameters)\n                except StopIteration:\n                    # No more parameters. That's it. Just need to check that\n                    # we have no `kwargs` after this while loop\n                    break\n                else:\n                    if param.kind == _VAR_POSITIONAL:\n                        # That's OK, just empty *args.  Let's start parsing\n                        # kwargs\n                        break\n                    elif param.name in kwargs:\n                        if param.kind == _POSITIONAL_ONLY:\n                            msg = (\n                                \"{arg!r} parameter is positional only, \"\n                                \"but was passed as a keyword\"\n                            )\n                            msg = msg.format(arg=param.name)\n                            raise TypeError(msg)\n                        parameters_ex = (param,)\n                        break\n                    elif param.kind == _VAR_KEYWORD or param.default is not _empty:\n                        # That's fine too - we have a default value for this\n                        # parameter.  So, lets start parsing `kwargs`, starting\n                        # with the current parameter\n                        parameters_ex = (param,)\n                        break\n                    else:\n                        if partial:\n                            parameters_ex = (param,)\n                            break\n                        else:\n                            msg = \"{arg!r} parameter lacking default value\"\n                            msg = msg.format(arg=param.name)\n                            raise TypeError(msg)\n            else:\n                # We have a positional argument to process\n                try:\n                    param = next(parameters)\n                except StopIteration:\n                    raise TypeError(\"too many positional arguments\")\n                else:\n                    if param.kind in (_VAR_KEYWORD, _KEYWORD_ONLY):\n                        # Looks like we have no parameter for this positional\n                        # argument\n                        raise TypeError(\"too many positional arguments\")\n\n                    if param.kind == _VAR_POSITIONAL:\n                        # We have an '*args'-like argument, let's fill it with\n                        # all positional arguments we have left and move on to\n                        # the next phase\n                        values = [arg_val]\n                        values.extend(arg_vals)\n                        arguments[param.name] = tuple(values)\n                        break\n\n                    if param.name in kwargs:\n                        raise TypeError(\n                            \"multiple values for argument \"\n                            \"{arg!r}\".format(arg=param.name)\n                        )\n\n                    arguments[param.name] = arg_val\n\n        # Now, we iterate through the remaining parameters to process\n        # keyword arguments\n        kwargs_param = None\n        for param in itertools.chain(parameters_ex, parameters):\n            if param.kind == _POSITIONAL_ONLY:\n                # This should never happen in case of a properly built\n                # Signature object (but let's have this check here\n                # to ensure correct behaviour just in case)\n                raise TypeError(\n                    \"{arg!r} parameter is positional only, \"\n                    \"but was passed as a keyword\".format(arg=param.name)\n                )\n\n            if param.kind == _VAR_KEYWORD:\n                # Memorize that we have a '**kwargs'-like parameter\n                kwargs_param = param\n                continue\n\n            param_name = param.name\n            try:\n                arg_val = kwargs.pop(param_name)\n            except KeyError:\n                # We have no value for this parameter.  It's fine though,\n                # if it has a default value, or it is an '*args'-like\n                # parameter, left alone by the processing of positional\n                # arguments.\n                if (\n                    not partial\n                    and param.kind != _VAR_POSITIONAL\n                    and param.default is _empty\n                ):\n                    raise TypeError(\n                        \"{arg!r} parameter lacking default value\".format(arg=param_name)\n                    )\n\n            else:\n                arguments[param_name] = arg_val\n\n        if kwargs:\n            if kwargs_param is not None:\n                # Process our '**kwargs'-like parameter\n                arguments[kwargs_param.name] = kwargs\n            else:\n                raise TypeError(\"too many keyword arguments\")\n\n        return self._bound_arguments_cls(self, arguments)", "response": "Private method to bind the parameters to the current object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a BoundArguments object that partially maps the passed args and kwargs to the function s signature.", "response": "def bind_partial(self, *args, **kwargs):\n        \"\"\"Get a BoundArguments object, that partially maps the\n        passed `args` and `kwargs` to the function's signature.\n        Raises `TypeError` if the passed arguments can not be bound.\n        \"\"\"\n        return self._bind(args, kwargs, partial=True)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_node(objecttype):\n    if not isclass(objecttype):\n        return False\n\n    if not issubclass(objecttype, ObjectType):\n        return False\n\n    for i in objecttype._meta.interfaces:\n        if issubclass(i, Node):\n            return True\n\n    return False", "response": "Check if the given objecttype has Node as an interface"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_complete_version(version=None):\n    if version is None:\n        from graphene import VERSION as version\n    else:\n        assert len(version) == 5\n        assert version[3] in (\"alpha\", \"beta\", \"rc\", \"final\")\n\n    return version", "response": "Returns a tuple of the graphene version."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef maybe_thenable(obj, on_resolve):\n    if isawaitable(obj) and not isinstance(obj, Promise):\n        return await_and_execute(obj, on_resolve)\n\n    if is_thenable(obj):\n        return Promise.resolve(obj).then(on_resolve)\n\n    # If it's not awaitable not a Promise, return\n    # the function executed over the object\n    return on_resolve(obj)", "response": "Execute a on_resolve function once the thenable is resolved and return the same type of object inputed."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nextract all the fields in given attributes and return them ordered by name.", "response": "def yank_fields_from_attrs(attrs, _as=None, sort=True):\n    \"\"\"\n    Extract all the fields in given attributes (dict)\n    and return them ordered\n    \"\"\"\n    fields_with_names = []\n    for attname, value in list(attrs.items()):\n        field = get_field_as(value, _as)\n        if not field:\n            continue\n        fields_with_names.append((attname, field))\n\n    if sort:\n        fields_with_names = sorted(fields_with_names, key=lambda f: f[1])\n    return OrderedDict(fields_with_names)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef import_string(dotted_path, dotted_attributes=None):\n    try:\n        module_path, class_name = dotted_path.rsplit(\".\", 1)\n    except ValueError:\n        raise ImportError(\"%s doesn't look like a module path\" % dotted_path)\n\n    module = import_module(module_path)\n\n    try:\n        result = getattr(module, class_name)\n    except AttributeError:\n        raise ImportError(\n            'Module \"%s\" does not define a \"%s\" attribute/class'\n            % (module_path, class_name)\n        )\n\n    if not dotted_attributes:\n        return result\n    else:\n        attributes = dotted_attributes.split(\".\")\n        traveled_attributes = []\n        try:\n            for attribute in attributes:\n                traveled_attributes.append(attribute)\n                result = getattr(result, attribute)\n            return result\n        except AttributeError:\n            raise ImportError(\n                'Module \"%s\" does not define a \"%s\" attribute inside attribute/class \"%s\"'\n                % (module_path, \".\".join(traveled_attributes), class_name)\n            )", "response": "Imports a dotted module path and returns the corresponding attribute or class."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef scan_aggs(search, source_aggs, inner_aggs={}, size=10):\n    def run_search(**kwargs):\n        s = search[:0]\n        s.aggs.bucket('comp', 'composite', sources=source_aggs, size=size, **kwargs)\n        for agg_name, agg in inner_aggs.items():\n            s.aggs['comp'][agg_name] = agg\n        return s.execute()\n\n    response = run_search()\n    while response.aggregations.comp.buckets:\n        for b in response.aggregations.comp.buckets:\n            yield b\n        if 'after_key' in response.aggregations.comp:\n            after = response.aggregations.comp.after_key\n        else:\n            after= response.aggregations.comp.buckets[-1].key\n        response = run_search(after=after)", "response": "Yields all possible bucket combinations of source_aggs and yields results of inner_aggs for each bucket."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef clean(self):\n        self.suggest = {\n            'input': [' '.join(p) for p in permutations(self.name.split())],\n            'weight': self.popularity\n        }", "response": "Clean the suggestion input and weight by taking all possible permutation of Person s name as input and taking their\n        popularity as weight."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nyields all the fields defined for our class", "response": "def __list_fields(cls):\n        \"\"\"\n        Get all the fields defined for our class, if we have an Index, try\n        looking at the index mappings as well, mark the fields from Index as\n        optional.\n        \"\"\"\n        for name in cls._doc_type.mapping:\n            field = cls._doc_type.mapping[name]\n            yield name, field, False\n\n        if hasattr(cls.__class__, '_index'):\n            if not cls._index._mapping:\n                return\n            for name in cls._index._mapping:\n                # don't return fields that are in _doc_type\n                if name in cls._doc_type.mapping:\n                    continue\n                field = cls._index._mapping[name]\n                yield name, field, True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _clone(self):\n        ubq = super(UpdateByQuery, self)._clone()\n\n        ubq._response_class = self._response_class\n        ubq._script = self._script.copy()\n        ubq.query._proxied = self.query._proxied\n        return ubq", "response": "Returns a shallow copy of the current search request. Performs a shallow copy of all the underlying objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\noverrides the default wrapper used for the response.", "response": "def response_class(self, cls):\n        \"\"\"\n        Override the default wrapper used for the response.\n        \"\"\"\n        ubq = self._clone()\n        ubq._response_class = cls\n        return ubq"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_from_dict(self, d):\n        d = d.copy()\n        if 'query' in d:\n            self.query._proxied = Q(d.pop('query'))\n        if 'script' in d:\n            self._script = d.pop('script')\n        self._extra = d\n        return self", "response": "Updates the current object with the options from a serialized body."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef script(self, **kwargs):\n        ubq = self._clone()\n        if ubq._script:\n            ubq._script = {}\n        ubq._script.update(kwargs)\n        return ubq", "response": "Define update action to take :\n           . script"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_dict(self, **kwargs):\n        d = {}\n        if self.query:\n            d[\"query\"] = self.query.to_dict()\n\n        if self._script:\n            d['script'] = self._script\n\n        d.update(self._extra)\n\n        d.update(kwargs)\n        return d", "response": "Serialize the search into a dictionary that will be sent over as the\n        request."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef execute(self):\n        es = connections.get_connection(self._using)\n\n        self._response = self._response_class(\n            self,\n            es.update_by_query(\n                index=self._index,\n                body=self.to_dict(),\n                **self._params\n            )\n        )\n        return self._response", "response": "Execute the search and return an instance of Response wrapping all\n            the data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _collect_fields(self):\n        for f in itervalues(self.properties.to_dict()):\n            yield f\n            # multi fields\n            if hasattr(f, 'fields'):\n                for inner_f in itervalues(f.fields.to_dict()):\n                    yield inner_f\n            # nested and inner objects\n            if hasattr(f, '_collect_fields'):\n                for inner_f in f._collect_fields():\n                    yield inner_f", "response": "Iterate over all Field objects within including multi fields."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a copy of the index with another name or connection alias.", "response": "def clone(self, name=None, using=None):\n        \"\"\"\n        Create a copy of the instance with another name or connection alias.\n        Useful for creating multiple indices with shared configuration::\n\n            i = Index('base-index')\n            i.settings(number_of_shards=1)\n            i.create()\n\n            i2 = i.clone('other-index')\n            i2.create()\n\n        :arg name: name of the index\n        :arg using: connection alias to use, defaults to ``'default'``\n        \"\"\"\n        i = Index(name or self._name, using=using or self._using)\n        i._settings = self._settings.copy()\n        i._aliases = self._aliases.copy()\n        i._analysis = self._analysis.copy()\n        i._doc_types = self._doc_types[:]\n        if self._mapping is not None:\n            i._mapping = self._mapping._clone()\n        return i"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nassociates a document with an index.", "response": "def document(self, document):\n        \"\"\"\n        Associate a :class:`~elasticsearch_dsl.Document` subclass with an index.\n        This means that, when this index is created, it will contain the\n        mappings for the ``Document``. If the ``Document`` class doesn't have a\n        default index yet (by defining ``class Index``), this instance will be\n        used. Can be used as a decorator::\n\n            i = Index('blog')\n\n            @i.document\n            class Post(Document):\n                title = Text()\n\n            # create the index, including Post mappings\n            i.create()\n\n            # .search() will now return a Search object that will return\n            # properly deserialized Post instances\n            s = i.search()\n        \"\"\"\n        self._doc_types.append(document)\n\n        # If the document index does not have any name, that means the user\n        # did not set any index already to the document.\n        # So set this index as document index\n        if document._index._name is None:\n            document._index = self\n\n        return document"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef analyzer(self, *args, **kwargs):\n        analyzer = analysis.analyzer(*args, **kwargs)\n        d = analyzer.get_analysis_definition()\n        # empty custom analyzer, probably already defined out of our control\n        if not d:\n            return\n\n        # merge the definition\n        merge(self._analysis, d, True)", "response": "Explicitly add an analyzer to an index."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef search(self, using=None):\n        return Search(\n            using=using or self._using,\n            index=self._name,\n            doc_type=self._doc_types\n        )", "response": "Returns a Search object searching over the the\n            index and its\n            document types."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef updateByQuery(self, using=None):\n        return UpdateByQuery(\n            using=using or self._using,\n            index=self._name,\n        )", "response": "Returns an UpdateByQuery object searching over the index and updating the Documents that match the search criteria."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating the index in elasticsearch.", "response": "def create(self, using=None, **kwargs):\n        \"\"\"\n        Creates the index in elasticsearch.\n\n        Any additional keyword arguments will be passed to\n        ``Elasticsearch.indices.create`` unchanged.\n        \"\"\"\n        self._get_connection(using).indices.create(index=self._name, body=self.to_dict(), **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef save(self, using=None):\n        if not self.exists(using=using):\n            return self.create(using=using)\n\n        body = self.to_dict()\n        settings = body.pop('settings', {})\n        analysis = settings.pop('analysis', None)\n        current_settings = self.get_settings(using=using)[self._name]['settings']['index']\n        if analysis:\n            if self.is_closed(using=using):\n                # closed index, update away\n                settings['analysis'] = analysis\n            else:\n                # compare analysis definition, if all analysis objects are\n                # already defined as requested, skip analysis update and\n                # proceed, otherwise raise IllegalOperation\n                existing_analysis = current_settings.get('analysis', {})\n                if any(\n                    existing_analysis.get(section, {}).get(k, None) != analysis[section][k]\n                    for section in analysis\n                    for k in analysis[section]\n                ):\n                    raise IllegalOperation(\n                        'You cannot update analysis configuration on an open index, you need to close index %s first.' % self._name)\n\n        # try and update the settings\n        if settings:\n            settings = settings.copy()\n            for k, v in list(settings.items()):\n                if k in current_settings and current_settings[k] == str(v):\n                    del settings[k]\n\n            if settings:\n                self.put_settings(using=using, body=settings)\n\n        # update the mappings, any conflict in the mappings will result in an\n        # exception\n        mappings = body.pop('mappings', {})\n        if mappings:\n            self.put_mapping(using=using, body=mappings)", "response": "Sync the index definition with elasticsearch creating the index if it doesn t exist and updating the settings and mappings if it does exist."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nperforms the analysis process on a text and return the tokens breakdown .", "response": "def analyze(self, using=None, **kwargs):\n        \"\"\"\n        Perform the analysis process on a text and return the tokens breakdown\n        of the text.\n\n        Any additional keyword arguments will be passed to\n        ``Elasticsearch.indices.analyze`` unchanged.\n        \"\"\"\n        return self._get_connection(using).indices.analyze(index=self._name, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef refresh(self, using=None, **kwargs):\n        return self._get_connection(using).indices.refresh(index=self._name, **kwargs)", "response": "Preforms a refresh operation on the index."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving the index s metadata.", "response": "def get(self, using=None, **kwargs):\n        \"\"\"\n        The get index API allows to retrieve information about the index.\n\n        Any additional keyword arguments will be passed to\n        ``Elasticsearch.indices.get`` unchanged.\n        \"\"\"\n        return self._get_connection(using).indices.get(index=self._name, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nopens the index in elasticsearch.", "response": "def open(self, using=None, **kwargs):\n        \"\"\"\n        Opens the index in elasticsearch.\n\n        Any additional keyword arguments will be passed to\n        ``Elasticsearch.indices.open`` unchanged.\n        \"\"\"\n        return self._get_connection(using).indices.open(index=self._name, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncloses the index in elasticsearch.", "response": "def close(self, using=None, **kwargs):\n        \"\"\"\n        Closes the index in elasticsearch.\n\n        Any additional keyword arguments will be passed to\n        ``Elasticsearch.indices.close`` unchanged.\n        \"\"\"\n        return self._get_connection(using).indices.close(index=self._name, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndeleting the index in elasticsearch.", "response": "def delete(self, using=None, **kwargs):\n        \"\"\"\n        Deletes the index in elasticsearch.\n\n        Any additional keyword arguments will be passed to\n        ``Elasticsearch.indices.delete`` unchanged.\n        \"\"\"\n        return self._get_connection(using).indices.delete(index=self._name, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef exists(self, using=None, **kwargs):\n        return self._get_connection(using).indices.exists(index=self._name, **kwargs)", "response": "Returns True if the index already exists in elasticsearch otherwise False."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef exists_type(self, using=None, **kwargs):\n        return self._get_connection(using).indices.exists_type(index=self._name, **kwargs)", "response": "Check if a type or types exists in the index."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nregisters specific mapping definition for a specific type of entry in the index.", "response": "def put_mapping(self, using=None, **kwargs):\n        \"\"\"\n        Register specific mapping definition for a specific type.\n\n        Any additional keyword arguments will be passed to\n        ``Elasticsearch.indices.put_mapping`` unchanged.\n        \"\"\"\n        return self._get_connection(using).indices.put_mapping(index=self._name, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nretrieve specific mapping definition for a specific type.", "response": "def get_mapping(self, using=None, **kwargs):\n        \"\"\"\n        Retrieve specific mapping definition for a specific type.\n\n        Any additional keyword arguments will be passed to\n        ``Elasticsearch.indices.get_mapping`` unchanged.\n        \"\"\"\n        return self._get_connection(using).indices.get_mapping(index=self._name, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nretrieving mapping definition of a specific field.", "response": "def get_field_mapping(self, using=None, **kwargs):\n        \"\"\"\n        Retrieve mapping definition of a specific field.\n\n        Any additional keyword arguments will be passed to\n        ``Elasticsearch.indices.get_field_mapping`` unchanged.\n        \"\"\"\n        return self._get_connection(using).indices.get_field_mapping(index=self._name, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate an alias for the index.", "response": "def put_alias(self, using=None, **kwargs):\n        \"\"\"\n        Create an alias for the index.\n\n        Any additional keyword arguments will be passed to\n        ``Elasticsearch.indices.put_alias`` unchanged.\n        \"\"\"\n        return self._get_connection(using).indices.put_alias(index=self._name, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef exists_alias(self, using=None, **kwargs):\n        return self._get_connection(using).indices.exists_alias(index=self._name, **kwargs)", "response": "Return a boolean indicating whether given alias exists for this index."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretrieves a specified alias.", "response": "def get_alias(self, using=None, **kwargs):\n        \"\"\"\n        Retrieve a specified alias.\n\n        Any additional keyword arguments will be passed to\n        ``Elasticsearch.indices.get_alias`` unchanged.\n        \"\"\"\n        return self._get_connection(using).indices.get_alias(index=self._name, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndeleting specific alias from the index.", "response": "def delete_alias(self, using=None, **kwargs):\n        \"\"\"\n        Delete specific alias.\n\n        Any additional keyword arguments will be passed to\n        ``Elasticsearch.indices.delete_alias`` unchanged.\n        \"\"\"\n        return self._get_connection(using).indices.delete_alias(index=self._name, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving the settings for the index.", "response": "def get_settings(self, using=None, **kwargs):\n        \"\"\"\n        Retrieve settings for the index.\n\n        Any additional keyword arguments will be passed to\n        ``Elasticsearch.indices.get_settings`` unchanged.\n        \"\"\"\n        return self._get_connection(using).indices.get_settings(index=self._name, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef put_settings(self, using=None, **kwargs):\n        return self._get_connection(using).indices.put_settings(index=self._name, **kwargs)", "response": "Change specific index level settings in real time."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef stats(self, using=None, **kwargs):\n        return self._get_connection(using).indices.stats(index=self._name, **kwargs)", "response": "Retrieve statistics on different operations happening on the index."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns low level segments information that a Lucene index is built with.", "response": "def segments(self, using=None, **kwargs):\n        \"\"\"\n        Provide low level segments information that a Lucene index (shard\n        level) is built with.\n\n        Any additional keyword arguments will be passed to\n        ``Elasticsearch.indices.segments`` unchanged.\n        \"\"\"\n        return self._get_connection(using).indices.segments(index=self._name, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nvalidate a potentially expensive query without executing it.", "response": "def validate_query(self, using=None, **kwargs):\n        \"\"\"\n        Validate a potentially expensive query without executing it.\n\n        Any additional keyword arguments will be passed to\n        ``Elasticsearch.indices.validate_query`` unchanged.\n        \"\"\"\n        return self._get_connection(using).indices.validate_query(index=self._name, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef clear_cache(self, using=None, **kwargs):\n        return self._get_connection(using).indices.clear_cache(index=self._name, **kwargs)", "response": "Clear all caches associated with this index."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef recovery(self, using=None, **kwargs):\n        return self._get_connection(using).indices.recovery(index=self._name, **kwargs)", "response": "Return the set of recovery entries for this index."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef upgrade(self, using=None, **kwargs):\n        return self._get_connection(using).indices.upgrade(index=self._name, **kwargs)", "response": "Upgrade the index to the latest format."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the latest index upgrade time for this index.", "response": "def get_upgrade(self, using=None, **kwargs):\n        \"\"\"\n        Monitor how much of the index is upgraded.\n\n        Any additional keyword arguments will be passed to\n        ``Elasticsearch.indices.get_upgrade`` unchanged.\n        \"\"\"\n        return self._get_connection(using).indices.get_upgrade(index=self._name, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nflushes all the keys in this index to the index and all shards.", "response": "def flush_synced(self, using=None, **kwargs):\n        \"\"\"\n        Perform a normal flush, then add a generated unique marker (sync_id) to\n        all shards.\n\n        Any additional keyword arguments will be passed to\n        ``Elasticsearch.indices.flush_synced`` unchanged.\n        \"\"\"\n        return self._get_connection(using).indices.flush_synced(index=self._name, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprovides store information for shard copies of the index.", "response": "def shard_stores(self, using=None, **kwargs):\n        \"\"\"\n        Provides store information for shard copies of the index. Store\n        information reports on which nodes shard copies exist, the shard copy\n        version, indicating how recent they are, and any exceptions encountered\n        while opening the shard index or from earlier engine failure.\n\n        Any additional keyword arguments will be passed to\n        ``Elasticsearch.indices.shard_stores`` unchanged.\n        \"\"\"\n        return self._get_connection(using).indices.shard_stores(index=self._name, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef forcemerge(self, using=None, **kwargs):\n        return self._get_connection(using).indices.forcemerge(index=self._name, **kwargs)", "response": "Force merge the index with the index s entry set."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nshrink an index into a new index with fewer primary shards.", "response": "def shrink(self, using=None, **kwargs):\n        \"\"\"\n        The shrink index API allows you to shrink an existing index into a new\n        index with fewer primary shards. The number of primary shards in the\n        target index must be a factor of the shards in the source index. For\n        example an index with 8 primary shards can be shrunk into 4, 2 or 1\n        primary shards or an index with 15 primary shards can be shrunk into 5,\n        3 or 1. If the number of shards in the index is a prime number it can\n        only be shrunk into a single primary shard. Before shrinking, a\n        (primary or replica) copy of every shard in the index must be present\n        on the same node.\n\n        Any additional keyword arguments will be passed to\n        ``Elasticsearch.indices.shrink`` unchanged.\n        \"\"\"\n        return self._get_connection(using).indices.shrink(index=self._name, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef configure(self, **kwargs):\n        for k in list(self._conns):\n            # try and preserve existing client to keep the persistent connections alive\n            if k in self._kwargs and kwargs.get(k, None) == self._kwargs[k]:\n                continue\n            del self._conns[k]\n        self._kwargs = kwargs", "response": "Configure multiple connections at once useful for passing in config\n        dictionary obtained from other sources like Django s settings or Django s settings tool."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nremove a connection from the registry. Raises KeyError if there is no such connection.", "response": "def remove_connection(self, alias):\n        \"\"\"\n        Remove connection from the registry. Raises ``KeyError`` if connection\n        wasn't found.\n        \"\"\"\n        errors = 0\n        for d in (self._conns, self._kwargs):\n            try:\n                del d[alias]\n            except KeyError:\n                errors += 1\n\n        if errors == 2:\n            raise KeyError('There is no connection with alias %r.' % alias)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate an instance of Elasticsearch and register it under given alias.", "response": "def create_connection(self, alias='default', **kwargs):\n        \"\"\"\n        Construct an instance of ``elasticsearch.Elasticsearch`` and register\n        it under given alias.\n        \"\"\"\n        kwargs.setdefault('serializer', serializer)\n        conn = self._conns[alias] = Elasticsearch(**kwargs)\n        return conn"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_connection(self, alias='default'):\n        # do not check isinstance(Elasticsearch) so that people can wrap their\n        # clients\n        if not isinstance(alias, string_types):\n            return alias\n\n        # connection already established\n        try:\n            return self._conns[alias]\n        except KeyError:\n            pass\n\n        # if not, try to create it\n        try:\n            return self.create_connection(alias, **self._kwargs[alias])\n        except KeyError:\n            # no connection and no kwargs to set one up\n            raise KeyError('There is no connection with alias %r.' % alias)", "response": "Retrieve a connection from the registry if necessary."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate the index template in elasticsearch specifying the mappings and any settings to be used.", "response": "def setup():\n    \"\"\"\n    Create the index template in elasticsearch specifying the mappings and any\n    settings to be used. This can be run at any time, ideally at every new code\n    deploy.\n    \"\"\"\n    # create an index template\n    index_template = BlogPost._index.as_template(ALIAS, PATTERN)\n    # upload the template into elasticsearch\n    # potentially overriding the one already there\n    index_template.save()\n\n    # create the first index if it doesn't exist\n    if not BlogPost._index.exists():\n        migrate(move_data=False)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef migrate(move_data=True, update_alias=True):\n    # construct a new index name by appending current timestamp\n    next_index = PATTERN.replace('*', datetime.now().strftime('%Y%m%d%H%M%S%f'))\n\n    # get the low level connection\n    es = connections.get_connection()\n\n    # create new index, it will use the settings from the template\n    es.indices.create(index=next_index)\n\n    if move_data:\n        # move data from current alias to the new index\n        es.reindex(\n            body={\"source\": {\"index\": ALIAS}, \"dest\": {\"index\": next_index}},\n            request_timeout=3600\n        )\n        # refresh the index to make the changes visible\n        es.indices.refresh(index=next_index)\n\n    if update_alias:\n        # repoint the alias to point to the newly created index\n        es.indices.update_aliases(body={\n            'actions': [\n                {\"remove\": {\"alias\": ALIAS, \"index\": PATTERN}},\n                {\"add\": {\"alias\": ALIAS, \"index\": next_index}},\n            ]\n        })", "response": "This function creates a new index and moves the data from the current index to the new index and updates the alias to point to the latest index."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef simulate(self, text, using='default', explain=False, attributes=None):\n        es = connections.get_connection(using)\n\n        body = {'text': text, 'explain': explain}\n        if attributes:\n            body['attributes'] = attributes\n\n        definition = self.get_analysis_definition()\n        analyzer_def = self.get_definition()\n\n        for section in ('tokenizer', 'char_filter', 'filter'):\n            if section not in analyzer_def:\n                continue\n            sec_def = definition.get(section, {})\n            sec_names = analyzer_def[section]\n\n            if isinstance(sec_names, six.string_types):\n                body[section] = sec_def.get(sec_names, sec_names)\n            else:\n                body[section] = [sec_def.get(sec_name, sec_name) for sec_name in sec_names]\n\n        if self._builtin_type != 'custom':\n            body['analyzer'] = self._builtin_type\n\n        return AttrDict(es.indices.analyze(body=body))", "response": "Simulate a text in elasticsearch."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets answers from inner_hits and answer by searching elasticsearch.", "response": "def get_answers(self):\n        \"\"\"\n        Get answers either from inner_hits already present or by searching\n        elasticsearch.\n        \"\"\"\n        if 'inner_hits' in self.meta and 'answer' in self.meta.inner_hits:\n            return self.meta.inner_hits.answer.hits\n        return list(self.search_answers())"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_aggregation(self):\n        agg = A(self.agg_type, **self._params)\n        if self._metric:\n            agg.metric('metric', self._metric)\n        return agg", "response": "Return the aggregation object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_values(self, data, filter_values):\n        out = []\n        for bucket in data.buckets:\n            key = self.get_value(bucket)\n            out.append((\n                key,\n                self.get_metric(bucket),\n                self.is_filtered(key, filter_values)\n            ))\n        return out", "response": "Turn the raw bucket data into a list of tuples containing the key number of documents and a flag indicating whether this value has been selected or not."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_filter(self, name, filter_values):\n        # normalize the value into a list\n        if not isinstance(filter_values, (tuple, list)):\n            if filter_values is None:\n                return\n            filter_values = [filter_values, ]\n\n        # remember the filter values for use in FacetedResponse\n        self.filter_values[name] = filter_values\n\n        # get the filter from the facet\n        f = self.facets[name].add_filter(filter_values)\n        if f is None:\n            return\n\n        self._filters[name] = f", "response": "Add a filter for a facet."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the base Search object to which the facets are added.", "response": "def search(self):\n        \"\"\"\n        Returns the base Search object to which the facets are added.\n\n        You can customize the query by overriding this method and returning a\n        modified search object.\n        \"\"\"\n        s = Search(doc_type=self.doc_types, index=self.index, using=self.using)\n        return s.response_class(FacetedResponse)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nqueries the database for the specified query.", "response": "def query(self, search, query):\n        \"\"\"\n        Add query part to ``search``.\n\n        Override this if you wish to customize the query used.\n        \"\"\"\n        if query:\n            if self.fields:\n                return search.query('multi_match', fields=self.fields, query=query)\n            else:\n                return search.query('multi_match', query=query)\n        return search"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef aggregate(self, search):\n        for f, facet in iteritems(self.facets):\n            agg = facet.get_aggregation()\n            agg_filter = MatchAll()\n            for field, filter in iteritems(self._filters):\n                if f == field:\n                    continue\n                agg_filter &= filter\n            search.aggs.bucket(\n                '_filter_' + f,\n                'filter',\n                filter=agg_filter\n            ).bucket(f, agg)", "response": "Add aggregations representing the facets selected including potential\n        filters."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef filter(self, search):\n        if not self._filters:\n            return search\n\n        post_filter = MatchAll()\n        for f in itervalues(self._filters):\n            post_filter &= f\n        return search.post_filter(post_filter)", "response": "Add a post_filter to the search request narrowing the results based on the facet filters."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nhighlighting the fields in the given search object.", "response": "def highlight(self, search):\n        \"\"\"\n        Add highlighting for all the fields\n        \"\"\"\n        return search.highlight(*(f if '^' not in f else f.split('^', 1)[0]\n                                  for f in self.fields))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsorting the list of items in the search.", "response": "def sort(self, search):\n        \"\"\"\n        Add sorting information to the request.\n        \"\"\"\n        if self._sort:\n            search = search.sort(*self._sort)\n        return search"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconstructs the search object.", "response": "def build_search(self):\n        \"\"\"\n        Construct the ``Search`` object.\n        \"\"\"\n        s = self.search()\n        s = self.query(s, self._query)\n        s = self.filter(s)\n        if self.fields:\n            s = self.highlight(s)\n        s = self.sort(s)\n        self.aggregate(s)\n        return s"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nexecutes the search and return the response.", "response": "def execute(self):\n        \"\"\"\n        Execute the search and return the response.\n        \"\"\"\n        r = self._s.execute()\n        r._faceted_search = self\n        return r"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a new instance of Search with the specified query params.", "response": "def params(self, **kwargs):\n        \"\"\"\n        Specify query params to be used when executing the search. All the\n        keyword arguments will override the current values. See\n        https://elasticsearch-py.readthedocs.io/en/master/api.html#elasticsearch.Elasticsearch.search\n        for all available parameters.\n\n        Example::\n\n            s = Search()\n            s = s.params(routing='user-1', preference='local')\n        \"\"\"\n        s = self._clone()\n        s._params.update(kwargs)\n        return s"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the index for the search.", "response": "def index(self, *index):\n        \"\"\"\n        Set the index for the search. If called empty it will remove all information.\n\n        Example:\n\n            s = Search()\n            s = s.index('twitter-2015.01.01', 'twitter-2015.01.02')\n            s = s.index(['twitter-2015.01.01', 'twitter-2015.01.02'])\n        \"\"\"\n        # .index() resets\n        s = self._clone()\n        if not index:\n            s._index = None\n        else:\n            indexes = []\n            for i in index:\n                if isinstance(i, string_types):\n                    indexes.append(i)\n                elif isinstance(i, list):\n                    indexes += i\n                elif isinstance(i, tuple):\n                    indexes += list(i)\n\n            s._index = (self._index or []) + indexes\n\n        return s"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef doc_type(self, *doc_type, **kwargs):\n        # .doc_type() resets\n        s = self._clone()\n        if not doc_type and not kwargs:\n            s._doc_type = []\n            s._doc_type_map = {}\n        else:\n            s._doc_type.extend(doc_type)\n            s._doc_type.extend(kwargs.keys())\n            s._doc_type_map.update(kwargs)\n        return s", "response": "Set the type of the information stored on this instance."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nassociates the search request with an elasticsearch client.", "response": "def using(self, client):\n        \"\"\"\n        Associate the search request with an elasticsearch client. A fresh copy\n        will be returned with current instance remaining unchanged.\n\n        :arg client: an instance of ``elasticsearch.Elasticsearch`` to use or\n            an alias to look up in ``elasticsearch_dsl.connections``\n\n        \"\"\"\n        s = self._clone()\n        s._using = client\n        return s"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef extra(self, **kwargs):\n        s = self._clone()\n        if 'from_' in kwargs:\n            kwargs['from'] = kwargs.pop('from_')\n        s._extra.update(kwargs)\n        return s", "response": "Add extra keys to the request body. Mostly here for backwards\n        compatibility."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a shallow copy of the current search request. Performs a shallow copy of all the underlying objects. Used internally by most state modifying APIs.", "response": "def _clone(self):\n        \"\"\"\n        Return a clone of the current search request. Performs a shallow copy\n        of all the underlying objects. Used internally by most state modifying\n        APIs.\n        \"\"\"\n        s = super(Search, self)._clone()\n\n        s._response_class = self._response_class\n        s._sort = self._sort[:]\n        s._source = copy.copy(self._source) \\\n            if self._source is not None else None\n        s._highlight = self._highlight.copy()\n        s._highlight_opts = self._highlight_opts.copy()\n        s._suggest = self._suggest.copy()\n        s._script_fields = self._script_fields.copy()\n        for x in ('query', 'post_filter'):\n            getattr(s, x)._proxied = getattr(self, x)._proxied\n\n        # copy top-level bucket definitions\n        if self.aggs._params.get('aggs'):\n            s.aggs._params = {'aggs': self.aggs._params['aggs'].copy()}\n        return s"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef response_class(self, cls):\n        s = self._clone()\n        s._response_class = cls\n        return s", "response": "Override the default wrapper used for the response."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update_from_dict(self, d):\n        d = d.copy()\n        if 'query' in d:\n            self.query._proxied = Q(d.pop('query'))\n        if 'post_filter' in d:\n            self.post_filter._proxied = Q(d.pop('post_filter'))\n\n        aggs = d.pop('aggs', d.pop('aggregations', {}))\n        if aggs:\n            self.aggs._params = {\n                'aggs': {\n                    name: A(value) for (name, value) in iteritems(aggs)}\n            }\n        if 'sort' in d:\n            self._sort = d.pop('sort')\n        if '_source' in d:\n            self._source = d.pop('_source')\n        if 'highlight' in d:\n            high = d.pop('highlight').copy()\n            self._highlight = high.pop('fields')\n            self._highlight_opts = high\n        if 'suggest' in d:\n            self._suggest = d.pop('suggest')\n            if 'text' in self._suggest:\n                text = self._suggest.pop('text')\n                for s in self._suggest.values():\n                    s.setdefault('text', text)\n        if 'script_fields' in d:\n            self._script_fields = d.pop('script_fields')\n        self._extra = d\n        return self", "response": "Updates the internal state of the object from a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef script_fields(self, **kwargs):\n        s = self._clone()\n        for name in kwargs:\n            if isinstance(kwargs[name], string_types):\n                kwargs[name] = {'script': kwargs[name]}\n        s._script_fields.update(kwargs)\n        return s", "response": "Define script fields to be calculated on hits. See\nSetException. html\n            for more details."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef source(self, fields=None, **kwargs):\n        s = self._clone()\n\n        if fields and kwargs:\n            raise ValueError(\"You cannot specify fields and kwargs at the same time.\")\n\n        if fields is not None:\n            s._source = fields\n            return s\n\n        if kwargs and not isinstance(s._source, dict):\n            s._source = {}\n\n        for key, value in kwargs.items():\n            if value is None:\n                try:\n                    del s._source[key]\n                except KeyError:\n                    pass\n            else:\n                s._source[key] = value\n\n        return s", "response": "Selectively control how the _source field is returned."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sort(self, *keys):\n        s = self._clone()\n        s._sort = []\n        for k in keys:\n            if isinstance(k, string_types) and k.startswith('-'):\n                if k[1:] == '_score':\n                    raise IllegalOperation('Sorting by `-_score` is not allowed.')\n                k = {k[1:]: {\"order\": \"desc\"}}\n            s._sort.append(k)\n        return s", "response": "Returns a new Search object with the sort information added to the search request."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a new instance of Search with highlight options set to the given kwargs.", "response": "def highlight_options(self, **kwargs):\n        \"\"\"\n        Update the global highlighting options used for this request. For\n        example::\n\n            s = Search()\n            s = s.highlight_options(order='score')\n        \"\"\"\n        s = self._clone()\n        s._highlight_opts.update(kwargs)\n        return s"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a new instance with highlighting of some fields.", "response": "def highlight(self, *fields, **kwargs):\n        \"\"\"\n        Request highlighting of some fields. All keyword arguments passed in will be\n        used as parameters for all the fields in the ``fields`` parameter. Example::\n\n            Search().highlight('title', 'body', fragment_size=50)\n\n        will produce the equivalent of::\n\n            {\n                \"highlight\": {\n                    \"fields\": {\n                        \"body\": {\"fragment_size\": 50},\n                        \"title\": {\"fragment_size\": 50}\n                    }\n                }\n            }\n\n        If you want to have different options for different fields you can call ``highlight`` twice::\n\n            Search().highlight('title', fragment_size=50).highlight('body', fragment_size=100)\n\n        which will produce::\n\n            {\n                \"highlight\": {\n                    \"fields\": {\n                        \"body\": {\"fragment_size\": 100},\n                        \"title\": {\"fragment_size\": 50}\n                    }\n                }\n            }\n\n        \"\"\"\n        s = self._clone()\n        for f in fields:\n            s._highlight[f] = kwargs\n        return s"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a suggestions request to the search.", "response": "def suggest(self, name, text, **kwargs):\n        \"\"\"\n        Add a suggestions request to the search.\n\n        :arg name: name of the suggestion\n        :arg text: text to suggest on\n\n        All keyword arguments will be added to the suggestions body. For example::\n\n            s = Search()\n            s = s.suggest('suggestion-1', 'Elasticsearch', term={'field': 'body'})\n        \"\"\"\n        s = self._clone()\n        s._suggest[name] = {'text': text}\n        s._suggest[name].update(kwargs)\n        return s"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nserialize the search into a dictionary that will be sent over as the request s body.", "response": "def to_dict(self, count=False, **kwargs):\n        \"\"\"\n        Serialize the search into the dictionary that will be sent over as the\n        request's body.\n\n        :arg count: a flag to specify if we are interested in a body for count -\n            no aggregations, no pagination bounds etc.\n\n        All additional keyword arguments will be included into the dictionary.\n        \"\"\"\n        d = {}\n\n        if self.query:\n            d[\"query\"] = self.query.to_dict()\n\n        # count request doesn't care for sorting and other things\n        if not count:\n            if self.post_filter:\n                d['post_filter'] = self.post_filter.to_dict()\n\n            if self.aggs.aggs:\n                d.update(self.aggs.to_dict())\n\n            if self._sort:\n                d['sort'] = self._sort\n\n            d.update(self._extra)\n\n            if self._source not in (None, {}):\n                d['_source'] = self._source\n\n            if self._highlight:\n                d['highlight'] = {'fields': self._highlight}\n                d['highlight'].update(self._highlight_opts)\n\n            if self._suggest:\n                d['suggest'] = self._suggest\n\n            if self._script_fields:\n                d['script_fields'] = self._script_fields\n\n        d.update(kwargs)\n        return d"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the number of hits matching the query and filters.", "response": "def count(self):\n        \"\"\"\n        Return the number of hits matching the query and filters. Note that\n        only the actual number is returned.\n        \"\"\"\n        if hasattr(self, '_response'):\n            return self._response.hits.total\n\n        es = connections.get_connection(self._using)\n\n        d = self.to_dict(count=True)\n        # TODO: failed shards detection\n        return es.count(\n            index=self._index,\n            body=d,\n            **self._params\n        )['count']"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef execute(self, ignore_cache=False):\n        if ignore_cache or not hasattr(self, '_response'):\n            es = connections.get_connection(self._using)\n\n            self._response = self._response_class(\n                self,\n                es.search(\n                    index=self._index,\n                    body=self.to_dict(),\n                    **self._params\n                )\n            )\n        return self._response", "response": "Execute the search and return an instance of Response"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nturning the search into a scan search and return a generator that will iterate over all the documents matching the query.", "response": "def scan(self):\n        \"\"\"\n        Turn the search into a scan search and return a generator that will\n        iterate over all the documents matching the query.\n\n        Use ``params`` method to specify any additional arguments you with to\n        pass to the underlying ``scan`` helper from ``elasticsearch-py`` -\n        https://elasticsearch-py.readthedocs.io/en/master/helpers.html#elasticsearch.helpers.scan\n\n        \"\"\"\n        es = connections.get_connection(self._using)\n\n        for hit in scan(\n                es,\n                query=self.to_dict(),\n                index=self._index,\n                **self._params\n        ):\n            yield self._get_result(hit)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef delete(self):\n\n        es = connections.get_connection(self._using)\n\n        return AttrDict(\n            es.delete_by_query(\n                index=self._index,\n                body=self.to_dict(),\n                **self._params\n            )\n        )", "response": "Delete the entry from the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a new search object to the request object.", "response": "def add(self, search):\n        \"\"\"\n        Adds a new :class:`~elasticsearch_dsl.Search` object to the request::\n\n            ms = MultiSearch(index='my-index')\n            ms = ms.add(Search(doc_type=Category).filter('term', category='python'))\n            ms = ms.add(Search(doc_type=Blog))\n        \"\"\"\n        ms = self._clone()\n        ms._searches.append(search)\n        return ms"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef execute(self, ignore_cache=False, raise_on_error=True):\n        if ignore_cache or not hasattr(self, '_response'):\n            es = connections.get_connection(self._using)\n\n            responses = es.msearch(\n                index=self._index,\n                body=self.to_dict(),\n                **self._params\n            )\n\n            out = []\n            for s, r in zip(self._searches, responses['responses']):\n                if r.get('error', False):\n                    if raise_on_error:\n                        raise TransportError('N/A', r['error']['type'], r['error'])\n                    r = None\n                else:\n                    r = Response(s, r)\n                out.append(r)\n\n            self._response = out\n\n        return self._response", "response": "Execute the multi search request and return a list of search results."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef make_tarball(\n    base_name,\n    base_dir,\n    compress=\"gzip\",\n    verbose=0,\n    dry_run=0,\n    owner=None,\n    group=None,\n):\n    \"\"\"Create a tar file from all the files under 'base_dir'.\n    This file may be compressed.\n\n    :param compress: Compression algorithms. Supported algorithms are:\n        'gzip': (the default)\n        'compress'\n        'bzip2'\n        None\n    For 'gzip' and 'bzip2' the internal tarfile module will be used.\n    For 'compress' the .tar will be created using tarfile, and then\n    we will spawn 'compress' afterwards.\n    The output tar file will be named 'base_name' + \".tar\",\n    possibly plus the appropriate compression extension (\".gz\",\n    \".bz2\" or \".Z\").  Return the output filename.\n    \"\"\"\n    # XXX GNU tar 1.13 has a nifty option to add a prefix directory.\n    # It's pretty new, though, so we certainly can't require it --\n    # but it would be nice to take advantage of it to skip the\n    # \"create a tree of hardlinks\" step!  (Would also be nice to\n    # detect GNU tar to use its 'z' option and save a step.)\n\n    compress_ext = {\"gzip\": \".gz\", \"bzip2\": \".bz2\", \"compress\": \".Z\"}\n\n    # flags for compression program, each element of list will be an argument\n    tarfile_compress_flag = {\"gzip\": \"gz\", \"bzip2\": \"bz2\"}\n    compress_flags = {\"compress\": [\"-f\"]}\n\n    if compress is not None and compress not in compress_ext.keys():\n        raise ValueError(\n            \"bad value for 'compress': must be None, 'gzip',\"\n            \"'bzip2' or 'compress'\"\n        )\n\n    archive_name = base_name + \".tar\"\n    if compress and compress in tarfile_compress_flag:\n        archive_name += compress_ext[compress]\n\n    mode = \"w:\" + tarfile_compress_flag.get(compress, \"\")\n\n    mkpath(os.path.dirname(archive_name), dry_run=dry_run)\n    log.info(\"Creating tar file %s with mode %s\" % (archive_name, mode))\n\n    uid = _get_uid(owner)\n    gid = _get_gid(group)\n\n    def _set_uid_gid(tarinfo):\n        if gid is not None:\n            tarinfo.gid = gid\n            tarinfo.gname = group\n        if uid is not None:\n            tarinfo.uid = uid\n            tarinfo.uname = owner\n        return tarinfo\n\n    if not dry_run:\n        tar = tarfile.open(archive_name, mode=mode)\n        # This recursively adds everything underneath base_dir\n        try:\n            try:\n                # Support for the `filter' parameter was added in Python 2.7,\n                # earlier versions will raise TypeError.\n                tar.add(base_dir, filter=_set_uid_gid)\n            except TypeError:\n                tar.add(base_dir)\n        finally:\n            tar.close()\n\n    if compress and compress not in tarfile_compress_flag:\n        spawn(\n            [compress] + compress_flags[compress] + [archive_name],\n            dry_run=dry_run,\n        )\n        return archive_name + compress_ext[compress]\n    else:\n        return archive_name", "response": "Create a tar file from all the files under base_dir."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstarting the GSS - API Key Exchange.", "response": "def start_kex(self):\n        \"\"\"\n        Start the GSS-API / SSPI Authenticated Diffie-Hellman Key Exchange.\n        \"\"\"\n        self._generate_x()\n        if self.transport.server_mode:\n            # compute f = g^x mod p, but don't send it yet\n            self.f = pow(self.G, self.x, self.P)\n            self.transport._expect_packet(MSG_KEXGSS_INIT)\n            return\n        # compute e = g^x mod p (where g=2), and send it\n        self.e = pow(self.G, self.x, self.P)\n        # Initialize GSS-API Key Exchange\n        self.gss_host = self.transport.gss_host\n        m = Message()\n        m.add_byte(c_MSG_KEXGSS_INIT)\n        m.add_string(self.kexgss.ssh_init_sec_context(target=self.gss_host))\n        m.add_mpint(self.e)\n        self.transport._send_message(m)\n        self.transport._expect_packet(\n            MSG_KEXGSS_HOSTKEY,\n            MSG_KEXGSS_CONTINUE,\n            MSG_KEXGSS_COMPLETE,\n            MSG_KEXGSS_ERROR,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_next(self, ptype, m):\n        if self.transport.server_mode and (ptype == MSG_KEXGSS_INIT):\n            return self._parse_kexgss_init(m)\n        elif not self.transport.server_mode and (ptype == MSG_KEXGSS_HOSTKEY):\n            return self._parse_kexgss_hostkey(m)\n        elif self.transport.server_mode and (ptype == MSG_KEXGSS_CONTINUE):\n            return self._parse_kexgss_continue(m)\n        elif not self.transport.server_mode and (ptype == MSG_KEXGSS_COMPLETE):\n            return self._parse_kexgss_complete(m)\n        elif ptype == MSG_KEXGSS_ERROR:\n            return self._parse_kexgss_error(m)\n        msg = \"GSS KexGroup1 asked to handle packet type {:d}\"\n        raise SSHException(msg.format(ptype))", "response": "Parse the next packet in the queue."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses the KEXGSS_HOSTKEY message.", "response": "def _parse_kexgss_hostkey(self, m):\n        \"\"\"\n        Parse the SSH2_MSG_KEXGSS_HOSTKEY message (client mode).\n\n        :param `.Message` m: The content of the SSH2_MSG_KEXGSS_HOSTKEY message\n        \"\"\"\n        # client mode\n        host_key = m.get_string()\n        self.transport.host_key = host_key\n        sig = m.get_string()\n        self.transport._verify_key(host_key, sig)\n        self.transport._expect_packet(MSG_KEXGSS_CONTINUE, MSG_KEXGSS_COMPLETE)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing the SSH2_MSG_KEXGSS_CONTINUE message.", "response": "def _parse_kexgss_continue(self, m):\n        \"\"\"\n        Parse the SSH2_MSG_KEXGSS_CONTINUE message.\n\n        :param `.Message` m: The content of the SSH2_MSG_KEXGSS_CONTINUE\n            message\n        \"\"\"\n        if not self.transport.server_mode:\n            srv_token = m.get_string()\n            m = Message()\n            m.add_byte(c_MSG_KEXGSS_CONTINUE)\n            m.add_string(\n                self.kexgss.ssh_init_sec_context(\n                    target=self.gss_host, recv_token=srv_token\n                )\n            )\n            self.transport.send_message(m)\n            self.transport._expect_packet(\n                MSG_KEXGSS_CONTINUE, MSG_KEXGSS_COMPLETE, MSG_KEXGSS_ERROR\n            )\n        else:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses the SSH2_MSG_KEXGSS_COMPLETE message.", "response": "def _parse_kexgss_complete(self, m):\n        \"\"\"\n        Parse the SSH2_MSG_KEXGSS_COMPLETE message (client mode).\n\n        :param `.Message` m: The content of the\n            SSH2_MSG_KEXGSS_COMPLETE message\n        \"\"\"\n        # client mode\n        if self.transport.host_key is None:\n            self.transport.host_key = NullHostKey()\n        self.f = m.get_mpint()\n        if (self.f < 1) or (self.f > self.P - 1):\n            raise SSHException('Server kex \"f\" is out of range')\n        mic_token = m.get_string()\n        # This must be TRUE, if there is a GSS-API token in this message.\n        bool = m.get_boolean()\n        srv_token = None\n        if bool:\n            srv_token = m.get_string()\n        K = pow(self.f, self.x, self.P)\n        # okay, build up the hash H of\n        # (V_C || V_S || I_C || I_S || K_S || e || f || K)\n        hm = Message()\n        hm.add(\n            self.transport.local_version,\n            self.transport.remote_version,\n            self.transport.local_kex_init,\n            self.transport.remote_kex_init,\n        )\n        hm.add_string(self.transport.host_key.__str__())\n        hm.add_mpint(self.e)\n        hm.add_mpint(self.f)\n        hm.add_mpint(K)\n        H = sha1(str(hm)).digest()\n        self.transport._set_K_H(K, H)\n        if srv_token is not None:\n            self.kexgss.ssh_init_sec_context(\n                target=self.gss_host, recv_token=srv_token\n            )\n            self.kexgss.ssh_check_mic(mic_token, H)\n        else:\n            self.kexgss.ssh_check_mic(mic_token, H)\n        self.transport.gss_kex_used = True\n        self.transport._activate_outbound()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _parse_kexgss_init(self, m):\n        # server mode\n        client_token = m.get_string()\n        self.e = m.get_mpint()\n        if (self.e < 1) or (self.e > self.P - 1):\n            raise SSHException('Client kex \"e\" is out of range')\n        K = pow(self.e, self.x, self.P)\n        self.transport.host_key = NullHostKey()\n        key = self.transport.host_key.__str__()\n        # okay, build up the hash H of\n        # (V_C || V_S || I_C || I_S || K_S || e || f || K)\n        hm = Message()\n        hm.add(\n            self.transport.remote_version,\n            self.transport.local_version,\n            self.transport.remote_kex_init,\n            self.transport.local_kex_init,\n        )\n        hm.add_string(key)\n        hm.add_mpint(self.e)\n        hm.add_mpint(self.f)\n        hm.add_mpint(K)\n        H = sha1(hm.asbytes()).digest()\n        self.transport._set_K_H(K, H)\n        srv_token = self.kexgss.ssh_accept_sec_context(\n            self.gss_host, client_token\n        )\n        m = Message()\n        if self.kexgss._gss_srv_ctxt_status:\n            mic_token = self.kexgss.ssh_get_mic(\n                self.transport.session_id, gss_kex=True\n            )\n            m.add_byte(c_MSG_KEXGSS_COMPLETE)\n            m.add_mpint(self.f)\n            m.add_string(mic_token)\n            if srv_token is not None:\n                m.add_boolean(True)\n                m.add_string(srv_token)\n            else:\n                m.add_boolean(False)\n            self.transport._send_message(m)\n            self.transport.gss_kex_used = True\n            self.transport._activate_outbound()\n        else:\n            m.add_byte(c_MSG_KEXGSS_CONTINUE)\n            m.add_string(srv_token)\n            self.transport._send_message(m)\n            self.transport._expect_packet(\n                MSG_KEXGSS_CONTINUE, MSG_KEXGSS_COMPLETE, MSG_KEXGSS_ERROR\n            )", "response": "Parse the SSH2_MSG_KEXGSS_INIT message."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nstarts the GSS - API Group Exchange", "response": "def start_kex(self):\n        \"\"\"\n        Start the GSS-API / SSPI Authenticated Diffie-Hellman Group Exchange\n        \"\"\"\n        if self.transport.server_mode:\n            self.transport._expect_packet(MSG_KEXGSS_GROUPREQ)\n            return\n        # request a bit range: we accept (min_bits) to (max_bits), but prefer\n        # (preferred_bits).  according to the spec, we shouldn't pull the\n        # minimum up above 1024.\n        self.gss_host = self.transport.gss_host\n        m = Message()\n        m.add_byte(c_MSG_KEXGSS_GROUPREQ)\n        m.add_int(self.min_bits)\n        m.add_int(self.preferred_bits)\n        m.add_int(self.max_bits)\n        self.transport._send_message(m)\n        self.transport._expect_packet(MSG_KEXGSS_GROUP)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_next(self, ptype, m):\n        if ptype == MSG_KEXGSS_GROUPREQ:\n            return self._parse_kexgss_groupreq(m)\n        elif ptype == MSG_KEXGSS_GROUP:\n            return self._parse_kexgss_group(m)\n        elif ptype == MSG_KEXGSS_INIT:\n            return self._parse_kexgss_gex_init(m)\n        elif ptype == MSG_KEXGSS_HOSTKEY:\n            return self._parse_kexgss_hostkey(m)\n        elif ptype == MSG_KEXGSS_CONTINUE:\n            return self._parse_kexgss_continue(m)\n        elif ptype == MSG_KEXGSS_COMPLETE:\n            return self._parse_kexgss_complete(m)\n        elif ptype == MSG_KEXGSS_ERROR:\n            return self._parse_kexgss_error(m)\n        msg = \"KexGex asked to handle packet type {:d}\"\n        raise SSHException(msg.format(ptype))", "response": "Parse the next packet in the queue."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _parse_kexgss_group(self, m):\n        self.p = m.get_mpint()\n        self.g = m.get_mpint()\n        # reject if p's bit length < 1024 or > 8192\n        bitlen = util.bit_length(self.p)\n        if (bitlen < 1024) or (bitlen > 8192):\n            raise SSHException(\n                \"Server-generated gex p (don't ask) is out of range \"\n                \"({} bits)\".format(bitlen)\n            )\n        self.transport._log(\n            DEBUG, \"Got server p ({} bits)\".format(bitlen)\n        )  # noqa\n        self._generate_x()\n        # now compute e = g^x mod p\n        self.e = pow(self.g, self.x, self.p)\n        m = Message()\n        m.add_byte(c_MSG_KEXGSS_INIT)\n        m.add_string(self.kexgss.ssh_init_sec_context(target=self.gss_host))\n        m.add_mpint(self.e)\n        self.transport._send_message(m)\n        self.transport._expect_packet(\n            MSG_KEXGSS_HOSTKEY,\n            MSG_KEXGSS_CONTINUE,\n            MSG_KEXGSS_COMPLETE,\n            MSG_KEXGSS_ERROR,\n        )", "response": "Parse the KEXGSS_GROUP message."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses the SSH2_MSG_KEXGSS_COMPLETE message.", "response": "def _parse_kexgss_complete(self, m):\n        \"\"\"\n        Parse the SSH2_MSG_KEXGSS_COMPLETE message (client mode).\n\n        :param `Message` m: The content of the SSH2_MSG_KEXGSS_COMPLETE message\n        \"\"\"\n        if self.transport.host_key is None:\n            self.transport.host_key = NullHostKey()\n        self.f = m.get_mpint()\n        mic_token = m.get_string()\n        # This must be TRUE, if there is a GSS-API token in this message.\n        bool = m.get_boolean()\n        srv_token = None\n        if bool:\n            srv_token = m.get_string()\n        if (self.f < 1) or (self.f > self.p - 1):\n            raise SSHException('Server kex \"f\" is out of range')\n        K = pow(self.f, self.x, self.p)\n        # okay, build up the hash H of\n        # (V_C || V_S || I_C || I_S || K_S || min || n || max || p || g || e || f || K)  # noqa\n        hm = Message()\n        hm.add(\n            self.transport.local_version,\n            self.transport.remote_version,\n            self.transport.local_kex_init,\n            self.transport.remote_kex_init,\n            self.transport.host_key.__str__(),\n        )\n        if not self.old_style:\n            hm.add_int(self.min_bits)\n        hm.add_int(self.preferred_bits)\n        if not self.old_style:\n            hm.add_int(self.max_bits)\n        hm.add_mpint(self.p)\n        hm.add_mpint(self.g)\n        hm.add_mpint(self.e)\n        hm.add_mpint(self.f)\n        hm.add_mpint(K)\n        H = sha1(hm.asbytes()).digest()\n        self.transport._set_K_H(K, H)\n        if srv_token is not None:\n            self.kexgss.ssh_init_sec_context(\n                target=self.gss_host, recv_token=srv_token\n            )\n            self.kexgss.ssh_check_mic(mic_token, H)\n        else:\n            self.kexgss.ssh_check_mic(mic_token, H)\n        self.transport.gss_kex_used = True\n        self.transport._activate_outbound()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse the GSS - API error message.", "response": "def _parse_kexgss_error(self, m):\n        \"\"\"\n        Parse the SSH2_MSG_KEXGSS_ERROR message (client mode).\n        The server may send a GSS-API error message. if it does, we display\n        the error by throwing an exception (client mode).\n\n        :param `Message` m:  The content of the SSH2_MSG_KEXGSS_ERROR message\n        :raise SSHException: Contains GSS-API major and minor status as well as\n                             the error message and the language tag of the\n                             message\n        \"\"\"\n        maj_status = m.get_int()\n        min_status = m.get_int()\n        err_msg = m.get_string()\n        m.get_string()  # we don't care about the language (lang_tag)!\n        raise SSHException(\n            \"\"\"GSS-API Error:\nMajor Status: {}\nMinor Status: {}\nError Message: {}\n\"\"\".format(\n                maj_status, min_status, err_msg\n            )\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef from_stat(cls, obj, filename=None):\n        attr = cls()\n        attr.st_size = obj.st_size\n        attr.st_uid = obj.st_uid\n        attr.st_gid = obj.st_gid\n        attr.st_mode = obj.st_mode\n        attr.st_atime = obj.st_atime\n        attr.st_mtime = obj.st_mtime\n        if filename is not None:\n            attr.filename = filename\n        return attr", "response": "Create an SFTPAttributes object from an existing stat object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate a new private RSA key. This factory function can be used to generate a new host key or authentication key. :param int bits: number of bits the generated key should be. :param progress_func: Unused :return: new `.RSAKey` private key", "response": "def generate(bits, progress_func=None):\n        \"\"\"\n        Generate a new private RSA key.  This factory function can be used to\n        generate a new host key or authentication key.\n\n        :param int bits: number of bits the generated key should be.\n        :param progress_func: Unused\n        :return: new `.RSAKey` private key\n        \"\"\"\n        key = rsa.generate_private_key(\n            public_exponent=65537, key_size=bits, backend=default_backend()\n        )\n        return RSAKey(key=key)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the offset of the relevant prefetch buffer at the given offset.", "response": "def _data_in_prefetch_buffers(self, offset):\n        \"\"\"\n        if a block of data is present in the prefetch buffers, at the given\n        offset, return the offset of the relevant prefetch buffer.  otherwise,\n        return None.  this guarantees nothing about the number of bytes\n        collected in the prefetch buffer so far.\n        \"\"\"\n        k = [i for i in self._prefetch_data.keys() if i <= offset]\n        if len(k) == 0:\n            return None\n        index = max(k)\n        buf_offset = offset - index\n        if buf_offset >= len(self._prefetch_data[index]):\n            # it's not here\n            return None\n        return index"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef seek(self, offset, whence=0):\n        self.flush()\n        if whence == self.SEEK_SET:\n            self._realpos = self._pos = offset\n        elif whence == self.SEEK_CUR:\n            self._pos += offset\n            self._realpos = self._pos\n        else:\n            self._realpos = self._pos = self._get_size() + offset\n        self._rbuffer = bytes()", "response": "Seek to the specified offset in the file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nretrieve information about this file.", "response": "def stat(self):\n        \"\"\"\n        Retrieve information about this file from the remote system.  This is\n        exactly like `.SFTPClient.stat`, except that it operates on an\n        already-open file.\n\n        :returns:\n            an `.SFTPAttributes` object containing attributes about this file.\n        \"\"\"\n        t, msg = self.sftp._request(CMD_FSTAT, self.handle)\n        if t != CMD_ATTRS:\n            raise SFTPError(\"Expected attributes\")\n        return SFTPAttributes._from_msg(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef truncate(self, size):\n        self.sftp._log(\n            DEBUG, \"truncate({}, {!r})\".format(hexlify(self.handle), size)\n        )\n        attr = SFTPAttributes()\n        attr.st_size = size\n        self.sftp._request(CMD_FSETSTAT, self.handle, attr)", "response": "Change the size of the file object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nasking the server for a hash of a section of this file. This can be used to verify a successful upload or download, or for various rsync-like operations. The file is hashed from ``offset``, for ``length`` bytes. If ``length`` is 0, the remainder of the file is hashed. Thus, if both ``offset`` and ``length`` are zero, the entire file is hashed. Normally, ``block_size`` will be 0 (the default), and this method will return a byte string representing the requested hash (for example, a string of length 16 for MD5, or 20 for SHA-1). If a non-zero ``block_size`` is given, each chunk of the file (from ``offset`` to ``offset + length``) of ``block_size`` bytes is computed as a separate hash. The hash results are all concatenated and returned as a single string. For example, ``check('sha1', 0, 1024, 512)`` will return a string of length 40. The first 20 bytes will be the SHA-1 of the first 512 bytes of the file, and the last 20 bytes will be the SHA-1 of the next 512 bytes. :param str hash_algorithm: the name of the hash algorithm to use (normally ``\"sha1\"`` or ``\"md5\"``) :param offset: offset into the file to begin hashing (0 means to start from the beginning) :param length: number of bytes to hash (0 means continue to the end of the file) :param int block_size: number of bytes to hash per result (must not be less than 256; 0 means to compute only one hash of the entire segment) :return: `str` of bytes representing the hash of each block, concatenated together :raises: ``IOError`` -- if the server doesn't support the \"check-file\" extension, or possibly doesn't support the hash algorithm requested .. note:: Many (most?) servers don't support this extension yet. .. versionadded:: 1.4", "response": "def check(self, hash_algorithm, offset=0, length=0, block_size=0):\n        \"\"\"\n        Ask the server for a hash of a section of this file.  This can be used\n        to verify a successful upload or download, or for various rsync-like\n        operations.\n\n        The file is hashed from ``offset``, for ``length`` bytes.\n        If ``length`` is 0, the remainder of the file is hashed.  Thus, if both\n        ``offset`` and ``length`` are zero, the entire file is hashed.\n\n        Normally, ``block_size`` will be 0 (the default), and this method will\n        return a byte string representing the requested hash (for example, a\n        string of length 16 for MD5, or 20 for SHA-1).  If a non-zero\n        ``block_size`` is given, each chunk of the file (from ``offset`` to\n        ``offset + length``) of ``block_size`` bytes is computed as a separate\n        hash.  The hash results are all concatenated and returned as a single\n        string.\n\n        For example, ``check('sha1', 0, 1024, 512)`` will return a string of\n        length 40.  The first 20 bytes will be the SHA-1 of the first 512 bytes\n        of the file, and the last 20 bytes will be the SHA-1 of the next 512\n        bytes.\n\n        :param str hash_algorithm:\n            the name of the hash algorithm to use (normally ``\"sha1\"`` or\n            ``\"md5\"``)\n        :param offset:\n            offset into the file to begin hashing (0 means to start from the\n            beginning)\n        :param length:\n            number of bytes to hash (0 means continue to the end of the file)\n        :param int block_size:\n            number of bytes to hash per result (must not be less than 256; 0\n            means to compute only one hash of the entire segment)\n        :return:\n            `str` of bytes representing the hash of each block, concatenated\n            together\n\n        :raises:\n            ``IOError`` -- if the server doesn't support the \"check-file\"\n            extension, or possibly doesn't support the hash algorithm requested\n\n        .. note:: Many (most?) servers don't support this extension yet.\n\n        .. versionadded:: 1.4\n        \"\"\"\n        t, msg = self.sftp._request(\n            CMD_EXTENDED,\n            \"check-file\",\n            self.handle,\n            hash_algorithm,\n            long(offset),\n            long(length),\n            block_size,\n        )\n        msg.get_text()  # ext\n        msg.get_text()  # alg\n        data = msg.get_remainder()\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef prefetch(self, file_size=None):\n        if file_size is None:\n            file_size = self.stat().st_size\n\n        # queue up async reads for the rest of the file\n        chunks = []\n        n = self._realpos\n        while n < file_size:\n            chunk = min(self.MAX_REQUEST_SIZE, file_size - n)\n            chunks.append((n, chunk))\n            n += chunk\n        if len(chunks) > 0:\n            self._start_prefetch(chunks)", "response": "Pre - fetch the remaining contents of this file in anticipation of future canicalis."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading a set of blocks from the file.", "response": "def readv(self, chunks):\n        \"\"\"\n        Read a set of blocks from the file by (offset, length).  This is more\n        efficient than doing a series of `.seek` and `.read` calls, since the\n        prefetch machinery is used to retrieve all the requested blocks at\n        once.\n\n        :param chunks:\n            a list of ``(offset, length)`` tuples indicating which sections of\n            the file to read\n        :return: a list of blocks read, in the same order as in ``chunks``\n\n        .. versionadded:: 1.5.4\n        \"\"\"\n        self.sftp._log(\n            DEBUG, \"readv({}, {!r})\".format(hexlify(self.handle), chunks)\n        )\n\n        read_chunks = []\n        for offset, size in chunks:\n            # don't fetch data that's already in the prefetch buffer\n            if self._data_in_prefetch_buffers(\n                offset\n            ) or self._data_in_prefetch_requests(offset, size):\n                continue\n\n            # break up anything larger than the max read size\n            while size > 0:\n                chunk_size = min(size, self.MAX_REQUEST_SIZE)\n                read_chunks.append((offset, chunk_size))\n                offset += chunk_size\n                size -= chunk_size\n\n        self._start_prefetch(read_chunks)\n        # now we can just devolve to a bunch of read()s :)\n        for x in chunks:\n            self.seek(x[0])\n            yield self.read(x[1])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _check_exception(self):\n        if self._saved_exception is not None:\n            x = self._saved_exception\n            self._saved_exception = None\n            raise x", "response": "Raise the exception if there is one."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nexecute a command on the server.", "response": "def exec_command(self, command):\n        \"\"\"\n        Execute a command on the server.  If the server allows it, the channel\n        will then be directly connected to the stdin, stdout, and stderr of\n        the command being executed.\n\n        When the command finishes executing, the channel will be closed and\n        can't be reused.  You must open a new channel if you wish to execute\n        another command.\n\n        :param str command: a shell command to execute.\n\n        :raises:\n            `.SSHException` -- if the request was rejected or the channel was\n            closed\n        \"\"\"\n        m = Message()\n        m.add_byte(cMSG_CHANNEL_REQUEST)\n        m.add_int(self.remote_chanid)\n        m.add_string(\"exec\")\n        m.add_boolean(True)\n        m.add_string(command)\n        self._event_pending()\n        self.transport._send_user_message(m)\n        self._wait_for_event()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef resize_pty(self, width=80, height=24, width_pixels=0, height_pixels=0):\n        m = Message()\n        m.add_byte(cMSG_CHANNEL_REQUEST)\n        m.add_int(self.remote_chanid)\n        m.add_string(\"window-change\")\n        m.add_boolean(False)\n        m.add_int(width)\n        m.add_int(height)\n        m.add_int(width_pixels)\n        m.add_int(height_pixels)\n        self.transport._send_user_message(m)", "response": "Resize the pseudo - terminal."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate the remote shell environment.", "response": "def update_environment(self, environment):\n        \"\"\"\n        Updates this channel's remote shell environment.\n\n        .. note::\n            This operation is additive - i.e. the current environment is not\n            reset before the given environment variables are set.\n\n        .. warning::\n            Servers may silently reject some environment variables; see the\n            warning in `set_environment_variable` for details.\n\n        :param dict environment:\n            a dictionary containing the name and respective values to set\n        :raises:\n            `.SSHException` -- if any of the environment variables was rejected\n            by the server or the channel was closed\n        \"\"\"\n        for name, value in environment.items():\n            try:\n                self.set_environment_variable(name, value)\n            except SSHException as e:\n                err = 'Failed to set environment variable \"{}\".'\n                raise SSHException(err.format(name), e)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the value of an environment variable.", "response": "def set_environment_variable(self, name, value):\n        \"\"\"\n        Set the value of an environment variable.\n\n        .. warning::\n            The server may reject this request depending on its ``AcceptEnv``\n            setting; such rejections will fail silently (which is common client\n            practice for this particular request type). Make sure you\n            understand your server's configuration before using!\n\n        :param str name: name of the environment variable\n        :param str value: value of the environment variable\n\n        :raises:\n            `.SSHException` -- if the request was rejected or the channel was\n            closed\n        \"\"\"\n        m = Message()\n        m.add_byte(cMSG_CHANNEL_REQUEST)\n        m.add_int(self.remote_chanid)\n        m.add_string(\"env\")\n        m.add_boolean(False)\n        m.add_string(name)\n        m.add_string(value)\n        self.transport._send_user_message(m)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef recv_exit_status(self):\n        self.status_event.wait()\n        assert self.status_event.is_set()\n        return self.exit_status", "response": "This method returns the exit status of the process on the server."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrequest an X11 session on this channel.", "response": "def request_x11(\n        self,\n        screen_number=0,\n        auth_protocol=None,\n        auth_cookie=None,\n        single_connection=False,\n        handler=None,\n    ):\n        \"\"\"\n        Request an x11 session on this channel.  If the server allows it,\n        further x11 requests can be made from the server to the client,\n        when an x11 application is run in a shell session.\n\n        From :rfc:`4254`::\n\n            It is RECOMMENDED that the 'x11 authentication cookie' that is\n            sent be a fake, random cookie, and that the cookie be checked and\n            replaced by the real cookie when a connection request is received.\n\n        If you omit the auth_cookie, a new secure random 128-bit value will be\n        generated, used, and returned.  You will need to use this value to\n        verify incoming x11 requests and replace them with the actual local\n        x11 cookie (which requires some knowledge of the x11 protocol).\n\n        If a handler is passed in, the handler is called from another thread\n        whenever a new x11 connection arrives.  The default handler queues up\n        incoming x11 connections, which may be retrieved using\n        `.Transport.accept`.  The handler's calling signature is::\n\n            handler(channel: Channel, (address: str, port: int))\n\n        :param int screen_number: the x11 screen number (0, 10, etc.)\n        :param str auth_protocol:\n            the name of the X11 authentication method used; if none is given,\n            ``\"MIT-MAGIC-COOKIE-1\"`` is used\n        :param str auth_cookie:\n            hexadecimal string containing the x11 auth cookie; if none is\n            given, a secure random 128-bit value is generated\n        :param bool single_connection:\n            if True, only a single x11 connection will be forwarded (by\n            default, any number of x11 connections can arrive over this\n            session)\n        :param handler:\n            an optional callable handler to use for incoming X11 connections\n        :return: the auth_cookie used\n        \"\"\"\n        if auth_protocol is None:\n            auth_protocol = \"MIT-MAGIC-COOKIE-1\"\n        if auth_cookie is None:\n            auth_cookie = binascii.hexlify(os.urandom(16))\n\n        m = Message()\n        m.add_byte(cMSG_CHANNEL_REQUEST)\n        m.add_int(self.remote_chanid)\n        m.add_string(\"x11-req\")\n        m.add_boolean(True)\n        m.add_boolean(single_connection)\n        m.add_string(auth_protocol)\n        m.add_string(auth_cookie)\n        m.add_int(screen_number)\n        self._event_pending()\n        self.transport._send_user_message(m)\n        self._wait_for_event()\n        self.transport._set_x11_handler(handler)\n        return auth_cookie"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef request_forward_agent(self, handler):\n        m = Message()\n        m.add_byte(cMSG_CHANNEL_REQUEST)\n        m.add_int(self.remote_chanid)\n        m.add_string(\"auth-agent-req@openssh.com\")\n        m.add_boolean(False)\n        self.transport._send_user_message(m)\n        self.transport._set_forward_agent_handler(handler)\n        return True", "response": "Request a forward SSH Agent on this channel."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset whether stderr should be combined into stdout on this channel.", "response": "def set_combine_stderr(self, combine):\n        \"\"\"\n        Set whether stderr should be combined into stdout on this channel.\n        The default is ``False``, but in some cases it may be convenient to\n        have both streams combined.\n\n        If this is ``False``, and `exec_command` is called (or ``invoke_shell``\n        with no pty), output to stderr will not show up through the `recv`\n        and `recv_ready` calls.  You will have to use `recv_stderr` and\n        `recv_stderr_ready` to get stderr output.\n\n        If this is ``True``, data will never show up via `recv_stderr` or\n        `recv_stderr_ready`.\n\n        :param bool combine:\n            ``True`` if stderr output should be combined into stdout on this\n            channel.\n        :return: the previous setting (a `bool`).\n\n        .. versionadded:: 1.1\n        \"\"\"\n        data = bytes()\n        self.lock.acquire()\n        try:\n            old = self.combine_stderr\n            self.combine_stderr = combine\n            if combine and not old:\n                # copy old stderr buffer into primary buffer\n                data = self.in_stderr_buffer.empty()\n        finally:\n            self.lock.release()\n        if len(data) > 0:\n            self._feed(data)\n        return old"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncloses the channel. All future read/write operations on the channel will fail. The remote end will receive no more data (after queued data is flushed). Channels are automatically closed when their `.Transport` is closed or when they are garbage collected.", "response": "def close(self):\n        \"\"\"\n        Close the channel.  All future read/write operations on the channel\n        will fail.  The remote end will receive no more data (after queued data\n        is flushed).  Channels are automatically closed when their `.Transport`\n        is closed or when they are garbage collected.\n        \"\"\"\n        self.lock.acquire()\n        try:\n            # only close the pipe when the user explicitly closes the channel.\n            # otherwise they will get unpleasant surprises.  (and do it before\n            # checking self.closed, since the remote host may have already\n            # closed the connection.)\n            if self._pipe is not None:\n                self._pipe.close()\n                self._pipe = None\n\n            if not self.active or self.closed:\n                return\n            msgs = self._close_internal()\n        finally:\n            self.lock.release()\n        for m in msgs:\n            if m is not None:\n                self.transport._send_user_message(m)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn True if data can be written to this channel without blocking.", "response": "def send_ready(self):\n        \"\"\"\n        Returns true if data can be written to this channel without blocking.\n        This means the channel is either closed (so any write attempt would\n        return immediately) or there is at least one byte of space in the\n        outbound buffer. If there is at least one byte of space in the\n        outbound buffer, a `send` call will succeed immediately and return\n        the number of bytes actually written.\n\n        :return:\n            ``True`` if a `send` call on this channel would immediately succeed\n            or fail\n        \"\"\"\n        self.lock.acquire()\n        try:\n            if self.closed or self.eof_sent:\n                return True\n            return self.out_window_size > 0\n        finally:\n            self.lock.release()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsend data to the channel.", "response": "def send(self, s):\n        \"\"\"\n        Send data to the channel.  Returns the number of bytes sent, or 0 if\n        the channel stream is closed.  Applications are responsible for\n        checking that all data has been sent: if only some of the data was\n        transmitted, the application needs to attempt delivery of the remaining\n        data.\n\n        :param str s: data to send\n        :return: number of bytes actually sent, as an `int`\n\n        :raises socket.timeout: if no data could be sent before the timeout set\n            by `settimeout`.\n        \"\"\"\n\n        m = Message()\n        m.add_byte(cMSG_CHANNEL_DATA)\n        m.add_int(self.remote_chanid)\n        return self._send(s, m)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef send_stderr(self, s):\n\n        m = Message()\n        m.add_byte(cMSG_CHANNEL_EXTENDED_DATA)\n        m.add_int(self.remote_chanid)\n        m.add_int(1)\n        return self._send(s, m)", "response": "Send data to the channel stderr stream."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsending all data to the channel.", "response": "def sendall(self, s):\n        \"\"\"\n        Send data to the channel, without allowing partial results.  Unlike\n        `send`, this method continues to send data from the given string until\n        either all data has been sent or an error occurs.  Nothing is returned.\n\n        :param str s: data to send.\n\n        :raises socket.timeout:\n            if sending stalled for longer than the timeout set by `settimeout`.\n        :raises socket.error:\n            if an error occurred before the entire string was sent.\n\n        .. note::\n            If the channel is closed while only part of the data has been\n            sent, there is no way to determine how much data (if any) was sent.\n            This is irritating, but identically follows Python's API.\n        \"\"\"\n        while s:\n            sent = self.send(s)\n            s = s[sent:]\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sendall_stderr(self, s):\n        while s:\n            sent = self.send_stderr(s)\n            s = s[sent:]\n        return None", "response": "Send data to the channel s stderr stream without allowing partial\n        results."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fileno(self):\n        self.lock.acquire()\n        try:\n            if self._pipe is not None:\n                return self._pipe.fileno()\n            # create the pipe and feed in any existing data\n            self._pipe = pipe.make_pipe()\n            p1, p2 = pipe.make_or_pipe(self._pipe)\n            self.in_buffer.set_event(p1)\n            self.in_stderr_buffer.set_event(p2)\n            return self._pipe.fileno()\n        finally:\n            self.lock.release()", "response": "Returns an OS - level file descriptor which can be used for polling or reading or writing."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef shutdown(self, how):\n        if (how == 0) or (how == 2):\n            # feign \"read\" shutdown\n            self.eof_received = 1\n        if (how == 1) or (how == 2):\n            self.lock.acquire()\n            try:\n                m = self._send_eof()\n            finally:\n                self.lock.release()\n            if m is not None:\n                self.transport._send_user_message(m)", "response": "Shut down one or both halves of the connection."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwait for the send window to be open up and allocate up to size bytes.", "response": "def _wait_for_send_window(self, size):\n        \"\"\"\n        (You are already holding the lock.)\n        Wait for the send window to open up, and allocate up to ``size`` bytes\n        for transmission.  If no space opens up before the timeout, a timeout\n        exception is raised.  Returns the number of bytes available to send\n        (may be less than requested).\n        \"\"\"\n        # you are already holding the lock\n        if self.closed or self.eof_sent:\n            return 0\n        if self.out_window_size == 0:\n            # should we block?\n            if self.timeout == 0.0:\n                raise socket.timeout()\n            # loop here in case we get woken up but a different thread has\n            # filled the buffer\n            timeout = self.timeout\n            while self.out_window_size == 0:\n                if self.closed or self.eof_sent:\n                    return 0\n                then = time.time()\n                self.out_buffer_cv.wait(timeout)\n                if timeout is not None:\n                    timeout -= time.time() - then\n                    if timeout <= 0.0:\n                        raise socket.timeout()\n        # we have some window to squeeze into\n        if self.closed or self.eof_sent:\n            return 0\n        if self.out_window_size < size:\n            size = self.out_window_size\n        if self.out_max_packet_size - 64 < size:\n            size = self.out_max_packet_size - 64\n        self.out_window_size -= size\n        if self.ultra_debug:\n            self._log(DEBUG, \"window down to {}\".format(self.out_window_size))\n        return size"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert an errno value into an appropriate SFTP result code.", "response": "def convert_errno(e):\n        \"\"\"\n        Convert an errno value (as from an ``OSError`` or ``IOError``) into a\n        standard SFTP result code.  This is a convenience function for trapping\n        exceptions in server code and returning an appropriate result.\n\n        :param int e: an errno code, as from ``OSError.errno``.\n        :return: an `int` SFTP error code like ``SFTP_NO_SUCH_FILE``.\n        \"\"\"\n        if e == errno.EACCES:\n            # permission denied\n            return SFTP_PERMISSION_DENIED\n        elif (e == errno.ENOENT) or (e == errno.ENOTDIR):\n            # no such file\n            return SFTP_NO_SUCH_FILE\n        else:\n            return SFTP_FAILURE"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _convert_pflags(self, pflags):\n        if (pflags & SFTP_FLAG_READ) and (pflags & SFTP_FLAG_WRITE):\n            flags = os.O_RDWR\n        elif pflags & SFTP_FLAG_WRITE:\n            flags = os.O_WRONLY\n        else:\n            flags = os.O_RDONLY\n        if pflags & SFTP_FLAG_APPEND:\n            flags |= os.O_APPEND\n        if pflags & SFTP_FLAG_CREATE:\n            flags |= os.O_CREAT\n        if pflags & SFTP_FLAG_TRUNC:\n            flags |= os.O_TRUNC\n        if pflags & SFTP_FLAG_EXCL:\n            flags |= os.O_EXCL\n        return flags", "response": "converts flags from SFTP - style open() flags to Python s os. open flags"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_event(self, event):\n        self._lock.acquire()\n        try:\n            self._event = event\n            # Make sure the event starts in `set` state if we appear to already\n            # be closed; otherwise, if we start in `clear` state & are closed,\n            # nothing will ever call `.feed` and the event (& OS pipe, if we're\n            # wrapping one - see `Channel.fileno`) will permanently stay in\n            # `clear`, causing deadlock if e.g. `select`ed upon.\n            if self._closed or len(self._buffer) > 0:\n                event.set()\n            else:\n                event.clear()\n        finally:\n            self._lock.release()", "response": "Sets an event on this buffer."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfeeds new data into the pipe.", "response": "def feed(self, data):\n        \"\"\"\n        Feed new data into this pipe.  This method is assumed to be called\n        from a separate thread, so synchronization is done.\n\n        :param data: the data to add, as a ``str`` or ``bytes``\n        \"\"\"\n        self._lock.acquire()\n        try:\n            if self._event is not None:\n                self._event.set()\n            self._buffer_frombytes(b(data))\n            self._cv.notifyAll()\n        finally:\n            self._lock.release()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read_ready(self):\n        self._lock.acquire()\n        try:\n            if len(self._buffer) == 0:\n                return False\n            return True\n        finally:\n            self._lock.release()", "response": "Returns True if data is buffered and ready to be read from this\n            feeder."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read(self, nbytes, timeout=None):\n        out = bytes()\n        self._lock.acquire()\n        try:\n            if len(self._buffer) == 0:\n                if self._closed:\n                    return out\n                # should we block?\n                if timeout == 0.0:\n                    raise PipeTimeout()\n                # loop here in case we get woken up but a different thread has\n                # grabbed everything in the buffer.\n                while (len(self._buffer) == 0) and not self._closed:\n                    then = time.time()\n                    self._cv.wait(timeout)\n                    if timeout is not None:\n                        timeout -= time.time() - then\n                        if timeout <= 0.0:\n                            raise PipeTimeout()\n\n            # something's in the buffer and we have the lock!\n            if len(self._buffer) <= nbytes:\n                out = self._buffer_tobytes()\n                del self._buffer[:]\n                if (self._event is not None) and not self._closed:\n                    self._event.clear()\n            else:\n                out = self._buffer_tobytes(nbytes)\n                del self._buffer[:nbytes]\n        finally:\n            self._lock.release()\n\n        return out", "response": "Read data from the pipe."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nclears out the buffer and return all data that was in it.", "response": "def empty(self):\n        \"\"\"\n        Clear out the buffer and return all data that was in it.\n\n        :return:\n            any data that was in the buffer prior to clearing it out, as a\n            `str`\n        \"\"\"\n        self._lock.acquire()\n        try:\n            out = self._buffer_tobytes()\n            del self._buffer[:]\n            if (self._event is not None) and not self._closed:\n                self._event.clear()\n            return out\n        finally:\n            self._lock.release()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nclose this pipe object.", "response": "def close(self):\n        \"\"\"\n        Close this pipe object.  Future calls to `read` after the buffer\n        has been emptied will return immediately with an empty string.\n        \"\"\"\n        self._lock.acquire()\n        try:\n            self._closed = True\n            self._cv.notifyAll()\n            if self._event is not None:\n                self._event.set()\n        finally:\n            self._lock.release()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread an OpenSSH config file from the given file - like object.", "response": "def parse(self, file_obj):\n        \"\"\"\n        Read an OpenSSH config from the given file object.\n\n        :param file_obj: a file-like object to read the config file from\n        \"\"\"\n        host = {\"host\": [\"*\"], \"config\": {}}\n        for line in file_obj:\n            # Strip any leading or trailing whitespace from the line.\n            # Refer to https://github.com/paramiko/paramiko/issues/499\n            line = line.strip()\n            if not line or line.startswith(\"#\"):\n                continue\n\n            match = re.match(self.SETTINGS_REGEX, line)\n            if not match:\n                raise Exception(\"Unparsable line {}\".format(line))\n            key = match.group(1).lower()\n            value = match.group(2)\n\n            if key == \"host\":\n                self._config.append(host)\n                host = {\"host\": self._get_hosts(value), \"config\": {}}\n            elif key == \"proxycommand\" and value.lower() == \"none\":\n                # Store 'none' as None; prior to 3.x, it will get stripped out\n                # at the end (for compatibility with issue #415). After 3.x, it\n                # will simply not get stripped, leaving a nice explicit marker.\n                host[\"config\"][key] = None\n            else:\n                if value.startswith('\"') and value.endswith('\"'):\n                    value = value[1:-1]\n\n                # identityfile, localforward, remoteforward keys are special\n                # cases, since they are allowed to be specified multiple times\n                # and they should be tried in order of specification.\n                if key in [\"identityfile\", \"localforward\", \"remoteforward\"]:\n                    if key in host[\"config\"]:\n                        host[\"config\"][key].append(value)\n                    else:\n                        host[\"config\"][key] = [value]\n                elif key not in host[\"config\"]:\n                    host[\"config\"][key] = value\n        self._config.append(host)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a dict of config options for a given hostname.", "response": "def lookup(self, hostname):\n        \"\"\"\n        Return a dict (`SSHConfigDict`) of config options for a given hostname.\n\n        The host-matching rules of OpenSSH's ``ssh_config`` man page are used:\n        For each parameter, the first obtained value will be used.  The\n        configuration files contain sections separated by ``Host``\n        specifications, and that section is only applied for hosts that match\n        one of the patterns given in the specification.\n\n        Since the first obtained value for each parameter is used, more host-\n        specific declarations should be given near the beginning of the file,\n        and general defaults at the end.\n\n        The keys in the returned dict are all normalized to lowercase (look for\n        ``\"port\"``, not ``\"Port\"``. The values are processed according to the\n        rules for substitution variable expansion in ``ssh_config``.\n\n        Finally, please see the docs for `SSHConfigDict` for deeper info on\n        features such as optional type conversion methods, e.g.::\n\n            conf = my_config.lookup('myhost')\n            assert conf['passwordauthentication'] == 'yes'\n            assert conf.as_bool('passwordauthentication') is True\n\n        :param str hostname: the hostname to lookup\n\n        .. versionchanged:: 2.5\n            Returns `SSHConfigDict` objects instead of dict literals.\n        \"\"\"\n        matches = [\n            config\n            for config in self._config\n            if self._allowed(config[\"host\"], hostname)\n        ]\n\n        ret = SSHConfigDict()\n        for match in matches:\n            for key, value in match[\"config\"].items():\n                if key not in ret:\n                    # Create a copy of the original value,\n                    # else it will reference the original list\n                    # in self._config and update that value too\n                    # when the extend() is being called.\n                    ret[key] = value[:] if value is not None else value\n                elif key == \"identityfile\":\n                    ret[key].extend(value)\n        ret = self._expand_variables(ret, hostname)\n        # TODO: remove in 3.x re #670\n        if \"proxycommand\" in ret and ret[\"proxycommand\"] is None:\n            del ret[\"proxycommand\"]\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the set of literal hostnames defined in the SSH config.", "response": "def get_hostnames(self):\n        \"\"\"\n        Return the set of literal hostnames defined in the SSH config (both\n        explicit hostnames and wildcard entries).\n        \"\"\"\n        hosts = set()\n        for entry in self._config:\n            hosts.update(entry[\"host\"])\n        return hosts"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _expand_variables(self, config, hostname):\n\n        if \"hostname\" in config:\n            config[\"hostname\"] = config[\"hostname\"].replace(\"%h\", hostname)\n        else:\n            config[\"hostname\"] = hostname\n\n        if \"port\" in config:\n            port = config[\"port\"]\n        else:\n            port = SSH_PORT\n\n        user = os.getenv(\"USER\")\n        if \"user\" in config:\n            remoteuser = config[\"user\"]\n        else:\n            remoteuser = user\n\n        host = socket.gethostname().split(\".\")[0]\n        fqdn = LazyFqdn(config, host)\n        homedir = os.path.expanduser(\"~\")\n        replacements = {\n            \"controlpath\": [\n                (\"%h\", config[\"hostname\"]),\n                (\"%l\", fqdn),\n                (\"%L\", host),\n                (\"%n\", hostname),\n                (\"%p\", port),\n                (\"%r\", remoteuser),\n                (\"%u\", user),\n            ],\n            \"identityfile\": [\n                (\"~\", homedir),\n                (\"%d\", homedir),\n                (\"%h\", config[\"hostname\"]),\n                (\"%l\", fqdn),\n                (\"%u\", user),\n                (\"%r\", remoteuser),\n            ],\n            \"proxycommand\": [\n                (\"~\", homedir),\n                (\"%h\", config[\"hostname\"]),\n                (\"%p\", port),\n                (\"%r\", remoteuser),\n            ],\n        }\n\n        for k in config:\n            if config[k] is None:\n                continue\n            if k in replacements:\n                for find, replace in replacements[k]:\n                    if isinstance(config[k], list):\n                        for item in range(len(config[k])):\n                            if find in config[k][item]:\n                                config[k][item] = config[k][item].replace(\n                                    find, str(replace)\n                                )\n                    else:\n                        if find in config[k]:\n                            config[k] = config[k].replace(find, str(replace))\n        return config", "response": "Return a dict of config options with expanded substitutions for a given hostname."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a list of host_names from host value.", "response": "def _get_hosts(self, host):\n        \"\"\"\n        Return a list of host_names from host value.\n        \"\"\"\n        try:\n            return shlex.split(host)\n        except ValueError:\n            raise Exception(\"Unparsable host {}\".format(host))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the value of the given key as a boolean.", "response": "def as_bool(self, key):\n        \"\"\"\n        Express given key's value as a boolean type.\n\n        Typically, this is used for ``ssh_config``'s pseudo-boolean values\n        which are either ``\"yes\"`` or ``\"no\"``. In such cases, ``\"yes\"`` yields\n        ``True`` and any other value becomes ``False``.\n\n        .. note::\n            If (for whatever reason) the stored value is already boolean in\n            nature, it's simply returned.\n\n        .. versionadded:: 2.5\n        \"\"\"\n        val = self[key]\n        if isinstance(val, bool):\n            return val\n        return val.lower() == \"yes\""}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef check_auth_gssapi_with_mic(\n        self, username, gss_authenticated=AUTH_FAILED, cc_file=None\n    ):\n        \"\"\"\n        Authenticate the given user to the server if he is a valid krb5\n        principal.\n\n        :param str username: The username of the authenticating client\n        :param int gss_authenticated: The result of the krb5 authentication\n        :param str cc_filename: The krb5 client credentials cache filename\n        :return: ``AUTH_FAILED`` if the user is not authenticated otherwise\n                 ``AUTH_SUCCESSFUL``\n        :rtype: int\n        :note: Kerberos credential delegation is not supported.\n        :see: `.ssh_gss`\n        :note: : We are just checking in L{AuthHandler} that the given user is\n                 a valid krb5 principal!\n                 We don't check if the krb5 principal is allowed to log in on\n                 the server, because there is no way to do that in python. So\n                 if you develop your own SSH server with paramiko for a cetain\n                 plattform like Linux, you should call C{krb5_kuserok()} in\n                 your local kerberos library to make sure that the\n                 krb5_principal has an account on the server and is allowed to\n                 log in as a user.\n        :see: http://www.unix.com/man-page/all/3/krb5_kuserok/\n        \"\"\"\n        if gss_authenticated == AUTH_SUCCESSFUL:\n            return AUTH_SUCCESSFUL\n        return AUTH_FAILED", "response": "Checks if the given user is authenticated with the kerberos API."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check_auth_gssapi_keyex(\n        self, username, gss_authenticated=AUTH_FAILED, cc_file=None\n    ):\n        \"\"\"\n        Authenticate the given user to the server if he is a valid krb5\n        principal and GSS-API Key Exchange was performed.\n        If GSS-API Key Exchange was not performed, this authentication method\n        won't be available.\n\n        :param str username: The username of the authenticating client\n        :param int gss_authenticated: The result of the krb5 authentication\n        :param str cc_filename: The krb5 client credentials cache filename\n        :return: ``AUTH_FAILED`` if the user is not authenticated otherwise\n                 ``AUTH_SUCCESSFUL``\n        :rtype: int\n        :note: Kerberos credential delegation is not supported.\n        :see: `.ssh_gss` `.kex_gss`\n        :note: : We are just checking in L{AuthHandler} that the given user is\n                 a valid krb5 principal!\n                 We don't check if the krb5 principal is allowed to log in on\n                 the server, because there is no way to do that in python. So\n                 if you develop your own SSH server with paramiko for a cetain\n                 plattform like Linux, you should call C{krb5_kuserok()} in\n                 your local kerberos library to make sure that the\n                 krb5_principal has an account on the server and is allowed\n                 to log in as a user.\n        :see: http://www.unix.com/man-page/all/3/krb5_kuserok/\n        \"\"\"\n        if gss_authenticated == AUTH_SUCCESSFUL:\n            return AUTH_SUCCESSFUL\n        return AUTH_FAILED", "response": "Checks if the given user is a valid kerberos - API key exchange and returns AUTH_SUCCESSFUL if the user is not authenticated otherwise AUTH_FAILED."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking if a requested subsystem request should be routed to the client on the given channel.", "response": "def check_channel_subsystem_request(self, channel, name):\n        \"\"\"\n        Determine if a requested subsystem will be provided to the client on\n        the given channel.  If this method returns ``True``, all future I/O\n        through this channel will be assumed to be connected to the requested\n        subsystem.  An example of a subsystem is ``sftp``.\n\n        The default implementation checks for a subsystem handler assigned via\n        `.Transport.set_subsystem_handler`.\n        If one has been set, the handler is invoked and this method returns\n        ``True``.  Otherwise it returns ``False``.\n\n        .. note:: Because the default implementation uses the `.Transport` to\n            identify valid subsystems, you probably won't need to override this\n            method.\n\n        :param .Channel channel: the `.Channel` the pty request arrived on.\n        :param str name: name of the requested subsystem.\n        :return:\n            ``True`` if this channel is now hooked up to the requested\n            subsystem; ``False`` if that subsystem can't or won't be provided.\n        \"\"\"\n        transport = channel.get_transport()\n        handler_class, larg, kwarg = transport._get_subsystem_handler(name)\n        if handler_class is None:\n            return False\n        handler = handler_class(channel, name, self, *larg, **kwarg)\n        handler.start()\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_prompt(self, prompt, echo=True):\n        self.prompts.append((prompt, echo))", "response": "Add a prompt to this query."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef GSSAuth(auth_method, gss_deleg_creds=True):\n    if _API == \"MIT\":\n        return _SSH_GSSAPI(auth_method, gss_deleg_creds)\n    elif _API == \"SSPI\" and os.name == \"nt\":\n        return _SSH_SSPI(auth_method, gss_deleg_creds)\n    else:\n        raise ImportError(\"Unable to import a GSS-API / SSPI module!\")", "response": "Returns an SSH2 GSS - API or SSPI authentication object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck if the given OID is the Kerberos V5 OID.", "response": "def ssh_check_mech(self, desired_mech):\n        \"\"\"\n        Check if the given OID is the Kerberos V5 OID (server mode).\n\n        :param str desired_mech: The desired GSS-API mechanism of the client\n        :return: ``True`` if the given OID is supported, otherwise C{False}\n        \"\"\"\n        from pyasn1.codec.der import decoder\n\n        mech, __ = decoder.decode(desired_mech)\n        if mech.__str__() != self._krb5_mech:\n            return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _ssh_build_mic(self, session_id, username, service, auth_method):\n        mic = self._make_uint32(len(session_id))\n        mic += session_id\n        mic += struct.pack(\"B\", MSG_USERAUTH_REQUEST)\n        mic += self._make_uint32(len(username))\n        mic += username.encode()\n        mic += self._make_uint32(len(service))\n        mic += service.encode()\n        mic += self._make_uint32(len(auth_method))\n        mic += auth_method.encode()\n        return mic", "response": "Builds the SSH2 MIC filed for gssapi - with - mic."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninitializes a GSS - API context.", "response": "def ssh_init_sec_context(\n        self, target, desired_mech=None, username=None, recv_token=None\n    ):\n        \"\"\"\n        Initialize a GSS-API context.\n\n        :param str username: The name of the user who attempts to login\n        :param str target: The hostname of the target to connect to\n        :param str desired_mech: The negotiated GSS-API mechanism\n                                 (\"pseudo negotiated\" mechanism, because we\n                                 support just the krb5 mechanism :-))\n        :param str recv_token: The GSS-API token received from the Server\n        :raises:\n            `.SSHException` -- Is raised if the desired mechanism of the client\n            is not supported\n        :return: A ``String`` if the GSS-API has returned a token or\n            ``None`` if no token was returned\n        \"\"\"\n        from pyasn1.codec.der import decoder\n\n        self._username = username\n        self._gss_host = target\n        targ_name = gssapi.Name(\n            \"host@\" + self._gss_host, gssapi.C_NT_HOSTBASED_SERVICE\n        )\n        ctx = gssapi.Context()\n        ctx.flags = self._gss_flags\n        if desired_mech is None:\n            krb5_mech = gssapi.OID.mech_from_string(self._krb5_mech)\n        else:\n            mech, __ = decoder.decode(desired_mech)\n            if mech.__str__() != self._krb5_mech:\n                raise SSHException(\"Unsupported mechanism OID.\")\n            else:\n                krb5_mech = gssapi.OID.mech_from_string(self._krb5_mech)\n        token = None\n        try:\n            if recv_token is None:\n                self._gss_ctxt = gssapi.InitContext(\n                    peer_name=targ_name,\n                    mech_type=krb5_mech,\n                    req_flags=ctx.flags,\n                )\n                token = self._gss_ctxt.step(token)\n            else:\n                token = self._gss_ctxt.step(recv_token)\n        except gssapi.GSSException:\n            message = \"{} Target: {}\".format(sys.exc_info()[1], self._gss_host)\n            raise gssapi.GSSException(message)\n        self._gss_ctxt_status = self._gss_ctxt.established\n        return token"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\naccept a GSS - API context.", "response": "def ssh_accept_sec_context(self, hostname, recv_token, username=None):\n        \"\"\"\n        Accept a GSS-API context (server mode).\n\n        :param str hostname: The servers hostname\n        :param str username: The name of the user who attempts to login\n        :param str recv_token: The GSS-API Token received from the server,\n                               if it's not the initial call.\n        :return: A ``String`` if the GSS-API has returned a token or ``None``\n                if no token was returned\n        \"\"\"\n        # hostname and username are not required for GSSAPI, but for SSPI\n        self._gss_host = hostname\n        self._username = username\n        if self._gss_srv_ctxt is None:\n            self._gss_srv_ctxt = gssapi.AcceptContext()\n        token = self._gss_srv_ctxt.step(recv_token)\n        self._gss_srv_ctxt_status = self._gss_srv_ctxt.established\n        return token"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninitialize the SSPI context.", "response": "def ssh_init_sec_context(\n        self, target, desired_mech=None, username=None, recv_token=None\n    ):\n        \"\"\"\n        Initialize a SSPI context.\n\n        :param str username: The name of the user who attempts to login\n        :param str target: The FQDN of the target to connect to\n        :param str desired_mech: The negotiated SSPI mechanism\n                                 (\"pseudo negotiated\" mechanism, because we\n                                 support just the krb5 mechanism :-))\n        :param recv_token: The SSPI token received from the Server\n        :raises:\n            `.SSHException` -- Is raised if the desired mechanism of the client\n            is not supported\n        :return: A ``String`` if the SSPI has returned a token or ``None`` if\n                 no token was returned\n        \"\"\"\n        from pyasn1.codec.der import decoder\n\n        self._username = username\n        self._gss_host = target\n        error = 0\n        targ_name = \"host/\" + self._gss_host\n        if desired_mech is not None:\n            mech, __ = decoder.decode(desired_mech)\n            if mech.__str__() != self._krb5_mech:\n                raise SSHException(\"Unsupported mechanism OID.\")\n        try:\n            if recv_token is None:\n                self._gss_ctxt = sspi.ClientAuth(\n                    \"Kerberos\", scflags=self._gss_flags, targetspn=targ_name\n                )\n            error, token = self._gss_ctxt.authorize(recv_token)\n            token = token[0].Buffer\n        except pywintypes.error as e:\n            e.strerror += \", Target: {}\".format(e, self._gss_host)\n            raise\n\n        if error == 0:\n            \"\"\"\n            if the status is GSS_COMPLETE (error = 0) the context is fully\n            established an we can set _gss_ctxt_status to True.\n            \"\"\"\n            self._gss_ctxt_status = True\n            token = None\n            \"\"\"\n            You won't get another token if the context is fully established,\n            so i set token to None instead of \"\"\n            \"\"\"\n        return token"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating the MIC token for a SSH2 message.", "response": "def ssh_get_mic(self, session_id, gss_kex=False):\n        \"\"\"\n        Create the MIC token for a SSH2 message.\n\n        :param str session_id: The SSH session ID\n        :param bool gss_kex: Generate the MIC for Key Exchange with SSPI or not\n        :return: gssapi-with-mic:\n                 Returns the MIC token from SSPI for the message we created\n                 with ``_ssh_build_mic``.\n                 gssapi-keyex:\n                 Returns the MIC token from SSPI with the SSH session ID as\n                 message.\n        \"\"\"\n        self._session_id = session_id\n        if not gss_kex:\n            mic_field = self._ssh_build_mic(\n                self._session_id,\n                self._username,\n                self._service,\n                self._auth_method,\n            )\n            mic_token = self._gss_ctxt.sign(mic_field)\n        else:\n            # for key exchange with gssapi-keyex\n            mic_token = self._gss_srv_ctxt.sign(self._session_id)\n        return mic_token"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\naccepts a SSPI context.", "response": "def ssh_accept_sec_context(self, hostname, username, recv_token):\n        \"\"\"\n        Accept a SSPI context (server mode).\n\n        :param str hostname: The servers FQDN\n        :param str username: The name of the user who attempts to login\n        :param str recv_token: The SSPI Token received from the server,\n                               if it's not the initial call.\n        :return: A ``String`` if the SSPI has returned a token or ``None`` if\n                 no token was returned\n        \"\"\"\n        self._gss_host = hostname\n        self._username = username\n        targ_name = \"host/\" + self._gss_host\n        self._gss_srv_ctxt = sspi.ServerAuth(\"Kerberos\", spn=targ_name)\n        error, token = self._gss_srv_ctxt.authorize(recv_token)\n        token = token[0].Buffer\n        if error == 0:\n            self._gss_srv_ctxt_status = True\n            token = None\n        return token"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nverifying the MIC token for a SSH2 message.", "response": "def ssh_check_mic(self, mic_token, session_id, username=None):\n        \"\"\"\n        Verify the MIC token for a SSH2 message.\n\n        :param str mic_token: The MIC token received from the client\n        :param str session_id: The SSH session ID\n        :param str username: The name of the user who attempts to login\n        :return: None if the MIC check was successful\n        :raises: ``sspi.error`` -- if the MIC check failed\n        \"\"\"\n        self._session_id = session_id\n        self._username = username\n        if username is not None:\n            # server mode\n            mic_field = self._ssh_build_mic(\n                self._session_id,\n                self._username,\n                self._service,\n                self._auth_method,\n            )\n            # Verifies data and its signature.  If verification fails, an\n            # sspi.error will be raised.\n            self._gss_srv_ctxt.verify(mic_field, mic_token)\n        else:\n            # for key exchange with gssapi-keyex\n            # client mode\n            # Verifies data and its signature.  If verification fails, an\n            # sspi.error will be raised.\n            self._gss_ctxt.verify(self._session_id, mic_token)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef credentials_delegated(self):\n        return self._gss_flags & sspicon.ISC_REQ_DELEGATE and (\n            self._gss_srv_ctxt_status or self._gss_flags\n        )", "response": "Checks if credentials are delegated."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates an SFTPClient object from an open. Transport object.", "response": "def from_transport(cls, t, window_size=None, max_packet_size=None):\n        \"\"\"\n        Create an SFTP client channel from an open `.Transport`.\n\n        Setting the window and packet sizes might affect the transfer speed.\n        The default settings in the `.Transport` class are the same as in\n        OpenSSH and should work adequately for both files transfers and\n        interactive sessions.\n\n        :param .Transport t: an open `.Transport` which is already\n            authenticated\n        :param int window_size:\n            optional window size for the `.SFTPClient` session.\n        :param int max_packet_size:\n            optional max packet size for the `.SFTPClient` session..\n\n        :return:\n            a new `.SFTPClient` object, referring to an sftp session (channel)\n            across the transport\n\n        .. versionchanged:: 1.15\n            Added the ``window_size`` and ``max_packet_size`` arguments.\n        \"\"\"\n        chan = t.open_session(\n            window_size=window_size, max_packet_size=max_packet_size\n        )\n        if chan is None:\n            return None\n        chan.invoke_subsystem(\"sftp\")\n        return cls(chan)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef listdir_iter(self, path=\".\", read_aheads=50):\n        path = self._adjust_cwd(path)\n        self._log(DEBUG, \"listdir({!r})\".format(path))\n        t, msg = self._request(CMD_OPENDIR, path)\n\n        if t != CMD_HANDLE:\n            raise SFTPError(\"Expected handle\")\n\n        handle = msg.get_string()\n\n        nums = list()\n        while True:\n            try:\n                # Send out a bunch of readdir requests so that we can read the\n                # responses later on Section 6.7 of the SSH file transfer RFC\n                # explains this\n                # http://filezilla-project.org/specs/draft-ietf-secsh-filexfer-02.txt\n                for i in range(read_aheads):\n                    num = self._async_request(type(None), CMD_READDIR, handle)\n                    nums.append(num)\n\n                # For each of our sent requests\n                # Read and parse the corresponding packets\n                # If we're at the end of our queued requests, then fire off\n                # some more requests\n                # Exit the loop when we've reached the end of the directory\n                # handle\n                for num in nums:\n                    t, pkt_data = self._read_packet()\n                    msg = Message(pkt_data)\n                    new_num = msg.get_int()\n                    if num == new_num:\n                        if t == CMD_STATUS:\n                            self._convert_status(msg)\n                    count = msg.get_int()\n                    for i in range(count):\n                        filename = msg.get_text()\n                        longname = msg.get_text()\n                        attr = SFTPAttributes._from_msg(\n                            msg, filename, longname\n                        )\n                        if (filename != \".\") and (filename != \"..\"):\n                            yield attr\n\n                # If we've hit the end of our queued requests, reset nums.\n                nums = list()\n\n            except EOFError:\n                self._request(CMD_CLOSE, handle)\n                return", "response": "Return an iterator over the contents of a directory."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef rename(self, oldpath, newpath):\n        oldpath = self._adjust_cwd(oldpath)\n        newpath = self._adjust_cwd(newpath)\n        self._log(DEBUG, \"rename({!r}, {!r})\".format(oldpath, newpath))\n        self._request(CMD_RENAME, oldpath, newpath)", "response": "Rename a file or folder from oldpath to newpath."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrename a file or folder from oldpath to newpath.", "response": "def posix_rename(self, oldpath, newpath):\n        \"\"\"\n        Rename a file or folder from ``oldpath`` to ``newpath``, following\n        posix conventions.\n\n        :param str oldpath: existing name of the file or folder\n        :param str newpath: new name for the file or folder, will be\n            overwritten if it already exists\n\n        :raises:\n            ``IOError`` -- if ``newpath`` is a folder, posix-rename is not\n            supported by the server or something else goes wrong\n\n        :versionadded: 2.2\n        \"\"\"\n        oldpath = self._adjust_cwd(oldpath)\n        newpath = self._adjust_cwd(newpath)\n        self._log(DEBUG, \"posix_rename({!r}, {!r})\".format(oldpath, newpath))\n        self._request(\n            CMD_EXTENDED, \"posix-rename@openssh.com\", oldpath, newpath\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef mkdir(self, path, mode=o777):\n        path = self._adjust_cwd(path)\n        self._log(DEBUG, \"mkdir({!r}, {!r})\".format(path, mode))\n        attr = SFTPAttributes()\n        attr.st_mode = mode\n        self._request(CMD_MKDIR, path, attr)", "response": "Create a folder named path with numeric mode mode."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef symlink(self, source, dest):\n        dest = self._adjust_cwd(dest)\n        self._log(DEBUG, \"symlink({!r}, {!r})\".format(source, dest))\n        source = b(source)\n        self._request(CMD_SYMLINK, source, dest)", "response": "Create a symbolic link to the source path at the destination."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchanging the owner and group of a file.", "response": "def chown(self, path, uid, gid):\n        \"\"\"\n        Change the owner (``uid``) and group (``gid``) of a file.  As with\n        Python's `os.chown` function, you must pass both arguments, so if you\n        only want to change one, use `stat` first to retrieve the current\n        owner and group.\n\n        :param str path: path of the file to change the owner and group of\n        :param int uid: new owner's uid\n        :param int gid: new group id\n        \"\"\"\n        path = self._adjust_cwd(path)\n        self._log(DEBUG, \"chown({!r}, {!r}, {!r})\".format(path, uid, gid))\n        attr = SFTPAttributes()\n        attr.st_uid, attr.st_gid = uid, gid\n        self._request(CMD_SETSTAT, path, attr)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the access and modified times of the file specified by path.", "response": "def utime(self, path, times):\n        \"\"\"\n        Set the access and modified times of the file specified by ``path``.\n        If ``times`` is ``None``, then the file's access and modified times\n        are set to the current time.  Otherwise, ``times`` must be a 2-tuple\n        of numbers, of the form ``(atime, mtime)``, which is used to set the\n        access and modified times, respectively.  This bizarre API is mimicked\n        from Python for the sake of consistency -- I apologize.\n\n        :param str path: path of the file to modify\n        :param tuple times:\n            ``None`` or a tuple of (access time, modified time) in standard\n            internet epoch time (seconds since 01 January 1970 GMT)\n        \"\"\"\n        path = self._adjust_cwd(path)\n        if times is None:\n            times = (time.time(), time.time())\n        self._log(DEBUG, \"utime({!r}, {!r})\".format(path, times))\n        attr = SFTPAttributes()\n        attr.st_atime, attr.st_mtime = times\n        self._request(CMD_SETSTAT, path, attr)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nchange the size of the file specified by path.", "response": "def truncate(self, path, size):\n        \"\"\"\n        Change the size of the file specified by ``path``.  This usually\n        extends or shrinks the size of the file, just like the `~file.truncate`\n        method on Python file objects.\n\n        :param str path: path of the file to modify\n        :param int size: the new size of the file\n        \"\"\"\n        path = self._adjust_cwd(path)\n        self._log(DEBUG, \"truncate({!r}, {!r})\".format(path, size))\n        attr = SFTPAttributes()\n        attr.st_size = size\n        self._request(CMD_SETSTAT, path, attr)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncopy the contents of an opened file object to the server and return the attributes about the given file.", "response": "def putfo(self, fl, remotepath, file_size=0, callback=None, confirm=True):\n        \"\"\"\n        Copy the contents of an open file object (``fl``) to the SFTP server as\n        ``remotepath``. Any exception raised by operations will be passed\n        through.\n\n        The SFTP operations use pipelining for speed.\n\n        :param fl: opened file or file-like object to copy\n        :param str remotepath: the destination path on the SFTP server\n        :param int file_size:\n            optional size parameter passed to callback. If none is specified,\n            size defaults to 0\n        :param callable callback:\n            optional callback function (form: ``func(int, int)``) that accepts\n            the bytes transferred so far and the total bytes to be transferred\n            (since 1.7.4)\n        :param bool confirm:\n            whether to do a stat() on the file afterwards to confirm the file\n            size (since 1.7.7)\n\n        :return:\n            an `.SFTPAttributes` object containing attributes about the given\n            file.\n\n        .. versionadded:: 1.10\n        \"\"\"\n        with self.file(remotepath, \"wb\") as fr:\n            fr.set_pipelined(True)\n            size = self._transfer_with_callback(\n                reader=fl, writer=fr, file_size=file_size, callback=callback\n            )\n        if confirm:\n            s = self.stat(remotepath)\n            if s.st_size != size:\n                raise IOError(\n                    \"size mismatch in put!  {} != {}\".format(s.st_size, size)\n                )\n        else:\n            s = SFTPAttributes()\n        return s"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef put(self, localpath, remotepath, callback=None, confirm=True):\n        file_size = os.stat(localpath).st_size\n        with open(localpath, \"rb\") as fl:\n            return self.putfo(fl, remotepath, file_size, callback, confirm)", "response": "Copy a local file to the server as remotepath."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncopying a remote file from the SFTP server and write to the local host or open file object fl.", "response": "def getfo(self, remotepath, fl, callback=None):\n        \"\"\"\n        Copy a remote file (``remotepath``) from the SFTP server and write to\n        an open file or file-like object, ``fl``.  Any exception raised by\n        operations will be passed through.  This method is primarily provided\n        as a convenience.\n\n        :param object remotepath: opened file or file-like object to copy to\n        :param str fl:\n            the destination path on the local host or open file object\n        :param callable callback:\n            optional callback function (form: ``func(int, int)``) that accepts\n            the bytes transferred so far and the total bytes to be transferred\n        :return: the `number <int>` of bytes written to the opened file object\n\n        .. versionadded:: 1.10\n        \"\"\"\n        file_size = self.stat(remotepath).st_size\n        with self.open(remotepath, \"rb\") as fr:\n            fr.prefetch(file_size)\n            return self._transfer_with_callback(\n                reader=fr, writer=fl, file_size=file_size, callback=callback\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get(self, remotepath, localpath, callback=None):\n        with open(localpath, \"wb\") as fl:\n            size = self.getfo(remotepath, fl, callback)\n        s = os.stat(localpath)\n        if s.st_size != size:\n            raise IOError(\n                \"size mismatch in get!  {} != {}\".format(s.st_size, size)\n            )", "response": "Copy a remote file from the SFTP server to the local file system."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting status code to errno and raise IOError if error code is not OK.", "response": "def _convert_status(self, msg):\n        \"\"\"\n        Raises EOFError or IOError on error status; otherwise does nothing.\n        \"\"\"\n        code = msg.get_int()\n        text = msg.get_text()\n        if code == SFTP_OK:\n            return\n        elif code == SFTP_EOF:\n            raise EOFError(text)\n        elif code == SFTP_NO_SUCH_FILE:\n            # clever idea from john a. meinel: map the error codes to errno\n            raise IOError(errno.ENOENT, text)\n        elif code == SFTP_PERMISSION_DENIED:\n            raise IOError(errno.EACCES, text)\n        else:\n            raise IOError(text)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadjusting the cwd to match the path that we re emulating a current working directory.", "response": "def _adjust_cwd(self, path):\n        \"\"\"\n        Return an adjusted path if we're emulating a \"current working\n        directory\" for the server.\n        \"\"\"\n        path = b(path)\n        if self._cwd is None:\n            return path\n        if len(path) and path[0:1] == b_slash:\n            # absolute path\n            return path\n        if self._cwd == b_slash:\n            return self._cwd + path\n        return self._cwd + b_slash + path"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef coverage(ctx, opts=\"\"):\n    return test(ctx, coverage=True, include_slow=True, opts=opts)", "response": "Execute all tests with coverage enabled."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nexecuting all tests and then watch for changes.", "response": "def guard(ctx, opts=\"\"):\n    \"\"\"\n    Execute all tests and then watch for changes, re-running.\n    \"\"\"\n    # TODO if coverage was run via pytest-cov, we could add coverage here too\n    return test(ctx, include_slow=True, loop_on_fail=True, opts=opts)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a new release to the current version of the current version.", "response": "def release(ctx, sdist=True, wheel=True, sign=True, dry_run=False, index=None):\n    \"\"\"\n    Wraps invocations.packaging.publish to add baked-in docs folder.\n    \"\"\"\n    # Build docs first. Use terribad workaround pending invoke #146\n    ctx.run(\"inv docs\", pty=True, hide=False)\n    # Move the built docs into where Epydocs used to live\n    target = \"docs\"\n    rmtree(target, ignore_errors=True)\n    # TODO: make it easier to yank out this config val from the docs coll\n    copytree(\"sites/docs/_build\", target)\n    # Publish\n    publish(\n        ctx, sdist=sdist, wheel=wheel, sign=sign, dry_run=dry_run, index=index\n    )\n    # Remind\n    print(\n        \"\\n\\nDon't forget to update RTD's versions page for new minor \"\n        \"releases!\"\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_gss_host(self, gss_host, trust_dns=True, gssapi_requested=True):\n        # No GSSAPI in play == nothing to do\n        if not gssapi_requested:\n            return\n        # Obtain the correct host first - did user request a GSS-specific name\n        # to use that is distinct from the actual SSH target hostname?\n        if gss_host is None:\n            gss_host = self.hostname\n        # Finally, canonicalize via DNS if DNS is trusted.\n        if trust_dns and gss_host is not None:\n            gss_host = socket.getfqdn(gss_host)\n        # And set attribute for reference later.\n        self.gss_host = gss_host", "response": "Sets the self. gss_host attribute of the object based on the values in the GSS - specific hostname field."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef start_client(self, event=None, timeout=None):\n        self.active = True\n        if event is not None:\n            # async, return immediately and let the app poll for completion\n            self.completion_event = event\n            self.start()\n            return\n\n        # synchronous, wait for a result\n        self.completion_event = event = threading.Event()\n        self.start()\n        max_time = time.time() + timeout if timeout is not None else None\n        while True:\n            event.wait(0.1)\n            if not self.active:\n                e = self.get_exception()\n                if e is not None:\n                    raise e\n                raise SSHException(\"Negotiation failed.\")\n            if event.is_set() or (\n                timeout is not None and time.time() >= max_time\n            ):\n                break", "response": "Starts a new SSH2 client."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads a file of prime moduli for use in server mode.", "response": "def load_server_moduli(filename=None):\n        \"\"\"\n        (optional)\n        Load a file of prime moduli for use in doing group-exchange key\n        negotiation in server mode.  It's a rather obscure option and can be\n        safely ignored.\n\n        In server mode, the remote client may request \"group-exchange\" key\n        negotiation, which asks the server to send a random prime number that\n        fits certain criteria.  These primes are pretty difficult to compute,\n        so they can't be generated on demand.  But many systems contain a file\n        of suitable primes (usually named something like ``/etc/ssh/moduli``).\n        If you call `load_server_moduli` and it returns ``True``, then this\n        file of primes has been loaded and we will support \"group-exchange\" in\n        server mode.  Otherwise server mode will just claim that it doesn't\n        support that method of key negotiation.\n\n        :param str filename:\n            optional path to the moduli file, if you happen to know that it's\n            not in a standard location.\n        :return:\n            True if a moduli file was successfully loaded; False otherwise.\n\n        .. note:: This has no effect when used in client mode.\n        \"\"\"\n        Transport._modulus_pack = ModulusPack()\n        # places to look for the openssh \"moduli\" file\n        file_list = [\"/etc/ssh/moduli\", \"/usr/local/etc/moduli\"]\n        if filename is not None:\n            file_list.insert(0, filename)\n        for fn in file_list:\n            try:\n                Transport._modulus_pack.read_file(fn)\n                return True\n            except IOError:\n                pass\n        # none succeeded\n        Transport._modulus_pack = None\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef close(self):\n        if not self.active:\n            return\n        self.stop_thread()\n        for chan in list(self._channels.values()):\n            chan._unlink()\n        self.sock.close()", "response": "Close this session and any open channels that are tied to it."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef open_session(\n        self, window_size=None, max_packet_size=None, timeout=None\n    ):\n        \"\"\"\n        Request a new channel to the server, of type ``\"session\"``.  This is\n        just an alias for calling `open_channel` with an argument of\n        ``\"session\"``.\n\n        .. note:: Modifying the the window and packet sizes might have adverse\n            effects on the session created. The default values are the same\n            as in the OpenSSH code base and have been battle tested.\n\n        :param int window_size:\n            optional window size for this session.\n        :param int max_packet_size:\n            optional max packet size for this session.\n\n        :return: a new `.Channel`\n\n        :raises:\n            `.SSHException` -- if the request is rejected or the session ends\n            prematurely\n\n        .. versionchanged:: 1.13.4/1.14.3/1.15.3\n            Added the ``timeout`` argument.\n        .. versionchanged:: 1.15\n            Added the ``window_size`` and ``max_packet_size`` arguments.\n        \"\"\"\n        return self.open_channel(\n            \"session\",\n            window_size=window_size,\n            max_packet_size=max_packet_size,\n            timeout=timeout,\n        )", "response": "This method opens a new session to the server."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef open_channel(\n        self,\n        kind,\n        dest_addr=None,\n        src_addr=None,\n        window_size=None,\n        max_packet_size=None,\n        timeout=None,\n    ):\n        \"\"\"\n        Request a new channel to the server. `Channels <.Channel>` are\n        socket-like objects used for the actual transfer of data across the\n        session. You may only request a channel after negotiating encryption\n        (using `connect` or `start_client`) and authenticating.\n\n        .. note:: Modifying the the window and packet sizes might have adverse\n            effects on the channel created. The default values are the same\n            as in the OpenSSH code base and have been battle tested.\n\n        :param str kind:\n            the kind of channel requested (usually ``\"session\"``,\n            ``\"forwarded-tcpip\"``, ``\"direct-tcpip\"``, or ``\"x11\"``)\n        :param tuple dest_addr:\n            the destination address (address + port tuple) of this port\n            forwarding, if ``kind`` is ``\"forwarded-tcpip\"`` or\n            ``\"direct-tcpip\"`` (ignored for other channel types)\n        :param src_addr: the source address of this port forwarding, if\n            ``kind`` is ``\"forwarded-tcpip\"``, ``\"direct-tcpip\"``, or ``\"x11\"``\n        :param int window_size:\n            optional window size for this session.\n        :param int max_packet_size:\n            optional max packet size for this session.\n        :param float timeout:\n            optional timeout opening a channel, default 3600s (1h)\n\n        :return: a new `.Channel` on success\n\n        :raises:\n            `.SSHException` -- if the request is rejected, the session ends\n            prematurely or there is a timeout openning a channel\n\n        .. versionchanged:: 1.15\n            Added the ``window_size`` and ``max_packet_size`` arguments.\n        \"\"\"\n        if not self.active:\n            raise SSHException(\"SSH session not active\")\n        timeout = 3600 if timeout is None else timeout\n        self.lock.acquire()\n        try:\n            window_size = self._sanitize_window_size(window_size)\n            max_packet_size = self._sanitize_packet_size(max_packet_size)\n            chanid = self._next_channel()\n            m = Message()\n            m.add_byte(cMSG_CHANNEL_OPEN)\n            m.add_string(kind)\n            m.add_int(chanid)\n            m.add_int(window_size)\n            m.add_int(max_packet_size)\n            if (kind == \"forwarded-tcpip\") or (kind == \"direct-tcpip\"):\n                m.add_string(dest_addr[0])\n                m.add_int(dest_addr[1])\n                m.add_string(src_addr[0])\n                m.add_int(src_addr[1])\n            elif kind == \"x11\":\n                m.add_string(src_addr[0])\n                m.add_int(src_addr[1])\n            chan = Channel(chanid)\n            self._channels.put(chanid, chan)\n            self.channel_events[chanid] = event = threading.Event()\n            self.channels_seen[chanid] = True\n            chan._set_transport(self)\n            chan._set_window(window_size, max_packet_size)\n        finally:\n            self.lock.release()\n        self._send_user_message(m)\n        start_ts = time.time()\n        while True:\n            event.wait(0.1)\n            if not self.active:\n                e = self.get_exception()\n                if e is None:\n                    e = SSHException(\"Unable to open channel.\")\n                raise e\n            if event.is_set():\n                break\n            elif start_ts + timeout < time.time():\n                raise SSHException(\"Timeout opening channel.\")\n        chan = self._channels.get(chanid)\n        if chan is not None:\n            return chan\n        e = self.get_exception()\n        if e is None:\n            e = SSHException(\"Unable to open channel.\")\n        raise e", "response": "Open a new channel to the server."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nrequests a TCP forwarding connection from a listening port on the server.", "response": "def request_port_forward(self, address, port, handler=None):\n        \"\"\"\n        Ask the server to forward TCP connections from a listening port on\n        the server, across this SSH session.\n\n        If a handler is given, that handler is called from a different thread\n        whenever a forwarded connection arrives.  The handler parameters are::\n\n            handler(\n                channel,\n                (origin_addr, origin_port),\n                (server_addr, server_port),\n            )\n\n        where ``server_addr`` and ``server_port`` are the address and port that\n        the server was listening on.\n\n        If no handler is set, the default behavior is to send new incoming\n        forwarded connections into the accept queue, to be picked up via\n        `accept`.\n\n        :param str address: the address to bind when forwarding\n        :param int port:\n            the port to forward, or 0 to ask the server to allocate any port\n        :param callable handler:\n            optional handler for incoming forwarded connections, of the form\n            ``func(Channel, (str, int), (str, int))``.\n\n        :return: the port number (`int`) allocated by the server\n\n        :raises:\n            `.SSHException` -- if the server refused the TCP forward request\n        \"\"\"\n        if not self.active:\n            raise SSHException(\"SSH session not active\")\n        port = int(port)\n        response = self.global_request(\n            \"tcpip-forward\", (address, port), wait=True\n        )\n        if response is None:\n            raise SSHException(\"TCP forwarding request denied\")\n        if port == 0:\n            port = response.get_int()\n        if handler is None:\n\n            def default_handler(channel, src_addr, dest_addr_port):\n                # src_addr, src_port = src_addr_port\n                # dest_addr, dest_port = dest_addr_port\n                self._queue_incoming_channel(channel)\n\n            handler = default_handler\n        self._tcp_handler = handler\n        return port"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cancel_port_forward(self, address, port):\n        if not self.active:\n            return\n        self._tcp_handler = None\n        self.global_request(\"cancel-tcpip-forward\", (address, port), wait=True)", "response": "Cancel a previous port forwarding request."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsending a junk packet across the encrypted link.", "response": "def send_ignore(self, byte_count=None):\n        \"\"\"\n        Send a junk packet across the encrypted link.  This is sometimes used\n        to add \"noise\" to a connection to confuse would-be attackers.  It can\n        also be used as a keep-alive for long lived connections traversing\n        firewalls.\n\n        :param int byte_count:\n            the number of random bytes to send in the payload of the ignored\n            packet -- defaults to a random number from 10 to 41.\n        \"\"\"\n        m = Message()\n        m.add_byte(cMSG_IGNORE)\n        if byte_count is None:\n            byte_count = (byte_ord(os.urandom(1)) % 32) + 10\n        m.add_bytes(os.urandom(byte_count))\n        self._send_user_message(m)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the next available channel in the server mode.", "response": "def accept(self, timeout=None):\n        \"\"\"\n        Return the next channel opened by the client over this transport, in\n        server mode.  If no channel is opened before the given timeout,\n        ``None`` is returned.\n\n        :param int timeout:\n            seconds to wait for a channel, or ``None`` to wait forever\n        :return: a new `.Channel` opened by the client\n        \"\"\"\n        self.lock.acquire()\n        try:\n            if len(self.server_accepts) > 0:\n                chan = self.server_accepts.pop(0)\n            else:\n                self.server_accept_cv.wait(timeout)\n                if len(self.server_accepts) > 0:\n                    chan = self.server_accepts.pop(0)\n                else:\n                    # timeout\n                    chan = None\n        finally:\n            self.lock.release()\n        return chan"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef connect(\n        self,\n        hostkey=None,\n        username=\"\",\n        password=None,\n        pkey=None,\n        gss_host=None,\n        gss_auth=False,\n        gss_kex=False,\n        gss_deleg_creds=True,\n        gss_trust_dns=True,\n    ):\n        \"\"\"\n        Negotiate an SSH2 session, and optionally verify the server's host key\n        and authenticate using a password or private key.  This is a shortcut\n        for `start_client`, `get_remote_server_key`, and\n        `Transport.auth_password` or `Transport.auth_publickey`.  Use those\n        methods if you want more control.\n\n        You can use this method immediately after creating a Transport to\n        negotiate encryption with a server.  If it fails, an exception will be\n        thrown.  On success, the method will return cleanly, and an encrypted\n        session exists.  You may immediately call `open_channel` or\n        `open_session` to get a `.Channel` object, which is used for data\n        transfer.\n\n        .. note::\n            If you fail to supply a password or private key, this method may\n            succeed, but a subsequent `open_channel` or `open_session` call may\n            fail because you haven't authenticated yet.\n\n        :param .PKey hostkey:\n            the host key expected from the server, or ``None`` if you don't\n            want to do host key verification.\n        :param str username: the username to authenticate as.\n        :param str password:\n            a password to use for authentication, if you want to use password\n            authentication; otherwise ``None``.\n        :param .PKey pkey:\n            a private key to use for authentication, if you want to use private\n            key authentication; otherwise ``None``.\n        :param str gss_host:\n            The target's name in the kerberos database. Default: hostname\n        :param bool gss_auth:\n            ``True`` if you want to use GSS-API authentication.\n        :param bool gss_kex:\n            Perform GSS-API Key Exchange and user authentication.\n        :param bool gss_deleg_creds:\n            Whether to delegate GSS-API client credentials.\n        :param gss_trust_dns:\n            Indicates whether or not the DNS is trusted to securely\n            canonicalize the name of the host being connected to (default\n            ``True``).\n\n        :raises: `.SSHException` -- if the SSH2 negotiation fails, the host key\n            supplied by the server is incorrect, or authentication fails.\n\n        .. versionchanged:: 2.3\n            Added the ``gss_trust_dns`` argument.\n        \"\"\"\n        if hostkey is not None:\n            self._preferred_keys = [hostkey.get_name()]\n\n        self.set_gss_host(\n            gss_host=gss_host,\n            trust_dns=gss_trust_dns,\n            gssapi_requested=gss_kex or gss_auth,\n        )\n\n        self.start_client()\n\n        # check host key if we were given one\n        # If GSS-API Key Exchange was performed, we are not required to check\n        # the host key.\n        if (hostkey is not None) and not gss_kex:\n            key = self.get_remote_server_key()\n            if (\n                key.get_name() != hostkey.get_name()\n                or key.asbytes() != hostkey.asbytes()\n            ):\n                self._log(DEBUG, \"Bad host key from server\")\n                self._log(\n                    DEBUG,\n                    \"Expected: {}: {}\".format(\n                        hostkey.get_name(), repr(hostkey.asbytes())\n                    ),\n                )\n                self._log(\n                    DEBUG,\n                    \"Got     : {}: {}\".format(\n                        key.get_name(), repr(key.asbytes())\n                    ),\n                )\n                raise SSHException(\"Bad host key from server\")\n            self._log(\n                DEBUG, \"Host key verified ({})\".format(hostkey.get_name())\n            )\n\n        if (pkey is not None) or (password is not None) or gss_auth or gss_kex:\n            if gss_auth:\n                self._log(\n                    DEBUG, \"Attempting GSS-API auth... (gssapi-with-mic)\"\n                )  # noqa\n                self.auth_gssapi_with_mic(\n                    username, self.gss_host, gss_deleg_creds\n                )\n            elif gss_kex:\n                self._log(DEBUG, \"Attempting GSS-API auth... (gssapi-keyex)\")\n                self.auth_gssapi_keyex(username)\n            elif pkey is not None:\n                self._log(DEBUG, \"Attempting public-key auth...\")\n                self.auth_publickey(username, pkey)\n            else:\n                self._log(DEBUG, \"Attempting password auth...\")\n                self.auth_password(username, password)\n\n        return", "response": "Connect to a remote server and return a new session."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_exception(self):\n        self.lock.acquire()\n        try:\n            e = self.saved_exception\n            self.saved_exception = None\n            return e\n        finally:\n            self.lock.release()", "response": "Returns any exception that happened during the last server request."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_subsystem_handler(self, name, handler, *larg, **kwarg):\n        try:\n            self.lock.acquire()\n            self.subsystem_table[name] = (handler, larg, kwarg)\n        finally:\n            self.lock.release()", "response": "Set the handler class for a subsystem in server mode."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef auth_password(self, username, password, event=None, fallback=True):\n        if (not self.active) or (not self.initial_kex_done):\n            # we should never try to send the password unless we're on a secure\n            # link\n            raise SSHException(\"No existing session\")\n        if event is None:\n            my_event = threading.Event()\n        else:\n            my_event = event\n        self.auth_handler = AuthHandler(self)\n        self.auth_handler.auth_password(username, password, my_event)\n        if event is not None:\n            # caller wants to wait for event themselves\n            return []\n        try:\n            return self.auth_handler.wait_for_response(my_event)\n        except BadAuthenticationType as e:\n            # if password auth isn't allowed, but keyboard-interactive *is*,\n            # try to fudge it\n            if not fallback or (\"keyboard-interactive\" not in e.allowed_types):\n                raise\n            try:\n\n                def handler(title, instructions, fields):\n                    if len(fields) > 1:\n                        raise SSHException(\"Fallback authentication failed.\")\n                    if len(fields) == 0:\n                        # for some reason, at least on os x, a 2nd request will\n                        # be made with zero fields requested.  maybe it's just\n                        # to try to fake out automated scripting of the exact\n                        # type we're doing here.  *shrug* :)\n                        return []\n                    return [password]\n\n                return self.auth_interactive(username, handler)\n            except SSHException:\n                # attempt failed; just raise the original exception\n                raise e", "response": "Authenticate to the server using a password."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef auth_publickey(self, username, key, event=None):\n        if (not self.active) or (not self.initial_kex_done):\n            # we should never try to authenticate unless we're on a secure link\n            raise SSHException(\"No existing session\")\n        if event is None:\n            my_event = threading.Event()\n        else:\n            my_event = event\n        self.auth_handler = AuthHandler(self)\n        self.auth_handler.auth_publickey(username, key, my_event)\n        if event is not None:\n            # caller wants to wait for event themselves\n            return []\n        return self.auth_handler.wait_for_response(my_event)", "response": "Authenticate to the server using a private key."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nauthenticates to the server interactively.", "response": "def auth_interactive(self, username, handler, submethods=\"\"):\n        \"\"\"\n        Authenticate to the server interactively.  A handler is used to answer\n        arbitrary questions from the server.  On many servers, this is just a\n        dumb wrapper around PAM.\n\n        This method will block until the authentication succeeds or fails,\n        peroidically calling the handler asynchronously to get answers to\n        authentication questions.  The handler may be called more than once\n        if the server continues to ask questions.\n\n        The handler is expected to be a callable that will handle calls of the\n        form: ``handler(title, instructions, prompt_list)``.  The ``title`` is\n        meant to be a dialog-window title, and the ``instructions`` are user\n        instructions (both are strings).  ``prompt_list`` will be a list of\n        prompts, each prompt being a tuple of ``(str, bool)``.  The string is\n        the prompt and the boolean indicates whether the user text should be\n        echoed.\n\n        A sample call would thus be:\n        ``handler('title', 'instructions', [('Password:', False)])``.\n\n        The handler should return a list or tuple of answers to the server's\n        questions.\n\n        If the server requires multi-step authentication (which is very rare),\n        this method will return a list of auth types permissible for the next\n        step.  Otherwise, in the normal case, an empty list is returned.\n\n        :param str username: the username to authenticate as\n        :param callable handler: a handler for responding to server questions\n        :param str submethods: a string list of desired submethods (optional)\n        :return:\n            list of auth types permissible for the next stage of\n            authentication (normally empty).\n\n        :raises: `.BadAuthenticationType` -- if public-key authentication isn't\n            allowed by the server for this user\n        :raises: `.AuthenticationException` -- if the authentication failed\n        :raises: `.SSHException` -- if there was a network error\n\n        .. versionadded:: 1.5\n        \"\"\"\n        if (not self.active) or (not self.initial_kex_done):\n            # we should never try to authenticate unless we're on a secure link\n            raise SSHException(\"No existing session\")\n        my_event = threading.Event()\n        self.auth_handler = AuthHandler(self)\n        self.auth_handler.auth_interactive(\n            username, handler, my_event, submethods\n        )\n        return self.auth_handler.wait_for_response(my_event)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef auth_interactive_dumb(self, username, handler=None, submethods=\"\"):\n\n        if not handler:\n\n            def handler(title, instructions, prompt_list):\n                answers = []\n                if title:\n                    print(title.strip())\n                if instructions:\n                    print(instructions.strip())\n                for prompt, show_input in prompt_list:\n                    print(prompt.strip(), end=\" \")\n                    answers.append(input())\n                return answers\n\n        return self.auth_interactive(username, handler, submethods)", "response": "This is a dumb version of auth_interactive that uses the user s interactive function to get the user s identity."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nauthenticate to the server using GSS - API and SSPI.", "response": "def auth_gssapi_with_mic(self, username, gss_host, gss_deleg_creds):\n        \"\"\"\n        Authenticate to the Server using GSS-API / SSPI.\n\n        :param str username: The username to authenticate as\n        :param str gss_host: The target host\n        :param bool gss_deleg_creds: Delegate credentials or not\n        :return: list of auth types permissible for the next stage of\n                 authentication (normally empty)\n        :raises: `.BadAuthenticationType` -- if gssapi-with-mic isn't\n            allowed by the server (and no event was passed in)\n        :raises:\n            `.AuthenticationException` -- if the authentication failed (and no\n            event was passed in)\n        :raises: `.SSHException` -- if there was a network error\n        \"\"\"\n        if (not self.active) or (not self.initial_kex_done):\n            # we should never try to authenticate unless we're on a secure link\n            raise SSHException(\"No existing session\")\n        my_event = threading.Event()\n        self.auth_handler = AuthHandler(self)\n        self.auth_handler.auth_gssapi_with_mic(\n            username, gss_host, gss_deleg_creds, my_event\n        )\n        return self.auth_handler.wait_for_response(my_event)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_log_channel(self, name):\n        self.log_name = name\n        self.logger = util.get_logger(name)\n        self.packetizer.set_log(self.logger)", "response": "Set the name of the logging channel for this transport."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _next_channel(self):\n        chanid = self._channel_counter\n        while self._channels.get(chanid) is not None:\n            self._channel_counter = (self._channel_counter + 1) & 0xffffff\n            chanid = self._channel_counter\n        self._channel_counter = (self._channel_counter + 1) & 0xffffff\n        return chanid", "response": "Returns the next available channel in the channel list."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing the key for the given id and nbytes.", "response": "def _compute_key(self, id, nbytes):\n        \"\"\"id is 'A' - 'F' for the various keys used by ssh\"\"\"\n        m = Message()\n        m.add_mpint(self.K)\n        m.add_bytes(self.H)\n        m.add_byte(b(id))\n        m.add_bytes(self.session_id)\n        # Fallback to SHA1 for kex engines that fail to specify a hex\n        # algorithm, or for e.g. transport tests that don't run kexinit.\n        hash_algo = getattr(self.kex_engine, \"hash_algo\", None)\n        hash_select_msg = \"kex engine {} specified hash_algo {!r}\".format(\n            self.kex_engine.__class__.__name__, hash_algo\n        )\n        if hash_algo is None:\n            hash_algo = sha1\n            hash_select_msg += \", falling back to sha1\"\n        if not hasattr(self, \"_logged_hash_selection\"):\n            self._log(DEBUG, hash_select_msg)\n            setattr(self, \"_logged_hash_selection\", True)\n        out = sofar = hash_algo(m.asbytes()).digest()\n        while len(out) < nbytes:\n            m = Message()\n            m.add_mpint(self.K)\n            m.add_bytes(self.H)\n            m.add_bytes(sofar)\n            digest = hash_algo(m.asbytes()).digest()\n            out += digest\n            sofar += digest\n        return out[:nbytes]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks message type against current auth state.", "response": "def _ensure_authed(self, ptype, message):\n        \"\"\"\n        Checks message type against current auth state.\n\n        If server mode, and auth has not succeeded, and the message is of a\n        post-auth type (channel open or global request) an appropriate error\n        response Message is crafted and returned to caller for sending.\n\n        Otherwise (client mode, authed, or pre-auth message) returns None.\n        \"\"\"\n        if (\n            not self.server_mode\n            or ptype <= HIGHEST_USERAUTH_MESSAGE_ID\n            or self.is_authenticated()\n        ):\n            return None\n        # WELP. We must be dealing with someone trying to do non-auth things\n        # without being authed. Tell them off, based on message class.\n        reply = Message()\n        # Global requests have no details, just failure.\n        if ptype == MSG_GLOBAL_REQUEST:\n            reply.add_byte(cMSG_REQUEST_FAILURE)\n        # Channel opens let us reject w/ a specific type + message.\n        elif ptype == MSG_CHANNEL_OPEN:\n            kind = message.get_text()  # noqa\n            chanid = message.get_int()\n            reply.add_byte(cMSG_CHANNEL_OPEN_FAILURE)\n            reply.add_int(chanid)\n            reply.add_int(OPEN_FAILED_ADMINISTRATIVELY_PROHIBITED)\n            reply.add_string(\"\")\n            reply.add_string(\"en\")\n        # NOTE: Post-open channel messages do not need checking; the above will\n        # reject attemps to open channels, meaning that even if a malicious\n        # user tries to send a MSG_CHANNEL_REQUEST, it will simply fall under\n        # the logic that handles unknown channel IDs (as the channel list will\n        # be empty.)\n        return reply"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nswitching on newly negotiated encryption parameters for outbound traffic", "response": "def _activate_outbound(self):\n        \"\"\"switch on newly negotiated encryption parameters for\n         outbound traffic\"\"\"\n        m = Message()\n        m.add_byte(cMSG_NEWKEYS)\n        self._send_message(m)\n        block_size = self._cipher_info[self.local_cipher][\"block-size\"]\n        if self.server_mode:\n            IV_out = self._compute_key(\"B\", block_size)\n            key_out = self._compute_key(\n                \"D\", self._cipher_info[self.local_cipher][\"key-size\"]\n            )\n        else:\n            IV_out = self._compute_key(\"A\", block_size)\n            key_out = self._compute_key(\n                \"C\", self._cipher_info[self.local_cipher][\"key-size\"]\n            )\n        engine = self._get_cipher(\n            self.local_cipher, key_out, IV_out, self._ENCRYPT\n        )\n        mac_size = self._mac_info[self.local_mac][\"size\"]\n        mac_engine = self._mac_info[self.local_mac][\"class\"]\n        # initial mac keys are done in the hash's natural size (not the\n        # potentially truncated transmission size)\n        if self.server_mode:\n            mac_key = self._compute_key(\"F\", mac_engine().digest_size)\n        else:\n            mac_key = self._compute_key(\"E\", mac_engine().digest_size)\n        sdctr = self.local_cipher.endswith(\"-ctr\")\n        self.packetizer.set_outbound_cipher(\n            engine, block_size, mac_engine, mac_size, mac_key, sdctr\n        )\n        compress_out = self._compression_info[self.local_compression][0]\n        if compress_out is not None and (\n            self.local_compression != \"zlib@openssh.com\" or self.authenticated\n        ):\n            self._log(DEBUG, \"Switching on outbound compression ...\")\n            self.packetizer.set_outbound_compressor(compress_out())\n        if not self.packetizer.need_rekey():\n            self.in_kex = False\n        # we always expect to receive NEWKEYS now\n        self._expect_packet(MSG_NEWKEYS)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nflushing all data in the write buffer.", "response": "def flush(self):\n        \"\"\"\n        Write out any data in the write buffer.  This may do nothing if write\n        buffering is not turned on.\n        \"\"\"\n        self._write_all(self._wbuffer.getvalue())\n        self._wbuffer = BytesIO()\n        return"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef readinto(self, buff):\n        data = self.read(len(buff))\n        buff[: len(data)] = data\n        return len(data)", "response": "Reads up to len ( buff ) bytes into buff and returns the number of bytes read."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef readlines(self, sizehint=None):\n        lines = []\n        byte_count = 0\n        while True:\n            line = self.readline()\n            if len(line) == 0:\n                break\n            lines.append(line)\n            byte_count += len(line)\n            if (sizehint is not None) and (byte_count >= sizehint):\n                break\n        return lines", "response": "Reads all remaining lines using readline and returns them as a list."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef write(self, data):\n        if isinstance(data, text_type):\n            # Accept text and encode as utf-8 for compatibility only.\n            data = data.encode(\"utf-8\")\n        if self._closed:\n            raise IOError(\"File is closed\")\n        if not (self._flags & self.FLAG_WRITE):\n            raise IOError(\"File not open for writing\")\n        if not (self._flags & self.FLAG_BUFFERED):\n            self._write_all(data)\n            return\n        self._wbuffer.write(data)\n        if self._flags & self.FLAG_LINE_BUFFERED:\n            # only scan the new data for linefeed, to avoid wasting time.\n            last_newline_pos = data.rfind(linefeed_byte)\n            if last_newline_pos >= 0:\n                wbuf = self._wbuffer.getvalue()\n                last_newline_pos += len(wbuf) - len(data)\n                self._write_all(wbuf[: last_newline_pos + 1])\n                self._wbuffer = BytesIO()\n                self._wbuffer.write(wbuf[last_newline_pos + 1 :])\n            return\n        # even if we're line buffering, if the buffer has grown past the\n        # buffer size, force a flush.\n        if self._wbuffer.tell() >= self._bufsize:\n            self.flush()\n        return", "response": "Writes data to the file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add(self, hostname, keytype, key):\n        for e in self._entries:\n            if (hostname in e.hostnames) and (e.key.get_name() == keytype):\n                e.key = key\n                return\n        self._entries.append(HostKeyEntry([hostname], key))", "response": "Add a new entry to the table."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load(self, filename):\n        with open(filename, \"r\") as f:\n            for lineno, line in enumerate(f, 1):\n                line = line.strip()\n                if (len(line) == 0) or (line[0] == \"#\"):\n                    continue\n                try:\n                    e = HostKeyEntry.from_line(line, lineno)\n                except SSHException:\n                    continue\n                if e is not None:\n                    _hostnames = e.hostnames\n                    for h in _hostnames:\n                        if self.check(h, e.key):\n                            e.hostnames.remove(h)\n                    if len(e.hostnames):\n                        self._entries.append(e)", "response": "Load a file of known SSH host keys from disk."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a dict of keytype to key objects associated with this hostkey.", "response": "def lookup(self, hostname):\n        \"\"\"\n        Find a hostkey entry for a given hostname or IP.  If no entry is found,\n        ``None`` is returned.  Otherwise a dictionary of keytype to key is\n        returned.  The keytype will be either ``\"ssh-rsa\"`` or ``\"ssh-dss\"``.\n\n        :param str hostname: the hostname (or IP) to lookup\n        :return: dict of `str` -> `.PKey` keys associated with this host\n            (or ``None``)\n        \"\"\"\n\n        class SubDict(MutableMapping):\n            def __init__(self, hostname, entries, hostkeys):\n                self._hostname = hostname\n                self._entries = entries\n                self._hostkeys = hostkeys\n\n            def __iter__(self):\n                for k in self.keys():\n                    yield k\n\n            def __len__(self):\n                return len(self.keys())\n\n            def __delitem__(self, key):\n                for e in list(self._entries):\n                    if e.key.get_name() == key:\n                        self._entries.remove(e)\n                else:\n                    raise KeyError(key)\n\n            def __getitem__(self, key):\n                for e in self._entries:\n                    if e.key.get_name() == key:\n                        return e.key\n                raise KeyError(key)\n\n            def __setitem__(self, key, val):\n                for e in self._entries:\n                    if e.key is None:\n                        continue\n                    if e.key.get_name() == key:\n                        # replace\n                        e.key = val\n                        break\n                else:\n                    # add a new one\n                    e = HostKeyEntry([hostname], val)\n                    self._entries.append(e)\n                    self._hostkeys._entries.append(e)\n\n            def keys(self):\n                return [\n                    e.key.get_name()\n                    for e in self._entries\n                    if e.key is not None\n                ]\n\n        entries = []\n        for e in self._entries:\n            if self._hostname_matches(hostname, e):\n                entries.append(e)\n        if len(entries) == 0:\n            return None\n        return SubDict(hostname, entries, self)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntesting whether hostname string matches given SubDict entry.", "response": "def _hostname_matches(self, hostname, entry):\n        \"\"\"\n        Tests whether ``hostname`` string matches given SubDict ``entry``.\n\n        :returns bool:\n        \"\"\"\n        for h in entry.hostnames:\n            if (\n                h == hostname\n                or h.startswith(\"|1|\")\n                and not hostname.startswith(\"|1|\")\n                and constant_time_bytes_eq(self.hash_host(hostname, h), h)\n            ):\n                return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse a line of text to find the names for the host the type of key and the key data.", "response": "def from_line(cls, line, lineno=None):\n        \"\"\"\n        Parses the given line of text to find the names for the host,\n        the type of key, and the key data. The line is expected to be in the\n        format used by the OpenSSH known_hosts file.\n\n        Lines are expected to not have leading or trailing whitespace.\n        We don't bother to check for comments or empty lines.  All of\n        that should be taken care of before sending the line to us.\n\n        :param str line: a line from an OpenSSH known_hosts file\n        \"\"\"\n        log = get_logger(\"paramiko.hostkeys\")\n        fields = line.split(\" \")\n        if len(fields) < 3:\n            # Bad number of fields\n            msg = \"Not enough fields found in known_hosts in line {} ({!r})\"\n            log.info(msg.format(lineno, line))\n            return None\n        fields = fields[:3]\n\n        names, keytype, key = fields\n        names = names.split(\",\")\n\n        # Decide what kind of key we're looking at and create an object\n        # to hold it accordingly.\n        try:\n            key = b(key)\n            if keytype == \"ssh-rsa\":\n                key = RSAKey(data=decodebytes(key))\n            elif keytype == \"ssh-dss\":\n                key = DSSKey(data=decodebytes(key))\n            elif keytype in ECDSAKey.supported_key_format_identifiers():\n                key = ECDSAKey(data=decodebytes(key), validate_point=False)\n            elif keytype == \"ssh-ed25519\":\n                key = Ed25519Key(data=decodebytes(key))\n            else:\n                log.info(\"Unable to handle key of type {}\".format(keytype))\n                return None\n\n        except binascii.Error as e:\n            raise InvalidHostKey(line, e)\n\n        return cls(names, key)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nqueries the pageant for a message.", "response": "def _query_pageant(msg):\n    \"\"\"\n    Communication with the Pageant process is done through a shared\n    memory-mapped file.\n    \"\"\"\n    hwnd = _get_pageant_window_object()\n    if not hwnd:\n        # Raise a failure to connect exception, pageant isn't running anymore!\n        return None\n\n    # create a name for the mmap\n    map_name = \"PageantRequest%08x\" % thread.get_ident()\n\n    pymap = _winapi.MemoryMap(\n        map_name, _AGENT_MAX_MSGLEN, _winapi.get_security_attributes_for_user()\n    )\n    with pymap:\n        pymap.write(msg)\n        # Create an array buffer containing the mapped filename\n        char_buffer = array.array(\"b\", b(map_name) + zero_byte)  # noqa\n        char_buffer_address, char_buffer_size = char_buffer.buffer_info()\n        # Create a string to use for the SendMessage function call\n        cds = COPYDATASTRUCT(\n            _AGENT_COPYDATA_ID, char_buffer_size, char_buffer_address\n        )\n\n        response = ctypes.windll.user32.SendMessageA(\n            hwnd, win32con_WM_COPYDATA, ctypes.sizeof(cds), ctypes.byref(cds)\n        )\n\n        if response > 0:\n            pymap.seek(0)\n            datalen = pymap.read(4)\n            retlen = struct.unpack(\">I\", datalen)[0]\n            return datalen + pymap.read(retlen)\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_remainder(self):\n        position = self.packet.tell()\n        remainder = self.packet.read()\n        self.packet.seek(position)\n        return remainder", "response": "Return the remainder of this message."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the bytes of this message that have been parsed and returned.", "response": "def get_so_far(self):\n        \"\"\"\n        Returns the `str` bytes of this message that have been parsed and\n        returned. The string passed into a message's constructor can be\n        regenerated by concatenating ``get_so_far`` and `get_remainder`.\n        \"\"\"\n        position = self.packet.tell()\n        self.rewind()\n        return self.packet.read(position)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_bytes(self, n):\n        b = self.packet.read(n)\n        max_pad_size = 1 << 20  # Limit padding to 1 MB\n        if len(b) < n < max_pad_size:\n            return b + zero_byte * (n - len(b))\n        return b", "response": "Returns the next n bytes of the message."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds a boolean value to the stream.", "response": "def add_boolean(self, b):\n        \"\"\"\n        Add a boolean value to the stream.\n\n        :param bool b: boolean value to add\n        \"\"\"\n        if b:\n            self.packet.write(one_byte)\n        else:\n            self.packet.write(zero_byte)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds an integer to the stream.", "response": "def add_adaptive_int(self, n):\n        \"\"\"\n        Add an integer to the stream.\n\n        :param int n: integer to add\n        \"\"\"\n        if n >= Message.big_int:\n            self.packet.write(max_byte)\n            self.add_string(util.deflate_long(n))\n        else:\n            self.packet.write(struct.pack(\">I\", n))\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_int64(self, n):\n        self.packet.write(struct.pack(\">Q\", n))\n        return self", "response": "Add a 64 - bit int to the stream."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _roll_random(n):\n    bits = util.bit_length(n - 1)\n    byte_count = (bits + 7) // 8\n    hbyte_mask = pow(2, bits % 8) - 1\n\n    # so here's the plan:\n    # we fetch as many random bits as we'd need to fit N-1, and if the\n    # generated number is >= N, we try again.  in the worst case (N-1 is a\n    # power of 2), we have slightly better than 50% odds of getting one that\n    # fits, so i can't guarantee that this loop will ever finish, but the odds\n    # of it looping forever should be infinitesimal.\n    while True:\n        x = os.urandom(byte_count)\n        if hbyte_mask > 0:\n            x = byte_mask(x[0], hbyte_mask) + x[1:]\n        num = util.inflate_long(x, 1)\n        if num < n:\n            break\n    return num", "response": "returns a random number from 0 to N - 1"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads a file and parses the modulus file.", "response": "def read_file(self, filename):\n        \"\"\"\n        :raises IOError: passed from any file operations that fail.\n        \"\"\"\n        self.pack = {}\n        with open(filename, \"r\") as f:\n            for line in f:\n                line = line.strip()\n                if (len(line) == 0) or (line[0] == \"#\"):\n                    continue\n                try:\n                    self._parse_modulus(line)\n                except:\n                    continue"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef generate(cls, curve=ec.SECP256R1(), progress_func=None, bits=None):\n        if bits is not None:\n            curve = cls._ECDSA_CURVES.get_by_key_length(bits)\n            if curve is None:\n                raise ValueError(\"Unsupported key length: {:d}\".format(bits))\n            curve = curve.curve_class()\n\n        private_key = ec.generate_private_key(curve, backend=default_backend())\n        return ECDSAKey(vals=(private_key, private_key.public_key()))", "response": "This factory function generates a new private ECDSA key."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a new. PKey object by reading a private key file.", "response": "def from_private_key_file(cls, filename, password=None):\n        \"\"\"\n        Create a key object by reading a private key file.  If the private\n        key is encrypted and ``password`` is not ``None``, the given password\n        will be used to decrypt the key (otherwise `.PasswordRequiredException`\n        is thrown).  Through the magic of Python, this factory method will\n        exist in all subclasses of PKey (such as `.RSAKey` or `.DSSKey`), but\n        is useless on the abstract PKey class.\n\n        :param str filename: name of the file to read\n        :param str password:\n            an optional password to use to decrypt the key file, if it's\n            encrypted\n        :return: a new `.PKey` based on the given private key\n\n        :raises: ``IOError`` -- if there was an error reading the file\n        :raises: `.PasswordRequiredException` -- if the private key file is\n            encrypted, and ``password`` is ``None``\n        :raises: `.SSHException` -- if the key file is invalid\n        \"\"\"\n        key = cls(filename=filename, password=password)\n        return key"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef from_private_key(cls, file_obj, password=None):\n        key = cls(file_obj=file_obj, password=password)\n        return key", "response": "Create a new. pkey object from a file - like object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _write_private_key_file(self, filename, key, format, password=None):\n        with open(filename, \"w\") as f:\n            os.chmod(filename, o600)\n            self._write_private_key(f, key, format, password=password)", "response": "Write a private key file in a format that can be read by SSH2 - format."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _check_type_and_load_cert(self, msg, key_type, cert_type):\n        # Normalization; most classes have a single key type and give a string,\n        # but eg ECDSA is a 1:N mapping.\n        key_types = key_type\n        cert_types = cert_type\n        if isinstance(key_type, string_types):\n            key_types = [key_types]\n        if isinstance(cert_types, string_types):\n            cert_types = [cert_types]\n        # Can't do much with no message, that should've been handled elsewhere\n        if msg is None:\n            raise SSHException(\"Key object may not be empty\")\n        # First field is always key type, in either kind of object. (make sure\n        # we rewind before grabbing it - sometimes caller had to do their own\n        # introspection first!)\n        msg.rewind()\n        type_ = msg.get_text()\n        # Regular public key - nothing special to do besides the implicit\n        # type check.\n        if type_ in key_types:\n            pass\n        # OpenSSH-compatible certificate - store full copy as .public_blob\n        # (so signing works correctly) and then fast-forward past the\n        # nonce.\n        elif type_ in cert_types:\n            # This seems the cleanest way to 'clone' an already-being-read\n            # message; they're *IO objects at heart and their .getvalue()\n            # always returns the full value regardless of pointer position.\n            self.load_certificate(Message(msg.asbytes()))\n            # Read out nonce as it comes before the public numbers.\n            # TODO: usefully interpret it & other non-public-number fields\n            # (requires going back into per-type subclasses.)\n            msg.get_string()\n        else:\n            err = \"Invalid key (class: {}, data type: {}\"\n            raise SSHException(err.format(self.__class__.__name__, type_))", "response": "Checks the message type - checking and certificate loading."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload the certificate contents from a file or a string containing such a file.", "response": "def load_certificate(self, value):\n        \"\"\"\n        Supplement the private key contents with data loaded from an OpenSSH\n        public key (``.pub``) or certificate (``-cert.pub``) file, a string\n        containing such a file, or a `.Message` object.\n\n        The .pub contents adds no real value, since the private key\n        file includes sufficient information to derive the public\n        key info. For certificates, however, this can be used on\n        the client side to offer authentication requests to the server\n        based on certificate instead of raw public key.\n\n        See:\n        https://github.com/openssh/openssh-portable/blob/master/PROTOCOL.certkeys\n\n        Note: very little effort is made to validate the certificate contents,\n        that is for the server to decide if it is good enough to authenticate\n        successfully.\n        \"\"\"\n        if isinstance(value, Message):\n            constructor = \"from_message\"\n        elif os.path.isfile(value):\n            constructor = \"from_file\"\n        else:\n            constructor = \"from_string\"\n        blob = getattr(PublicBlob, constructor)(value)\n        if not blob.key_type.startswith(self.get_name()):\n            err = \"PublicBlob type {} incompatible with key type {}\"\n            raise ValueError(err.format(blob.key_type, self.get_name()))\n        self.public_blob = blob"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef from_file(cls, filename):\n        with open(filename) as f:\n            string = f.read()\n        return cls.from_string(string)", "response": "Create a public blob from a file on disk."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef from_string(cls, string):\n        fields = string.split(None, 2)\n        if len(fields) < 2:\n            msg = \"Not enough fields for public blob: {}\"\n            raise ValueError(msg.format(fields))\n        key_type = fields[0]\n        key_blob = decodebytes(b(fields[1]))\n        try:\n            comment = fields[2].strip()\n        except IndexError:\n            comment = None\n        # Verify that the blob message first (string) field matches the\n        # key_type\n        m = Message(key_blob)\n        blob_type = m.get_text()\n        if blob_type != key_type:\n            deets = \"key type={!r}, but blob type={!r}\".format(\n                key_type, blob_type\n            )\n            raise ValueError(\"Invalid PublicBlob contents: {}\".format(deets))\n        # All good? All good.\n        return cls(type_=key_type, blob=key_blob, comment=comment)", "response": "Create a public blob from a string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_message(cls, message):\n        type_ = message.get_text()\n        return cls(type_=type_, blob=message.asbytes())", "response": "Create a public blob from a network. Message."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a pipe that is ord together into two pipe - like objects which are ord together affect the real pipe.", "response": "def make_or_pipe(pipe):\n    \"\"\"\n    wraps a pipe into two pipe-like objects which are \"or\"d together to\n    affect the real pipe. if either returned pipe is set, the wrapped pipe\n    is set. when both are cleared, the wrapped pipe is cleared.\n    \"\"\"\n    p1 = OrPipe(pipe)\n    p2 = OrPipe(pipe)\n    p1._partner = p2\n    p2._partner = p1\n    return p1, p2"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a pair of socket object and string address.", "response": "def get_connection(self):\n        \"\"\"\n        Return a pair of socket object and string address.\n\n        May block!\n        \"\"\"\n        conn = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)\n        try:\n            conn.bind(self._agent._get_filename())\n            conn.listen(1)\n            (r, addr) = conn.accept()\n            return r, addr\n        except:\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nclosing the current connection and terminate the agent", "response": "def close(self):\n        \"\"\"\n        Close the current connection and terminate the agent\n        Should be called manually\n        \"\"\"\n        if hasattr(self, \"thread\"):\n            self.thread._exit = True\n            self.thread.join(1000)\n        if self._conn is not None:\n            self._conn.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef close(self):\n        os.remove(self._file)\n        os.rmdir(self._dir)\n        self.thread._exit = True\n        self.thread.join(1000)\n        self._close()", "response": "Terminate the agent clean the files clean the directories close connections"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nswitching outbound data cipher.", "response": "def set_outbound_cipher(\n        self,\n        block_engine,\n        block_size,\n        mac_engine,\n        mac_size,\n        mac_key,\n        sdctr=False,\n    ):\n        \"\"\"\n        Switch outbound data cipher.\n        \"\"\"\n        self.__block_engine_out = block_engine\n        self.__sdctr_out = sdctr\n        self.__block_size_out = block_size\n        self.__mac_engine_out = mac_engine\n        self.__mac_size_out = mac_size\n        self.__mac_key_out = mac_key\n        self.__sent_bytes = 0\n        self.__sent_packets = 0\n        # wait until the reset happens in both directions before clearing\n        # rekey flag\n        self.__init_count |= 1\n        if self.__init_count == 3:\n            self.__init_count = 0\n            self.__need_rekey = False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_inbound_cipher(\n        self, block_engine, block_size, mac_engine, mac_size, mac_key\n    ):\n        \"\"\"\n        Switch inbound data cipher.\n        \"\"\"\n        self.__block_engine_in = block_engine\n        self.__block_size_in = block_size\n        self.__mac_engine_in = mac_engine\n        self.__mac_size_in = mac_size\n        self.__mac_key_in = mac_key\n        self.__received_bytes = 0\n        self.__received_packets = 0\n        self.__received_bytes_overflow = 0\n        self.__received_packets_overflow = 0\n        # wait until the reset happens in both directions before clearing\n        # rekey flag\n        self.__init_count |= 2\n        if self.__init_count == 3:\n            self.__init_count = 0\n            self.__need_rekey = False", "response": "Switch inbound data cipher."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nturning on the keepalive callback.", "response": "def set_keepalive(self, interval, callback):\n        \"\"\"\n        Turn on/off the callback keepalive.  If ``interval`` seconds pass with\n        no data read from or written to the socket, the callback will be\n        executed and the timer will be reset.\n        \"\"\"\n        self.__keepalive_interval = interval\n        self.__keepalive_callback = callback\n        self.__keepalive_last = time.time()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef start_handshake(self, timeout):\n        if not self.__timer:\n            self.__timer = threading.Timer(float(timeout), self.read_timer)\n            self.__timer.start()", "response": "Starts a book keeping timer that can signal a timeout in the recordset."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef handshake_timed_out(self):\n        if not self.__timer:\n            return False\n        if self.__handshake_complete:\n            return False\n        return self.__timer_expired", "response": "Checks if the handshake has timed out."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef complete_handshake(self):\n        if self.__timer:\n            self.__timer.cancel()\n            self.__timer_expired = False\n            self.__handshake_complete = True", "response": "Tells Packetizer that the handshake has completed."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef close(self):\n        readfile = getattr(self, \"readfile\", None)\n        if readfile is not None:\n            readfile.close()\n        writefile = getattr(self, \"writefile\", None)\n        if writefile is not None:\n            writefile.close()", "response": "Closes the underlying OS level file object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_next_files(self):\n        fnlist = self.__files[:16]\n        self.__files = self.__files[16:]\n        return fnlist", "response": "Returns a list of next files in the cache directory."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef generate_key_bytes(hash_alg, salt, key, nbytes):\n    keydata = bytes()\n    digest = bytes()\n    if len(salt) > 8:\n        salt = salt[:8]\n    while nbytes > 0:\n        hash_obj = hash_alg()\n        if len(digest) > 0:\n            hash_obj.update(digest)\n        hash_obj.update(b(key))\n        hash_obj.update(salt)\n        digest = hash_obj.digest()\n        size = min(nbytes, len(digest))\n        keydata += digest[:size]\n        nbytes -= size\n    return keydata", "response": "This function generates a key from a secure hash of a password passphrase or other human - source key."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef log_to_file(filename, level=DEBUG):\n    logger = logging.getLogger(\"paramiko\")\n    if len(logger.handlers) > 0:\n        return\n    logger.setLevel(level)\n    f = open(filename, \"a\")\n    handler = logging.StreamHandler(f)\n    frm = \"%(levelname)-.3s [%(asctime)s.%(msecs)03d] thr=%(_threadid)-3d\"\n    frm += \" %(name)s: %(message)s\"\n    handler.setFormatter(logging.Formatter(frm, \"%Y%m%d-%H:%M:%S\"))\n    logger.addHandler(handler)", "response": "send paramiko logs to a logfile"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef send(self, content):\n        try:\n            self.process.stdin.write(content)\n        except IOError as e:\n            # There was a problem with the child process. It probably\n            # died and we can't proceed. The best option here is to\n            # raise an exception informing the user that the informed\n            # ProxyCommand is not working.\n            raise ProxyCommandFailure(\" \".join(self.cmd), e.strerror)\n        return len(content)", "response": "Write the content received from the SSH client to the standard\n            input of the forked command."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef recv(self, size):\n        try:\n            buffer = b\"\"\n            start = time.time()\n            while len(buffer) < size:\n                select_timeout = None\n                if self.timeout is not None:\n                    elapsed = time.time() - start\n                    if elapsed >= self.timeout:\n                        raise socket.timeout()\n                    select_timeout = self.timeout - elapsed\n\n                r, w, x = select([self.process.stdout], [], [], select_timeout)\n                if r and r[0] == self.process.stdout:\n                    buffer += os.read(\n                        self.process.stdout.fileno(), size - len(buffer)\n                    )\n            return buffer\n        except socket.timeout:\n            if buffer:\n                # Don't raise socket.timeout, return partial result instead\n                return buffer\n            raise  # socket.timeout is a subclass of IOError\n        except IOError as e:\n            raise ProxyCommandFailure(\" \".join(self.cmd), e.strerror)", "response": "Read from the forked program and return the string of bytes read."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a descriptive error message from a system error number.", "response": "def format_system_message(errno):\n    \"\"\"\n    Call FormatMessage with a system error number to retrieve\n    the descriptive error message.\n    \"\"\"\n    # first some flags used by FormatMessageW\n    ALLOCATE_BUFFER = 0x100\n    FROM_SYSTEM = 0x1000\n\n    # Let FormatMessageW allocate the buffer (we'll free it below)\n    # Also, let it know we want a system error message.\n    flags = ALLOCATE_BUFFER | FROM_SYSTEM\n    source = None\n    message_id = errno\n    language_id = 0\n    result_buffer = ctypes.wintypes.LPWSTR()\n    buffer_size = 0\n    arguments = None\n    bytes = ctypes.windll.kernel32.FormatMessageW(\n        flags,\n        source,\n        message_id,\n        language_id,\n        ctypes.byref(result_buffer),\n        buffer_size,\n        arguments,\n    )\n    # note the following will cause an infinite loop if GetLastError\n    #  repeatedly returns an error that cannot be formatted, although\n    #  this should not happen.\n    handle_nonzero_success(bytes)\n    message = result_buffer.value\n    ctypes.windll.kernel32.LocalFree(result_buffer)\n    return message"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngiving a token get the token information for it.", "response": "def GetTokenInformation(token, information_class):\n    \"\"\"\n    Given a token, get the token information for it.\n    \"\"\"\n    data_size = ctypes.wintypes.DWORD()\n    ctypes.windll.advapi32.GetTokenInformation(\n        token, information_class.num, 0, 0, ctypes.byref(data_size)\n    )\n    data = ctypes.create_string_buffer(data_size.value)\n    handle_nonzero_success(\n        ctypes.windll.advapi32.GetTokenInformation(\n            token,\n            information_class.num,\n            ctypes.byref(data),\n            ctypes.sizeof(data),\n            ctypes.byref(data_size),\n        )\n    )\n    return ctypes.cast(data, ctypes.POINTER(TOKEN_USER)).contents"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_current_user():\n    process = OpenProcessToken(\n        ctypes.windll.kernel32.GetCurrentProcess(), TokenAccess.TOKEN_QUERY\n    )\n    return GetTokenInformation(process, TOKEN_USER)", "response": "Returns a TOKEN_USER for the owner of this process."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating a new private DSS key. This factory function can be used to generate a new host key or authentication key. :param int bits: number of bits the generated key should be. :param progress_func: Unused :return: new `.DSSKey` private key", "response": "def generate(bits=1024, progress_func=None):\n        \"\"\"\n        Generate a new private DSS key.  This factory function can be used to\n        generate a new host key or authentication key.\n\n        :param int bits: number of bits the generated key should be.\n        :param progress_func: Unused\n        :return: new `.DSSKey` private key\n        \"\"\"\n        numbers = dsa.generate_private_key(\n            bits, backend=default_backend()\n        ).private_numbers()\n        key = DSSKey(\n            vals=(\n                numbers.public_numbers.parameter_numbers.p,\n                numbers.public_numbers.parameter_numbers.q,\n                numbers.public_numbers.parameter_numbers.g,\n                numbers.public_numbers.y,\n            )\n        )\n        key.x = numbers.x\n        return key"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload host keys from a system file.", "response": "def load_system_host_keys(self, filename=None):\n        \"\"\"\n        Load host keys from a system (read-only) file.  Host keys read with\n        this method will not be saved back by `save_host_keys`.\n\n        This method can be called multiple times.  Each new set of host keys\n        will be merged with the existing set (new replacing old if there are\n        conflicts).\n\n        If ``filename`` is left as ``None``, an attempt will be made to read\n        keys from the user's local \"known hosts\" file, as used by OpenSSH,\n        and no exception will be raised if the file can't be read.  This is\n        probably only useful on posix.\n\n        :param str filename: the filename to read, or ``None``\n\n        :raises: ``IOError`` --\n            if a filename was provided and the file could not be read\n        \"\"\"\n        if filename is None:\n            # try the user's .ssh key file, and mask exceptions\n            filename = os.path.expanduser(\"~/.ssh/known_hosts\")\n            try:\n                self._system_host_keys.load(filename)\n            except IOError:\n                pass\n            return\n        self._system_host_keys.load(filename)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads host keys from a local host - key file.", "response": "def load_host_keys(self, filename):\n        \"\"\"\n        Load host keys from a local host-key file.  Host keys read with this\n        method will be checked after keys loaded via `load_system_host_keys`,\n        but will be saved back by `save_host_keys` (so they can be modified).\n        The missing host key policy `.AutoAddPolicy` adds keys to this set and\n        saves them, when connecting to a previously-unknown server.\n\n        This method can be called multiple times.  Each new set of host keys\n        will be merged with the existing set (new replacing old if there are\n        conflicts).  When automatically saving, the last hostname is used.\n\n        :param str filename: the filename to read\n\n        :raises: ``IOError`` -- if the filename could not be read\n        \"\"\"\n        self._host_keys_filename = filename\n        self._host_keys.load(filename)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_missing_host_key_policy(self, policy):\n        if inspect.isclass(policy):\n            policy = policy()\n        self._policy = policy", "response": "Sets the policy to use when connecting to servers without a known host key."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nyield pairs of address families and addresses to try for connecting.", "response": "def _families_and_addresses(self, hostname, port):\n        \"\"\"\n        Yield pairs of address families and addresses to try for connecting.\n\n        :param str hostname: the server to connect to\n        :param int port: the server port to connect to\n        :returns: Yields an iterable of ``(family, address)`` tuples\n        \"\"\"\n        guess = True\n        addrinfos = socket.getaddrinfo(\n            hostname, port, socket.AF_UNSPEC, socket.SOCK_STREAM\n        )\n        for (family, socktype, proto, canonname, sockaddr) in addrinfos:\n            if socktype == socket.SOCK_STREAM:\n                yield family, sockaddr\n                guess = False\n\n        # some OS like AIX don't indicate SOCK_STREAM support, so just\n        # guess. :(  We only do this if we did not get a single result marked\n        # as socktype == SOCK_STREAM.\n        if guess:\n            for family, _, _, _, sockaddr in addrinfos:\n                yield family, sockaddr"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef connect(\n        self,\n        hostname,\n        port=SSH_PORT,\n        username=None,\n        password=None,\n        pkey=None,\n        key_filename=None,\n        timeout=None,\n        allow_agent=True,\n        look_for_keys=True,\n        compress=False,\n        sock=None,\n        gss_auth=False,\n        gss_kex=False,\n        gss_deleg_creds=True,\n        gss_host=None,\n        banner_timeout=None,\n        auth_timeout=None,\n        gss_trust_dns=True,\n        passphrase=None,\n    ):\n        \"\"\"\n        Connect to an SSH server and authenticate to it.  The server's host key\n        is checked against the system host keys (see `load_system_host_keys`)\n        and any local host keys (`load_host_keys`).  If the server's hostname\n        is not found in either set of host keys, the missing host key policy\n        is used (see `set_missing_host_key_policy`).  The default policy is\n        to reject the key and raise an `.SSHException`.\n\n        Authentication is attempted in the following order of priority:\n\n            - The ``pkey`` or ``key_filename`` passed in (if any)\n\n              - ``key_filename`` may contain OpenSSH public certificate paths\n                as well as regular private-key paths; when files ending in\n                ``-cert.pub`` are found, they are assumed to match a private\n                key, and both components will be loaded. (The private key\n                itself does *not* need to be listed in ``key_filename`` for\n                this to occur - *just* the certificate.)\n\n            - Any key we can find through an SSH agent\n            - Any \"id_rsa\", \"id_dsa\" or \"id_ecdsa\" key discoverable in\n              ``~/.ssh/``\n\n              - When OpenSSH-style public certificates exist that match an\n                existing such private key (so e.g. one has ``id_rsa`` and\n                ``id_rsa-cert.pub``) the certificate will be loaded alongside\n                the private key and used for authentication.\n\n            - Plain username/password auth, if a password was given\n\n        If a private key requires a password to unlock it, and a password is\n        passed in, that password will be used to attempt to unlock the key.\n\n        :param str hostname: the server to connect to\n        :param int port: the server port to connect to\n        :param str username:\n            the username to authenticate as (defaults to the current local\n            username)\n        :param str password:\n            Used for password authentication; is also used for private key\n            decryption if ``passphrase`` is not given.\n        :param str passphrase:\n            Used for decrypting private keys.\n        :param .PKey pkey: an optional private key to use for authentication\n        :param str key_filename:\n            the filename, or list of filenames, of optional private key(s)\n            and/or certs to try for authentication\n        :param float timeout:\n            an optional timeout (in seconds) for the TCP connect\n        :param bool allow_agent:\n            set to False to disable connecting to the SSH agent\n        :param bool look_for_keys:\n            set to False to disable searching for discoverable private key\n            files in ``~/.ssh/``\n        :param bool compress: set to True to turn on compression\n        :param socket sock:\n            an open socket or socket-like object (such as a `.Channel`) to use\n            for communication to the target host\n        :param bool gss_auth:\n            ``True`` if you want to use GSS-API authentication\n        :param bool gss_kex:\n            Perform GSS-API Key Exchange and user authentication\n        :param bool gss_deleg_creds: Delegate GSS-API client credentials or not\n        :param str gss_host:\n            The targets name in the kerberos database. default: hostname\n        :param bool gss_trust_dns:\n            Indicates whether or not the DNS is trusted to securely\n            canonicalize the name of the host being connected to (default\n            ``True``).\n        :param float banner_timeout: an optional timeout (in seconds) to wait\n            for the SSH banner to be presented.\n        :param float auth_timeout: an optional timeout (in seconds) to wait for\n            an authentication response.\n\n        :raises:\n            `.BadHostKeyException` -- if the server's host key could not be\n            verified\n        :raises: `.AuthenticationException` -- if authentication failed\n        :raises:\n            `.SSHException` -- if there was any other error connecting or\n            establishing an SSH session\n        :raises socket.error: if a socket error occurred while connecting\n\n        .. versionchanged:: 1.15\n            Added the ``banner_timeout``, ``gss_auth``, ``gss_kex``,\n            ``gss_deleg_creds`` and ``gss_host`` arguments.\n        .. versionchanged:: 2.3\n            Added the ``gss_trust_dns`` argument.\n        .. versionchanged:: 2.4\n            Added the ``passphrase`` argument.\n        \"\"\"\n        if not sock:\n            errors = {}\n            # Try multiple possible address families (e.g. IPv4 vs IPv6)\n            to_try = list(self._families_and_addresses(hostname, port))\n            for af, addr in to_try:\n                try:\n                    sock = socket.socket(af, socket.SOCK_STREAM)\n                    if timeout is not None:\n                        try:\n                            sock.settimeout(timeout)\n                        except:\n                            pass\n                    retry_on_signal(lambda: sock.connect(addr))\n                    # Break out of the loop on success\n                    break\n                except socket.error as e:\n                    # Raise anything that isn't a straight up connection error\n                    # (such as a resolution error)\n                    if e.errno not in (ECONNREFUSED, EHOSTUNREACH):\n                        raise\n                    # Capture anything else so we know how the run looks once\n                    # iteration is complete. Retain info about which attempt\n                    # this was.\n                    errors[addr] = e\n\n            # Make sure we explode usefully if no address family attempts\n            # succeeded. We've no way of knowing which error is the \"right\"\n            # one, so we construct a hybrid exception containing all the real\n            # ones, of a subclass that client code should still be watching for\n            # (socket.error)\n            if len(errors) == len(to_try):\n                raise NoValidConnectionsError(errors)\n\n        t = self._transport = Transport(\n            sock, gss_kex=gss_kex, gss_deleg_creds=gss_deleg_creds\n        )\n        t.use_compression(compress=compress)\n        t.set_gss_host(\n            # t.hostname may be None, but GSS-API requires a target name.\n            # Therefore use hostname as fallback.\n            gss_host=gss_host or hostname,\n            trust_dns=gss_trust_dns,\n            gssapi_requested=gss_auth or gss_kex,\n        )\n        if self._log_channel is not None:\n            t.set_log_channel(self._log_channel)\n        if banner_timeout is not None:\n            t.banner_timeout = banner_timeout\n        if auth_timeout is not None:\n            t.auth_timeout = auth_timeout\n\n        if port == SSH_PORT:\n            server_hostkey_name = hostname\n        else:\n            server_hostkey_name = \"[{}]:{}\".format(hostname, port)\n        our_server_keys = None\n\n        our_server_keys = self._system_host_keys.get(server_hostkey_name)\n        if our_server_keys is None:\n            our_server_keys = self._host_keys.get(server_hostkey_name)\n        if our_server_keys is not None:\n            keytype = our_server_keys.keys()[0]\n            sec_opts = t.get_security_options()\n            other_types = [x for x in sec_opts.key_types if x != keytype]\n            sec_opts.key_types = [keytype] + other_types\n\n        t.start_client(timeout=timeout)\n\n        # If GSS-API Key Exchange is performed we are not required to check the\n        # host key, because the host is authenticated via GSS-API / SSPI as\n        # well as our client.\n        if not self._transport.gss_kex_used:\n            server_key = t.get_remote_server_key()\n            if our_server_keys is None:\n                # will raise exception if the key is rejected\n                self._policy.missing_host_key(\n                    self, server_hostkey_name, server_key\n                )\n            else:\n                our_key = our_server_keys.get(server_key.get_name())\n                if our_key != server_key:\n                    if our_key is None:\n                        our_key = list(our_server_keys.values())[0]\n                    raise BadHostKeyException(hostname, server_key, our_key)\n\n        if username is None:\n            username = getpass.getuser()\n\n        if key_filename is None:\n            key_filenames = []\n        elif isinstance(key_filename, string_types):\n            key_filenames = [key_filename]\n        else:\n            key_filenames = key_filename\n\n        self._auth(\n            username,\n            password,\n            pkey,\n            key_filenames,\n            allow_agent,\n            look_for_keys,\n            gss_auth,\n            gss_kex,\n            gss_deleg_creds,\n            t.gss_host,\n            passphrase,\n        )", "response": "Connect to an SSH server and authenticate to it."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef close(self):\n        if self._transport is None:\n            return\n        self._transport.close()\n        self._transport = None\n\n        if self._agent is not None:\n            self._agent.close()\n            self._agent = None", "response": "Closes this SSHClient and its underlying. Transport and its underlying. Agent objects."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nexecute a command on the SSH server and return the output streams.", "response": "def exec_command(\n        self,\n        command,\n        bufsize=-1,\n        timeout=None,\n        get_pty=False,\n        environment=None,\n    ):\n        \"\"\"\n        Execute a command on the SSH server.  A new `.Channel` is opened and\n        the requested command is executed.  The command's input and output\n        streams are returned as Python ``file``-like objects representing\n        stdin, stdout, and stderr.\n\n        :param str command: the command to execute\n        :param int bufsize:\n            interpreted the same way as by the built-in ``file()`` function in\n            Python\n        :param int timeout:\n            set command's channel timeout. See `.Channel.settimeout`\n        :param bool get_pty:\n            Request a pseudo-terminal from the server (default ``False``).\n            See `.Channel.get_pty`\n        :param dict environment:\n            a dict of shell environment variables, to be merged into the\n            default environment that the remote command executes within.\n\n            .. warning::\n                Servers may silently reject some environment variables; see the\n                warning in `.Channel.set_environment_variable` for details.\n\n        :return:\n            the stdin, stdout, and stderr of the executing command, as a\n            3-tuple\n\n        :raises: `.SSHException` -- if the server fails to execute the command\n\n        .. versionchanged:: 1.10\n            Added the ``get_pty`` kwarg.\n        \"\"\"\n        chan = self._transport.open_session(timeout=timeout)\n        if get_pty:\n            chan.get_pty()\n        chan.settimeout(timeout)\n        if environment:\n            chan.update_environment(environment)\n        chan.exec_command(command)\n        stdin = chan.makefile(\"wb\", bufsize)\n        stdout = chan.makefile(\"r\", bufsize)\n        stderr = chan.makefile_stderr(\"r\", bufsize)\n        return stdin, stdout, stderr"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef invoke_shell(\n        self,\n        term=\"vt100\",\n        width=80,\n        height=24,\n        width_pixels=0,\n        height_pixels=0,\n        environment=None,\n    ):\n        \"\"\"\n        Start an interactive shell session on the SSH server.  A new `.Channel`\n        is opened and connected to a pseudo-terminal using the requested\n        terminal type and size.\n\n        :param str term:\n            the terminal type to emulate (for example, ``\"vt100\"``)\n        :param int width: the width (in characters) of the terminal window\n        :param int height: the height (in characters) of the terminal window\n        :param int width_pixels: the width (in pixels) of the terminal window\n        :param int height_pixels: the height (in pixels) of the terminal window\n        :param dict environment: the command's environment\n        :return: a new `.Channel` connected to the remote shell\n\n        :raises: `.SSHException` -- if the server fails to invoke a shell\n        \"\"\"\n        chan = self._transport.open_session()\n        chan.get_pty(term, width, height, width_pixels, height_pixels)\n        chan.invoke_shell()\n        return chan", "response": "Invoke a shell command on the remote server."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nattempting to derive a. PKey from a given file path.", "response": "def _key_from_filepath(self, filename, klass, password):\n        \"\"\"\n        Attempt to derive a `.PKey` from given string path ``filename``:\n\n        - If ``filename`` appears to be a cert, the matching private key is\n          loaded.\n        - Otherwise, the filename is assumed to be a private key, and the\n          matching public cert will be loaded if it exists.\n        \"\"\"\n        cert_suffix = \"-cert.pub\"\n        # Assume privkey, not cert, by default\n        if filename.endswith(cert_suffix):\n            key_path = filename[: -len(cert_suffix)]\n            cert_path = filename\n        else:\n            key_path = filename\n            cert_path = filename + cert_suffix\n        # Blindly try the key path; if no private key, nothing will work.\n        key = klass.from_private_key_file(key_path, password)\n        # TODO: change this to 'Loading' instead of 'Trying' sometime; probably\n        # when #387 is released, since this is a critical log message users are\n        # likely testing/filtering for (bah.)\n        msg = \"Trying discovered key {} in {}\".format(\n            hexlify(key.get_fingerprint()), key_path\n        )\n        self._log(DEBUG, msg)\n        # Attempt to load cert if it exists.\n        if os.path.isfile(cert_path):\n            key.load_certificate(cert_path)\n            self._log(DEBUG, \"Adding public certificate {}\".format(cert_path))\n        return key"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nauthenticating the user with the specified key.", "response": "def _auth(\n        self,\n        username,\n        password,\n        pkey,\n        key_filenames,\n        allow_agent,\n        look_for_keys,\n        gss_auth,\n        gss_kex,\n        gss_deleg_creds,\n        gss_host,\n        passphrase,\n    ):\n        \"\"\"\n        Try, in order:\n\n            - The key(s) passed in, if one was passed in.\n            - Any key we can find through an SSH agent (if allowed).\n            - Any \"id_rsa\", \"id_dsa\" or \"id_ecdsa\" key discoverable in ~/.ssh/\n              (if allowed).\n            - Plain username/password auth, if a password was given.\n\n        (The password might be needed to unlock a private key [if 'passphrase'\n        isn't also given], or for two-factor authentication [for which it is\n        required].)\n        \"\"\"\n        saved_exception = None\n        two_factor = False\n        allowed_types = set()\n        two_factor_types = {\"keyboard-interactive\", \"password\"}\n        if passphrase is None and password is not None:\n            passphrase = password\n\n        # If GSS-API support and GSS-PI Key Exchange was performed, we attempt\n        # authentication with gssapi-keyex.\n        if gss_kex and self._transport.gss_kex_used:\n            try:\n                self._transport.auth_gssapi_keyex(username)\n                return\n            except Exception as e:\n                saved_exception = e\n\n        # Try GSS-API authentication (gssapi-with-mic) only if GSS-API Key\n        # Exchange is not performed, because if we use GSS-API for the key\n        # exchange, there is already a fully established GSS-API context, so\n        # why should we do that again?\n        if gss_auth:\n            try:\n                return self._transport.auth_gssapi_with_mic(\n                    username, gss_host, gss_deleg_creds\n                )\n            except Exception as e:\n                saved_exception = e\n\n        if pkey is not None:\n            try:\n                self._log(\n                    DEBUG,\n                    \"Trying SSH key {}\".format(\n                        hexlify(pkey.get_fingerprint())\n                    ),\n                )\n                allowed_types = set(\n                    self._transport.auth_publickey(username, pkey)\n                )\n                two_factor = allowed_types & two_factor_types\n                if not two_factor:\n                    return\n            except SSHException as e:\n                saved_exception = e\n\n        if not two_factor:\n            for key_filename in key_filenames:\n                for pkey_class in (RSAKey, DSSKey, ECDSAKey, Ed25519Key):\n                    try:\n                        key = self._key_from_filepath(\n                            key_filename, pkey_class, passphrase\n                        )\n                        allowed_types = set(\n                            self._transport.auth_publickey(username, key)\n                        )\n                        two_factor = allowed_types & two_factor_types\n                        if not two_factor:\n                            return\n                        break\n                    except SSHException as e:\n                        saved_exception = e\n\n        if not two_factor and allow_agent:\n            if self._agent is None:\n                self._agent = Agent()\n\n            for key in self._agent.get_keys():\n                try:\n                    id_ = hexlify(key.get_fingerprint())\n                    self._log(DEBUG, \"Trying SSH agent key {}\".format(id_))\n                    # for 2-factor auth a successfully auth'd key password\n                    # will return an allowed 2fac auth method\n                    allowed_types = set(\n                        self._transport.auth_publickey(username, key)\n                    )\n                    two_factor = allowed_types & two_factor_types\n                    if not two_factor:\n                        return\n                    break\n                except SSHException as e:\n                    saved_exception = e\n\n        if not two_factor:\n            keyfiles = []\n\n            for keytype, name in [\n                (RSAKey, \"rsa\"),\n                (DSSKey, \"dsa\"),\n                (ECDSAKey, \"ecdsa\"),\n                (Ed25519Key, \"ed25519\"),\n            ]:\n                # ~/ssh/ is for windows\n                for directory in [\".ssh\", \"ssh\"]:\n                    full_path = os.path.expanduser(\n                        \"~/{}/id_{}\".format(directory, name)\n                    )\n                    if os.path.isfile(full_path):\n                        # TODO: only do this append if below did not run\n                        keyfiles.append((keytype, full_path))\n                        if os.path.isfile(full_path + \"-cert.pub\"):\n                            keyfiles.append((keytype, full_path + \"-cert.pub\"))\n\n            if not look_for_keys:\n                keyfiles = []\n\n            for pkey_class, filename in keyfiles:\n                try:\n                    key = self._key_from_filepath(\n                        filename, pkey_class, passphrase\n                    )\n                    # for 2-factor auth a successfully auth'd key will result\n                    # in ['password']\n                    allowed_types = set(\n                        self._transport.auth_publickey(username, key)\n                    )\n                    two_factor = allowed_types & two_factor_types\n                    if not two_factor:\n                        return\n                    break\n                except (SSHException, IOError) as e:\n                    saved_exception = e\n\n        if password is not None:\n            try:\n                self._transport.auth_password(username, password)\n                return\n            except SSHException as e:\n                saved_exception = e\n        elif two_factor:\n            try:\n                self._transport.auth_interactive_dumb(username)\n                return\n            except SSHException as e:\n                saved_exception = e\n\n        # if we got an auth-failed exception earlier, re-raise it\n        if saved_exception is not None:\n            raise saved_exception\n        raise SSHException(\"No authentication methods available\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef language(self, language):\n        if language is None:\n            raise ValueError(\"Invalid value for `language`, must not be `None`\")  # noqa: E501\n        allowed_values = [\"python\", \"r\", \"rmarkdown\"]  # noqa: E501\n        if language not in allowed_values:\n            raise ValueError(\n                \"Invalid value for `language` ({0}), must be one of {1}\"  # noqa: E501\n                .format(language, allowed_values)\n            )\n\n        self._language = language", "response": "Sets the language of this KernelPushRequest."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef kernel_type(self, kernel_type):\n        if kernel_type is None:\n            raise ValueError(\"Invalid value for `kernel_type`, must not be `None`\")  # noqa: E501\n        allowed_values = [\"script\", \"notebook\"]  # noqa: E501\n        if kernel_type not in allowed_values:\n            raise ValueError(\n                \"Invalid value for `kernel_type` ({0}), must be one of {1}\"  # noqa: E501\n                .format(kernel_type, allowed_values)\n            )\n\n        self._kernel_type = kernel_type", "response": "Sets the kernel_type of this KernelPushRequest."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndownload a competition leaderboard.", "response": "def competition_download_leaderboard(self, id, **kwargs):  # noqa: E501\n        \"\"\"Download competition leaderboard  # noqa: E501\n\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.competition_download_leaderboard(id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str id: Competition name (required)\n        :return: Result\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.competition_download_leaderboard_with_http_info(id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.competition_download_leaderboard_with_http_info(id, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nviews competition leaderboard # noqa: E501 This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass async_req=True >>> thread = api.competition_view_leaderboard(id, async_req=True) >>> result = thread.get() :param async_req bool :param str id: Competition name (required) :return: Result If the method is called asynchronously, returns the request thread.", "response": "def competition_view_leaderboard(self, id, **kwargs):  # noqa: E501\n        \"\"\"VIew competition leaderboard  # noqa: E501\n\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.competition_view_leaderboard(id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str id: Competition name (required)\n        :return: Result\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.competition_view_leaderboard_with_http_info(id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.competition_view_leaderboard_with_http_info(id, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef competitions_data_download_file(self, id, file_name, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.competitions_data_download_file_with_http_info(id, file_name, **kwargs)  # noqa: E501\n        else:\n            (data) = self.competitions_data_download_file_with_http_info(id, file_name, **kwargs)  # noqa: E501\n            return data", "response": "Download competition data file"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef competitions_data_list_files(self, id, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.competitions_data_list_files_with_http_info(id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.competitions_data_list_files_with_http_info(id, **kwargs)  # noqa: E501\n            return data", "response": "List competition data files."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlists competitions by category group and terms.", "response": "def competitions_list(self, **kwargs):  # noqa: E501\n        \"\"\"List competitions  # noqa: E501\n\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.competitions_list(async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str group: Filter competitions by a particular group\n        :param str category: Filter competitions by a particular category\n        :param str sort_by: Sort the results\n        :param int page: Page number\n        :param str search: Search terms\n        :return: Result\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.competitions_list_with_http_info(**kwargs)  # noqa: E501\n        else:\n            (data) = self.competitions_list_with_http_info(**kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef competitions_submissions_list(self, id, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.competitions_submissions_list_with_http_info(id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.competitions_submissions_list_with_http_info(id, **kwargs)  # noqa: E501\n            return data", "response": "List competition submissions  # noqa: E501\n\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.competitions_submissions_list(id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str id: Competition name (required)\n        :param int page: Page number\n        :return: Result\n                 If the method is called asynchronously,\n                 returns the request thread."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsubmit to competition # noqa: E501 This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass async_req=True >>> thread = api.competitions_submissions_submit(blob_file_tokens, submission_description, id, async_req=True) >>> result = thread.get() :param async_req bool :param str blob_file_tokens: Token identifying location of uploaded submission file (required) :param str submission_description: Description of competition submission (required) :param str id: Competition name (required) :return: Result If the method is called asynchronously, returns the request thread.", "response": "def competitions_submissions_submit(self, blob_file_tokens, submission_description, id, **kwargs):  # noqa: E501\n        \"\"\"Submit to competition  # noqa: E501\n\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.competitions_submissions_submit(blob_file_tokens, submission_description, id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str blob_file_tokens: Token identifying location of uploaded submission file (required)\n        :param str submission_description: Description of competition submission (required)\n        :param str id: Competition name (required)\n        :return: Result\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.competitions_submissions_submit_with_http_info(blob_file_tokens, submission_description, id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.competitions_submissions_submit_with_http_info(blob_file_tokens, submission_description, id, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nuploading a competition submission file to a specific location.", "response": "def competitions_submissions_upload(self, file, guid, content_length, last_modified_date_utc, **kwargs):  # noqa: E501\n        \"\"\"Upload competition submission file  # noqa: E501\n\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.competitions_submissions_upload(file, guid, content_length, last_modified_date_utc, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param file file: Competition submission file (required)\n        :param str guid: Location where submission should be uploaded (required)\n        :param int content_length: Content length of file in bytes (required)\n        :param int last_modified_date_utc: Last modified date of file in milliseconds since epoch in UTC (required)\n        :return: Result\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.competitions_submissions_upload_with_http_info(file, guid, content_length, last_modified_date_utc, **kwargs)  # noqa: E501\n        else:\n            (data) = self.competitions_submissions_upload_with_http_info(file, guid, content_length, last_modified_date_utc, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating a competition submission URL for a specific competition.", "response": "def competitions_submissions_url(self, id, content_length, last_modified_date_utc, **kwargs):  # noqa: E501\n        \"\"\"Generate competition submission URL  # noqa: E501\n\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.competitions_submissions_url(id, content_length, last_modified_date_utc, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str id: Competition name, as it appears in the competition's URL (required)\n        :param int content_length: Content length of file in bytes (required)\n        :param int last_modified_date_utc: Last modified date of file in milliseconds since epoch in UTC (required)\n        :param str file_name: Competition submission file name\n        :return: Result\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.competitions_submissions_url_with_http_info(id, content_length, last_modified_date_utc, **kwargs)  # noqa: E501\n        else:\n            (data) = self.competitions_submissions_url_with_http_info(id, content_length, last_modified_date_utc, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a new dataset", "response": "def datasets_create_new(self, dataset_new_request, **kwargs):  # noqa: E501\n        \"\"\"Create a new dataset  # noqa: E501\n\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.datasets_create_new(dataset_new_request, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param DatasetNewRequest dataset_new_request: Information for creating a new dataset (required)\n        :return: Result\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.datasets_create_new_with_http_info(dataset_new_request, **kwargs)  # noqa: E501\n        else:\n            (data) = self.datasets_create_new_with_http_info(dataset_new_request, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a new dataset version", "response": "def datasets_create_version(self, owner_slug, dataset_slug, dataset_new_version_request, **kwargs):  # noqa: E501\n        \"\"\"Create a new dataset version  # noqa: E501\n\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.datasets_create_version(owner_slug, dataset_slug, dataset_new_version_request, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str owner_slug: Dataset owner (required)\n        :param str dataset_slug: Dataset name (required)\n        :param DatasetNewVersionRequest dataset_new_version_request: Information for creating a new dataset version (required)\n        :return: Result\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.datasets_create_version_with_http_info(owner_slug, dataset_slug, dataset_new_version_request, **kwargs)  # noqa: E501\n        else:\n            (data) = self.datasets_create_version_with_http_info(owner_slug, dataset_slug, dataset_new_version_request, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a new dataset version by id", "response": "def datasets_create_version_by_id(self, id, dataset_new_version_request, **kwargs):  # noqa: E501\n        \"\"\"Create a new dataset version by id  # noqa: E501\n\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.datasets_create_version_by_id(id, dataset_new_version_request, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param int id: Dataset ID (required)\n        :param DatasetNewVersionRequest dataset_new_version_request: Information for creating a new dataset version (required)\n        :return: Result\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.datasets_create_version_by_id_with_http_info(id, dataset_new_version_request, **kwargs)  # noqa: E501\n        else:\n            (data) = self.datasets_create_version_by_id_with_http_info(id, dataset_new_version_request, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef datasets_download(self, owner_slug, dataset_slug, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.datasets_download_with_http_info(owner_slug, dataset_slug, **kwargs)  # noqa: E501\n        else:\n            (data) = self.datasets_download_with_http_info(owner_slug, dataset_slug, **kwargs)  # noqa: E501\n            return data", "response": "Download dataset file  # noqa: E501\n\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.datasets_download(owner_slug, dataset_slug, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str owner_slug: Dataset owner (required)\n        :param str dataset_slug: Dataset name (required)\n        :param str dataset_version_number: Dataset version number\n        :return: Result\n                 If the method is called asynchronously,\n                 returns the request thread."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef datasets_download_file(self, owner_slug, dataset_slug, file_name, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.datasets_download_file_with_http_info(owner_slug, dataset_slug, file_name, **kwargs)  # noqa: E501\n        else:\n            (data) = self.datasets_download_file_with_http_info(owner_slug, dataset_slug, file_name, **kwargs)  # noqa: E501\n            return data", "response": "Download a file from an existing dataset."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nlists datasets by a specific key.", "response": "def datasets_list(self, **kwargs):  # noqa: E501\n        \"\"\"List datasets  # noqa: E501\n\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.datasets_list(async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str group: Display datasets by a particular group\n        :param str sort_by: Sort the results\n        :param str size: Display datasets of a specific size\n        :param str filetype: Display datasets of a specific file type\n        :param str license: Display datasets with a specific license\n        :param str tagids: A comma separated list of tags to filter by\n        :param str search: Search terms\n        :param str user: Display datasets by a specific user or organization\n        :param int page: Page number\n        :return: Result\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.datasets_list_with_http_info(**kwargs)  # noqa: E501\n        else:\n            (data) = self.datasets_list_with_http_info(**kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlisting dataset files # noqa: E501 This method makes a synchronous HTTP request by default. To make an asynchronous HTTP request, please pass async_req=True >>> thread = api.datasets_list_files(owner_slug, dataset_slug, async_req=True) >>> result = thread.get() :param async_req bool :param str owner_slug: Dataset owner (required) :param str dataset_slug: Dataset name (required) :return: Result If the method is called asynchronously, returns the request thread.", "response": "def datasets_list_files(self, owner_slug, dataset_slug, **kwargs):  # noqa: E501\n        \"\"\"List dataset files  # noqa: E501\n\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.datasets_list_files(owner_slug, dataset_slug, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str owner_slug: Dataset owner (required)\n        :param str dataset_slug: Dataset name (required)\n        :return: Result\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.datasets_list_files_with_http_info(owner_slug, dataset_slug, **kwargs)  # noqa: E501\n        else:\n            (data) = self.datasets_list_files_with_http_info(owner_slug, dataset_slug, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget dataset creation status", "response": "def datasets_status(self, owner_slug, dataset_slug, **kwargs):  # noqa: E501\n        \"\"\"Get dataset creation status  # noqa: E501\n\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.datasets_status(owner_slug, dataset_slug, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str owner_slug: Dataset owner (required)\n        :param str dataset_slug: Dataset name (required)\n        :return: Result\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.datasets_status_with_http_info(owner_slug, dataset_slug, **kwargs)  # noqa: E501\n        else:\n            (data) = self.datasets_status_with_http_info(owner_slug, dataset_slug, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef datasets_upload_file(self, file_name, content_length, last_modified_date_utc, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.datasets_upload_file_with_http_info(file_name, content_length, last_modified_date_utc, **kwargs)  # noqa: E501\n        else:\n            (data) = self.datasets_upload_file_with_http_info(file_name, content_length, last_modified_date_utc, **kwargs)  # noqa: E501\n            return data", "response": "Uploads a file to the dataset store."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nshow details about a dataset", "response": "def datasets_view(self, owner_slug, dataset_slug, **kwargs):  # noqa: E501\n        \"\"\"Show details about a dataset  # noqa: E501\n\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.datasets_view(owner_slug, dataset_slug, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str owner_slug: Dataset owner (required)\n        :param str dataset_slug: Dataset name (required)\n        :return: Result\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.datasets_view_with_http_info(owner_slug, dataset_slug, **kwargs)  # noqa: E501\n        else:\n            (data) = self.datasets_view_with_http_info(owner_slug, dataset_slug, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndownloads the latest output from a kernel", "response": "def kernel_output(self, user_name, kernel_slug, **kwargs):  # noqa: E501\n        \"\"\"Download the latest output from a kernel  # noqa: E501\n\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.kernel_output(user_name, kernel_slug, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str user_name: Kernel owner (required)\n        :param str kernel_slug: Kernel name (required)\n        :return: Result\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.kernel_output_with_http_info(user_name, kernel_slug, **kwargs)  # noqa: E501\n        else:\n            (data) = self.kernel_output_with_http_info(user_name, kernel_slug, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npulls the latest code from a kernel", "response": "def kernel_pull(self, user_name, kernel_slug, **kwargs):  # noqa: E501\n        \"\"\"Pull the latest code from a kernel  # noqa: E501\n\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.kernel_pull(user_name, kernel_slug, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str user_name: Kernel owner (required)\n        :param str kernel_slug: Kernel name (required)\n        :return: Result\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.kernel_pull_with_http_info(user_name, kernel_slug, **kwargs)  # noqa: E501\n        else:\n            (data) = self.kernel_pull_with_http_info(user_name, kernel_slug, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npushing a new kernel version.", "response": "def kernel_push(self, kernel_push_request, **kwargs):  # noqa: E501\n        \"\"\"Push a new kernel version.  Can be used to create a new kernel and update an existing one.  # noqa: E501\n\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.kernel_push(kernel_push_request, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param KernelPushRequest kernel_push_request: Information for pushing a new kernel version (required)\n        :return: Result\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.kernel_push_with_http_info(kernel_push_request, **kwargs)  # noqa: E501\n        else:\n            (data) = self.kernel_push_with_http_info(kernel_push_request, **kwargs)  # noqa: E501\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef kernel_status(self, user_name, kernel_slug, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.kernel_status_with_http_info(user_name, kernel_slug, **kwargs)  # noqa: E501\n        else:\n            (data) = self.kernel_status_with_http_info(user_name, kernel_slug, **kwargs)  # noqa: E501\n            return data", "response": "Get the status of the latest kernel version"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef kernels_list(self, **kwargs):  # noqa: E501\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.kernels_list_with_http_info(**kwargs)  # noqa: E501\n        else:\n            (data) = self.kernels_list_with_http_info(**kwargs)  # noqa: E501\n            return data", "response": "List kernels in a specific language."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nauthenticates the user with the Kaggle API", "response": "def authenticate(self):\n        \"\"\"authenticate the user with the Kaggle API. This method will generate\n           a configuration, first checking the environment for credential\n           variables, and falling back to looking for the .kaggle/kaggle.json\n           configuration file.\n        \"\"\"\n\n        config_data = {}\n\n        # Step 1: try getting username/password from environment\n        config_data = self.read_config_environment(config_data)\n\n        # Step 2: if credentials were not in env read in configuration file\n        if self.CONFIG_NAME_USER not in config_data \\\n                or self.CONFIG_NAME_KEY not in config_data:\n            if os.path.exists(self.config):\n                config_data = self.read_config_file(config_data)\n            else:\n                raise IOError('Could not find {}. Make sure it\\'s located in'\n                              ' {}. Or use the environment method.'.format(\n                                  self.config_file, self.config_dir))\n\n        # Step 3: load into configuration!\n        self._load_config(config_data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef read_config_environment(self, config_data=None, quiet=False):\n\n        # Add all variables that start with KAGGLE_ to config data\n\n        if config_data is None:\n            config_data = {}\n        for key, val in os.environ.items():\n            if key.startswith('KAGGLE_'):\n                config_key = key.replace('KAGGLE_', '', 1).lower()\n                config_data[config_key] = val\n\n        return config_data", "response": "Read the Kaggle environment variables and return a config dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _load_config(self, config_data):\n        # Username and password are required.\n\n        for item in [self.CONFIG_NAME_USER, self.CONFIG_NAME_KEY]:\n            if item not in config_data:\n                raise ValueError('Error: Missing %s in configuration.' % item)\n\n        configuration = Configuration()\n\n        # Add to the final configuration (required)\n\n        configuration.username = config_data[self.CONFIG_NAME_USER]\n        configuration.password = config_data[self.CONFIG_NAME_KEY]\n\n        # Proxy\n\n        if self.CONFIG_NAME_PROXY in config_data:\n            configuration.proxy = config_data[self.CONFIG_NAME_PROXY]\n\n        # Cert File\n\n        if self.CONFIG_NAME_SSL_CA_CERT in config_data:\n            configuration.ssl_ca_cert = config_data[self.\n                                                    CONFIG_NAME_SSL_CA_CERT]\n\n        # Keep config values with class instance, and load api client!\n\n        self.config_values = config_data\n\n        try:\n            self.api_client = ApiClient(configuration)\n\n        except Exception as error:\n\n            if 'Proxy' in type(error).__name__:\n                raise ValueError(\n                    'The specified proxy ' +\n                    config_data[self.CONFIG_NAME_PROXY] +\n                    ' is not valid, please check your proxy settings')\n            else:\n                raise ValueError(\n                    'Unauthorized: you must download an API key or export '\n                    'credentials to the environment. Please see\\n ' +\n                    'https://github.com/Kaggle/kaggle-api#api-credentials ' +\n                    'for instructions.')", "response": "Load the configuration values from the given dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read_config_file(self, config_data=None, quiet=False):\n        if config_data is None:\n            config_data = {}\n\n        if os.path.exists(self.config):\n\n            try:\n                if os.name != 'nt':\n                    permissions = os.stat(self.config).st_mode\n                    if (permissions & 4) or (permissions & 32):\n                        print(\n                            'Warning: Your Kaggle API key is readable by other '\n                            'users on this system! To fix this, you can run ' +\n                            '\\'chmod 600 {}\\''.format(self.config))\n\n                with open(self.config) as f:\n                    config_data = json.load(f)\n            except:\n                pass\n\n        else:\n\n            # Warn the user that configuration will be reliant on environment\n            if not quiet:\n                print('No Kaggle API config file found, will use environment.')\n\n        return config_data", "response": "Read the Kaggle API configuration file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _read_config_file(self):\n\n        try:\n            with open(self.config, 'r') as f:\n                config_data = json.load(f)\n        except FileNotFoundError:\n            config_data = {}\n\n        return config_data", "response": "read in the configuration file a json file defined at self. config"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwrites config data to file.", "response": "def _write_config_file(self, config_data, indent=2):\n        \"\"\"write config data to file.\n\n           Parameters\n           ==========\n           config_data: the Configuration object to save a username and\n                        password, if defined\n           indent: number of tab indentations to use when writing json\n        \"\"\"\n        with open(self.config, 'w') as f:\n            json.dump(config_data, f, indent=indent)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_default_download_dir(self, *subdirs):\n        # Look up value for key \"path\" in the config\n        path = self.get_config_value(self.CONFIG_NAME_PATH)\n\n        # If not set in config, default to present working directory\n        if path is None:\n            return os.getcwd()\n\n        return os.path.join(path, *subdirs)", "response": "Get the default download directory for a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef print_config_value(self, name, prefix='- ', separator=': '):\n\n        value_out = 'None'\n        if name in self.config_values and self.config_values[name] is not None:\n            value_out = self.config_values[name]\n        print(prefix + name + separator + value_out)", "response": "print a single configuration value based on a prefix and separator"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a list of Competition instances from the API", "response": "def competitions_list(self,\n                          group=None,\n                          category=None,\n                          sort_by=None,\n                          page=1,\n                          search=None):\n        \"\"\" make call to list competitions, format the response, and return\n            a list of Competition instances\n\n            Parameters\n            ==========\n\n            page: the page to return (default is 1)\n            search: a search term to use (default is empty string)\n            sort_by: how to sort the result, see valid_sort_by for options\n            category: category to filter result to\n            group: group to filter result to\n        \"\"\"\n        valid_groups = ['general', 'entered', 'inClass']\n        if group and group not in valid_groups:\n            raise ValueError('Invalid group specified. Valid options are ' +\n                             str(valid_groups))\n\n        valid_categories = [\n            'all', 'featured', 'research', 'recruitment', 'gettingStarted',\n            'masters', 'playground'\n        ]\n        if category and category not in valid_categories:\n            raise ValueError('Invalid category specified. Valid options are ' +\n                             str(valid_categories))\n\n        valid_sort_by = [\n            'grouped', 'prize', 'earliestDeadline', 'latestDeadline',\n            'numberOfTeams', 'recentlyCreated'\n        ]\n        if sort_by and sort_by not in valid_sort_by:\n            raise ValueError('Invalid sort_by specified. Valid options are ' +\n                             str(valid_sort_by))\n\n        competitions_list_result = self.process_response(\n            self.competitions_list_with_http_info(\n                group=group or '',\n                category=category or '',\n                sort_by=sort_by or '',\n                page=page,\n                search=search or ''))\n        return [Competition(c) for c in competitions_list_result]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef competitions_list_cli(self,\n                              group=None,\n                              category=None,\n                              sort_by=None,\n                              page=1,\n                              search=None,\n                              csv_display=False):\n        \"\"\" a wrapper for competitions_list for the client.\n\n            Parameters\n            ==========\n            group: group to filter result to\n            category: category to filter result to\n            sort_by: how to sort the result, see valid_sort_by for options\n            page: the page to return (default is 1)\n            search: a search term to use (default is empty string)\n            csv_display: if True, print comma separated values\n        \"\"\"\n        competitions = self.competitions_list(\n            group=group,\n            category=category,\n            sort_by=sort_by,\n            page=page,\n            search=search)\n        fields = [\n            'ref', 'deadline', 'category', 'reward', 'teamCount',\n            'userHasEntered'\n        ]\n        if competitions:\n            if csv_display:\n                self.print_csv(competitions, fields)\n            else:\n                self.print_table(competitions, fields)\n        else:\n            print('No competitions found')", "response": "a wrapper for competitions_list for the client."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsubmit a competition! Parameters ========== file_name: the competition metadata file message: the submission description competition: the competition name quiet: suppress verbose output (default is False)", "response": "def competition_submit(self, file_name, message, competition, quiet=False):\n        \"\"\" submit a competition!\n\n            Parameters\n            ==========\n            file_name: the competition metadata file\n            message: the submission description\n            competition: the competition name\n            quiet: suppress verbose output (default is False)\n        \"\"\"\n        if competition is None:\n            competition = self.get_config_value(self.CONFIG_NAME_COMPETITION)\n            if competition is not None and not quiet:\n                print('Using competition: ' + competition)\n\n        if competition is None:\n            raise ValueError('No competition specified')\n        else:\n            url_result = self.process_response(\n                self.competitions_submissions_url_with_http_info(\n                    id=competition,\n                    file_name=os.path.basename(file_name),\n                    content_length=os.path.getsize(file_name),\n                    last_modified_date_utc=int(os.path.getmtime(file_name))))\n\n            # Temporary while new worker is gradually turned on.  'isComplete'\n            # exists on the old DTO but not the new, so this is an hacky but\n            # easy solution to figure out which submission logic to use\n            if 'isComplete' in url_result:\n                # Old submissions path\n                url_result_list = url_result['createUrl'].split('/')\n                upload_result = self.process_response(\n                    self.competitions_submissions_upload_with_http_info(\n                        file=file_name,\n                        guid=url_result_list[-3],\n                        content_length=url_result_list[-2],\n                        last_modified_date_utc=url_result_list[-1]))\n                upload_result_token = upload_result['token']\n            else:\n                # New submissions path!\n                success = self.upload_complete(file_name,\n                                               url_result['createUrl'], quiet)\n                if not success:\n                    # Actual error is printed during upload_complete.  Not\n                    # ideal but changing would not be backwards compatible\n                    return \"Could not submit to competition\"\n\n                upload_result_token = url_result['token']\n\n            submit_result = self.process_response(\n                self.competitions_submissions_submit_with_http_info(\n                    id=competition,\n                    blob_file_tokens=upload_result_token,\n                    submission_description=message))\n            return SubmitResult(submit_result)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsubmits a competition using the client", "response": "def competition_submit_cli(self,\n                               file_name,\n                               message,\n                               competition,\n                               competition_opt=None,\n                               quiet=False):\n        \"\"\" submit a competition using the client. Arguments are same as for\n            competition_submit, except for extra arguments provided here.\n             Parameters\n            ==========\n            competition_opt: an alternative competition option provided by cli\n        \"\"\"\n        competition = competition or competition_opt\n        try:\n            submit_result = self.competition_submit(file_name, message,\n                                                    competition, quiet)\n        except ApiException as e:\n            if e.status == 404:\n                print('Could not find competition - please verify that you '\n                      'entered the correct competition ID and that the '\n                      'competition is still accepting submissions.')\n                return None\n            else:\n                raise e\n        return submit_result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef competition_submissions(self, competition):\n        submissions_result = self.process_response(\n            self.competitions_submissions_list_with_http_info(id=competition))\n        return [Submission(s) for s in submissions_result]", "response": "get the list of Submission for a particular competition"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrap to competition_submission, will return either json or csv to the user. Additional parameters are listed below, see competition_submissions for rest. Parameters ========== competition: the name of the competition. If None, look to config competition_opt: an alternative competition option provided by cli csv_display: if True, print comma separated values quiet: suppress verbose output (default is False)", "response": "def competition_submissions_cli(self,\n                                    competition=None,\n                                    competition_opt=None,\n                                    csv_display=False,\n                                    quiet=False):\n        \"\"\" wrapper to competition_submission, will return either json or csv\n            to the user. Additional parameters are listed below, see\n            competition_submissions for rest.\n\n            Parameters\n            ==========\n            competition: the name of the competition. If None, look to config\n            competition_opt: an alternative competition option provided by cli\n            csv_display: if True, print comma separated values\n            quiet: suppress verbose output (default is False)\n        \"\"\"\n        competition = competition or competition_opt\n        if competition is None:\n            competition = self.get_config_value(self.CONFIG_NAME_COMPETITION)\n            if competition is not None and not quiet:\n                print('Using competition: ' + competition)\n\n        if competition is None:\n            raise ValueError('No competition specified')\n        else:\n            submissions = self.competition_submissions(competition)\n            fields = [\n                'fileName', 'date', 'description', 'status', 'publicScore',\n                'privateScore'\n            ]\n            if submissions:\n                if csv_display:\n                    self.print_csv(submissions, fields)\n                else:\n                    self.print_table(submissions, fields)\n            else:\n                print('No submissions found')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef competition_list_files(self, competition):\n        competition_list_files_result = self.process_response(\n            self.competitions_data_list_files_with_http_info(id=competition))\n        return [File(f) for f in competition_list_files_result]", "response": "list files for a specific competition"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlisting files for a competition, if it exists Parameters ========== competition: the name of the competition. If None, look to config competition_opt: an alternative competition option provided by cli csv_display: if True, print comma separated values quiet: suppress verbose output (default is False)", "response": "def competition_list_files_cli(self,\n                                   competition,\n                                   competition_opt=None,\n                                   csv_display=False,\n                                   quiet=False):\n        \"\"\" List files for a competition, if it exists\n\n            Parameters\n            ==========\n            competition: the name of the competition. If None, look to config\n            competition_opt: an alternative competition option provided by cli\n            csv_display: if True, print comma separated values\n            quiet: suppress verbose output (default is False)\n        \"\"\"\n        competition = competition or competition_opt\n        if competition is None:\n            competition = self.get_config_value(self.CONFIG_NAME_COMPETITION)\n            if competition is not None and not quiet:\n                print('Using competition: ' + competition)\n\n        if competition is None:\n            raise ValueError('No competition specified')\n        else:\n            files = self.competition_list_files(competition)\n            fields = ['name', 'size', 'creationDate']\n            if files:\n                if csv_display:\n                    self.print_csv(files, fields)\n                else:\n                    self.print_table(files, fields)\n            else:\n                print('No files found')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef competition_download_file(self,\n                                  competition,\n                                  file_name,\n                                  path=None,\n                                  force=False,\n                                  quiet=False):\n        \"\"\" download a competition file to a designated location, or use\n            a default location\n\n            Paramters\n            =========\n            competition: the name of the competition\n            file_name: the configuration file name\n            path: a path to download the file to\n            force: force the download if the file already exists (default False)\n            quiet: suppress verbose output (default is False)\n        \"\"\"\n        if path is None:\n            effective_path = self.get_default_download_dir(\n                'competitions', competition)\n        else:\n            effective_path = path\n\n        response = self.process_response(\n            self.competitions_data_download_file_with_http_info(\n                id=competition, file_name=file_name, _preload_content=False))\n        url = response.retries.history[0].redirect_location.split('?')[0]\n        outfile = os.path.join(effective_path, url.split('/')[-1])\n\n        if force or self.download_needed(response, outfile, quiet):\n            self.download_file(response, outfile, quiet)", "response": "download a file from a specific competition"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef competition_download_files(self,\n                                   competition,\n                                   path=None,\n                                   force=False,\n                                   quiet=True):\n        \"\"\" a wrapper to competition_download_file to download all competition\n            files.\n\n            Parameters\n            =========\n            competition: the name of the competition\n            path: a path to download the file to\n            force: force the download if the file already exists (default False)\n            quiet: suppress verbose output (default is True)\n        \"\"\"\n        files = self.competition_list_files(competition)\n        if not files:\n            print('This competition does not have any available data files')\n        for file_name in files:\n            self.competition_download_file(competition, file_name.ref, path,\n                                           force, quiet)", "response": "a wrapper to competition_download_file to download all competition\n            files"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef competition_download_cli(self,\n                                 competition,\n                                 competition_opt=None,\n                                 file_name=None,\n                                 path=None,\n                                 force=False,\n                                 quiet=False):\n        \"\"\" a wrapper to competition_download_files, but first will parse input\n            from API client. Additional parameters are listed here, see\n            competition_download for remaining.\n\n            Parameters\n            =========\n            competition: the name of the competition\n            competition_opt: an alternative competition option provided by cli\n            file_name: the configuration file name\n            path: a path to download the file to\n            force: force the download if the file already exists (default False)\n            quiet: suppress verbose output (default is False)\n        \"\"\"\n        competition = competition or competition_opt\n        if competition is None:\n            competition = self.get_config_value(self.CONFIG_NAME_COMPETITION)\n            if competition is not None and not quiet:\n                print('Using competition: ' + competition)\n\n        if competition is None:\n            raise ValueError('No competition specified')\n        else:\n            if file_name is None:\n                self.competition_download_files(competition, path, force,\n                                                quiet)\n            else:\n                self.competition_download_file(competition, file_name, path,\n                                               force, quiet)", "response": "a wrapper to competition_download_files, but first will parse input\n            from API client. Additional parameters are listed here, see\n            competition_download for remaining.\n\n            Parameters\n            =========\n            competition: the name of the competition\n            competition_opt: an alternative competition option provided by cli\n            file_name: the configuration file name\n            path: a path to download the file to\n            force: force the download if the file already exists (default False)\n            quiet: suppress verbose output (default is False)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndownloading the leaderboards of a specific competition.", "response": "def competition_leaderboard_download(self, competition, path, quiet=True):\n        \"\"\" Download competition leaderboards\n\n            Parameters\n            =========\n            competition: the name of the competition\n            path: a path to download the file to\n            quiet: suppress verbose output (default is True)\n        \"\"\"\n        response = self.process_response(\n            self.competition_download_leaderboard_with_http_info(\n                competition, _preload_content=False))\n\n        if path is None:\n            effective_path = self.get_default_download_dir(\n                'competitions', competition)\n        else:\n            effective_path = path\n\n        file_name = competition + '.zip'\n        outfile = os.path.join(effective_path, file_name)\n        self.download_file(response, outfile, quiet)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef competition_leaderboard_view(self, competition):\n        result = self.process_response(\n            self.competition_view_leaderboard_with_http_info(competition))\n        return [LeaderboardEntry(e) for e in result['submissions']]", "response": "view a leaderboard based on a competition name"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef competition_leaderboard_cli(self,\n                                    competition,\n                                    competition_opt=None,\n                                    path=None,\n                                    view=False,\n                                    download=False,\n                                    csv_display=False,\n                                    quiet=False):\n        \"\"\" a wrapper for competition_leaderbord_view that will print the\n            results as a table or comma separated values\n\n            Parameters\n            ==========\n            competition: the competition name to view leadboard for\n            competition_opt: an alternative competition option provided by cli\n            path: a path to download to, if download is True\n            view: if True, show the results in the terminal as csv or table\n            download: if True, download the entire leaderboard\n            csv_display: if True, print comma separated values instead of table\n            quiet: suppress verbose output (default is False)\n        \"\"\"\n        competition = competition or competition_opt\n        if not view and not download:\n            raise ValueError('Either --show or --download must be specified')\n\n        if competition is None:\n            competition = self.get_config_value(self.CONFIG_NAME_COMPETITION)\n            if competition is not None and not quiet:\n                print('Using competition: ' + competition)\n\n        if competition is None:\n            raise ValueError('No competition specified')\n\n        if download:\n            self.competition_leaderboard_download(competition, path, quiet)\n\n        if view:\n            results = self.competition_leaderboard_view(competition)\n            fields = ['teamId', 'teamName', 'submissionDate', 'score']\n            if results:\n                if csv_display:\n                    self.print_csv(results, fields)\n                else:\n                    self.print_table(results, fields)\n            else:\n                print('No results found')", "response": "a wrapper for competition_leaderboard_view that will print the leaderboards for the current competition"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dataset_list(self,\n                     sort_by=None,\n                     size=None,\n                     file_type=None,\n                     license_name=None,\n                     tag_ids=None,\n                     search=None,\n                     user=None,\n                     mine=False,\n                     page=1):\n        \"\"\" return a list of datasets!\n\n            Parameters\n            ==========\n            sort_by: how to sort the result, see valid_sort_bys for options\n            size: the size of the dataset, see valid_sizes for string options\n            file_type: the format, see valid_file_types for string options\n            license_name: string descriptor for license, see valid_license_names\n            tag_ids: tag identifiers to filter the search\n            search: a search term to use (default is empty string)\n            user: username to filter the search to\n            mine: boolean if True, group is changed to \"my\" to return personal\n            page: the page to return (default is 1)\n        \"\"\"\n        valid_sort_bys = ['hottest', 'votes', 'updated', 'active', 'published']\n        if sort_by and sort_by not in valid_sort_bys:\n            raise ValueError('Invalid sort by specified. Valid options are ' +\n                             str(valid_sort_bys))\n\n        valid_sizes = ['all', 'small', 'medium', 'large']\n        if size and size not in valid_sizes:\n            raise ValueError('Invalid size specified. Valid options are ' +\n                             str(valid_sizes))\n\n        valid_file_types = ['all', 'csv', 'sqlite', 'json', 'bigQuery']\n        if file_type and file_type not in valid_file_types:\n            raise ValueError('Invalid file type specified. Valid options are '\n                             + str(valid_file_types))\n\n        valid_license_names = ['all', 'cc', 'gpl', 'odb', 'other']\n        if license_name and license_name not in valid_license_names:\n            raise ValueError('Invalid license specified. Valid options are ' +\n                             str(valid_license_names))\n\n        if int(page) <= 0:\n            raise ValueError('Page number must be >= 1')\n\n        group = 'public'\n        if mine:\n            group = 'my'\n            if user:\n                raise ValueError('Cannot specify both mine and a user')\n        if user:\n            group = 'user'\n\n        datasets_list_result = self.process_response(\n            self.datasets_list_with_http_info(\n                group=group,\n                sort_by=sort_by or 'hottest',\n                size=size or 'all',\n                filetype=file_type or 'all',\n                license=license_name or 'all',\n                tagids=tag_ids or '',\n                search=search or '',\n                user=user or '',\n                page=page))\n        return [Dataset(d) for d in datasets_list_result]", "response": "returns a list of datasets in the national order"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dataset_list_cli(self,\n                         sort_by=None,\n                         size=None,\n                         file_type=None,\n                         license_name=None,\n                         tag_ids=None,\n                         search=None,\n                         user=None,\n                         mine=False,\n                         page=1,\n                         csv_display=False):\n        \"\"\" a wrapper to datasets_list for the client. Additional parameters\n            are described here, see dataset_list for others.\n\n            Parameters\n            ==========\n            sort_by: how to sort the result, see valid_sort_bys for options\n            size: the size of the dataset, see valid_sizes for string options\n            file_type: the format, see valid_file_types for string options\n            license_name: string descriptor for license, see valid_license_names\n            tag_ids: tag identifiers to filter the search\n            search: a search term to use (default is empty string)\n            user: username to filter the search to\n            mine: boolean if True, group is changed to \"my\" to return personal\n            page: the page to return (default is 1)\n            csv_display: if True, print comma separated values instead of table\n        \"\"\"\n        datasets = self.dataset_list(sort_by, size, file_type, license_name,\n                                     tag_ids, search, user, mine, page)\n        fields = ['ref', 'title', 'size', 'lastUpdated', 'downloadCount']\n        if datasets:\n            if csv_display:\n                self.print_csv(datasets, fields)\n            else:\n                self.print_table(datasets, fields)\n        else:\n            print('No datasets found')", "response": "a wrapper to dataset_list for the client."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nviewing metadata for a dataset", "response": "def dataset_view(self, dataset):\n        \"\"\" view metadata for a dataset.\n\n            Parameters\n            ==========\n            dataset: the string identified of the dataset\n                     should be in format [owner]/[dataset-name]\n        \"\"\"\n        if '/' in dataset:\n            self.validate_dataset_string(dataset)\n            dataset_urls = dataset.split('/')\n            owner_slug = dataset_urls[0]\n            dataset_slug = dataset_urls[1]\n        else:\n            owner_slug = self.get_config_value(self.CONFIG_NAME_USER)\n            dataset_slug = dataset\n\n        result = self.process_response(\n            self.datasets_view_with_http_info(owner_slug, dataset_slug))\n        return Dataset(result)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dataset_list_files(self, dataset):\n        if dataset is None:\n            raise ValueError('A dataset must be specified')\n        if '/' in dataset:\n            self.validate_dataset_string(dataset)\n            dataset_urls = dataset.split('/')\n            owner_slug = dataset_urls[0]\n            dataset_slug = dataset_urls[1]\n        else:\n            owner_slug = self.get_config_value(self.CONFIG_NAME_USER)\n            dataset_slug = dataset\n        dataset_list_files_result = self.process_response(\n            self.datasets_list_files_with_http_info(\n                owner_slug=owner_slug, dataset_slug=dataset_slug))\n        return ListFilesResult(dataset_list_files_result)", "response": "list files for a dataset"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dataset_status(self, dataset):\n        if dataset is None:\n            raise ValueError('A dataset must be specified')\n        if '/' in dataset:\n            self.validate_dataset_string(dataset)\n            dataset_urls = dataset.split('/')\n            owner_slug = dataset_urls[0]\n            dataset_slug = dataset_urls[1]\n        else:\n            owner_slug = self.get_config_value(self.CONFIG_NAME_USER)\n            dataset_slug = dataset\n        dataset_status_result = self.process_response(\n            self.datasets_status_with_http_info(\n                owner_slug=owner_slug, dataset_slug=dataset_slug))\n        return dataset_status_result", "response": "get the status of a dataset from the API\n            "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dataset_status_cli(self, dataset, dataset_opt=None):\n        dataset = dataset or dataset_opt\n        return self.dataset_status(dataset)", "response": "wrapper for dataset_status with additional\n            dataset_opt to get the status of a dataset from the API\n            "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndownloading a single file for a dataset", "response": "def dataset_download_file(self,\n                              dataset,\n                              file_name,\n                              path=None,\n                              force=False,\n                              quiet=True):\n        \"\"\" download a single file for a dataset\n\n            Parameters\n            ==========\n            dataset: the string identified of the dataset\n                     should be in format [owner]/[dataset-name]\n            file_name: the dataset configuration file\n            path: if defined, download to this location\n            force: force the download if the file already exists (default False)\n            quiet: suppress verbose output (default is True)\n        \"\"\"\n        if '/' in dataset:\n            self.validate_dataset_string(dataset)\n            dataset_urls = dataset.split('/')\n            owner_slug = dataset_urls[0]\n            dataset_slug = dataset_urls[1]\n        else:\n            owner_slug = self.get_config_value(self.CONFIG_NAME_USER)\n            dataset_slug = dataset\n\n        if path is None:\n            effective_path = self.get_default_download_dir(\n                'datasets', owner_slug, dataset_slug)\n        else:\n            effective_path = path\n\n        response = self.process_response(\n            self.datasets_download_file_with_http_info(\n                owner_slug=owner_slug,\n                dataset_slug=dataset_slug,\n                file_name=file_name,\n                _preload_content=False))\n        url = response.retries.history[0].redirect_location.split('?')[0]\n        outfile = os.path.join(effective_path, url.split('/')[-1])\n        if force or self.download_needed(response, outfile, quiet):\n            self.download_file(response, outfile, quiet)\n            return True\n        else:\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dataset_download_files(self,\n                               dataset,\n                               path=None,\n                               force=False,\n                               quiet=True,\n                               unzip=False):\n        \"\"\" download all files for a dataset\n\n            Parameters\n            ==========\n            dataset: the string identified of the dataset\n                     should be in format [owner]/[dataset-name]\n            path: the path to download the dataset to\n            force: force the download if the file already exists (default False)\n            quiet: suppress verbose output (default is True)\n            unzip: if True, unzip files upon download (default is False)\n        \"\"\"\n        if dataset is None:\n            raise ValueError('A dataset must be specified')\n        if '/' in dataset:\n            self.validate_dataset_string(dataset)\n            dataset_urls = dataset.split('/')\n            owner_slug = dataset_urls[0]\n            dataset_slug = dataset_urls[1]\n        else:\n            owner_slug = self.get_config_value(self.CONFIG_NAME_USER)\n            dataset_slug = dataset\n\n        if path is None:\n            effective_path = self.get_default_download_dir(\n                'datasets', owner_slug, dataset_slug)\n        else:\n            effective_path = path\n\n        response = self.process_response(\n            self.datasets_download_with_http_info(\n                owner_slug=owner_slug,\n                dataset_slug=dataset_slug,\n                _preload_content=False))\n\n        outfile = os.path.join(effective_path, dataset_slug + '.zip')\n        if force or self.download_needed(response, outfile, quiet):\n            self.download_file(response, outfile, quiet)\n            downloaded = True\n        else:\n            downloaded = False\n\n        if downloaded:\n            outfile = os.path.join(effective_path, dataset_slug + '.zip')\n            if unzip:\n                try:\n                    with zipfile.ZipFile(outfile) as z:\n                        z.extractall(effective_path)\n                except zipfile.BadZipFile as e:\n                    raise ValueError(\n                        'Bad zip file, please report on '\n                        'www.github.com/kaggle/kaggle-api', e)\n\n                try:\n                    os.remove(outfile)\n                except OSError as e:\n                    print('Could not delete zip file, got %s' % e)", "response": "download all files for a dataset"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nuploading a dataset file to the ACS Azure CARTO server", "response": "def dataset_upload_file(self, path, quiet):\n        \"\"\" upload a dataset file\n\n            Parameters\n            ==========\n            path: the complete path to upload\n            quiet: suppress verbose output (default is False)\n        \"\"\"\n        file_name = os.path.basename(path)\n        content_length = os.path.getsize(path)\n        last_modified_date_utc = int(os.path.getmtime(path))\n        result = FileUploadInfo(\n            self.process_response(\n                self.datasets_upload_file_with_http_info(\n                    file_name, content_length, last_modified_date_utc)))\n\n        success = self.upload_complete(path, result.createUrl, quiet)\n\n        if success:\n            return result.token\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dataset_create_version(self,\n                               folder,\n                               version_notes,\n                               quiet=False,\n                               convert_to_csv=True,\n                               delete_old_versions=False,\n                               dir_mode='skip'):\n        \"\"\" create a version of a dataset\n\n            Parameters\n            ==========\n            folder: the folder with the dataset configuration / data files\n            version_notes: notes to add for the version\n            quiet: suppress verbose output (default is False)\n            convert_to_csv: on upload, if data should be converted to csv\n            delete_old_versions: if True, do that (default False)\n            dir_mode: What to do with directories: \"skip\" - ignore; \"zip\" - compress and upload\n        \"\"\"\n        if not os.path.isdir(folder):\n            raise ValueError('Invalid folder: ' + folder)\n\n        meta_file = self.get_dataset_metadata_file(folder)\n\n        # read json\n        with open(meta_file) as f:\n            meta_data = json.load(f)\n        ref = self.get_or_default(meta_data, 'id', None)\n        id_no = self.get_or_default(meta_data, 'id_no', None)\n        if not ref and not id_no:\n            raise ValueError('ID or slug must be specified in the metadata')\n\n        subtitle = meta_data.get('subtitle')\n        if subtitle and (len(subtitle) < 20 or len(subtitle) > 80):\n            raise ValueError(\n                'Subtitle length must be between 20 and 80 characters')\n        resources = meta_data.get('resources')\n        if resources:\n            self.validate_resources(folder, resources)\n\n        description = meta_data.get('description')\n        keywords = self.get_or_default(meta_data, 'keywords', [])\n\n        request = DatasetNewVersionRequest(\n            version_notes=version_notes,\n            subtitle=subtitle,\n            description=description,\n            files=[],\n            convert_to_csv=convert_to_csv,\n            category_ids=keywords,\n            delete_old_versions=delete_old_versions)\n        self.upload_files(request, resources, folder, quiet, dir_mode)\n\n        if id_no:\n            result = DatasetNewVersionResponse(\n                self.process_response(\n                    self.datasets_create_version_by_id_with_http_info(\n                        id_no, request)))\n        else:\n            if ref == self.config_values[\n                    self.CONFIG_NAME_USER] + '/INSERT_SLUG_HERE':\n                raise ValueError(\n                    'Default slug detected, please change values before '\n                    'uploading')\n            self.validate_dataset_string(ref)\n            ref_list = ref.split('/')\n            owner_slug = ref_list[0]\n            dataset_slug = ref_list[1]\n            result = DatasetNewVersionResponse(\n                self.process_response(\n                    self.datasets_create_version_with_http_info(\n                        owner_slug, dataset_slug, request)))\n\n        return result", "response": "Create a new version of a dataset."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dataset_create_version_cli(self,\n                                   folder,\n                                   version_notes,\n                                   quiet=False,\n                                   convert_to_csv=True,\n                                   delete_old_versions=False,\n                                   dir_mode='skip'):\n        \"\"\" client wrapper for creating a version of a dataset\n             Parameters\n            ==========\n            folder: the folder with the dataset configuration / data files\n            version_notes: notes to add for the version\n            quiet: suppress verbose output (default is False)\n            convert_to_csv: on upload, if data should be converted to csv\n            delete_old_versions: if True, do that (default False)\n            dir_mode: What to do with directories: \"skip\" - ignore; \"zip\" - compress and upload\n        \"\"\"\n        folder = folder or os.getcwd()\n        result = self.dataset_create_version(\n            folder,\n            version_notes,\n            quiet=quiet,\n            convert_to_csv=convert_to_csv,\n            delete_old_versions=delete_old_versions,\n            dir_mode=dir_mode)\n        if result.invalidTags:\n            print(\n                ('The following are not valid tags and could not be added to '\n                 'the dataset: ') + str(result.invalidTags))\n        if result is None:\n            print('Dataset version creation error: See previous output')\n        elif result.status.lower() == 'ok':\n            print('Dataset version is being created. Please check progress at '\n                  + result.url)\n        else:\n            print('Dataset version creation error: ' + result.error)", "response": "This function is a wrapper for the dataset_create_version function. It is used by the dataset_create_version function in the base class."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dataset_initialize(self, folder):\n        if not os.path.isdir(folder):\n            raise ValueError('Invalid folder: ' + folder)\n\n        ref = self.config_values[self.CONFIG_NAME_USER] + '/INSERT_SLUG_HERE'\n        licenses = []\n        default_license = {'name': 'CC0-1.0'}\n        licenses.append(default_license)\n\n        meta_data = {\n            'title': 'INSERT_TITLE_HERE',\n            'id': ref,\n            'licenses': licenses\n        }\n        meta_file = os.path.join(folder, self.DATASET_METADATA_FILE)\n        with open(meta_file, 'w') as f:\n            json.dump(meta_data, f, indent=2)\n\n        print('Data package template written to: ' + meta_file)\n        return meta_file", "response": "initialize a folder with a dataset configuration file"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef dataset_create_new(self,\n                           folder,\n                           public=False,\n                           quiet=False,\n                           convert_to_csv=True,\n                           dir_mode='skip'):\n        \"\"\" create a new dataset, meaning the same as creating a version but\n            with extra metadata like license and user/owner.\n             Parameters\n            ==========\n            folder: the folder to initialize the metadata file in\n            public: should the dataset be public?\n            quiet: suppress verbose output (default is False)\n            convert_to_csv: if True, convert data to comma separated value\n            dir_mode: What to do with directories: \"skip\" - ignore; \"zip\" - compress and upload\n        \"\"\"\n        if not os.path.isdir(folder):\n            raise ValueError('Invalid folder: ' + folder)\n\n        meta_file = self.get_dataset_metadata_file(folder)\n\n        # read json\n        with open(meta_file) as f:\n            meta_data = json.load(f)\n        ref = self.get_or_fail(meta_data, 'id')\n        title = self.get_or_fail(meta_data, 'title')\n        licenses = self.get_or_fail(meta_data, 'licenses')\n        ref_list = ref.split('/')\n        owner_slug = ref_list[0]\n        dataset_slug = ref_list[1]\n\n        # validations\n        if ref == self.config_values[self.\n                                     CONFIG_NAME_USER] + '/INSERT_SLUG_HERE':\n            raise ValueError(\n                'Default slug detected, please change values before uploading')\n        if title == 'INSERT_TITLE_HERE':\n            raise ValueError(\n                'Default title detected, please change values before uploading'\n            )\n        if len(licenses) != 1:\n            raise ValueError('Please specify exactly one license')\n        if len(dataset_slug) < 6 or len(dataset_slug) > 50:\n            raise ValueError(\n                'The dataset slug must be between 6 and 50 characters')\n        if len(title) < 6 or len(title) > 50:\n            raise ValueError(\n                'The dataset title must be between 6 and 50 characters')\n        resources = meta_data.get('resources')\n        if resources:\n            self.validate_resources(folder, resources)\n\n        license_name = self.get_or_fail(licenses[0], 'name')\n        description = meta_data.get('description')\n        keywords = self.get_or_default(meta_data, 'keywords', [])\n\n        subtitle = meta_data.get('subtitle')\n        if subtitle and (len(subtitle) < 20 or len(subtitle) > 80):\n            raise ValueError(\n                'Subtitle length must be between 20 and 80 characters')\n\n        request = DatasetNewRequest(\n            title=title,\n            slug=dataset_slug,\n            owner_slug=owner_slug,\n            license_name=license_name,\n            subtitle=subtitle,\n            description=description,\n            files=[],\n            is_private=not public,\n            convert_to_csv=convert_to_csv,\n            category_ids=keywords)\n        resources = meta_data.get('resources')\n        self.upload_files(request, resources, folder, quiet, dir_mode)\n        result = DatasetNewResponse(\n            self.process_response(\n                self.datasets_create_new_with_http_info(request)))\n\n        return result", "response": "Create a new dataset in folder."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndownload a file to an output file based on a chunk size", "response": "def download_file(self, response, outfile, quiet=True, chunk_size=1048576):\n        \"\"\" download a file to an output file based on a chunk size\n\n            Parameters\n            ==========\n            response: the response to download\n            outfile: the output file to download to\n            quiet: suppress verbose output (default is True)\n            chunk_size: the size of the chunk to stream\n        \"\"\"\n\n        outpath = os.path.dirname(outfile)\n        if not os.path.exists(outpath):\n            os.makedirs(outpath)\n        size = int(response.headers['Content-Length'])\n        size_read = 0\n        if not quiet:\n            print('Downloading ' + os.path.basename(outfile) + ' to ' +\n                  outpath)\n        with tqdm(\n                total=size,\n                unit='B',\n                unit_scale=True,\n                unit_divisor=1024,\n                disable=quiet) as pbar:\n            with open(outfile, 'wb') as out:\n                while True:\n                    data = response.read(chunk_size)\n                    if not data:\n                        break\n                    out.write(data)\n                    size_read = min(size, size_read + chunk_size)\n                    pbar.update(len(data))\n            if not quiet:\n                print('\\n', end='')"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of kernels from the specified set of kernels", "response": "def kernels_list(self,\n                     page=1,\n                     page_size=20,\n                     dataset=None,\n                     competition=None,\n                     parent_kernel=None,\n                     search=None,\n                     mine=False,\n                     user=None,\n                     language=None,\n                     kernel_type=None,\n                     output_type=None,\n                     sort_by=None):\n        \"\"\" list kernels based on a set of search criteria\n\n            Parameters\n            ==========\n            page: the page of results to return (default is 1)\n            page_size: results per page (default is 20)\n            dataset: if defined, filter to this dataset (default None)\n            competition: if defined, filter to this competition (default None)\n            parent_kernel: if defined, filter to those with specified parent\n            search: a custom search string to pass to the list query\n            mine: if true, group is specified as \"my\" to return personal kernels\n            user: filter results to a specific user\n            language: the programming language of the kernel\n            kernel_type: the type of kernel, one of valid_kernel_types (str)\n            output_type: the output type, one of valid_output_types (str)\n            sort_by: if defined, sort results by this string (valid_sort_by)\n        \"\"\"\n        if int(page) <= 0:\n            raise ValueError('Page number must be >= 1')\n\n        page_size = int(page_size)\n        if page_size <= 0:\n            raise ValueError('Page size must be >= 1')\n        if page_size > 100:\n            page_size = 100\n\n        valid_languages = ['all', 'python', 'r', 'sqlite', 'julia']\n        if language and language not in valid_languages:\n            raise ValueError('Invalid language specified. Valid options are ' +\n                             str(valid_languages))\n\n        valid_kernel_types = ['all', 'script', 'notebook']\n        if kernel_type and kernel_type not in valid_kernel_types:\n            raise ValueError(\n                'Invalid kernel type specified. Valid options are ' +\n                str(valid_kernel_types))\n\n        valid_output_types = ['all', 'visualization', 'data']\n        if output_type and output_type not in valid_output_types:\n            raise ValueError(\n                'Invalid output type specified. Valid options are ' +\n                str(valid_output_types))\n\n        valid_sort_by = [\n            'hotness', 'commentCount', 'dateCreated', 'dateRun', 'relevance',\n            'scoreAscending', 'scoreDescending', 'viewCount', 'voteCount'\n        ]\n        if sort_by and sort_by not in valid_sort_by:\n            raise ValueError(\n                'Invalid sort by type specified. Valid options are ' +\n                str(valid_sort_by))\n        if sort_by == 'relevance' and search == '':\n            raise ValueError('Cannot sort by relevance without a search term.')\n\n        self.validate_dataset_string(dataset)\n        self.validate_kernel_string(parent_kernel)\n\n        group = 'everyone'\n        if mine:\n            group = 'profile'\n\n        kernels_list_result = self.process_response(\n            self.kernels_list_with_http_info(\n                page=page,\n                page_size=page_size,\n                group=group,\n                user=user or '',\n                language=language or 'all',\n                kernel_type=kernel_type or 'all',\n                output_type=output_type or 'all',\n                sort_by=sort_by or 'hotness',\n                dataset=dataset or '',\n                competition=competition or '',\n                parent_kernel=parent_kernel or '',\n                search=search or ''))\n        return [Kernel(k) for k in kernels_list_result]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef kernels_list_cli(self,\n                         mine=False,\n                         page=1,\n                         page_size=20,\n                         search=None,\n                         csv_display=False,\n                         parent=None,\n                         competition=None,\n                         dataset=None,\n                         user=None,\n                         language=None,\n                         kernel_type=None,\n                         output_type=None,\n                         sort_by=None):\n        \"\"\" client wrapper for kernels_list, see this function for arguments.\n            Additional arguments are provided here.\n             Parameters\n            ==========\n            csv_display: if True, print comma separated values instead of table\n        \"\"\"\n        kernels = self.kernels_list(\n            page=page,\n            page_size=page_size,\n            search=search,\n            mine=mine,\n            dataset=dataset,\n            competition=competition,\n            parent_kernel=parent,\n            user=user,\n            language=language,\n            kernel_type=kernel_type,\n            output_type=output_type,\n            sort_by=sort_by)\n        fields = ['ref', 'title', 'author', 'lastRunTime', 'totalVotes']\n        if kernels:\n            if csv_display:\n                self.print_csv(kernels, fields)\n            else:\n                self.print_table(kernels, fields)\n        else:\n            print('No kernels found')", "response": "This function is a wrapper for the kernels_list function."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a new kernel in a specified folder from the template.", "response": "def kernels_initialize(self, folder):\n        \"\"\" create a new kernel in a specified folder from template, including\n            json metadata that grabs values from the configuration.\n             Parameters\n            ==========\n            folder: the path of the folder\n        \"\"\"\n        if not os.path.isdir(folder):\n            raise ValueError('Invalid folder: ' + folder)\n\n        resources = []\n        resource = {'path': 'INSERT_SCRIPT_PATH_HERE'}\n        resources.append(resource)\n\n        username = self.get_config_value(self.CONFIG_NAME_USER)\n        meta_data = {\n            'id': username + '/INSERT_KERNEL_SLUG_HERE',\n            'title': 'INSERT_TITLE_HERE',\n            'code_file': 'INSERT_CODE_FILE_PATH_HERE',\n            'language': 'INSERT_LANGUAGE_HERE',\n            'kernel_type': 'INSERT_KERNEL_TYPE_HERE',\n            'is_private': 'true',\n            'enable_gpu': 'false',\n            'enable_internet': 'false',\n            'dataset_sources': [],\n            'competition_sources': [],\n            'kernel_sources': [],\n        }\n        meta_file = os.path.join(folder, self.KERNEL_METADATA_FILE)\n        with open(meta_file, 'w') as f:\n            json.dump(meta_data, f, indent=2)\n\n        return meta_file"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef kernels_push(self, folder):\n        if not os.path.isdir(folder):\n            raise ValueError('Invalid folder: ' + folder)\n\n        meta_file = os.path.join(folder, self.KERNEL_METADATA_FILE)\n        if not os.path.isfile(meta_file):\n            raise ValueError('Metadata file not found: ' +\n                             self.KERNEL_METADATA_FILE)\n\n        with open(meta_file) as f:\n            meta_data = json.load(f)\n\n        title = self.get_or_default(meta_data, 'title', None)\n        if title and len(title) < 5:\n            raise ValueError('Title must be at least five characters')\n\n        code_path = self.get_or_default(meta_data, 'code_file', '')\n        if not code_path:\n            raise ValueError('A source file must be specified in the metadata')\n\n        code_file = os.path.join(folder, code_path)\n        if not os.path.isfile(code_file):\n            raise ValueError('Source file not found: ' + code_file)\n\n        slug = meta_data.get('id')\n        id_no = meta_data.get('id_no')\n        if not slug and not id_no:\n            raise ValueError('ID or slug must be specified in the metadata')\n        if slug:\n            self.validate_kernel_string(slug)\n            if '/' in slug:\n                kernel_slug = slug.split('/')[1]\n            else:\n                kernel_slug = slug\n            if title:\n                as_slug = slugify(title)\n                if kernel_slug.lower() != as_slug:\n                    print(\n                        'Your kernel title does not resolve to the specified '\n                        'id. This may result in surprising behavior. We '\n                        'suggest making your title something that resolves to '\n                        'the specified id. See %s for more information on '\n                        'how slugs are determined.' %\n                        'https://en.wikipedia.org/wiki/Clean_URL#Slug')\n\n        valid_languages = ['python', 'r', 'rmarkdown']\n        language = self.get_or_default(meta_data, 'language', '')\n        if language not in valid_languages:\n            raise ValueError(\n                'A valid language must be specified in the metadata. Valid '\n                'options are ' + str(valid_languages))\n\n        valid_kernel_types = ['script', 'notebook']\n        kernel_type = self.get_or_default(meta_data, 'kernel_type', '')\n        if kernel_type not in valid_kernel_types:\n            raise ValueError(\n                'A valid kernel type must be specified in the metadata. Valid '\n                'options are ' + str(valid_kernel_types))\n\n        if kernel_type == 'notebook' and language == 'rmarkdown':\n            language = 'r'\n\n        dataset_sources = self.get_or_default(meta_data, 'dataset_sources', [])\n        for source in dataset_sources:\n            self.validate_dataset_string(source)\n\n        kernel_sources = self.get_or_default(meta_data, 'kernel_sources', [])\n        for source in kernel_sources:\n            self.validate_kernel_string(source)\n\n        with open(code_file) as f:\n            script_body = f.read()\n\n        if kernel_type == 'notebook':\n            json_body = json.loads(script_body)\n            if 'cells' in json_body:\n                for cell in json_body['cells']:\n                    if 'outputs' in cell and cell['cell_type'] == 'code':\n                        cell['outputs'] = []\n            script_body = json.dumps(json_body)\n\n        kernel_push_request = KernelPushRequest(\n            id=id_no,\n            slug=slug,\n            new_title=self.get_or_default(meta_data, 'title', None),\n            text=script_body,\n            language=language,\n            kernel_type=kernel_type,\n            is_private=self.get_or_default(meta_data, 'is_private', None),\n            enable_gpu=self.get_or_default(meta_data, 'enable_gpu', None),\n            enable_internet=self.get_or_default(meta_data, 'enable_internet',\n                                                None),\n            dataset_data_sources=dataset_sources,\n            competition_data_sources=self.get_or_default(\n                meta_data, 'competition_sources', []),\n            kernel_data_sources=kernel_sources,\n            category_ids=self.get_or_default(meta_data, 'keywords', []))\n\n        result = KernelPushResponse(\n            self.process_response(\n                self.kernel_push_with_http_info(\n                    kernel_push_request=kernel_push_request)))\n        return result", "response": "read the metadata file and kernel files from a notebook validate them and push them to Kaggle"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef kernels_pull(self, kernel, path, metadata=False, quiet=True):\n        existing_metadata = None\n        if kernel is None:\n            if path is None:\n                existing_metadata_path = os.path.join(\n                    os.getcwd(), self.KERNEL_METADATA_FILE)\n            else:\n                existing_metadata_path = os.path.join(\n                    path, self.KERNEL_METADATA_FILE)\n            if os.path.exists(existing_metadata_path):\n                with open(existing_metadata_path) as f:\n                    existing_metadata = json.load(f)\n                    kernel = existing_metadata['id']\n                    if 'INSERT_KERNEL_SLUG_HERE' in kernel:\n                        raise ValueError('A kernel must be specified')\n                    else:\n                        print('Using kernel ' + kernel)\n\n        if '/' in kernel:\n            self.validate_kernel_string(kernel)\n            kernel_url_list = kernel.split('/')\n            owner_slug = kernel_url_list[0]\n            kernel_slug = kernel_url_list[1]\n        else:\n            owner_slug = self.get_config_value(self.CONFIG_NAME_USER)\n            kernel_slug = kernel\n\n        if path is None:\n            effective_path = self.get_default_download_dir(\n                'kernels', owner_slug, kernel_slug)\n        else:\n            effective_path = path\n\n        if not os.path.exists(effective_path):\n            os.makedirs(effective_path)\n\n        response = self.process_response(\n            self.kernel_pull_with_http_info(owner_slug, kernel_slug))\n        blob = response['blob']\n\n        if os.path.isfile(effective_path):\n            effective_dir = os.path.dirname(effective_path)\n        else:\n            effective_dir = effective_path\n        metadata_path = os.path.join(effective_dir, self.KERNEL_METADATA_FILE)\n\n        if not os.path.isfile(effective_path):\n            language = blob['language'].lower()\n            kernel_type = blob['kernelType'].lower()\n\n            file_name = None\n            if existing_metadata:\n                file_name = existing_metadata['code_file']\n            elif os.path.isfile(metadata_path):\n                with open(metadata_path) as f:\n                    file_name = json.load(f)['code_file']\n\n            if not file_name or file_name == \"INSERT_CODE_FILE_PATH_HERE\":\n                extension = None\n                if kernel_type == 'script':\n                    if language == 'python':\n                        extension = '.py'\n                    elif language == 'r':\n                        extension = '.R'\n                    elif language == 'rmarkdown':\n                        extension = '.Rmd'\n                    elif language == 'sqlite':\n                        extension = '.sql'\n                    elif language == 'julia':\n                        extension = '.jl'\n                elif kernel_type == 'notebook':\n                    if language == 'python':\n                        extension = '.ipynb'\n                    elif language == 'r':\n                        extension = '.irnb'\n                    elif language == 'julia':\n                        extension = '.ijlnb'\n                file_name = blob['slug'] + extension\n\n            if file_name is None:\n                print(\n                    'Unknown language %s + kernel type %s - please report this '\n                    'on the kaggle-api github issues' % (language,\n                                                         kernel_type))\n                print(\n                    'Saving as a python file, even though this may not be the '\n                    'correct language')\n                file_name = 'script.py'\n            script_path = os.path.join(effective_path, file_name)\n        else:\n            script_path = effective_path\n            file_name = os.path.basename(effective_path)\n\n        with open(script_path, 'w') as f:\n            f.write(blob['source'])\n\n        if metadata:\n            data = {}\n\n            server_metadata = response['metadata']\n            data['id'] = server_metadata['ref']\n            data['id_no'] = server_metadata['id']\n            data['title'] = server_metadata['title']\n            data['code_file'] = file_name\n            data['language'] = server_metadata['language']\n            data['kernel_type'] = server_metadata['kernelType']\n            self.set_if_present(server_metadata, 'isPrivate', data,\n                                'is_private')\n            self.set_if_present(server_metadata, 'enableGpu', data,\n                                'enable_gpu')\n            self.set_if_present(server_metadata, 'enableInternet', data,\n                                'enable_internet')\n            self.set_if_present(server_metadata, 'categoryIds', data,\n                                'keywords')\n            self.set_if_present(server_metadata, 'datasetDataSources', data,\n                                'dataset_sources')\n            self.set_if_present(server_metadata, 'kernelDataSources', data,\n                                'kernel_sources')\n            self.set_if_present(server_metadata, 'competitionDataSources',\n                                data, 'competition_sources')\n            with open(metadata_path, 'w') as f:\n                json.dump(data, f, indent=2)\n\n            return effective_dir\n        else:\n            return script_path", "response": "Pull a kernel from the kernels directory"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretrieves output for a kernel", "response": "def kernels_output(self, kernel, path, force=False, quiet=True):\n        \"\"\" retrieve output for a specified kernel\n             Parameters\n            ==========\n            kernel: the kernel to output\n            path: the path to pull files to on the filesystem\n            force: if output already exists, force overwrite (default False)\n            quiet: suppress verbosity (default is True)\n        \"\"\"\n        if kernel is None:\n            raise ValueError('A kernel must be specified')\n        if '/' in kernel:\n            self.validate_kernel_string(kernel)\n            kernel_url_list = kernel.split('/')\n            owner_slug = kernel_url_list[0]\n            kernel_slug = kernel_url_list[1]\n        else:\n            owner_slug = self.get_config_value(self.CONFIG_NAME_USER)\n            kernel_slug = kernel\n\n        if path is None:\n            target_dir = self.get_default_download_dir('kernels', owner_slug,\n                                                       kernel_slug, 'output')\n        else:\n            target_dir = path\n\n        if not os.path.exists(target_dir):\n            os.makedirs(target_dir)\n\n        if not os.path.isdir(target_dir):\n            raise ValueError(\n                'You must specify a directory for the kernels output')\n\n        response = self.process_response(\n            self.kernel_output_with_http_info(owner_slug, kernel_slug))\n        outfiles = []\n        for item in response['files']:\n            outfile = os.path.join(target_dir, item['fileName'])\n            outfiles.append(outfile)\n            download_response = requests.get(item['url'])\n            if force or self.download_needed(item, outfile, quiet):\n                os.makedirs(os.path.split(outfile)[0], exist_ok=True)\n                with open(outfile, 'wb') as out:\n                    out.write(download_response.content)\n                if not quiet:\n                    print('Output file downloaded to %s' % outfile)\n\n        log = response['log']\n        if log:\n            outfile = os.path.join(target_dir, kernel_slug + '.log')\n            outfiles.append(outfile)\n            with open(outfile, 'w') as out:\n                out.write(log)\n            if not quiet:\n                print('Kernel log downloaded to %s ' % outfile)\n\n        return outfiles"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef kernels_output_cli(self,\n                           kernel,\n                           kernel_opt=None,\n                           path=None,\n                           force=False,\n                           quiet=False):\n        \"\"\" client wrapper for kernels_output, with same arguments. Extra\n            arguments are described below, and see kernels_output for others.\n             Parameters\n            ==========\n            kernel_opt: option from client instead of kernel, if not defined\n        \"\"\"\n        kernel = kernel or kernel_opt\n        self.kernels_output(kernel, path, force, quiet)", "response": "wrapper for kernels_output with same arguments."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef kernels_status(self, kernel):\n        if kernel is None:\n            raise ValueError('A kernel must be specified')\n        if '/' in kernel:\n            self.validate_kernel_string(kernel)\n            kernel_url_list = kernel.split('/')\n            owner_slug = kernel_url_list[0]\n            kernel_slug = kernel_url_list[1]\n        else:\n            owner_slug = self.get_config_value(self.CONFIG_NAME_USER)\n            kernel_slug = kernel\n        response = self.process_response(\n            self.kernel_status_with_http_info(owner_slug, kernel_slug))\n        return response", "response": "get the status of a kernel"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef kernels_status_cli(self, kernel, kernel_opt=None):\n        kernel = kernel or kernel_opt\n        response = self.kernels_status(kernel)\n        status = response['status']\n        message = response['failureMessage']\n        if message:\n            print('%s has status \"%s\"' % (kernel, status))\n            print('Failure message: \"%s\"' % message)\n        else:\n            print('%s has status \"%s\"' % (kernel, status))", "response": "client wrapper for kernel_status"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndetermine if a download is needed based on the timestamp of the remote API object. Return True if needed False otherwise.", "response": "def download_needed(self, response, outfile, quiet=True):\n        \"\"\" determine if a download is needed based on timestamp. Return True\n            if needed (remote is newer) or False if local is newest.\n             Parameters\n            ==========\n            response: the response from the API\n            outfile: the output file to write to\n            quiet: suppress verbose output (default is True)\n        \"\"\"\n        try:\n            remote_date = datetime.strptime(response.headers['Last-Modified'],\n                                            '%a, %d %b %Y %X %Z')\n            if isfile(outfile):\n                local_date = datetime.fromtimestamp(os.path.getmtime(outfile))\n                if remote_date <= local_date:\n                    if not quiet:\n                        print(os.path.basename(outfile) +\n                              ': Skipping, found more recently modified local '\n                              'copy (use --force to force download)')\n                    return False\n        except:\n            pass\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef print_table(self, items, fields):\n        formats = []\n        borders = []\n        for f in fields:\n            length = max(\n                len(f), max([len(self.string(getattr(i, f))) for i in items]))\n            justify = '>' if isinstance(getattr(\n                items[0], f), int) or f == 'size' or f == 'reward' else '<'\n            formats.append('{:' + justify + self.string(length + 2) + '}')\n            borders.append('-' * length + '  ')\n        row_format = u''.join(formats)\n        headers = [f + '  ' for f in fields]\n        print(row_format.format(*headers))\n        print(row_format.format(*borders))\n        for i in items:\n            i_fields = [self.string(getattr(i, f)) + '  ' for f in fields]\n            try:\n                print(row_format.format(*i_fields))\n            except UnicodeEncodeError:\n                print(row_format.format(*i_fields).encode('utf-8'))", "response": "print a table of items for a set of fields defined by items\n           "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprint a set of fields in a set of items using a csv. writer", "response": "def print_csv(self, items, fields):\n        \"\"\" print a set of fields in a set of items using a csv.writer\n\n            Parameters\n            ==========\n            items: a list of items to print\n            fields: a list of fields to select from items\n        \"\"\"\n        writer = csv.writer(sys.stdout)\n        writer.writerow(fields)\n        for i in items:\n            i_fields = [self.string(getattr(i, f)) for f in fields]\n            writer.writerow(i_fields)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprocess a response from the API", "response": "def process_response(self, result):\n        \"\"\" process a response from the API. We check the API version against\n            the client's to see if it's old, and give them a warning (once)\n\n            Parameters\n            ==========\n            result: the result from the API\n        \"\"\"\n        if len(result) == 3:\n            data = result[0]\n            headers = result[2]\n            if self.HEADER_API_VERSION in headers:\n                api_version = headers[self.HEADER_API_VERSION]\n                if (not self.already_printed_version_warning\n                        and not self.is_up_to_date(api_version)):\n                    print('Warning: Looks like you\\'re using an outdated API '\n                          'Version, please consider updating (server ' +\n                          api_version + ' / client ' + self.__version__ + ')')\n                    self.already_printed_version_warning = True\n            return data\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a boolean with True if the client is up to date with the version provided on the server.", "response": "def is_up_to_date(self, server_version):\n        \"\"\" determine if a client (on the local user's machine) is up to date\n            with the version provided on the server. Return a boolean with True\n            or False\n             Parameters\n            ==========\n            server_version: the server version string to compare to the host\n        \"\"\"\n        client_split = self.__version__.split('.')\n        client_len = len(client_split)\n        server_split = server_version.split('.')\n        server_len = len(server_split)\n\n        # Make both lists the same length\n        for i in range(client_len, server_len):\n            client_split.append('0')\n        for i in range(server_len, client_len):\n            server_split.append('0')\n\n        for i in range(0, client_len):\n            if 'b' in client_split[i]:\n                # Using a beta version, don't check\n                return True\n            client = int(client_split[i])\n            server = int(server_split[i])\n            if client < server:\n                return False\n            elif server < client:\n                return True\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nuploading files in a folder Parameters ========== request: the prepared request resources: the files to upload folder: the folder to upload from quiet: suppress verbose output (default is False)", "response": "def upload_files(self,\n                     request,\n                     resources,\n                     folder,\n                     quiet=False,\n                     dir_mode='skip'):\n        \"\"\" upload files in a folder\n             Parameters\n            ==========\n            request: the prepared request\n            resources: the files to upload\n            folder: the folder to upload from\n            quiet: suppress verbose output (default is False)\n        \"\"\"\n        for file_name in os.listdir(folder):\n            if (file_name == self.DATASET_METADATA_FILE\n                    or file_name == self.OLD_DATASET_METADATA_FILE\n                    or file_name == self.KERNEL_METADATA_FILE):\n                continue\n            full_path = os.path.join(folder, file_name)\n\n            if os.path.isfile(full_path):\n                exitcode = self._upload_file(file_name, full_path, quiet,\n                                             request, resources)\n                if exitcode:\n                    return\n            elif os.path.isdir(full_path):\n                if dir_mode in ['zip', 'tar']:\n                    temp_dir = tempfile.mkdtemp()\n                    try:\n                        _, dir_name = os.path.split(full_path)\n                        archive_path = shutil.make_archive(\n                            os.path.join(temp_dir, dir_name), dir_mode,\n                            full_path)\n                        _, archive_name = os.path.split(archive_path)\n                        exitcode = self._upload_file(archive_name,\n                                                     archive_path, quiet,\n                                                     request, resources)\n                    finally:\n                        shutil.rmtree(temp_dir)\n                    if exitcode:\n                        return\n                elif not quiet:\n                    print(\"Skipping folder: \" + file_name +\n                          \"; use '--dir-mode' to upload folders\")\n            else:\n                if not quiet:\n                    print('Skipping: ' + file_name)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef process_column(self, column):\n        processed_column = DatasetColumn(\n            name=self.get_or_fail(column, 'name'),\n            description=self.get_or_default(column, 'description', ''))\n        if 'type' in column:\n            original_type = column['type'].lower()\n            processed_column.original_type = original_type\n            if (original_type == 'string' or original_type == 'date'\n                    or original_type == 'time' or original_type == 'yearmonth'\n                    or original_type == 'duration'\n                    or original_type == 'geopoint'\n                    or original_type == 'geojson'):\n                processed_column.type = 'string'\n            elif (original_type == 'numeric' or original_type == 'number'\n                  or original_type == 'year'):\n                processed_column.type = 'numeric'\n            elif original_type == 'boolean':\n                processed_column.type = 'boolean'\n            elif original_type == 'datetime':\n                processed_column.type = 'datetime'\n            else:\n                # Possibly extended data type - not going to try to track those\n                # here. Will set the type and let the server handle it.\n                processed_column.type = original_type\n        return processed_column", "response": "process a column and return the processed version of the column"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfunction to complete an upload to retrieve a path from a url", "response": "def upload_complete(self, path, url, quiet):\n        \"\"\" function to complete an upload to retrieve a path from a url\n             Parameters\n            ==========\n            path: the path for the upload that is read in\n            url: the url to send the POST to\n            quiet: suppress verbose output (default is False)\n        \"\"\"\n        file_size = os.path.getsize(path)\n        try:\n            with tqdm(\n                    total=file_size,\n                    unit='B',\n                    unit_scale=True,\n                    unit_divisor=1024,\n                    disable=quiet) as progress_bar:\n                with io.open(path, 'rb', buffering=0) as fp:\n                    reader = TqdmBufferedReader(fp, progress_bar)\n                    session = requests.Session()\n                    retries = Retry(total=10, backoff_factor=0.5)\n                    adapter = HTTPAdapter(max_retries=retries)\n                    session.mount('http://', adapter)\n                    session.mount('https://', adapter)\n                    response = session.put(url, data=reader)\n        except Exception as error:\n            print(error)\n            return False\n        return response.status_code == 200 or response.status_code == 201"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef validate_dataset_string(self, dataset):\n        if dataset:\n            if '/' not in dataset:\n                raise ValueError('Dataset must be specified in the form of '\n                                 '\\'{username}/{dataset-slug}\\'')\n\n            split = dataset.split('/')\n            if not split[0] or not split[1]:\n                raise ValueError('Invalid dataset specification ' + dataset)", "response": "determine if a dataset string is valid"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef validate_kernel_string(self, kernel):\n        if kernel:\n            if '/' not in kernel:\n                raise ValueError('Kernel must be specified in the form of '\n                                 '\\'{username}/{kernel-slug}\\'')\n\n            split = kernel.split('/')\n            if not split[0] or not split[1]:\n                raise ValueError('Kernel must be specified in the form of '\n                                 '\\'{username}/{kernel-slug}\\'')\n\n            if len(split[1]) < 5:\n                raise ValueError(\n                    'Kernel slug must be at least five characters')", "response": "determine if a kernel string is valid"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef validate_resources(self, folder, resources):\n        self.validate_files_exist(folder, resources)\n        self.validate_no_duplicate_paths(resources)", "response": "validate the existence of files in folder and set of resources"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nensuring that one or more resource files exist in a folder", "response": "def validate_files_exist(self, folder, resources):\n        \"\"\" ensure that one or more resource files exist in a folder\n\n            Parameters\n            ==========\n            folder: the folder to validate\n            resources: one or more resources to validate within the folder\n        \"\"\"\n        for item in resources:\n            file_name = item.get('path')\n            full_path = os.path.join(folder, file_name)\n            if not os.path.isfile(full_path):\n                raise ValueError('%s does not exist' % full_path)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef validate_no_duplicate_paths(self, resources):\n        paths = set()\n        for item in resources:\n            file_name = item.get('path')\n            if file_name in paths:\n                raise ValueError(\n                    '%s path was specified more than once in the metadata' %\n                    file_name)\n            paths.add(file_name)", "response": "ensure that the user has not provided duplicate paths in\n            a list of resources."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts a set of file data to a metadata file at path", "response": "def convert_to_dataset_file_metadata(self, file_data, path):\n        \"\"\" convert a set of file_data to a metadata file at path\n\n            Parameters\n            ==========\n            file_data: a dictionary of file data to write to file\n            path: the path to write the metadata to\n        \"\"\"\n        as_metadata = {\n            'path': os.path.join(path, file_data['name']),\n            'description': file_data['description']\n        }\n\n        schema = {}\n        fields = []\n        for column in file_data['columns']:\n            field = {\n                'name': column['name'],\n                'title': column['description'],\n                'type': column['type']\n            }\n            fields.append(field)\n        schema['fields'] = fields\n        as_metadata['schema'] = schema\n\n        return as_metadata"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef read(self, *args, **kwargs):\n        buf = io.BufferedReader.read(self, *args, **kwargs)\n        self.increment(len(buf))\n        return buf", "response": "read the buffer passing named and non named arguments to the\n            io. BufferedReader function."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parameters_to_tuples(self, params, collection_formats):\n        new_params = []\n        if collection_formats is None:\n            collection_formats = {}\n        for k, v in six.iteritems(params) if isinstance(params, dict) else params:  # noqa: E501\n            if k in collection_formats:\n                collection_format = collection_formats[k]\n                if collection_format == 'multi':\n                    new_params.extend((k, value) for value in v)\n                else:\n                    if collection_format == 'ssv':\n                        delimiter = ' '\n                    elif collection_format == 'tsv':\n                        delimiter = '\\t'\n                    elif collection_format == 'pipes':\n                        delimiter = '|'\n                    else:  # csv is the default\n                        delimiter = ','\n                    new_params.append(\n                        (k, delimiter.join(str(value) for value in v)))\n            else:\n                new_params.append((k, v))\n        return new_params", "response": "Get parameters as list of tuples formatting collections.\nCOOKIE"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbuilds form parameters. :param post_params: Normal form parameters. :param files: File parameters. :return: Form parameters with files.", "response": "def prepare_post_parameters(self, post_params=None, files=None):\n        \"\"\"Builds form parameters.\n\n        :param post_params: Normal form parameters.\n        :param files: File parameters.\n        :return: Form parameters with files.\n        \"\"\"\n        params = []\n\n        if post_params:\n            params = post_params\n\n        if files:\n            for k, v in six.iteritems(files):\n                if not v:\n                    continue\n                file_names = v if type(v) is list else [v]\n                for n in file_names:\n                    with open(n, 'rb') as f:\n                        filename = os.path.basename(f.name)\n                        filedata = f.read()\n                        mimetype = (mimetypes.guess_type(filename)[0] or\n                                    'application/octet-stream')\n                        params.append(\n                            tuple([k, tuple([filename, filedata, mimetype])]))\n\n        return params"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef __deserialize_primitive(self, data, klass):\n        try:\n            return klass(data)\n        except UnicodeEncodeError:\n            return six.text_type(data)\n        except TypeError:\n            return data", "response": "Deserializes string to primitive type."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the logger file.", "response": "def logger_file(self, value):\n        \"\"\"The logger file.\n\n        If the logger_file is None, then add stream handler and remove file\n        handler. Otherwise, add file handler and remove stream handler.\n\n        :param value: The logger_file path.\n        :type: str\n        \"\"\"\n        self.__logger_file = value\n        if self.__logger_file:\n            # If set logging file,\n            # then add file handler and remove stream handler.\n            self.logger_file_handler = logging.FileHandler(self.__logger_file)\n            self.logger_file_handler.setFormatter(self.logger_formatter)\n            for _, logger in six.iteritems(self.logger):\n                logger.addHandler(self.logger_file_handler)\n                if self.logger_stream_handler:\n                    logger.removeHandler(self.logger_stream_handler)\n        else:\n            # If not set logging file,\n            # then add stream handler and remove file handler.\n            self.logger_stream_handler = logging.StreamHandler()\n            self.logger_stream_handler.setFormatter(self.logger_formatter)\n            for _, logger in six.iteritems(self.logger):\n                logger.addHandler(self.logger_stream_handler)\n                if self.logger_file_handler:\n                    logger.removeHandler(self.logger_file_handler)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the license_name of this DatasetNewRequest.", "response": "def license_name(self, license_name):\n        \"\"\"Sets the license_name of this DatasetNewRequest.\n\n        The license that should be associated with the dataset  # noqa: E501\n\n        :param license_name: The license_name of this DatasetNewRequest.  # noqa: E501\n        :type: str\n        \"\"\"\n        allowed_values = [\"CC0-1.0\", \"CC-BY-SA-4.0\", \"GPL-2.0\", \"ODbL-1.0\", \"CC-BY-NC-SA-4.0\", \"unknown\", \"DbCL-1.0\", \"CC-BY-SA-3.0\", \"copyright-authors\", \"other\", \"reddit-api\", \"world-bank\"]  # noqa: E501\n        if license_name not in allowed_values:\n            raise ValueError(\n                \"Invalid value for `license_name` ({0}), must be one of {1}\"  # noqa: E501\n                .format(license_name, allowed_values)\n            )\n\n        self._license_name = license_name"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef train(net, train_data, test_data):\n    start_pipeline_time = time.time()\n    net, trainer = text_cnn.init(net, vocab, args.model_mode, context, args.lr)\n    random.shuffle(train_data)\n    sp = int(len(train_data)*0.9)\n    train_dataloader = DataLoader(dataset=train_data[:sp],\n                                  batch_size=args.batch_size,\n                                  shuffle=True)\n    val_dataloader = DataLoader(dataset=train_data[sp:],\n                                batch_size=args.batch_size,\n                                shuffle=False)\n    test_dataloader = DataLoader(dataset=test_data,\n                                 batch_size=args.batch_size,\n                                 shuffle=False)\n    # Training/Testing.\n    best_val_acc = 0\n    for epoch in range(args.epochs):\n        # Epoch training stats.\n        start_epoch_time = time.time()\n        epoch_L = 0.0\n        epoch_sent_num = 0\n        epoch_wc = 0\n        # Log interval training stats.\n        start_log_interval_time = time.time()\n        log_interval_wc = 0\n        log_interval_sent_num = 0\n        log_interval_L = 0.0\n        for i, (data, label) in enumerate(train_dataloader):\n            data = mx.nd.transpose(data.as_in_context(context))\n            label = label.as_in_context(context)\n            wc = max_len\n            log_interval_wc += wc\n            epoch_wc += wc\n            log_interval_sent_num += data.shape[1]\n            epoch_sent_num += data.shape[1]\n\n            with autograd.record():\n                output = net(data)\n                L = loss(output, label).mean()\n            L.backward()\n            # Update parameter.\n            trainer.step(1)\n            log_interval_L += L.asscalar()\n            epoch_L += L.asscalar()\n            if (i + 1) % args.log_interval == 0:\n                print('[Epoch %d Batch %d/%d] avg loss %g, throughput %gK wps' % (\n                    epoch, i + 1, len(train_dataloader),\n                    log_interval_L / log_interval_sent_num,\n                    log_interval_wc / 1000 / (time.time() - start_log_interval_time)))\n                # Clear log interval training stats.\n                start_log_interval_time = time.time()\n                log_interval_wc = 0\n                log_interval_sent_num = 0\n                log_interval_L = 0\n        end_epoch_time = time.time()\n        val_avg_L, val_acc = evaluate(net, val_dataloader)\n        print('[Epoch %d] train avg loss %g, '\n              'test acc %.4f, test avg loss %g, throughput %gK wps' % (\n                  epoch, epoch_L / epoch_sent_num,\n                  val_acc, val_avg_L,\n                  epoch_wc / 1000 / (end_epoch_time - start_epoch_time)))\n\n        if val_acc >= best_val_acc:\n            print('Observed Improvement.')\n            best_val_acc = val_acc\n            test_avg_L, test_acc = evaluate(net, test_dataloader)\n\n    print('Test loss %g, test acc %.4f'%(test_avg_L, test_acc))\n    print('Total time cost %.2fs'%(time.time()-start_pipeline_time))\n    return test_acc", "response": "Train textCNN model for sentiment analysis."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget tokens and tokens embedding for the given list of sentences.", "response": "def embedding(self, sentences, oov_way='avg'):\n        \"\"\"\n        Get tokens, tokens embedding\n\n        Parameters\n        ----------\n        sentences : List[str]\n            sentences for encoding.\n        oov_way : str, default avg.\n            use **avg**, **sum** or **last** to get token embedding for those out of\n            vocabulary words\n\n        Returns\n        -------\n        List[(List[str], List[ndarray])]\n            List of tokens, and tokens embedding\n        \"\"\"\n        data_iter = self.data_loader(sentences=sentences)\n        batches = []\n        for token_ids, valid_length, token_types in data_iter:\n            token_ids = token_ids.as_in_context(self.ctx)\n            valid_length = valid_length.as_in_context(self.ctx)\n            token_types = token_types.as_in_context(self.ctx)\n            sequence_outputs = self.bert(token_ids, token_types,\n                                         valid_length.astype(self.dtype))\n            for token_id, sequence_output in zip(token_ids.asnumpy(),\n                                                 sequence_outputs.asnumpy()):\n                batches.append((token_id, sequence_output))\n        return self.oov(batches, oov_way)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload tokenize and prepare the input sentences.", "response": "def data_loader(self, sentences, shuffle=False):\n        \"\"\"Load, tokenize and prepare the input sentences.\"\"\"\n        dataset = BertEmbeddingDataset(sentences, self.transform)\n        return DataLoader(dataset=dataset, batch_size=self.batch_size, shuffle=shuffle)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_bert_model(model_name=None, dataset_name=None, vocab=None,\n                   pretrained=True, ctx=mx.cpu(),\n                   use_pooler=True, use_decoder=True, use_classifier=True,\n                   output_attention=False, output_all_encodings=False,\n                   root=os.path.join(get_home_dir(), 'models'), **kwargs):\n    \"\"\"Any BERT pretrained model.\n\n    Parameters\n    ----------\n    model_name : str or None, default None\n        Options include 'bert_24_1024_16' and 'bert_12_768_12'.\n    dataset_name : str or None, default None\n        Options include 'book_corpus_wiki_en_cased', 'book_corpus_wiki_en_uncased'\n        for both bert_24_1024_16 and bert_12_768_12.\n        'wiki_cn_cased', 'wiki_multilingual_uncased' and 'wiki_multilingual_cased'\n        for bert_12_768_12 only.\n    vocab : gluonnlp.vocab.BERTVocab or None, default None\n        Vocabulary for the dataset. Must be provided if dataset is not specified.\n    pretrained : bool, default True\n        Whether to load the pretrained weights for model.\n    ctx : Context, default CPU\n        The context in which to load the pretrained weights.\n    root : str, default '$MXNET_HOME/models'\n        Location for keeping the model parameters.\n        MXNET_HOME defaults to '~/.mxnet'.\n    use_pooler : bool, default True\n        Whether to include the pooler which converts the encoded sequence tensor of shape\n        (batch_size, seq_length, units) to a tensor of shape (batch_size, units)\n        for for segment level classification task.\n    use_decoder : bool, default True\n        Whether to include the decoder for masked language model prediction.\n    use_classifier : bool, default True\n        Whether to include the classifier for next sentence classification.\n    output_attention : bool, default False\n        Whether to include attention weights of each encoding cell to the output.\n    output_all_encodings : bool, default False\n        Whether to output encodings of all encoder cells.\n\n    Returns\n    -------\n    BERTModel, gluonnlp.vocab.BERTVocab\n    \"\"\"\n    predefined_args = bert_hparams[model_name]\n    mutable_args = ['use_residual', 'dropout', 'embed_dropout', 'word_embed']\n    mutable_args = frozenset(mutable_args)\n    assert all((k not in kwargs or k in mutable_args) for k in predefined_args), \\\n        'Cannot override predefined model settings.'\n    predefined_args.update(kwargs)\n    # encoder\n    encoder = BERTEncoder(attention_cell=predefined_args['attention_cell'],\n                          num_layers=predefined_args['num_layers'],\n                          units=predefined_args['units'],\n                          hidden_size=predefined_args['hidden_size'],\n                          max_length=predefined_args['max_length'],\n                          num_heads=predefined_args['num_heads'],\n                          scaled=predefined_args['scaled'],\n                          dropout=predefined_args['dropout'],\n                          output_attention=output_attention,\n                          output_all_encodings=output_all_encodings,\n                          use_residual=predefined_args['use_residual'])\n    # bert_vocab\n    from ..vocab import BERTVocab\n    if dataset_name in ['wiki_cn', 'wiki_multilingual']:\n        warnings.warn('wiki_cn/wiki_multilingual will be deprecated.'\n                      ' Please use wiki_cn_cased/wiki_multilingual_uncased instead.')\n    bert_vocab = _load_vocab(dataset_name, vocab, root, cls=BERTVocab)\n    # BERT\n    net = BERTModel(encoder, len(bert_vocab),\n                    token_type_vocab_size=predefined_args['token_type_vocab_size'],\n                    units=predefined_args['units'],\n                    embed_size=predefined_args['embed_size'],\n                    embed_dropout=predefined_args['embed_dropout'],\n                    word_embed=predefined_args['word_embed'],\n                    use_pooler=use_pooler, use_decoder=use_decoder,\n                    use_classifier=use_classifier)\n    if pretrained:\n        ignore_extra = not (use_pooler and use_decoder and use_classifier)\n        _load_pretrained_params(net, model_name, dataset_name, root, ctx,\n                                ignore_extra=ignore_extra)\n    return net, bert_vocab", "response": "Returns a BERT pretrained model."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconstructs a decoder for the next sentence prediction task.", "response": "def _get_classifier(self, prefix):\n        \"\"\" Construct a decoder for the next sentence prediction task \"\"\"\n        with self.name_scope():\n            classifier = nn.Dense(2, prefix=prefix)\n        return classifier"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconstruct a decoder for the masked language model task.", "response": "def _get_decoder(self, units, vocab_size, embed, prefix):\n        \"\"\" Construct a decoder for the masked language model task \"\"\"\n        with self.name_scope():\n            decoder = nn.HybridSequential(prefix=prefix)\n            decoder.add(nn.Dense(units, flatten=False))\n            decoder.add(GELU())\n            decoder.add(BERTLayerNorm(in_channels=units))\n            decoder.add(nn.Dense(vocab_size, flatten=False, params=embed.collect_params()))\n        assert decoder[3].weight == list(embed.collect_params().values())[0], \\\n            'The weights of word embedding are not tied with those of decoder'\n        return decoder"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_embed(self, embed, vocab_size, embed_size, initializer, dropout, prefix):\n        if embed is None:\n            assert embed_size is not None, '\"embed_size\" cannot be None if \"word_embed\" or ' \\\n                                           'token_type_embed is not given.'\n            with self.name_scope():\n                embed = nn.HybridSequential(prefix=prefix)\n                with embed.name_scope():\n                    embed.add(nn.Embedding(input_dim=vocab_size, output_dim=embed_size,\n                                           weight_initializer=initializer))\n                    if dropout:\n                        embed.add(nn.Dropout(rate=dropout))\n        assert isinstance(embed, Block)\n        return embed", "response": "Construct an embedding block."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconstruct the pooler for the given units and prefix.", "response": "def _get_pooler(self, units, prefix):\n        \"\"\" Construct pooler.\n\n        The pooler slices and projects the hidden output of first token\n        in the sequence for segment level classification.\n\n        \"\"\"\n        with self.name_scope():\n            pooler = nn.Dense(units=units, flatten=False, activation='tanh',\n                              prefix=prefix)\n        return pooler"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _encode_sequence(self, inputs, token_types, valid_length=None):\n        # embedding\n        word_embedding = self.word_embed(inputs)\n        type_embedding = self.token_type_embed(token_types)\n        embedding = word_embedding + type_embedding\n        # encoding\n        outputs, additional_outputs = self.encoder(embedding, None, valid_length)\n        return outputs, additional_outputs", "response": "Generate the representation given the input sequences."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate unnormalized prediction for the masked language model task. This is only used for pre-training the BERT model. Inputs: - **sequence**: input tensor of sequence encodings. Shape (batch_size, seq_length, units). - **masked_positions**: input tensor of position of tokens for masked LM decoding. Shape (batch_size, num_masked_positions). For each sample in the batch, the values in this tensor must not be out of bound considering the length of the sequence. Outputs: - **masked_lm_outputs**: output tensor of token predictions for target masked_positions. Shape (batch_size, num_masked_positions, vocab_size).", "response": "def _decode(self, sequence, masked_positions):\n        \"\"\"Generate unnormalized prediction for the masked language model task.\n\n        This is only used for pre-training the BERT model.\n\n        Inputs:\n            - **sequence**: input tensor of sequence encodings.\n              Shape (batch_size, seq_length, units).\n            - **masked_positions**: input tensor of position of tokens for masked LM decoding.\n              Shape (batch_size, num_masked_positions). For each sample in the batch, the values\n              in this tensor must not be out of bound considering the length of the sequence.\n\n        Outputs:\n            - **masked_lm_outputs**: output tensor of token predictions for target masked_positions.\n                Shape (batch_size, num_masked_positions, vocab_size).\n        \"\"\"\n        batch_size = sequence.shape[0]\n        num_masked_positions = masked_positions.shape[1]\n        ctx = masked_positions.context\n        dtype = masked_positions.dtype\n        # batch_idx = [0,0,0,1,1,1,2,2,2...]\n        # masked_positions = [1,2,4,0,3,4,2,3,5...]\n        batch_idx = mx.nd.arange(0, batch_size, repeat=num_masked_positions, dtype=dtype, ctx=ctx)\n        batch_idx = batch_idx.reshape((1, -1))\n        masked_positions = masked_positions.reshape((1, -1))\n        position_idx = mx.nd.Concat(batch_idx, masked_positions, dim=0)\n        encoded = mx.nd.gather_nd(sequence, position_idx)\n        encoded = encoded.reshape((batch_size, num_masked_positions, sequence.shape[-1]))\n        decoded = self.decoder(encoded)\n        return decoded"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _ngrams(segment, n):\n    ngram_counts = Counter()\n    for i in range(0, len(segment) - n + 1):\n        ngram = tuple(segment[i:i + n])\n        ngram_counts[ngram] += 1\n    return ngram_counts", "response": "Extracts n - grams from an input segment."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts a sequence of bpe words into words.", "response": "def _bpe_to_words(sentence, delimiter='@@'):\n    \"\"\"Convert a sequence of bpe words into sentence.\"\"\"\n    words = []\n    word = ''\n    delimiter_len = len(delimiter)\n    for subwords in sentence:\n        if len(subwords) >= delimiter_len and subwords[-delimiter_len:] == delimiter:\n            word += subwords[:-delimiter_len]\n        else:\n            word += subwords\n            words.append(word)\n            word = ''\n    return words"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _tokenize_mteval_13a(segment):\n\n    norm = segment.rstrip()\n\n    norm = norm.replace('<skipped>', '')\n    norm = norm.replace('-\\n', '')\n    norm = norm.replace('\\n', ' ')\n    norm = norm.replace('&quot;', '\"')\n    norm = norm.replace('&amp;', '&')\n    norm = norm.replace('&lt;', '<')\n    norm = norm.replace('&gt;', '>')\n\n    norm = u' {} '.format(norm)\n    norm = re.sub(r'([\\{-\\~\\[-\\` -\\&\\(-\\+\\:-\\@\\/])', ' \\\\1 ', norm)\n    norm = re.sub(r'([^0-9])([\\.,])', '\\\\1 \\\\2 ', norm)\n    norm = re.sub(r'([\\.,])([^0-9])', ' \\\\1 \\\\2', norm)\n    norm = re.sub(r'([0-9])(-)', '\\\\1 \\\\2 ', norm)\n    norm = re.sub(r'\\s+', ' ', norm)\n    norm = re.sub(r'^\\s+', '', norm)\n    norm = re.sub(r'\\s+$', '', norm)\n\n    return norm", "response": "r Tokenizes a string following the tokenizer in mteval - v13a. pl.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _tokenize_mteval_v14_intl(segment):\n    segment = segment.rstrip()\n    segment = unicodeRegex.nondigit_punct_re.sub(r'\\1 \\2 ', segment)\n    segment = unicodeRegex.punct_nondigit_re.sub(r' \\1 \\2', segment)\n    segment = unicodeRegex.symbol_re.sub(r' \\1 ', segment)\n    return segment.strip()", "response": "Tokenize a string following the international tokenizer in mteval - v14a. pl.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _compute_precision(references, translation, n):\n    matches = 0\n    candidates = 0\n    ref_ngram_counts = Counter()\n\n    for reference in references:\n        ref_ngram_counts |= _ngrams(reference, n)\n    trans_ngram_counts = _ngrams(translation, n)\n    overlap_ngram_counts = trans_ngram_counts & ref_ngram_counts\n    matches += sum(overlap_ngram_counts.values())\n    possible_matches = len(translation) - n + 1\n    if possible_matches > 0:\n        candidates += possible_matches\n\n    return matches, candidates", "response": "Compute ngram precision.\n\n    Parameters\n    ----------\n    references: list(list(str))\n        A list of references.\n    translation: list(str)\n        A translation.\n    n: int\n        Order of n-gram.\n\n    Returns\n    -------\n    matches: int\n        Number of matched nth order n-grams\n    candidates\n        Number of possible nth order n-grams"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _brevity_penalty(ref_length, trans_length):\n    if trans_length > ref_length:\n        return 1\n    # If translation is empty, brevity penalty = 0 should result in BLEU = 0.0\n    elif trans_length == 0:\n        return 0\n    else:\n        return math.exp(1 - float(ref_length) / trans_length)", "response": "Calculate brevity penalty for a single BLEU."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _closest_ref_length(references, trans_length):\n    ref_lengths = (len(reference) for reference in references)\n    closest_ref_len = min(ref_lengths,\n                          key=lambda ref_length: (abs(ref_length - trans_length), ref_length))\n\n    return closest_ref_len", "response": "Find the closest length to the translation."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _smoothing(precision_fractions, c=1):\n    ratios = [0] * len(precision_fractions)\n    for i, precision_fraction in enumerate(precision_fractions):\n        if precision_fraction[1] > 0:\n            ratios[i] = float(precision_fraction[0] + c) / (precision_fraction[1] + c)\n        else:\n            ratios[i] = 0.0\n\n    return ratios", "response": "Compute the smoothed precision for all the orders."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef forward(self, true_classes):\n        num_sampled = self._num_sampled\n        ctx = true_classes.context\n        num_tries = 0\n        log_range = math.log(self._range_max + 1)\n\n        # sample candidates\n        f = ndarray._internal._sample_unique_zipfian\n        sampled_classes, num_tries = f(self._range_max, shape=(1, num_sampled))\n        sampled_classes = sampled_classes.reshape((-1,))\n        sampled_classes = sampled_classes.as_in_context(ctx)\n        num_tries = num_tries.as_in_context(ctx)\n\n        # expected count for true classes\n        true_cls = true_classes.as_in_context(ctx).astype('float64')\n        prob_true = ((true_cls + 2.0) / (true_cls + 1.0)).log() / log_range\n        count_true = self._prob_helper(num_tries, prob_true)\n        # expected count for sampled classes\n        sampled_classes = ndarray.array(sampled_classes, ctx=ctx, dtype='int64')\n        sampled_cls_fp64 = sampled_classes.astype('float64')\n        prob_sampled = ((sampled_cls_fp64 + 2.0) / (sampled_cls_fp64 + 1.0)).log() / log_range\n        count_sampled = self._prob_helper(num_tries, prob_sampled)\n        # convert to dtype\n        sampled_classes = sampled_classes.astype(self._dtype, copy=False)\n        count_true = count_true.astype(self._dtype, copy=False)\n        count_sampled = count_sampled.astype(self._dtype, copy=False)\n        return sampled_classes, count_sampled, count_true", "response": "Draw samples from log uniform distribution and returns sampled candidates expected count for true classes and sampled classes."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef wiki(wiki_root, wiki_date, wiki_language, max_vocab_size=None):\n    data = WikiDumpStream(\n        root=os.path.expanduser(wiki_root), language=wiki_language,\n        date=wiki_date)\n    vocab = data.vocab\n    if max_vocab_size:\n        for token in vocab.idx_to_token[max_vocab_size:]:\n            vocab.token_to_idx.pop(token)\n        vocab.idx_to_token = vocab.idx_to_token[:max_vocab_size]\n    idx_to_counts = data.idx_to_counts\n\n    def code(shard):\n        return [[vocab[token] for token in sentence if token in vocab]\n                for sentence in shard]\n\n    data = data.transform(code)\n    return data, vocab, idx_to_counts", "response": "Wikipedia dump helper.\n\n    Parameters\n    ----------\n    wiki_root : str\n        Parameter for WikiDumpStream\n    wiki_date : str\n        Parameter for WikiDumpStream\n    wiki_language : str\n        Parameter for WikiDumpStream\n    max_vocab_size : int, optional\n        Specifies a maximum size for the vocabulary.\n\n    Returns\n    -------\n    gluonnlp.data.DataStream\n        Each sample is a valid input to\n        gluonnlp.data.EmbeddingCenterContextBatchify.\n    gluonnlp.Vocab\n        Vocabulary of all tokens in the Wikipedia corpus as provided by\n        WikiDumpStream but with maximum size max_vocab_size.\n    idx_to_counts : list of int\n        Mapping from token indices to their occurrence-counts in the Wikipedia\n        corpus."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntransforms a DataStream of coded DataSets into a DataStream of batches.", "response": "def transform_data_fasttext(data, vocab, idx_to_counts, cbow, ngram_buckets,\n                            ngrams, batch_size, window_size,\n                            frequent_token_subsampling=1E-4, dtype='float32',\n                            index_dtype='int64'):\n    \"\"\"Transform a DataStream of coded DataSets to a DataStream of batches.\n\n    Parameters\n    ----------\n    data : gluonnlp.data.DataStream\n        DataStream where each sample is a valid input to\n        gluonnlp.data.EmbeddingCenterContextBatchify.\n    vocab : gluonnlp.Vocab\n        Vocabulary containing all tokens whose indices occur in data. For each\n        token, it's associated subwords will be computed and used for\n        constructing the batches. No subwords are used if ngram_buckets is 0.\n    idx_to_counts : list of int\n        List of integers such that idx_to_counts[idx] represents the count of\n        vocab.idx_to_token[idx] in the underlying dataset. The count\n        information is used to subsample frequent words in the dataset.\n        Each token is independently dropped with probability 1 - sqrt(t /\n        (count / sum_counts)) where t is the hyperparameter\n        frequent_token_subsampling.\n    cbow : boolean\n        If True, batches for CBOW are returned.\n    ngram_buckets : int\n        Number of hash buckets to consider for the fastText\n        nlp.vocab.NGramHashes subword function.\n    ngrams : list of int\n        For each integer n in the list, all ngrams of length n will be\n        considered by the nlp.vocab.NGramHashes subword function.\n    batch_size : int\n        The returned data stream iterates over batches of batch_size.\n    window_size : int\n        The context window size for\n        gluonnlp.data.EmbeddingCenterContextBatchify.\n    frequent_token_subsampling : float\n        Hyperparameter for subsampling. See idx_to_counts above for more\n        information.\n    dtype : str or np.dtype, default 'float32'\n        Data type of data array.\n    index_dtype : str or np.dtype, default 'int64'\n        Data type of index arrays.\n\n    Returns\n    -------\n    gluonnlp.data.DataStream\n        Stream over batches. Each returned element is a list corresponding to\n        the arguments for the forward pass of model.SG or model.CBOW\n        respectively based on if cbow is False or True. If ngarm_buckets > 0,\n        the returned sample will contain ngrams. Both model.SG or model.CBOW\n        will handle them correctly as long as they are initialized with the\n        subword_function returned as second argument by this function (see\n        below).\n    gluonnlp.vocab.NGramHashes\n        The subword_function used for obtaining the subwords in the returned\n        batches.\n\n    \"\"\"\n    if ngram_buckets <= 0:\n        raise ValueError('Invalid ngram_buckets. Use Word2Vec training '\n                         'pipeline if not interested in ngrams.')\n\n    sum_counts = float(sum(idx_to_counts))\n    idx_to_pdiscard = [\n        1 - math.sqrt(frequent_token_subsampling / (count / sum_counts))\n        for count in idx_to_counts]\n\n    def subsample(shard):\n        return [[\n            t for t, r in zip(sentence,\n                              np.random.uniform(0, 1, size=len(sentence)))\n            if r > idx_to_pdiscard[t]] for sentence in shard]\n\n    data = data.transform(subsample)\n\n    batchify = nlp.data.batchify.EmbeddingCenterContextBatchify(\n        batch_size=batch_size, window_size=window_size, cbow=cbow,\n        weight_dtype=dtype, index_dtype=index_dtype)\n    data = data.transform(batchify)\n\n    with print_time('prepare subwords'):\n        subword_function = nlp.vocab.create_subword_function(\n            'NGramHashes', ngrams=ngrams, num_subwords=ngram_buckets)\n\n        # Store subword indices for all words in vocabulary\n        idx_to_subwordidxs = list(subword_function(vocab.idx_to_token))\n        subwordidxs = np.concatenate(idx_to_subwordidxs)\n        subwordidxsptr = np.cumsum([\n            len(subwordidxs) for subwordidxs in idx_to_subwordidxs])\n        subwordidxsptr = np.concatenate([\n            np.zeros(1, dtype=np.int64), subwordidxsptr])\n        if cbow:\n            subword_lookup = functools.partial(\n                cbow_lookup, subwordidxs=subwordidxs,\n                subwordidxsptr=subwordidxsptr, offset=len(vocab))\n        else:\n            subword_lookup = functools.partial(\n                skipgram_lookup, subwordidxs=subwordidxs,\n                subwordidxsptr=subwordidxsptr, offset=len(vocab))\n        max_subwordidxs_len = max(len(s) for s in idx_to_subwordidxs)\n        if max_subwordidxs_len > 500:\n            warnings.warn(\n                'The word with largest number of subwords '\n                'has {} subwords, suggesting there are '\n                'some noisy words in your vocabulary. '\n                'You should filter out very long words '\n                'to avoid memory issues.'.format(max_subwordidxs_len))\n\n    data = UnchainStream(data)\n\n    if cbow:\n        batchify_fn = cbow_fasttext_batch\n    else:\n        batchify_fn = skipgram_fasttext_batch\n    batchify_fn = functools.partial(\n        batchify_fn, num_tokens=len(vocab) + len(subword_function),\n        subword_lookup=subword_lookup, dtype=dtype, index_dtype=index_dtype)\n\n    return data, batchify_fn, subword_function"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ntransforms a DataStream of coded DataSets to a DataStream of coded DataSets with a context window size.", "response": "def transform_data_word2vec(data, vocab, idx_to_counts, cbow, batch_size,\n                            window_size, frequent_token_subsampling=1E-4,\n                            dtype='float32', index_dtype='int64'):\n    \"\"\"Transform a DataStream of coded DataSets to a DataStream of batches.\n\n    Parameters\n    ----------\n    data : gluonnlp.data.DataStream\n        DataStream where each sample is a valid input to\n        gluonnlp.data.EmbeddingCenterContextBatchify.\n    vocab : gluonnlp.Vocab\n        Vocabulary containing all tokens whose indices occur in data.\n    idx_to_counts : list of int\n        List of integers such that idx_to_counts[idx] represents the count of\n        vocab.idx_to_token[idx] in the underlying dataset. The count\n        information is used to subsample frequent words in the dataset.\n        Each token is independently dropped with probability 1 - sqrt(t /\n        (count / sum_counts)) where t is the hyperparameter\n        frequent_token_subsampling.\n    batch_size : int\n        The returned data stream iterates over batches of batch_size.\n    window_size : int\n        The context window size for\n        gluonnlp.data.EmbeddingCenterContextBatchify.\n    frequent_token_subsampling : float\n        Hyperparameter for subsampling. See idx_to_counts above for more\n        information.\n    dtype : str or np.dtype, default 'float32'\n        Data type of data array.\n    index_dtype : str or np.dtype, default 'int64'\n        Data type of index arrays.\n\n    Returns\n    -------\n    gluonnlp.data.DataStream\n        Stream over batches.\n    \"\"\"\n\n    sum_counts = float(sum(idx_to_counts))\n    idx_to_pdiscard = [\n        1 - math.sqrt(frequent_token_subsampling / (count / sum_counts))\n        for count in idx_to_counts]\n\n    def subsample(shard):\n        return [[\n            t for t, r in zip(sentence,\n                              np.random.uniform(0, 1, size=len(sentence)))\n            if r > idx_to_pdiscard[t]] for sentence in shard]\n\n    data = data.transform(subsample)\n\n    batchify = nlp.data.batchify.EmbeddingCenterContextBatchify(\n        batch_size=batch_size, window_size=window_size, cbow=cbow,\n        weight_dtype=dtype, index_dtype=index_dtype)\n    data = data.transform(batchify)\n    data = UnchainStream(data)\n\n    if cbow:\n        batchify_fn = cbow_batch\n    else:\n        batchify_fn = skipgram_batch\n    batchify_fn = functools.partial(batchify_fn, num_tokens=len(vocab),\n                                    dtype=dtype, index_dtype=index_dtype)\n\n    return data, batchify_fn,"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a batch for CBOW training objective with subwords.", "response": "def cbow_fasttext_batch(centers, contexts, num_tokens, subword_lookup, dtype,\n                        index_dtype):\n    \"\"\"Create a batch for CBOW training objective with subwords.\"\"\"\n    _, contexts_row, contexts_col = contexts\n    data, row, col = subword_lookup(contexts_row, contexts_col)\n    centers = mx.nd.array(centers, dtype=index_dtype)\n    contexts = mx.nd.sparse.csr_matrix(\n        (data, (row, col)), dtype=dtype,\n        shape=(len(centers), num_tokens))  # yapf: disable\n    return centers, contexts"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a batch for SG training objective with subwords.", "response": "def skipgram_fasttext_batch(centers, contexts, num_tokens, subword_lookup,\n                            dtype, index_dtype):\n    \"\"\"Create a batch for SG training objective with subwords.\"\"\"\n    contexts = mx.nd.array(contexts[2], dtype=index_dtype)\n    data, row, col = subword_lookup(centers)\n    centers = mx.nd.array(centers, dtype=index_dtype)\n    centers_csr = mx.nd.sparse.csr_matrix(\n        (data, (row, col)), dtype=dtype,\n        shape=(len(centers), num_tokens))  # yapf: disable\n    return centers_csr, contexts, centers"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a batch for CBOW training objective.", "response": "def cbow_batch(centers, contexts, num_tokens, dtype, index_dtype):\n    \"\"\"Create a batch for CBOW training objective.\"\"\"\n    contexts_data, contexts_row, contexts_col = contexts\n    centers = mx.nd.array(centers, dtype=index_dtype)\n    contexts = mx.nd.sparse.csr_matrix(\n        (contexts_data, (contexts_row, contexts_col)),\n        dtype=dtype, shape=(len(centers), num_tokens))  # yapf: disable\n    return centers, contexts"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef skipgram_batch(centers, contexts, num_tokens, dtype, index_dtype):\n    contexts = mx.nd.array(contexts[2], dtype=index_dtype)\n    indptr = mx.nd.arange(len(centers) + 1)\n    centers = mx.nd.array(centers, dtype=index_dtype)\n    centers_csr = mx.nd.sparse.csr_matrix(\n        (mx.nd.ones(centers.shape), centers, indptr), dtype=dtype,\n        shape=(len(centers), num_tokens))\n    return centers_csr, contexts, centers", "response": "Create a batch for SG training objective."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef skipgram_lookup(indices, subwordidxs, subwordidxsptr, offset=0):\n    row = []\n    col = []\n    data = []\n    for i, idx in enumerate(indices):\n        start = subwordidxsptr[idx]\n        end = subwordidxsptr[idx + 1]\n\n        row.append(i)\n        col.append(idx)\n        data.append(1 / (1 + end - start))\n        for subword in subwordidxs[start:end]:\n            row.append(i)\n            col.append(subword + offset)\n            data.append(1 / (1 + end - start))\n\n    return (np.array(data, dtype=np.float32), np.array(row, dtype=np.int64),\n            np.array(col, dtype=np.int64))", "response": "This function returns a sparse COO array of words and subwords for SkipGram."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting a sparse COO array of words and subwords for CBOW. Parameters ---------- context_row : numpy.ndarray of dtype int64 Array of same length as context_col containing numbers in [0, batch_size). For each idx, context_row[idx] specifies the row that context_col[idx] occurs in a sparse matrix. context_col : numpy.ndarray of dtype int64 Array of same length as context_row containing numbers in [0, vocabulary_size). For each idx, context_col[idx] is one of the context words in the context_row[idx] row of the batch. subwordidxs : numpy.ndarray Array containing concatenation of all subwords of all tokens in the vocabulary, in order of their occurrence in the vocabulary. For example np.concatenate(idx_to_subwordidxs) subwordidxsptr Array containing pointers into subwordidxs array such that subwordidxs[subwordidxsptr[i]:subwordidxsptr[i+1]] returns all subwords of of token i. For example subwordidxsptr = np.cumsum([ len(subwordidxs) for subwordidxs in idx_to_subwordidxs]) offset : int, default 0 Offset to add to each subword index. Returns ------- numpy.ndarray of dtype float32 Array containing weights summing to 1. The weights are chosen such that the sum of weights for all subwords and word units of a given context word is equal to 1 / number_of_context_words_in_the_row. This array is the data array of a sparse array of COO format. numpy.ndarray of dtype int64 This array is the row array of a sparse array of COO format. numpy.ndarray of dtype int64 This array is the col array of a sparse array of COO format. Array containing weights such that for each row, all weights sum to 1. In particular, all elements in a row have weight 1 / num_elements_in_the_row", "response": "def cbow_lookup(context_row, context_col, subwordidxs, subwordidxsptr,\n                offset=0):\n    \"\"\"Get a sparse COO array of words and subwords for CBOW.\n\n    Parameters\n    ----------\n    context_row : numpy.ndarray of dtype int64\n        Array of same length as context_col containing numbers in [0,\n        batch_size). For each idx, context_row[idx] specifies the row that\n        context_col[idx] occurs in a sparse matrix.\n    context_col : numpy.ndarray of dtype int64\n        Array of same length as context_row containing numbers in [0,\n        vocabulary_size). For each idx, context_col[idx] is one of the\n        context words in the context_row[idx] row of the batch.\n    subwordidxs : numpy.ndarray\n        Array containing concatenation of all subwords of all tokens in the\n        vocabulary, in order of their occurrence in the vocabulary.\n        For example np.concatenate(idx_to_subwordidxs)\n    subwordidxsptr\n        Array containing pointers into subwordidxs array such that\n        subwordidxs[subwordidxsptr[i]:subwordidxsptr[i+1]] returns all subwords\n        of of token i. For example subwordidxsptr = np.cumsum([\n        len(subwordidxs) for subwordidxs in idx_to_subwordidxs])\n    offset : int, default 0\n        Offset to add to each subword index.\n\n    Returns\n    -------\n    numpy.ndarray of dtype float32\n        Array containing weights summing to 1. The weights are chosen such\n        that the sum of weights for all subwords and word units of a given\n        context word is equal to 1 / number_of_context_words_in_the_row.\n        This array is the data array of a sparse array of COO format.\n    numpy.ndarray of dtype int64\n        This array is the row array of a sparse array of COO format.\n    numpy.ndarray of dtype int64\n        This array is the col array of a sparse array of COO format.\n        Array containing weights such that for each row, all weights sum to\n        1. In particular, all elements in a row have weight 1 /\n        num_elements_in_the_row\n\n    \"\"\"\n    row = []\n    col = []\n    data = []\n\n    num_rows = np.max(context_row) + 1\n    row_to_numwords = np.zeros(num_rows)\n\n    for i, idx in enumerate(context_col):\n        start = subwordidxsptr[idx]\n        end = subwordidxsptr[idx + 1]\n\n        row_ = context_row[i]\n        row_to_numwords[row_] += 1\n\n        row.append(row_)\n        col.append(idx)\n        data.append(1 / (1 + end - start))\n        for subword in subwordidxs[start:end]:\n            row.append(row_)\n            col.append(subword + offset)\n            data.append(1 / (1 + end - start))\n\n    # Normalize by number of words\n    for i, row_ in enumerate(row):\n        assert 0 <= row_ <= num_rows\n        data[i] /= row_to_numwords[row_]\n\n    return (np.array(data, dtype=np.float32), np.array(row, dtype=np.int64),\n            np.array(col, dtype=np.int64))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the source vocabulary of the Dataset.", "response": "def src_vocab(self):\n        \"\"\"Source Vocabulary of the Dataset.\n\n        Returns\n        -------\n        src_vocab : Vocab\n            Source vocabulary.\n        \"\"\"\n        if self._src_vocab is None:\n            src_vocab_file_name, src_vocab_hash = \\\n                self._data_file[self._pair_key]['vocab' + '_' + self._src_lang]\n            [src_vocab_path] = self._fetch_data_path([(src_vocab_file_name, src_vocab_hash)])\n            with io.open(src_vocab_path, 'r', encoding='utf-8') as in_file:\n                self._src_vocab = Vocab.from_json(in_file.read())\n        return self._src_vocab"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the target vocabulary of the Dataset.", "response": "def tgt_vocab(self):\n        \"\"\"Target Vocabulary of the Dataset.\n\n        Returns\n        -------\n        tgt_vocab : Vocab\n            Target vocabulary.\n        \"\"\"\n        if self._tgt_vocab is None:\n            tgt_vocab_file_name, tgt_vocab_hash = \\\n                self._data_file[self._pair_key]['vocab' + '_' + self._tgt_lang]\n            [tgt_vocab_path] = self._fetch_data_path([(tgt_vocab_file_name, tgt_vocab_hash)])\n            with io.open(tgt_vocab_path, 'r', encoding='utf-8') as in_file:\n                self._tgt_vocab = Vocab.from_json(in_file.read())\n        return self._tgt_vocab"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nevaluate given the data loader", "response": "def evaluate(data_loader):\n    \"\"\"Evaluate given the data loader\n\n    Parameters\n    ----------\n    data_loader : DataLoader\n\n    Returns\n    -------\n    avg_loss : float\n        Average loss\n    real_translation_out : list of list of str\n        The translation output\n    \"\"\"\n    translation_out = []\n    all_inst_ids = []\n    avg_loss_denom = 0\n    avg_loss = 0.0\n    for _, (src_seq, tgt_seq, src_valid_length, tgt_valid_length, inst_ids) \\\n            in enumerate(data_loader):\n        src_seq = src_seq.as_in_context(ctx)\n        tgt_seq = tgt_seq.as_in_context(ctx)\n        src_valid_length = src_valid_length.as_in_context(ctx)\n        tgt_valid_length = tgt_valid_length.as_in_context(ctx)\n        # Calculating Loss\n        out, _ = model(src_seq, tgt_seq[:, :-1], src_valid_length, tgt_valid_length - 1)\n        loss = loss_function(out, tgt_seq[:, 1:], tgt_valid_length - 1).mean().asscalar()\n        all_inst_ids.extend(inst_ids.asnumpy().astype(np.int32).tolist())\n        avg_loss += loss * (tgt_seq.shape[1] - 1)\n        avg_loss_denom += (tgt_seq.shape[1] - 1)\n        # Translate\n        samples, _, sample_valid_length =\\\n            translator.translate(src_seq=src_seq, src_valid_length=src_valid_length)\n        max_score_sample = samples[:, 0, :].asnumpy()\n        sample_valid_length = sample_valid_length[:, 0].asnumpy()\n        for i in range(max_score_sample.shape[0]):\n            translation_out.append(\n                [tgt_vocab.idx_to_token[ele] for ele in\n                 max_score_sample[i][1:(sample_valid_length[i] - 1)]])\n    avg_loss = avg_loss / avg_loss_denom\n    real_translation_out = [None for _ in range(len(all_inst_ids))]\n    for ind, sentence in zip(all_inst_ids, translation_out):\n        real_translation_out[ind] = sentence\n    return avg_loss, real_translation_out"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_cache_model(name, dataset_name='wikitext-2', window=2000,\n                    theta=0.6, lambdas=0.2, ctx=mx.cpu(), **kwargs):\n    r\"\"\"Returns a cache model using a pre-trained language model.\n\n    We implement the neural cache language model proposed in the following work::\n\n        @article{grave2016improving,\n        title={Improving neural language models with a continuous cache},\n        author={Grave, Edouard and Joulin, Armand and Usunier, Nicolas},\n        journal={ICLR},\n        year={2017}\n        }\n\n    Parameters\n    ----------\n    name : str\n        Name of the cache language model.\n    dataset_name : str or None, default 'wikitext-2'.\n        The dataset name on which the pre-trained model is trained.\n        Options are 'wikitext-2'. If specified, then the returned vocabulary is extracted from\n        the training set of the dataset.\n        If None, then vocab is required, for specifying embedding weight size, and is directly\n        returned.\n    window : int\n        Size of cache window\n    theta : float\n        The scala controls the flatness of the cache distribution\n        that predict the next word as shown below:\n\n        .. math::\n\n            p_{cache} \\propto \\sum_{i=1}^{t-1} \\mathbb{1}_{w=x_{i+1}} exp(\\theta {h_t}^T h_i)\n\n        where :math:`p_{cache}` is the cache distribution, :math:`\\mathbb{1}` is\n        the identity function, and :math:`h_i` is the output of timestep i.\n    lambdas : float\n        Linear scalar between only cache and vocab distribution, the formulation is as below:\n\n        .. math::\n\n            p = (1 - \\lambda) p_{vocab} + \\lambda p_{cache}\n\n        where :math:`p_{vocab}` is the vocabulary distribution and :math:`p_{cache}`\n        is the cache distribution.\n    vocab : gluonnlp.Vocab or None, default None\n        Vocabulary object to be used with the language model.\n        Required when dataset_name is not specified.\n    pretrained : bool, default False\n        Whether to load the pre-trained weights for model.\n    ctx : Context, default CPU\n        The context in which to load the pre-trained weights.\n    root : str, default '~/.mxnet/models'\n        Location for keeping the pre-trained model parameters.\n\n    Returns\n    -------\n    Block\n        The model.\n    \"\"\"\n    lm_model, vocab = nlp.model.\\\n        get_model(name, dataset_name=dataset_name, pretrained=True, ctx=ctx, **kwargs)\n    cache_cell = CacheCell(lm_model, len(vocab), window, theta, lambdas)\n    return cache_cell", "response": "r Returns a neural cache model using a pre - trained language model."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntraining the internal list of n - grams.", "response": "def train(args):\n    \"\"\"Training helper.\"\"\"\n    if not args.model.lower() in ['cbow', 'skipgram']:\n        logging.error('Unsupported model %s.', args.model)\n        sys.exit(1)\n\n    if args.data.lower() == 'toy':\n        data = mx.gluon.data.SimpleDataset(nlp.data.Text8(segment='train')[:2])\n        data, vocab, idx_to_counts = preprocess_dataset(\n            data, max_vocab_size=args.max_vocab_size)\n    elif args.data.lower() == 'text8':\n        data = nlp.data.Text8(segment='train')\n        data, vocab, idx_to_counts = preprocess_dataset(\n            data, max_vocab_size=args.max_vocab_size)\n    elif args.data.lower() == 'fil9':\n        data = nlp.data.Fil9(max_sentence_length=10000)\n        data, vocab, idx_to_counts = preprocess_dataset(\n            data, max_vocab_size=args.max_vocab_size)\n    elif args.data.lower() == 'wiki':\n        data, vocab, idx_to_counts = wiki(args.wiki_root, args.wiki_date,\n                                          args.wiki_language,\n                                          args.max_vocab_size)\n\n    if args.ngram_buckets > 0:\n        data, batchify_fn, subword_function = transform_data_fasttext(\n            data, vocab, idx_to_counts, cbow=args.model.lower() == 'cbow',\n            ngram_buckets=args.ngram_buckets, ngrams=args.ngrams,\n            batch_size=args.batch_size, window_size=args.window,\n            frequent_token_subsampling=args.frequent_token_subsampling)\n    else:\n        subword_function = None\n        data, batchify_fn = transform_data_word2vec(\n            data, vocab, idx_to_counts, cbow=args.model.lower() == 'cbow',\n            batch_size=args.batch_size, window_size=args.window,\n            frequent_token_subsampling=args.frequent_token_subsampling)\n\n    num_tokens = float(sum(idx_to_counts))\n\n    model = CBOW if args.model.lower() == 'cbow' else SG\n    embedding = model(token_to_idx=vocab.token_to_idx, output_dim=args.emsize,\n                      batch_size=args.batch_size, num_negatives=args.negative,\n                      negatives_weights=mx.nd.array(idx_to_counts),\n                      subword_function=subword_function)\n    context = get_context(args)\n    embedding.initialize(ctx=context)\n    if not args.no_hybridize:\n        embedding.hybridize(static_alloc=True, static_shape=True)\n\n    optimizer_kwargs = dict(learning_rate=args.lr)\n    try:\n        trainer = mx.gluon.Trainer(embedding.collect_params(), args.optimizer,\n                                   optimizer_kwargs)\n    except ValueError as e:\n        if args.optimizer == 'groupadagrad':\n            logging.warning('MXNet <= v1.3 does not contain '\n                            'GroupAdaGrad support. Falling back to AdaGrad')\n            trainer = mx.gluon.Trainer(embedding.collect_params(), 'adagrad',\n                                       optimizer_kwargs)\n        else:\n            raise e\n\n    try:\n        if args.no_prefetch_batch:\n            data = data.transform(batchify_fn)\n        else:\n            from executors import LazyThreadPoolExecutor\n            num_cpu = len(os.sched_getaffinity(0))\n            ex = LazyThreadPoolExecutor(num_cpu)\n    except (ImportError, SyntaxError, AttributeError):\n        # Py2 - no async prefetching is supported\n        logging.warning(\n            'Asynchronous batch prefetching is not supported on Python 2. '\n            'Consider upgrading to Python 3 for improved performance.')\n        data = data.transform(batchify_fn)\n\n    num_update = 0\n    prefetched_iters = []\n    for _ in range(min(args.num_prefetch_epoch, args.epochs)):\n        prefetched_iters.append(iter(data))\n    for epoch in range(args.epochs):\n        if epoch + len(prefetched_iters) < args.epochs:\n            prefetched_iters.append(iter(data))\n        data_iter = prefetched_iters.pop(0)\n        try:\n            batches = ex.map(batchify_fn, data_iter)\n        except NameError:  # Py 2 or batch prefetching disabled\n            batches = data_iter\n\n        # Logging variables\n        log_wc = 0\n        log_start_time = time.time()\n        log_avg_loss = 0\n\n        for i, batch in enumerate(batches):\n            ctx = context[i % len(context)]\n            batch = [array.as_in_context(ctx) for array in batch]\n            with mx.autograd.record():\n                loss = embedding(*batch)\n            loss.backward()\n\n            num_update += loss.shape[0]\n            if len(context) == 1 or (i + 1) % len(context) == 0:\n                trainer.step(batch_size=1)\n\n            # Logging\n            log_wc += loss.shape[0]\n            log_avg_loss += loss.mean().as_in_context(context[0])\n            if (i + 1) % args.log_interval == 0:\n                # Forces waiting for computation by computing loss value\n                log_avg_loss = log_avg_loss.asscalar() / args.log_interval\n                wps = log_wc / (time.time() - log_start_time)\n                # Due to subsampling, the overall number of batches is an upper\n                # bound\n                num_batches = num_tokens // args.batch_size\n                if args.model.lower() == 'skipgram':\n                    num_batches = (num_tokens * args.window * 2) // args.batch_size\n                else:\n                    num_batches = num_tokens // args.batch_size\n                logging.info('[Epoch {} Batch {}/{}] loss={:.4f}, '\n                             'throughput={:.2f}K wps, wc={:.2f}K'.format(\n                                 epoch, i + 1, num_batches, log_avg_loss,\n                                 wps / 1000, log_wc / 1000))\n                log_start_time = time.time()\n                log_avg_loss = 0\n                log_wc = 0\n\n            if args.eval_interval and (i + 1) % args.eval_interval == 0:\n                with print_time('mx.nd.waitall()'):\n                    mx.nd.waitall()\n                with print_time('evaluate'):\n                    evaluate(args, embedding, vocab, num_update)\n\n    # Evaluate\n    with print_time('mx.nd.waitall()'):\n        mx.nd.waitall()\n    with print_time('evaluate'):\n        evaluate(args, embedding, vocab, num_update,\n                 eval_analogy=not args.no_eval_analogy)\n\n    # Save params\n    with print_time('save parameters'):\n        embedding.save_parameters(os.path.join(args.logdir, 'embedding.params'))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nevaluating the word2vec model for the current word.", "response": "def evaluate(args, embedding, vocab, global_step, eval_analogy=False):\n    \"\"\"Evaluation helper\"\"\"\n    if 'eval_tokens' not in globals():\n        global eval_tokens\n\n        eval_tokens_set = evaluation.get_tokens_in_evaluation_datasets(args)\n        if not args.no_eval_analogy:\n            eval_tokens_set.update(vocab.idx_to_token)\n\n        if not args.ngram_buckets:\n            # Word2Vec does not support computing vectors for OOV words\n            eval_tokens_set = filter(lambda t: t in vocab, eval_tokens_set)\n\n        eval_tokens = list(eval_tokens_set)\n\n    if not os.path.isdir(args.logdir):\n        os.makedirs(args.logdir)\n\n    # Compute their word vectors\n    context = get_context(args)\n    mx.nd.waitall()\n\n    token_embedding = nlp.embedding.TokenEmbedding(unknown_token=None,\n                                                   allow_extend=True)\n    token_embedding[eval_tokens] = embedding[eval_tokens]\n\n    results = evaluation.evaluate_similarity(\n        args, token_embedding, context[0], logfile=os.path.join(\n            args.logdir, 'similarity.tsv'), global_step=global_step)\n    if eval_analogy:\n        assert not args.no_eval_analogy\n        results += evaluation.evaluate_analogy(\n            args, token_embedding, context[0], logfile=os.path.join(\n                args.logdir, 'analogy.tsv'))\n\n    return results"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the dataset corresponds to the provided field.", "response": "def get_field(self, field):\n        \"\"\"Return the dataset corresponds to the provided key.\n\n        Example::\n            a = np.ones((2,2))\n            b = np.zeros((2,2))\n            np.savez('data.npz', a=a, b=b)\n            dataset = NumpyDataset('data.npz')\n            data_a = dataset.get_field('a')\n            data_b = dataset.get_field('b')\n\n        Parameters\n        ----------\n        field : str\n            The name of the field to retrieve.\n        \"\"\"\n        idx = self._keys.index(field)\n        return self._data[idx]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_final_text(pred_text, orig_text, tokenizer):\n\n    # When we created the data, we kept track of the alignment between original\n    # (whitespace tokenized) tokens and our WordPiece tokenized tokens. So\n    # now `orig_text` contains the span of our original text corresponding to the\n    # span that we predicted.\n    #\n    # However, `orig_text` may contain extra characters that we don't want in\n    # our prediction.\n    #\n    # For example, let's say:\n    #   pred_text = steve smith\n    #   orig_text = Steve Smith's\n    #\n    # We don't want to return `orig_text` because it contains the extra \"'s\".\n    #\n    # We don't want to return `pred_text` because it's already been normalized\n    # (the SQuAD eval script also does punctuation stripping/lower casing but\n    # our tokenizer does additional normalization like stripping accent\n    # characters).\n    #\n    # What we really want to return is \"Steve Smith\".\n    #\n    # Therefore, we have to apply a semi-complicated alignment heruistic between\n    # `pred_text` and `orig_text` to get a character-to-charcter alignment. This\n    # can fail in certain cases in which case we just return `orig_text`.\n\n    def _strip_spaces(text):\n        ns_chars = []\n        ns_to_s_map = OrderedDict()\n        for (i, c) in enumerate(text):\n            if c == ' ':\n                continue\n            ns_to_s_map[len(ns_chars)] = i\n            ns_chars.append(c)\n        ns_text = ''.join(ns_chars)\n        return (ns_text, ns_to_s_map)\n\n    # We first tokenize `orig_text`, strip whitespace from the result\n    # and `pred_text`, and check if they are the same length. If they are\n    # NOT the same length, the heuristic has failed. If they are the same\n    # length, we assume the characters are one-to-one aligned.\n\n    tok_text = ' '.join(tokenizer(orig_text))\n\n    start_position = tok_text.find(pred_text)\n    if start_position == -1:\n        return orig_text\n    end_position = start_position + len(pred_text) - 1\n\n    (orig_ns_text, orig_ns_to_s_map) = _strip_spaces(orig_text)\n    (tok_ns_text, tok_ns_to_s_map) = _strip_spaces(tok_text)\n\n    if len(orig_ns_text) != len(tok_ns_text):\n        return orig_text\n\n    # We then project the characters in `pred_text` back to `orig_text` using\n    # the character-to-character alignment.\n    tok_s_to_ns_map = {}\n    for (i, tok_index) in six.iteritems(tok_ns_to_s_map):\n        tok_s_to_ns_map[tok_index] = i\n\n    orig_start_position = None\n    if start_position in tok_s_to_ns_map:\n        ns_start_position = tok_s_to_ns_map[start_position]\n        if ns_start_position in orig_ns_to_s_map:\n            orig_start_position = orig_ns_to_s_map[ns_start_position]\n\n    if orig_start_position is None:\n        return orig_text\n\n    orig_end_position = None\n    if end_position in tok_s_to_ns_map:\n        ns_end_position = tok_s_to_ns_map[end_position]\n        if ns_end_position in orig_ns_to_s_map:\n            orig_end_position = orig_ns_to_s_map[ns_end_position]\n\n    if orig_end_position is None:\n        return orig_text\n\n    output_text = orig_text[orig_start_position:(orig_end_position + 1)]\n    return output_text", "response": "Project the tokenized prediction back to the original text."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef predictions(dev_dataset,\n                all_results,\n                tokenizer,\n                max_answer_length=64,\n                null_score_diff_threshold=0.0,\n                n_best_size=10,\n                version_2=False):\n    \"\"\"Get prediction results\n\n    Parameters\n    ----------\n    dev_dataset: dataset\n        Examples of transform.\n    all_results: dict\n        A dictionary containing model prediction results.\n    tokenizer: callable\n        Tokenizer function.\n    max_answer_length: int, default 64\n        Maximum length of the answer tokens.\n    null_score_diff_threshold: float, default 0.0\n        If null_score - best_non_null is greater than the threshold predict null.\n    n_best_size: int, default 10\n        The total number of n-best predictions.\n    version_2: bool, default False\n        If true, the SQuAD examples contain some that do not have an answer.\n    Returns\n    -------\n    all_predictions: dict\n        All final predictions.\n    all_nbest_json: dict\n        All n-best predictions.\n    scores_diff_json: dict\n        Record the difference between null score and the score of best non-null.\n        when version_2 is True.\n    \"\"\"\n\n    _PrelimPrediction = namedtuple('PrelimPrediction',\n                                   ['feature_index', 'start_index', 'end_index',\n                                    'start_logit', 'end_logit'])\n\n    _NbestPrediction = namedtuple(\n        'NbestPrediction', ['text', 'start_logit', 'end_logit'])\n\n    all_predictions = OrderedDict()\n    all_nbest_json = OrderedDict()\n    scores_diff_json = OrderedDict()\n\n    for features in dev_dataset:\n        results = all_results[features[0].example_id]\n        example_qas_id = features[0].qas_id\n        prelim_predictions = []\n\n        score_null = 1000000  # large and positive\n        min_null_feature_index = 0  # the paragraph slice with min mull score\n        null_start_logit = 0  # the start logit at the slice with min null score\n        null_end_logit = 0  # the end logit at the slice with min null score\n\n        for features_id, (result, feature) in enumerate(zip(results, features)):\n            start_indexes = _get_best_indexes(result.start_logits, n_best_size)\n            end_indexes = _get_best_indexes(result.end_logits, n_best_size)\n\n            if version_2:\n                feature_null_score = result.start_logits[0] + \\\n                    result.end_logits[0]\n                if feature_null_score < score_null:\n                    score_null = feature_null_score\n                    min_null_feature_index = features_id\n                    null_start_logit = result.start_logits[0]\n                    null_end_logit = result.end_logits[0]\n\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    # We could hypothetically create invalid predictions, e.g., predict\n                    # that the start of the span is in the question. We throw out all\n                    # invalid predictions.\n                    if start_index >= len(feature.tokens):\n                        continue\n                    if end_index >= len(feature.tokens):\n                        continue\n                    if start_index not in feature.token_to_orig_map:\n                        continue\n                    if end_index not in feature.token_to_orig_map:\n                        continue\n                    if not feature.token_is_max_context.get(start_index, False):\n                        continue\n                    if end_index < start_index:\n                        continue\n                    length = end_index - start_index + 1\n                    if length > max_answer_length:\n                        continue\n                    prelim_predictions.append(\n                        _PrelimPrediction(\n                            feature_index=features_id,\n                            start_index=start_index,\n                            end_index=end_index,\n                            start_logit=result.start_logits[start_index],\n                            end_logit=result.end_logits[end_index]))\n\n        if version_2:\n            prelim_predictions.append(\n                _PrelimPrediction(\n                    feature_index=min_null_feature_index,\n                    start_index=0,\n                    end_index=0,\n                    start_logit=null_start_logit,\n                    end_logit=null_end_logit))\n\n        prelim_predictions = sorted(\n            prelim_predictions,\n            key=lambda x: (x.start_logit + x.end_logit),\n            reverse=True)\n\n        seen_predictions = {}\n        nbest = []\n        for pred in prelim_predictions:\n            if len(nbest) >= n_best_size:\n                break\n            feature = features[pred.feature_index]\n            if pred.start_index > 0:  # this is a non-null prediction\n                tok_tokens = feature.tokens[pred.start_index:(\n                    pred.end_index + 1)]\n                orig_doc_start = feature.token_to_orig_map[pred.start_index]\n                orig_doc_end = feature.token_to_orig_map[pred.end_index]\n                orig_tokens = feature.doc_tokens[orig_doc_start:(\n                    orig_doc_end + 1)]\n                tok_text = ' '.join(tok_tokens)\n\n                # De-tokenize WordPieces that have been split off.\n                tok_text = tok_text.replace(' ##', '')\n                tok_text = tok_text.replace('##', '')\n\n                # Clean whitespace\n                tok_text = tok_text.strip()\n                tok_text = ' '.join(tok_text.split())\n                orig_text = ' '.join(orig_tokens)\n\n                final_text = get_final_text(tok_text, orig_text, tokenizer)\n                if final_text in seen_predictions:\n                    continue\n\n                seen_predictions[final_text] = True\n            else:\n                final_text = ''\n                seen_predictions[final_text] = True\n\n            nbest.append(\n                _NbestPrediction(\n                    text=final_text,\n                    start_logit=pred.start_logit,\n                    end_logit=pred.end_logit))\n\n        # if we didn't inlude the empty option in the n-best, inlcude it\n        if version_2:\n            if '' not in seen_predictions:\n                nbest.append(\n                    _NbestPrediction(\n                        text='',\n                        start_logit=null_start_logit,\n                        end_logit=null_end_logit))\n        # In very rare edge cases we could have no valid predictions. So we\n        # just create a nonce prediction in this case to avoid failure.\n        if not nbest:\n            nbest.append(\n                _NbestPrediction(text='empty', start_logit=0.0, end_logit=0.0))\n\n        assert len(nbest) >= 1\n\n        total_scores = []\n        best_non_null_entry = None\n        for entry in nbest:\n            total_scores.append(entry.start_logit + entry.end_logit)\n            if not best_non_null_entry:\n                if entry.text:\n                    best_non_null_entry = entry\n\n        probs = nd.softmax(nd.array(total_scores)).asnumpy()\n\n        nbest_json = []\n        for (i, entry) in enumerate(nbest):\n            output = OrderedDict()\n            output['text'] = entry.text\n            output['probability'] = float(probs[i])\n            output['start_logit'] = entry.start_logit\n            output['end_logit'] = entry.end_logit\n            nbest_json.append(output)\n\n        if not version_2:\n            all_predictions[example_qas_id] = nbest_json[0]['text']\n        else:\n            # predict '' iff the null score - the score of best non-null > threshold\n            score_diff = score_null - best_non_null_entry.start_logit - \\\n                best_non_null_entry.end_logit\n\n            scores_diff_json[example_qas_id] = score_diff\n            if score_diff > null_score_diff_threshold:\n                all_predictions[example_qas_id] = ''\n            else:\n                all_predictions[example_qas_id] = best_non_null_entry.text\n\n        all_nbest_json[example_qas_id] = nbest_json\n    return all_predictions, all_nbest_json, scores_diff_json", "response": "Get predictions from the SQuAD dataset."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_F1_EM(dataset, predict_data):\n    f1 = exact_match = total = 0\n    for record in dataset:\n        total += 1\n        if record[1] not in predict_data:\n            message = 'Unanswered question ' + record[1] + \\\n                ' will receive score 0.'\n            print(message)\n            continue\n        ground_truths = record[4]\n        prediction = predict_data[record[1]]\n        exact_match += metric_max_over_ground_truths(\n            exact_match_score, prediction, ground_truths)\n        f1 += metric_max_over_ground_truths(f1_score, prediction,\n                                            ground_truths)\n    exact_match = 100.0 * exact_match / total\n    f1 = 100.0 * f1 / total\n\n    scores = {'exact_match': exact_match, 'f1': f1}\n\n    return scores", "response": "Calculate the F1 and EM scores of the predicted results. Use only with the SQuAD1. 1 dataset."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef preprocess_data(tokenizer, task, batch_size, dev_batch_size, max_len, pad=False):\n    # transformation\n    trans = BERTDatasetTransform(\n        tokenizer,\n        max_len,\n        labels=task.get_labels(),\n        pad=pad,\n        pair=task.is_pair,\n        label_dtype='float32' if not task.get_labels() else 'int32')\n\n    data_train = task('train').transform(trans, lazy=False)\n    data_train_len = data_train.transform(\n        lambda input_id, length, segment_id, label_id: length)\n\n    num_samples_train = len(data_train)\n    # bucket sampler\n    batchify_fn = nlp.data.batchify.Tuple(\n        nlp.data.batchify.Pad(axis=0), nlp.data.batchify.Stack(),\n        nlp.data.batchify.Pad(axis=0),\n        nlp.data.batchify.Stack(\n            'float32' if not task.get_labels() else 'int32'))\n    batch_sampler = nlp.data.sampler.FixedBucketSampler(\n        data_train_len,\n        batch_size=batch_size,\n        num_buckets=10,\n        ratio=0,\n        shuffle=True)\n    # data loaders\n    dataloader_train = gluon.data.DataLoader(\n        dataset=data_train,\n        num_workers=1,\n        batch_sampler=batch_sampler,\n        batchify_fn=batchify_fn)\n    if task.task_name == 'MNLI':\n        data_dev_matched = task('dev_matched').transform(trans, lazy=False)\n        data_dev_mismatched = task('dev_mismatched').transform(trans, lazy=False)\n\n        dataloader_dev_matched = mx.gluon.data.DataLoader(\n            data_dev_matched, batch_size=dev_batch_size,\n            num_workers=1, shuffle=False, batchify_fn=batchify_fn)\n        dataloader_dev_mismatched = mx.gluon.data.DataLoader(\n            data_dev_mismatched, batch_size=dev_batch_size,\n            num_workers=1, shuffle=False, batchify_fn=batchify_fn)\n        return dataloader_train, dataloader_dev_matched, \\\n            dataloader_dev_mismatched, num_samples_train\n    else:\n        data_dev = task('dev').transform(trans, lazy=False)\n        dataloader_dev = mx.gluon.data.DataLoader(\n            data_dev,\n            batch_size=dev_batch_size,\n            num_workers=1,\n            shuffle=False,\n            batchify_fn=batchify_fn)\n        return dataloader_train, dataloader_dev, num_samples_train", "response": "Preprocess data for the current application."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef evaluate(dataloader_eval, metric):\n    metric.reset()\n    for _, seqs in enumerate(dataloader_eval):\n        input_ids, valid_len, type_ids, label = seqs\n        out = model(\n            input_ids.as_in_context(ctx), type_ids.as_in_context(ctx),\n            valid_len.astype('float32').as_in_context(ctx))\n        metric.update([label], [out])\n    metric_nm, metric_val = metric.get()\n    if not isinstance(metric_nm, list):\n        metric_nm = [metric_nm]\n        metric_val = [metric_val]\n    metric_str = 'validation metrics:' + ','.join(\n        [i + ':%.4f' for i in metric_nm])\n    logging.info(metric_str, *metric_val)", "response": "Evaluate the model on validation dataset."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef log_train(batch_id, batch_num, metric, step_loss, log_interval, epoch_id, learning_rate):\n    metric_nm, metric_val = metric.get()\n    if not isinstance(metric_nm, list):\n        metric_nm = [metric_nm]\n        metric_val = [metric_val]\n\n    train_str = '[Epoch %d Batch %d/%d] loss=%.4f, lr=%.7f, metrics:' + \\\n                ','.join([i + ':%.4f' for i in metric_nm])\n    logging.info(train_str, epoch_id + 1, batch_id + 1, batch_num, \\\n                 step_loss / log_interval, \\\n                 learning_rate, \\\n                 *metric_val)", "response": "Generate and print out the log message for training."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating and print out the log message for inference.", "response": "def log_inference(batch_id, batch_num, metric, step_loss, log_interval):\n    \"\"\"Generate and print out the log message for inference.\n    \"\"\"\n    metric_nm, metric_val = metric.get()\n    if not isinstance(metric_nm, list):\n        metric_nm = [metric_nm]\n        metric_val = [metric_val]\n\n    eval_str = '[Batch %d/%d] loss=%.4f, metrics:' + \\\n               ','.join([i + ':%.4f' for i in metric_nm])\n    logging.info(eval_str, batch_id + 1, batch_num, \\\n                 step_loss / log_interval, \\\n                 *metric_val)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef train(metric):\n\n    logging.info('Now we are doing BERT classification training on %s!', ctx)\n    optimizer_params = {'learning_rate': lr, 'epsilon': epsilon, 'wd': 0.01}\n    try:\n        trainer = gluon.Trainer(\n            model.collect_params(),\n            args.optimizer,\n            optimizer_params,\n            update_on_kvstore=False)\n    except ValueError as e:\n        print(e)\n        warnings.warn(\n            'AdamW optimizer is not found. Please consider upgrading to '\n            'mxnet>=1.5.0. Now the original Adam optimizer is used instead.')\n        trainer = gluon.Trainer(\n            model.collect_params(),\n            'adam',\n            optimizer_params,\n            update_on_kvstore=False)\n\n    step_size = batch_size * accumulate if accumulate else batch_size\n    num_train_steps = int(num_train_examples / step_size * args.epochs)\n    warmup_ratio = args.warmup_ratio\n    num_warmup_steps = int(num_train_steps * warmup_ratio)\n    step_num = 0\n\n    # Do not apply weight decay on LayerNorm and bias terms\n    for _, v in model.collect_params('.*beta|.*gamma|.*bias').items():\n        v.wd_mult = 0.0\n    # Collect differentiable parameters\n    params = [\n        p for p in model.collect_params().values() if p.grad_req != 'null'\n    ]\n    # Set grad_req if gradient accumulation is required\n    if accumulate:\n        for p in params:\n            p.grad_req = 'add'\n\n    for epoch_id in range(args.epochs):\n        metric.reset()\n        step_loss = 0\n        tic = time.time()\n        for batch_id, seqs in enumerate(train_data):\n            # set grad to zero for gradient accumulation\n            if accumulate:\n                if batch_id % accumulate == 0:\n                    model.collect_params().zero_grad()\n                    step_num += 1\n            else:\n                step_num += 1\n            # learning rate schedule\n            if step_num < num_warmup_steps:\n                new_lr = lr * step_num / num_warmup_steps\n            else:\n                offset = (step_num - num_warmup_steps) * lr / (\n                    num_train_steps - num_warmup_steps)\n                new_lr = lr - offset\n            trainer.set_learning_rate(new_lr)\n            # forward and backward\n            with mx.autograd.record():\n                input_ids, valid_length, type_ids, label = seqs\n                out = model(\n                    input_ids.as_in_context(ctx), type_ids.as_in_context(ctx),\n                    valid_length.astype('float32').as_in_context(ctx))\n                ls = loss_function(out, label.as_in_context(ctx)).mean()\n            ls.backward()\n            # update\n            if not accumulate or (batch_id + 1) % accumulate == 0:\n                trainer.allreduce_grads()\n                nlp.utils.clip_grad_global_norm(params, 1)\n                trainer.update(accumulate if accumulate else 1)\n            step_loss += ls.asscalar()\n            metric.update([label], [out])\n            if (batch_id + 1) % (args.log_interval) == 0:\n                log_train(batch_id, len(train_data), metric, step_loss, args.log_interval,\n                          epoch_id, trainer.learning_rate)\n                step_loss = 0\n        mx.nd.waitall()\n        if task.task_name == 'MNLI':\n            logging.info('On MNLI Matched: ')\n            evaluate(dev_data_matched, metric)\n            logging.info('On MNLI Mismatched: ')\n            evaluate(dev_data_mismatched, metric)\n        else:\n            evaluate(dev_data, metric)\n\n        # save params\n        params_saved = os.path.join(output_dir,\n                                    'model_bert_{0}_{1}.params'.format(task.task_name, epoch_id))\n        model.save_parameters(params_saved)\n        logging.info('params saved in : %s', params_saved)\n        toc = time.time()\n        logging.info('Time cost=%.2fs', toc - tic)\n        tic = toc", "response": "Train the BERT classification on the given context."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef preprocess_dataset(dataset, question_max_length, context_max_length):\n    vocab_provider = VocabProvider(dataset)\n    transformer = SQuADTransform(\n        vocab_provider, question_max_length, context_max_length)\n    processed_dataset = SimpleDataset(\n        dataset.transform(transformer, lazy=False))\n    return processed_dataset", "response": "Preprocess dataset by creating NDArray version of data\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_answer_spans(answer_list, answer_start_list):\n        return [(answer_start_list[i], answer_start_list[i] + len(answer))\n                for i, answer in enumerate(answer_list)]", "response": "Find all answer spans from the context returning start_index and end_index"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_word_level_vocab(self):\n\n        def simple_tokenize(source_str, token_delim=' ', seq_delim='\\n'):\n            return list(filter(None, re.split(token_delim + '|' + seq_delim, source_str)))\n\n        return VocabProvider._create_squad_vocab(simple_tokenize, self._dataset)", "response": "Provides word level vocabulary\n\n            Returns ------- Vocab\n            Word level vocabulary\n       "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nforwarding computation for the log - likelihood of a stack of states.", "response": "def hybrid_forward(self, F, *states): # pylint: disable=arguments-differ\n        \"\"\"\n        Parameters\n        ----------\n        states : list\n            the stack outputs from RNN, which consists of output from each time step (TNC).\n\n        Returns\n        --------\n        loss : NDArray\n            loss tensor with shape (batch_size,). Dimensions other than batch_axis are averaged out.\n        \"\"\"\n        # pylint: disable=unused-argument\n        if self._beta != 0:\n            if states:\n                means = [self._beta * (state[1:] - state[:-1]).__pow__(2).mean()\n                         for state in states[-1:]]\n                return F.add_n(*means)\n            else:\n                return F.zeros(1)\n        return F.zeros(1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _clean_text(self, text):\n        output = []\n        for char in text:\n            cp = ord(char)\n            if cp in (0, 0xfffd) or self._is_control(char):\n                continue\n            if self._is_whitespace(char):\n                output.append(' ')\n            else:\n                output.append(char)\n        return ''.join(output)", "response": "Performs invalid character removal and whitespace cleanup on text."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck whether the character is a control character.", "response": "def _is_control(self, char):\n        \"\"\"Checks whether `chars` is a control character.\"\"\"\n        # These are technically control characters but we count them as whitespace\n        # characters.\n        if char in ['\\t', '\\n', '\\r']:\n            return False\n        cat = unicodedata.category(char)\n        if cat.startswith('C'):\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _run_split_on_punc(self, text):\n        chars = list(text)\n        i = 0\n        start_new_word = True\n        output = []\n        while i < len(chars):\n            char = chars[i]\n            if self._is_punctuation(char):\n                output.append([char])\n                start_new_word = True\n            else:\n                if start_new_word:\n                    output.append([])\n                start_new_word = False\n                output[-1].append(char)\n            i += 1\n\n        return [''.join(x) for x in output]", "response": "Splits punctuation on a piece of text."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _is_punctuation(self, char):\n        cp = ord(char)\n        # We treat all non-letter/number ASCII as punctuation.\n        # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n        # Punctuation class but we treat them as punctuation anyways, for\n        # consistency.\n        group0 = cp >= 33 and cp <= 47\n        group1 = cp >= 58 and cp <= 64\n        group2 = cp >= 91 and cp <= 96\n        group3 = cp >= 123 and cp <= 126\n        if (group0 or group1 or group2 or group3):\n            return True\n        cat = unicodedata.category(char)\n        if cat.startswith('P'):\n            return True\n        return False", "response": "Checks whether the character is a punctuation character."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks whether characters is a whitespace character.", "response": "def _is_whitespace(self, char):\n        \"\"\"Checks whether `chars` is a whitespace character.\"\"\"\n        # \\t, \\n, and \\r are technically contorl characters but we treat them\n        # as whitespace since they are generally considered as such.\n        if char in [' ', '\\t', '\\n', '\\r']:\n            return True\n        cat = unicodedata.category(char)\n        if cat == 'Zs':\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrunning basic whitespace cleaning and splitting on a piece of text.", "response": "def _whitespace_tokenize(self, text):\n        \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n        text = text.strip()\n        tokens = text.split()\n        return tokens"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _truncate_seq_pair(self, tokens_a, tokens_b, max_length):\n        # This is a simple heuristic which will always truncate the longer sequence\n        # one token at a time. This makes more sense than truncating an equal percent\n        # of tokens from each, since if one sequence is very short then each token\n        # that's truncated likely contains more information than a longer sequence.\n        while True:\n            total_length = len(tokens_a) + len(tokens_b)\n            if total_length <= max_length:\n                break\n            if len(tokens_a) > len(tokens_b):\n                tokens_a.pop()\n            else:\n                tokens_b.pop()", "response": "Truncates a sequence pair in place to the maximum length."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconstructs the argument parser.", "response": "def get_args():\n    \"\"\"Construct the argument parser.\"\"\"\n    parser = argparse.ArgumentParser(\n        description='Word embedding evaluation with Gluon.',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    # Embeddings arguments\n    group = parser.add_argument_group('Embedding arguments')\n    group.add_argument('--embedding-path', type=str,\n                       help='Path to a .vec in Word2Vec text foramt or '\n                       '.bin binary fastText model file. ')\n    group.add_argument('--embedding-name', type=str,\n                       help=('Name of embedding type to load. '\n                             'Valid entries: {}'.format(\n                                 ', '.join(\n                                     nlp.embedding.list_sources().keys()))))\n    group.add_argument('--embedding-source', type=str,\n                       help=('Source from which to initialize the embedding.'\n                             'Pass --list-embedding-sources to get a list of '\n                             'valid sources for a given --embedding-name.'))\n    group.add_argument(\n        '--fasttext-load-ngrams',\n        action='store_true',\n        help=('Specify load_ngrams=True '\n              'when loading pretrained fastText embedding.'))\n    group.add_argument(\n        '--analogy-max-vocab-size', type=int, default=None,\n        help=('Only retain the X first tokens from the pre-trained embedding. '\n              'The tokens are ordered by decreasing frequency.'\n              'As the analogy task takes the whole vocabulary into account, '\n              'removing very infrequent words improves performance.'))\n    group.add_argument('--list-embedding-sources', action='store_true')\n\n    # Computation options\n    group = parser.add_argument_group('Computation arguments')\n    group.add_argument('--batch-size', type=int, default=1024,\n                       help='Batch size to use on analogy task. '\n                       'Decrease batch size if evaluation crashes.')\n    group.add_argument('--gpu', type=int,\n                       help=('Number (index) of GPU to run on, e.g. 0. '\n                             'If not specified, uses CPU.'))\n    group.add_argument('--no-hybridize', action='store_true',\n                       help='Disable hybridization of gluon HybridBlocks.')\n\n    # Logging\n    group = parser.add_argument_group('Logging arguments')\n    group.add_argument('--logdir', type=str, default='logs',\n                       help='Directory to store logs.')\n\n    # Evaluation options\n    evaluation.add_parameters(parser)\n\n    args = parser.parse_args()\n\n    validate_args(args)\n    evaluation.validate_args(args)\n\n    return args"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nvalidates arguments and act on help.", "response": "def validate_args(args):\n    \"\"\"Validate provided arguments and act on --help.\"\"\"\n    if args.list_embedding_sources:\n        print('Listing all sources for {} embeddings.'.format(\n            args.embedding_name))\n        print('Specify --embedding-name if you wish to '\n              'list sources of other embeddings')\n        print('')\n        if args.embedding_name not in nlp.embedding.list_sources().keys():\n            print('Invalid embedding name.')\n            print('Only {} are supported.'.format(', '.join(\n                nlp.embedding.list_sources().keys())))\n            sys.exit(1)\n        print(' '.join(nlp.embedding.list_sources()[args.embedding_name]))\n        sys.exit(0)\n\n    if not (args.embedding_path or args.embedding_name):\n        print('You must specify either --embedding-path or --embedding-name ')\n        print('Use --embedding-path to load and evaluate '\n              'word embeddings from a Word2Vec text format '\n              'or fastText binary format file')\n        print('Use --embedding-name or to download one of '\n              'the pre-trained embedding files included in GluonNLP.')\n        sys.exit(1)\n\n    if args.embedding_name and not args.embedding_source:\n        print('Please also specify --embedding-source'\n              ' to select the version of the pre-trained embedding. '\n              'Use --list-embedding-sources to see all available sources')\n        sys.exit(1)\n\n    print(args)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating the 2 - norm of gradients of parameters and how much they should be scaled down by max_norm.", "response": "def grad_global_norm(parameters, max_norm):\n    \"\"\"Calculate the 2-norm of gradients of parameters, and how much they should be scaled down\n    such that their 2-norm does not exceed `max_norm`.\n\n    If gradients exist for more than one context for a parameter, user needs to explicitly call\n    ``trainer.allreduce_grads`` so that the gradients are summed first before calculating\n    the 2-norm.\n\n    .. note::\n\n        This function is only for use when `update_on_kvstore` is set to False in trainer.\n\n    Example::\n\n        trainer = Trainer(net.collect_params(), update_on_kvstore=False, ...)\n        for x, y in mx.gluon.utils.split_and_load(X, [mx.gpu(0), mx.gpu(1)]):\n            with mx.autograd.record():\n                y = net(x)\n                loss = loss_fn(y, label)\n            loss.backward()\n        trainer.allreduce_grads()\n        norm, ratio = grad_global_norm(net.collect_params().values(), max_norm)\n        trainer.update(batch_size * ratio)\n        ...\n\n    Parameters\n    ----------\n    parameters : list of Parameters\n\n    Returns\n    -------\n    NDArray\n      Total norm. Shape is (1,)\n    NDArray\n      Ratio for rescaling gradients based on max_norm s.t. grad = grad / ratio.\n      If total norm is NaN, ratio will be NaN, too. Shape is (1,)\n    NDArray\n      Whether the total norm is finite. Shape is (1,)\n    \"\"\"\n    # collect gradient arrays\n    arrays = []\n    idx = 0\n    for p in parameters:\n        if p.grad_req != 'null':\n            p_grads = p.list_grad()\n            arrays.append(p_grads[idx % len(p_grads)])\n            idx += 1\n    assert len(arrays) > 0, 'No parameter found available for gradient norm.'\n\n    # compute gradient norms\n    def _norm(array):\n        # TODO(haibin) norm operator does not support fp16 safe reduction.\n        # Issue is tracked at: https://github.com/apache/incubator-mxnet/issues/14126\n        x = array.reshape((-1,)).astype('float32', copy=False)\n        return nd.dot(x, x)\n\n    norm_arrays = [_norm(arr) for arr in arrays]\n\n    # group norm arrays by ctx\n    def group_by_ctx(arr_list):\n        groups = collections.defaultdict(list)\n        for arr in arr_list:\n            ctx = arr.context\n            groups[ctx].append(arr)\n        return groups\n    norm_groups = group_by_ctx(norm_arrays)\n\n    # reduce\n    ctx, dtype = arrays[0].context, 'float32'\n    norms = [nd.add_n(*g).as_in_context(ctx) for g in norm_groups.values()]\n    total_norm = nd.add_n(*norms).sqrt()\n    scale = total_norm / max_norm\n    # is_finite = 0 if NaN or Inf, 1 otherwise.\n    is_finite = nd.contrib.isfinite(scale)\n    # if scale is finite, nd.maximum selects the max between scale and 1. That is,\n    # 1 is returned if total_norm does not exceed max_norm.\n    # if scale = NaN or Inf, the result of nd.minimum is undefined. Therefore, we use\n    # choices.take to return NaN or Inf.\n    scale_or_one = nd.maximum(nd.ones((1,), dtype=dtype, ctx=ctx), scale)\n    choices = nd.concat(scale, scale_or_one, dim=0)\n    chosen_scale = choices.take(is_finite)\n    return total_norm, chosen_scale, is_finite"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmaking one step of parameter update. Should be called after fp16_optimizer. forward and after record scope.", "response": "def step(self, batch_size, max_norm=None):\n        \"\"\"Makes one step of parameter update. Should be called after\n        `fp16_optimizer.backward()`, and outside of `record()` scope.\n\n        Parameters\n        ----------\n        batch_size : int\n            Batch size of data processed. Gradient will be normalized by `1/batch_size`.\n            Set this to 1 if you normalized loss manually with `loss = mean(loss)`.\n        max_norm : NDArray, optional, default is None\n            max value for global 2-norm of gradients.\n        \"\"\"\n        self.fp32_trainer.allreduce_grads()\n        step_size = batch_size * self._scaler.loss_scale\n        if max_norm:\n            norm, ratio, is_finite = grad_global_norm(self.fp32_trainer._params,\n                                                      max_norm * self._scaler.loss_scale)\n            step_size = ratio * step_size\n            if self._support_nan_check:\n                self.fp32_trainer.update(step_size)\n                overflow = is_finite.asscalar() < 1\n            else:\n                overflow = not np.isfinite(norm.asscalar())\n                if not overflow:\n                    self.fp32_trainer.update(step_size)\n        else:\n            # TODO(haibin) optimize the performance when max_norm is not present\n            # sequentially adding isnan/isinf results may be slow\n            if self._support_nan_check:\n                self.fp32_trainer.update(step_size)\n                overflow = self._scaler.has_overflow(self.fp32_trainer._params)\n            else:\n                overflow = self._scaler.has_overflow(self.fp32_trainer._params)\n                if not overflow:\n                    self.fp32_trainer.update(step_size)\n        # update scale based on overflow information\n        self._scaler.update_scale(overflow)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndetects inf and nan", "response": "def has_overflow(self, params):\n        \"\"\" detect inf and nan \"\"\"\n        is_not_finite = 0\n        for param in params:\n            if param.grad_req != 'null':\n                grad = param.list_grad()[0]\n                is_not_finite += mx.nd.contrib.isnan(grad).sum()\n                is_not_finite += mx.nd.contrib.isinf(grad).sum()\n        # NDArray is implicitly converted to bool\n        if is_not_finite == 0:\n            return False\n        else:\n            return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef stats(self):\n        ret = '{name}:\\n' \\\n            '  sample_num={sample_num}, batch_num={batch_num}\\n' \\\n            '  key={bucket_keys}\\n' \\\n            '  cnt={bucket_counts}\\n' \\\n            '  batch_size={bucket_batch_sizes}'\\\n            .format(name=self.__class__.__name__,\n                    sample_num=len(self._lengths),\n                    batch_num=len(self._batch_infos),\n                    bucket_keys=self._bucket_keys,\n                    bucket_counts=[len(sample_ids) for sample_ids in self._bucket_sample_ids],\n                    bucket_batch_sizes=self._bucket_batch_sizes)\n        return ret", "response": "Return a string representing the statistics of the bucketing sampler."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef train():\n    print(model)\n    from_epoch = 0\n    model.initialize(mx.init.Xavier(factor_type='out'), ctx=context)\n    trainer_params = {'learning_rate': args.lr, 'wd': 0, 'eps': args.eps}\n    trainer = gluon.Trainer(model.collect_params(), 'adagrad', trainer_params)\n    if args.from_epoch:\n        from_epoch = args.from_epoch\n        checkpoint_name = '%s.%s'%(args.save, format(from_epoch - 1, '02d'))\n        model.load_parameters(checkpoint_name)\n        trainer.load_states('%s.state'%args.save)\n        print('Loaded parameters from checkpoint %s'%(checkpoint_name))\n\n    model.hybridize(static_alloc=True, static_shape=True)\n    encoder_params = model.encoder.collect_params().values()\n    embedding_params = list(model.embedding.collect_params().values())\n    parallel_model = ParallelBigRNN(model, loss)\n    parallel = Parallel(len(context), parallel_model)\n    for epoch in range(from_epoch, args.epochs):\n        sys.stdout.flush()\n        total_L = 0.0\n        start_epoch_time = time.time()\n        start_log_interval_time = time.time()\n        hiddens = [model.begin_state(batch_size=args.batch_size,\n                                     func=mx.nd.zeros, ctx=ctx) for ctx in context]\n        nbatch = 0\n        has_next = True\n        train_data_iter = iter(train_data)\n        data, target, mask, sample = next(train_data_iter)\n\n        while has_next:\n            nbatch += 1\n            hiddens = detach(hiddens)\n            Ls = []\n            for _, batch in enumerate(zip(data, target, mask, sample, hiddens)):\n                parallel.put(batch)\n\n            for _ in range(len(data)):\n                hidden, ls = parallel.get()\n                # hidden states are ordered by context id\n                index = context.index(hidden[0].context)\n                hiddens[index] = hidden\n                Ls.append(ls)\n\n            # prefetch the next batch of data\n            try:\n                data, target, mask, sample = next(train_data_iter)\n            except StopIteration:\n                has_next = False\n\n            # rescale embedding grad\n            for ctx in context:\n                x = embedding_params[0].grad(ctx)\n                x[:] *= args.batch_size\n                encoder_grad = [p.grad(ctx) for p in encoder_params]\n                # perform gradient clipping per ctx\n                gluon.utils.clip_global_norm(encoder_grad, args.clip)\n\n            trainer.step(len(context))\n\n            total_L += sum([mx.nd.sum(L).asscalar() / args.bptt for L in Ls])\n\n            if nbatch % args.log_interval == 0:\n                cur_L = total_L / args.log_interval / len(context)\n                ppl = math.exp(cur_L) if cur_L < 100 else float('inf')\n                print('[Epoch %d Batch %d] loss %.2f, ppl %.2f, '\n                      'throughput %.2f samples/s'\n                      %(epoch, nbatch, cur_L, ppl,\n                        train_batch_size*args.log_interval/(time.time()-start_log_interval_time)))\n                total_L = 0.0\n                start_log_interval_time = time.time()\n                sys.stdout.flush()\n\n        end_epoch_time = time.time()\n        print('Epoch %d took %.2f seconds.'%(epoch, end_epoch_time - start_epoch_time))\n        mx.nd.waitall()\n        checkpoint_name = '%s.%s'%(args.save, format(epoch, '02d'))\n        model.save_parameters(checkpoint_name)\n        trainer.save_states('%s.state'%args.save)", "response": "Train the language model."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nevaluate the trained model for the current epoch", "response": "def evaluate():\n    \"\"\" Evaluate loop for the trained model \"\"\"\n    print(eval_model)\n    eval_model.initialize(mx.init.Xavier(), ctx=context[0])\n    eval_model.hybridize(static_alloc=True, static_shape=True)\n    epoch = args.from_epoch if args.from_epoch else 0\n    while epoch < args.epochs:\n        checkpoint_name = '%s.%s'%(args.save, format(epoch, '02d'))\n        if not os.path.exists(checkpoint_name):\n            print('Wait for a new checkpoint...')\n            # check again after 600 seconds\n            time.sleep(600)\n            continue\n        eval_model.load_parameters(checkpoint_name)\n        print('Loaded parameters from checkpoint %s'%(checkpoint_name))\n        start_epoch_time = time.time()\n        final_test_L = test(test_data, test_batch_size, ctx=context[0])\n        end_epoch_time = time.time()\n        print('[Epoch %d] test loss %.2f, test ppl %.2f'%\n              (epoch, final_test_L, math.exp(final_test_L)))\n        print('Epoch %d took %.2f seconds.'%(epoch, end_epoch_time - start_epoch_time))\n        sys.stdout.flush()\n        epoch += 1"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget home directory for storing datasets for pre - trained word embeddings", "response": "def get_home_dir():\n    \"\"\"Get home directory for storing datasets/models/pre-trained word embeddings\"\"\"\n    _home_dir = os.environ.get('MXNET_HOME', os.path.join('~', '.mxnet'))\n    # expand ~ to actual path\n    _home_dir = os.path.expanduser(_home_dir)\n    return _home_dir"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading dataset from tokenized files.", "response": "def read_dataset(args, dataset):\n    \"\"\"\n    Read dataset from tokenized files.\n    \"\"\"\n    path = os.path.join(vars(args)[dataset])\n    logger.info('reading data from {}'.format(path))\n    examples = [line.strip().split('\\t') for line in open(path)]\n    if args.max_num_examples > 0:\n        examples = examples[:args.max_num_examples]\n    # NOTE: assume data has been tokenized\n    dataset = gluon.data.SimpleDataset([(e[0], e[1], LABEL_TO_IDX[e[2]]) for e in examples])\n    dataset = dataset.transform(lambda s1, s2, label: (\n        ['NULL'] + s1.lower().split(),\n        ['NULL'] + s2.lower().split(), label),\n                                lazy=False)\n    logger.info('read {} examples'.format(len(dataset)))\n    return dataset"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nbuilds a vocab given a dataset.", "response": "def build_vocab(dataset):\n    \"\"\"\n    Build vocab given a dataset.\n    \"\"\"\n    counter = nlp.data.count_tokens([w for e in dataset for s in e[:2] for w in s],\n                                    to_lower=True)\n    vocab = nlp.Vocab(counter)\n    return vocab"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef prepare_data_loader(args, dataset, vocab, test=False):\n    # Preprocess\n    dataset = dataset.transform(lambda s1, s2, label: (vocab(s1), vocab(s2), label),\n                                lazy=False)\n\n    # Batching\n    batchify_fn = btf.Tuple(btf.Pad(), btf.Pad(), btf.Stack(dtype='int32'))\n    data_lengths = [max(len(d[0]), len(d[1])) for d in dataset]\n    batch_sampler = nlp.data.FixedBucketSampler(lengths=data_lengths,\n                                                batch_size=args.batch_size,\n                                                shuffle=(not test))\n    data_loader = gluon.data.DataLoader(dataset=dataset,\n                                        batch_sampler=batch_sampler,\n                                        batchify_fn=batchify_fn)\n    return data_loader", "response": "Read data and build data loader."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef mxnet_prefer_gpu():\n    gpu = int(os.environ.get('MXNET_GPU', default=0))\n    if gpu in mx.test_utils.list_gpus():\n        return mx.gpu(gpu)\n    return mx.cpu()", "response": "Returns the preferred GPU context."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ninitializing a logger with a file handler and a console handler.", "response": "def init_logger(root_dir, name=\"train.log\"):\n    \"\"\"Initialize a logger\n\n    Parameters\n    ----------\n    root_dir : str\n        directory for saving log\n    name : str\n        name of logger\n\n    Returns\n    -------\n    logger : logging.Logger\n        a logger\n    \"\"\"\n    os.makedirs(root_dir, exist_ok=True)\n    log_formatter = logging.Formatter(\"%(message)s\")\n    logger = logging.getLogger(name)\n    file_handler = logging.FileHandler(\"{0}/{1}\".format(root_dir, name), mode='w')\n    file_handler.setFormatter(log_formatter)\n    logger.addHandler(file_handler)\n    console_handler = logging.StreamHandler()\n    console_handler.setFormatter(log_formatter)\n    logger.addHandler(console_handler)\n    logger.setLevel(logging.INFO)\n    return logger"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nbuilding a standard LSTM cell with variational dropout initialized to be orthonormal", "response": "def orthonormal_VanillaLSTMBuilder(lstm_layers, input_dims, lstm_hiddens, dropout_x=0., dropout_h=0., debug=False):\n    \"\"\"Build a standard LSTM cell, with variational dropout,\n    with weights initialized to be orthonormal (https://arxiv.org/abs/1312.6120)\n\n    Parameters\n    ----------\n    lstm_layers : int\n        Currently only support one layer\n    input_dims : int\n        word vector dimensions\n    lstm_hiddens : int\n        hidden size\n    dropout_x : float\n        dropout on inputs, not used in this implementation, see `biLSTM` below\n    dropout_h : float\n        dropout on hidden states\n    debug : bool\n        set to True to skip orthonormal initialization\n\n    Returns\n    -------\n    lstm_cell : VariationalDropoutCell\n        A LSTM cell\n    \"\"\"\n    assert lstm_layers == 1, 'only accept one layer lstm'\n    W = orthonormal_initializer(lstm_hiddens, lstm_hiddens + input_dims, debug)\n    W_h, W_x = W[:, :lstm_hiddens], W[:, lstm_hiddens:]\n    b = nd.zeros((4 * lstm_hiddens,))\n    b[lstm_hiddens:2 * lstm_hiddens] = -1.0\n    lstm_cell = rnn.LSTMCell(input_size=input_dims, hidden_size=lstm_hiddens,\n                             i2h_weight_initializer=mx.init.Constant(np.concatenate([W_x] * 4, 0)),\n                             h2h_weight_initializer=mx.init.Constant(np.concatenate([W_h] * 4, 0)),\n                             h2h_bias_initializer=mx.init.Constant(b))\n    wrapper = VariationalDropoutCell(lstm_cell, drop_states=dropout_h)\n    return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef biLSTM(f_lstm, b_lstm, inputs, batch_size=None, dropout_x=0., dropout_h=0.):\n    for f, b in zip(f_lstm, b_lstm):\n        inputs = nd.Dropout(inputs, dropout_x, axes=[0])  # important for variational dropout\n        fo, fs = f.unroll(length=inputs.shape[0], inputs=inputs, layout='TNC', merge_outputs=True)\n        bo, bs = b.unroll(length=inputs.shape[0], inputs=inputs.flip(axis=0), layout='TNC', merge_outputs=True)\n        f.reset(), b.reset()\n        inputs = nd.concat(fo, bo.flip(axis=0), dim=2)\n    return inputs", "response": "Feature extraction through BiLSTM"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndo xWyMacro for bilinear.", "response": "def bilinear(x, W, y, input_size, seq_len, batch_size, num_outputs=1, bias_x=False, bias_y=False):\n    \"\"\"Do xWy\n\n    Parameters\n    ----------\n    x : NDArray\n        (input_size x seq_len) x batch_size\n    W : NDArray\n        (num_outputs x ny) x nx\n    y : NDArray\n        (input_size x seq_len) x batch_size\n    input_size : int\n        input dimension\n    seq_len : int\n        sequence length\n    batch_size : int\n        batch size\n    num_outputs : int\n        number of outputs\n    bias_x : bool\n        whether concat bias vector to input x\n    bias_y : bool\n        whether concat bias vector to input y\n\n    Returns\n    -------\n    output : NDArray\n        [seq_len_y x seq_len_x if output_size == 1 else seq_len_y x num_outputs x seq_len_x] x batch_size\n    \"\"\"\n    if bias_x:\n        x = nd.concat(x, nd.ones((1, seq_len, batch_size)), dim=0)\n    if bias_y:\n        y = nd.concat(y, nd.ones((1, seq_len, batch_size)), dim=0)\n\n    nx, ny = input_size + bias_x, input_size + bias_y\n    # W: (num_outputs x ny) x nx\n    lin = nd.dot(W, x)\n    if num_outputs > 1:\n        lin = reshape_fortran(lin, (ny, num_outputs * seq_len, batch_size))\n    y = y.transpose([2, 1, 0])  # May cause performance issues\n    lin = lin.transpose([2, 1, 0])\n    blin = nd.batch_dot(lin, y, transpose_b=True)\n    blin = blin.transpose([2, 1, 0])\n    if num_outputs > 1:\n        blin = reshape_fortran(blin, (seq_len, num_outputs, seq_len, batch_size))\n    return blin"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfixes the relation prediction by heuristic rules", "response": "def rel_argmax(rel_probs, length, ensure_tree=True):\n    \"\"\"Fix the relation prediction by heuristic rules\n\n    Parameters\n    ----------\n    rel_probs : NDArray\n        seq_len x rel_size\n    length :\n        real sentence length\n    ensure_tree :\n        whether to apply rules\n    Returns\n    -------\n    rel_preds : np.ndarray\n        prediction of relations of size (seq_len,)\n    \"\"\"\n    if ensure_tree:\n        rel_probs[:, ParserVocabulary.PAD] = 0\n        root = ParserVocabulary.ROOT\n        tokens = np.arange(1, length)\n        rel_preds = np.argmax(rel_probs, axis=1)\n        roots = np.where(rel_preds[tokens] == root)[0] + 1\n        if len(roots) < 1:\n            rel_preds[1 + np.argmax(rel_probs[tokens, root])] = root\n        elif len(roots) > 1:\n            root_probs = rel_probs[roots, root]\n            rel_probs[roots, root] = 0\n            new_rel_preds = np.argmax(rel_probs[roots], axis=1)\n            new_rel_probs = rel_probs[roots, new_rel_preds] / root_probs\n            new_root = roots[np.argmin(new_rel_probs)]\n            rel_preds[roots] = new_rel_preds\n            rel_preds[new_root] = root\n        return rel_preds\n    else:\n        rel_probs[:, ParserVocabulary.PAD] = 0\n        rel_preds = np.argmax(rel_probs, axis=1)\n        return rel_preds"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef reshape_fortran(tensor, shape):\n    return tensor.T.reshape(tuple(reversed(shape))).T", "response": "The missing Fortran reshape for mx. NDArray\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nupdates the progress bar.", "response": "def update(self, current, values=[], exact=[], strict=[]):\n        \"\"\"\n        Updates the progress bar.\n        # Arguments\n            current: Index of current step.\n            values: List of tuples (name, value_for_last_step).\n                The progress bar will display averages for these values.\n            exact: List of tuples (name, value_for_last_step).\n                The progress bar will display these values directly.\n        \"\"\"\n\n        for k, v in values:\n            if k not in self.sum_values:\n                self.sum_values[k] = [v * (current - self.seen_so_far), current - self.seen_so_far]\n                self.unique_values.append(k)\n            else:\n                self.sum_values[k][0] += v * (current - self.seen_so_far)\n                self.sum_values[k][1] += (current - self.seen_so_far)\n\n        for cells in exact:\n            k, v, w = cells[0], cells[1], 4\n            if len(cells) == 3:\n                w = cells[2]\n            if k not in self.sum_values:\n                self.unique_values.append(k)\n            self.sum_values[k] = [v, 1, w]\n\n        for k, v in strict:\n            if k not in self.sum_values:\n                self.unique_values.append(k)\n            self.sum_values[k] = v\n\n        self.seen_so_far = current\n\n        now = time.time()\n        if self.verbose == 1:\n            prev_total_width = self.total_width\n            sys.stdout.write(\"\\b\" * prev_total_width)\n            sys.stdout.write(\"\\r\")\n\n            numdigits = 0 if self.target == 0 or math.isnan(self.target) else int(np.floor(np.log10(self.target))) + 1\n            barstr = '%%%dd/%%%dd [' % (numdigits, numdigits)\n            bar = barstr % (current, self.target)\n            prog = 0 if self.target == 0 else float(current) / self.target\n            prog_width = int(self.width * prog)\n            if prog_width > 0:\n                bar += ('=' * (prog_width - 1))\n                if current < self.target:\n                    bar += '>'\n                else:\n                    bar += '='\n            bar += ('.' * (self.width - prog_width))\n            bar += ']'\n            sys.stdout.write(bar)\n            self.total_width = len(bar)\n\n            if current:\n                time_per_unit = (now - self.start) / current\n            else:\n                time_per_unit = 0\n            eta = time_per_unit * (self.target - current)\n            info = ''\n            if current < self.target:\n                info += ' - ETA: %ds' % eta\n            else:\n                info += ' - %ds' % (now - self.start)\n            for k in self.unique_values:\n                if type(self.sum_values[k]) is list:\n                    info += (' - %s: %.' + str(self.sum_values[k][2]) + 'f') % (\n                        k, self.sum_values[k][0] / max(1, self.sum_values[k][1]))\n                else:\n                    info += ' - %s: %s' % (k, self.sum_values[k])\n\n            self.total_width += len(info)\n            if prev_total_width > self.total_width:\n                info += ((prev_total_width - self.total_width) * \" \")\n\n            sys.stdout.write(info)\n            sys.stdout.flush()\n\n            if current >= self.target:\n                sys.stdout.write(\"\\n\")\n\n        if self.verbose == 2:\n            if current >= self.target:\n                info = '%ds' % (now - self.start)\n                for k in self.unique_values:\n                    info += ' - %s: %.4f' % (k, self.sum_values[k][0] / max(1, self.sum_values[k][1]))\n                sys.stdout.write(info + \"\\n\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets mini - batches of the dataset.", "response": "def get_batch(data_source, i, seq_len=None):\n    \"\"\"Get mini-batches of the dataset.\n\n    Parameters\n    ----------\n    data_source : NDArray\n        The dataset is evaluated on.\n    i : int\n        The index of the batch, starting from 0.\n    seq_len : int\n        The length of each sample in the batch.\n\n    Returns\n    -------\n    data: NDArray\n        The context\n    target: NDArray\n        The words to predict\n    \"\"\"\n    seq_len = min(seq_len if seq_len else args.bptt, len(data_source) - 1 - i)\n    data = data_source[i:i+seq_len]\n    target = data_source[i+1:i+1+seq_len]\n    return data, target"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nevaluating the model on the dataset.", "response": "def evaluate(data_source, batch_size, params_file_name, ctx=None):\n    \"\"\"Evaluate the model on the dataset.\n\n    Parameters\n    ----------\n    data_source : NDArray\n        The dataset is evaluated on.\n    batch_size : int\n        The size of the mini-batch.\n    params_file_name : str\n        The parameter file to use to evaluate,\n        e.g., val.params or args.save\n    ctx : mx.cpu() or mx.gpu()\n        The context of the computation.\n\n    Returns\n    -------\n    loss: float\n        The loss on the dataset\n    \"\"\"\n\n    total_L = 0.0\n    ntotal = 0\n\n    model_eval.load_parameters(params_file_name, context)\n\n    hidden = model_eval.begin_state(batch_size=batch_size, func=mx.nd.zeros, ctx=context[0])\n    i = 0\n    while i < len(data_source) - 1 - 1:\n        data, target = get_batch(data_source, i, seq_len=args.bptt)\n        data = data.as_in_context(ctx)\n        target = target.as_in_context(ctx)\n        output, hidden = model_eval(data, hidden)\n        hidden = detach(hidden)\n        L = loss(output.reshape(-3, -1),\n                 target.reshape(-1,))\n        total_L += mx.nd.sum(L).asscalar()\n        ntotal += L.size\n        i += args.bptt\n    return total_L / ntotal"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef train():\n    ntasgd = False\n    best_val = float('Inf')\n    start_train_time = time.time()\n    parameters = model.collect_params()\n    param_dict_avg = None\n    t = 0\n    avg_trigger = 0\n    n = 5\n    valid_losses = []\n    for epoch in range(args.epochs):\n        total_L = 0.0\n        start_epoch_time = time.time()\n        start_log_interval_time = time.time()\n        hiddens = [model.begin_state(args.batch_size//len(context),\n                                     func=mx.nd.zeros, ctx=ctx) for ctx in context]\n        batch_i, i = 0, 0\n        while i < len(train_data) - 1 - 1:\n            bptt = args.bptt if mx.nd.random.uniform().asscalar() < 0.95 else args.bptt / 2\n            seq_len = max(5, int(mx.nd.random.normal(bptt, 5).asscalar()))\n            lr_batch_start = trainer.learning_rate\n            trainer.set_learning_rate(lr_batch_start*seq_len/args.bptt)\n\n            data, target = get_batch(train_data, i, seq_len=seq_len)\n            data_list = gluon.utils.split_and_load(data, context, batch_axis=1, even_split=True)\n            target_list = gluon.utils.split_and_load(target, context, batch_axis=1, even_split=True)\n            hiddens = detach(hiddens)\n            Ls = []\n            with autograd.record():\n                for j, (X, y, h) in enumerate(zip(data_list, target_list, hiddens)):\n                    output, h, encoder_hs, dropped_encoder_hs = model(X, h)\n                    l = joint_loss(output, y, encoder_hs, dropped_encoder_hs)\n                    Ls.append(l / (len(context) * X.size))\n                    hiddens[j] = h\n            for L in Ls:\n                L.backward()\n\n            grads = [p.grad(d.context) for p in parameters.values() for d in data_list]\n            gluon.utils.clip_global_norm(grads, args.clip)\n\n            if args.ntasgd and ntasgd:\n                if param_dict_avg is None:\n                    param_dict_avg = {k.split(model._prefix)[1]: v.data(context[0]).copy()\n                                      for k, v in parameters.items()}\n\n            trainer.step(1)\n\n            if args.ntasgd and ntasgd:\n                gamma = 1.0 / max(1, epoch * (len(train_data) // args.bptt)\n                                  + batch_i - avg_trigger + 2)\n                for name, param_avg in param_dict_avg.items():\n                    param_avg[:] += gamma * (parameters['{}{}'.format(model._prefix, name)]\n                                             .data(context[0]) - param_avg)\n\n            total_L += sum([mx.nd.sum(L).asscalar() for L in Ls])\n            trainer.set_learning_rate(lr_batch_start)\n\n            if batch_i % args.log_interval == 0 and batch_i > 0:\n                cur_L = total_L / args.log_interval\n                print('[Epoch %d Batch %d/%d] current loss %.2f, ppl %.2f, '\n                      'throughput %.2f samples/s, lr %.2f'\n                      % (epoch, batch_i, len(train_data) // args.bptt, cur_L, math.exp(cur_L),\n                         args.batch_size * args.log_interval\n                         / (time.time() - start_log_interval_time),\n                         lr_batch_start * seq_len / args.bptt))\n                total_L = 0.0\n                start_log_interval_time = time.time()\n            i += seq_len\n            batch_i += 1\n\n        mx.nd.waitall()\n\n        print('[Epoch %d] throughput %.2f samples/s' % (\n            epoch, (args.batch_size * len(train_data)) / (time.time() - start_epoch_time)))\n\n        if args.ntasgd and ntasgd:\n            mx.nd.save('{}.val.params'.format(args.save), param_dict_avg)\n        else:\n            model.save_parameters('{}.val.params'.format(args.save))\n        val_L = evaluate(val_data, val_batch_size, '{}.val.params'.format(args.save), context[0])\n        print('[Epoch %d] time cost %.2fs, valid loss %.2f, valid ppl %.2f\uff0clr %.2f' % (\n            epoch, time.time() - start_epoch_time, val_L, math.exp(val_L),\n            trainer.learning_rate))\n\n        if args.ntasgd and avg_trigger == 0:\n            if t > n and val_L > min(valid_losses[-n:]):\n                if param_dict_avg is None:\n                    param_dict_avg = {k.split(model._prefix)[1]: v.data(context[0]).copy()\n                                      for k, v in parameters.items()}\n                else:\n                    for k, v in parameters.items():\n                        param_dict_avg[k.split(model._prefix)[1]] \\\n                            = v.data(context[0]).copy()\n                avg_trigger = epoch * (len(train_data) // args.bptt) + len(train_data) // args.bptt\n                print('Switching to NTASGD and avg_trigger is : %d' % avg_trigger)\n                ntasgd = True\n            valid_losses.append(val_L)\n            t += 1\n\n        if val_L < best_val:\n            update_lr_epoch = 0\n            best_val = val_L\n            if args.ntasgd and ntasgd:\n                mx.nd.save(args.save, param_dict_avg)\n            else:\n                model.save_parameters(args.save)\n            test_L = evaluate(test_data, test_batch_size, args.save, context[0])\n            print('[Epoch %d] test loss %.2f, test ppl %.2f'\n                  % (epoch, test_L, math.exp(test_L)))\n        else:\n            update_lr_epoch += 1\n            if update_lr_epoch % args.lr_update_interval == 0 and update_lr_epoch != 0:\n                lr_scale = trainer.learning_rate * args.lr_update_factor\n                print('Learning rate after interval update %f' % lr_scale)\n                trainer.set_learning_rate(lr_scale)\n                update_lr_epoch = 0\n\n    print('Total training throughput %.2f samples/s'\n          % ((args.batch_size * len(train_data) * args.epochs) / (time.time() - start_train_time)))", "response": "Train the model for awd language."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nregister a new word embedding evaluation function.", "response": "def register(class_):\n    \"\"\"Registers a new word embedding evaluation function.\n\n    Once registered, we can create an instance with\n    :func:`~gluonnlp.embedding.evaluation.create`.\n\n    Examples\n    --------\n    >>> @gluonnlp.embedding.evaluation.register\n    ... class MySimilarityFunction(gluonnlp.embedding.evaluation.WordEmbeddingSimilarityFunction):\n    ...     def __init__(self, eps=1e-10):\n    ...         pass\n    >>> similarity_function = gluonnlp.embedding.evaluation.create('similarity',\n    ...                                                            'MySimilarityFunction')\n    >>> print(type(similarity_function))\n    <class 'MySimilarityFunction'>\n\n    >>> @gluonnlp.embedding.evaluation.register\n    ... class MyAnalogyFunction(gluonnlp.embedding.evaluation.WordEmbeddingAnalogyFunction):\n    ...     def __init__(self, k=1, eps=1E-10):\n    ...         pass\n    >>> analogy_function = gluonnlp.embedding.evaluation.create('analogy', 'MyAnalogyFunction')\n    >>> print(type(analogy_function))\n    <class 'MyAnalogyFunction'>\n\n    \"\"\"\n\n    if issubclass(class_, WordEmbeddingSimilarityFunction):\n        register_ = registry.get_register_func(\n            WordEmbeddingSimilarityFunction,\n            'word embedding similarity evaluation function')\n    elif issubclass(class_, WordEmbeddingAnalogyFunction):\n        register_ = registry.get_register_func(\n            WordEmbeddingAnalogyFunction,\n            'word embedding analogy evaluation function')\n    else:\n        raise RuntimeError(\n            'The custom function must either subclass '\n            'WordEmbeddingSimilarityFunction or WordEmbeddingAnalogyFunction')\n\n    return register_(class_)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create(kind, name, **kwargs):\n    if kind not in _REGSITRY_KIND_CLASS_MAP.keys():\n        raise KeyError(\n            'Cannot find `kind` {}. Use '\n            '`list_evaluation_functions(kind=None).keys()` to get'\n            'all the valid kinds of evaluation functions.'.format(kind))\n\n    create_ = registry.get_create_func(\n        _REGSITRY_KIND_CLASS_MAP[kind],\n        'word embedding {} evaluation function'.format(kind))\n\n    return create_(name, **kwargs)", "response": "Creates an instance of a registered word embedding evaluation function."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of all the valid word embedding functions names for the specified kind.", "response": "def list_evaluation_functions(kind=None):\n    \"\"\"Get valid word embedding functions names.\n\n    Parameters\n    ----------\n    kind : ['similarity', 'analogy', None]\n        Return only valid names for similarity, analogy or both kinds of functions.\n\n    Returns\n    -------\n    dict or list:\n        A list of all the valid evaluation function names for the specified\n        kind. If kind is set to None, returns a dict mapping each valid name to\n        its respective output list. The valid names can be plugged in\n        `gluonnlp.model.word_evaluation_model.create(name)`.\n\n    \"\"\"\n\n    if kind is None:\n        kind = tuple(_REGSITRY_KIND_CLASS_MAP.keys())\n\n    if not isinstance(kind, tuple):\n        if kind not in _REGSITRY_KIND_CLASS_MAP.keys():\n            raise KeyError(\n                'Cannot find `kind` {}. Use '\n                '`list_evaluation_functions(kind=None).keys()` to get all the'\n                'valid kinds of evaluation functions.'.format(kind))\n\n        reg = registry.get_registry(_REGSITRY_KIND_CLASS_MAP[kind])\n        return list(reg.keys())\n    else:\n        return {name: list_evaluation_functions(kind=name) for name in kind}"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\npredicting the similarity of words1 and words2.", "response": "def hybrid_forward(self, F, words1, words2, weight):  # pylint: disable=arguments-differ\n        \"\"\"Predict the similarity of words1 and words2.\n\n        Parameters\n        ----------\n        words1 : Symbol or NDArray\n            The indices of the words the we wish to compare to the words in words2.\n        words2 : Symbol or NDArray\n            The indices of the words the we wish to compare to the words in words1.\n\n        Returns\n        -------\n        similarity : Symbol or NDArray\n            The similarity computed by WordEmbeddingSimilarity.similarity_function.\n        \"\"\"\n        embeddings_words1 = F.Embedding(words1, weight,\n                                        input_dim=self._vocab_size,\n                                        output_dim=self._embed_size)\n        embeddings_words2 = F.Embedding(words2, weight,\n                                        input_dim=self._vocab_size,\n                                        output_dim=self._embed_size)\n        similarity = self.similarity(embeddings_words1, embeddings_words2)\n        return similarity"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute the analogies for given question words.", "response": "def hybrid_forward(self, F, words1, words2, words3):  # pylint: disable=arguments-differ, unused-argument\n        \"\"\"Compute analogies for given question words.\n\n        Parameters\n        ----------\n        words1 : Symbol or NDArray\n            Word indices of first question words. Shape (batch_size, ).\n        words2 : Symbol or NDArray\n            Word indices of second question words. Shape (batch_size, ).\n        words3 : Symbol or NDArray\n            Word indices of third question words. Shape (batch_size, ).\n\n        Returns\n        -------\n        predicted_indices : Symbol or NDArray\n            Indices of predicted analogies of shape (batch_size, k)\n        \"\"\"\n        return self.analogy(words1, words2, words3)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef evaluate(data_source, batch_size, ctx=None):\n    total_L = 0\n    hidden = cache_cell.\\\n        begin_state(func=mx.nd.zeros, batch_size=batch_size, ctx=context[0])\n    next_word_history = None\n    cache_history = None\n    for i in range(0, len(data_source) - 1, args.bptt):\n        if i > 0:\n            print('Batch %d/%d, ppl %f'%\n                  (i, len(data_source), math.exp(total_L/i)))\n        data, target = get_batch(data_source, i)\n        data = data.as_in_context(ctx)\n        target = target.as_in_context(ctx)\n        L = 0\n        outs, next_word_history, cache_history, hidden = \\\n            cache_cell(data, target, next_word_history, cache_history, hidden)\n        for out in outs:\n            L += (-mx.nd.log(out)).asscalar()\n        total_L += L / data.shape[1]\n        hidden = detach(hidden)\n    return total_L / len(data_source)", "response": "Evaluate the model on the dataset with cache model."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_model(name, dataset_name='wikitext-2', **kwargs):\n    models = {'bert_12_768_12': bert_12_768_12,\n              'bert_24_1024_16': bert_24_1024_16}\n    name = name.lower()\n    if name not in models:\n        raise ValueError(\n            'Model %s is not supported. Available options are\\n\\t%s' % (\n                name, '\\n\\t'.join(sorted(models.keys()))))\n    kwargs['dataset_name'] = dataset_name\n    return models[name](**kwargs)", "response": "Returns a pre - defined model by name."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef bert_12_768_12(dataset_name=None, vocab=None, pretrained=True, ctx=mx.cpu(),\n                   root=os.path.join(get_home_dir(), 'models'), use_pooler=True,\n                   use_decoder=True, use_classifier=True, input_size=None, seq_length=None,\n                   **kwargs):\n    \"\"\"Static BERT BASE model.\n\n    The number of layers (L) is 12, number of units (H) is 768, and the\n    number of self-attention heads (A) is 12.\n\n    Parameters\n    ----------\n    dataset_name : str or None, default None\n        Options include 'book_corpus_wiki_en_cased', 'book_corpus_wiki_en_uncased',\n        'wiki_cn_cased', 'wiki_multilingual_uncased' and 'wiki_multilingual_cased'.\n    vocab : gluonnlp.vocab.BERTVocab or None, default None\n        Vocabulary for the dataset. Must be provided if dataset is not specified.\n    pretrained : bool, default True\n        Whether to load the pretrained weights for model.\n    ctx : Context, default CPU\n        The context in which to load the pretrained weights.\n    root : str, default '$MXNET_HOME/models'\n        Location for keeping the model parameters.\n        MXNET_HOME defaults to '~/.mxnet'.\n    use_pooler : bool, default True\n        Whether to include the pooler which converts the encoded sequence tensor of shape\n        (batch_size, seq_length, units) to a tensor of shape (batch_size, units)\n        for for segment level classification task.\n    use_decoder : bool, default True\n        Whether to include the decoder for masked language model prediction.\n    use_classifier : bool, default True\n        Whether to include the classifier for next sentence classification.\n    input_size : int, default None\n        Represents the embedding size of the input.\n    seq_length : int, default None\n        Stands for the sequence length of the input.\n\n    Returns\n    -------\n    StaticBERTModel, gluonnlp.vocab.BERTVocab\n    \"\"\"\n    return get_static_bert_model(model_name='bert_12_768_12', vocab=vocab,\n                                 dataset_name=dataset_name, pretrained=pretrained, ctx=ctx,\n                                 use_pooler=use_pooler, use_decoder=use_decoder,\n                                 use_classifier=use_classifier, root=root, input_size=input_size,\n                                 seq_length=seq_length, **kwargs)", "response": "Static BERT BASE model."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef hybrid_forward(self, F, inputs, token_types, valid_length=None, masked_positions=None):\n        # pylint: disable=arguments-differ\n        # pylint: disable=unused-argument\n        \"\"\"Generate the representation given the inputs.\n\n        This is used in training or fine-tuning a static (hybridized) BERT model.\n        \"\"\"\n        outputs = []\n        seq_out, attention_out = self._encode_sequence(F, inputs, token_types, valid_length)\n        outputs.append(seq_out)\n\n        if self.encoder._output_all_encodings:\n            assert isinstance(seq_out, list)\n            output = seq_out[-1]\n        else:\n            output = seq_out\n\n        if attention_out:\n            outputs.append(attention_out)\n\n        if self._use_pooler:\n            pooled_out = self._apply_pooling(output)\n            outputs.append(pooled_out)\n            if self._use_classifier:\n                next_sentence_classifier_out = self.classifier(pooled_out)\n                outputs.append(next_sentence_classifier_out)\n        if self._use_decoder:\n            assert masked_positions is not None, \\\n                'masked_positions tensor is required for decoding masked language model'\n            decoder_out = self._decode(output, masked_positions)\n            outputs.append(decoder_out)\n        return tuple(outputs) if len(outputs) > 1 else outputs[0]", "response": "Generate the representation given the inputs."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_parameters(self, filename, ctx=mx.cpu()): # pylint: disable=arguments-differ\n        self.lm_model.load_parameters(filename, ctx=ctx)", "response": "Load parameters from file."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndefines the forward computation for cache cell.", "response": "def forward(self, inputs, target, next_word_history, cache_history, begin_state=None): # pylint: disable=arguments-differ\n        \"\"\"Defines the forward computation for cache cell. Arguments can be either\n        :py:class:`NDArray` or :py:class:`Symbol`.\n\n        Parameters\n        ----------\n        inputs: NDArray\n            The input data\n        target: NDArray\n            The label\n        next_word_history: NDArray\n            The next word in memory\n        cache_history: NDArray\n            The hidden state in cache history\n\n\n        Returns\n        --------\n        out: NDArray\n            The linear interpolation of the cache language model\n            with the regular word-level language model\n        next_word_history: NDArray\n            The next words to be kept in the memory for look up\n            (size is equal to the window size)\n        cache_history: NDArray\n            The hidden states to be kept in the memory for look up\n            (size is equal to the window size)\n        \"\"\"\n        output, hidden, encoder_hs, _ = \\\n            super(self.lm_model.__class__, self.lm_model).\\\n                forward(inputs, begin_state)\n        encoder_h = encoder_hs[-1].reshape(-3, -2)\n        output = output.reshape(-1, self._vocab_size)\n\n        start_idx = len(next_word_history) \\\n            if next_word_history is not None else 0\n        next_word_history = nd.concat(*[nd.one_hot(t[0], self._vocab_size, on_value=1, off_value=0)\n                                        for t in target], dim=0) if next_word_history is None \\\n            else nd.concat(next_word_history,\n                           nd.concat(*[nd.one_hot(t[0], self._vocab_size, on_value=1, off_value=0)\n                                       for t in target], dim=0), dim=0)\n        cache_history = encoder_h if cache_history is None \\\n            else nd.concat(cache_history, encoder_h, dim=0)\n\n        out = None\n        softmax_output = nd.softmax(output)\n        for idx, vocab_L in enumerate(softmax_output):\n            joint_p = vocab_L\n            if start_idx + idx > self._window:\n                valid_next_word = next_word_history[start_idx + idx - self._window:start_idx + idx]\n                valid_cache_history = cache_history[start_idx + idx - self._window:start_idx + idx]\n                logits = nd.dot(valid_cache_history, encoder_h[idx])\n                cache_attn = nd.softmax(self._theta * logits).reshape(-1, 1)\n                cache_dist = (cache_attn.broadcast_to(valid_next_word.shape)\n                              * valid_next_word).sum(axis=0)\n                joint_p = self._lambdas * cache_dist + (1 - self._lambdas) * vocab_L\n\n            out = joint_p[target[idx]] if out is None \\\n                else nd.concat(out, joint_p[target[idx]], dim=0)\n        next_word_history = next_word_history[-self._window:]\n        cache_history = cache_history[-self._window:]\n        return out, next_word_history, cache_history, hidden"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nassigning input x to an available worker and invoke Taxonomy. forward_backward with x.", "response": "def put(self, x):\n        \"\"\"Assign input `x` to an available worker and invoke\n        `parallizable.forward_backward` with x. \"\"\"\n        if self._num_serial > 0 or len(self._threads) == 0:\n            self._num_serial -= 1\n            out = self._parallizable.forward_backward(x)\n            self._out_queue.put(out)\n        else:\n            self._in_queue.put(x)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef from_json(cls, json_str):\n        vocab_dict = json.loads(json_str)\n\n        unknown_token = vocab_dict.get('unknown_token')\n        bert_vocab = cls(unknown_token=unknown_token)\n        bert_vocab._idx_to_token = vocab_dict.get('idx_to_token')\n        bert_vocab._token_to_idx = vocab_dict.get('token_to_idx')\n        if unknown_token:\n            bert_vocab._token_to_idx = DefaultLookupDict(bert_vocab._token_to_idx[unknown_token],\n                                                         bert_vocab._token_to_idx)\n        bert_vocab._reserved_tokens = vocab_dict.get('reserved_tokens')\n        bert_vocab._padding_token = vocab_dict.get('padding_token')\n        bert_vocab._bos_token = vocab_dict.get('bos_token')\n        bert_vocab._eos_token = vocab_dict.get('eos_token')\n        bert_vocab._mask_token = vocab_dict.get('mask_token')\n        bert_vocab._sep_token = vocab_dict.get('sep_token')\n        bert_vocab._cls_token = vocab_dict.get('cls_token')\n\n        return bert_vocab", "response": "Deserialize a BERTVocab object from a json string."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndefines the forward computation.", "response": "def forward(self, inputs, begin_state=None): # pylint: disable=arguments-differ\n        \"\"\"Defines the forward computation. Arguments can be either\n        :py:class:`NDArray` or :py:class:`Symbol`.\n\n        Parameters\n        -----------\n        inputs : NDArray\n            input tensor with shape `(sequence_length, batch_size)`\n            when `layout` is \"TNC\".\n        begin_state : list\n            initial recurrent state tensor with length equals to num_layers-1.\n            the initial state with shape `(num_layers, batch_size, num_hidden)`\n\n        Returns\n        --------\n        out: NDArray\n            output tensor with shape `(sequence_length, batch_size, input_size)`\n            when `layout` is \"TNC\".\n        out_states: list\n            output recurrent state tensor with length equals to num_layers-1.\n            the state with shape `(num_layers, batch_size, num_hidden)`\n        encoded_raw: list\n            The list of last output of the model's encoder.\n            the shape of last encoder's output `(sequence_length, batch_size, num_hidden)`\n        encoded_dropped: list\n            The list of last output with dropout of the model's encoder.\n            the shape of last encoder's dropped output `(sequence_length, batch_size, num_hidden)`\n        \"\"\"\n        encoded = self.embedding(inputs)\n        if not begin_state:\n            begin_state = self.begin_state(batch_size=inputs.shape[1])\n        encoded_raw = []\n        encoded_dropped = []\n        encoded, state = self.encoder(encoded, begin_state)\n        encoded_raw.append(encoded)\n        if self._dropout:\n            encoded = nd.Dropout(encoded, p=self._dropout, axes=(0,))\n        out = self.decoder(encoded)\n        return out, state, encoded_raw, encoded_dropped"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef forward(self, inputs, label, begin_state, sampled_values): # pylint: disable=arguments-differ\n        encoded = self.embedding(inputs)\n        length = inputs.shape[0]\n        batch_size = inputs.shape[1]\n        encoded, out_states = self.encoder.unroll(length, encoded, begin_state,\n                                                  layout='TNC', merge_outputs=True)\n        out, new_target = self.decoder(encoded, sampled_values, label)\n        out = out.reshape((length, batch_size, -1))\n        new_target = new_target.reshape((length, batch_size))\n        return out, out_states, new_target", "response": "Defines the forward computation."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nskip Gram forward pass.", "response": "def hybrid_forward(self, F, center, context, center_words):\n        \"\"\"SkipGram forward pass.\n\n        Parameters\n        ----------\n        center : mxnet.nd.NDArray or mxnet.sym.Symbol\n            Sparse CSR array of word / subword indices of shape (batch_size,\n            len(token_to_idx) + num_subwords). Embedding for center words are\n            computed via F.sparse.dot between the CSR center array and the\n            weight matrix.\n        context : mxnet.nd.NDArray or mxnet.sym.Symbol\n            Dense array of context words of shape (batch_size, ). Also used for\n            row-wise independently masking negatives equal to one of context.\n        center_words : mxnet.nd.NDArray or mxnet.sym.Symbol\n            Dense array of center words of shape (batch_size, ). Only used for\n            row-wise independently masking negatives equal to one of\n            center_words.\n        \"\"\"\n\n        # negatives sampling\n        negatives = []\n        mask = []\n        for _ in range(self._kwargs['num_negatives']):\n            negatives.append(self.negatives_sampler(center_words))\n            mask_ = negatives[-1] != center_words\n            mask_ = F.stack(mask_, (negatives[-1] != context))\n            mask.append(mask_.min(axis=0))\n\n        negatives = F.stack(*negatives, axis=1)\n        mask = F.stack(*mask, axis=1).astype(np.float32)\n\n        # center - context pairs\n        emb_center = self.embedding(center).expand_dims(1)\n        emb_context = self.embedding_out(context).expand_dims(2)\n        pred_pos = F.batch_dot(emb_center, emb_context).squeeze()\n        loss_pos = (F.relu(pred_pos) - pred_pos + F.Activation(\n            -F.abs(pred_pos), act_type='softrelu')) / (mask.sum(axis=1) + 1)\n\n        # center - negatives pairs\n        emb_negatives = self.embedding_out(negatives).reshape(\n            (-1, self._kwargs['num_negatives'],\n             self._kwargs['output_dim'])).swapaxes(1, 2)\n        pred_neg = F.batch_dot(emb_center, emb_negatives).squeeze()\n        mask = mask.reshape((-1, self._kwargs['num_negatives']))\n        loss_neg = (F.relu(pred_neg) + F.Activation(\n            -F.abs(pred_neg), act_type='softrelu')) * mask\n        loss_neg = loss_neg.sum(axis=1) / (mask.sum(axis=1) + 1)\n\n        return loss_pos + loss_neg"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nevaluating network on the specified dataset", "response": "def evaluate(dataloader):\n    \"\"\"Evaluate network on the specified dataset\"\"\"\n    total_L = 0.0\n    total_sample_num = 0\n    total_correct_num = 0\n    start_log_interval_time = time.time()\n    print('Begin Testing...')\n    for i, ((data, valid_length), label) in enumerate(dataloader):\n        data = mx.nd.transpose(data.as_in_context(context))\n        valid_length = valid_length.as_in_context(context).astype(np.float32)\n        label = label.as_in_context(context)\n        output = net(data, valid_length)\n        L = loss(output, label)\n        pred = (output > 0.5).reshape((-1,))\n        total_L += L.sum().asscalar()\n        total_sample_num += label.shape[0]\n        total_correct_num += (pred == label).sum().asscalar()\n        if (i + 1) % args.log_interval == 0:\n            print('[Batch {}/{}] elapsed {:.2f} s'.format(\n                i + 1, len(dataloader), time.time() - start_log_interval_time))\n            start_log_interval_time = time.time()\n    avg_L = total_L / float(total_sample_num)\n    acc = total_correct_num / float(total_sample_num)\n    return avg_L, acc"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef train():\n    start_pipeline_time = time.time()\n\n    # Training/Testing\n    best_valid_acc = 0\n    stop_early = 0\n    for epoch in range(args.epochs):\n        # Epoch training stats\n        start_epoch_time = time.time()\n        epoch_L = 0.0\n        epoch_sent_num = 0\n        epoch_wc = 0\n        # Log interval training stats\n        start_log_interval_time = time.time()\n        log_interval_wc = 0\n        log_interval_sent_num = 0\n        log_interval_L = 0.0\n\n        for i, ((data, valid_length), label) in enumerate(train_dataloader):\n            data = mx.nd.transpose(data.as_in_context(context))\n            label = label.as_in_context(context)\n            valid_length = valid_length.as_in_context(context).astype(np.float32)\n            wc = valid_length.sum().asscalar()\n            log_interval_wc += wc\n            epoch_wc += wc\n            log_interval_sent_num += data.shape[1]\n            epoch_sent_num += data.shape[1]\n            with autograd.record():\n                output = net(data, valid_length)\n                L = loss(output, label).mean()\n            L.backward()\n            # Clip gradient\n            if args.clip is not None:\n                grads = [p.grad(context) for p in net.collect_params().values()]\n                gluon.utils.clip_global_norm(grads, args.clip)\n            # Update parameter\n            trainer.step(1)\n            log_interval_L += L.asscalar()\n            epoch_L += L.asscalar()\n            if (i + 1) % args.log_interval == 0:\n                print('[Epoch %d Batch %d/%d] avg loss %g, throughput %gK wps' % (\n                    epoch, i + 1, len(train_dataloader),\n                    log_interval_L / log_interval_sent_num,\n                    log_interval_wc / 1000 / (time.time() - start_log_interval_time)))\n                # Clear log interval training stats\n                start_log_interval_time = time.time()\n                log_interval_wc = 0\n                log_interval_sent_num = 0\n                log_interval_L = 0\n        end_epoch_time = time.time()\n        valid_avg_L, valid_acc = evaluate(valid_dataloader)\n        test_avg_L, test_acc = evaluate(test_dataloader)\n        print('[Epoch %d] train avg loss %g, '\n              'valid acc %.4f, valid avg loss %g, '\n              'test acc %.4f, test avg loss %g, throughput %gK wps' % (\n                  epoch, epoch_L / epoch_sent_num,\n                  valid_acc, valid_avg_L, test_acc, test_avg_L,\n                  epoch_wc / 1000 / (end_epoch_time - start_epoch_time)))\n\n        if valid_acc < best_valid_acc:\n            print('No Improvement.')\n            stop_early += 1\n            if stop_early == 3:\n                break\n        else:\n            # Reset stop_early if the validation loss finds a new low value\n            print('Observed Improvement.')\n            stop_early = 0\n            net.save_parameters(args.save_prefix + '_{:04d}.params'.format(epoch))\n            best_valid_acc = valid_acc\n\n    net.load_parameters(glob.glob(args.save_prefix+'_*.params')[-1], context)\n    valid_avg_L, valid_acc = evaluate(valid_dataloader)\n    test_avg_L, test_acc = evaluate(test_dataloader)\n    print('Best validation loss %g, validation acc %.4f'%(valid_avg_L, valid_acc))\n    print('Best test loss %g, test acc %.4f'%(test_avg_L, test_acc))\n    print('Total time cost %.2fs'%(time.time()-start_pipeline_time))", "response": "Train the node in the pipeline."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nforwarding logic for the base class.", "response": "def hybrid_forward(self, F, data, valid_length): # pylint: disable=arguments-differ\n        \"\"\"Forward logic\"\"\"\n        # Data will have shape (T, N, C)\n        if self._use_mean_pool:\n            masked_encoded = F.SequenceMask(data,\n                                            sequence_length=valid_length,\n                                            use_sequence_length=True)\n            agg_state = F.broadcast_div(F.sum(masked_encoded, axis=0),\n                                        F.expand_dims(valid_length, axis=1))\n        else:\n            agg_state = F.SequenceLast(data,\n                                       sequence_length=valid_length,\n                                       use_sequence_length=True)\n        return agg_state"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef hybrid_forward(self, F, inputs, states, i2h_weight,\n                       h2h_weight, h2r_weight, i2h_bias, h2h_bias):\n        r\"\"\"Hybrid forward computation for Long-Short Term Memory Projected network cell\n        with cell clip and projection clip.\n\n        Parameters\n        ----------\n        inputs : input tensor with shape `(batch_size, input_size)`.\n        states : a list of two initial recurrent state tensors, with shape\n            `(batch_size, projection_size)` and `(batch_size, hidden_size)` respectively.\n\n        Returns\n        --------\n        out : output tensor with shape `(batch_size, num_hidden)`.\n        next_states : a list of two output recurrent state tensors. Each has\n            the same shape as `states`.\n        \"\"\"\n        prefix = 't%d_'%self._counter\n        i2h = F.FullyConnected(data=inputs, weight=i2h_weight, bias=i2h_bias,\n                               num_hidden=self._hidden_size*4, name=prefix+'i2h')\n        h2h = F.FullyConnected(data=states[0], weight=h2h_weight, bias=h2h_bias,\n                               num_hidden=self._hidden_size*4, name=prefix+'h2h')\n        gates = i2h + h2h\n        slice_gates = F.SliceChannel(gates, num_outputs=4, name=prefix+'slice')\n        in_gate = F.Activation(slice_gates[0], act_type='sigmoid', name=prefix+'i')\n        forget_gate = F.Activation(slice_gates[1], act_type='sigmoid', name=prefix+'f')\n        in_transform = F.Activation(slice_gates[2], act_type='tanh', name=prefix+'c')\n        out_gate = F.Activation(slice_gates[3], act_type='sigmoid', name=prefix+'o')\n        next_c = F._internal._plus(forget_gate * states[1], in_gate * in_transform,\n                                   name=prefix+'state')\n        if self._cell_clip is not None:\n            next_c = next_c.clip(-self._cell_clip, self._cell_clip)\n        hidden = F._internal._mul(out_gate, F.Activation(next_c, act_type='tanh'),\n                                  name=prefix+'hidden')\n        next_r = F.FullyConnected(data=hidden, num_hidden=self._projection_size,\n                                  weight=h2r_weight, no_bias=True, name=prefix+'out')\n        if self._projection_clip is not None:\n            next_r = next_r.clip(-self._projection_clip, self._projection_clip)\n\n        return next_r, [next_r, next_c]", "response": "r Hybrid forward computation for Long - Short Term Memory Projected network cell."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef clip_grad_global_norm(parameters, max_norm, check_isfinite=True):\n    def _norm(array):\n        if array.stype == 'default':\n            x = array.reshape((-1))\n            return nd.dot(x, x)\n        return array.norm().square()\n\n    arrays = []\n    i = 0\n    for p in parameters:\n        if p.grad_req != 'null':\n            grad_list = p.list_grad()\n            arrays.append(grad_list[i % len(grad_list)])\n            i += 1\n    assert len(arrays) > 0, 'No parameter found available for gradient norm clipping.'\n    ctx, dtype = arrays[0].context, arrays[0].dtype\n    total_norm = nd.add_n(*[_norm(arr).as_in_context(ctx) for arr in arrays])\n    total_norm = nd.sqrt(total_norm)\n    if check_isfinite:\n        total_norm = total_norm.asscalar()\n        if not np.isfinite(total_norm):\n            warnings.warn(\n                UserWarning('nan or inf is detected. '\n                            'Clipping results will be undefined.'), stacklevel=2)\n    scale = max_norm / (total_norm + 1e-8)\n    if check_isfinite:\n        scale = nd.array([scale], dtype=dtype, ctx=ctx)\n    scale = nd.min(nd.concat(scale, nd.ones((1,), dtype=dtype, ctx=ctx), dim=0))\n    for p in parameters:\n        if p.grad_req != 'null':\n            for arr in p.list_grad():\n                arr *= scale.as_in_context(arr.context)\n    return total_norm", "response": "Clip gradients of parameters so that the sum of their 2 - norm is smaller than max_norm."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntrain the model on the data_train and store the training context.", "response": "def train(data_train, model, nsp_loss, mlm_loss, vocab_size, ctx, store):\n    \"\"\"Training function.\"\"\"\n    mlm_metric = nlp.metric.MaskedAccuracy()\n    nsp_metric = nlp.metric.MaskedAccuracy()\n    mlm_metric.reset()\n    nsp_metric.reset()\n\n    lr = args.lr\n    optim_params = {'learning_rate': lr, 'epsilon': 1e-6, 'wd': 0.01}\n    if args.dtype == 'float16':\n        optim_params['multi_precision'] = True\n\n    trainer = mx.gluon.Trainer(model.collect_params(), 'bertadam', optim_params,\n                               update_on_kvstore=False, kvstore=store)\n    dynamic_loss_scale = args.dtype == 'float16'\n    fp16_trainer = FP16Trainer(trainer, dynamic_loss_scale=dynamic_loss_scale)\n\n    if args.ckpt_dir and args.start_step:\n        state_path = os.path.join(args.ckpt_dir, '%07d.states' % args.start_step)\n        logging.info('Loading trainer state from %s', state_path)\n        trainer.load_states(state_path)\n\n    accumulate = args.accumulate\n    num_train_steps = args.num_steps\n    warmup_ratio = args.warmup_ratio\n    num_warmup_steps = int(num_train_steps * warmup_ratio)\n    params = [p for p in model.collect_params().values() if p.grad_req != 'null']\n    param_dict = model.collect_params()\n\n    # Do not apply weight decay on LayerNorm and bias terms\n    for _, v in model.collect_params('.*beta|.*gamma|.*bias').items():\n        v.wd_mult = 0.0\n    if accumulate > 1:\n        for p in params:\n            p.grad_req = 'add'\n\n    train_begin_time = time.time()\n    begin_time = time.time()\n    running_mlm_loss = running_nsp_loss = running_num_tks = 0\n    batch_num = 0\n    step_num = args.start_step\n\n    parallel_model = ParallelBERT(model, mlm_loss, nsp_loss, vocab_size,\n                                  store.num_workers * accumulate, trainer=fp16_trainer)\n    num_ctxes = len(ctx)\n    parallel = nlp.utils.Parallel(num_ctxes if num_ctxes > 1 else 0, parallel_model)\n\n    while step_num < num_train_steps:\n        for _, dataloader in enumerate(data_train):\n            if step_num >= num_train_steps:\n                break\n\n            # create dummy data loader if needed\n            if args.dummy_data_len:\n                target_shape = (args.batch_size*num_ctxes, args.dummy_data_len)\n                dataloader = get_dummy_dataloader(dataloader, target_shape)\n\n            for _, data_batch in enumerate(dataloader):\n                if step_num >= num_train_steps:\n                    break\n                if batch_num % accumulate == 0:\n                    step_num += 1\n                    # if accumulate > 1, grad_req is set to 'add', and zero_grad is required\n                    if accumulate > 1:\n                        param_dict.zero_grad()\n                    # update learning rate\n                    if step_num <= num_warmup_steps:\n                        new_lr = lr * step_num / num_warmup_steps\n                    else:\n                        offset = lr * step_num / num_train_steps\n                        new_lr = lr - offset\n                    trainer.set_learning_rate(new_lr)\n                    if args.profile:\n                        profile(step_num, 10, 12, profile_name=args.profile)\n                if args.use_avg_len:\n                    data_list = [[seq.as_in_context(context) for seq in shard]\n                                 for context, shard in zip(ctx, data_batch)]\n                else:\n                    if data_batch[0].shape[0] < len(ctx):\n                        continue\n                    data_list = split_and_load(data_batch, ctx)\n\n                ns_label_list, ns_pred_list = [], []\n                mask_label_list, mask_pred_list, mask_weight_list = [], [], []\n\n                # parallel forward / backward\n                for data in data_list:\n                    parallel.put(data)\n                for _ in range(len(ctx)):\n                    (_, next_sentence_label, classified, masked_id,\n                     decoded, masked_weight, ls1, ls2, valid_length) = parallel.get()\n                    ns_label_list.append(next_sentence_label)\n                    ns_pred_list.append(classified)\n                    mask_label_list.append(masked_id)\n                    mask_pred_list.append(decoded)\n                    mask_weight_list.append(masked_weight)\n                    running_mlm_loss += ls1.as_in_context(mx.cpu()) / num_ctxes\n                    running_nsp_loss += ls2.as_in_context(mx.cpu()) / num_ctxes\n                    running_num_tks += valid_length.sum().as_in_context(mx.cpu())\n\n                # update\n                if (batch_num + 1) % accumulate == 0:\n                    fp16_trainer.step(1, max_norm=1)\n                nsp_metric.update(ns_label_list, ns_pred_list)\n                mlm_metric.update(mask_label_list, mask_pred_list, mask_weight_list)\n                # logging\n                if (step_num + 1) % (args.log_interval) == 0 and (batch_num + 1) % accumulate == 0:\n                    log(begin_time, running_num_tks, running_mlm_loss / accumulate,\n                        running_nsp_loss / accumulate, step_num, mlm_metric,\n                        nsp_metric, trainer, args.log_interval)\n                    begin_time = time.time()\n                    running_mlm_loss = running_nsp_loss = running_num_tks = 0\n                    mlm_metric.reset_local()\n                    nsp_metric.reset_local()\n\n                # saving checkpoints\n                if args.ckpt_dir and (step_num + 1) % (args.ckpt_interval) == 0 \\\n                   and (batch_num + 1) % accumulate == 0:\n                    save_params(step_num, model, trainer, args.ckpt_dir)\n                batch_num += 1\n    save_params(step_num, model, trainer, args.ckpt_dir)\n    mx.nd.waitall()\n    train_end_time = time.time()\n    logging.info('Train cost={:.1f}s'.format(train_end_time - train_begin_time))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprint statistical information via the provided logger", "response": "def log_info(self, logger):\n        \"\"\"Print statistical information via the provided logger\n\n        Parameters\n        ----------\n        logger : logging.Logger\n            logger created using logging.getLogger()\n        \"\"\"\n        logger.info('#words in training set: %d' % self._words_in_train_data)\n        logger.info(\"Vocab info: #words %d, #tags %d #rels %d\" % (self.vocab_size, self.tag_size, self.rel_size))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads pre - trained embedding file for extending vocabulary", "response": "def _add_pret_words(self, pret_embeddings):\n        \"\"\"Read pre-trained embedding file for extending vocabulary\n\n        Parameters\n        ----------\n        pret_embeddings : tuple\n            (embedding_name, source), used for gluonnlp.embedding.create(embedding_name, source)\n        \"\"\"\n        words_in_train_data = set(self._id2word)\n        pret_embeddings = gluonnlp.embedding.create(pret_embeddings[0], source=pret_embeddings[1])\n\n        for idx, token in enumerate(pret_embeddings.idx_to_token):\n            if token not in words_in_train_data:\n                self._id2word.append(token)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_pret_embs(self, word_dims=None):\n        assert (self._pret_embeddings is not None), \"No pretrained file provided.\"\n        pret_embeddings = gluonnlp.embedding.create(self._pret_embeddings[0], source=self._pret_embeddings[1])\n        embs = [None] * len(self._id2word)\n        for idx, vec in enumerate(pret_embeddings.idx_to_vec):\n            embs[idx] = vec.asnumpy()\n        if word_dims is None:\n            word_dims = len(pret_embeddings.idx_to_vec[0])\n        for idx, emb in enumerate(embs):\n            if emb is None:\n                embs[idx] = np.zeros(word_dims)\n        pret_embs = np.array(embs, dtype=np.float32)\n        return pret_embs / np.std(pret_embs)", "response": "Read pre - trained embedding file and return the pre - trained embedding matrix."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget randomly initialized embeddings when pre - trained embeddings are used otherwise zero vectors", "response": "def get_word_embs(self, word_dims):\n        \"\"\"Get randomly initialized embeddings when pre-trained embeddings are used, otherwise zero vectors\n\n        Parameters\n        ----------\n        word_dims : int\n            word vector size\n        Returns\n        -------\n        numpy.ndarray\n            T x C numpy NDArray\n        \"\"\"\n        if self._pret_embeddings is not None:\n            return np.random.randn(self.words_in_train, word_dims).astype(np.float32)\n        return np.zeros((self.words_in_train, word_dims), dtype=np.float32)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_tag_embs(self, tag_dims):\n        return np.random.randn(self.tag_size, tag_dims).astype(np.float32)", "response": "Randomly initialize embeddings for tag\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef word2id(self, xs):\n        if isinstance(xs, list):\n            return [self._word2id.get(x, self.UNK) for x in xs]\n        return self._word2id.get(xs, self.UNK)", "response": "Map word to its id"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmap id to word", "response": "def id2word(self, xs):\n        \"\"\"Map id(s) to word(s)\n\n        Parameters\n        ----------\n        xs : int\n            id or a list of ids\n\n        Returns\n        -------\n        str or list\n            word or a list of words\n        \"\"\"\n        if isinstance(xs, list):\n            return [self._id2word[x] for x in xs]\n        return self._id2word[xs]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmap relation to id", "response": "def rel2id(self, xs):\n        \"\"\"Map relation(s) to id(s)\n\n        Parameters\n        ----------\n        xs : str or list\n            relation\n\n        Returns\n        -------\n        int or list\n            id(s) of relation\n        \"\"\"\n        if isinstance(xs, list):\n            return [self._rel2id[x] for x in xs]\n        return self._rel2id[xs]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef id2rel(self, xs):\n        if isinstance(xs, list):\n            return [self._id2rel[x] for x in xs]\n        return self._id2rel[xs]", "response": "Map id to relation"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef tag2id(self, xs):\n        if isinstance(xs, list):\n            return [self._tag2id.get(x, self.UNK) for x in xs]\n        return self._tag2id.get(xs, self.UNK)", "response": "Map tag or list of tags to id"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_batches(self, batch_size, shuffle=True):\n        batches = []\n        for bkt_idx, bucket in enumerate(self._buckets):\n            bucket_size = bucket.shape[1]\n            n_tokens = bucket_size * self._bucket_lengths[bkt_idx]\n            n_splits = min(max(n_tokens // batch_size, 1), bucket_size)\n            range_func = np.random.permutation if shuffle else np.arange\n            for bkt_batch in np.array_split(range_func(bucket_size), n_splits):\n                batches.append((bkt_idx, bkt_batch))\n\n        if shuffle:\n            np.random.shuffle(batches)\n\n        for bkt_idx, bkt_batch in batches:\n            word_inputs = self._buckets[bkt_idx][:, bkt_batch, 0]  # word_id x sent_id\n            tag_inputs = self._buckets[bkt_idx][:, bkt_batch, 1]\n            arc_targets = self._buckets[bkt_idx][:, bkt_batch, 2]\n            rel_targets = self._buckets[bkt_idx][:, bkt_batch, 3]\n            yield word_inputs, tag_inputs, arc_targets, rel_targets", "response": "Get a batch iterator that returns a list of tuples each containing word_inputs tag_inputs arc_targets and rel_targets."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_ngram_set(input_list, ngram_value=2):\n    return set(zip(*[input_list[i:] for i in range(ngram_value)]))", "response": "Extract a set of n - grams from a list of integers."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\naugment the input list of list of list of lists by appending n - grams values.", "response": "def add_ngram(sequences, token_indice, ngram_range=2):\n    \"\"\"\n    Augment the input list of list (sequences) by appending n-grams values.\n    Example: adding bi-gram\n    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017}\n    >>> add_ngram(sequences, token_indice, ngram_range=2)\n    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42]]\n    Example: adding tri-gram\n    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017, (7, 9, 2): 2018}\n    >>> add_ngram(sequences, token_indice, ngram_range=3)\n    [[1, 3, 4, 5, 1337], [1, 3, 7, 9, 2, 1337, 2018]]\n    \"\"\"\n    new_sequences = []\n    for input_list in sequences:\n        new_list = input_list[:]\n        for i in range(len(new_list) - ngram_range + 1):\n            for ngram_value in range(2, ngram_range + 1):\n                ngram = tuple(new_list[i:i + ngram_value])\n                if ngram in token_indice:\n                    new_list.append(token_indice[ngram])\n        new_sequences.append(new_list)\n\n    return new_sequences"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef read_input_data(filename):\n    logging.info('Opening file %s for reading input', filename)\n    input_file = open(filename, 'r')\n    data = []\n    labels = []\n    for line in input_file:\n        tokens = line.split(',', 1)\n        labels.append(tokens[0].strip())\n        data.append(tokens[1].strip())\n    return labels, data", "response": "Helper function to get training data from a file"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description='Text Classification with FastText',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    # Computation options\n    group = parser.add_argument_group('Computation arguments')\n    group.add_argument('--input', type=str, help='Input file location')\n    group.add_argument(\n        '--validation', type=str, help='Validation file Location ')\n    group.add_argument(\n        '--output', type=str, help='Location to save trained model')\n    group.add_argument(\n        '--ngrams', type=int, default=1, help='NGrams used for training')\n    group.add_argument(\n        '--batch_size', type=int, default=16, help='Batch size for training.')\n    group.add_argument('--epochs', type=int, default=10, help='Epoch limit')\n    group.add_argument(\n        '--gpu',\n        type=int,\n        help=('Number (index) of GPU to run on, e.g. 0. '\n              'If not specified, uses CPU.'))\n    group.add_argument(\n        '--no-hybridize',\n        action='store_true',\n        help='Disable hybridization of gluon HybridBlocks.')\n\n    # Model\n    group = parser.add_argument_group('Model arguments')\n    group.add_argument(\n        '--emsize', type=int, default=100, help='Size of embedding vectors.')\n\n    # Optimization options\n    group = parser.add_argument_group('Optimization arguments')\n    group.add_argument('--optimizer', type=str, default='adam')\n    group.add_argument('--lr', type=float, default=0.05)\n\n    args = parser.parse_args()\n    return args", "response": "Parse command line arguments."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate the mapping from label to numeric label", "response": "def get_label_mapping(train_labels):\n    \"\"\"\n    Create the mapping from label to numeric label\n    \"\"\"\n    sorted_labels = np.sort(np.unique(train_labels))\n    label_mapping = {}\n    for i, label in enumerate(sorted_labels):\n        label_mapping[label] = i\n    logging.info('Label mapping:%s', format(label_mapping))\n    return label_mapping"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_dataloader(train_dataset, train_data_lengths,\n                   test_dataset, batch_size):\n    \"\"\" Construct the DataLoader. Pad data, stack label and lengths\"\"\"\n    bucket_num, bucket_ratio = 20, 0.2\n    batchify_fn = gluonnlp.data.batchify.Tuple(\n        gluonnlp.data.batchify.Pad(axis=0, ret_length=True),\n        gluonnlp.data.batchify.Stack(dtype='float32'))\n    batch_sampler = gluonnlp.data.sampler.FixedBucketSampler(\n        train_data_lengths,\n        batch_size=batch_size,\n        num_buckets=bucket_num,\n        ratio=bucket_ratio,\n        shuffle=True)\n    train_dataloader = gluon.data.DataLoader(\n        dataset=train_dataset,\n        batch_sampler=batch_sampler,\n        batchify_fn=batchify_fn)\n    test_dataloader = gluon.data.DataLoader(\n        dataset=test_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        batchify_fn=batchify_fn)\n    return train_dataloader, test_dataloader", "response": "Construct the DataLoader. Pad data, stack label and lengths"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef train(args):\n    train_file = args.input\n    test_file = args.validation\n    ngram_range = args.ngrams\n    logging.info('Ngrams range for the training run : %s', ngram_range)\n    logging.info('Loading Training data')\n    train_labels, train_data = read_input_data(train_file)\n    logging.info('Loading Test data')\n    test_labels, test_data = read_input_data(test_file)\n    tokens_list = []\n    for line in train_data:\n        tokens_list.extend(line.split())\n\n    cntr = Counter(tokens_list)\n    train_vocab = gluonnlp.Vocab(cntr)\n    logging.info('Vocabulary size: %s', len(train_vocab))\n    logging.info('Training data converting to sequences...')\n    embedding_matrix_len = len(train_vocab)\n    # Preprocess the dataset\n    train_sequences = convert_to_sequences(train_data, train_vocab)\n    test_sequences = convert_to_sequences(test_data, train_vocab)\n\n    if ngram_range >= 2:\n        logging.info('Adding %s-gram features', ngram_range)\n        # Create set of unique n-gram from the training set.\n        ngram_set = set()\n        for input_list in train_sequences:\n            for i in range(2, ngram_range + 1):\n                set_of_ngram = create_ngram_set(input_list, ngram_value=i)\n                ngram_set.update(set_of_ngram)\n        start_index = len(cntr)\n        token_indices = {v: k + start_index for k, v in enumerate(ngram_set)}\n        embedding_matrix_len = embedding_matrix_len + len(token_indices)\n        train_sequences = add_ngram(train_sequences, token_indices,\n                                    ngram_range)\n        test_sequences = add_ngram(test_sequences, token_indices, ngram_range)\n        logging.info('Added n-gram features to train and test datasets!! ')\n    logging.info('Encoding labels')\n\n    label_mapping = get_label_mapping(train_labels)\n    y_train_final = list(map(lambda x: label_mapping[x], train_labels))\n    y_test_final = list(map(lambda x: label_mapping[x], test_labels))\n\n    train_sequences, train_data_lengths = preprocess_dataset(\n        train_sequences, y_train_final)\n    test_sequences, _ = preprocess_dataset(\n        test_sequences, y_test_final)\n    train_dataloader, test_dataloader = get_dataloader(train_sequences, train_data_lengths,\n                                                       test_sequences, args.batch_size)\n\n    num_classes = len(np.unique(train_labels))\n    logging.info('Number of labels: %s', num_classes)\n    logging.info('Initializing network')\n    ctx = get_context(args)\n    logging.info('Running Training on ctx:%s', ctx)\n    embedding_dim = args.emsize\n    logging.info('Embedding Matrix Length:%s', embedding_matrix_len)\n    net = FastTextClassificationModel(\n        embedding_matrix_len, embedding_dim, num_classes)\n    net.hybridize()\n    net.collect_params().initialize(mx.init.Xavier(), ctx=ctx)\n    logging.info('Network initialized')\n\n    softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n    sigmoid_loss_fn = gluon.loss.SigmoidBinaryCrossEntropyLoss()\n\n    loss_function = softmax_cross_entropy\n    if num_classes == 2:\n        logging.info(\n            'Changing the loss function to sigmoid since its Binary Classification'\n        )\n        loss_function = sigmoid_loss_fn\n    logging.info('Loss function for training:%s', loss_function)\n    num_epochs = args.epochs\n    batch_size = args.batch_size\n    logging.info('Starting Training!')\n    learning_rate = args.lr\n    trainer = gluon.Trainer(net.collect_params(), 'adam',\n                            {'learning_rate': learning_rate})\n    num_batches = len(train_data) / batch_size\n    display_batch_cadence = int(math.ceil(num_batches / 10))\n    logging.info('Training on %s samples and testing on %s samples',\n                 len(train_data), len(test_data))\n    logging.info('Number of batches for each epoch : %s, Display cadence: %s',\n                 num_batches, display_batch_cadence)\n    for epoch in range(num_epochs):\n        for batch, ((data, length), label) in enumerate(train_dataloader):\n            data = data.as_in_context(ctx)\n            label = label.as_in_context(ctx)\n            length = length.astype('float32').as_in_context(ctx)\n            with autograd.record():\n                output = net(data, length)\n                loss = loss_function(output, label)\n            loss.backward()\n            trainer.step(data.shape[0])\n            if batch % display_batch_cadence == 0:\n                logging.info('Epoch : %s, Batches complete :%s', epoch, batch)\n        logging.info('Epoch complete :%s, Computing Accuracy', epoch)\n        test_accuracy, test_loss = evaluate_accuracy(\n            test_dataloader, net, ctx, loss_function, num_classes)\n        logging.info('Epochs completed : %s Test Accuracy: %s, Test Loss: %s',\n                     epoch, test_accuracy, test_loss)\n        learning_rate = learning_rate * 0.5\n        trainer.set_learning_rate(learning_rate)\n    save_model(net, args.output)", "response": "Train the n - grams in the dataset."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef hybrid_forward(self, F, inputs, token_types, valid_length=None):\n        # pylint: disable=arguments-differ\n        # pylint: disable=unused-argument\n        \"\"\"Generate the unnormalized score for the given the input sequences.\n\n        Parameters\n        ----------\n        inputs : NDArray, shape (batch_size, seq_length)\n            Input words for the sequences.\n        token_types : NDArray, shape (batch_size, seq_length)\n            Token types for the sequences, used to indicate whether the word belongs to the\n            first sentence or the second one.\n        valid_length : NDArray or None, shape (batch_size,)\n            Valid length of the sequence. This is used to mask the padded tokens.\n\n        Returns\n        -------\n        outputs : NDArray\n            Shape (batch_size, seq_length, 2)\n        \"\"\"\n        bert_output = self.bert(inputs, token_types, valid_length)\n        output = self.span_classifier(bert_output)\n        return output", "response": "This is the unnormalized score function that uses the BER algorithm to generate the unnormalized score for the given input sequences."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef hybrid_forward(self, F, pred, label):  # pylint: disable=arguments-differ\n        pred = F.split(pred, axis=2, num_outputs=2)\n        start_pred = pred[0].reshape((0, -3))\n        start_label = label[0]\n        end_pred = pred[1].reshape((0, -3))\n        end_label = label[1]\n        return (self.loss(start_pred, start_label) + self.loss(\n            end_pred, end_label)) / 2", "response": "A BERTSquad forward implementation of the log - likelihood."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nencodes the input sequence.", "response": "def encode(self, inputs, states=None, valid_length=None):\n        \"\"\"Encode the input sequence.\n\n        Parameters\n        ----------\n        inputs : NDArray\n        states : list of NDArrays or None, default None\n        valid_length : NDArray or None, default None\n\n        Returns\n        -------\n        outputs : list\n            Outputs of the encoder.\n        \"\"\"\n        return self.encoder(self.src_embed(inputs), states, valid_length)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndecode given the input sequence.", "response": "def decode_seq(self, inputs, states, valid_length=None):\n        \"\"\"Decode given the input sequence.\n\n        Parameters\n        ----------\n        inputs : NDArray\n        states : list of NDArrays\n        valid_length : NDArray or None, default None\n\n        Returns\n        -------\n        output : NDArray\n            The output of the decoder. Shape is (batch_size, length, tgt_word_num)\n        states: list\n            The new states of the decoder\n        additional_outputs : list\n            Additional outputs of the decoder, e.g, the attention weights\n        \"\"\"\n        outputs, states, additional_outputs =\\\n            self.decoder.decode_seq(inputs=self.tgt_embed(inputs),\n                                    states=states,\n                                    valid_length=valid_length)\n        outputs = self.tgt_proj(outputs)\n        return outputs, states, additional_outputs"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates the prediction given the src_seq and tgt_seq.", "response": "def forward(self, src_seq, tgt_seq, src_valid_length=None, tgt_valid_length=None):  #pylint: disable=arguments-differ\n        \"\"\"Generate the prediction given the src_seq and tgt_seq.\n\n        This is used in training an NMT model.\n\n        Parameters\n        ----------\n        src_seq : NDArray\n        tgt_seq : NDArray\n        src_valid_length : NDArray or None\n        tgt_valid_length : NDArray or None\n\n        Returns\n        -------\n        outputs : NDArray\n            Shape (batch_size, tgt_length, tgt_word_num)\n        additional_outputs : list of list\n            Additional outputs of encoder and decoder, e.g, the attention weights\n        \"\"\"\n        additional_outputs = []\n        encoder_outputs, encoder_additional_outputs = self.encode(src_seq,\n                                                                  valid_length=src_valid_length)\n        decoder_states = self.decoder.init_state_from_encoder(encoder_outputs,\n                                                              encoder_valid_length=src_valid_length)\n        outputs, _, decoder_additional_outputs =\\\n            self.decode_seq(tgt_seq, decoder_states, tgt_valid_length)\n        additional_outputs.append(encoder_additional_outputs)\n        additional_outputs.append(decoder_additional_outputs)\n        return outputs, additional_outputs"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate an instance of a subword function.", "response": "def create_subword_function(subword_function_name, **kwargs):\n    \"\"\"Creates an instance of a subword function.\"\"\"\n\n    create_ = registry.get_create_func(SubwordFunction, 'token embedding')\n    return create_(subword_function_name, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _index_special_tokens(self, unknown_token, special_tokens):\n        self._idx_to_token = [unknown_token] if unknown_token else []\n\n        if not special_tokens:\n            self._reserved_tokens = None\n        else:\n            self._reserved_tokens = special_tokens[:]\n            self._idx_to_token.extend(special_tokens)\n\n        if unknown_token:\n            self._token_to_idx = DefaultLookupDict(C.UNK_IDX)\n        else:\n            self._token_to_idx = {}\n        self._token_to_idx.update((token, idx) for idx, token in enumerate(self._idx_to_token))", "response": "Indexes unknown and reserved tokens."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _index_counter_keys(self, counter, unknown_token, special_tokens, max_size,\n                            min_freq):\n        \"\"\"Indexes keys of `counter`.\n\n\n        Indexes keys of `counter` according to frequency thresholds such as `max_size` and\n        `min_freq`.\n        \"\"\"\n\n        unknown_and_special_tokens = set(special_tokens) if special_tokens else set()\n\n        if unknown_token:\n            unknown_and_special_tokens.add(unknown_token)\n\n        token_freqs = sorted(counter.items(), key=lambda x: x[0])\n        token_freqs.sort(key=lambda x: x[1], reverse=True)\n\n        token_cap = len(unknown_and_special_tokens) + (\n            len(counter) if not max_size else max_size)\n\n        for token, freq in token_freqs:\n            if freq < min_freq or len(self._idx_to_token) == token_cap:\n                break\n            if token not in unknown_and_special_tokens:\n                self._idx_to_token.append(token)\n                self._token_to_idx[token] = len(self._idx_to_token) - 1", "response": "Indexes keys of counter according to frequency thresholds such as max_size and min_freq."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the embedding of the indexed text tokens.", "response": "def set_embedding(self, *embeddings):\n        \"\"\"Attaches one or more embeddings to the indexed text tokens.\n\n\n        Parameters\n        ----------\n        embeddings : None or tuple of :class:`gluonnlp.embedding.TokenEmbedding` instances\n            The embedding to be attached to the indexed tokens. If a tuple of multiple embeddings\n            are provided, their embedding vectors will be concatenated for the same token.\n        \"\"\"\n\n        if len(embeddings) == 1 and embeddings[0] is None:\n            self._embedding = None\n            return\n\n        for embs in embeddings:\n            assert isinstance(embs, emb.TokenEmbedding), \\\n                'The argument `embeddings` must be an instance or a list of instances of ' \\\n                '`gluonnlp.embedding.TokenEmbedding`.'\n            assert embs.idx_to_vec is not None, \\\n                'For all specified `embeddings`, `embeddings.idx_to_vec` must be initialized. ' \\\n                'Use eg. `emb[emb.unknown_token] = nd.zeros(emsize)` to initialize, ' \\\n                'where `emsize` is the desired embedding dimensionality.'\n\n        assert all([embs.unknown_token for embs in embeddings]) or \\\n            all([not embs.unknown_token for embs in embeddings]), \\\n            'Either all or none of the TokenEmbeddings must have an ' \\\n            'unknown_token set.'\n\n        new_embedding = emb.TokenEmbedding(self.unknown_token, allow_extend=False)\n        new_embedding._token_to_idx = self.token_to_idx\n        new_embedding._idx_to_token = self.idx_to_token\n\n        new_vec_len = sum(embs.idx_to_vec.shape[1] for embs in embeddings)\n        new_idx_to_vec = nd.zeros(shape=(len(self), new_vec_len))\n\n        col_start = 0\n        # Concatenate all the embedding vectors in embedding.\n        for embs in embeddings:\n            if embs and embs.idx_to_vec is not None:\n                col_end = col_start + embs.idx_to_vec.shape[1]\n                # Cancatenate vectors of the unknown token.\n                new_idx_to_vec[0, col_start:col_end] = embs.idx_to_vec[0]\n                new_idx_to_vec[1:, col_start:col_end] = embs[self._idx_to_token[1:]]\n                col_start = col_end\n\n        new_embedding._idx_to_vec = new_idx_to_vec\n        self._embedding = new_embedding"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_tokens(self, indices):\n\n        to_reduce = False\n        if not isinstance(indices, (list, tuple)):\n            indices = [indices]\n            to_reduce = True\n\n        max_idx = len(self._idx_to_token) - 1\n\n        tokens = []\n        for idx in indices:\n            if not isinstance(idx, int) or idx > max_idx:\n                raise ValueError('Token index {} in the provided `indices` is invalid.'.format(idx))\n            else:\n                tokens.append(self._idx_to_token[idx])\n\n        return tokens[0] if to_reduce else tokens", "response": "Converts token indices to tokens according to the vocabulary."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nserializing the Vocab object to json string.", "response": "def to_json(self):\n        \"\"\"Serialize Vocab object to json string.\n\n        This method does not serialize the underlying embedding.\n        \"\"\"\n        if self._embedding:\n            warnings.warn('Serialization of attached embedding '\n                          'to json is not supported. '\n                          'You may serialize the embedding to a binary format '\n                          'separately using vocab.embedding.serialize')\n        vocab_dict = {}\n        vocab_dict['idx_to_token'] = self._idx_to_token\n        vocab_dict['token_to_idx'] = dict(self._token_to_idx)\n        vocab_dict['reserved_tokens'] = self._reserved_tokens\n        vocab_dict['unknown_token'] = self._unknown_token\n        vocab_dict['padding_token'] = self._padding_token\n        vocab_dict['bos_token'] = self._bos_token\n        vocab_dict['eos_token'] = self._eos_token\n        return json.dumps(vocab_dict)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef from_json(cls, json_str):\n        vocab_dict = json.loads(json_str)\n\n        unknown_token = vocab_dict.get('unknown_token')\n        vocab = cls(unknown_token=unknown_token)\n        vocab._idx_to_token = vocab_dict.get('idx_to_token')\n        vocab._token_to_idx = vocab_dict.get('token_to_idx')\n        if unknown_token:\n            vocab._token_to_idx = DefaultLookupDict(vocab._token_to_idx[unknown_token],\n                                                    vocab._token_to_idx)\n        vocab._reserved_tokens = vocab_dict.get('reserved_tokens')\n        vocab._padding_token = vocab_dict.get('padding_token')\n        vocab._bos_token = vocab_dict.get('bos_token')\n        vocab._eos_token = vocab_dict.get('eos_token')\n        return vocab", "response": "Deserialize a Vocab object from a json string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef train(data_train, model, nsp_loss, mlm_loss, vocab_size, ctx):\n    hvd.broadcast_parameters(model.collect_params(), root_rank=0)\n\n    mlm_metric = nlp.metric.MaskedAccuracy()\n    nsp_metric = nlp.metric.MaskedAccuracy()\n    mlm_metric.reset()\n    nsp_metric.reset()\n\n    logging.debug('Creating distributed trainer...')\n    lr = args.lr\n    optim_params = {'learning_rate': lr, 'epsilon': 1e-6, 'wd': 0.01}\n    if args.dtype == 'float16':\n        optim_params['multi_precision'] = True\n\n    dynamic_loss_scale = args.dtype == 'float16'\n    if dynamic_loss_scale:\n        loss_scale_param = {'scale_window': 2000 / num_workers}\n    else:\n        loss_scale_param = None\n    trainer = hvd.DistributedTrainer(model.collect_params(), 'bertadam', optim_params)\n    fp16_trainer = FP16Trainer(trainer, dynamic_loss_scale=dynamic_loss_scale,\n                               loss_scaler_params=loss_scale_param)\n\n    if args.ckpt_dir and args.start_step:\n        trainer.load_states(os.path.join(args.ckpt_dir, '%07d.states'%args.start_step))\n\n    accumulate = args.accumulate\n    num_train_steps = args.num_steps\n    warmup_ratio = args.warmup_ratio\n    num_warmup_steps = int(num_train_steps * warmup_ratio)\n    params = [p for p in model.collect_params().values() if p.grad_req != 'null']\n    param_dict = model.collect_params()\n\n    # Do not apply weight decay on LayerNorm and bias terms\n    for _, v in model.collect_params('.*beta|.*gamma|.*bias').items():\n        v.wd_mult = 0.0\n    if accumulate > 1:\n        for p in params:\n            p.grad_req = 'add'\n\n    train_begin_time = time.time()\n    begin_time = time.time()\n    running_mlm_loss, running_nsp_loss = 0, 0\n    running_num_tks = 0\n    batch_num = 0\n    step_num = args.start_step\n\n    logging.debug('Training started')\n    while step_num < num_train_steps:\n        for _, dataloader in enumerate(data_train):\n            if step_num >= num_train_steps:\n                break\n\n            # create dummy data loader if needed\n            if args.dummy_data_len:\n                target_shape = (args.batch_size, args.dummy_data_len)\n                dataloader = get_dummy_dataloader(dataloader, target_shape)\n\n            for _, data_batch in enumerate(dataloader):\n                if step_num >= num_train_steps:\n                    break\n                if batch_num % accumulate == 0:\n                    step_num += 1\n                    # if accumulate > 1, grad_req is set to 'add', and zero_grad is required\n                    if accumulate > 1:\n                        param_dict.zero_grad()\n                    # update learning rate\n                    if step_num <= num_warmup_steps:\n                        new_lr = lr * step_num / num_warmup_steps\n                    else:\n                        offset = lr * step_num / num_train_steps\n                        new_lr = lr - offset\n                    trainer.set_learning_rate(new_lr)\n                    if args.profile:\n                        profile(step_num, 10, 14, profile_name=args.profile + str(rank))\n\n                # load data\n                if args.use_avg_len:\n                    data_list = [[seq.as_in_context(context) for seq in shard]\n                                 for context, shard in zip([ctx], data_batch)]\n                else:\n                    data_list = list(split_and_load(data_batch, [ctx]))\n                data = data_list[0]\n\n                # forward\n                with mx.autograd.record():\n                    (ls, ns_label, classified, masked_id, decoded, \\\n                     masked_weight, ls1, ls2, valid_len) = forward(data, model, mlm_loss,\n                                                                   nsp_loss, vocab_size, args.dtype)\n                    ls = ls / accumulate\n                    # backward\n                    if args.dtype == 'float16':\n                        fp16_trainer.backward(ls)\n                    else:\n                        ls.backward()\n\n                running_mlm_loss += ls1.as_in_context(mx.cpu())\n                running_nsp_loss += ls2.as_in_context(mx.cpu())\n                running_num_tks += valid_len.sum().as_in_context(mx.cpu())\n\n                # update\n                if (batch_num + 1) % accumulate == 0:\n                    # step() performs 3 things:\n                    # 1. allreduce gradients from all workers\n                    # 2. checking the global_norm of gradients and clip them if necessary\n                    # 3. averaging the gradients and apply updates\n                    fp16_trainer.step(1, max_norm=1*num_workers)\n\n                nsp_metric.update([ns_label], [classified])\n                mlm_metric.update([masked_id], [decoded], [masked_weight])\n\n                # logging\n                if (step_num + 1) % (args.log_interval) == 0 and (batch_num + 1) % accumulate == 0:\n                    log(begin_time, running_num_tks, running_mlm_loss / accumulate,\n                        running_nsp_loss / accumulate, step_num, mlm_metric, nsp_metric,\n                        trainer, args.log_interval)\n                    begin_time = time.time()\n                    running_mlm_loss = running_nsp_loss = running_num_tks = 0\n                    mlm_metric.reset_local()\n                    nsp_metric.reset_local()\n\n                # saving checkpoints\n                if args.ckpt_dir and (step_num + 1) % (args.ckpt_interval) == 0 \\\n                   and (batch_num + 1) % accumulate == 0 and local_rank == 0:\n                    save_params(step_num, model, trainer, args.ckpt_dir)\n\n                batch_num += 1\n\n    if local_rank == 0:\n        save_params(step_num, model, trainer, args.ckpt_dir)\n    mx.nd.waitall()\n    train_end_time = time.time()\n    logging.info('Train cost={:.1f}s'.format(train_end_time - train_begin_time))", "response": "Train the model on the data_train."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntraining the data in the base dataset.", "response": "def train():\n    \"\"\"Training function.\"\"\"\n    log.info('Loader Train data...')\n    if version_2:\n        train_data = SQuAD('train', version='2.0')\n    else:\n        train_data = SQuAD('train', version='1.1')\n    log.info('Number of records in Train data:{}'.format(len(train_data)))\n\n    train_data_transform, _ = preprocess_dataset(\n        train_data, SQuADTransform(\n            berttoken,\n            max_seq_length=max_seq_length,\n            doc_stride=doc_stride,\n            max_query_length=max_query_length,\n            is_pad=True,\n            is_training=True))\n    log.info('The number of examples after preprocessing:{}'.format(\n        len(train_data_transform)))\n\n    train_dataloader = mx.gluon.data.DataLoader(\n        train_data_transform, batchify_fn=batchify_fn,\n        batch_size=batch_size, num_workers=4, shuffle=True)\n\n    log.info('Start Training')\n\n    optimizer_params = {'learning_rate': lr}\n    try:\n        trainer = gluon.Trainer(net.collect_params(), optimizer,\n                                optimizer_params, update_on_kvstore=False)\n    except ValueError as e:\n        print(e)\n        warnings.warn('AdamW optimizer is not found. Please consider upgrading to '\n                      'mxnet>=1.5.0. Now the original Adam optimizer is used instead.')\n        trainer = gluon.Trainer(net.collect_params(), 'adam',\n                                optimizer_params, update_on_kvstore=False)\n\n    num_train_examples = len(train_data_transform)\n    step_size = batch_size * accumulate if accumulate else batch_size\n    num_train_steps = int(num_train_examples / step_size * epochs)\n    num_warmup_steps = int(num_train_steps * warmup_ratio)\n    step_num = 0\n\n    def set_new_lr(step_num, batch_id):\n        \"\"\"set new learning rate\"\"\"\n        # set grad to zero for gradient accumulation\n        if accumulate:\n            if batch_id % accumulate == 0:\n                net.collect_params().zero_grad()\n                step_num += 1\n        else:\n            step_num += 1\n        # learning rate schedule\n        # Notice that this learning rate scheduler is adapted from traditional linear learning\n        # rate scheduler where step_num >= num_warmup_steps, new_lr = 1 - step_num/num_train_steps\n        if step_num < num_warmup_steps:\n            new_lr = lr * step_num / num_warmup_steps\n        else:\n            offset = (step_num - num_warmup_steps) * lr / \\\n                     (num_train_steps - num_warmup_steps)\n            new_lr = lr - offset\n        trainer.set_learning_rate(new_lr)\n        return step_num\n\n    # Do not apply weight decay on LayerNorm and bias terms\n    for _, v in net.collect_params('.*beta|.*gamma|.*bias').items():\n        v.wd_mult = 0.0\n    # Collect differentiable parameters\n    params = [p for p in net.collect_params().values()\n              if p.grad_req != 'null']\n    # Set grad_req if gradient accumulation is required\n    if accumulate:\n        for p in params:\n            p.grad_req = 'add'\n\n    epoch_tic = time.time()\n    total_num = 0\n    log_num = 0\n    for epoch_id in range(epochs):\n        step_loss = 0.0\n        tic = time.time()\n        for batch_id, data in enumerate(train_dataloader):\n            # set new lr\n            step_num = set_new_lr(step_num, batch_id)\n            # forward and backward\n            with mx.autograd.record():\n                _, inputs, token_types, valid_length, start_label, end_label = data\n\n                log_num += len(inputs)\n                total_num += len(inputs)\n\n                out = net(inputs.astype('float32').as_in_context(ctx),\n                          token_types.astype('float32').as_in_context(ctx),\n                          valid_length.astype('float32').as_in_context(ctx))\n\n                ls = loss_function(out, [\n                    start_label.astype('float32').as_in_context(ctx),\n                    end_label.astype('float32').as_in_context(ctx)]).mean()\n\n                if accumulate:\n                    ls = ls / accumulate\n            ls.backward()\n            # update\n            if not accumulate or (batch_id + 1) % accumulate == 0:\n                trainer.allreduce_grads()\n                nlp.utils.clip_grad_global_norm(params, 1)\n                trainer.update(1)\n\n            step_loss += ls.asscalar()\n\n            if (batch_id + 1) % log_interval == 0:\n                toc = time.time()\n                log.info(\n                    'Epoch: {}, Batch: {}/{}, Loss={:.4f}, lr={:.7f} Time cost={:.1f} Thoughput={:.2f} samples/s'  # pylint: disable=line-too-long\n                        .format(epoch_id, batch_id, len(train_dataloader),\n                                step_loss / log_interval,\n                                trainer.learning_rate, toc - tic, log_num / (toc - tic)))\n                tic = time.time()\n                step_loss = 0.0\n                log_num = 0\n        epoch_toc = time.time()\n        log.info('Epoch: {}, Time cost={:.2f} s, Thoughput={:.2f} samples/s'\n                 .format(epoch_id, epoch_toc - epoch_tic,\n                         len(train_dataloader) / (epoch_toc - epoch_tic)))\n\n    net.save_parameters(os.path.join(output_dir, 'net.params'))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef evaluate():\n    log.info('Loader dev data...')\n    if version_2:\n        dev_data = SQuAD('dev', version='2.0')\n    else:\n        dev_data = SQuAD('dev', version='1.1')\n    log.info('Number of records in Train data:{}'.format(len(dev_data)))\n\n    dev_dataset = dev_data.transform(\n        SQuADTransform(\n            berttoken,\n            max_seq_length=max_seq_length,\n            doc_stride=doc_stride,\n            max_query_length=max_query_length,\n            is_pad=True,\n            is_training=False)._transform)\n\n    dev_data_transform, _ = preprocess_dataset(\n        dev_data, SQuADTransform(\n            berttoken,\n            max_seq_length=max_seq_length,\n            doc_stride=doc_stride,\n            max_query_length=max_query_length,\n            is_pad=True,\n            is_training=False))\n    log.info('The number of examples after preprocessing:{}'.format(\n        len(dev_data_transform)))\n\n    dev_dataloader = mx.gluon.data.DataLoader(\n        dev_data_transform,\n        batchify_fn=batchify_fn,\n        num_workers=4, batch_size=test_batch_size, shuffle=False, last_batch='keep')\n\n    log.info('Start predict')\n\n    _Result = collections.namedtuple(\n        '_Result', ['example_id', 'start_logits', 'end_logits'])\n    all_results = {}\n\n    epoch_tic = time.time()\n    total_num = 0\n    for data in dev_dataloader:\n        example_ids, inputs, token_types, valid_length, _, _ = data\n        total_num += len(inputs)\n        out = net(inputs.astype('float32').as_in_context(ctx),\n                  token_types.astype('float32').as_in_context(ctx),\n                  valid_length.astype('float32').as_in_context(ctx))\n\n        output = nd.split(out, axis=2, num_outputs=2)\n        start_logits = output[0].reshape((0, -3)).asnumpy()\n        end_logits = output[1].reshape((0, -3)).asnumpy()\n\n        for example_id, start, end in zip(example_ids, start_logits, end_logits):\n            example_id = example_id.asscalar()\n            if example_id not in all_results:\n                all_results[example_id] = []\n            all_results[example_id].append(\n                _Result(example_id, start.tolist(), end.tolist()))\n    epoch_toc = time.time()\n    log.info('Inference time cost={:.2f} s, Thoughput={:.2f} samples/s'\n             .format(epoch_toc - epoch_tic,\n                     len(dev_dataloader) / (epoch_toc - epoch_tic)))\n    log.info('Get prediction results...')\n\n    all_predictions, all_nbest_json, scores_diff_json = predictions(\n        dev_dataset=dev_dataset,\n        all_results=all_results,\n        tokenizer=nlp.data.BERTBasicTokenizer(lower=lower),\n        max_answer_length=max_answer_length,\n        null_score_diff_threshold=null_score_diff_threshold,\n        n_best_size=n_best_size,\n        version_2=version_2)\n\n    with open(os.path.join(output_dir, 'predictions.json'),\n              'w', encoding='utf-8') as all_predictions_write:\n        all_predictions_write.write(json.dumps(all_predictions))\n\n    with open(os.path.join(output_dir, 'nbest_predictions.json'),\n              'w', encoding='utf-8') as all_predictions_write:\n        all_predictions_write.write(json.dumps(all_nbest_json))\n\n    if version_2:\n        with open(os.path.join(output_dir, 'null_odds.json'),\n                  'w', encoding='utf-8') as all_predictions_write:\n            all_predictions_write.write(json.dumps(scores_diff_json))\n    else:\n        log.info(get_F1_EM(dev_data, all_predictions))", "response": "Evaluate the model on validation dataset."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _pad_arrs_to_max_length(arrs, pad_axis, pad_val, use_shared_mem, dtype):\n    if isinstance(arrs[0], mx.nd.NDArray):\n        dtype = arrs[0].dtype if dtype is None else dtype\n        arrs = [arr.asnumpy() for arr in arrs]\n    elif not isinstance(arrs[0], np.ndarray):\n        arrs = [np.asarray(ele) for ele in arrs]\n    else:\n        dtype = arrs[0].dtype if dtype is None else dtype\n\n    original_length = [ele.shape[pad_axis] for ele in arrs]\n    max_size = max(original_length)\n\n    ret_shape = list(arrs[0].shape)\n    ret_shape[pad_axis] = max_size\n    ret_shape = (len(arrs), ) + tuple(ret_shape)\n\n    ret = np.full(shape=ret_shape, fill_value=pad_val, dtype=dtype)\n\n    for i, arr in enumerate(arrs):\n        if arr.shape[pad_axis] == max_size:\n            ret[i] = arr\n        else:\n            slices = [slice(None) for _ in range(arr.ndim)]\n            slices[pad_axis] = slice(0, arr.shape[pad_axis])\n            if slices[pad_axis].start != slices[pad_axis].stop:\n                slices = [slice(i, i + 1)] + slices\n                ret[tuple(slices)] = arr\n\n    ctx = mx.Context('cpu_shared', 0) if use_shared_mem else mx.cpu()\n    ret = mx.nd.array(ret, ctx=ctx, dtype=dtype)\n    original_length = mx.nd.array(original_length, ctx=ctx, dtype=np.int32)\n\n    return ret, original_length", "response": "This function pads the array to the maximum length of the object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntrain a deep biaffine dependency parser.", "response": "def train(self, train_file, dev_file, test_file, save_dir, pretrained_embeddings=None, min_occur_count=2,\n              lstm_layers=3, word_dims=100, tag_dims=100, dropout_emb=0.33, lstm_hiddens=400,\n              dropout_lstm_input=0.33, dropout_lstm_hidden=0.33, mlp_arc_size=500, mlp_rel_size=100,\n              dropout_mlp=0.33, learning_rate=2e-3, decay=.75, decay_steps=5000, beta_1=.9, beta_2=.9, epsilon=1e-12,\n              num_buckets_train=40,\n              num_buckets_valid=10, num_buckets_test=10, train_iters=50000, train_batch_size=5000,\n              test_batch_size=5000, validate_every=100, save_after=5000, debug=False):\n        \"\"\"Train a deep biaffine dependency parser\n\n        Parameters\n        ----------\n        train_file : str\n            path to training set\n        dev_file : str\n            path to dev set\n        test_file : str\n            path to test set\n        save_dir : str\n            a directory for saving model and related meta-data\n        pretrained_embeddings : tuple\n            (embedding_name, source), used for gluonnlp.embedding.create(embedding_name, source)\n        min_occur_count : int\n            threshold of rare words, which will be replaced with UNKs,\n        lstm_layers : int\n            layers of lstm\n        word_dims : int\n            dimension of word embedding\n        tag_dims : int\n            dimension of tag embedding\n        dropout_emb : float\n            word dropout\n        lstm_hiddens : int\n            size of lstm hidden states\n        dropout_lstm_input : int\n            dropout on x in variational RNN\n        dropout_lstm_hidden : int\n            dropout on h in variational RNN\n        mlp_arc_size : int\n            output size of MLP for arc feature extraction\n        mlp_rel_size : int\n            output size of MLP for rel feature extraction\n        dropout_mlp : float\n            dropout on the output of LSTM\n        learning_rate : float\n            learning rate\n        decay : float\n            see ExponentialScheduler\n        decay_steps : int\n            see ExponentialScheduler\n        beta_1 : float\n            see ExponentialScheduler\n        beta_2 : float\n            see ExponentialScheduler\n        epsilon : float\n            see ExponentialScheduler\n        num_buckets_train : int\n            number of buckets for training data set\n        num_buckets_valid : int\n            number of buckets for dev data set\n        num_buckets_test : int\n            number of buckets for testing data set\n        train_iters : int\n            training iterations\n        train_batch_size : int\n            training batch size\n        test_batch_size : int\n            test batch size\n        validate_every : int\n            validate on dev set every such number of batches\n        save_after : int\n            skip saving model in early epochs\n        debug : bool\n            debug mode\n\n        Returns\n        -------\n        DepParser\n            parser itself\n        \"\"\"\n        logger = init_logger(save_dir)\n        config = _Config(train_file, dev_file, test_file, save_dir, pretrained_embeddings, min_occur_count,\n                         lstm_layers, word_dims, tag_dims, dropout_emb, lstm_hiddens, dropout_lstm_input,\n                         dropout_lstm_hidden, mlp_arc_size, mlp_rel_size, dropout_mlp, learning_rate, decay,\n                         decay_steps,\n                         beta_1, beta_2, epsilon, num_buckets_train, num_buckets_valid, num_buckets_test, train_iters,\n                         train_batch_size, debug)\n        config.save()\n        self._vocab = vocab = ParserVocabulary(train_file,\n                                               pretrained_embeddings,\n                                               min_occur_count)\n        vocab.save(config.save_vocab_path)\n        vocab.log_info(logger)\n\n        with mx.Context(mxnet_prefer_gpu()):\n\n            self._parser = parser = BiaffineParser(vocab, word_dims, tag_dims,\n                                                   dropout_emb,\n                                                   lstm_layers,\n                                                   lstm_hiddens, dropout_lstm_input,\n                                                   dropout_lstm_hidden,\n                                                   mlp_arc_size,\n                                                   mlp_rel_size, dropout_mlp, debug)\n            parser.initialize()\n            scheduler = ExponentialScheduler(learning_rate, decay, decay_steps)\n            optimizer = mx.optimizer.Adam(learning_rate, beta_1, beta_2, epsilon,\n                                          lr_scheduler=scheduler)\n            trainer = gluon.Trainer(parser.collect_params(), optimizer=optimizer)\n            data_loader = DataLoader(train_file, num_buckets_train, vocab)\n            global_step = 0\n            best_UAS = 0.\n            batch_id = 0\n            epoch = 1\n            total_epoch = math.ceil(train_iters / validate_every)\n            logger.info(\"Epoch {} out of {}\".format(epoch, total_epoch))\n            bar = Progbar(target=min(validate_every, data_loader.samples))\n            while global_step < train_iters:\n                for words, tags, arcs, rels in data_loader.get_batches(batch_size=train_batch_size,\n                                                                       shuffle=True):\n                    with autograd.record():\n                        arc_accuracy, rel_accuracy, overall_accuracy, loss = parser.forward(words, tags, arcs,\n                                                                                            rels)\n                        loss_value = loss.asscalar()\n                    loss.backward()\n                    trainer.step(train_batch_size)\n                    batch_id += 1\n                    try:\n                        bar.update(batch_id,\n                                   exact=[(\"UAS\", arc_accuracy, 2),\n                                          # (\"LAS\", rel_accuracy, 2),\n                                          # (\"ALL\", overall_accuracy, 2),\n                                          (\"loss\", loss_value)])\n                    except OverflowError:\n                        pass  # sometimes loss can be 0 or infinity, crashes the bar\n\n                    global_step += 1\n                    if global_step % validate_every == 0:\n                        bar = Progbar(target=min(validate_every, train_iters - global_step))\n                        batch_id = 0\n                        UAS, LAS, speed = evaluate_official_script(parser, vocab, num_buckets_valid,\n                                                                   test_batch_size,\n                                                                   dev_file,\n                                                                   os.path.join(save_dir, 'valid_tmp'))\n                        logger.info('Dev: UAS %.2f%% LAS %.2f%% %d sents/s' % (UAS, LAS, speed))\n                        epoch += 1\n                        if global_step < train_iters:\n                            logger.info(\"Epoch {} out of {}\".format(epoch, total_epoch))\n                        if global_step > save_after and UAS > best_UAS:\n                            logger.info('- new best score!')\n                            best_UAS = UAS\n                            parser.save(config.save_model_path)\n\n        # When validate_every is too big\n        if not os.path.isfile(config.save_model_path) or best_UAS != UAS:\n            parser.save(config.save_model_path)\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load(self, path):\n        config = _Config.load(os.path.join(path, 'config.pkl'))\n        config.save_dir = path  # redirect root path to what user specified\n        self._vocab = vocab = ParserVocabulary.load(config.save_vocab_path)\n        with mx.Context(mxnet_prefer_gpu()):\n            self._parser = BiaffineParser(vocab, config.word_dims, config.tag_dims, config.dropout_emb,\n                                          config.lstm_layers,\n                                          config.lstm_hiddens, config.dropout_lstm_input, config.dropout_lstm_hidden,\n                                          config.mlp_arc_size,\n                                          config.mlp_rel_size, config.dropout_mlp, config.debug)\n            self._parser.load(config.save_model_path)\n        return self", "response": "Loads the DepParser\n            from disk and returns the DepParser\n            object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nevaluates on the test set and return the UAS and LAS.", "response": "def evaluate(self, test_file, save_dir=None, logger=None, num_buckets_test=10, test_batch_size=5000):\n        \"\"\"Run evaluation on test set\n\n        Parameters\n        ----------\n        test_file : str\n            path to test set\n        save_dir : str\n            where to store intermediate results and log\n        logger : logging.logger\n            logger for printing results\n        num_buckets_test : int\n            number of clusters for sentences from test set\n        test_batch_size : int\n            batch size of test set\n\n        Returns\n        -------\n        tuple\n            UAS, LAS\n        \"\"\"\n        parser = self._parser\n        vocab = self._vocab\n        with mx.Context(mxnet_prefer_gpu()):\n            UAS, LAS, speed = evaluate_official_script(parser, vocab, num_buckets_test, test_batch_size,\n                                                       test_file, os.path.join(save_dir, 'valid_tmp'))\n        if logger is None:\n            logger = init_logger(save_dir, 'test.log')\n        logger.info('Test: UAS %.2f%% LAS %.2f%% %d sents/s' % (UAS, LAS, speed))\n\n        return UAS, LAS"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse(self, sentence):\n        words = np.zeros((len(sentence) + 1, 1), np.int32)\n        tags = np.zeros((len(sentence) + 1, 1), np.int32)\n        words[0, 0] = ParserVocabulary.ROOT\n        tags[0, 0] = ParserVocabulary.ROOT\n        vocab = self._vocab\n\n        for i, (word, tag) in enumerate(sentence):\n            words[i + 1, 0], tags[i + 1, 0] = vocab.word2id(word.lower()), vocab.tag2id(tag)\n\n        with mx.Context(mxnet_prefer_gpu()):\n            outputs = self._parser.forward(words, tags)\n        words = []\n        for arc, rel, (word, tag) in zip(outputs[0][0], outputs[0][1], sentence):\n            words.append(ConllWord(id=len(words) + 1, form=word, pos=tag, head=arc, relation=vocab.id2rel(rel)))\n        return ConllSentence(words)", "response": "Parse a raw sentence into a ConllSentence object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef apply_weight_drop(block, local_param_regex, rate, axes=(),\n                      weight_dropout_mode='training'):\n    \"\"\"Apply weight drop to the parameter of a block.\n\n    Parameters\n    ----------\n    block : Block or HybridBlock\n        The block whose parameter is to be applied weight-drop.\n    local_param_regex : str\n        The regex for parameter names used in the self.params.get(), such as 'weight'.\n    rate : float\n        Fraction of the input units to drop. Must be a number between 0 and 1.\n    axes : tuple of int, default ()\n        The axes on which dropout mask is shared. If empty, regular dropout is applied.\n    weight_drop_mode : {'training', 'always'}, default 'training'\n        Whether the weight dropout should be applied only at training time, or always be applied.\n\n    Examples\n    --------\n    >>> net = gluon.rnn.LSTM(10, num_layers=2, bidirectional=True)\n    >>> gluonnlp.model.apply_weight_drop(net, r'.*h2h_weight', 0.5)\n    >>> net.collect_params()\n    lstm0_ (\n      Parameter lstm0_l0_i2h_weight (shape=(40, 0), dtype=<class 'numpy.float32'>)\n      WeightDropParameter lstm0_l0_h2h_weight (shape=(40, 10), dtype=<class 'numpy.float32'>, \\\nrate=0.5, mode=training)\n      Parameter lstm0_l0_i2h_bias (shape=(40,), dtype=<class 'numpy.float32'>)\n      Parameter lstm0_l0_h2h_bias (shape=(40,), dtype=<class 'numpy.float32'>)\n      Parameter lstm0_r0_i2h_weight (shape=(40, 0), dtype=<class 'numpy.float32'>)\n      WeightDropParameter lstm0_r0_h2h_weight (shape=(40, 10), dtype=<class 'numpy.float32'>, \\\nrate=0.5, mode=training)\n      Parameter lstm0_r0_i2h_bias (shape=(40,), dtype=<class 'numpy.float32'>)\n      Parameter lstm0_r0_h2h_bias (shape=(40,), dtype=<class 'numpy.float32'>)\n      Parameter lstm0_l1_i2h_weight (shape=(40, 20), dtype=<class 'numpy.float32'>)\n      WeightDropParameter lstm0_l1_h2h_weight (shape=(40, 10), dtype=<class 'numpy.float32'>, \\\nrate=0.5, mode=training)\n      Parameter lstm0_l1_i2h_bias (shape=(40,), dtype=<class 'numpy.float32'>)\n      Parameter lstm0_l1_h2h_bias (shape=(40,), dtype=<class 'numpy.float32'>)\n      Parameter lstm0_r1_i2h_weight (shape=(40, 20), dtype=<class 'numpy.float32'>)\n      WeightDropParameter lstm0_r1_h2h_weight (shape=(40, 10), dtype=<class 'numpy.float32'>, \\\nrate=0.5, mode=training)\n      Parameter lstm0_r1_i2h_bias (shape=(40,), dtype=<class 'numpy.float32'>)\n      Parameter lstm0_r1_h2h_bias (shape=(40,), dtype=<class 'numpy.float32'>)\n    )\n    >>> ones = mx.nd.ones((3, 4, 5))\n    >>> net.initialize()\n    >>> with mx.autograd.train_mode():\n    ...     net(ones).max().asscalar() != net(ones).max().asscalar()\n    True\n    \"\"\"\n    if not rate:\n        return\n\n    existing_params = _find_params(block, local_param_regex)\n    for (local_param_name, param), \\\n            (ref_params_list, ref_reg_params_list) in existing_params.items():\n        dropped_param = WeightDropParameter(param, rate, weight_dropout_mode, axes)\n        for ref_params in ref_params_list:\n            ref_params[param.name] = dropped_param\n        for ref_reg_params in ref_reg_params_list:\n            ref_reg_params[local_param_name] = dropped_param\n            if hasattr(block, local_param_name):\n                local_attr = getattr(block, local_param_name)\n                if local_attr == param:\n                    local_attr = dropped_param\n                elif isinstance(local_attr, (list, tuple)):\n                    if isinstance(local_attr, tuple):\n                        local_attr = list(local_attr)\n                    for i, v in enumerate(local_attr):\n                        if v == param:\n                            local_attr[i] = dropped_param\n                elif isinstance(local_attr, dict):\n                    for k, v in local_attr:\n                        if v == param:\n                            local_attr[k] = dropped_param\n                else:\n                    continue\n                if local_attr:\n                    super(Block, block).__setattr__(local_param_name, local_attr)", "response": "Applies weight drop to the parameter of a hybrid block."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_rnn_cell(mode, num_layers, input_size, hidden_size,\n                  dropout, weight_dropout,\n                  var_drop_in, var_drop_state, var_drop_out,\n                  skip_connection, proj_size=None, cell_clip=None, proj_clip=None):\n    \"\"\"create rnn cell given specs\n\n    Parameters\n    ----------\n    mode : str\n        The type of RNN cell to use. Options are 'lstmpc', 'rnn_tanh', 'rnn_relu', 'lstm', 'gru'.\n    num_layers : int\n        The number of RNN cells in the encoder.\n    input_size : int\n        The initial input size of in the RNN cell.\n    hidden_size : int\n        The hidden size of the RNN cell.\n    dropout : float\n        The dropout rate to use for encoder output.\n    weight_dropout: float\n        The dropout rate to the hidden to hidden connections.\n    var_drop_in: float\n        The variational dropout rate for inputs. Won\u2019t apply dropout if it equals 0.\n    var_drop_state: float\n        The variational dropout rate for state inputs on the first state channel.\n        Won\u2019t apply dropout if it equals 0.\n    var_drop_out: float\n        The variational dropout rate for outputs. Won\u2019t apply dropout if it equals 0.\n    skip_connection : bool\n        Whether to add skip connections (add RNN cell input to output)\n    proj_size : int\n        The projection size of each LSTMPCellWithClip cell.\n        Only available when the mode=lstmpc.\n    cell_clip : float\n        Clip cell state between [-cellclip, cell_clip] in LSTMPCellWithClip cell.\n        Only available when the mode=lstmpc.\n    proj_clip : float\n        Clip projection between [-projclip, projclip] in LSTMPCellWithClip cell\n        Only available when the mode=lstmpc.\n    \"\"\"\n\n    assert mode == 'lstmpc' and proj_size is not None, \\\n        'proj_size takes effect only when mode is lstmpc'\n    assert mode == 'lstmpc' and cell_clip is not None, \\\n        'cell_clip takes effect only when mode is lstmpc'\n    assert mode == 'lstmpc' and proj_clip is not None, \\\n        'proj_clip takes effect only when mode is lstmpc'\n\n    rnn_cell = rnn.HybridSequentialRNNCell()\n    with rnn_cell.name_scope():\n        for i in range(num_layers):\n            if mode == 'rnn_relu':\n                cell = rnn.RNNCell(hidden_size, 'relu', input_size=input_size)\n            elif mode == 'rnn_tanh':\n                cell = rnn.RNNCell(hidden_size, 'tanh', input_size=input_size)\n            elif mode == 'lstm':\n                cell = rnn.LSTMCell(hidden_size, input_size=input_size)\n            elif mode == 'gru':\n                cell = rnn.GRUCell(hidden_size, input_size=input_size)\n            elif mode == 'lstmpc':\n                cell = LSTMPCellWithClip(hidden_size, proj_size,\n                                         cell_clip=cell_clip,\n                                         projection_clip=proj_clip,\n                                         input_size=input_size)\n            if var_drop_in + var_drop_state + var_drop_out != 0:\n                cell = contrib.rnn.VariationalDropoutCell(cell,\n                                                          var_drop_in,\n                                                          var_drop_state,\n                                                          var_drop_out)\n\n            if skip_connection:\n                cell = rnn.ResidualCell(cell)\n\n            rnn_cell.add(cell)\n\n            if i != num_layers - 1 and dropout != 0:\n                rnn_cell.add(rnn.DropoutCell(dropout))\n\n            if weight_dropout:\n                apply_weight_drop(rnn_cell, 'h2h_weight', rate=weight_dropout)\n\n    return rnn_cell", "response": "Create a new rnn cell given the parameters."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_rnn_layer(mode, num_layers, input_size, hidden_size, dropout, weight_dropout):\n    if mode == 'rnn_relu':\n        rnn_block = functools.partial(rnn.RNN, activation='relu')\n    elif mode == 'rnn_tanh':\n        rnn_block = functools.partial(rnn.RNN, activation='tanh')\n    elif mode == 'lstm':\n        rnn_block = rnn.LSTM\n    elif mode == 'gru':\n        rnn_block = rnn.GRU\n\n    block = rnn_block(hidden_size, num_layers, dropout=dropout,\n                      input_size=input_size)\n\n    if weight_dropout:\n        apply_weight_drop(block, '.*h2h_weight', rate=weight_dropout)\n\n    return block", "response": "create rnn layer given specs"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _extract_and_flatten_nested_structure(data, flattened=None):\n    if flattened is None:\n        flattened = []\n        structure = _extract_and_flatten_nested_structure(data, flattened)\n        return structure, flattened\n    if isinstance(data, list):\n        return list(_extract_and_flatten_nested_structure(x, flattened) for x in data)\n    elif isinstance(data, tuple):\n        return tuple(_extract_and_flatten_nested_structure(x, flattened) for x in data)\n    elif isinstance(data, dict):\n        return {k: _extract_and_flatten_nested_structure(v) for k, v in data.items()}\n    elif isinstance(data, (mx.sym.Symbol, mx.nd.NDArray)):\n        flattened.append(data)\n        return len(flattened) - 1\n    else:\n        raise NotImplementedError", "response": "Flatten the structure of a nested container to a list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreconstruct the flattened list back to ( possibly nested structure.", "response": "def _reconstruct_flattened_structure(structure, flattened):\n    \"\"\"Reconstruct the flattened list back to (possibly) nested structure.\n\n    Parameters\n    ----------\n    structure : An integer or a nested container with integers.\n        The extracted structure of the container of `data`.\n    flattened : list or None\n        The container thats holds flattened result.\n    Returns\n    -------\n    data : A single NDArray/Symbol or nested container with NDArrays/Symbol.\n        The nested container that was flattened.\n    \"\"\"\n    if isinstance(structure, list):\n        return list(_reconstruct_flattened_structure(x, flattened) for x in structure)\n    elif isinstance(structure, tuple):\n        return tuple(_reconstruct_flattened_structure(x, flattened) for x in structure)\n    elif isinstance(structure, dict):\n        return {k: _reconstruct_flattened_structure(v, flattened) for k, v in structure.items()}\n    elif isinstance(structure, int):\n        return flattened[structure]\n    else:\n        raise NotImplementedError"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _expand_to_beam_size(data, beam_size, batch_size, state_info=None):\n    assert not state_info or isinstance(state_info, (type(data), dict)), \\\n            'data and state_info doesn\\'t match, ' \\\n            'got: {} vs {}.'.format(type(state_info), type(data))\n    if isinstance(data, list):\n        if not state_info:\n            state_info = [None] * len(data)\n        return [_expand_to_beam_size(d, beam_size, batch_size, s)\n                for d, s in zip(data, state_info)]\n    elif isinstance(data, tuple):\n        if not state_info:\n            state_info = [None] * len(data)\n            state_info = tuple(state_info)\n        return tuple(_expand_to_beam_size(d, beam_size, batch_size, s)\n                     for d, s in zip(data, state_info))\n    elif isinstance(data, dict):\n        if not state_info:\n            state_info = {k: None for k in data.keys()}\n        return {k: _expand_to_beam_size(v, beam_size, batch_size, state_info[k])\n                for k, v in data.items()}\n    elif isinstance(data, mx.nd.NDArray):\n        if not state_info:\n            batch_axis = 0\n        else:\n            batch_axis = state_info['__layout__'].find('N')\n        if data.shape[batch_axis] != batch_size:\n            raise ValueError('The batch dimension of all the inner elements in states must be '\n                             '{}, Found shape={}'.format(batch_size, data.shape))\n        new_shape = list(data.shape)\n        new_shape[batch_axis] = batch_size * beam_size\n        new_shape = tuple(new_shape)\n        return data.expand_dims(batch_axis+1)\\\n                   .broadcast_axes(axis=batch_axis+1, size=beam_size)\\\n                   .reshape(new_shape)\n    elif isinstance(data, mx.sym.Symbol):\n        if not state_info:\n            batch_axis = 0\n        else:\n            batch_axis = state_info['__layout__'].find('N')\n        new_shape = (0, ) * batch_axis + (-3, -2)\n        return data.expand_dims(batch_axis+1)\\\n                   .broadcast_axes(axis=batch_axis+1, size=beam_size)\\\n                   .reshape(new_shape)\n    else:\n        raise NotImplementedError", "response": "Expand all the states in data to have batch_size * beam_size on the batch axis."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsample by beam search. Parameters ---------- F inputs : NDArray or Symbol The initial input of the decoder. Shape is (batch_size,). states : Object that contains NDArrays or Symbols The initial states of the decoder. Returns ------- samples : NDArray or Symbol Samples draw by beam search. Shape (batch_size, beam_size, length). dtype is int32. scores : NDArray or Symbol Scores of the samples. Shape (batch_size, beam_size). We make sure that scores[i, :] are in descending order. valid_length : NDArray or Symbol The valid length of the samples. Shape (batch_size, beam_size). dtype will be int32.", "response": "def hybrid_forward(self, F, inputs, states):   # pylint: disable=arguments-differ\n        \"\"\"Sample by beam search.\n\n        Parameters\n        ----------\n        F\n        inputs : NDArray or Symbol\n            The initial input of the decoder. Shape is (batch_size,).\n        states : Object that contains NDArrays or Symbols\n            The initial states of the decoder.\n        Returns\n        -------\n        samples : NDArray or Symbol\n            Samples draw by beam search. Shape (batch_size, beam_size, length). dtype is int32.\n        scores : NDArray or Symbol\n            Scores of the samples. Shape (batch_size, beam_size). We make sure that scores[i, :] are\n            in descending order.\n        valid_length : NDArray or Symbol\n            The valid length of the samples. Shape (batch_size, beam_size). dtype will be int32.\n        \"\"\"\n        batch_size = self._batch_size\n        beam_size = self._beam_size\n        vocab_size = self._vocab_size\n        # Tile the states and inputs to have shape (batch_size * beam_size, ...)\n        state_info = self._state_info_func(batch_size)\n        step_input = _expand_to_beam_size(inputs, beam_size=beam_size,\n                                          batch_size=batch_size).astype(np.int32)\n        states = _expand_to_beam_size(states, beam_size=beam_size, batch_size=batch_size,\n                                      state_info=state_info)\n        state_structure, states = _extract_and_flatten_nested_structure(states)\n        if beam_size == 1:\n            init_scores = F.zeros(shape=(batch_size, 1))\n        else:\n            init_scores = F.concat(\n                F.zeros(shape=(batch_size, 1)),\n                F.full(shape=(batch_size, beam_size - 1), val=LARGE_NEGATIVE_FLOAT),\n                dim=1)\n        vocab_size = F.full(shape=(1,), val=vocab_size, dtype=np.int32)\n        batch_shift = F.arange(0, batch_size * beam_size, beam_size, dtype=np.int32)\n\n        def _loop_cond(_i, _samples, _indices, _step_input, _valid_length, _scores, \\\n            beam_alive_mask, *_states):\n            return F.sum(beam_alive_mask) > 0\n\n        def _loop_func(i, samples, indices, step_input, valid_length, scores, \\\n            beam_alive_mask, *states):\n            outputs, new_states = self._decoder(\n                step_input, _reconstruct_flattened_structure(state_structure, states))\n            step = i + 1\n            new_samples, new_valid_length, new_scores, \\\n                chosen_word_ids, new_beam_alive_mask, new_new_states = \\\n                self._updater(samples, valid_length, outputs, scores, step.astype(np.float32),\n                              beam_alive_mask,\n                              _extract_and_flatten_nested_structure(new_states)[-1],\n                              vocab_size, batch_shift)\n            new_step_input = F.relu(chosen_word_ids).reshape((-1,))\n            # We are doing `new_indices = indices[1 : ] + indices[ : 1]`\n            new_indices = F.concat(\n                indices.slice_axis(axis=0, begin=1, end=None),\n                indices.slice_axis(axis=0, begin=0, end=1),\n                dim=0)\n            return [], (step, new_samples, new_indices, new_step_input, new_valid_length, \\\n                   new_scores, new_beam_alive_mask) + tuple(new_new_states)\n\n        _, pad_samples, indices, _, new_valid_length, new_scores, new_beam_alive_mask = \\\n            F.contrib.while_loop(\n                cond=_loop_cond, func=_loop_func, max_iterations=self._max_length,\n                loop_vars=(\n                    F.zeros(shape=(1,), dtype=np.int32),                        # i\n                    F.zeros(shape=(batch_size, beam_size, self._max_length),\n                            dtype=np.int32),                                    # samples\n                    F.arange(start=0, stop=self._max_length, dtype=np.int32),   # indices\n                    step_input,                                                 # step_input\n                    F.ones(shape=(batch_size, beam_size), dtype=np.int32),      # valid_length\n                    init_scores,                                                # scores\n                    F.ones(shape=(batch_size, beam_size), dtype=np.int32),      # beam_alive_mask\n                ) + tuple(states)\n            )[1][:7]                                                            # I hate Python 2\n        samples = pad_samples.take(indices, axis=2)\n\n        def _then_func():\n            new_samples = F.concat(\n                step_input.reshape((batch_size, beam_size, 1)),\n                samples,\n                F.full(shape=(batch_size, beam_size, 1), val=-1, dtype=np.int32),\n                dim=2,\n                name='concat3')\n            new_new_valid_length = new_valid_length\n            return new_samples, new_new_valid_length\n\n        def _else_func():\n            final_word = F.where(new_beam_alive_mask,\n                                 F.full(shape=(batch_size, beam_size), val=self._eos_id,\n                                        dtype=np.int32),\n                                 F.full(shape=(batch_size, beam_size), val=-1, dtype=np.int32))\n            new_samples = F.concat(\n                step_input.reshape((batch_size, beam_size, 1)),\n                samples,\n                final_word.reshape((0, 0, 1)),\n                dim=2)\n            new_new_valid_length = new_valid_length + new_beam_alive_mask\n            return new_samples, new_new_valid_length\n\n        new_samples, new_new_valid_length = \\\n            F.contrib.cond(F.sum(new_beam_alive_mask) == 0, _then_func, _else_func)\n        return new_samples, new_scores, new_new_valid_length"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a copy of this parameter on one context.", "response": "def data(self, ctx=None):\n        \"\"\"Returns a copy of this parameter on one context. Must have been\n        initialized on this context before.\n\n        Parameters\n        ----------\n        ctx : Context\n            Desired context.\n        Returns\n        -------\n        NDArray on ctx\n        \"\"\"\n        d = self._check_and_get(self._data, ctx)\n        if self._rate:\n            d = nd.Dropout(d, self._rate, self._mode, self._axes)\n        return d"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef elmo_2x1024_128_2048cnn_1xhighway(dataset_name=None, pretrained=False, ctx=mx.cpu(),\n                                      root=os.path.join(get_home_dir(), 'models'), **kwargs):\n    r\"\"\"ELMo 2-layer BiLSTM with 1024 hidden units, 128 projection size, 1 highway layer.\n\n    Parameters\n    ----------\n    dataset_name : str or None, default None\n        The dataset name on which the pre-trained model is trained.\n        Options are 'gbw'.\n    pretrained : bool, default False\n        Whether to load the pre-trained weights for model.\n    ctx : Context, default CPU\n        The context in which to load the pre-trained weights.\n    root : str, default '$MXNET_HOME/models'\n        Location for keeping the model parameters.\n        MXNET_HOME defaults to '~/.mxnet'.\n\n    Returns\n    -------\n    gluon.Block\n    \"\"\"\n\n    predefined_args = {'rnn_type': 'lstmpc',\n                       'output_size': 128,\n                       'filters': [[1, 32], [2, 32], [3, 64], [4, 128],\n                                   [5, 256], [6, 512], [7, 1024]],\n                       'char_embed_size': 16,\n                       'num_highway': 1,\n                       'conv_layer_activation': 'relu',\n                       'max_chars_per_token': 50,\n                       'input_size': 128,\n                       'hidden_size': 1024,\n                       'proj_size': 128,\n                       'num_layers': 2,\n                       'cell_clip': 3,\n                       'proj_clip': 3,\n                       'skip_connection': True}\n    assert all((k not in kwargs) for k in predefined_args), \\\n           'Cannot override predefined model settings.'\n    predefined_args.update(kwargs)\n    return _get_elmo_model(ELMoBiLM, 'elmo_2x1024_128_2048cnn_1xhighway', dataset_name, pretrained,\n                           ctx, root, **predefined_args)", "response": "r Returns a pre - trained 2 - layer BiLSTM model with 1024 hidden units 128 projection size 1 highway layer."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute context insensitive token embeddings for ELMo representations.", "response": "def hybrid_forward(self, F, inputs):\n        # pylint: disable=arguments-differ\n        \"\"\"\n        Compute context insensitive token embeddings for ELMo representations.\n\n        Parameters\n        ----------\n        inputs : NDArray\n            Shape (batch_size, sequence_length, max_character_per_token)\n            of character ids representing the current batch.\n\n        Returns\n        -------\n        token_embedding : NDArray\n            Shape (batch_size, sequence_length, embedding_size) with context\n            insensitive token representations.\n        \"\"\"\n        # the character id embedding\n        # (batch_size * sequence_length, max_chars_per_token, embed_dim)\n        character_embedding = self._char_embedding(inputs.reshape((-1, self._max_chars_per_token)))\n\n        character_embedding = F.transpose(character_embedding, axes=(1, 0, 2))\n        token_embedding = self._convolutions(character_embedding)\n\n        out_shape_ref = inputs.slice_axis(axis=-1, begin=0, end=1)\n        out_shape_ref = out_shape_ref.broadcast_axes(axis=(2,),\n                                                     size=(self._output_size))\n\n        return token_embedding.reshape_like(out_shape_ref)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef hybrid_forward(self, F, inputs, states=None, mask=None):\n        # pylint: disable=arguments-differ\n        \"\"\"\n        Parameters\n        ----------\n        inputs : NDArray\n            Shape (batch_size, sequence_length, max_character_per_token)\n            of character ids representing the current batch.\n        states : (list of list of NDArray, list of list of NDArray)\n            The states. First tuple element is the forward layer states, while the second is\n            the states from backward layer. Each is a list of states for each layer.\n            The state of each layer has a list of two initial tensors with\n            shape (batch_size, proj_size) and (batch_size, hidden_size).\n        mask :  NDArray\n            Shape (batch_size, sequence_length) with sequence mask.\n\n        Returns\n        -------\n        output : list of NDArray\n            A list of activations at each layer of the network, each of shape\n            (batch_size, sequence_length, embedding_size)\n        states : (list of list of NDArray, list of list of NDArray)\n            The states. First tuple element is the forward layer states, while the second is\n            the states from backward layer. Each is a list of states for each layer.\n            The state of each layer has a list of two initial tensors with\n            shape (batch_size, proj_size) and (batch_size, hidden_size).\n        \"\"\"\n\n        type_representation = self._elmo_char_encoder(inputs)\n        type_representation = type_representation.transpose(axes=(1, 0, 2))\n        lstm_outputs, states = self._elmo_lstm(type_representation, states, mask)\n        lstm_outputs = lstm_outputs.transpose(axes=(0, 2, 1, 3))\n        type_representation = type_representation.transpose(axes=(1, 0, 2))\n\n        # Prepare the output. The first layer is duplicated.\n        output = F.concat(*[type_representation, type_representation], dim=-1)\n        if mask is not None:\n            output = output * mask.expand_dims(axis=-1)\n        output = [output]\n        output.extend([layer_activations.squeeze(axis=0) for layer_activations\n                       in F.split(lstm_outputs, self._num_layers, axis=0)])\n        return output, states", "response": "This is the forward transformation for the current batch."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef big_rnn_lm_2048_512(dataset_name=None, vocab=None, pretrained=False, ctx=cpu(),\n                        root=os.path.join(get_home_dir(), 'models'), **kwargs):\n    r\"\"\"Big 1-layer LSTMP language model.\n\n    Both embedding and projection size are 512. Hidden size is 2048.\n\n    Parameters\n    ----------\n    dataset_name : str or None, default None\n        The dataset name on which the pre-trained model is trained.\n        Options are 'gbw'. If specified, then the returned vocabulary is extracted from\n        the training set of the dataset.\n        If None, then vocab is required, for specifying embedding weight size, and is directly\n        returned.\n        The pre-trained model achieves 44.05 ppl on Test of GBW dataset.\n    vocab : gluonnlp.Vocab or None, default None\n        Vocabulary object to be used with the language model.\n        Required when dataset_name is not specified.\n    pretrained : bool, default False\n        Whether to load the pre-trained weights for model.\n    ctx : Context, default CPU\n        The context in which to load the pre-trained weights.\n    root : str, default '$MXNET_HOME/models'\n        Location for keeping the model parameters.\n        MXNET_HOME defaults to '~/.mxnet'.\n\n    Returns\n    -------\n    gluon.Block, gluonnlp.Vocab\n    \"\"\"\n    predefined_args = {'embed_size': 512,\n                       'hidden_size': 2048,\n                       'projection_size': 512,\n                       'num_layers': 1,\n                       'embed_dropout': 0.1,\n                       'encode_dropout': 0.1}\n    mutable_args = ['embed_dropout', 'encode_dropout']\n    assert all((k not in kwargs or k in mutable_args) for k in predefined_args), \\\n           'Cannot override predefined model settings.'\n    predefined_args.update(kwargs)\n    return _get_rnn_model(BigRNN, 'big_rnn_lm_2048_512', dataset_name, vocab, pretrained,\n                          ctx, root, **predefined_args)", "response": "r Big 1 - layer LSTMP language model."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef forward(self, inputs, begin_state=None): # pylint: disable=arguments-differ\n        encoded = self.embedding(inputs)\n        if begin_state is None:\n            begin_state = self.begin_state(batch_size=inputs.shape[1])\n        out_states = []\n        for i, (e, s) in enumerate(zip(self.encoder, begin_state)):\n            encoded, state = e(encoded, s)\n            out_states.append(state)\n            if self._drop_h and i != len(self.encoder)-1:\n                encoded = nd.Dropout(encoded, p=self._drop_h, axes=(0,))\n        if self._dropout:\n            encoded = nd.Dropout(encoded, p=self._dropout, axes=(0,))\n        with autograd.predict_mode():\n            out = self.decoder(encoded)\n        return out, out_states", "response": "Implement forward computation.\n\n        Parameters\n        -----------\n        inputs : NDArray\n            input tensor with shape `(sequence_length, batch_size)`\n            when `layout` is \"TNC\".\n        begin_state : list\n            initial recurrent state tensor with length equals to num_layers.\n            the initial state with shape `(1, batch_size, num_hidden)`\n\n        Returns\n        --------\n        out: NDArray\n            output tensor with shape `(sequence_length, batch_size, input_size)`\n            when `layout` is \"TNC\".\n        out_states: list\n            output recurrent state tensor with length equals to num_layers.\n            the state with shape `(1, batch_size, num_hidden)`"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef forward(self, inputs, begin_state): # pylint: disable=arguments-differ\n        encoded = self.embedding(inputs)\n        length = inputs.shape[0]\n        batch_size = inputs.shape[1]\n        encoded, state = self.encoder.unroll(length, encoded, begin_state,\n                                             layout='TNC', merge_outputs=True)\n        encoded = encoded.reshape((-1, self._projection_size))\n        out = self.decoder(encoded)\n        out = out.reshape((length, batch_size, -1))\n        return out, state", "response": "Implement forward computation.\n\n        Parameters\n        -----------\n        inputs : NDArray\n            input tensor with shape `(sequence_length, batch_size)`\n            when `layout` is \"TNC\".\n        begin_state : list\n            initial recurrent state tensor with length equals to num_layers*2.\n            For each layer the two initial states have shape `(batch_size, num_hidden)`\n            and `(batch_size, num_projection)`\n\n        Returns\n        --------\n        out : NDArray\n            output tensor with shape `(sequence_length, batch_size, vocab_size)`\n              when `layout` is \"TNC\".\n        out_states : list\n            output recurrent state tensor with length equals to num_layers*2.\n            For each layer the two initial states have shape `(batch_size, num_hidden)`\n            and `(batch_size, num_projection)`"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_cell_type(cell_type):\n    if isinstance(cell_type, str):\n        if cell_type == 'lstm':\n            return rnn.LSTMCell\n        elif cell_type == 'gru':\n            return rnn.GRUCell\n        elif cell_type == 'relu_rnn':\n            return partial(rnn.RNNCell, activation='relu')\n        elif cell_type == 'tanh_rnn':\n            return partial(rnn.RNNCell, activation='tanh')\n        else:\n            raise NotImplementedError\n    else:\n        return cell_type", "response": "Get the object type of the input cell_type by parsing the input\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_context(center_idx, sentence_boundaries, window_size,\n                 random_window_size, seed):\n    \"\"\"Compute the context with respect to a center word in a sentence.\n\n    Takes an numpy array of sentences boundaries.\n\n    \"\"\"\n    random.seed(seed + center_idx)\n\n    sentence_index = np.searchsorted(sentence_boundaries, center_idx)\n    sentence_start, sentence_end = _get_sentence_start_end(\n        sentence_boundaries, sentence_index)\n\n    if random_window_size:\n        window_size = random.randint(1, window_size)\n    start_idx = max(sentence_start, center_idx - window_size)\n    end_idx = min(sentence_end, center_idx + window_size + 1)\n\n    if start_idx != center_idx and center_idx + 1 != end_idx:\n        context = np.concatenate((np.arange(start_idx, center_idx),\n                                  np.arange(center_idx + 1, end_idx)))\n    elif start_idx != center_idx:\n        context = np.arange(start_idx, center_idx)\n    elif center_idx + 1 != end_idx:\n        context = np.arange(center_idx + 1, end_idx)\n    else:\n        context = None\n\n    return context", "response": "Compute the context with respect to a center word in a sentence."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef preprocess_dataset(dataset, transform, num_workers=8):\n    worker_fn = partial(_worker_fn, transform=transform)\n    start = time.time()\n\n    pool = mp.Pool(num_workers)\n    dataset_transform = []\n    dataset_len = []\n    for data in pool.map(worker_fn, dataset):\n        if data:\n            for _data in data:\n                dataset_transform.append(_data[:-1])\n                dataset_len.append(_data[-1])\n\n    dataset = SimpleDataset(dataset_transform).transform(\n        lambda x: (x[0], x[1], x[2], x[3], x[4], x[5]))\n    end = time.time()\n    pool.close()\n    print('Done! Transform dataset costs %.2f seconds.' % (end-start))\n    return dataset, dataset_len", "response": "Use multiprocessing to perform transform for dataset."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_model(name, dataset_name='wikitext-2', **kwargs):\n    models = {'standard_lstm_lm_200' : standard_lstm_lm_200,\n              'standard_lstm_lm_650' : standard_lstm_lm_650,\n              'standard_lstm_lm_1500': standard_lstm_lm_1500,\n              'awd_lstm_lm_1150': awd_lstm_lm_1150,\n              'awd_lstm_lm_600': awd_lstm_lm_600,\n              'big_rnn_lm_2048_512': big_rnn_lm_2048_512,\n              'elmo_2x1024_128_2048cnn_1xhighway': elmo_2x1024_128_2048cnn_1xhighway,\n              'elmo_2x2048_256_2048cnn_1xhighway': elmo_2x2048_256_2048cnn_1xhighway,\n              'elmo_2x4096_512_2048cnn_2xhighway': elmo_2x4096_512_2048cnn_2xhighway,\n              'transformer_en_de_512': transformer_en_de_512,\n              'bert_12_768_12'       : bert_12_768_12,\n              'bert_24_1024_16'      : bert_24_1024_16}\n    name = name.lower()\n    if name not in models:\n        raise ValueError(\n            'Model %s is not supported. Available options are\\n\\t%s'%(\n                name, '\\n\\t'.join(sorted(models.keys()))))\n    kwargs['dataset_name'] = dataset_name\n    return models[name](**kwargs)", "response": "Returns a pre - defined model by name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _masked_softmax(F, att_score, mask, dtype):\n    if mask is not None:\n        # Fill in the masked scores with a very small value\n        neg = -1e4 if np.dtype(dtype) == np.float16 else -1e18\n        att_score = F.where(mask, att_score, neg * F.ones_like(att_score))\n        att_weights = F.softmax(att_score, axis=-1) * mask\n    else:\n        att_weights = F.softmax(att_score, axis=-1)\n    return att_weights", "response": "Compute the softmax of the node - level elements in a cluster."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _read_by_weight(self, F, att_weights, value):\n        output = F.batch_dot(att_weights, value)\n        return output", "response": "Read from the value matrix given the attention weights."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntranslate the input sentence into a set of samples and scores.", "response": "def translate(self, src_seq, src_valid_length):\n        \"\"\"Get the translation result given the input sentence.\n\n        Parameters\n        ----------\n        src_seq : mx.nd.NDArray\n            Shape (batch_size, length)\n        src_valid_length : mx.nd.NDArray\n            Shape (batch_size,)\n\n        Returns\n        -------\n        samples : NDArray\n            Samples draw by beam search. Shape (batch_size, beam_size, length). dtype is int32.\n        scores : NDArray\n            Scores of the samples. Shape (batch_size, beam_size). We make sure that scores[i, :] are\n            in descending order.\n        valid_length : NDArray\n            The valid length of the samples. Shape (batch_size, beam_size). dtype will be int32.\n        \"\"\"\n        batch_size = src_seq.shape[0]\n        encoder_outputs, _ = self._model.encode(src_seq, valid_length=src_valid_length)\n        decoder_states = self._model.decoder.init_state_from_encoder(encoder_outputs,\n                                                                     src_valid_length)\n        inputs = mx.nd.full(shape=(batch_size,), ctx=src_seq.context, dtype=np.float32,\n                            val=self._model.tgt_vocab.token_to_idx[self._model.tgt_vocab.bos_token])\n        samples, scores, sample_valid_length = self._sampler(inputs, decoder_states)\n        return samples, scores, sample_valid_length"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef evaluate_official_script(parser, vocab, num_buckets_test, test_batch_size, test_file, output_file,\n                             debug=False):\n    \"\"\"Evaluate parser on a data set\n\n    Parameters\n    ----------\n    parser : BiaffineParser\n        biaffine parser\n    vocab : ParserVocabulary\n        vocabulary built from data set\n    num_buckets_test : int\n        size of buckets (cluster sentences into this number of clusters)\n    test_batch_size : int\n        batch size\n    test_file : str\n        gold test file\n    output_file : str\n        output result to this file\n    debug : bool\n        only evaluate first 1000 sentences for debugging\n\n    Returns\n    -------\n    tuple\n        UAS, LAS, speed\n    \"\"\"\n    if output_file is None:\n        output_file = tempfile.NamedTemporaryFile().name\n    data_loader = DataLoader(test_file, num_buckets_test, vocab)\n    record = data_loader.idx_sequence\n    results = [None] * len(record)\n    idx = 0\n    seconds = time.time()\n    for words, tags, arcs, rels in data_loader.get_batches(batch_size=test_batch_size,\n                                                           shuffle=False):\n        outputs = parser.forward(words, tags)\n        for output in outputs:\n            sent_idx = record[idx]\n            results[sent_idx] = output\n            idx += 1\n    assert idx == len(results), 'parser swallowed some sentences'\n    seconds = time.time() - seconds\n    speed = len(record) / seconds\n\n    arcs = reduce(lambda x, y: x + y, [list(result[0]) for result in results])\n    rels = reduce(lambda x, y: x + y, [list(result[1]) for result in results])\n    idx = 0\n    with open(test_file) as f:\n        if debug:\n            f = f.readlines()[:1000]\n        with open(output_file, 'w') as fo:\n            for line in f:\n                info = line.strip().split()\n                if info:\n                    arc_offset = 5\n                    rel_offset = 6\n                    if len(info) == 10:  # conll or conllx\n                        arc_offset = 6\n                        rel_offset = 7\n                    # assert len(info) == 10, 'Illegal line: %s' % line\n                    info[arc_offset] = str(arcs[idx])\n                    info[rel_offset] = vocab.id2rel(rels[idx])\n                    fo.write('\\t'.join(info) + '\\n')\n                    idx += 1\n                else:\n                    fo.write('\\n')\n\n    os.system('perl %s -q -b -g %s -s %s -o tmp' % (\n        os.path.join(os.path.dirname(os.path.realpath(__file__)), 'eval.pl'), test_file, output_file))\n    os.system('tail -n 3 tmp > score_tmp')\n    LAS, UAS = [float(line.strip().split()[-2]) for line in open('score_tmp').readlines()[:2]]\n    # print('UAS %.2f, LAS %.2f' % (UAS, LAS))\n    os.system('rm tmp score_tmp')\n    os.remove(output_file)\n    return UAS, LAS, speed", "response": "Evaluate parser on a data set and save the result to output_file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parameter_from_numpy(self, name, array):\n        p = self.params.get(name, shape=array.shape, init=mx.init.Constant(array))\n        return p", "response": "Create a parameter with its value initialized according to a numpy tensor\n           ."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a parameter given name shape and initiator.", "response": "def parameter_init(self, name, shape, init):\n        \"\"\"Create parameter given name, shape and initiator\n\n        Parameters\n        ----------\n        name : str\n            parameter name\n        shape : tuple\n            parameter shape\n        init : mxnet.initializer\n            an initializer\n\n        Returns\n        -------\n        mxnet.gluon.parameter\n            a parameter object\n        \"\"\"\n        p = self.params.get(name, shape=shape, init=init)\n        return p"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrun decoding and training on a set of words.", "response": "def forward(self, word_inputs, tag_inputs, arc_targets=None, rel_targets=None):\n        \"\"\"Run decoding\n\n        Parameters\n        ----------\n        word_inputs : mxnet.ndarray.NDArray\n            word indices of seq_len x batch_size\n        tag_inputs : mxnet.ndarray.NDArray\n            tag indices of seq_len x batch_size\n        arc_targets : mxnet.ndarray.NDArray\n            gold arc indices of seq_len x batch_size\n        rel_targets : mxnet.ndarray.NDArray\n            gold rel indices of seq_len x batch_size\n        Returns\n        -------\n        tuple\n            (arc_accuracy, rel_accuracy, overall_accuracy, loss) when training, else if given gold target\n        then return arc_accuracy, rel_accuracy, overall_accuracy, outputs, otherwise return outputs, where outputs is a\n        list of (arcs, rels).\n        \"\"\"\n        is_train = autograd.is_training()\n\n        def flatten_numpy(ndarray):\n            \"\"\"Flatten nd-array to 1-d column vector\n\n            Parameters\n            ----------\n            ndarray : numpy.ndarray\n                input tensor\n\n            Returns\n            -------\n            numpy.ndarray\n                A column vector\n\n            \"\"\"\n            return np.reshape(ndarray, (-1,), 'F')\n\n        batch_size = word_inputs.shape[1]\n        seq_len = word_inputs.shape[0]\n        mask = np.greater(word_inputs, self._vocab.ROOT).astype(np.float32)\n        num_tokens = int(np.sum(mask))  # non padding, non root token number\n\n        if is_train or arc_targets is not None:\n            mask_1D = flatten_numpy(mask)\n            mask_1D_tensor = nd.array(mask_1D)\n\n        unked_words = np.where(word_inputs < self._vocab.words_in_train, word_inputs, self._vocab.UNK)\n        word_embs = self.word_embs(nd.array(unked_words, dtype='int'))\n        if self.pret_word_embs:\n            word_embs = word_embs + self.pret_word_embs(nd.array(word_inputs))\n        tag_embs = self.tag_embs(nd.array(tag_inputs))\n\n        # Dropout\n        emb_inputs = nd.concat(word_embs, tag_embs, dim=2)  # seq_len x batch_size\n\n        top_recur = biLSTM(self.f_lstm, self.b_lstm, emb_inputs, batch_size,\n                           dropout_x=self.dropout_lstm_input if is_train else 0)\n        top_recur = nd.Dropout(data=top_recur, axes=[0], p=self.dropout_mlp)\n\n        W_dep, b_dep = self.mlp_dep_W.data(), self.mlp_dep_b.data()\n        W_head, b_head = self.mlp_head_W.data(), self.mlp_head_b.data()\n        dep, head = leaky_relu(nd.dot(top_recur, W_dep.T) + b_dep), leaky_relu(nd.dot(top_recur, W_head.T) + b_head)\n        dep, head = nd.Dropout(data=dep, axes=[0], p=self.dropout_mlp), nd.Dropout(data=head, axes=[0],\n                                                                                       p=self.dropout_mlp)\n        dep, head = nd.transpose(dep, axes=[2, 0, 1]), nd.transpose(head, axes=[2, 0, 1])\n        dep_arc, dep_rel = dep[:self.mlp_arc_size], dep[self.mlp_arc_size:]\n        head_arc, head_rel = head[:self.mlp_arc_size], head[self.mlp_arc_size:]\n\n        W_arc = self.arc_W.data()\n        arc_logits = bilinear(dep_arc, W_arc, head_arc, self.mlp_arc_size, seq_len, batch_size, num_outputs=1,\n                              bias_x=True, bias_y=False)\n        # (#head x #dep) x batch_size\n\n        flat_arc_logits = reshape_fortran(arc_logits, (seq_len, seq_len * batch_size))\n        # (#head ) x (#dep x batch_size)\n\n        arc_preds = arc_logits.argmax(0)\n        # seq_len x batch_size\n\n        if is_train or arc_targets is not None:\n            correct = np.equal(arc_preds.asnumpy(), arc_targets)\n            arc_correct = correct.astype(np.float32) * mask\n            arc_accuracy = np.sum(arc_correct) / num_tokens\n            targets_1D = flatten_numpy(arc_targets)\n            losses = self.softmax_loss(flat_arc_logits, nd.array(targets_1D))\n            arc_loss = nd.sum(losses * mask_1D_tensor) / num_tokens\n\n        if not is_train:\n            arc_probs = np.transpose(\n                np.reshape(nd.softmax(flat_arc_logits, axis=0).asnumpy(), (seq_len, seq_len, batch_size), 'F'))\n        # #batch_size x #dep x #head\n\n        W_rel = self.rel_W.data()\n        rel_logits = bilinear(dep_rel, W_rel, head_rel, self.mlp_rel_size, seq_len, batch_size,\n                              num_outputs=self._vocab.rel_size, bias_x=True, bias_y=True)\n        # (#head x rel_size x #dep) x batch_size\n\n        flat_rel_logits = reshape_fortran(rel_logits, (seq_len, self._vocab.rel_size, seq_len * batch_size))\n        # (#head x rel_size) x (#dep x batch_size)\n\n        _target_vec = nd.array(targets_1D if is_train else flatten_numpy(arc_preds.asnumpy())).reshape(\n            seq_len * batch_size, 1)\n        _target_mat = _target_vec * nd.ones((1, self._vocab.rel_size))\n\n        partial_rel_logits = nd.pick(flat_rel_logits, _target_mat.T, axis=0)\n        # (rel_size) x (#dep x batch_size)\n\n        if is_train or arc_targets is not None:\n            rel_preds = partial_rel_logits.argmax(0)\n            targets_1D = flatten_numpy(rel_targets)\n            rel_correct = np.equal(rel_preds.asnumpy(), targets_1D).astype(np.float32) * mask_1D\n            rel_accuracy = np.sum(rel_correct) / num_tokens\n            losses = self.softmax_loss(partial_rel_logits, nd.array(targets_1D))\n            rel_loss = nd.sum(losses * mask_1D_tensor) / num_tokens\n\n        if not is_train:\n            rel_probs = np.transpose(np.reshape(nd.softmax(flat_rel_logits.transpose([1, 0, 2]), axis=0).asnumpy(),\n                                                (self._vocab.rel_size, seq_len, seq_len, batch_size), 'F'))\n        # batch_size x #dep x #head x #nclasses\n\n        if is_train or arc_targets is not None:\n            loss = arc_loss + rel_loss\n            correct = rel_correct * flatten_numpy(arc_correct)\n            overall_accuracy = np.sum(correct) / num_tokens\n\n        if is_train:\n            return arc_accuracy, rel_accuracy, overall_accuracy, loss\n\n        outputs = []\n\n        for msk, arc_prob, rel_prob in zip(np.transpose(mask), arc_probs, rel_probs):\n            # parse sentences one by one\n            msk[0] = 1.\n            sent_len = int(np.sum(msk))\n            arc_pred = arc_argmax(arc_prob, sent_len, msk)\n            rel_prob = rel_prob[np.arange(len(arc_pred)), arc_pred]\n            rel_pred = rel_argmax(rel_prob, sent_len)\n            outputs.append((arc_pred[1:sent_len], rel_pred[1:sent_len]))\n\n        if arc_targets is not None:\n            return arc_accuracy, rel_accuracy, overall_accuracy, outputs\n        return outputs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsaving model parameters to file.", "response": "def save_parameters(self, filename):\n        \"\"\"Save model\n\n        Parameters\n        ----------\n        filename : str\n            path to model file\n        \"\"\"\n        params = self._collect_params_with_prefix()\n        if self.pret_word_embs:  # don't save word embeddings inside model\n            params.pop('pret_word_embs.weight', None)\n        arg_dict = {key: val._reduce() for key, val in params.items()}\n        ndarray.save(filename, arg_dict)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _worker_fn(samples, batchify_fn, dataset=None):\n    # pylint: disable=unused-argument\n    # it is required that each worker process has to fork a new MXIndexedRecordIO handle\n    # preserving dataset as global variable can save tons of overhead and is safe in new process\n    global _worker_dataset\n    if isinstance(samples[0], (list, tuple)):\n        batch = [batchify_fn([_worker_dataset[i] for i in shard]) for shard in samples]\n    else:\n        batch = batchify_fn([_worker_dataset[i] for i in samples])\n    buf = io.BytesIO()\n    ForkingPickler(buf, pickle.HIGHEST_PROTOCOL).dump(batch)\n    return buf.getvalue()", "response": "Function for processing data in worker process."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _thread_worker_fn(samples, batchify_fn, dataset):\n    if isinstance(samples[0], (list, tuple)):\n        batch = [batchify_fn([dataset[i] for i in shard]) for shard in samples]\n    else:\n        batch = batchify_fn([dataset[i] for i in samples])\n    return batch", "response": "Threadpool worker function for processing data."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates an instance of token embedding.", "response": "def create(embedding_name, **kwargs):\n    \"\"\"Creates an instance of token embedding.\n\n\n    Creates a token embedding instance by loading embedding vectors from an externally hosted\n    pre-trained token embedding file, such as those of GloVe and FastText. To get all the valid\n    `embedding_name` and `source`, use :func:`gluonnlp.embedding.list_sources`.\n\n\n    Parameters\n    ----------\n    embedding_name : str\n        The token embedding name (case-insensitive).\n    kwargs : dict\n        All other keyword arguments are passed to the initializer of token\n        embedding class. For example `create(embedding_name='fasttext',\n        source='wiki.simple', load_ngrams=True)` will return\n        `FastText(source='wiki.simple', load_ngrams=True)`.\n\n\n    Returns\n    -------\n    An instance of :class:`gluonnlp.embedding.TokenEmbedding`:\n        A token embedding instance that loads embedding vectors from an externally hosted\n        pre-trained token embedding file.\n    \"\"\"\n\n    create_text_embedding = registry.get_create_func(TokenEmbedding, 'token embedding')\n    return create_text_embedding(embedding_name, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget all the valid token embedding names and their pre - trained file names.", "response": "def list_sources(embedding_name=None):\n    \"\"\"Get valid token embedding names and their pre-trained file names.\n\n\n    To load token embedding vectors from an externally hosted pre-trained token embedding file,\n    such as those of GloVe and FastText, one should use\n    `gluonnlp.embedding.create(embedding_name, source)`. This method returns all the\n    valid names of `source` for the specified `embedding_name`. If `embedding_name` is set to\n    None, this method returns all the valid names of `embedding_name` with their associated\n    `source`.\n\n\n    Parameters\n    ----------\n    embedding_name : str or None, default None\n        The pre-trained token embedding name.\n\n\n    Returns\n    -------\n    dict or list:\n        A list of all the valid pre-trained token embedding file names (`source`) for the\n        specified token embedding name (`embedding_name`). If the text embedding name is set to\n        None, returns a dict mapping each valid token embedding name to a list of valid pre-trained\n        files (`source`). They can be plugged into\n        `gluonnlp.embedding.create(embedding_name, source)`.\n    \"\"\"\n\n    text_embedding_reg = registry.get_registry(TokenEmbedding)\n\n    if embedding_name is not None:\n        embedding_name = embedding_name.lower()\n        if embedding_name not in text_embedding_reg:\n            raise KeyError('Cannot find `embedding_name` {}. Use '\n                           '`list_sources(embedding_name=None).keys()` to get all the valid'\n                           'embedding names.'.format(embedding_name))\n        return list(text_embedding_reg[embedding_name].source_file_hash.keys())\n    else:\n        return {embedding_name: list(embedding_cls.source_file_hash.keys())\n                for embedding_name, embedding_cls in registry.get_registry(TokenEmbedding).items()}"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _load_embedding(self, pretrained_file_path, elem_delim,\n                        encoding='utf8'):\n        \"\"\"Load embedding vectors from a pre-trained token embedding file.\n\n        Both text files and TokenEmbedding serialization files are supported.\n        elem_delim and encoding are ignored for non-text files.\n\n        For every unknown token, if its representation `self.unknown_token` is encountered in the\n        pre-trained token embedding file, index 0 of `self.idx_to_vec` maps to the pre-trained token\n        embedding vector loaded from the file; otherwise, index 0 of `self.idx_to_vec` maps to the\n        text embedding vector initialized by `self._init_unknown_vec`.\n\n        If a token is encountered multiple times in the pre-trained text embedding file, only the\n        first-encountered token embedding vector will be loaded and the rest will be skipped.\n\n        \"\"\"\n\n        pretrained_file_path = os.path.expanduser(pretrained_file_path)\n\n        if not os.path.isfile(pretrained_file_path):\n            raise ValueError('`pretrained_file_path` must be a valid path '\n                             'to the pre-trained token embedding file.')\n\n        logging.info('Loading pre-trained token embedding vectors from %s',\n                     pretrained_file_path)\n\n        if pretrained_file_path.endswith('.npz'):\n            self._load_embedding_serialized(\n                pretrained_file_path=pretrained_file_path)\n        else:\n            self._load_embedding_txt(\n                pretrained_file_path=pretrained_file_path,\n                elem_delim=elem_delim, encoding=encoding)", "response": "Load embedding vectors from a pre - trained token embedding file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _load_embedding_txt(self, pretrained_file_path, elem_delim, encoding='utf8'):\n\n        vec_len = None\n        all_elems = []\n        tokens = set()\n        loaded_unknown_vec = None\n        with io.open(pretrained_file_path, 'rb') as f:\n            for line_num, line in enumerate(f):\n                try:\n                    line = line.decode(encoding)\n                except ValueError:\n                    warnings.warn('line {} in {}: failed to decode. Skipping.'\n                                  .format(line_num, pretrained_file_path))\n                    continue\n\n                elems = line.rstrip().split(elem_delim)\n\n                assert len(elems) > 1, 'line {} in {}: unexpected data format.'.format(\n                    line_num, pretrained_file_path)\n\n                token, elems = elems[0], [float(i) for i in elems[1:]]\n\n                if token == self.unknown_token and loaded_unknown_vec is None:\n                    loaded_unknown_vec = elems\n                    tokens.add(self.unknown_token)\n                elif token in tokens:\n                    warnings.warn('line {} in {}: duplicate embedding found for '\n                                  'token \"{}\". Skipped.'.format(line_num, pretrained_file_path,\n                                                                token))\n                elif len(elems) == 1 and line_num == 0:\n                    warnings.warn('line {} in {}: skipped likely header line.'\n                                  .format(line_num, pretrained_file_path))\n                else:\n                    if not vec_len:\n                        vec_len = len(elems)\n                        if self.unknown_token:\n                            # Reserve a vector slot for the unknown token at the very beggining\n                            # because the unknown token index is 0.\n                            all_elems.extend([0] * vec_len)\n                    else:\n                        assert len(elems) == vec_len, \\\n                            'line {} in {}: found vector of inconsistent dimension for token ' \\\n                            '\"{}\". expected dim: {}, found: {}'.format(line_num,\n                                                                       pretrained_file_path,\n                                                                       token, vec_len, len(elems))\n                    all_elems.extend(elems)\n                    self._idx_to_token.append(token)\n                    self._token_to_idx[token] = len(self._idx_to_token) - 1\n                    tokens.add(token)\n\n        self._idx_to_vec = nd.array(all_elems).reshape((-1, vec_len))\n\n        if self.unknown_token:\n            if loaded_unknown_vec is None:\n                self._idx_to_vec[C.UNK_IDX] = self._init_unknown_vec(shape=vec_len)\n            else:\n                self._idx_to_vec[C.UNK_IDX] = nd.array(loaded_unknown_vec)", "response": "Load embedding vectors from a pre - trained text embedding file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _load_embedding_serialized(self, pretrained_file_path):\n\n        deserialized_embedding = TokenEmbedding.deserialize(pretrained_file_path)\n        if deserialized_embedding.unknown_token:\n            # Some .npz files on S3 may contain an unknown token and its\n            # respective embedding. As a workaround, we assume that C.UNK_IDX\n            # is the same now as it was when the .npz was generated. Under this\n            # assumption we can safely overwrite the respective token and\n            # vector from the npz.\n            if deserialized_embedding.unknown_token:\n                idx_to_token = deserialized_embedding.idx_to_token\n                idx_to_vec = deserialized_embedding.idx_to_vec\n                idx_to_token[C.UNK_IDX] = self.unknown_token\n                if self._init_unknown_vec:\n                    vec_len = idx_to_vec.shape[1]\n                    idx_to_vec[C.UNK_IDX] = self._init_unknown_vec(shape=vec_len)\n            else:\n                # If the TokenEmbedding shall not have an unknown token, we\n                # just delete the one in the npz.\n                assert C.UNK_IDX == 0\n                idx_to_token = deserialized_embedding.idx_to_token[C.UNK_IDX + 1:]\n                idx_to_vec = deserialized_embedding.idx_to_vec[C.UNK_IDX + 1:]\n        else:\n            idx_to_token = deserialized_embedding.idx_to_token\n            idx_to_vec = deserialized_embedding.idx_to_vec\n\n        if not len(set(idx_to_token)) == len(idx_to_token):\n            raise ValueError('Serialized embedding invalid. '\n                             'It contains duplicate tokens.')\n\n        if self.unknown_token:\n            try:\n                unknown_token_idx = deserialized_embedding.idx_to_token.index(\n                    self.unknown_token)\n                idx_to_token[C.UNK_IDX], idx_to_token[\n                    unknown_token_idx] = idx_to_token[\n                        unknown_token_idx], idx_to_token[C.UNK_IDX]\n                idxs = [C.UNK_IDX, unknown_token_idx]\n                idx_to_vec[idxs] = idx_to_vec[idxs[::-1]]\n            except ValueError:\n                vec_len = idx_to_vec.shape[1]\n                idx_to_token.insert(0, self.unknown_token)\n                idx_to_vec = nd.concat(\n                    self._init_unknown_vec(shape=vec_len).reshape((1, -1)),\n                    idx_to_vec, dim=0)\n\n        self._idx_to_token = idx_to_token\n        self._idx_to_vec = idx_to_vec\n        self._token_to_idx.update((token, idx) for idx, token in enumerate(self._idx_to_token))", "response": "Load embedding vectors from a pre - trained token embedding file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _check_vector_update(self, tokens, new_embedding):\n        assert self._idx_to_vec is not None, '`idx_to_vec` has not been initialized.'\n\n        if not isinstance(tokens, (list, tuple)) or len(tokens) == 1:\n            assert isinstance(new_embedding, nd.NDArray) and len(new_embedding.shape) in [1, 2], \\\n                '`new_embedding` must be a 1-D or 2-D NDArray if `tokens` is a single token.'\n            if not isinstance(tokens, (list, tuple)):\n                tokens = [tokens]\n            if len(new_embedding.shape) == 1:\n                new_embedding = new_embedding.expand_dims(0)\n\n        else:\n            assert isinstance(new_embedding, nd.NDArray) and len(new_embedding.shape) == 2, \\\n                '`new_embedding` must be a 2-D NDArray if `tokens` is a list of multiple strings.'\n        if self._idx_to_vec is not None:\n            assert new_embedding.shape == (len(tokens), self._idx_to_vec.shape[1]), \\\n                'The length of `new_embedding` must be equal to the number ' \\\n                'of tokens and the width of new_embedding must be equal ' \\\n                'to the dimension of embedding of the glossary.'\n        else:\n            assert new_embedding.shape[0] == len(tokens), \\\n                'The length of `new_embedding` must be equal to the number of tokens'\n        return tokens", "response": "Check that tokens and embedding are in the format for __setitem__."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _check_source(cls, source_file_hash, source):\n        embedding_name = cls.__name__.lower()\n        if source not in source_file_hash:\n            raise KeyError('Cannot find pre-trained source {} for token embedding {}. '\n                           'Valid pre-trained file names for embedding {}: {}'.format(\n                               source, embedding_name, embedding_name,\n                               ', '.join(source_file_hash.keys())))", "response": "Checks if a pre - trained token embedding source name is valid."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef from_file(file_path, elem_delim=' ', encoding='utf8', **kwargs):\n        embedding = TokenEmbedding(**kwargs)\n        embedding._load_embedding(file_path, elem_delim=elem_delim, encoding=encoding)\n        return embedding", "response": "Creates a user - defined token embedding from a pre - trained token embedding file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef serialize(self, file_path, compress=True):\n        if self.unknown_lookup is not None:\n            warnings.warn(\n                'Serialization of `unknown_lookup` is not supported. '\n                'Save it manually and pass the loaded lookup object '\n                'during deserialization.')\n\n        unknown_token = np.array(self.unknown_token)\n        idx_to_token = np.array(self.idx_to_token, dtype='O')\n        idx_to_vec = self.idx_to_vec.asnumpy()\n\n        if not unknown_token:  # Store empty string instead of None\n            unknown_token = ''\n        else:\n            assert unknown_token == idx_to_token[C.UNK_IDX]\n\n        if not compress:\n            np.savez(file=file_path, unknown_token=unknown_token,\n                     idx_to_token=idx_to_token, idx_to_vec=idx_to_vec)\n        else:\n            np.savez_compressed(file=file_path, unknown_token=unknown_token,\n                                idx_to_token=idx_to_token,\n                                idx_to_vec=idx_to_vec)", "response": "Serializes the TokenEmbedding object to a file specified by file_path."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef deserialize(cls, file_path, **kwargs):\n        # idx_to_token is of dtype 'O' so we need to allow pickle\n        npz_dict = np.load(file_path, allow_pickle=True)\n\n        unknown_token = npz_dict['unknown_token']\n        if not unknown_token:\n            unknown_token = None\n        else:\n            if isinstance(unknown_token, np.ndarray):\n                if unknown_token.dtype.kind == 'S':\n                    unknown_token = unknown_token.tobytes().decode()\n                else:\n                    unknown_token = str(unknown_token)\n        idx_to_token = npz_dict['idx_to_token'].tolist()\n        idx_to_vec = nd.array(npz_dict['idx_to_vec'])\n\n        embedding = cls(unknown_token=unknown_token, **kwargs)\n        if unknown_token:\n            assert unknown_token == idx_to_token[C.UNK_IDX]\n            embedding._token_to_idx = DefaultLookupDict(C.UNK_IDX)\n        else:\n            embedding._token_to_idx = {}\n\n        embedding._idx_to_token = idx_to_token\n        embedding._idx_to_vec = idx_to_vec\n        embedding._token_to_idx.update((token, idx) for idx, token in enumerate(idx_to_token))\n\n        return embedding", "response": "Create a new TokenEmbedding from a serialized one."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef evaluate(data_source):\n    log.info('Start predict')\n    tic = time.time()\n    for batch in data_source:\n        inputs, token_types, valid_length = batch\n        out = net(inputs.astype('float32').as_in_context(ctx),\n                  token_types.astype('float32').as_in_context(ctx),\n                  valid_length.astype('float32').as_in_context(ctx))\n    toc = time.time()\n    log.info('Inference time cost={:.2f} s, Thoughput={:.2f} samples/s'\n             .format(toc - tic,\n                     len(data_source) / (toc - tic)))", "response": "Evaluate the model on a mini - batch."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef register(class_=None, **kwargs):\n\n    def _real_register(class_):\n        # Assert that the passed kwargs are meaningful\n        for kwarg_name, values in kwargs.items():\n            try:\n                real_args = inspect.getfullargspec(class_).args\n            except AttributeError:\n                # pylint: disable=deprecated-method\n                real_args = inspect.getargspec(class_.__init__).args\n\n            if not kwarg_name in real_args:\n                raise RuntimeError(\n                    ('{} is not a valid argument for {}. '\n                     'Only valid arguments can be registered.').format(\n                         kwarg_name, class_.__name__))\n\n            if not isinstance(values, (list, tuple)):\n                raise RuntimeError(('{} should be a list of '\n                                    'valid arguments for {}. ').format(\n                                        values, kwarg_name))\n\n        # Save the kwargs associated with this class_\n        _REGSITRY_NAME_KWARGS[class_] = kwargs\n\n        register_ = registry.get_register_func(Dataset, 'dataset')\n        return register_(class_)\n\n    if class_ is not None:\n        # Decorator was called without arguments\n        return _real_register(class_)\n\n    return _real_register", "response": "Registers a new dataset with the specified hyperparameters."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate an instance of a registered dataset.", "response": "def create(name, **kwargs):\n    \"\"\"Creates an instance of a registered dataset.\n\n    Parameters\n    ----------\n    name : str\n        The dataset name (case-insensitive).\n\n    Returns\n    -------\n    An instance of :class:`mxnet.gluon.data.Dataset` constructed with the\n    keyword arguments passed to the create function.\n\n    \"\"\"\n    create_ = registry.get_create_func(Dataset, 'dataset')\n    return create_(name, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets valid datasets and registered parameters.", "response": "def list_datasets(name=None):\n    \"\"\"Get valid datasets and registered parameters.\n\n    Parameters\n    ----------\n    name : str or None, default None\n        Return names and registered parameters of registered datasets. If name\n        is specified, only registered parameters of the respective dataset are\n        returned.\n\n    Returns\n    -------\n    dict:\n        A dict of all the valid keyword parameters names for the specified\n        dataset. If name is set to None, returns a dict mapping each valid name\n        to its respective keyword parameter dict. The valid names can be\n        plugged in `gluonnlp.model.word_evaluation_model.create(name)`.\n\n    \"\"\"\n    reg = registry.get_registry(Dataset)\n\n    if name is not None:\n        class_ = reg[name.lower()]\n        return _REGSITRY_NAME_KWARGS[class_]\n    else:\n        return {\n            dataset_name: _REGSITRY_NAME_KWARGS[class_]\n            for dataset_name, class_ in registry.get_registry(Dataset).items()\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description='Vocabulary extractor.',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument('--max-size', type=int, default=None)\n    parser.add_argument('--min-freq', type=int, default=5)\n    parser.add_argument('--max-word-length', type=int, default=50)\n    parser.add_argument('files', type=str, nargs='+')\n    parser.add_argument('--vocab-output', type=str, default='vocab.json')\n    parser.add_argument('--counts-output', type=str, default='counts.json')\n    args = parser.parse_args()\n    return args", "response": "Parse command line arguments."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef forward(self, inputs, token_types, valid_length=None):  # pylint: disable=arguments-differ\n        _, pooler_out = self.bert(inputs, token_types, valid_length)\n        return self.classifier(pooler_out)", "response": "Generate the unnormalized score for the given input sequences."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd evaluation specific parameters to parser.", "response": "def add_parameters(parser):\n    \"\"\"Add evaluation specific parameters to parser.\"\"\"\n    group = parser.add_argument_group('Evaluation arguments')\n\n    group.add_argument('--eval-batch-size', type=int, default=1024)\n\n    # Datasets\n    group.add_argument(\n        '--similarity-datasets', type=str,\n        default=nlp.data.word_embedding_evaluation.word_similarity_datasets,\n        nargs='*',\n        help='Word similarity datasets to use for intrinsic evaluation.')\n    group.add_argument(\n        '--similarity-functions', type=str,\n        default=nlp.embedding.evaluation.list_evaluation_functions(\n            'similarity'), nargs='+',\n        help='Word similarity functions to use for intrinsic evaluation.')\n    group.add_argument(\n        '--analogy-datasets', type=str, default=['GoogleAnalogyTestSet'],\n        nargs='*',\n        help='Word similarity datasets to use for intrinsic evaluation.')\n    group.add_argument(\n        '--analogy-functions', type=str,\n        default=nlp.embedding.evaluation.list_evaluation_functions('analogy'),\n        nargs='+',\n        help='Word analogy functions to use for intrinsic evaluation. ')\n\n    ## Analogy evaluation specific arguments\n    group.add_argument(\n        '--analogy-dont-exclude-question-words', action='store_true',\n        help=('Exclude input words from valid output analogies.'\n              'The performance of word embeddings on the analogy task '\n              'is around 0% accuracy if input words are not excluded.'))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nvalidating provided arguments and act on -- help.", "response": "def validate_args(args):\n    \"\"\"Validate provided arguments and act on --help.\"\"\"\n    # Check correctness of similarity dataset names\n    for dataset_name in args.similarity_datasets:\n        if dataset_name.lower() not in map(\n                str.lower,\n                nlp.data.word_embedding_evaluation.word_similarity_datasets):\n            print('{} is not a supported dataset.'.format(dataset_name))\n            sys.exit(1)\n\n    # Check correctness of analogy dataset names\n    for dataset_name in args.analogy_datasets:\n        if dataset_name.lower() not in map(\n                str.lower,\n                nlp.data.word_embedding_evaluation.word_analogy_datasets):\n            print('{} is not a supported dataset.'.format(dataset_name))\n            sys.exit(1)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef iterate_analogy_datasets(args):\n    for dataset_name in args.analogy_datasets:\n        parameters = nlp.data.list_datasets(dataset_name)\n        for key_values in itertools.product(*parameters.values()):\n            kwargs = dict(zip(parameters.keys(), key_values))\n            yield dataset_name, kwargs, nlp.data.create(dataset_name, **kwargs)", "response": "Generator over all analogy evaluation datasets."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_similarity_task_tokens(args):\n    tokens = set()\n    for _, _, dataset in iterate_similarity_datasets(args):\n        tokens.update(\n            itertools.chain.from_iterable((d[0], d[1]) for d in dataset))\n    return tokens", "response": "Returns a set of all tokens occurring the evaluation datasets."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_analogy_task_tokens(args):\n    tokens = set()\n    for _, _, dataset in iterate_analogy_datasets(args):\n        tokens.update(\n            itertools.chain.from_iterable(\n                (d[0], d[1], d[2], d[3]) for d in dataset))\n    return tokens", "response": "Returns a set of all tokens occuring the evaluation datasets."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nevaluate on specified similarity datasets.", "response": "def evaluate_similarity(args, token_embedding, ctx, logfile=None,\n                        global_step=0):\n    \"\"\"Evaluate on specified similarity datasets.\"\"\"\n\n    results = []\n    for similarity_function in args.similarity_functions:\n        evaluator = nlp.embedding.evaluation.WordEmbeddingSimilarity(\n            idx_to_vec=token_embedding.idx_to_vec,\n            similarity_function=similarity_function)\n        evaluator.initialize(ctx=ctx)\n        if not args.no_hybridize:\n            evaluator.hybridize()\n\n        # Evaluate all datasets\n        for (dataset_name, dataset_kwargs,\n             dataset) in iterate_similarity_datasets(args):\n            initial_length = len(dataset)\n            dataset_coded = [[\n                token_embedding.token_to_idx[d[0]],\n                token_embedding.token_to_idx[d[1]], d[2]\n            ] for d in dataset if d[0] in token_embedding.token_to_idx\n                             and d[1] in token_embedding.token_to_idx]\n            num_dropped = initial_length - len(dataset_coded)\n\n            # All words are unknown\n            if not len(dataset_coded):\n                correlation = 0\n            else:\n                words1, words2, scores = zip(*dataset_coded)\n                pred_similarity = evaluator(\n                    mx.nd.array(words1, ctx=ctx), mx.nd.array(words2, ctx=ctx))\n                sr = stats.spearmanr(pred_similarity.asnumpy(),\n                                     np.array(scores))\n                correlation = sr.correlation\n\n            logging.info(\n                'Spearman rank correlation on %s (%s pairs) %s with %s:\\t%s',\n                dataset.__class__.__name__, len(dataset_coded),\n                str(dataset_kwargs), similarity_function, correlation)\n\n            result = dict(\n                task='similarity',\n                dataset_name=dataset_name,\n                dataset_kwargs=dataset_kwargs,\n                similarity_function=similarity_function,\n                spearmanr=correlation,\n                num_dropped=num_dropped,\n                global_step=global_step,\n            )\n            log_similarity_result(logfile, result)\n            results.append(result)\n\n    return results"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef evaluate_analogy(args, token_embedding, ctx, logfile=None, global_step=0):\n    results = []\n    exclude_question_words = not args.analogy_dont_exclude_question_words\n    for analogy_function in args.analogy_functions:\n        evaluator = nlp.embedding.evaluation.WordEmbeddingAnalogy(\n            idx_to_vec=token_embedding.idx_to_vec,\n            exclude_question_words=exclude_question_words,\n            analogy_function=analogy_function)\n        evaluator.initialize(ctx=ctx)\n        if not args.no_hybridize:\n            evaluator.hybridize()\n\n        for (dataset_name, dataset_kwargs,\n             dataset) in iterate_analogy_datasets(args):\n            initial_length = len(dataset)\n            dataset_coded = [[\n                token_embedding.token_to_idx[d[0]],\n                token_embedding.token_to_idx[d[1]],\n                token_embedding.token_to_idx[d[2]],\n                token_embedding.token_to_idx[d[3]]\n            ] for d in dataset if d[0] in token_embedding.token_to_idx\n                             and d[1] in token_embedding.token_to_idx\n                             and d[2] in token_embedding.token_to_idx\n                             and d[3] in token_embedding.token_to_idx]\n            num_dropped = initial_length - len(dataset_coded)\n\n            dataset_coded_batched = mx.gluon.data.DataLoader(\n                dataset_coded, batch_size=args.eval_batch_size)\n\n            acc = mx.metric.Accuracy()\n            for batch in dataset_coded_batched:\n                batch = batch.as_in_context(ctx)\n                words1, words2, words3, words4 = (batch[:, 0], batch[:, 1],\n                                                  batch[:, 2], batch[:, 3])\n                pred_idxs = evaluator(words1, words2, words3)\n                acc.update(pred_idxs[:, 0], words4.astype(np.float32))\n\n            logging.info('Accuracy on %s (%s quadruples) %s with %s:\\t%s',\n                         dataset.__class__.__name__, len(dataset_coded),\n                         str(dataset_kwargs), analogy_function,\n                         acc.get()[1])\n\n            result = dict(\n                task='analogy',\n                dataset_name=dataset_name,\n                dataset_kwargs=dataset_kwargs,\n                analogy_function=analogy_function,\n                accuracy=acc.get()[1],\n                num_dropped=num_dropped,\n                global_step=global_step,\n            )\n            log_analogy_result(logfile, result)\n            results.append(result)\n    return results", "response": "Evaluate on the specified analogy datasets."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlog a similarity evaluation result dictionary as TSV to logfile.", "response": "def log_similarity_result(logfile, result):\n    \"\"\"Log a similarity evaluation result dictionary as TSV to logfile.\"\"\"\n    assert result['task'] == 'similarity'\n\n    if not logfile:\n        return\n\n    with open(logfile, 'a') as f:\n        f.write('\\t'.join([\n            str(result['global_step']),\n            result['task'],\n            result['dataset_name'],\n            json.dumps(result['dataset_kwargs']),\n            result['similarity_function'],\n            str(result['spearmanr']),\n            str(result['num_dropped']),\n        ]))\n\n        f.write('\\n')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_model_loss(ctx, model, pretrained, dataset_name, dtype, ckpt_dir=None, start_step=None):\n    # model\n    model, vocabulary = nlp.model.get_model(model,\n                                            dataset_name=dataset_name,\n                                            pretrained=pretrained, ctx=ctx)\n\n    if not pretrained:\n        model.initialize(init=mx.init.Normal(0.02), ctx=ctx)\n    model.cast(dtype)\n\n    if ckpt_dir and start_step:\n        param_path = os.path.join(ckpt_dir, '%07d.params'%start_step)\n        model.load_parameters(param_path, ctx=ctx)\n        logging.info('Loading step %d checkpoints from %s.', start_step, param_path)\n\n    model.hybridize(static_alloc=True)\n\n    # losses\n    nsp_loss = mx.gluon.loss.SoftmaxCELoss()\n    mlm_loss = mx.gluon.loss.SoftmaxCELoss()\n    nsp_loss.hybridize(static_alloc=True)\n    mlm_loss.hybridize(static_alloc=True)\n\n    return model, nsp_loss, mlm_loss, vocabulary", "response": "Get model for pre - training."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a pretraining dataset based on the data.", "response": "def get_pretrain_dataset(data, batch_size, num_ctxes, shuffle, use_avg_len,\n                         num_buckets, num_parts=1, part_idx=0, prefetch=True):\n    \"\"\"create dataset for pretraining.\"\"\"\n    num_files = len(glob.glob(os.path.expanduser(data)))\n    logging.debug('%d files found.', num_files)\n    assert num_files >= num_parts, \\\n        'Number of training files must be greater than the number of partitions'\n    split_sampler = nlp.data.SplitSampler(num_files, num_parts=num_parts, part_index=part_idx)\n    stream = nlp.data.SimpleDatasetStream(nlp.data.NumpyDataset, data, split_sampler)\n    if prefetch:\n        stream = nlp.data.PrefetchingStream(stream)\n\n    def get_dataloader(dataset):\n        \"\"\"create data loader based on the dataset chunk\"\"\"\n        lengths = dataset.get_field('valid_lengths')\n        # A batch includes: input_id, masked_id, masked_position, masked_weight,\n        #                   next_sentence_label, segment_id, valid_length\n        batchify_fn = Tuple(Pad(), Pad(), Pad(), Pad(), Stack(), Pad(), Stack())\n        if use_avg_len:\n            # sharded data loader\n            sampler = nlp.data.FixedBucketSampler(lengths=lengths,\n                                                  # batch_size per shard\n                                                  batch_size=batch_size,\n                                                  num_buckets=num_buckets,\n                                                  shuffle=shuffle,\n                                                  use_average_length=True,\n                                                  num_shards=num_ctxes)\n            dataloader = nlp.data.ShardedDataLoader(dataset,\n                                                    batch_sampler=sampler,\n                                                    batchify_fn=batchify_fn,\n                                                    num_workers=num_ctxes)\n        else:\n            sampler = nlp.data.FixedBucketSampler(lengths,\n                                                  batch_size=batch_size * num_ctxes,\n                                                  num_buckets=num_buckets,\n                                                  ratio=0,\n                                                  shuffle=shuffle)\n            dataloader = DataLoader(dataset=dataset,\n                                    batch_sampler=sampler,\n                                    batchify_fn=batchify_fn,\n                                    num_workers=1)\n        logging.debug('Sampler created for a new dataset:\\n%s', sampler.stats())\n        return dataloader\n\n    stream = stream.transform(get_dataloader)\n    return stream"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_dummy_dataloader(dataloader, target_shape):\n    data_iter = enumerate(dataloader)\n    _, data_batch = next(data_iter)\n    logging.debug('Searching target batch shape: %s', target_shape)\n    while data_batch[0].shape != target_shape:\n        logging.debug('Skip batch with shape %s', data_batch[0].shape)\n        _, data_batch = next(data_iter)\n    logging.debug('Found target dummy batch.')\n\n    class DummyIter():\n        def __init__(self, batch):\n            self._batch = batch\n\n        def __iter__(self):\n            while True:\n                yield self._batch\n\n    return DummyIter(data_batch)", "response": "Return a dummy data loader which returns a fixed data batch of target_shape."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsaves the model parameter marked by step_num.", "response": "def save_params(step_num, model, trainer, ckpt_dir):\n    \"\"\"Save the model parameter, marked by step_num.\"\"\"\n    param_path = os.path.join(ckpt_dir, '%07d.params'%step_num)\n    trainer_path = os.path.join(ckpt_dir, '%07d.states'%step_num)\n    logging.info('[step %d] Saving checkpoints to %s, %s.',\n                 step_num, param_path, trainer_path)\n    model.save_parameters(param_path)\n    trainer.save_states(trainer_path)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef split_and_load(arrs, ctx):\n    assert isinstance(arrs, (list, tuple))\n    # split and load\n    loaded_arrs = [mx.gluon.utils.split_and_load(arr, ctx, even_split=False) for arr in arrs]\n    return zip(*loaded_arrs)", "response": "split and load arrays to a list of contexts"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nforwarding computation for evaluation", "response": "def forward(data, model, mlm_loss, nsp_loss, vocab_size, dtype):\n    \"\"\"forward computation for evaluation\"\"\"\n    (input_id, masked_id, masked_position, masked_weight, \\\n     next_sentence_label, segment_id, valid_length) = data\n    num_masks = masked_weight.sum() + 1e-8\n    valid_length = valid_length.reshape(-1)\n    masked_id = masked_id.reshape(-1)\n    valid_length_typed = valid_length.astype(dtype, copy=False)\n    _, _, classified, decoded = model(input_id, segment_id, valid_length_typed,\n                                      masked_position)\n    decoded = decoded.reshape((-1, vocab_size))\n    ls1 = mlm_loss(decoded.astype('float32', copy=False),\n                   masked_id, masked_weight.reshape((-1, 1)))\n    ls2 = nsp_loss(classified.astype('float32', copy=False), next_sentence_label)\n    ls1 = ls1.sum() / num_masks\n    ls2 = ls2.mean()\n    ls = ls1 + ls2\n    return ls, next_sentence_label, classified, masked_id, decoded, \\\n           masked_weight, ls1, ls2, valid_length.astype('float32', copy=False)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nevaluating the model on the data.", "response": "def evaluate(data_eval, model, nsp_loss, mlm_loss, vocab_size, ctx, log_interval, dtype):\n    \"\"\"Evaluation function.\"\"\"\n    mlm_metric = MaskedAccuracy()\n    nsp_metric = MaskedAccuracy()\n    mlm_metric.reset()\n    nsp_metric.reset()\n\n    eval_begin_time = time.time()\n    begin_time = time.time()\n    step_num = 0\n    running_mlm_loss = running_nsp_loss = 0\n    total_mlm_loss = total_nsp_loss = 0\n    running_num_tks = 0\n    for _, dataloader in enumerate(data_eval):\n        for _, data_batch in enumerate(dataloader):\n            step_num += 1\n\n            data_list = split_and_load(data_batch, ctx)\n            loss_list = []\n            ns_label_list, ns_pred_list = [], []\n            mask_label_list, mask_pred_list, mask_weight_list = [], [], []\n            for data in data_list:\n                out = forward(data, model, mlm_loss, nsp_loss, vocab_size, dtype)\n                (ls, next_sentence_label, classified, masked_id,\n                 decoded, masked_weight, ls1, ls2, valid_length) = out\n                loss_list.append(ls)\n                ns_label_list.append(next_sentence_label)\n                ns_pred_list.append(classified)\n                mask_label_list.append(masked_id)\n                mask_pred_list.append(decoded)\n                mask_weight_list.append(masked_weight)\n\n                running_mlm_loss += ls1.as_in_context(mx.cpu())\n                running_nsp_loss += ls2.as_in_context(mx.cpu())\n                running_num_tks += valid_length.sum().as_in_context(mx.cpu())\n            nsp_metric.update(ns_label_list, ns_pred_list)\n            mlm_metric.update(mask_label_list, mask_pred_list, mask_weight_list)\n\n            # logging\n            if (step_num + 1) % (log_interval) == 0:\n                total_mlm_loss += running_mlm_loss\n                total_nsp_loss += running_nsp_loss\n                log(begin_time, running_num_tks, running_mlm_loss, running_nsp_loss,\n                    step_num, mlm_metric, nsp_metric, None, log_interval)\n                begin_time = time.time()\n                running_mlm_loss = running_nsp_loss = running_num_tks = 0\n                mlm_metric.reset_local()\n                nsp_metric.reset_local()\n\n    mx.nd.waitall()\n    eval_end_time = time.time()\n    total_mlm_loss /= step_num\n    total_nsp_loss /= step_num\n    logging.info('mlm_loss={:.3f}\\tmlm_acc={:.1f}\\tnsp_loss={:.3f}\\tnsp_acc={:.1f}\\t'\n                 .format(total_mlm_loss.asscalar(), mlm_metric.get_global()[1] * 100,\n                         total_nsp_loss.asscalar(), nsp_metric.get_global()[1] * 100))\n    logging.info('Eval cost={:.1f}s'.format(eval_end_time - eval_begin_time))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_argparser():\n    parser = argparse.ArgumentParser(description='BERT pretraining example.')\n    parser.add_argument('--num_steps', type=int, default=20, help='Number of optimization steps')\n    parser.add_argument('--num_buckets', type=int, default=1,\n                        help='Number of buckets for variable length sequence sampling')\n    parser.add_argument('--dtype', type=str, default='float32', help='data dtype')\n    parser.add_argument('--batch_size', type=int, default=8, help='Batch size per GPU.')\n    parser.add_argument('--accumulate', type=int, default=1,\n                        help='Number of batches for gradient accumulation. '\n                             'The effective batch size = batch_size * accumulate.')\n    parser.add_argument('--use_avg_len', action='store_true',\n                        help='Use average length information for the bucket sampler. '\n                             'The batch size is approximately the number of tokens in the batch')\n    parser.add_argument('--batch_size_eval', type=int, default=8,\n                        help='Batch size per GPU for evaluation.')\n    parser.add_argument('--dataset_name', type=str, default='book_corpus_wiki_en_uncased',\n                        help='The dataset from which the vocabulary is created. Options include '\n                             'book_corpus_wiki_en_uncased, book_corpus_wiki_en_cased. '\n                             'Default is book_corpus_wiki_en_uncased')\n    parser.add_argument('--pretrained', action='store_true',\n                        help='Load the pretrained model released by Google.')\n    parser.add_argument('--model', type=str, default='bert_12_768_12',\n                        help='Model to run pre-training on. '\n                             'Options are bert_12_768_12, bert_24_1024_16')\n    parser.add_argument('--data', type=str, default=None,\n                        help='Path to training data. Training is skipped if not set.')\n    parser.add_argument('--data_eval', type=str, default=None,\n                        help='Path to evaluation data. Evaluation is skipped if not set.')\n    parser.add_argument('--ckpt_dir', type=str, required=True,\n                        help='Path to checkpoint directory')\n    parser.add_argument('--start_step', type=int, default=0,\n                        help='Start optimization step from the checkpoint.')\n    parser.add_argument('--lr', type=float, default=1e-4, help='Learning rate')\n    parser.add_argument('--warmup_ratio', type=float, default=0.1,\n                        help='ratio of warmup steps used in NOAM\\'s stepsize schedule')\n    parser.add_argument('--log_interval', type=int, default=10, help='Report interval')\n    parser.add_argument('--ckpt_interval', type=int, default=250000, help='Checkpoint interval')\n    parser.add_argument('--dummy_data_len', type=int, default=None,\n                        help='If provided, a data batch of target sequence length is '\n                             'used. For benchmarking purpuse only.')\n    parser.add_argument('--seed', type=int, default=0, help='Random seed')\n    parser.add_argument('--verbose', action='store_true', help='verbose logging')\n    parser.add_argument('--profile', type=str, default=None,\n                        help='output profiling result to the target file')\n    return parser", "response": "Returns an argument parser for the bert pretraining example."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _cache_dataset(dataset, prefix):\n    if not os.path.exists(_constants.CACHE_PATH):\n        os.makedirs(_constants.CACHE_PATH)\n    src_data = np.concatenate([e[0] for e in dataset])\n    tgt_data = np.concatenate([e[1] for e in dataset])\n    src_cumlen = np.cumsum([0]+[len(e[0]) for e in dataset])\n    tgt_cumlen = np.cumsum([0]+[len(e[1]) for e in dataset])\n    np.savez(os.path.join(_constants.CACHE_PATH, prefix + '.npz'),\n             src_data=src_data, tgt_data=tgt_data,\n             src_cumlen=src_cumlen, tgt_cumlen=tgt_cumlen)", "response": "Cache the processed npy dataset into a npz\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_translation_data(dataset, bleu, args):\n    src_lang, tgt_lang = args.src_lang, args.tgt_lang\n    if dataset == 'IWSLT2015':\n        common_prefix = 'IWSLT2015_{}_{}_{}_{}'.format(src_lang, tgt_lang,\n                                                       args.src_max_len, args.tgt_max_len)\n        data_train = nlp.data.IWSLT2015('train', src_lang=src_lang, tgt_lang=tgt_lang)\n        data_val = nlp.data.IWSLT2015('val', src_lang=src_lang, tgt_lang=tgt_lang)\n        data_test = nlp.data.IWSLT2015('test', src_lang=src_lang, tgt_lang=tgt_lang)\n    elif dataset == 'WMT2016BPE':\n        common_prefix = 'WMT2016BPE_{}_{}_{}_{}'.format(src_lang, tgt_lang,\n                                                        args.src_max_len, args.tgt_max_len)\n        data_train = nlp.data.WMT2016BPE('train', src_lang=src_lang, tgt_lang=tgt_lang)\n        data_val = nlp.data.WMT2016BPE('newstest2013', src_lang=src_lang, tgt_lang=tgt_lang)\n        data_test = nlp.data.WMT2016BPE('newstest2014', src_lang=src_lang, tgt_lang=tgt_lang)\n    elif dataset == 'WMT2014BPE':\n        common_prefix = 'WMT2014BPE_{}_{}_{}_{}'.format(src_lang, tgt_lang,\n                                                        args.src_max_len, args.tgt_max_len)\n        data_train = nlp.data.WMT2014BPE('train', src_lang=src_lang, tgt_lang=tgt_lang)\n        data_val = nlp.data.WMT2014BPE('newstest2013', src_lang=src_lang, tgt_lang=tgt_lang)\n        data_test = nlp.data.WMT2014BPE('newstest2014', src_lang=src_lang, tgt_lang=tgt_lang,\n                                        full=args.full)\n    elif dataset == 'TOY':\n        common_prefix = 'TOY_{}_{}_{}_{}'.format(src_lang, tgt_lang,\n                                                 args.src_max_len, args.tgt_max_len)\n        data_train = _dataset.TOY('train', src_lang=src_lang, tgt_lang=tgt_lang)\n        data_val = _dataset.TOY('val', src_lang=src_lang, tgt_lang=tgt_lang)\n        data_test = _dataset.TOY('test', src_lang=src_lang, tgt_lang=tgt_lang)\n    else:\n        raise NotImplementedError\n    src_vocab, tgt_vocab = data_train.src_vocab, data_train.tgt_vocab\n    data_train_processed = _load_cached_dataset(common_prefix + '_train')\n    if not data_train_processed:\n        data_train_processed = process_dataset(data_train, src_vocab, tgt_vocab,\n                                               args.src_max_len, args.tgt_max_len)\n        _cache_dataset(data_train_processed, common_prefix + '_train')\n    data_val_processed = _load_cached_dataset(common_prefix + '_val')\n    if not data_val_processed:\n        data_val_processed = process_dataset(data_val, src_vocab, tgt_vocab)\n        _cache_dataset(data_val_processed, common_prefix + '_val')\n    if dataset == 'WMT2014BPE':\n        filename = common_prefix + '_' + str(args.full) + '_test'\n    else:\n        filename = common_prefix + '_test'\n    data_test_processed = _load_cached_dataset(filename)\n    if not data_test_processed:\n        data_test_processed = process_dataset(data_test, src_vocab, tgt_vocab)\n        _cache_dataset(data_test_processed, filename)\n    if bleu == 'tweaked':\n        fetch_tgt_sentence = lambda src, tgt: tgt.split()\n        val_tgt_sentences = list(data_val.transform(fetch_tgt_sentence))\n        test_tgt_sentences = list(data_test.transform(fetch_tgt_sentence))\n    elif bleu == '13a' or bleu == 'intl':\n        fetch_tgt_sentence = lambda src, tgt: tgt\n        if dataset == 'WMT2016BPE':\n            val_text = nlp.data.WMT2016('newstest2013', src_lang=src_lang, tgt_lang=tgt_lang)\n            test_text = nlp.data.WMT2016('newstest2014', src_lang=src_lang, tgt_lang=tgt_lang)\n        elif dataset == 'WMT2014BPE':\n            val_text = nlp.data.WMT2014('newstest2013', src_lang=src_lang, tgt_lang=tgt_lang)\n            test_text = nlp.data.WMT2014('newstest2014', src_lang=src_lang, tgt_lang=tgt_lang,\n                                         full=args.full)\n        elif dataset == 'IWSLT2015' or dataset == 'TOY':\n            val_text = data_val\n            test_text = data_test\n        else:\n            raise NotImplementedError\n        val_tgt_sentences = list(val_text.transform(fetch_tgt_sentence))\n        test_tgt_sentences = list(test_text.transform(fetch_tgt_sentence))\n    else:\n        raise NotImplementedError\n    return data_train_processed, data_val_processed, data_test_processed, \\\n           val_tgt_sentences, test_tgt_sentences, src_vocab, tgt_vocab", "response": "Load translation dataset\n\n    Parameters\n    ----------\n    dataset : str\n    args : argparse result\n\n    Returns\n    -------"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a data loader for training validation and test.", "response": "def make_dataloader(data_train, data_val, data_test, args,\n                    use_average_length=False, num_shards=0, num_workers=8):\n    \"\"\"Create data loaders for training/validation/test.\"\"\"\n    data_train_lengths = get_data_lengths(data_train)\n    data_val_lengths = get_data_lengths(data_val)\n    data_test_lengths = get_data_lengths(data_test)\n    train_batchify_fn = btf.Tuple(btf.Pad(), btf.Pad(),\n                                  btf.Stack(dtype='float32'), btf.Stack(dtype='float32'))\n    test_batchify_fn = btf.Tuple(btf.Pad(), btf.Pad(),\n                                 btf.Stack(dtype='float32'), btf.Stack(dtype='float32'),\n                                 btf.Stack())\n    target_val_lengths = list(map(lambda x: x[-1], data_val_lengths))\n    target_test_lengths = list(map(lambda x: x[-1], data_test_lengths))\n    if args.bucket_scheme == 'constant':\n        bucket_scheme = nlp.data.ConstWidthBucket()\n    elif args.bucket_scheme == 'linear':\n        bucket_scheme = nlp.data.LinearWidthBucket()\n    elif args.bucket_scheme == 'exp':\n        bucket_scheme = nlp.data.ExpWidthBucket(bucket_len_step=1.2)\n    else:\n        raise NotImplementedError\n    train_batch_sampler = nlp.data.FixedBucketSampler(lengths=data_train_lengths,\n                                                      batch_size=args.batch_size,\n                                                      num_buckets=args.num_buckets,\n                                                      ratio=args.bucket_ratio,\n                                                      shuffle=True,\n                                                      use_average_length=use_average_length,\n                                                      num_shards=num_shards,\n                                                      bucket_scheme=bucket_scheme)\n    logging.info('Train Batch Sampler:\\n%s', train_batch_sampler.stats())\n    train_data_loader = nlp.data.ShardedDataLoader(data_train,\n                                                   batch_sampler=train_batch_sampler,\n                                                   batchify_fn=train_batchify_fn,\n                                                   num_workers=num_workers)\n\n    val_batch_sampler = nlp.data.FixedBucketSampler(lengths=target_val_lengths,\n                                                    batch_size=args.test_batch_size,\n                                                    num_buckets=args.num_buckets,\n                                                    ratio=args.bucket_ratio,\n                                                    shuffle=False,\n                                                    use_average_length=use_average_length,\n                                                    bucket_scheme=bucket_scheme)\n    logging.info('Valid Batch Sampler:\\n%s', val_batch_sampler.stats())\n    val_data_loader = gluon.data.DataLoader(data_val,\n                                            batch_sampler=val_batch_sampler,\n                                            batchify_fn=test_batchify_fn,\n                                            num_workers=num_workers)\n    test_batch_sampler = nlp.data.FixedBucketSampler(lengths=target_test_lengths,\n                                                     batch_size=args.test_batch_size,\n                                                     num_buckets=args.num_buckets,\n                                                     ratio=args.bucket_ratio,\n                                                     shuffle=False,\n                                                     use_average_length=use_average_length,\n                                                     bucket_scheme=bucket_scheme)\n    logging.info('Test Batch Sampler:\\n%s', test_batch_sampler.stats())\n    test_data_loader = gluon.data.DataLoader(data_test,\n                                             batch_sampler=test_batch_sampler,\n                                             batchify_fn=test_batchify_fn,\n                                             num_workers=num_workers)\n    return train_data_loader, val_data_loader, test_data_loader"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run(self):\n        random.seed(self.seed)\n        np.random.seed(self.np_seed)\n        if not isinstance(self, multiprocessing.Process):\n            # Calling mxnet methods in a subprocess will raise an exception if\n            # mxnet is built with GPU support\n            # https://github.com/apache/incubator-mxnet/issues/4659\n            mx.random.seed(self.mx_seed)\n\n        # Startup - Master waits for this\n        try:\n            stream_iter = iter(self.stream)\n            self._errorq.put(None)\n        except Exception as e:  # pylint: disable=broad-except\n            tb = traceback.format_exc()\n            self._errorq.put((e, tb))\n\n        # Async work\n        while True:\n            try:  # Check control queue\n                c = self._controlq.get(False)\n                if c is None:\n                    break\n                else:\n                    raise RuntimeError('Got unexpected control code {}'.format(repr(c)))\n            except queue.Empty:\n                pass\n            except RuntimeError as e:\n                tb = traceback.format_exc()\n                self._errorq.put((e, tb))\n                self._dataq.put(None)\n\n            try:\n                data = next(stream_iter)\n                error = None\n            except Exception as e:  # pylint: disable=broad-except\n                tb = traceback.format_exc()\n                error = (e, tb)\n                data = None\n            finally:\n                self._errorq.put(error)\n                self._dataq.put(data)", "response": "Method representing the process s activity."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndefining the forward computation. Arguments can be eitherNDArray or Symbol.", "response": "def forward(self, inputs, states=None): # pylint: disable=arguments-differ\n        \"\"\"Defines the forward computation. Arguments can be either\n        :py:class:`NDArray` or :py:class:`Symbol`.\"\"\"\n        batch_size = inputs.shape[self._batch_axis]\n        skip_states = states is None\n        if skip_states:\n            states = self.cell.begin_state(batch_size, ctx=inputs.context)\n        if isinstance(states, ndarray.NDArray):\n            states = [states]\n        for state, info in zip(states, self.cell.state_info(batch_size)):\n            if state.shape != info['shape']:\n                raise ValueError(\n                    'Invalid recurrent state shape. Expecting %s, got %s.'%(\n                        str(info['shape']), str(state.shape)))\n        states = sum(zip(*((j for j in i) for i in states)), ())\n        outputs, states = self.cell.unroll(\n            inputs.shape[self._axis], inputs, states,\n            layout=self._layout, merge_outputs=True)\n\n        if skip_states:\n            return outputs\n        return outputs, states"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef hybrid_forward(self, F, words, weight):\n        #pylint: disable=arguments-differ\n        embeddings = F.sparse.dot(words, weight)\n        return embeddings", "response": "Compute embedding of words in batch."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload the class from the fastText binary format.", "response": "def load_fasttext_format(cls, path, ctx=cpu(), **kwargs):\n        \"\"\"Create an instance of the class and load weights.\n\n        Load the weights from the fastText binary format created by\n        https://github.com/facebookresearch/fastText\n\n        Parameters\n        ----------\n        path : str\n            Path to the .bin model file.\n        ctx : mx.Context, default mx.cpu()\n            Context to initialize the weights on.\n        kwargs : dict\n            Keyword arguments are passed to the class initializer.\n\n        \"\"\"\n        with open(path, 'rb') as f:\n            new_format, dim, bucket, minn, maxn, = cls._read_model_params(f)\n            idx_to_token = cls._read_vocab(f, new_format)\n            dim, matrix = cls._read_vectors(f, new_format, bucket,\n                                            len(idx_to_token))\n\n        token_to_idx = {token: idx for idx, token in enumerate(idx_to_token)}\n        if len(token_to_idx) != len(idx_to_token):\n            # If multiple tokens with invalid encoding were collapsed in a\n            # single token due to replacement of invalid bytes with Unicode\n            # replacement character\n            warnings.warn(\n                'There are duplicate tokens in the embedding file. '\n                'This is likely due to decoding errors for some tokens, '\n                'where invalid bytes were replaced by '\n                'the Unicode replacement character. '\n                'This affects {} tokens.'.format(\n                    len(idx_to_token) - len(token_to_idx)))\n            for _ in range(len(token_to_idx), len(idx_to_token)):\n                # Add pseudo tokens to make sure length is the same\n                token_to_idx[object()] = -1\n        assert len(token_to_idx) == len(idx_to_token)\n\n        subword_function = create_subword_function(\n            'NGramHashes', num_subwords=matrix.shape[0] - len(idx_to_token),\n            ngrams=list(range(minn, maxn + 1)), special_tokens={'</s>'})\n\n        self = cls(token_to_idx, subword_function, output_dim=dim, **kwargs)\n\n        self.initialize(ctx=ctx)\n        self.weight.set_data(nd.array(matrix))\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses command line arguments.", "response": "def parse_args():\n    \"\"\"Parse command line arguments.\"\"\"\n    parser = argparse.ArgumentParser(\n        description='GloVe with GluonNLP',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    # Data options\n    group = parser.add_argument_group('Data arguments')\n    group.add_argument(\n        'cooccurrences', type=str,\n        help='Path to cooccurrences.npz containing a sparse (COO) '\n        'representation of the co-occurrence matrix in numpy archive format. '\n        'Output of ./cooccur')\n    group.add_argument('vocab', type=str,\n                       help='Vocabulary indices. Output of vocab_count tool.')\n\n    # Computation options\n    group = parser.add_argument_group('Computation arguments')\n    group.add_argument('--batch-size', type=int, default=512,\n                       help='Batch size for training.')\n    group.add_argument('--epochs', type=int, default=50, help='Epoch limit')\n    group.add_argument(\n        '--gpu', type=int, nargs='+',\n        help='Number (index) of GPU to run on, e.g. 0. '\n        'If not specified, uses CPU.')\n    group.add_argument('--no-hybridize', action='store_true',\n                       help='Disable hybridization of gluon HybridBlocks.')\n    group.add_argument(\n        '--no-static-alloc', action='store_true',\n        help='Disable static memory allocation for HybridBlocks.')\n\n    # Model\n    group = parser.add_argument_group('Model arguments')\n    group.add_argument('--emsize', type=int, default=300,\n                       help='Size of embedding vectors.')\n    group.add_argument('--x-max', type=int, default=100)\n    group.add_argument('--alpha', type=float, default=0.75)\n\n    # Optimization options\n    group = parser.add_argument_group('Optimization arguments')\n    group.add_argument('--adagrad-eps', type=float, default=1,\n                       help='Initial AdaGrad state value.')\n    group.add_argument('--lr', type=float, default=0.1, help='Learning rate')\n    group.add_argument('--seed', type=int, default=1, help='Random seed')\n    group.add_argument('--dropout', type=float, default=0.15)\n\n    # Logging\n    group = parser.add_argument_group('Logging arguments')\n    group.add_argument('--logdir', type=str, default='logs',\n                       help='Directory to store logs.')\n    group.add_argument('--log-interval', type=int, default=100)\n    group.add_argument(\n        '--eval-interval', type=int,\n        help='Evaluate every --eval-interval iterations '\n        'in addition to at the end of every epoch.')\n    group.add_argument('--no-eval-analogy', action='store_true',\n                       help='Don\\'t evaluate on the analogy task.')\n\n    # Evaluation options\n    evaluation.add_parameters(parser)\n\n    args = parser.parse_args()\n    evaluation.validate_args(args)\n\n    random.seed(args.seed)\n    mx.random.seed(args.seed)\n    np.random.seed(args.seed)\n\n    return args"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_train_data(args):\n    counter = dict()\n    with io.open(args.vocab, 'r', encoding='utf-8') as f:\n        for line in f:\n            token, count = line.split('\\t')\n            counter[token] = int(count)\n    vocab = nlp.Vocab(counter, unknown_token=None, padding_token=None,\n                      bos_token=None, eos_token=None, min_freq=1)\n\n    npz = np.load(args.cooccurrences)\n    row, col, counts = npz['row'], npz['col'], npz['data']\n\n    rank_dtype = 'int32'\n    if row.max() >= np.iinfo(np.int32).max:\n        rank_dtype = 'int64'\n        # MXNet has no support for uint32, so we must fall back to int64\n        logging.info('More words than could be counted using int32. '\n                     'Using int64 to represent word indices.')\n    row = mx.nd.array(row, dtype=rank_dtype)\n    col = mx.nd.array(col, dtype=rank_dtype)\n    # row is always used as 'source' and col as 'context' word. Therefore\n    # duplicate the entries.\n\n    assert row.shape == col.shape\n    row = mx.nd.concatenate([row, col])\n    col = mx.nd.concatenate([col, row[:len(row) // 2]])\n\n    counts = mx.nd.array(counts, dtype='float32')\n    counts = mx.nd.concatenate([counts, counts])\n\n    return vocab, row, col, counts", "response": "Helper function to get training data."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef train(args):\n    vocab, row, col, counts = get_train_data(args)\n    model = GloVe(token_to_idx=vocab.token_to_idx, output_dim=args.emsize,\n                  dropout=args.dropout, x_max=args.x_max, alpha=args.alpha,\n                  weight_initializer=mx.init.Uniform(scale=1 / args.emsize))\n    context = get_context(args)\n    model.initialize(ctx=context)\n    if not args.no_hybridize:\n        model.hybridize(static_alloc=not args.no_static_alloc)\n\n    optimizer_kwargs = dict(learning_rate=args.lr, eps=args.adagrad_eps)\n    params = list(model.collect_params().values())\n    try:\n        trainer = mx.gluon.Trainer(params, 'groupadagrad', optimizer_kwargs)\n    except ValueError:\n        logging.warning('MXNet <= v1.3 does not contain '\n                        'GroupAdaGrad support. Falling back to AdaGrad')\n        trainer = mx.gluon.Trainer(params, 'adagrad', optimizer_kwargs)\n\n    index_dtype = 'int32'\n    if counts.shape[0] >= np.iinfo(np.int32).max:\n        index_dtype = 'int64'\n        logging.info('Co-occurrence matrix is large. '\n                     'Using int64 to represent sample indices.')\n    indices = mx.nd.arange(counts.shape[0], dtype=index_dtype)\n    for epoch in range(args.epochs):\n        # Logging variables\n        log_wc = 0\n        log_start_time = time.time()\n        log_avg_loss = 0\n\n        mx.nd.shuffle(indices, indices)  # inplace shuffle\n        bs = args.batch_size\n        num_batches = indices.shape[0] // bs\n        for i in range(num_batches):\n            batch_indices = indices[bs * i:bs * (i + 1)]\n            ctx = context[i % len(context)]\n            batch_row = row[batch_indices].as_in_context(ctx)\n            batch_col = col[batch_indices].as_in_context(ctx)\n            batch_counts = counts[batch_indices].as_in_context(ctx)\n            with mx.autograd.record():\n                loss = model(batch_row, batch_col, batch_counts)\n                loss.backward()\n\n            if len(context) == 1 or (i + 1) % len(context) == 0:\n                trainer.step(batch_size=1)\n\n            # Logging\n            log_wc += loss.shape[0]\n            log_avg_loss += loss.mean().as_in_context(context[0])\n            if (i + 1) % args.log_interval == 0:\n                # Forces waiting for computation by computing loss value\n                log_avg_loss = log_avg_loss.asscalar() / args.log_interval\n                wps = log_wc / (time.time() - log_start_time)\n                logging.info('[Epoch {} Batch {}/{}] loss={:.4f}, '\n                             'throughput={:.2f}K wps, wc={:.2f}K'.format(\n                                 epoch, i + 1, num_batches, log_avg_loss,\n                                 wps / 1000, log_wc / 1000))\n                log_dict = dict(\n                    global_step=epoch * len(indices) + i * args.batch_size,\n                    epoch=epoch, batch=i + 1, loss=log_avg_loss,\n                    wps=wps / 1000)\n                log(args, log_dict)\n\n                log_start_time = time.time()\n                log_avg_loss = 0\n                log_wc = 0\n\n            if args.eval_interval and (i + 1) % args.eval_interval == 0:\n                with print_time('mx.nd.waitall()'):\n                    mx.nd.waitall()\n                with print_time('evaluate'):\n                    evaluate(args, model, vocab, i + num_batches * epoch)\n\n    # Evaluate\n    with print_time('mx.nd.waitall()'):\n        mx.nd.waitall()\n    with print_time('evaluate'):\n        evaluate(args, model, vocab, num_batches * args.epochs,\n                 eval_analogy=not args.no_eval_analogy)\n\n    # Save params\n    with print_time('save parameters'):\n        model.save_parameters(os.path.join(args.logdir, 'glove.params'))", "response": "Train the GloVe model."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlogs to a file.", "response": "def log(args, kwargs):\n    \"\"\"Log to a file.\"\"\"\n    logfile = os.path.join(args.logdir, 'log.tsv')\n\n    if 'log_created' not in globals():\n        if os.path.exists(logfile):\n            logging.error('Logfile %s already exists.', logfile)\n            sys.exit(1)\n\n        global log_created\n\n        log_created = sorted(kwargs.keys())\n        header = '\\t'.join((str(k) for k in log_created)) + '\\n'\n        with open(logfile, 'w') as f:\n            f.write(header)\n\n    # Log variables shouldn't change during training\n    assert log_created == sorted(kwargs.keys())\n\n    with open(logfile, 'a') as f:\n        f.write('\\t'.join((str(kwargs[k]) for k in log_created)) + '\\n')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the embedding of words in batch.", "response": "def hybrid_forward(self, F, row, col, counts):\n        \"\"\"Compute embedding of words in batch.\n\n        Parameters\n        ----------\n        row : mxnet.nd.NDArray or mxnet.sym.Symbol\n            Array of token indices for source words. Shape (batch_size, ).\n        row : mxnet.nd.NDArray or mxnet.sym.Symbol\n            Array of token indices for context words. Shape (batch_size, ).\n        counts : mxnet.nd.NDArray or mxnet.sym.Symbol\n            Their co-occurrence counts. Shape (batch_size, ).\n\n        Returns\n        -------\n        mxnet.nd.NDArray or mxnet.sym.Symbol\n            Loss. Shape (batch_size, ).\n\n        \"\"\"\n\n        emb_in = self.source_embedding(row)\n        emb_out = self.context_embedding(col)\n\n        if self._dropout:\n            emb_in = F.Dropout(emb_in, p=self._dropout)\n            emb_out = F.Dropout(emb_out, p=self._dropout)\n\n        bias_in = self.source_bias(row).squeeze()\n        bias_out = self.context_bias(col).squeeze()\n        dot = F.batch_dot(emb_in.expand_dims(1),\n                          emb_out.expand_dims(2)).squeeze()\n        tmp = dot + bias_in + bias_out - F.log(counts).squeeze()\n        weight = F.clip(((counts / self._x_max)**self._alpha), a_min=0,\n                        a_max=1).squeeze()\n        loss = weight * F.square(tmp)\n        return loss"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update(self, labels, preds, masks=None):\n        # pylint: disable=arguments-differ\n        \"\"\"Updates the internal evaluation result.\n\n        Parameters\n        ----------\n        labels : list of `NDArray`\n            The labels of the data with class indices as values, one per sample.\n        preds : list of `NDArray`\n            Prediction values for samples. Each prediction value can either be the class index,\n            or a vector of likelihoods for all classes.\n        masks : list of `NDArray` or None, optional\n            Masks for samples, with the same shape as `labels`. value of its element must\n            be either 1 or 0. If None, all samples are considered valid.\n        \"\"\"\n        labels, preds = check_label_shapes(labels, preds, True)\n        masks = [None] * len(labels) if masks is None else masks\n\n        for label, pred_label, mask in zip(labels, preds, masks):\n            if pred_label.shape != label.shape:\n                # TODO(haibin) topk does not support fp16. Issue tracked at:\n                # https://github.com/apache/incubator-mxnet/issues/14125\n                # topk is used because argmax is slow:\n                # https://github.com/apache/incubator-mxnet/issues/11061\n                pred_label = ndarray.topk(pred_label.astype('float32', copy=False),\n                                          k=1, ret_typ='indices', axis=self.axis)\n\n            # flatten before checking shapes to avoid shape miss match\n            pred_label = pred_label.astype('int32', copy=False).reshape((-1,))\n            label = label.astype('int32', copy=False).reshape((-1,))\n            check_label_shapes(label, pred_label)\n\n            if mask is not None:\n                mask = mask.astype('int32', copy=False).reshape((-1,))\n                check_label_shapes(label, mask)\n                num_correct = ((pred_label == label) * mask).sum().asscalar()\n                num_inst = mask.sum().asscalar()\n            else:\n                num_correct = (pred_label == label).sum().asscalar()\n                num_inst = len(label)\n            self.sum_metric += num_correct\n            self.global_sum_metric += num_correct\n            self.num_inst += num_inst\n            self.global_num_inst += num_inst", "response": "Updates the internal evaluation result of the internal data structures."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\npredicting the relation of two sentences.", "response": "def hybrid_forward(self, F, sentence1, sentence2):\n        \"\"\"\n        Predict the relation of two sentences.\n\n        Parameters\n        ----------\n        sentence1 : NDArray\n            Shape (batch_size, length)\n        sentence2 : NDArray\n            Shape (batch_size, length)\n\n        Returns\n        -------\n        pred : NDArray\n            Shape (batch_size, num_classes). num_classes == 3.\n\n        \"\"\"\n        feature1 = self.lin_proj(self.word_emb(sentence1))\n        feature2 = self.lin_proj(self.word_emb(sentence2))\n        if self.use_intra_attention:\n            feature1 = F.concat(feature1, self.intra_attention(feature1), dim=-1)\n            feature2 = F.concat(feature2, self.intra_attention(feature2), dim=-1)\n        pred = self.model(feature1, feature2)\n        return pred"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef hybrid_forward(self, F, feature_a):\n        tilde_a = self.intra_attn_emb(feature_a)\n        e_matrix = F.batch_dot(tilde_a, tilde_a, transpose_b=True)\n        alpha = F.batch_dot(e_matrix.softmax(), tilde_a)\n        return alpha", "response": "Computes intra - sentence attention given embedded words."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef hybrid_forward(self, F, a, b):\n        # a.shape = [B, L1, H]\n        # b.shape = [B, L2, H]\n        # extract features\n        tilde_a = self.f(a)  # shape = [B, L1, H]\n        tilde_b = self.f(b)  # shape = [B, L2, H]\n        # attention\n        # e.shape = [B, L1, L2]\n        e = F.batch_dot(tilde_a, tilde_b, transpose_b=True)\n        # beta: b align to a, [B, L1, H]\n        beta = F.batch_dot(e.softmax(), tilde_b)\n        # alpha: a align to b, [B, L2, H]\n        alpha = F.batch_dot(e.transpose([0, 2, 1]).softmax(), tilde_a)\n        # compare\n        feature1 = self.g(F.concat(tilde_a, beta, dim=2))\n        feature2 = self.g(F.concat(tilde_b, alpha, dim=2))\n        feature1 = feature1.sum(axis=1)\n        feature2 = feature2.sum(axis=1)\n        yhat = self.h(F.concat(feature1, feature2, dim=1))\n        return yhat", "response": "Forward implementation of the log - likelihood layer."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nslice a flat sequence of tokens into sequences tokens with length equal to the specified length.", "response": "def slice_sequence(sequence, length, pad_last=False, pad_val=C.PAD_TOKEN, overlap=0):\n    \"\"\"Slice a flat sequence of tokens into sequences tokens, with each\n    inner sequence's length equal to the specified `length`, taking into account the requested\n    sequence overlap.\n\n    Parameters\n    ----------\n    sequence : list of object\n        A flat list of tokens.\n    length : int\n        The length of each of the samples.\n    pad_last : bool, default False\n        Whether to pad the last sequence when its length doesn't align. If the last sequence's\n        length doesn't align and ``pad_last`` is False, it will be dropped.\n    pad_val : object, default\n        The padding value to use when the padding of the last sequence is enabled. In general,\n        the type of ``pad_val`` should be the same as the tokens.\n    overlap : int, default 0\n        The extra number of items in current sample that should overlap with the\n        next sample.\n\n    Returns\n    -------\n    List of list of tokens, with the length of each inner list equal to `length`.\n\n    \"\"\"\n    if length <= overlap:\n        raise ValueError('length needs to be larger than overlap')\n\n    if pad_last:\n        pad_len = _slice_pad_length(len(sequence), length, overlap)\n        sequence = sequence + [pad_val] * pad_len\n    num_samples = (len(sequence) - length) // (length - overlap) + 1\n\n    return [sequence[i * (length - overlap): ((i + 1) * length - i * overlap)]\n            for i in range(num_samples)]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _slice_pad_length(num_items, length, overlap=0):\n    if length <= overlap:\n        raise ValueError('length needs to be larger than overlap')\n\n    step = length - overlap\n    span = num_items - length\n    residual = span % step\n    if residual:\n        return step - residual\n    else:\n        return 0", "response": "Calculate the padding length needed for a sliced dataset."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsplitting the dataset into training and validation sets.", "response": "def train_valid_split(dataset, valid_ratio=0.05):\n    \"\"\"Split the dataset into training and validation sets.\n\n    Parameters\n    ----------\n    dataset : list\n        A list of training samples.\n    valid_ratio : float, default 0.05\n        Proportion of training samples to use for validation set\n        range: [0, 1]\n\n    Returns\n    -------\n    train : SimpleDataset\n    valid : SimpleDataset\n    \"\"\"\n    if not 0.0 <= valid_ratio <= 1.0:\n        raise ValueError('valid_ratio should be in [0, 1]')\n\n    num_train = len(dataset)\n    num_valid = np.ceil(num_train * valid_ratio).astype('int')\n    indices = np.arange(num_train)\n\n    np.random.shuffle(indices)\n    valid = SimpleDataset([dataset[indices[i]] for i in range(num_valid)])\n    train = SimpleDataset([dataset[indices[i + num_valid]] for i in range(num_train - num_valid)])\n    return train, valid"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nloading the accompanying vocabulary object for a pre - trained model.", "response": "def _load_pretrained_vocab(name, root=os.path.join(get_home_dir(), 'models'), cls=None):\n    \"\"\"Load the accompanying vocabulary object for pre-trained model.\n\n    Parameters\n    ----------\n    name : str\n        Name of the vocabulary, usually the name of the dataset.\n    root : str, default '$MXNET_HOME/models'\n        Location for keeping the model parameters.\n        MXNET_HOME defaults to '~/.mxnet'.\n    cls : nlp.Vocab or nlp.vocab.BERTVocab, default nlp.Vocab\n\n    Returns\n    -------\n    Vocab or nlp.vocab.BERTVocab\n        Loaded vocabulary object for the pre-trained model.\n    \"\"\"\n    file_name = '{name}-{short_hash}'.format(name=name,\n                                             short_hash=short_hash(name))\n    root = os.path.expanduser(root)\n    file_path = os.path.join(root, file_name + '.vocab')\n    sha1_hash = _vocab_sha1[name]\n    if os.path.exists(file_path):\n        if check_sha1(file_path, sha1_hash):\n            return _load_vocab_file(file_path, cls)\n        else:\n            print('Detected mismatch in the content of model vocab file. Downloading again.')\n    else:\n        print('Vocab file is not found. Downloading.')\n\n    if not os.path.exists(root):\n        os.makedirs(root)\n\n    zip_file_path = os.path.join(root, file_name + '.zip')\n    repo_url = _get_repo_url()\n    if repo_url[-1] != '/':\n        repo_url = repo_url + '/'\n    download(_url_format.format(repo_url=repo_url, file_name=file_name),\n             path=zip_file_path,\n             overwrite=True)\n    with zipfile.ZipFile(zip_file_path) as zf:\n        zf.extractall(root)\n    os.remove(zip_file_path)\n\n    if check_sha1(file_path, sha1_hash):\n        return _load_vocab_file(file_path, cls)\n    else:\n        raise ValueError('Downloaded file has different hash. Please try again.')"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nextracts the archive file to target_dir", "response": "def _extract_archive(file, target_dir):\n    \"\"\"Extract archive file\n\n    Parameters\n    ----------\n    file : str\n        Absolute path of the archive file.\n    target_dir : str\n        Target directory of the archive to be uncompressed\n\n    \"\"\"\n    if file.endswith('.gz') or file.endswith('.tar') or file.endswith('.tgz'):\n        archive = tarfile.open(file, 'r')\n    elif file.endswith('.zip'):\n        archive = zipfile.ZipFile(file, 'r')\n    else:\n        raise Exception('Unrecognized file type: ' + file)\n    archive.extractall(path=target_dir)\n    archive.close()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndiscard tokens with frequency below min_frequency and represents them as unknown_token.", "response": "def discard(self, min_freq, unknown_token):\n        \"\"\"Discards tokens with frequency below min_frequency and represents them\n        as `unknown_token`.\n\n        Parameters\n        ----------\n        min_freq: int\n            Tokens whose frequency is under min_freq is counted as `unknown_token` in\n            the Counter returned.\n        unknown_token: str\n            The representation for any unknown token.\n\n        Returns\n        -------\n        The Counter instance.\n\n        Examples\n        --------\n        >>> a = gluonnlp.data.Counter({'a': 10, 'b': 1, 'c': 1})\n        >>> a.discard(3, '<unk>')\n        Counter({'a': 10, '<unk>': 2})\n        \"\"\"\n        freq = 0\n        ret = Counter({})\n        for token, count in self.items():\n            if count < min_freq:\n                freq += count\n            else:\n                ret[token] = count\n        ret[unknown_token] = ret.get(unknown_token, 0) + freq\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef forward(self, inputs, token_types, valid_length=None):  # pylint: disable=arguments-differ\n        bert_output = self.bert(inputs, token_types, valid_length)\n        output = self.span_classifier(bert_output)\n        return output", "response": "Generate the unnormalized score for the given input sequences."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndefine the forward computation for cache cell.", "response": "def hybrid_forward(self, F, inputs, states=None, mask=None):\n        # pylint: disable=arguments-differ\n        # pylint: disable=unused-argument\n        \"\"\"Defines the forward computation for cache cell. Arguments can be either\n        :py:class:`NDArray` or :py:class:`Symbol`.\n\n        Parameters\n        ----------\n        inputs : NDArray\n            The input data layout='TNC'.\n        states : Tuple[List[List[NDArray]]]\n            The states. including:\n            states[0] indicates the states used in forward layer,\n            Each layer has a list of two initial tensors with\n            shape (batch_size, proj_size) and (batch_size, hidden_size).\n            states[1] indicates the states used in backward layer,\n            Each layer has a list of two initial tensors with\n            shape (batch_size, proj_size) and (batch_size, hidden_size).\n\n        Returns\n        --------\n        out: NDArray\n            The output data with shape (num_layers, seq_len, batch_size, 2*input_size).\n        [states_forward, states_backward] : List\n            Including:\n            states_forward: The out states from forward layer,\n            which has the same structure with *states[0]*.\n            states_backward: The out states from backward layer,\n            which has the same structure with *states[1]*.\n        \"\"\"\n        states_forward, states_backward = states\n        if mask is not None:\n            sequence_length = mask.sum(axis=1)\n\n        outputs_forward = []\n        outputs_backward = []\n\n        for layer_index in range(self._num_layers):\n            if layer_index == 0:\n                layer_inputs = inputs\n            else:\n                layer_inputs = outputs_forward[layer_index-1]\n            output, states_forward[layer_index] = F.contrib.foreach(\n                self.forward_layers[layer_index],\n                layer_inputs,\n                states_forward[layer_index])\n            outputs_forward.append(output)\n\n            if layer_index == 0:\n                layer_inputs = inputs\n            else:\n                layer_inputs = outputs_backward[layer_index-1]\n\n            if mask is not None:\n                layer_inputs = F.SequenceReverse(layer_inputs,\n                                                 sequence_length=sequence_length,\n                                                 use_sequence_length=True, axis=0)\n            else:\n                layer_inputs = F.SequenceReverse(layer_inputs, axis=0)\n            output, states_backward[layer_index] = F.contrib.foreach(\n                self.backward_layers[layer_index],\n                layer_inputs,\n                states_backward[layer_index])\n            if mask is not None:\n                backward_out = F.SequenceReverse(output,\n                                                 sequence_length=sequence_length,\n                                                 use_sequence_length=True, axis=0)\n            else:\n                backward_out = F.SequenceReverse(output, axis=0)\n            outputs_backward.append(backward_out)\n        out = F.concat(*[F.stack(*outputs_forward, axis=0),\n                         F.stack(*outputs_backward, axis=0)], dim=-1)\n\n        return out, [states_forward, states_backward]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading tokens from the provided parse tree in the SNLI dataset.", "response": "def main(args):\n    \"\"\"\n    Read tokens from the provided parse tree in the SNLI dataset.\n    Illegal examples are removed.\n    \"\"\"\n    examples = []\n    with open(args.input, 'r') as fin:\n        reader = csv.DictReader(fin, delimiter='\\t')\n        for cols in reader:\n            s1 = read_tokens(cols['sentence1_parse'])\n            s2 = read_tokens(cols['sentence2_parse'])\n            label = cols['gold_label']\n            if label in ('neutral', 'contradiction', 'entailment'):\n                examples.append((s1, s2, label))\n    with open(args.output, 'w') as fout:\n        for s1, s2, l in examples:\n            fout.write('{}\\t{}\\t{}\\n'.format(' '.join(s1), ' '.join(s2), l))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrecentering the k - meansCOOKIE.", "response": "def _recenter(self):\n        \"\"\"\n        one iteration of k-means\n        \"\"\"\n        for split_idx in range(len(self._splits)):\n            split = self._splits[split_idx]\n            len_idx = self._split2len_idx[split]\n            if split == self._splits[-1]:\n                continue\n            right_split = self._splits[split_idx + 1]\n\n            # Try shifting the centroid to the left\n            if len_idx > 0 and self._lengths[len_idx - 1] not in self._split_cntr:\n                new_split = self._lengths[len_idx - 1]\n                left_delta = self._len_cntr[split] * (right_split - new_split) - self._split_cntr[split] * (\n                        split - new_split)\n                if left_delta < 0:\n                    self._splits[split_idx] = new_split\n                    self._split2len_idx[new_split] = len_idx - 1\n                    del self._split2len_idx[split]\n                    self._split_cntr[split] -= self._len_cntr[split]\n                    self._split_cntr[right_split] += self._len_cntr[split]\n                    self._split_cntr[new_split] = self._split_cntr[split]\n                    del self._split_cntr[split]\n\n            # Try shifting the centroid to the right\n            elif len_idx < len(self._lengths) - 2 and self._lengths[len_idx + 1] not in self._split_cntr:\n                new_split = self._lengths[len_idx + 1]\n                right_delta = self._split_cntr[split] * (new_split - split) - self._len_cntr[split] * (\n                        new_split - split)\n                if right_delta <= 0:\n                    self._splits[split_idx] = new_split\n                    self._split2len_idx[new_split] = len_idx + 1\n                    del self._split2len_idx[split]\n                    self._split_cntr[split] += self._len_cntr[split]\n                    self._split_cntr[right_split] -= self._len_cntr[split]\n                    self._split_cntr[new_split] = self._split_cntr[split]\n                    del self._split_cntr[split]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _reindex(self):\n        self._len2split_idx = {}\n        last_split = -1\n        for split_idx, split in enumerate(self._splits):\n            self._len2split_idx.update(\n                dict(list(zip(list(range(last_split + 1, split)), [split_idx] * (split - (last_split + 1))))))", "response": "Reindex the cluster by adding all the words into the cluster."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_gnmt_encoder_decoder(cell_type='lstm', attention_cell='scaled_luong', num_layers=2,\n                             num_bi_layers=1, hidden_size=128, dropout=0.0, use_residual=False,\n                             i2h_weight_initializer=None, h2h_weight_initializer=None,\n                             i2h_bias_initializer=mx.init.LSTMBias(forget_bias=1.0),\n                             h2h_bias_initializer='zeros',\n                             prefix='gnmt_', params=None):\n    \"\"\"Build a pair of GNMT encoder/decoder\n\n    Parameters\n    ----------\n    cell_type : str or type\n    attention_cell : str or AttentionCell\n    num_layers : int\n    num_bi_layers : int\n    hidden_size : int\n    dropout : float\n    use_residual : bool\n    i2h_weight_initializer : mx.init.Initializer or None\n    h2h_weight_initializer : mx.init.Initializer or None\n    i2h_bias_initializer : mx.init.Initializer or None\n    h2h_bias_initializer : mx.init.Initializer or None\n    prefix : str, default 'gnmt_'\n        Prefix for name of `Block`s.\n    params : Parameter or None\n        Container for weight sharing between cells.\n        Created if `None`.\n\n    Returns\n    -------\n    encoder : GNMTEncoder\n    decoder : GNMTDecoder\n    \"\"\"\n    encoder = GNMTEncoder(cell_type=cell_type, num_layers=num_layers, num_bi_layers=num_bi_layers,\n                          hidden_size=hidden_size, dropout=dropout,\n                          use_residual=use_residual,\n                          i2h_weight_initializer=i2h_weight_initializer,\n                          h2h_weight_initializer=h2h_weight_initializer,\n                          i2h_bias_initializer=i2h_bias_initializer,\n                          h2h_bias_initializer=h2h_bias_initializer,\n                          prefix=prefix + 'enc_', params=params)\n    decoder = GNMTDecoder(cell_type=cell_type, attention_cell=attention_cell, num_layers=num_layers,\n                          hidden_size=hidden_size, dropout=dropout,\n                          use_residual=use_residual,\n                          i2h_weight_initializer=i2h_weight_initializer,\n                          h2h_weight_initializer=h2h_weight_initializer,\n                          i2h_bias_initializer=i2h_bias_initializer,\n                          h2h_bias_initializer=h2h_bias_initializer,\n                          prefix=prefix + 'dec_', params=params)\n    return encoder, decoder", "response": "Returns a GNMTEncoder and GNMTDecoder object for a single block of GNMT."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef init_state_from_encoder(self, encoder_outputs, encoder_valid_length=None):\n        mem_value, rnn_states = encoder_outputs\n        batch_size, _, mem_size = mem_value.shape\n        attention_vec = mx.nd.zeros(shape=(batch_size, mem_size), ctx=mem_value.context)\n        decoder_states = [rnn_states, attention_vec, mem_value]\n        mem_length = mem_value.shape[1]\n        if encoder_valid_length is not None:\n            mem_masks = mx.nd.broadcast_lesser(\n                mx.nd.arange(mem_length, ctx=encoder_valid_length.context).reshape((1, -1)),\n                encoder_valid_length.reshape((-1, 1)))\n            decoder_states.append(mem_masks)\n        return decoder_states", "response": "Initialize the decoder states from the encoder outputs."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef decode_seq(self, inputs, states, valid_length=None):\n        length = inputs.shape[1]\n        output = []\n        additional_outputs = []\n        inputs = _as_list(mx.nd.split(inputs, num_outputs=length, axis=1, squeeze_axis=True))\n        rnn_states_l = []\n        attention_output_l = []\n        fixed_states = states[2:]\n        for i in range(length):\n            ele_output, states, ele_additional_outputs = self.forward(inputs[i], states)\n            rnn_states_l.append(states[0])\n            attention_output_l.append(states[1])\n            output.append(ele_output)\n            additional_outputs.extend(ele_additional_outputs)\n        output = mx.nd.stack(*output, axis=1)\n        if valid_length is not None:\n            states = [_nested_sequence_last(rnn_states_l, valid_length),\n                      _nested_sequence_last(attention_output_l, valid_length)] + fixed_states\n            output = mx.nd.SequenceMask(output,\n                                        sequence_length=valid_length,\n                                        use_sequence_length=True,\n                                        axis=1)\n        if self._output_attention:\n            additional_outputs = [mx.nd.concat(*additional_outputs, dim=-2)]\n        return output, states, additional_outputs", "response": "This function decodes the input tensor and returns the decoder outputs."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef transform(instance, tokenizer, max_seq_length, max_predictions_per_seq, do_pad=True):\n    pad = tokenizer.convert_tokens_to_ids(['[PAD]'])[0]\n    input_ids = tokenizer.convert_tokens_to_ids(instance.tokens)\n    input_mask = [1] * len(input_ids)\n    segment_ids = list(instance.segment_ids)\n    assert len(input_ids) <= max_seq_length\n    valid_lengths = len(input_ids)\n\n    masked_lm_positions = list(instance.masked_lm_positions)\n    masked_lm_ids = tokenizer.convert_tokens_to_ids(\n        instance.masked_lm_labels)\n    masked_lm_weights = [1.0] * len(masked_lm_ids)\n    masked_lm_valid_lengths = len(masked_lm_ids)\n    if do_pad:\n        while len(input_ids) < max_seq_length:\n            input_ids.append(pad)\n            # Padding index MUST be defined to 0 on input_mask, segment_ids\n            input_mask.append(0)\n            segment_ids.append(0)\n        while len(masked_lm_positions) < max_predictions_per_seq:\n            masked_lm_positions.append(0)\n            masked_lm_ids.append(pad)\n            masked_lm_weights.append(0.0)\n\n        assert len(input_ids) == max_seq_length\n        assert len(input_mask) == max_seq_length\n        assert len(segment_ids) == max_seq_length\n\n    next_sentence_label = 1 if instance.is_random_next else 0\n\n    features = {}\n    features['input_ids'] = input_ids\n    features['input_mask'] = input_mask\n    features['segment_ids'] = segment_ids\n    features['masked_lm_positions'] = masked_lm_positions\n    features['masked_lm_ids'] = masked_lm_ids\n    features['segment_a_lengths'] = [instance.segment_a_lengths]\n    features['segment_b_lengths'] = [instance.segment_b_lengths]\n    features['masked_lm_ids'] = masked_lm_ids\n    features['masked_lm_weights'] = masked_lm_weights\n    features['next_sentence_labels'] = [next_sentence_label]\n    features['valid_lengths'] = [valid_lengths]\n    features['masked_lm_valid_lengths'] = [masked_lm_valid_lengths]\n\n    return features", "response": "Transform instance to inputs for MLM and NSP."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef write_to_files_np(features, tokenizer, max_seq_length,\n                      max_predictions_per_seq, output_files):\n    # pylint: disable=unused-argument\n    \"\"\"Write to numpy files from `TrainingInstance`s.\"\"\"\n    next_sentence_labels = []\n    valid_lengths = []\n\n    assert len(output_files) == 1, 'numpy format only support single output file'\n    output_file = output_files[0]\n    (input_ids, segment_ids, masked_lm_positions, masked_lm_ids,\n     masked_lm_weights, next_sentence_labels, valid_lengths) = features\n    total_written = len(next_sentence_labels)\n\n    # store variable length numpy array object directly.\n    outputs = collections.OrderedDict()\n    outputs['input_ids'] = np.array(input_ids, dtype=object)\n    outputs['segment_ids'] = np.array(segment_ids, dtype=object)\n    outputs['masked_lm_positions'] = np.array(masked_lm_positions, dtype=object)\n    outputs['masked_lm_ids'] = np.array(masked_lm_ids, dtype=object)\n    outputs['masked_lm_weights'] = np.array(masked_lm_weights, dtype=object)\n    outputs['next_sentence_labels'] = np.array(next_sentence_labels, dtype='int32')\n    outputs['valid_lengths'] = np.array(valid_lengths, dtype='int32')\n\n    np.savez_compressed(output_file, **outputs)\n    logging.info('Wrote %d total instances', total_written)", "response": "Write to numpy files from training instance s."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates IndexedRecordIO files from TrainingInstance s.", "response": "def write_to_files_rec(instances, tokenizer, max_seq_length,\n                       max_predictions_per_seq, output_files):\n    \"\"\"Create IndexedRecordIO files from `TrainingInstance`s.\"\"\"\n    writers = []\n    for output_file in output_files:\n        writers.append(\n            mx.recordio.MXIndexedRecordIO(\n                os.path.splitext(output_file)[0] + '.idx', output_file, 'w'))\n\n    writer_index = 0\n    total_written = 0\n    for (inst_index, instance) in enumerate(instances):\n        features = transform(instance, tokenizer, max_seq_length, max_predictions_per_seq)\n        example = features\n\n        writers[writer_index].write_idx(inst_index, json.dumps(example).encode('UTF-8'))\n        writer_index = (writer_index + 1) % len(writers)\n        total_written += 1\n\n    for writer in writers:\n        writer.close()\n\n    logging.info('Wrote %d total instances', total_written)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_training_instances(x):\n    (input_files, out, tokenizer, max_seq_length, dupe_factor,\n     short_seq_prob, masked_lm_prob, max_predictions_per_seq, rng) = x\n    time_start = time.time()\n    logging.info('Processing %s', input_files)\n    all_documents = [[]]\n\n    # Input file format:\n    # (1) One sentence per line. These should ideally be actual sentences, not\n    # entire paragraphs or arbitrary spans of text. (Because we use the\n    # sentence boundaries for the \"next sentence prediction\" task).\n    # (2) Blank lines between documents. Document boundaries are needed so\n    # that the \"next sentence prediction\" task doesn't span between documents.\n    for input_file in input_files:\n        with io.open(input_file, 'r', encoding='UTF-8') as reader:\n            while True:\n                line = reader.readline()\n                if not line:\n                    break\n                line = line.strip()\n\n                # Empty lines are used as document delimiters\n                if not line:\n                    all_documents.append([])\n                tokens = tokenizer(line) if not args.tokenized else line.split(' ')\n                if tokens:\n                    all_documents[-1].append(tokens)\n\n    # Remove empty documents\n    all_documents = [x for x in all_documents if x]\n    rng.shuffle(all_documents)\n\n    vocab_words = tokenizer.vocab.idx_to_token\n    instances = []\n    for _ in range(dupe_factor):\n        for document_index in range(len(all_documents)):\n            instances.extend(\n                create_instances_from_document(\n                    all_documents, document_index, max_seq_length,\n                    short_seq_prob, masked_lm_prob, max_predictions_per_seq,\n                    vocab_words, rng))\n\n    rng.shuffle(instances)\n    input_ids = []\n    segment_ids = []\n    masked_lm_positions = []\n    masked_lm_ids = []\n    masked_lm_weights = []\n    next_sentence_labels = []\n    valid_lengths = []\n\n    for inst_index, instance in enumerate(instances):\n        feature = transform(instance, tokenizer, max_seq_length, max_predictions_per_seq, False)\n        input_ids.append(\n            np.ascontiguousarray(feature['input_ids'], dtype='int32'))\n        segment_ids.append(\n            np.ascontiguousarray(feature['segment_ids'], dtype='int32'))\n        masked_lm_positions.append(\n            np.ascontiguousarray(feature['masked_lm_positions'], dtype='int32'))\n        masked_lm_ids.append(np.ascontiguousarray(feature['masked_lm_ids'], dtype='int32'))\n        masked_lm_weights.append(\n            np.ascontiguousarray(feature['masked_lm_weights'], dtype='float32'))\n        next_sentence_labels.append(feature['next_sentence_labels'][0])\n        valid_lengths.append(feature['valid_lengths'][0])\n        if inst_index < 20:\n            print_example(instance, feature)\n\n    features = (input_ids, segment_ids, masked_lm_positions, masked_lm_ids, \\\n                masked_lm_weights, next_sentence_labels, valid_lengths)\n\n    logging.info('*** Writing to output file %s ***', out)\n    if args.format == 'numpy':\n        write_to_files_np(features, tokenizer, args.max_seq_length,\n                          args.max_predictions_per_seq, [out])\n    elif args.format == 'recordio':\n        write_to_files_rec(instances, tokenizer, args.max_seq_length,\n                           args.max_predictions_per_seq, [out])\n    else:\n        raise ValueError('unsupported format: %s'%args.format)\n\n    time_end = time.time()\n    logging.info('Process %d files took %.1f s', len(input_files), time_end - time_start)", "response": "Create training instances from raw text."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_instances_from_document(\n        all_documents, document_index, max_seq_length, short_seq_prob,\n        masked_lm_prob, max_predictions_per_seq, vocab_words, rng):\n    \"\"\"Creates `TrainingInstance`s for a single document.\"\"\"\n    document = all_documents[document_index]\n\n    # Account for [CLS], [SEP], [SEP]\n    max_num_tokens = max_seq_length - 3\n\n    # We *usually* want to fill up the entire sequence since we are padding\n    # to `max_seq_length` anyways, so short sequences are generally wasted\n    # computation. However, we *sometimes*\n    # (i.e., short_seq_prob == 0.1 == 10% of the time) want to use shorter\n    # sequences to minimize the mismatch between pre-training and fine-tuning.\n    # The `target_seq_length` is just a rough target however, whereas\n    # `max_seq_length` is a hard limit.\n    target_seq_length = max_num_tokens\n    if rng.random() < short_seq_prob:\n        target_seq_length = rng.randint(2, max_num_tokens)\n\n    # We DON'T just concatenate all of the tokens from a document into a long\n    # sequence and choose an arbitrary split point because this would make the\n    # next sentence prediction task too easy. Instead, we split the input into\n    # segments \"A\" and \"B\" based on the actual \"sentences\" provided by the user\n    # input.\n    instances = []\n    current_chunk = []\n    current_length = 0\n    i = 0\n    while i < len(document):  # pylint: disable=R1702\n        segment = document[i]\n        current_chunk.append(segment)\n        current_length += len(segment)\n        if i == len(document) - 1 or current_length >= target_seq_length:\n            if current_chunk:\n                # `a_end` is how many segments from `current_chunk` go into the `A`\n                # (first) sentence.\n                a_end = 1\n                if len(current_chunk) >= 2:\n                    a_end = rng.randint(1, len(current_chunk) - 1)\n\n                tokens_a = []\n                for j in range(a_end):\n                    tokens_a.extend(current_chunk[j])\n\n                tokens_b = []\n                # Random next\n                is_random_next = False\n                if len(current_chunk) == 1 or rng.random() < 0.5:\n                    is_random_next = True\n                    target_b_length = target_seq_length - len(tokens_a)\n\n                    # This should rarely go for more than one iteration for large\n                    # corpora. However, just to be careful, we try to make sure that\n                    # the random document is not the same as the document\n                    # we're processing.\n                    for _ in range(10):\n                        random_document_index = rng.randint(\n                            0,\n                            len(all_documents) - 1)\n                        if random_document_index != document_index:\n                            break\n\n                    random_document = all_documents[random_document_index]\n                    random_start = rng.randint(0, len(random_document) - 1)\n                    for j in range(random_start, len(random_document)):\n                        tokens_b.extend(random_document[j])\n                        if len(tokens_b) >= target_b_length:\n                            break\n                    # We didn't actually use these segments so we 'put them back' so\n                    # they don't go to waste.\n                    num_unused_segments = len(current_chunk) - a_end\n                    i -= num_unused_segments\n                # Actual next\n                else:\n                    is_random_next = False\n                    for j in range(a_end, len(current_chunk)):\n                        tokens_b.extend(current_chunk[j])\n                truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\n                assert len(tokens_a) >= 1\n                assert len(tokens_b) >= 1\n\n                tokens = []\n                segment_ids = []\n                tokens.append('[CLS]')\n                segment_ids.append(0)\n                for token in tokens_a:\n                    tokens.append(token)\n                    segment_ids.append(0)\n                tokens.append('[SEP]')\n                segment_ids.append(0)\n                segment_a_lengths = len(segment_ids)\n\n                for token in tokens_b:\n                    tokens.append(token)\n                    segment_ids.append(1)\n                tokens.append('[SEP]')\n                segment_ids.append(1)\n                segment_b_lengths = len(segment_ids) - segment_a_lengths\n\n                (tokens, masked_lm_positions,\n                 masked_lm_labels) = create_masked_lm_predictions(\n                     tokens, masked_lm_prob, max_predictions_per_seq,\n                     vocab_words, rng)\n                instance = TrainingInstance(\n                    tokens=tokens,\n                    segment_ids=segment_ids,\n                    is_random_next=is_random_next,\n                    masked_lm_positions=masked_lm_positions,\n                    masked_lm_labels=masked_lm_labels,\n                    segment_a_lengths=segment_a_lengths,\n                    segment_b_lengths=segment_b_lengths)\n                instances.append(instance)\n            current_chunk = []\n            current_length = 0\n        i += 1\n\n    return instances", "response": "Creates training instance for a single document."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_masked_lm_predictions(tokens, masked_lm_prob,\n                                 max_predictions_per_seq, vocab_words, rng):\n    \"\"\"Creates the predictions for the masked LM objective.\"\"\"\n\n    cand_indexes = []\n    for (i, token) in enumerate(tokens):\n        if token in ['[CLS]', '[SEP]']:\n            continue\n        cand_indexes.append(i)\n\n    rng.shuffle(cand_indexes)\n\n    output_tokens = list(tokens)\n\n    num_to_predict = min(max_predictions_per_seq,\n                         max(1, int(round(len(tokens) * masked_lm_prob))))\n\n    masked_lms = []\n    covered_indexes = set()\n    for index in cand_indexes:\n        if len(masked_lms) >= num_to_predict:\n            break\n        if index in covered_indexes:\n            continue\n        covered_indexes.add(index)\n\n        masked_token = None\n        # 80% of the time, replace with [MASK]\n        if rng.random() < 0.8:\n            masked_token = '[MASK]'\n        else:\n            # 10% of the time, keep original\n            if rng.random() < 0.5:\n                masked_token = tokens[index]\n            # 10% of the time, replace with random word\n            else:\n                masked_token = vocab_words[rng.randint(0,\n                                                       len(vocab_words) - 1)]\n\n        output_tokens[index] = masked_token\n\n        masked_lms.append(MaskedLmInstance(index=index, label=tokens[index]))\n\n    masked_lms = sorted(masked_lms, key=lambda x: x.index)\n\n    masked_lm_positions = []\n    masked_lm_labels = []\n    for p in masked_lms:\n        masked_lm_positions.append(p.index)\n        masked_lm_labels.append(p.label)\n\n    return (output_tokens, masked_lm_positions, masked_lm_labels)", "response": "Creates the predictions for the masked LM objective."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntruncating a pair of sequences to a maximum sequence length.", "response": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng):\n    \"\"\"Truncates a pair of sequences to a maximum sequence length.\"\"\"\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef convert_vocab(vocab_file):\n    original_vocab = load_vocab(vocab_file)\n    token_to_idx = dict(original_vocab)\n    num_tokens = len(token_to_idx)\n    idx_to_token = [None] * len(original_vocab)\n    for word in original_vocab:\n        idx = int(original_vocab[word])\n        idx_to_token[idx] = word\n\n    def swap(token, target_idx, token_to_idx, idx_to_token, swap_idx):\n        original_idx = token_to_idx[token]\n        original_token = idx_to_token[target_idx]\n        token_to_idx[token] = target_idx\n        token_to_idx[original_token] = original_idx\n        idx_to_token[target_idx] = token\n        idx_to_token[original_idx] = original_token\n        swap_idx.append((original_idx, target_idx))\n\n    reserved_tokens = [gluonnlp.vocab.BERTVocab.PADDING_TOKEN, gluonnlp.vocab.BERTVocab.CLS_TOKEN,\n                       gluonnlp.vocab.BERTVocab.SEP_TOKEN, gluonnlp.vocab.BERTVocab.MASK_TOKEN]\n\n    unknown_token = gluonnlp.vocab.BERTVocab.UNKNOWN_TOKEN\n    padding_token = gluonnlp.vocab.BERTVocab.PADDING_TOKEN\n    swap_idx = []\n    assert unknown_token in token_to_idx\n    assert padding_token in token_to_idx\n    swap(unknown_token, 0, token_to_idx, idx_to_token, swap_idx)\n    for i, token in enumerate(reserved_tokens):\n        swap(token, i + 1, token_to_idx, idx_to_token, swap_idx)\n\n    # sanity checks\n    assert len(token_to_idx) == num_tokens\n    assert len(idx_to_token) == num_tokens\n    assert None not in idx_to_token\n    assert len(set(idx_to_token)) == num_tokens\n\n    bert_vocab_dict = {}\n    bert_vocab_dict['idx_to_token'] = idx_to_token\n    bert_vocab_dict['token_to_idx'] = token_to_idx\n    bert_vocab_dict['reserved_tokens'] = reserved_tokens\n    bert_vocab_dict['unknown_token'] = unknown_token\n    bert_vocab_dict['padding_token'] = padding_token\n    bert_vocab_dict['bos_token'] = None\n    bert_vocab_dict['eos_token'] = None\n    bert_vocab_dict['mask_token'] = gluonnlp.vocab.BERTVocab.MASK_TOKEN\n    bert_vocab_dict['sep_token'] = gluonnlp.vocab.BERTVocab.SEP_TOKEN\n    bert_vocab_dict['cls_token'] = gluonnlp.vocab.BERTVocab.CLS_TOKEN\n    json_str = json.dumps(bert_vocab_dict)\n    converted_vocab = gluonnlp.vocab.BERTVocab.from_json(json_str)\n    return converted_vocab, swap_idx", "response": "GluonNLP specific code to convert the original vocabulary to nlp. vocab.BERTVocab."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef profile(curr_step, start_step, end_step, profile_name='profile.json',\n            early_exit=True):\n    \"\"\"profile the program between [start_step, end_step).\"\"\"\n    if curr_step == start_step:\n        mx.nd.waitall()\n        mx.profiler.set_config(profile_memory=False, profile_symbolic=True,\n                               profile_imperative=True, filename=profile_name,\n                               aggregate_stats=True)\n        mx.profiler.set_state('run')\n    elif curr_step == end_step:\n        mx.nd.waitall()\n        mx.profiler.set_state('stop')\n        logging.info(mx.profiler.dumps())\n        mx.profiler.dump()\n        if early_exit:\n            exit()", "response": "profile the program between start_step and end_step"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nloading a vocabulary file into a dictionary.", "response": "def load_vocab(vocab_file):\n    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n    vocab = collections.OrderedDict()\n    index = 0\n    with io.open(vocab_file, 'r') as reader:\n        while True:\n            token = reader.readline()\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef hybrid_forward(self, F, inputs, mask=None): # pylint: disable=arguments-differ\n        if mask is not None:\n            inputs = F.broadcast_mul(inputs, mask.expand_dims(-1))\n\n        inputs = F.transpose(inputs, axes=(1, 2, 0))\n\n        output = self._convs(inputs)\n\n        if self._highways:\n            output = self._highways(output)\n\n        if self._projection:\n            output = self._projection(output)\n\n        return output", "response": "r Forward computation for char_encoder\n           "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninitialize the sinusoid position encoding table", "response": "def _position_encoding_init(max_length, dim):\n    \"\"\"Init the sinusoid position encoding table \"\"\"\n    position_enc = np.arange(max_length).reshape((-1, 1)) \\\n                   / (np.power(10000, (2. / dim) * np.arange(dim).reshape((1, -1))))\n    # Apply the cosine to even columns and sin to odds.\n    position_enc[:, 0::2] = np.sin(position_enc[:, 0::2])  # dim 2i\n    position_enc[:, 1::2] = np.cos(position_enc[:, 1::2])  # dim 2i+1\n    return position_enc"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a pair of Parallel Transformer encoder and decoder.", "response": "def get_transformer_encoder_decoder(num_layers=2,\n                                    num_heads=8, scaled=True,\n                                    units=512, hidden_size=2048, dropout=0.0, use_residual=True,\n                                    max_src_length=50, max_tgt_length=50,\n                                    weight_initializer=None, bias_initializer='zeros',\n                                    prefix='transformer_', params=None):\n    \"\"\"Build a pair of Parallel Transformer encoder/decoder\n\n    Parameters\n    ----------\n    num_layers : int\n    num_heads : int\n    scaled : bool\n    units : int\n    hidden_size : int\n    dropout : float\n    use_residual : bool\n    max_src_length : int\n    max_tgt_length : int\n    weight_initializer : mx.init.Initializer or None\n    bias_initializer : mx.init.Initializer or None\n    prefix : str, default 'transformer_'\n        Prefix for name of `Block`s.\n    params : Parameter or None\n        Container for weight sharing between layers.\n        Created if `None`.\n\n    Returns\n    -------\n    encoder : TransformerEncoder\n    decoder :TransformerDecoder\n    \"\"\"\n    encoder = TransformerEncoder(num_layers=num_layers,\n                                 num_heads=num_heads,\n                                 max_length=max_src_length,\n                                 units=units,\n                                 hidden_size=hidden_size,\n                                 dropout=dropout,\n                                 scaled=scaled,\n                                 use_residual=use_residual,\n                                 weight_initializer=weight_initializer,\n                                 bias_initializer=bias_initializer,\n                                 prefix=prefix + 'enc_', params=params)\n    decoder = TransformerDecoder(num_layers=num_layers,\n                                 num_heads=num_heads,\n                                 max_length=max_tgt_length,\n                                 units=units,\n                                 hidden_size=hidden_size,\n                                 dropout=dropout,\n                                 scaled=scaled,\n                                 use_residual=use_residual,\n                                 weight_initializer=weight_initializer,\n                                 bias_initializer=bias_initializer,\n                                 prefix=prefix + 'dec_', params=params)\n    return encoder, decoder"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the activation block based on the name.", "response": "def _get_activation(self, act):\n        \"\"\"Get activation block based on the name. \"\"\"\n        if isinstance(act, str):\n            if act.lower() == 'gelu':\n                return GELU()\n            else:\n                return gluon.nn.Activation(act)\n        assert isinstance(act, gluon.Block)\n        return act"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npositions - wise encoding of the inputs.", "response": "def hybrid_forward(self, F, inputs):  # pylint: disable=arguments-differ\n        # pylint: disable=unused-argument\n        \"\"\"Position-wise encoding of the inputs.\n\n        Parameters\n        ----------\n        inputs : Symbol or NDArray\n            Input sequence. Shape (batch_size, length, C_in)\n\n        Returns\n        -------\n        outputs : Symbol or NDArray\n            Shape (batch_size, length, C_out)\n        \"\"\"\n        outputs = self.ffn_1(inputs)\n        if self.activation:\n            outputs = self.activation(outputs)\n        outputs = self.ffn_2(outputs)\n        if self._dropout:\n            outputs = self.dropout_layer(outputs)\n        if self._use_residual:\n            outputs = outputs + inputs\n        outputs = self.layer_norm(outputs)\n        return outputs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef hybrid_forward(self, F, inputs, mask=None):  # pylint: disable=arguments-differ\n        # pylint: disable=unused-argument\n        \"\"\"Transformer Encoder Attention Cell.\n\n        Parameters\n        ----------\n        inputs : Symbol or NDArray\n            Input sequence. Shape (batch_size, length, C_in)\n        mask : Symbol or NDArray or None\n            Mask for inputs. Shape (batch_size, length, length)\n\n        Returns\n        -------\n        encoder_cell_outputs: list\n            Outputs of the encoder cell. Contains:\n\n            - outputs of the transformer encoder cell. Shape (batch_size, length, C_out)\n            - additional_outputs of all the transformer encoder cell\n        \"\"\"\n        outputs, attention_weights =\\\n            self.attention_cell(inputs, inputs, inputs, mask)\n        outputs = self.proj(outputs)\n        if self._dropout:\n            outputs = self.dropout_layer(outputs)\n        if self._use_residual:\n            outputs = outputs + inputs\n        outputs = self.layer_norm(outputs)\n        outputs = self.ffn(outputs)\n        additional_outputs = []\n        if self._output_attention:\n            additional_outputs.append(attention_weights)\n        return outputs, additional_outputs", "response": "Transformer Encoder Attention Cell.\n\n        Parameters\n        ----------\n        inputs : Symbol or NDArray\n            Input sequence. Shape (batch_size, length, C_in)\n        mask : Symbol or NDArray or None\n            Mask for inputs. Shape (batch_size, length, length)\n\n        Returns\n        -------\n        encoder_cell_outputs: list\n            Outputs of the encoder cell. Contains:\n\n            - outputs of the transformer encoder cell. Shape (batch_size, length, C_out)\n            - additional_outputs of all the transformer encoder cell"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef init_state_from_encoder(self, encoder_outputs, encoder_valid_length=None):\n        mem_value = encoder_outputs\n        decoder_states = [mem_value]\n        mem_length = mem_value.shape[1]\n        if encoder_valid_length is not None:\n            dtype = encoder_valid_length.dtype\n            ctx = encoder_valid_length.context\n            mem_masks = mx.nd.broadcast_lesser(\n                mx.nd.arange(mem_length, ctx=ctx, dtype=dtype).reshape((1, -1)),\n                encoder_valid_length.reshape((-1, 1)))\n            decoder_states.append(mem_masks)\n        self._encoder_valid_length = encoder_valid_length\n        return decoder_states", "response": "Initialize the state from the encoder outputs."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef decode_seq(self, inputs, states, valid_length=None):\n        batch_size = inputs.shape[0]\n        length = inputs.shape[1]\n        length_array = mx.nd.arange(length, ctx=inputs.context, dtype=inputs.dtype)\n        mask = mx.nd.broadcast_lesser_equal(\n            length_array.reshape((1, -1)),\n            length_array.reshape((-1, 1)))\n        if valid_length is not None:\n            arange = mx.nd.arange(length, ctx=valid_length.context, dtype=valid_length.dtype)\n            batch_mask = mx.nd.broadcast_lesser(\n                arange.reshape((1, -1)),\n                valid_length.reshape((-1, 1)))\n            mask = mx.nd.broadcast_mul(mx.nd.expand_dims(batch_mask, -1),\n                                       mx.nd.expand_dims(mask, 0))\n        else:\n            mask = mx.nd.broadcast_axes(mx.nd.expand_dims(mask, axis=0), axis=0, size=batch_size)\n        states = [None] + states\n        output, states, additional_outputs = self.forward(inputs, states, mask)\n        states = states[1:]\n        if valid_length is not None:\n            output = mx.nd.SequenceMask(output,\n                                        sequence_length=valid_length,\n                                        use_sequence_length=True,\n                                        axis=1)\n        return output, states, additional_outputs", "response": "Decode the inputs. This function is only used for training."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef forward_backward(self, x):\n        (src_seq, tgt_seq, src_valid_length, tgt_valid_length), batch_size = x\n        with mx.autograd.record():\n            out, _ = self._model(src_seq, tgt_seq[:, :-1],\n                                 src_valid_length, tgt_valid_length - 1)\n            smoothed_label = self._label_smoothing(tgt_seq[:, 1:])\n            ls = self._loss(out, smoothed_label, tgt_valid_length - 1).sum()\n            ls = (ls * (tgt_seq.shape[1] - 1)) / batch_size / self._rescale_loss\n        ls.backward()\n        return ls", "response": "Perform forward and backward computation for a batch of src seq and dst seq"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse command line arguments.", "response": "def parse_args():\n    \"\"\"\n    Parse arguments.\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--gpu-id', type=int, default=0,\n                        help='GPU id (-1 means CPU)')\n    parser.add_argument('--train-file', default='snli_1.0/snli_1.0_train.txt',\n                        help='training set file')\n    parser.add_argument('--test-file', default='snli_1.0/snli_1.0_dev.txt',\n                        help='validation set file')\n    parser.add_argument('--max-num-examples', type=int, default=-1,\n                        help='maximum number of examples to load (for debugging)')\n    parser.add_argument('--batch-size', type=int, default=32,\n                        help='batch size')\n    parser.add_argument('--print-interval', type=int, default=20,\n                        help='the interval of two print')\n    parser.add_argument('--mode', choices=['train', 'test'], default='train',\n                        help='train or test')\n    parser.add_argument('--lr', type=float, default=0.025,\n                        help='learning rate')\n    parser.add_argument('--epochs', type=int, default=300,\n                        help='maximum number of epochs to train')\n    parser.add_argument('--embedding', default='glove',\n                        help='word embedding type')\n    parser.add_argument('--embedding-source', default='glove.840B.300d',\n                        help='embedding file source')\n    parser.add_argument('--embedding-size', type=int, default=300,\n                        help='size of pretrained word embedding')\n    parser.add_argument('--hidden-size', type=int, default=200,\n                        help='hidden layer size')\n    parser.add_argument('--output-dir', default='./output',\n                        help='directory for all experiment output')\n    parser.add_argument('--model-dir', default='./output',\n                        help='directory to load model')\n    parser.add_argument('--seed', type=int, default=0,\n                        help='random seed')\n    parser.add_argument('--dropout', type=float, default=0.,\n                        help='dropout rate')\n    parser.add_argument('--weight-decay', type=float, default=0.,\n                        help='l2 regularization weight')\n    parser.add_argument('--intra-attention', action='store_true',\n                        help='use intra-sentence attention')\n\n    return parser.parse_args()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef train_model(model, train_data_loader, val_data_loader, embedding, ctx, args):\n    logger.info(vars(args))\n\n    # Initialization\n    model.hybridize()\n    model.collect_params().initialize(mx.init.Normal(0.01), ctx=ctx)\n    model.word_emb.weight.set_data(embedding.idx_to_vec)\n    # Fix word embedding\n    model.word_emb.weight.grad_req = 'null'\n\n    loss_func = gluon.loss.SoftmaxCrossEntropyLoss()\n    trainer = gluon.Trainer(model.collect_params(), 'adagrad',\n                            {'learning_rate': args.lr,\n                             'wd': args.weight_decay,\n                             'clip_gradient': 5})\n\n    checkpoints_dir = os.path.join(args.output_dir, 'checkpoints')\n    if not os.path.exists(checkpoints_dir):\n        os.makedirs(checkpoints_dir)\n\n    best_val_acc = 0.\n    for epoch_id in range(args.epochs):\n        avg_loss = 0.\n        avg_acc = 0.\n        for batch_id, example in enumerate(train_data_loader):\n            s1, s2, label = example\n            s1 = s1.as_in_context(ctx)\n            s2 = s2.as_in_context(ctx)\n            label = label.as_in_context(ctx)\n\n            with autograd.record():\n                output = model(s1, s2)\n                loss = loss_func(output, label).mean()\n            loss.backward()\n            trainer.step(1)\n            avg_loss += loss.sum().asscalar()\n\n            pred = output.argmax(axis=1)\n            acc = (pred == label.astype(np.float32)).mean()\n            avg_acc += acc.asscalar()\n\n            if (batch_id + 1) % args.print_interval == 0:\n                avg_loss /= args.print_interval\n                avg_acc /= args.print_interval\n                logger.info('[Epoch {} Batch {}/{}] loss={:.4f}, acc={:.4f}'\n                            .format(epoch_id, batch_id + 1, len(train_data_loader),\n                                    avg_loss, avg_acc))\n                avg_loss = 0.\n                avg_acc = 0.\n\n        # Validation\n        val_loss, val_acc = test_model(model, val_data_loader, loss_func, ctx)\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            checkpoint_path = os.path.join(args.output_dir, 'checkpoints', 'valid_best.params')\n            model.save_parameters(checkpoint_path)\n        logger.info('[Epoch {}] valid loss={:.4f}, valid acc={:.4f}, best valid acc={:.4f}'\n                    .format(epoch_id, val_loss, val_acc, best_val_acc))\n\n        # Save checkpoint of last epoch\n        checkpoint_path = os.path.join(args.output_dir, 'checkpoints', 'last.params')\n        model.save_parameters(checkpoint_path)", "response": "Train a model on the data and validate every epoch."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef hybrid_forward(self, F, candidates_like, prob, alias):\n        # pylint: disable=unused-argument\n        \"\"\"Draw samples from uniform distribution and return sampled candidates.\n\n        Parameters\n        ----------\n        candidates_like: mxnet.nd.NDArray or mxnet.sym.Symbol\n            This input specifies the shape of the to be sampled candidates. #\n            TODO shape selection is not yet supported. Shape must be specified\n            in the constructor.\n\n        Returns\n        -------\n        samples: mxnet.nd.NDArray or mxnet.sym.Symbol\n            The sampled candidates of shape candidates_like.shape. Candidates\n            are sampled based on the weights specified on creation of the\n            UnigramCandidateSampler.\n        \"\"\"\n        flat_shape = functools.reduce(operator.mul, self._shape)\n        idx = F.random.uniform(low=0, high=self.N, shape=flat_shape,\n                               dtype='float64').floor()\n        prob = F.gather_nd(prob, idx.reshape((1, -1)))\n        alias = F.gather_nd(alias, idx.reshape((1, -1)))\n        where = F.random.uniform(shape=flat_shape,\n                                 dtype='float64') < prob\n        hit = idx * where\n        alt = alias * (1 - where)\n        candidates = (hit + alt).reshape(self._shape)\n\n        return candidates.astype(self._dtype)", "response": "Draw samples from uniform distribution and return sampled candidates."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add(\n        self,\n        sink,\n        *,\n        level=_defaults.LOGURU_LEVEL,\n        format=_defaults.LOGURU_FORMAT,\n        filter=_defaults.LOGURU_FILTER,\n        colorize=_defaults.LOGURU_COLORIZE,\n        serialize=_defaults.LOGURU_SERIALIZE,\n        backtrace=_defaults.LOGURU_BACKTRACE,\n        diagnose=_defaults.LOGURU_DIAGNOSE,\n        enqueue=_defaults.LOGURU_ENQUEUE,\n        catch=_defaults.LOGURU_CATCH,\n        **kwargs\n    ):\n        r\"\"\"Add a handler sending log messages to a sink adequately configured.\n\n        Parameters\n        ----------\n        sink : |file-like object|_, |str|, |Path|, |function|_, |Handler| or |class|_\n            An object in charge of receiving formatted logging messages and propagating them to an\n            appropriate endpoint.\n        level : |int| or |str|, optional\n            The minimum severity level from which logged messages should be sent to the sink.\n        format : |str| or |function|_, optional\n            The template used to format logged messages before being sent to the sink.\n        filter : |function|_ or |str|, optional\n            A directive used to optionally filter out logged messages before they are sent to the\n            sink.\n        colorize : |bool|, optional\n            Whether or not the color markups contained in the formatted message should be converted\n            to ansi codes for terminal coloration, or stripped otherwise. If ``None``, the choice\n            is automatically made based on the sink being a tty or not.\n        serialize : |bool|, optional\n            Whether or not the logged message and its records should be first converted to a JSON\n            string before being sent to the sink.\n        backtrace : |bool|, optional\n            Whether or not the exception trace formatted should be extended upward, beyond the\n            catching point, to show the full stacktrace which generated the error.\n        diagnose : |bool|, optional\n            Whether or not the exception trace should display the variables values to eases the\n            debugging. This should be set to ``False`` in production to avoid leaking sensitive\n            data.\n        enqueue : |bool|, optional\n            Whether or not the messages to be logged should first pass through a multiprocess-safe\n            queue before reaching the sink. This is useful while logging to a file through multiple\n            processes.\n        catch : |bool|, optional\n            Whether or not errors occurring while sink handles logs messages should be caught or\n            not. If ``True``, an exception message is displayed on |sys.stderr| but the exception is\n            not propagated to the caller, preventing your app to crash.\n        **kwargs\n            Additional parameters that will be passed to the sink while creating it or while\n            logging messages (the exact behavior depends on the sink type).\n\n\n        If and only if the sink is a file, the following parameters apply:\n\n        Parameters\n        ----------\n        rotation : |str|, |int|, |time|, |timedelta| or |function|_, optional\n            A condition indicating whenever the current logged file should be closed and a new one\n            started.\n        retention : |str|, |int|, |timedelta| or |function|_, optional\n            A directive filtering old files that should be removed during rotation or end of\n            program.\n        compression : |str| or |function|_, optional\n            A compression or archive format to which log files should be converted at closure.\n        delay : |bool|, optional\n            Whether or not the file should be created as soon as the sink is configured, or delayed\n            until first logged message. It defaults to ``False``.\n        mode : |str|, optional\n            The opening mode as for built-in |open| function. It defaults to ``\"a\"`` (open the\n            file in appending mode).\n        buffering : |int|, optional\n            The buffering policy as for built-in |open| function. It defaults to ``1`` (line\n            buffered file).\n        encoding : |str|, optional\n            The file encoding as for built-in |open| function. If ``None``, it defaults to\n            ``locale.getpreferredencoding()``.\n        **kwargs\n            Others parameters are passed to the built-in |open| function.\n\n        Returns\n        -------\n        :class:`int`\n            An identifier associated with the added sink and which should be used to\n            |remove| it.\n\n        Notes\n        -----\n        Extended summary follows.\n\n        .. _sink:\n\n        .. rubric:: The sink parameter\n\n        The ``sink`` handles incoming log messages and proceed to their writing somewhere and\n        somehow. A sink can take many forms:\n\n        - A |file-like object|_ like ``sys.stderr`` or ``open(\"somefile.log\", \"w\")``. Anything with\n          a ``.write()`` method is considered as a file-like object. If it has a ``.flush()``\n          method, it will be automatically called after each logged message. If it has a ``.stop()``\n          method, it will be automatically called at sink termination.\n        - A file path as |str| or |Path|. It can be parametrized with some additional parameters,\n          see below.\n        - A simple |function|_ like ``lambda msg: print(msg)``. This allows for logging\n          procedure entirely defined by user preferences and needs.\n        - A built-in |Handler| like ``logging.StreamHandler``. In such a case, the `Loguru` records\n          are automatically converted to the structure expected by the |logging| module.\n        - A |class|_ object that will be used to instantiate the sink using ``**kwargs`` attributes\n          passed. Hence the class should instantiate objects which are therefore valid sinks.\n\n        Note that you should avoid using  the ``logger`` inside any of your sinks as this would\n        result in infinite recursion or dead lock if the module's sink was not explicitly disabled.\n\n        .. _message:\n\n        .. rubric:: The logged message\n\n        The logged message passed to all added sinks is nothing more than a string of the\n        formatted log, to which a special attribute is associated: the ``.record`` which is a dict\n        containing all contextual information possibly needed (see below).\n\n        Logged messages are formatted according to the ``format`` of the added sink. This format\n        is usually a string containing braces fields to display attributes from the record dict.\n\n        If fine-grained control is needed, the ``format`` can also be a function which takes the\n        record as parameter and return the format template string. However, note that in such a\n        case, you should take care of appending the line ending and exception field to the returned\n        format, while ``\"\\n{exception}\"`` is automatically appended for convenience if ``format`` is\n        a string.\n\n        The ``filter`` attribute can be used to control which messages are effectively passed to the\n        sink and which one are ignored. A function can be used, accepting the record as an\n        argument, and returning ``True`` if the message should be logged, ``False`` otherwise. If\n        a string is used, only the records with the same ``name`` and its children will be allowed.\n\n        .. _levels:\n\n        .. rubric:: The severity levels\n\n        Each logged message is associated with a severity level. These levels make it possible to\n        prioritize messages and to choose the verbosity of the logs according to usages. For\n        example, it allows to display some debugging information to a developer, while hiding it to\n        the end user running the application.\n\n        The ``level`` attribute of every added sink controls the minimum threshold from which log\n        messages are allowed to be emitted. While using the ``logger``, you are in charge of\n        configuring the appropriate granularity of your logs. It is possible to add even more custom\n        levels by using the |level| method.\n\n        Here are the standard levels with their default severity value, each one is associated with\n        a logging method of the same name:\n\n        +----------------------+------------------------+------------------------+\n        | Level name           | Severity value         | Logger method          |\n        +======================+========================+========================+\n        | ``TRACE``            | 5                      | |logger.trace|         |\n        +----------------------+------------------------+------------------------+\n        | ``DEBUG``            | 10                     | |logger.debug|         |\n        +----------------------+------------------------+------------------------+\n        | ``INFO``             | 20                     | |logger.info|          |\n        +----------------------+------------------------+------------------------+\n        | ``SUCCESS``          | 25                     | |logger.success|       |\n        +----------------------+------------------------+------------------------+\n        | ``WARNING``          | 30                     | |logger.warning|       |\n        +----------------------+------------------------+------------------------+\n        | ``ERROR``            | 40                     | |logger.error|         |\n        +----------------------+------------------------+------------------------+\n        | ``CRITICAL``         | 50                     | |logger.critical|      |\n        +----------------------+------------------------+------------------------+\n\n        .. _record:\n\n        .. rubric:: The record dict\n\n        The record is just a Python dict, accessible from sinks by ``message.record``. It contains\n        all contextual information of the logging call (time, function, file, line, level, etc.).\n        Each of its key can be used in the handler's ``format`` so the corresponding value is\n        properly displayed in the logged message (eg. ``\"{level}\"`` -> ``\"INFO\"``). Some record's\n        values are objects with two or more attributes, those can be formatted with ``\"{key.attr}\"``\n        (``\"{key}\"`` would display one by default). Formatting directives like ``\"{key: >3}\"`` also\n        works and is particularly useful for time (see below).\n\n        +------------+---------------------------------+----------------------------+\n        | Key        | Description                     | Attributes                 |\n        +============+=================================+============================+\n        | elapsed    | The time elapsed since the      | See |timedelta|            |\n        |            | start of the program            |                            |\n        +------------+---------------------------------+----------------------------+\n        | exception  | The formatted exception if any, | ``type``, ``value``,       |\n        |            | ``None`` otherwise              | ``traceback``              |\n        +------------+---------------------------------+----------------------------+\n        | extra      | The dict of attributes          | None                       |\n        |            | bound by the user (see |bind|)  |                            |\n        +------------+---------------------------------+----------------------------+\n        | file       | The file where the logging call | ``name`` (default),        |\n        |            | was made                        | ``path``                   |\n        +------------+---------------------------------+----------------------------+\n        | function   | The function from which the     | None                       |\n        |            | logging call was made           |                            |\n        +------------+---------------------------------+----------------------------+\n        | level      | The severity used to log the    | ``name`` (default),        |\n        |            | the message                     | ``no``, ``icon``           |\n        +------------+---------------------------------+----------------------------+\n        | line       | The line number in the source   | None                       |\n        |            | code                            |                            |\n        +------------+---------------------------------+----------------------------+\n        | message    | The logged message (not yet     | None                       |\n        |            | formatted)                      |                            |\n        +------------+---------------------------------+----------------------------+\n        | module     | The module where the logging    | None                       |\n        |            | call was made                   |                            |\n        +------------+---------------------------------+----------------------------+\n        | name       | The ``__name__`` where the      | None                       |\n        |            | logging call was made           |                            |\n        +------------+---------------------------------+----------------------------+\n        | process    | The process in which the        | ``name``, ``id`` (default) |\n        |            | logging call was made           |                            |\n        +------------+---------------------------------+----------------------------+\n        | thread     | The thread in which the         | ``name``, ``id`` (default) |\n        |            | logging call was made           |                            |\n        +------------+---------------------------------+----------------------------+\n        | time       | The aware local time when the   | See |datetime|             |\n        |            | logging call was made           |                            |\n        +------------+---------------------------------+----------------------------+\n\n        .. _time:\n\n        .. rubric:: The time formatting\n\n        To use your favorite time representation, you can set it directly in the time formatter\n        specifier of your handler format, like for example ``format=\"{time:HH:mm:ss} {message}\"``.\n        Note that this datetime represents your local time, and it is also made timezone-aware,\n        so you can display the UTC offset to avoid ambiguities.\n\n        The time field can be formatted using more human-friendly tokens. Those constitute a subset\n        of the one used by the `Pendulum`_ library of `@sdispater`_. To escape a token, just add\n        square brackets around it, for example ``\"[YY]\"`` would display literally ``\"YY\"``.\n\n        If no time formatter specifier is used, like for example if ``format=\"{time} {message}\"``,\n        the default one will use ISO 8601.\n\n        +------------------------+---------+----------------------------------------+\n        |                        | Token   | Output                                 |\n        +========================+=========+========================================+\n        | Year                   | YYYY    | 2000, 2001, 2002 ... 2012, 2013        |\n        |                        +---------+----------------------------------------+\n        |                        | YY      | 00, 01, 02 ... 12, 13                  |\n        +------------------------+---------+----------------------------------------+\n        | Quarter                | Q       | 1 2 3 4                                |\n        +------------------------+---------+----------------------------------------+\n        | Month                  | MMMM    | January, February, March ...           |\n        |                        +---------+----------------------------------------+\n        |                        | MMM     | Jan, Feb, Mar ...                      |\n        |                        +---------+----------------------------------------+\n        |                        | MM      | 01, 02, 03 ... 11, 12                  |\n        |                        +---------+----------------------------------------+\n        |                        | M       | 1, 2, 3 ... 11, 12                     |\n        +------------------------+---------+----------------------------------------+\n        | Day of Year            | DDDD    | 001, 002, 003 ... 364, 365             |\n        |                        +---------+----------------------------------------+\n        |                        | DDD     | 1, 2, 3 ... 364, 365                   |\n        +------------------------+---------+----------------------------------------+\n        | Day of Month           | DD      | 01, 02, 03 ... 30, 31                  |\n        |                        +---------+----------------------------------------+\n        |                        | D       | 1, 2, 3 ... 30, 31                     |\n        +------------------------+---------+----------------------------------------+\n        | Day of Week            | dddd    | Monday, Tuesday, Wednesday ...         |\n        |                        +---------+----------------------------------------+\n        |                        | ddd     | Mon, Tue, Wed ...                      |\n        |                        +---------+----------------------------------------+\n        |                        | d       | 0, 1, 2 ... 6                          |\n        +------------------------+---------+----------------------------------------+\n        | Days of ISO Week       | E       | 1, 2, 3 ... 7                          |\n        +------------------------+---------+----------------------------------------+\n        | Hour                   | HH      | 00, 01, 02 ... 23, 24                  |\n        |                        +---------+----------------------------------------+\n        |                        | H       | 0, 1, 2 ... 23, 24                     |\n        |                        +---------+----------------------------------------+\n        |                        | hh      | 01, 02, 03 ... 11, 12                  |\n        |                        +---------+----------------------------------------+\n        |                        | h       | 1, 2, 3 ... 11, 12                     |\n        +------------------------+---------+----------------------------------------+\n        | Minute                 | mm      | 00, 01, 02 ... 58, 59                  |\n        |                        +---------+----------------------------------------+\n        |                        | m       | 0, 1, 2 ... 58, 59                     |\n        +------------------------+---------+----------------------------------------+\n        | Second                 | ss      | 00, 01, 02 ... 58, 59                  |\n        |                        +---------+----------------------------------------+\n        |                        | s       | 0, 1, 2 ... 58, 59                     |\n        +------------------------+---------+----------------------------------------+\n        | Fractional Second      | S       | 0 1 ... 8 9                            |\n        |                        +---------+----------------------------------------+\n        |                        | SS      | 00, 01, 02 ... 98, 99                  |\n        |                        +---------+----------------------------------------+\n        |                        | SSS     | 000 001 ... 998 999                    |\n        |                        +---------+----------------------------------------+\n        |                        | SSSS... | 000[0..] 001[0..] ... 998[0..] 999[0..]|\n        |                        +---------+----------------------------------------+\n        |                        | SSSSSS  | 000000 000001 ... 999998 999999        |\n        +------------------------+---------+----------------------------------------+\n        | AM / PM                | A       | AM, PM                                 |\n        +------------------------+---------+----------------------------------------+\n        | Timezone               | Z       | -07:00, -06:00 ... +06:00, +07:00      |\n        |                        +---------+----------------------------------------+\n        |                        | ZZ      | -0700, -0600 ... +0600, +0700          |\n        |                        +---------+----------------------------------------+\n        |                        | zz      | EST CST ... MST PST                    |\n        +------------------------+---------+----------------------------------------+\n        | Seconds timestamp      | X       | 1381685817, 1234567890.123             |\n        +------------------------+---------+----------------------------------------+\n        | Microseconds timestamp | x       | 1234567890123                          |\n        +------------------------+---------+----------------------------------------+\n\n        .. _file:\n\n        .. rubric:: The file sinks\n\n        If the sink is a |str| or a |Path|, the corresponding file will be opened for writing logs.\n        The path can also contain a special ``\"{time}\"`` field that will be formatted with the\n        current date at file creation.\n\n        The ``rotation`` check is made before logging each message. If there is already an existing\n        file with the same name that the file to be created, then the existing file is renamed by\n        appending the date to its basename to prevent file overwriting. This parameter accepts:\n\n        - an |int| which corresponds to the maximum file size in bytes before that the current\n          logged file is closed and a new one started over.\n        - a |timedelta| which indicates the frequency of each new rotation.\n        - a |time| which specifies the hour when the daily rotation should occur.\n        - a |str| for human-friendly parametrization of one of the previously enumerated types.\n          Examples: ``\"100 MB\"``, ``\"0.5 GB\"``, ``\"1 month 2 weeks\"``, ``\"4 days\"``, ``\"10h\"``,\n          ``\"monthly\"``, ``\"18:00\"``, ``\"sunday\"``, ``\"w0\"``, ``\"monday at 12:00\"``, ...\n        - a |function|_ which will be called before logging. It should accept two\n          arguments: the logged message and the file object, and it should return ``True`` if\n          the rotation should happen now, ``False`` otherwise.\n\n        The ``retention`` occurs at rotation or at sink stop if rotation is ``None``. Files are\n        selected according to their basename, if it is the same that the sink file, with possible\n        time field being replaced with ``.*``. This parameter accepts:\n\n        - an |int| which indicates the number of log files to keep, while older files are removed.\n        - a |timedelta| which specifies the maximum age of files to keep.\n        - a |str| for human-friendly parametrization of the maximum age of files to keep.\n          Examples: ``\"1 week, 3 days\"``, ``\"2 months\"``, ...\n        - a |function|_ which will be called before the retention process. It should accept the list\n          of log files as argument and process to whatever it wants (moving files, removing them,\n          etc.).\n\n        The ``compression`` happens at rotation or at sink stop if rotation is ``None``. This\n        parameter accepts:\n\n        - a |str| which corresponds to the compressed or archived file extension. This can be one\n          of: ``\"gz\"``, ``\"bz2\"``, ``\"xz\"``, ``\"lzma\"``, ``\"tar\"``, ``\"tar.gz\"``, ``\"tar.bz2\"``,\n          ``\"tar.xz\"``, ``\"zip\"``.\n        - a |function|_ which will be called before file termination. It should accept the path\n          of the log file as argument and process to whatever it wants (custom compression,\n          network sending, removing it, etc.).\n\n        .. _color:\n\n        .. rubric:: The color markups\n\n        To add colors to your logs, you just have to enclose your format string with the appropriate\n        tags. This is based on the great `ansimarkup`_ library from `@gvalkov`_. Those tags are\n        removed if the sink don't support ansi codes.\n\n        The special tag ``<level>`` (abbreviated with ``<lvl>``) is transformed according to\n        the configured color of the logged message level.\n\n        Here are the available tags (note that compatibility may vary depending on terminal):\n\n        +------------------------------------+--------------------------------------+\n        | Color (abbr)                       | Styles (abbr)                        |\n        +====================================+======================================+\n        | Black (k)                          | Bold (b)                             |\n        +------------------------------------+--------------------------------------+\n        | Blue (e)                           | Dim (d)                              |\n        +------------------------------------+--------------------------------------+\n        | Cyan (c)                           | Normal (n)                           |\n        +------------------------------------+--------------------------------------+\n        | Green (g)                          | Italic (i)                           |\n        +------------------------------------+--------------------------------------+\n        | Magenta (m)                        | Underline (u)                        |\n        +------------------------------------+--------------------------------------+\n        | Red (r)                            | Strike (s)                           |\n        +------------------------------------+--------------------------------------+\n        | White (w)                          | Reverse (r)                          |\n        +------------------------------------+--------------------------------------+\n        | Yellow (y)                         | Blink (l)                            |\n        +------------------------------------+--------------------------------------+\n        |                                    | Hide (h)                             |\n        +------------------------------------+--------------------------------------+\n\n        Usage:\n\n        +-----------------+-------------------------------------------------------------------+\n        | Description     | Examples                                                          |\n        |                 +---------------------------------+---------------------------------+\n        |                 | Foreground                      | Background                      |\n        +=================+=================================+=================================+\n        | Basic colors    | ``<red>``, ``<r>``              | ``<GREEN>``, ``<G>``            |\n        +-----------------+---------------------------------+---------------------------------+\n        | Light colors    | ``<light-blue>``, ``<le>``      | ``<LIGHT-CYAN>``, ``<LC>``      |\n        +-----------------+---------------------------------+---------------------------------+\n        | Xterm colors    | ``<fg 86>``, ``<fg 255>``       | ``<bg 42>``, ``<bg 9>``         |\n        +-----------------+---------------------------------+---------------------------------+\n        | Hex colors      | ``<fg #00005f>``, ``<fg #EE1>`` | ``<bg #AF5FD7>``, ``<bg #fff>`` |\n        +-----------------+---------------------------------+---------------------------------+\n        | RGB colors      | ``<fg 0,95,0>``                 | ``<bg 72,119,65>``              |\n        +-----------------+---------------------------------+---------------------------------+\n        | Stylizing       | ``<bold>``, ``<b>`` , ``<underline>``, ``<u>``                    |\n        +-----------------+-------------------------------------------------------------------+\n        | Shorthand       | ``<red, yellow>``, ``<r, y>``                                     |\n        | (FG, BG)        |                                                                   |\n        +-----------------+-------------------------------------------------------------------+\n        | Shorthand       | ``<bold, cyan, white>``, ``<b,,w>``, ``<b,c,>``                   |\n        | (Style, FG, BG) |                                                                   |\n        +-----------------+-------------------------------------------------------------------+\n\n        .. _env:\n\n        .. rubric:: The environment variables\n\n        The default values of sink parameters can be entirely customized. This is particularly\n        useful if you don't like the log format of the pre-configured sink.\n\n        Each of the |add| default parameter can be modified by setting the ``LOGURU_[PARAM]``\n        environment variable. For example on Linux: ``export LOGURU_FORMAT=\"{time} - {message}\"``\n        or ``export LOGURU_DIAGNOSE=NO``.\n\n        The default levels' attributes can also be modified by setting the ``LOGURU_[LEVEL]_[ATTR]``\n        environment variable. For example, on Windows: ``setx LOGURU_DEBUG_COLOR \"<blue>\"``\n        or ``setx LOGURU_TRACE_ICON \"\ud83d\ude80\"``.\n\n        If you want to disable the pre-configured sink, you can set the ``LOGURU_AUTOINIT``\n        variable to ``False``.\n\n        On Linux, you will probably need to edit the ``~/.profile`` file to make this persistent. On\n        Windows, don't forget to restart your terminal for the change to be taken into account.\n\n        Examples\n        --------\n        >>> logger.add(sys.stdout, format=\"{time} - {level} - {message}\", filter=\"sub.module\")\n\n        >>> logger.add(\"file_{time}.log\", level=\"TRACE\", rotation=\"100 MB\")\n\n        >>> def my_sink(message):\n        ...     record = message.record\n        ...     update_db(message, time=record.time, level=record.level)\n        ...\n        >>> logger.add(my_sink)\n\n        >>> from logging import StreamHandler\n        >>> logger.add(StreamHandler(sys.stderr), format=\"{message}\")\n\n        >>> class RandomStream:\n        ...     def __init__(self, seed, threshold):\n        ...         self.threshold = threshold\n        ...         random.seed(seed)\n        ...     def write(self, message):\n        ...         if random.random() > self.threshold:\n        ...             print(message)\n        ...\n        >>> stream_object = RandomStream(seed=12345, threhold=0.25)\n        >>> logger.add(stream_object, level=\"INFO\")\n        >>> logger.add(RandomStream, level=\"DEBUG\", seed=34567, threshold=0.5)\n        \"\"\"\n        if colorize is None and serialize:\n            colorize = False\n\n        if isclass(sink):\n            sink = sink(**kwargs)\n            return self.add(\n                sink,\n                level=level,\n                format=format,\n                filter=filter,\n                colorize=colorize,\n                serialize=serialize,\n                backtrace=backtrace,\n                diagnose=diagnose,\n                enqueue=enqueue,\n                catch=catch,\n            )\n        elif isinstance(sink, (str, PathLike)):\n            path = sink\n            sink = FileSink(path, **kwargs)\n            return self.add(\n                sink,\n                level=level,\n                format=format,\n                filter=filter,\n                colorize=colorize,\n                serialize=serialize,\n                backtrace=backtrace,\n                diagnose=diagnose,\n                enqueue=enqueue,\n                catch=catch,\n            )\n        elif hasattr(sink, \"write\") and callable(sink.write):\n            if colorize is False:\n                stream = sink\n            else:\n                try:\n                    converter = AnsiToWin32(sink, convert=None, strip=False)\n                    isatty = converter.stream.isatty()\n                except Exception:\n                    if colorize is None:\n                        colorize = False\n                    stream = sink\n                else:\n                    if colorize is None:\n                        colorize = isatty\n                    if converter.should_wrap() and colorize:\n                        stream = converter.stream\n                    else:\n                        stream = sink\n\n            stream_write = stream.write\n            if kwargs:\n\n                def write(m):\n                    return stream_write(m, **kwargs)\n\n            else:\n                write = stream_write\n\n            if hasattr(stream, \"flush\") and callable(stream.flush):\n                stream_flush = stream.flush\n\n                def writer(m):\n                    write(m)\n                    stream_flush()\n\n            else:\n                writer = write\n\n            if hasattr(stream, \"stop\") and callable(stream.stop):\n                stopper = stream.stop\n            else:\n\n                def stopper():\n                    return None\n\n        elif isinstance(sink, logging.Handler):\n\n            def writer(m):\n                message = str(m)\n                r = m.record\n                exc = r[\"exception\"]\n                if not is_formatter_dynamic:\n                    message = message[:-1]\n                record = logging.root.makeRecord(\n                    r[\"name\"],\n                    r[\"level\"].no,\n                    r[\"file\"].path,\n                    r[\"line\"],\n                    message,\n                    (),\n                    (exc.type, exc.value, exc.traceback) if exc else None,\n                    r[\"function\"],\n                    r[\"extra\"],\n                    **kwargs\n                )\n                if exc:\n                    record.exc_text = \"\\n\"\n                sink.handle(record)\n\n            stopper = sink.close\n            if colorize is None:\n                colorize = False\n        elif callable(sink):\n            if kwargs:\n\n                def writer(m):\n                    return sink(m, **kwargs)\n\n            else:\n                writer = sink\n\n            def stopper():\n                return None\n\n            if colorize is None:\n                colorize = False\n        else:\n            raise ValueError(\"Cannot log to objects of type '%s'.\" % type(sink).__name__)\n\n        if filter is None or filter == \"\":\n            filter_func = None\n        elif isinstance(filter, str):\n            parent = filter + \".\"\n            length = len(parent)\n\n            def filter_func(r):\n                return (r[\"name\"] + \".\")[:length] == parent\n\n        elif callable(filter):\n            filter_func = filter\n        else:\n            raise ValueError(\n                \"Invalid filter, it should be a function or a string, not: '%s'\"\n                % type(filter).__name__\n            )\n\n        if isinstance(level, str):\n            levelno = self.level(level).no\n        elif isinstance(level, int):\n            levelno = level\n        else:\n            raise ValueError(\n                \"Invalid level, it should be an integer or a string, not: '%s'\"\n                % type(level).__name__\n            )\n\n        if levelno < 0:\n            raise ValueError(\n                \"Invalid level value, it should be a positive integer, not: %d\" % levelno\n            )\n\n        if isinstance(format, str):\n            formatter = format + \"\\n{exception}\"\n            is_formatter_dynamic = False\n        elif callable(format):\n            formatter = format\n            is_formatter_dynamic = True\n        else:\n            raise ValueError(\n                \"Invalid format, it should be a string or a function, not: '%s'\"\n                % type(format).__name__\n            )\n\n        try:\n            encoding = sink.encoding\n        except AttributeError:\n            encoding = None\n\n        if not encoding:\n            encoding = \"ascii\"\n\n        with self._lock:\n            handler_id = next(self._handlers_count)\n            colors = [lvl.color for lvl in self._levels.values()] + [\"\"]\n\n            exception_formatter = ExceptionFormatter(\n                colorize=colorize,\n                encoding=encoding,\n                diagnose=diagnose,\n                backtrace=backtrace,\n                hidden_frames_filename=self.catch.__code__.co_filename,\n            )\n\n            handler = Handler(\n                writer=writer,\n                stopper=stopper,\n                levelno=levelno,\n                formatter=formatter,\n                is_formatter_dynamic=is_formatter_dynamic,\n                filter_=filter_func,\n                colorize=colorize,\n                serialize=serialize,\n                catch=catch,\n                enqueue=enqueue,\n                id_=handler_id,\n                exception_formatter=exception_formatter,\n                colors=colors,\n            )\n\n            handlers = self._handlers.copy()\n            handlers[handler_id] = handler\n\n            self.__class__._min_level = min(self.__class__._min_level, levelno)\n            self.__class__._handlers = handlers\n\n        return handler_id", "response": "r Add a handler sending log messages to an anonymized endpoint."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef remove(self, handler_id=None):\n        with self._lock:\n            handlers = self._handlers.copy()\n\n            if handler_id is None:\n                for handler in handlers.values():\n                    handler.stop()\n                handlers.clear()\n            else:\n                try:\n                    handler = handlers.pop(handler_id)\n                except KeyError:\n                    raise ValueError(\"There is no existing handler with id '%s'\" % handler_id)\n                handler.stop()\n\n            levelnos = (h.levelno for h in handlers.values())\n            self.__class__._min_level = min(levelnos, default=float(\"inf\"))\n            self.__class__._handlers = handlers", "response": "Remove a previously added handler and stop sending logs to its sink."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef catch(\n        self,\n        exception=Exception,\n        *,\n        level=\"ERROR\",\n        reraise=False,\n        message=\"An error has been caught in function '{record[function]}', \"\n        \"process '{record[process].name}' ({record[process].id}), \"\n        \"thread '{record[thread].name}' ({record[thread].id}):\"\n    ):\n        \"\"\"Return a decorator to automatically log possibly caught error in wrapped function.\n\n        This is useful to ensure unexpected exceptions are logged, the entire program can be\n        wrapped by this method. This is also very useful to decorate |Thread.run| methods while\n        using threads to propagate errors to the main logger thread.\n\n        Note that the visibility of variables values (which uses the cool `better_exceptions`_\n        library from `@Qix-`_) depends on the ``diagnose`` option of each configured sink.\n\n        The returned object can also be used as a context manager.\n\n        Parameters\n        ----------\n        exception : |Exception|, optional\n            The type of exception to intercept. If several types should be caught, a tuple of\n            exceptions can be used too.\n        level : |str| or |int|, optional\n            The level name or severity with which the message should be logged.\n        reraise : |bool|, optional\n            Whether or not the exception should be raised again and hence propagated to the caller.\n        message : |str|, optional\n            The message that will be automatically logged if an exception occurs. Note that it will\n            be formatted with the ``record`` attribute.\n\n        Returns\n        -------\n        decorator / context manager\n            An object that can be used to decorate a function or as a context manager to log\n            exceptions possibly caught.\n\n        Examples\n        --------\n        >>> @logger.catch\n        ... def f(x):\n        ...     100 / x\n        ...\n        >>> def g():\n        ...     f(10)\n        ...     f(0)\n        ...\n        >>> g()\n        ERROR - An error has been caught in function 'g', process 'Main' (367), thread 'ch1' (1398):\n        Traceback (most recent call last):\n          File \"program.py\", line 12, in <module>\n            g()\n            \u2514 <function g at 0x7f225fe2bc80>\n        > File \"program.py\", line 10, in g\n            f(0)\n            \u2514 <function f at 0x7f225fe2b9d8>\n          File \"program.py\", line 6, in f\n            100 / x\n                  \u2514 0\n        ZeroDivisionError: division by zero\n\n        >>> with logger.catch(message=\"Because we never know...\"):\n        ...    main()  # No exception, no logs\n        ...\n        \"\"\"\n        if callable(exception) and (\n            not isclass(exception) or not issubclass(exception, BaseException)\n        ):\n            return self.catch()(exception)\n\n        class Catcher:\n            def __init__(self, as_decorator):\n                self._as_decorator = as_decorator\n\n            def __enter__(self_):\n                return None\n\n            def __exit__(self_, type_, value, traceback_):\n                if type_ is None:\n                    return\n\n                if not issubclass(type_, exception):\n                    return False\n\n                if self_._as_decorator:\n                    back = 2\n                else:\n                    back = 1\n\n                logger_ = self.opt(\n                    exception=True,\n                    record=True,\n                    lazy=self._lazy,\n                    ansi=self._ansi,\n                    raw=self._raw,\n                    depth=self._depth + back,\n                )\n\n                log = logger_._make_log_function(level)\n\n                log(logger_, message)\n\n                return not reraise\n\n            def __call__(_, function):\n                catcher = Catcher(True)\n\n                if inspect.iscoroutinefunction(function):\n                    async def catch_wrapper(*args, **kwargs):\n                        with catcher:\n                            return await function(*args, **kwargs)\n                elif inspect.isgeneratorfunction(function):\n                    def catch_wrapper(*args, **kwargs):\n                        with catcher:\n                            return (yield from function(*args, **kwargs))\n                else:\n                    def catch_wrapper(*args, **kwargs):\n                        with catcher:\n                            return function(*args, **kwargs)\n\n                functools.update_wrapper(catch_wrapper, function)\n                return catch_wrapper\n\n        return Catcher(False)", "response": "This is a non - recursive function that wraps a function and returns a context manager that logs unexpected exceptions."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a new Logger that will log each logged message record.", "response": "def bind(_self, **kwargs):\n        \"\"\"Bind attributes to the ``extra`` dict of each logged message record.\n\n        This is used to add custom context to each logging call.\n\n        Parameters\n        ----------\n        **kwargs\n            Mapping between keys and values that will be added to the ``extra`` dict.\n\n        Returns\n        -------\n        :class:`~Logger`\n            A logger wrapping the core logger, but which sends record with the customized ``extra``\n            dict.\n\n        Examples\n        --------\n        >>> logger.add(sys.stderr, format=\"{extra[ip]} - {message}\")\n        1\n        >>> class Server:\n        ...     def __init__(self, ip):\n        ...         self.ip = ip\n        ...         self.logger = logger.bind(ip=ip)\n        ...     def call(self, message):\n        ...         self.logger.info(message)\n        ...\n        >>> instance_1 = Server(\"192.168.0.200\")\n        >>> instance_2 = Server(\"127.0.0.1\")\n        >>> instance_1.call(\"First instance\")\n        192.168.0.200 - First instance\n        >>> instance_2.call(\"Second instance\")\n        127.0.0.1 - Second instance\n        \"\"\"\n        return Logger(\n            {**_self._extra, **kwargs},\n            _self._exception,\n            _self._record,\n            _self._lazy,\n            _self._ansi,\n            _self._raw,\n            _self._depth,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef level(self, name, no=None, color=None, icon=None):\n        if not isinstance(name, str):\n            raise ValueError(\n                \"Invalid level name, it should be a string, not: '%s'\" % type(name).__name__\n            )\n\n        if no is color is icon is None:\n            try:\n                return self._levels[name]\n            except KeyError:\n                raise ValueError(\"Level '%s' does not exist\" % name)\n\n        if name not in self._levels:\n            if no is None:\n                raise ValueError(\n                    \"Level '%s' does not exist, you have to create it by specifying a level no\"\n                    % name\n                )\n            else:\n                old_no, old_color, old_icon = None, \"\", \" \"\n        else:\n            old_no, old_color, old_icon = self.level(name)\n\n        if no is None:\n            no = old_no\n\n        if color is None:\n            color = old_color\n\n        if icon is None:\n            icon = old_icon\n\n        if not isinstance(no, int):\n            raise ValueError(\n                \"Invalid level no, it should be an integer, not: '%s'\" % type(no).__name__\n            )\n\n        if no < 0:\n            raise ValueError(\"Invalid level no, it should be a positive integer, not: %d\" % no)\n\n        self._levels[name] = Level(no, color, icon)\n\n        with self._lock:\n            for handler in self._handlers.values():\n                handler.update_format(color)\n\n        return self.level(name)", "response": "Add or update or retrieve a logging level."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef configure(self, *, handlers=None, levels=None, extra=None, activation=None):\n        if handlers is not None:\n            self.remove()\n        else:\n            handlers = []\n\n        if levels is not None:\n            for params in levels:\n                self.level(**params)\n\n        if extra is not None:\n            with self._lock:\n                self._extra_class.clear()\n                self._extra_class.update(extra)\n\n        if activation is not None:\n            for name, state in activation:\n                if state:\n                    self.enable(name)\n                else:\n                    self.disable(name)\n\n        return [self.add(**params) for params in handlers]", "response": "Configure the core logger."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse(file, pattern, *, cast={}, chunk=2 ** 16):\n        if isinstance(file, (str, PathLike)):\n            should_close = True\n            fileobj = open(str(file))\n        elif hasattr(file, \"read\") and callable(file.read):\n            should_close = False\n            fileobj = file\n        else:\n            raise ValueError(\n                \"Invalid file, it should be a string path or a file object, not: '%s'\"\n                % type(file).__name__\n            )\n\n        if isinstance(cast, dict):\n\n            def cast_function(groups):\n                for key, converter in cast.items():\n                    if key in groups:\n                        groups[key] = converter(groups[key])\n\n        elif callable(cast):\n            cast_function = cast\n        else:\n            raise ValueError(\n                \"Invalid cast, it should be a function or a dict, not: '%s'\" % type(cast).__name__\n            )\n\n        try:\n            regex = re.compile(pattern)\n        except TypeError:\n            raise ValueError(\n                \"Invalid pattern, it should be a string or a compiled regex, not: '%s'\"\n                % type(pattern).__name__\n            )\n\n        matches = Logger._find_iter(fileobj, regex, chunk)\n\n        for match in matches:\n            groups = match.groupdict()\n            cast_function(groups)\n            yield groups\n\n        if should_close:\n            fileobj.close()", "response": "Parse the log file and extract each entry as a |dict|."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef log(_self, _level, _message, *args, **kwargs):\n        logger = _self.opt(\n            exception=_self._exception,\n            record=_self._record,\n            lazy=_self._lazy,\n            ansi=_self._ansi,\n            raw=_self._raw,\n            depth=_self._depth + 1,\n        )\n        logger._make_log_function(_level)(logger, _message, *args, **kwargs)", "response": "r Log a message with severity _level."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndeprecating function to add a new handler.", "response": "def start(self, *args, **kwargs):\n        \"\"\"Deprecated function to |add| a new handler.\n\n        Warnings\n        --------\n        .. deprecated:: 0.2.2\n          ``start()`` will be removed in Loguru 1.0.0, it is replaced by ``add()`` which is a less\n          confusing name.\n        \"\"\"\n        warnings.warn(\n            \"The 'start()' method is deprecated, please use 'add()' instead\", DeprecationWarning\n        )\n        return self.add(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndeprecates function to remove an existing handler.", "response": "def stop(self, *args, **kwargs):\n        \"\"\"Deprecated function to |remove| an existing handler.\n\n        Warnings\n        --------\n        .. deprecated:: 0.2.2\n          ``stop()`` will be removed in Loguru 1.0.0, it is replaced by ``remove()`` which is a less\n          confusing name.\n        \"\"\"\n        warnings.warn(\n            \"The 'stop()' method is deprecated, please use 'remove()' instead\", DeprecationWarning\n        )\n        return self.remove(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef perform_import(val, setting_name):\n    if val is None:\n        return None\n    elif isinstance(val, six.string_types):\n        return import_from_string(val, setting_name)\n    elif isinstance(val, (list, tuple)):\n        return [import_from_string(item, setting_name) for item in val]\n    return val", "response": "Perform a necessary import or imports."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a custom filterset for the given model.", "response": "def custom_filterset_factory(model, filterset_base_class=FilterSet, **meta):\n    \"\"\" Create a filterset for the given model using the provided meta data\n    \"\"\"\n    meta.update({\"model\": model})\n    meta_class = type(str(\"Meta\"), (object,), meta)\n    filterset = type(\n        str(\"%sFilterSet\" % model._meta.object_name),\n        (filterset_base_class, GrapheneFilterSetMixin),\n        {\"Meta\": meta_class},\n    )\n    return filterset"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert the filter value to a primary key before filtering", "response": "def filter(self, qs, value):\n        \"\"\" Convert the filter value to a primary key before filtering \"\"\"\n        _id = None\n        if value is not None:\n            _, _id = from_global_id(value)\n        return super(GlobalIDFilter, self).filter(qs, _id)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_filtering_args_from_filterset(filterset_class, type):\n    from ..forms.converter import convert_form_field\n\n    args = {}\n    for name, filter_field in six.iteritems(filterset_class.base_filters):\n        field_type = convert_form_field(filter_field.field).Argument()\n        field_type.description = filter_field.label\n        args[name] = field_type\n\n    return args", "response": "Inspects a FilterSet and returns the arguments to pass to\n        a Graphene Field"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _make_topics_result(f, futmap):\n        try:\n            result = f.result()\n            for topic, error in result.items():\n                fut = futmap.get(topic, None)\n                if fut is None:\n                    raise RuntimeError(\"Topic {} not found in future-map: {}\".format(topic, futmap))\n\n                if error is not None:\n                    # Topic-level exception\n                    fut.set_exception(KafkaException(error))\n                else:\n                    # Topic-level success\n                    fut.set_result(None)\n        except Exception as e:\n            # Request-level exception, raise the same for all topics\n            for topic, fut in futmap.items():\n                fut.set_exception(e)", "response": "Make result values for all topics in the given futures."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _make_resource_result(f, futmap):\n        try:\n            result = f.result()\n            for resource, configs in result.items():\n                fut = futmap.get(resource, None)\n                if fut is None:\n                    raise RuntimeError(\"Resource {} not found in future-map: {}\".format(resource, futmap))\n                if resource.error is not None:\n                    # Resource-level exception\n                    fut.set_exception(KafkaException(resource.error))\n                else:\n                    # Resource-level success\n                    # configs will be a dict for describe_configs()\n                    # and None for alter_configs()\n                    fut.set_result(configs)\n        except Exception as e:\n            # Request-level exception, raise the same for all resources\n            for resource, fut in futmap.items():\n                fut.set_exception(e)", "response": "Make a result for each resource in the given futures."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate futures and a futuremap for the keys in futmap_keys and create a request - level futuremap for the keys in futmap_keys and set the result of make_result_fn.", "response": "def _make_futures(futmap_keys, class_check, make_result_fn):\n        \"\"\"\n        Create futures and a futuremap for the keys in futmap_keys,\n        and create a request-level future to be bassed to the C API.\n        \"\"\"\n        futmap = {}\n        for key in futmap_keys:\n            if class_check is not None and not isinstance(key, class_check):\n                raise ValueError(\"Expected list of {}\".format(type(class_check)))\n            futmap[key] = concurrent.futures.Future()\n            if not futmap[key].set_running_or_notify_cancel():\n                raise RuntimeError(\"Future was cancelled prematurely\")\n\n        # Create an internal future for the entire request,\n        # this future will trigger _make_..._result() and set result/exception\n        # per topic,future in futmap.\n        f = concurrent.futures.Future()\n        f.add_done_callback(lambda f: make_result_fn(f, futmap))\n\n        if not f.set_running_or_notify_cancel():\n            raise RuntimeError(\"Future was cancelled prematurely\")\n\n        return f, futmap"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_topics(self, new_topics, **kwargs):\n\n        f, futmap = AdminClient._make_futures([x.topic for x in new_topics],\n                                              None,\n                                              AdminClient._make_topics_result)\n\n        super(AdminClient, self).create_topics(new_topics, f, **kwargs)\n\n        return futmap", "response": "Create new topics in cluster."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef delete_topics(self, topics, **kwargs):\n\n        f, futmap = AdminClient._make_futures(topics, None,\n                                              AdminClient._make_topics_result)\n\n        super(AdminClient, self).delete_topics(topics, f, **kwargs)\n\n        return futmap", "response": "Delete topics.\n\n        The future result() value is None.\n\n        :param list(str) topics: Topics to mark for deletion.\n        :param float operation_timeout: Set broker's operation timeout in seconds,\n                  controlling how long the DeleteTopics request will block\n                  on the broker waiting for the topic deletion to propagate\n                  in the cluster. A value of 0 returns immediately. Default: 0\n        :param float request_timeout: Set the overall request timeout in seconds,\n                  including broker lookup, request transmission, operation time\n                  on broker, and response. Default: `socket.timeout.ms*1000.0`\n\n        :returns: a dict of futures for each topic, keyed by the topic name.\n        :rtype: dict(<topic_name, future>)\n\n        :raises KafkaException: Operation failed locally or on broker.\n        :raises TypeException: Invalid input.\n        :raises ValueException: Invalid input."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_partitions(self, new_partitions, **kwargs):\n\n        f, futmap = AdminClient._make_futures([x.topic for x in new_partitions],\n                                              None,\n                                              AdminClient._make_topics_result)\n\n        super(AdminClient, self).create_partitions(new_partitions, f, **kwargs)\n\n        return futmap", "response": "Create additional partitions for the given topics."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the configuration for the specified resources.", "response": "def describe_configs(self, resources, **kwargs):\n        \"\"\"\n        Get configuration for the specified resources.\n\n        The future result() value is a dict(<configname, ConfigEntry>).\n\n        :warning: Multiple resources and resource types may be requested,\n                  but at most one resource of type RESOURCE_BROKER is allowed\n                  per call since these resource requests must be sent to the\n                  broker specified in the resource.\n\n        :param list(ConfigResource) resources: Resources to get configuration for.\n        :param float request_timeout: Set the overall request timeout in seconds,\n                  including broker lookup, request transmission, operation time\n                  on broker, and response. Default: `socket.timeout.ms*1000.0`\n        :param bool validate_only: Tell broker to only validate the request,\n                  without creating the partitions. Default: False\n\n        :returns: a dict of futures for each resource, keyed by the ConfigResource.\n        :rtype: dict(<ConfigResource, future>)\n\n        :raises KafkaException: Operation failed locally or on broker.\n        :raises TypeException: Invalid input.\n        :raises ValueException: Invalid input.\n        \"\"\"\n\n        f, futmap = AdminClient._make_futures(resources, ConfigResource,\n                                              AdminClient._make_resource_result)\n\n        super(AdminClient, self).describe_configs(resources, f, **kwargs)\n\n        return futmap"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef loads(schema_str):\n    try:\n        if sys.version_info[0] < 3:\n            return schema.parse(schema_str)\n        else:\n            return schema.Parse(schema_str)\n    except schema.SchemaParseException as e:\n        raise ClientError(\"Schema parse failed: %s\" % (str(e)))", "response": "Parse a schema given a schema string"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef produce(self, **kwargs):\n        # get schemas from  kwargs if defined\n        key_schema = kwargs.pop('key_schema', self._key_schema)\n        value_schema = kwargs.pop('value_schema', self._value_schema)\n        topic = kwargs.pop('topic', None)\n        if not topic:\n            raise ClientError(\"Topic name not specified.\")\n        value = kwargs.pop('value', None)\n        key = kwargs.pop('key', None)\n\n        if value is not None:\n            if value_schema:\n                value = self._serializer.encode_record_with_schema(topic, value_schema, value)\n            else:\n                raise ValueSerializerError(\"Avro schema required for values\")\n\n        if key is not None:\n            if key_schema:\n                key = self._serializer.encode_record_with_schema(topic, key_schema, key, True)\n            else:\n                raise KeySerializerError(\"Avro schema required for key\")\n\n        super(AvroProducer, self).produce(topic, value, key, **kwargs)", "response": "Asynchronously sends message to Kafka by encoding with specified avro schema."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngiving a parsed avro schema encode a record for the given topic.", "response": "def encode_record_with_schema(self, topic, schema, record, is_key=False):\n        \"\"\"\n        Given a parsed avro schema, encode a record for the given topic.  The\n        record is expected to be a dictionary.\n\n        The schema is registered with the subject of 'topic-value'\n        :param str topic: Topic name\n        :param schema schema: Avro Schema\n        :param dict record: An object to serialize\n        :param bool is_key: If the record is a key\n        :returns: Encoded record with schema ID as bytes\n        :rtype: bytes\n        \"\"\"\n        serialize_err = KeySerializerError if is_key else ValueSerializerError\n\n        subject_suffix = ('-key' if is_key else '-value')\n        # get the latest schema for the subject\n        subject = topic + subject_suffix\n        # register it\n        schema_id = self.registry_client.register(subject, schema)\n        if not schema_id:\n            message = \"Unable to retrieve schema id for subject %s\" % (subject)\n            raise serialize_err(message)\n\n        # cache writer\n        self.id_to_writers[schema_id] = self._get_encoder_func(schema)\n\n        return self.encode_record_with_schema_id(schema_id, record, is_key=is_key)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef encode_record_with_schema_id(self, schema_id, record, is_key=False):\n        serialize_err = KeySerializerError if is_key else ValueSerializerError\n\n        # use slow avro\n        if schema_id not in self.id_to_writers:\n            # get the writer + schema\n\n            try:\n                schema = self.registry_client.get_by_id(schema_id)\n                if not schema:\n                    raise serialize_err(\"Schema does not exist\")\n                self.id_to_writers[schema_id] = self._get_encoder_func(schema)\n            except ClientError:\n                exc_type, exc_value, exc_traceback = sys.exc_info()\n                raise serialize_err(repr(traceback.format_exception(exc_type, exc_value, exc_traceback)))\n\n        # get the writer\n        writer = self.id_to_writers[schema_id]\n        with ContextStringIO() as outf:\n            # Write the magic byte and schema ID in network byte order (big endian)\n            outf.write(struct.pack('>bI', MAGIC_BYTE, schema_id))\n\n            # write the record to the rest of the buffer\n            writer(record, outf)\n\n            return outf.getvalue()", "response": "Encode a record with a given schema ID."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndecoding a message from kafka that has been encoded for use with the schema registry.", "response": "def decode_message(self, message, is_key=False):\n        \"\"\"\n        Decode a message from kafka that has been encoded for use with\n        the schema registry.\n        :param str|bytes or None message: message key or value to be decoded\n        :returns: Decoded message contents.\n        :rtype dict:\n        \"\"\"\n\n        if message is None:\n            return None\n\n        if len(message) <= 5:\n            raise SerializerError(\"message is too small to decode\")\n\n        with ContextStringIO(message) as payload:\n            magic, schema_id = struct.unpack('>bI', payload.read(5))\n            if magic != MAGIC_BYTE:\n                raise SerializerError(\"message does not start with magic byte\")\n            decoder_func = self._get_decoder_func(schema_id, payload, is_key)\n            return decoder_func(payload)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndelete topics from the broker.", "response": "def example_delete_topics(a, topics):\n    \"\"\" delete topics \"\"\"\n\n    # Call delete_topics to asynchronously delete topics, a future is returned.\n    # By default this operation on the broker returns immediately while\n    # topics are deleted in the background. But here we give it some time (30s)\n    # to propagate in the cluster before returning.\n    #\n    # Returns a dict of <topic,future>.\n    fs = a.delete_topics(topics, operation_timeout=30)\n\n    # Wait for operation to finish.\n    for topic, f in fs.items():\n        try:\n            f.result()  # The result itself is None\n            print(\"Topic {} deleted\".format(topic))\n        except Exception as e:\n            print(\"Failed to delete topic {}: {}\".format(topic, e))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef example_create_partitions(a, topics):\n\n    new_parts = [NewPartitions(topic, int(new_total_count)) for\n                 topic, new_total_count in zip(topics[0::2], topics[1::2])]\n\n    # Try switching validate_only to True to only validate the operation\n    # on the broker but not actually perform it.\n    fs = a.create_partitions(new_parts, validate_only=False)\n\n    # Wait for operation to finish.\n    for topic, f in fs.items():\n        try:\n            f.result()  # The result itself is None\n            print(\"Additional partitions created for topic {}\".format(topic))\n        except Exception as e:\n            print(\"Failed to add partitions to topic {}: {}\".format(topic, e))", "response": "Create partitions for all topics."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef example_describe_configs(a, args):\n\n    resources = [ConfigResource(restype, resname) for\n                 restype, resname in zip(args[0::2], args[1::2])]\n\n    fs = a.describe_configs(resources)\n\n    # Wait for operation to finish.\n    for res, f in fs.items():\n        try:\n            configs = f.result()\n            for config in iter(configs.values()):\n                print_config(config, 1)\n\n        except KafkaException as e:\n            print(\"Failed to describe {}: {}\".format(res, e))\n        except Exception:\n            raise", "response": "describe configs for all resources in the cluster"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef example_alter_configs(a, args):\n\n    resources = []\n    for restype, resname, configs in zip(args[0::3], args[1::3], args[2::3]):\n        resource = ConfigResource(restype, resname)\n        resources.append(resource)\n        for k, v in [conf.split('=') for conf in configs.split(',')]:\n            resource.set_config(k, v)\n\n    fs = a.alter_configs(resources)\n\n    # Wait for operation to finish.\n    for res, f in fs.items():\n        try:\n            f.result()  # empty, but raises exception on failure\n            print(\"{} configuration successfully altered\".format(res))\n        except Exception:\n            raise", "response": "Alter configs atomically replacing non - specified configuration properties with their default values."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef example_delta_alter_configs(a, args):\n\n    # Convert supplied config to resources.\n    # We can reuse the same resources both for describe_configs and\n    # alter_configs.\n    resources = []\n    for restype, resname, configs in zip(args[0::3], args[1::3], args[2::3]):\n        resource = ConfigResource(restype, resname)\n        resources.append(resource)\n        for k, v in [conf.split('=') for conf in configs.split(',')]:\n            resource.set_config(k, v)\n\n    # Set up a locked counter and an Event (for signaling) to track when the\n    # second level of futures are done. This is a bit of contrived example\n    # due to no other asynchronous mechanism being used, so we'll need\n    # to wait on something to signal completion.\n\n    class WaitZero(object):\n        def __init__(self, waitcnt):\n            self.cnt = waitcnt\n            self.lock = threading.Lock()\n            self.event = threading.Event()\n\n        def decr(self):\n            \"\"\" Decrement cnt by 1\"\"\"\n            with self.lock:\n                assert self.cnt > 0\n                self.cnt -= 1\n            self.event.set()\n\n        def wait(self):\n            \"\"\" Wait until cnt reaches 0 \"\"\"\n            self.lock.acquire()\n            while self.cnt > 0:\n                self.lock.release()\n                self.event.wait()\n                self.event.clear()\n                self.lock.acquire()\n            self.lock.release()\n\n        def __len__(self):\n            with self.lock:\n                return self.cnt\n\n    wait_zero = WaitZero(len(resources))\n\n    # Read existing configuration from cluster\n    fs = a.describe_configs(resources)\n\n    def delta_alter_configs_done(fut, resource):\n        e = fut.exception()\n        if e is not None:\n            print(\"Config update for {} failed: {}\".format(resource, e))\n        else:\n            print(\"Config for {} updated\".format(resource))\n        wait_zero.decr()\n\n    def delta_alter_configs(resource, remote_config):\n        print(\"Updating {} supplied config entries {} with {} config entries read from cluster\".format(\n            len(resource), resource, len(remote_config)))\n        # Only set configuration that is not default\n        for k, entry in [(k, v) for k, v in remote_config.items() if not v.is_default]:\n            resource.set_config(k, entry.value, overwrite=False)\n\n        fs = a.alter_configs([resource])\n        fs[resource].add_done_callback(lambda fut: delta_alter_configs_done(fut, resource))\n\n    # For each resource's future set up a completion callback\n    # that in turn calls alter_configs() on that single resource.\n    # This is ineffective since the resources can usually go in\n    # one single alter_configs() call, but we're also show-casing\n    # the futures here.\n    for res, f in fs.items():\n        f.add_done_callback(lambda fut, resource=res: delta_alter_configs(resource, fut.result()))\n\n    # Wait for done callbacks to be triggered and operations to complete.\n    print(\"Waiting for {} resource updates to finish\".format(len(wait_zero)))\n    wait_zero.wait()", "response": "This example shows how to modify the supplied configuration entries in Kafka."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef example_list(a, args):\n\n    if len(args) == 0:\n        what = \"all\"\n    else:\n        what = args[0]\n\n    md = a.list_topics(timeout=10)\n\n    print(\"Cluster {} metadata (response from broker {}):\".format(md.cluster_id, md.orig_broker_name))\n\n    if what in (\"all\", \"brokers\"):\n        print(\" {} brokers:\".format(len(md.brokers)))\n        for b in iter(md.brokers.values()):\n            if b.id == md.controller_id:\n                print(\"  {}  (controller)\".format(b))\n            else:\n                print(\"  {}\".format(b))\n\n    if what not in (\"all\", \"topics\"):\n        return\n\n    print(\" {} topics:\".format(len(md.topics)))\n    for t in iter(md.topics.values()):\n        if t.error is not None:\n            errstr = \": {}\".format(t.error)\n        else:\n            errstr = \"\"\n\n        print(\"  \\\"{}\\\" with {} partition(s){}\".format(t, len(t.partitions), errstr))\n\n        for p in iter(t.partitions.values()):\n            if p.error is not None:\n                errstr = \": {}\".format(p.error)\n            else:\n                errstr = \"\"\n\n            print(\"    partition {} leader: {}, replicas: {}, isrs: {}\".format(\n                p.id, p.leader, p.replicas, p.isrs, errstr))", "response": "list topics and cluster metadata"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _resolve_plugins(plugins):\n    import os\n    from sys import platform\n\n    # Location of __init__.py and the embedded library directory\n    basedir = os.path.dirname(__file__)\n\n    if platform in ('win32', 'cygwin'):\n        paths_sep = ';'\n        ext = '.dll'\n        libdir = basedir\n    elif platform in ('linux', 'linux2'):\n        paths_sep = ':'\n        ext = '.so'\n        libdir = os.path.join(basedir, '.libs')\n    elif platform == 'darwin':\n        paths_sep = ':'\n        ext = '.dylib'\n        libdir = os.path.join(basedir, '.dylibs')\n    else:\n        # Unknown platform, there are probably no embedded plugins.\n        return plugins\n\n    if not os.path.isdir(libdir):\n        # No embedded library directory, probably not a wheel installation.\n        return plugins\n\n    resolved = []\n    for plugin in plugins.split(paths_sep):\n        if '/' in plugin or '\\\\' in plugin:\n            # Path specified, leave unchanged\n            resolved.append(plugin)\n            continue\n\n        # See if the plugin can be found in the wheel's\n        # embedded library directory.\n        # The user might not have supplied a file extension, so try both.\n        good = None\n        for file in [plugin, plugin + ext]:\n            fpath = os.path.join(libdir, file)\n            if os.path.isfile(fpath):\n                good = fpath\n                break\n\n        if good is not None:\n            resolved.append(good)\n        else:\n            resolved.append(plugin)\n\n    return paths_sep.join(resolved)", "response": "Resolve embedded plugins from the wheel s library directory."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nhandling delivery reports served from producer. poll.", "response": "def on_delivery(err, msg, obj):\n    \"\"\"\n        Handle delivery reports served from producer.poll.\n        This callback takes an extra argument, obj.\n        This allows the original contents to be included for debugging purposes.\n    \"\"\"\n    if err is not None:\n        print('Message {} delivery failed for user {} with error {}'.format(\n            obj.id, obj.name, err))\n    else:\n        print('Message {} successfully produced to {} [{}] at offset {}'.format(\n            obj.id, msg.topic(), msg.partition(), msg.offset()))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nproduces user records to a topic.", "response": "def produce(topic, conf):\n    \"\"\"\n        Produce User records\n    \"\"\"\n\n    from confluent_kafka.avro import AvroProducer\n\n    producer = AvroProducer(conf, default_value_schema=record_schema)\n\n    print(\"Producing user records to topic {}. ^c to exit.\".format(topic))\n    while True:\n        # Instantiate new User, populate fields, produce record, execute callbacks.\n        record = User()\n        try:\n            record.name = input(\"Enter name: \")\n            record.favorite_number = int(input(\"Enter favorite number: \"))\n            record.favorite_color = input(\"Enter favorite color: \")\n\n            # The message passed to the delivery callback will already be serialized.\n            # To aid in debugging we provide the original object to the delivery callback.\n            producer.produce(topic=topic, value=record.to_dict(),\n                             callback=lambda err, msg, obj=record: on_delivery(err, msg, obj))\n            # Serve on_delivery callbacks from previous asynchronous produce()\n            producer.poll(0)\n        except KeyboardInterrupt:\n            break\n        except ValueError:\n            print(\"Invalid input, discarding record...\")\n            continue\n\n    print(\"\\nFlushing records...\")\n    producer.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconsuming user records from kafka topic with group.", "response": "def consume(topic, conf):\n    \"\"\"\n        Consume User records\n    \"\"\"\n    from confluent_kafka.avro import AvroConsumer\n    from confluent_kafka.avro.serializer import SerializerError\n\n    print(\"Consuming user records from topic {} with group {}. ^c to exit.\".format(topic, conf[\"group.id\"]))\n\n    c = AvroConsumer(conf, reader_value_schema=record_schema)\n    c.subscribe([topic])\n\n    while True:\n        try:\n            msg = c.poll(1)\n\n            # There were no messages on the queue, continue polling\n            if msg is None:\n                continue\n\n            if msg.error():\n                print(\"Consumer error: {}\".format(msg.error()))\n                continue\n\n            record = User(msg.value())\n            print(\"name: {}\\n\\tfavorite_number: {}\\n\\tfavorite_color: {}\\n\".format(\n                record.name, record.favorite_number, record.favorite_color))\n        except SerializerError as e:\n            # Report malformed record, discard results, continue polling\n            print(\"Message deserialization failed {}\".format(e))\n            continue\n        except KeyboardInterrupt:\n            break\n\n    print(\"Shutting down consumer..\")\n    c.close()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nregistering a schema with the registry under the given subject.", "response": "def register(self, subject, avro_schema):\n        \"\"\"\n        POST /subjects/(string: subject)/versions\n        Register a schema with the registry under the given subject\n        and receive a schema id.\n\n        avro_schema must be a parsed schema from the python avro library\n\n        Multiple instances of the same schema will result in cache misses.\n\n        :param str subject: subject name\n        :param schema avro_schema: Avro schema to be registered\n        :returns: schema_id\n        :rtype: int\n        \"\"\"\n\n        schemas_to_id = self.subject_to_schema_ids[subject]\n        schema_id = schemas_to_id.get(avro_schema, None)\n        if schema_id is not None:\n            return schema_id\n        # send it up\n        url = '/'.join([self.url, 'subjects', subject, 'versions'])\n        # body is { schema : json_string }\n\n        body = {'schema': json.dumps(avro_schema.to_json())}\n        result, code = self._send_request(url, method='POST', body=body)\n        if (code == 401 or code == 403):\n            raise ClientError(\"Unauthorized access. Error code:\" + str(code))\n        elif code == 409:\n            raise ClientError(\"Incompatible Avro schema:\" + str(code))\n        elif code == 422:\n            raise ClientError(\"Invalid Avro schema:\" + str(code))\n        elif not (code >= 200 and code <= 299):\n            raise ClientError(\"Unable to register schema. Error code:\" + str(code))\n        # result is a dict\n        schema_id = result['id']\n        # cache it\n        self._cache_schema(avro_schema, schema_id, subject)\n        return schema_id"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndelete the specified subject and its associated compatibility level if registered.", "response": "def delete_subject(self, subject):\n        \"\"\"\n        DELETE /subjects/(string: subject)\n        Deletes the specified subject and its associated compatibility level if registered.\n        It is recommended to use this API only when a topic needs to be recycled or in development environments.\n        :param subject: subject name\n        :returns: version of the schema deleted under this subject\n        :rtype: (int)\n        \"\"\"\n\n        url = '/'.join([self.url, 'subjects', subject])\n\n        result, code = self._send_request(url, method=\"DELETE\")\n        if not (code >= 200 and code <= 299):\n            raise ClientError('Unable to delete subject: {}'.format(result))\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_by_id(self, schema_id):\n        if schema_id in self.id_to_schema:\n            return self.id_to_schema[schema_id]\n        # fetch from the registry\n        url = '/'.join([self.url, 'schemas', 'ids', str(schema_id)])\n\n        result, code = self._send_request(url)\n        if code == 404:\n            log.error(\"Schema not found:\" + str(code))\n            return None\n        elif not (code >= 200 and code <= 299):\n            log.error(\"Unable to get schema for the specific ID:\" + str(code))\n            return None\n        else:\n            # need to parse the schema\n            schema_str = result.get(\"schema\")\n            try:\n                result = loads(schema_str)\n                # cache it\n                self._cache_schema(result, schema_id)\n                return result\n            except ClientError as e:\n                # bad schema - should not happen\n                raise ClientError(\"Received bad schema (id %s) from registry: %s\" % (schema_id, e))", "response": "Get an Avro schema by its ID."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_version(self, subject, avro_schema):\n        schemas_to_version = self.subject_to_schema_versions[subject]\n        version = schemas_to_version.get(avro_schema, None)\n        if version is not None:\n            return version\n\n        url = '/'.join([self.url, 'subjects', subject])\n        body = {'schema': json.dumps(avro_schema.to_json())}\n\n        result, code = self._send_request(url, method='POST', body=body)\n        if code == 404:\n            log.error(\"Not found:\" + str(code))\n            return None\n        elif not (code >= 200 and code <= 299):\n            log.error(\"Unable to get version of a schema:\" + str(code))\n            return None\n        schema_id = result['id']\n        version = result['version']\n        self._cache_schema(avro_schema, schema_id, subject, version)\n        return version", "response": "Get the version of a schema for a given subject."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating the compatibility level for a subject.", "response": "def update_compatibility(self, level, subject=None):\n        \"\"\"\n        PUT /config/(string: subject)\n\n        Update the compatibility level for a subject.  Level must be one of:\n\n        :param str level: ex: 'NONE','FULL','FORWARD', or 'BACKWARD'\n        \"\"\"\n        if level not in VALID_LEVELS:\n            raise ClientError(\"Invalid level specified: %s\" % (str(level)))\n\n        url = '/'.join([self.url, 'config'])\n        if subject:\n            url += '/' + subject\n\n        body = {\"compatibility\": level}\n        result, code = self._send_request(url, method='PUT', body=body)\n        if code >= 200 and code <= 299:\n            return result['compatibility']\n        else:\n            raise ClientError(\"Unable to update level: %s. Error code: %d\" % (str(level)), code)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_compatibility(self, subject=None):\n        url = '/'.join([self.url, 'config'])\n        if subject:\n            url = '/'.join([url, subject])\n\n        result, code = self._send_request(url)\n        is_successful_request = code >= 200 and code <= 299\n        if not is_successful_request:\n            raise ClientError('Unable to fetch compatibility level. Error code: %d' % code)\n\n        compatibility = result.get('compatibilityLevel', None)\n        if compatibility not in VALID_LEVELS:\n            if compatibility is None:\n                error_msg_suffix = 'No compatibility was returned'\n            else:\n                error_msg_suffix = str(compatibility)\n            raise ClientError('Invalid compatibility level received: %s' % error_msg_suffix)\n\n        return compatibility", "response": "GET /config\n        Get the current compatibility level for a subject.  Result will be one of:\n\n        :param str subject: subject name\n        :raises ClientError: if the request was unsuccessful or an invalid compatibility level was returned\n        :returns: one of 'NONE','FULL','FORWARD', or 'BACKWARD'\n        :rtype: bool"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef download(self, dirpath):\n        if os.path.isfile(self.lpath) and os.path.getsize(self.lpath) > 0:\n            return\n        print('Downloading %s -> %s' % (self.path, self.lpath))\n        if dry_run:\n            return\n        self.arts.s3_bucket.download_file(self.path, self.lpath)", "response": "Download the artifact from S3 and store in dirpath."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef collect_single_s3(self, path):\n\n        # The S3 folder contains the tokens needed to perform\n        # matching of project, gitref, etc.\n        folder = os.path.dirname(path)\n\n        rinfo = re.findall(r'(?P<tag>[^-]+)-(?P<val>.*?)__', folder)\n        if rinfo is None or len(rinfo) == 0:\n            # print('Incorrect folder/file name format for %s' % folder)\n            return None\n\n        info = dict(rinfo)\n\n        # Ignore AppVeyor Debug builds\n        if info.get('bldtype', '').lower() == 'debug':\n            print('Ignoring debug artifact %s' % folder)\n            return None\n\n        tag = info.get('tag', None)\n        if tag is not None and (len(tag) == 0 or tag.startswith('$(')):\n            # AppVeyor doesn't substite $(APPVEYOR_REPO_TAG_NAME)\n            # with an empty value when not set, it leaves that token\n            # in the string - so translate that to no tag.\n            tag = None\n\n        sha = info.get('sha', None)\n\n        # Match tag or sha to gitref\n        if (tag is not None and tag == self.gitref) or (sha is not None and sha.startswith(self.gitref)):\n            return Artifact(self, path, info)\n\n        return None", "response": "Collect a single artifact from an S3 path."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncollect and download build - artifacts from S3 based on git reference", "response": "def collect_s3(self):\n        \"\"\" Collect and download build-artifacts from S3 based on git reference \"\"\"\n        print('Collecting artifacts matching tag/sha %s from S3 bucket %s' % (self.gitref, s3_bucket))\n        self.s3 = boto3.resource('s3')\n        self.s3_bucket = self.s3.Bucket(s3_bucket)\n        self.s3.meta.client.head_bucket(Bucket=s3_bucket)\n        for key in self.s3_bucket.objects.all():\n            self.collect_single_s3(key.key)\n\n        for a in self.artifacts:\n            a.download(self.dlpath)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncollect artifacts from a local directory possibly previously collected from s3", "response": "def collect_local(self, path):\n        \"\"\" Collect artifacts from a local directory possibly previously\n        collected from s3 \"\"\"\n        for f in os.listdir(path):\n            lpath = os.path.join(path, f)\n            if not os.path.isfile(lpath):\n                continue\n            Artifact(self, lpath)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _config(name, key=None, **kwargs):\n    '''\n    Return a value for 'name' from command line args then config file options.\n    Specify 'key' if the config file option is not the same as 'name'.\n    '''\n    if key is None:\n        key = name\n    if name in kwargs:\n        value = kwargs[name]\n    else:\n        value = __salt__['config.option']('ddns.{0}'.format(key))\n        if not value:\n            value = None\n    return value", "response": "Return a value for name from command line args then config file options."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd or update the A and PTR records for a host.", "response": "def add_host(zone, name, ttl, ip, nameserver='127.0.0.1', replace=True,\n             timeout=5, port=53, **kwargs):\n    '''\n    Add, replace, or update the A and PTR (reverse) records for a host.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt ns1 ddns.add_host example.com host1 60 10.1.1.1\n    '''\n    res = update(zone, name, ttl, 'A', ip, nameserver, timeout, replace, port,\n                 **kwargs)\n    if res is False:\n        return False\n\n    fqdn = '{0}.{1}.'.format(name, zone)\n    parts = ip.split('.')[::-1]\n    popped = []\n\n    # Iterate over possible reverse zones\n    while len(parts) > 1:\n        p = parts.pop(0)\n        popped.append(p)\n        zone = '{0}.{1}'.format('.'.join(parts), 'in-addr.arpa.')\n        name = '.'.join(popped)\n        ptr = update(zone, name, ttl, 'PTR', fqdn, nameserver, timeout,\n                     replace, port, **kwargs)\n        if ptr:\n            return True\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndeletes the forward and reverse records for a host.", "response": "def delete_host(zone, name, nameserver='127.0.0.1', timeout=5, port=53,\n                **kwargs):\n    '''\n    Delete the forward and reverse records for a host.\n\n    Returns true if any records are deleted.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt ns1 ddns.delete_host example.com host1\n    '''\n    fqdn = '{0}.{1}'.format(name, zone)\n    request = dns.message.make_query(fqdn, 'A')\n    answer = dns.query.udp(request, nameserver, timeout, port)\n    try:\n        ips = [i.address for i in answer.answer[0].items]\n    except IndexError:\n        ips = []\n\n    res = delete(zone, name, nameserver=nameserver, timeout=timeout, port=port,\n                 **kwargs)\n\n    fqdn = fqdn + '.'\n    for ip in ips:\n        parts = ip.split('.')[::-1]\n        popped = []\n\n        # Iterate over possible reverse zones\n        while len(parts) > 1:\n            p = parts.pop(0)\n            popped.append(p)\n            zone = '{0}.{1}'.format('.'.join(parts), 'in-addr.arpa.')\n            name = '.'.join(popped)\n            ptr = delete(zone, name, 'PTR', fqdn, nameserver=nameserver,\n                         timeout=timeout, port=port, **kwargs)\n        if ptr:\n            res = True\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate a DNS record in a named zone.", "response": "def update(zone, name, ttl, rdtype, data, nameserver='127.0.0.1', timeout=5,\n           replace=False, port=53, **kwargs):\n    '''\n    Add, replace, or update a DNS record.\n    nameserver must be an IP address and the minion running this module\n    must have update privileges on that server.\n    If replace is true, first deletes all records for this name and type.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt ns1 ddns.update example.com host1 60 A 10.0.0.1\n    '''\n    name = six.text_type(name)\n\n    if name[-1:] == '.':\n        fqdn = name\n    else:\n        fqdn = '{0}.{1}'.format(name, zone)\n\n    request = dns.message.make_query(fqdn, rdtype)\n    answer = dns.query.udp(request, nameserver, timeout, port)\n\n    rdtype = dns.rdatatype.from_text(rdtype)\n    rdata = dns.rdata.from_text(dns.rdataclass.IN, rdtype, data)\n\n    keyring = _get_keyring(_config('keyfile', **kwargs))\n    keyname = _config('keyname', **kwargs)\n    keyalgorithm = _config('keyalgorithm',\n                           **kwargs) or 'HMAC-MD5.SIG-ALG.REG.INT'\n\n    is_exist = False\n    for rrset in answer.answer:\n        if rdata in rrset.items:\n            if ttl == rrset.ttl:\n                if len(answer.answer) >= 1 or len(rrset.items) >= 1:\n                    is_exist = True\n                    break\n\n    dns_update = dns.update.Update(zone, keyring=keyring, keyname=keyname,\n                                   keyalgorithm=keyalgorithm)\n    if replace:\n        dns_update.replace(name, ttl, rdata)\n    elif not is_exist:\n        dns_update.add(name, ttl, rdata)\n    else:\n        return None\n    answer = dns.query.udp(dns_update, nameserver, timeout, port)\n    if answer.rcode() > 0:\n        return False\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _set_callables(modules):\n    '''\n    Set all Ansible modules callables\n    :return:\n    '''\n    def _set_function(cmd_name, doc):\n        '''\n        Create a Salt function for the Ansible module.\n        '''\n        def _cmd(*args, **kw):\n            '''\n            Call an Ansible module as a function from the Salt.\n            '''\n            kwargs = {}\n            if kw.get('__pub_arg'):\n                for _kw in kw.get('__pub_arg', []):\n                    if isinstance(_kw, dict):\n                        kwargs = _kw\n                        break\n\n            return _caller.call(cmd_name, *args, **kwargs)\n        _cmd.__doc__ = doc\n        return _cmd\n\n    for mod in modules:\n        setattr(sys.modules[__name__], mod, _set_function(mod, 'Available'))", "response": "Set all Ansible modules callables\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef help(module=None, *args):\n    '''\n    Display help on Ansible standard module.\n\n    :param module:\n    :return:\n    '''\n    if not module:\n        raise CommandExecutionError('Please tell me what module you want to have helped with. '\n                                    'Or call \"ansible.list\" to know what is available.')\n    try:\n        module = _resolver.load_module(module)\n    except (ImportError, LoaderError) as err:\n        raise CommandExecutionError('Module \"{0}\" is currently not functional on your system.'.format(module))\n\n    doc = {}\n    ret = {}\n    for docset in module.DOCUMENTATION.split('---'):\n        try:\n            docset = salt.utils.yaml.safe_load(docset)\n            if docset:\n                doc.update(docset)\n        except Exception as err:\n            log.error(\"Error parsing doc section: %s\", err)\n    if not args:\n        if 'description' in doc:\n            description = doc.get('description') or ''\n            del doc['description']\n            ret['Description'] = description\n        ret['Available sections on module \"{}\"'.format(module.__name__.replace('ansible.modules.', ''))] = doc.keys()\n    else:\n        for arg in args:\n            info = doc.get(arg)\n            if info is not None:\n                ret[arg] = info\n\n    return ret", "response": "Display help on Ansible standard module."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nruns Ansible Playbooks :param playbook: Which playbook to run. :param rundir: Directory to run `ansible-playbook` in. (Default: None) :param check: don't make any changes; instead, try to predict some of the changes that may occur (Default: False) :param diff: when changing (small) files and templates, show the differences in those files; works great with --check (default: False) :param extra_vars: set additional variables as key=value or YAML/JSON, if filename prepend with @, (default: None) :param flush_cache: clear the fact cache for every host in inventory (default: False) :param forks: specify number of parallel processes to use (Default: 5) :param inventory: specify inventory host path or comma separated host list. (Default: None) (Ansible's default is /etc/ansible/hosts) :param limit: further limit selected hosts to an additional pattern (Default: None) :param list_hosts: outputs a list of matching hosts; does not execute anything else (Default: False) :param list_tags: list all available tags (Default: False) :param list_tasks: list all tasks that would be executed (Default: False) :param module_path: prepend colon-separated path(s) to module library. (Default: None) :param skip_tags: only run plays and tasks whose tags do not match these values (Default: False) :param start_at_task: start the playbook at the task matching this name (Default: None) :param: syntax_check: perform a syntax check on the playbook, but do not execute it (Default: False) :param tags: only run plays and tasks tagged with these values (Default: None) :return: Playbook return CLI Example: .. code-block:: bash salt 'ansiblehost' ansible.playbook playbook=/srv/playbooks/play.yml", "response": "def playbooks(playbook, rundir=None, check=False, diff=False, extra_vars=None,\n              flush_cache=False, forks=5, inventory=None, limit=None,\n              list_hosts=False, list_tags=False, list_tasks=False,\n              module_path=None, skip_tags=None, start_at_task=None,\n              syntax_check=False, tags=None, playbook_kwargs=None):\n    '''\n    Run Ansible Playbooks\n\n    :param playbook: Which playbook to run.\n    :param rundir: Directory to run `ansible-playbook` in. (Default: None)\n    :param check: don't make any changes; instead, try to predict some\n                  of the changes that may occur (Default: False)\n    :param diff: when changing (small) files and templates, show the\n                 differences in those files; works great with --check\n                 (default: False)\n    :param extra_vars: set additional variables as key=value or YAML/JSON, if\n                       filename prepend with @, (default: None)\n    :param flush_cache: clear the fact cache for every host in inventory\n                        (default: False)\n    :param forks: specify number of parallel processes to use\n                  (Default: 5)\n    :param inventory: specify inventory host path or comma separated host\n                      list. (Default: None) (Ansible's default is /etc/ansible/hosts)\n    :param limit: further limit selected hosts to an additional pattern (Default: None)\n    :param list_hosts: outputs a list of matching hosts; does not execute anything else\n                       (Default: False)\n    :param list_tags: list all available tags (Default: False)\n    :param list_tasks: list all tasks that would be executed (Default: False)\n    :param module_path: prepend colon-separated path(s) to module library. (Default: None)\n    :param skip_tags: only run plays and tasks whose tags do not match these\n                      values (Default: False)\n    :param start_at_task: start the playbook at the task matching this name (Default: None)\n    :param: syntax_check: perform a syntax check on the playbook, but do not execute it\n                          (Default: False)\n    :param tags: only run plays and tasks tagged with these values (Default: None)\n\n    :return: Playbook return\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt 'ansiblehost'  ansible.playbook playbook=/srv/playbooks/play.yml\n    '''\n    command = ['ansible-playbook', playbook]\n    if check:\n        command.append('--check')\n    if diff:\n        command.append('--diff')\n    if isinstance(extra_vars, dict):\n        command.append(\"--extra-vars='{0}'\".format(json.dumps(extra_vars)))\n    elif isinstance(extra_vars, six.text_type) and extra_vars.startswith('@'):\n        command.append('--extra-vars={0}'.format(extra_vars))\n    if flush_cache:\n        command.append('--flush-cache')\n    if inventory:\n        command.append('--inventory={0}'.format(inventory))\n    if limit:\n        command.append('--limit={0}'.format(limit))\n    if list_hosts:\n        command.append('--list-hosts')\n    if list_tags:\n        command.append('--list-tags')\n    if list_tasks:\n        command.append('--list-tasks')\n    if module_path:\n        command.append('--module-path={0}'.format(module_path))\n    if skip_tags:\n        command.append('--skip-tags={0}'.format(skip_tags))\n    if start_at_task:\n        command.append('--start-at-task={0}'.format(start_at_task))\n    if syntax_check:\n        command.append('--syntax-check')\n    if tags:\n        command.append('--tags={0}'.format(tags))\n    if playbook_kwargs:\n        for key, value in six.iteritems(playbook_kwargs):\n            key = key.replace('_', '-')\n            if value is True:\n                command.append('--{0}'.format(key))\n            elif isinstance(value, six.text_type):\n                command.append('--{0}={1}'.format(key, value))\n            elif isinstance(value, dict):\n                command.append('--{0}={1}'.format(key, json.dumps(value)))\n    command.append('--forks={0}'.format(forks))\n    cmd_kwargs = {\n        'env': {'ANSIBLE_STDOUT_CALLBACK': 'json', 'ANSIBLE_RETRY_FILES_ENABLED': '0'},\n        'cwd': rundir,\n        'cmd': ' '.join(command)\n    }\n    ret = __salt__['cmd.run_all'](**cmd_kwargs)\n    log.debug('Ansible Playbook Return: %s', ret)\n    retdata = json.loads(ret['stdout'])\n    if ret['retcode']:\n        __context__['retcode'] = ret['retcode']\n    return retdata"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets installed Ansible modules and return a dict of the names of the modules that are installed.", "response": "def _get_modules_map(self, path=None):\n        '''\n        Get installed Ansible modules\n        :return:\n        '''\n        paths = {}\n        root = ansible.modules.__path__[0]\n        if not path:\n            path = root\n        for p_el in os.listdir(path):\n            p_el_path = os.path.join(path, p_el)\n            if os.path.islink(p_el_path):\n                continue\n            if os.path.isdir(p_el_path):\n                paths.update(self._get_modules_map(p_el_path))\n            else:\n                if (any(p_el.startswith(elm) for elm in ['__', '.']) or\n                        not p_el.endswith('.py') or\n                        p_el in ansible.constants.IGNORE_FILES):\n                    continue\n                p_el_path = p_el_path.replace(root, '').split('.')[0]\n                als_name = p_el_path.replace('.', '').replace('/', '', 1).replace('/', '.')\n                paths[als_name] = p_el_path\n\n        return paths"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload an ansible module.", "response": "def load_module(self, module):\n        '''\n        Introspect Ansible module.\n\n        :param module:\n        :return:\n        '''\n        m_ref = self._modules_map.get(module)\n        if m_ref is None:\n            raise LoaderError('Module \"{0}\" was not found'.format(module))\n        mod = importlib.import_module('ansible.modules{0}'.format(\n            '.'.join([elm.split('.')[0] for elm in m_ref.split(os.path.sep)])))\n\n        return mod"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of all modules that match the pattern.", "response": "def get_modules_list(self, pattern=None):\n        '''\n        Return module map references.\n        :return:\n        '''\n        if pattern and '*' not in pattern:\n            pattern = '*{0}*'.format(pattern)\n        modules = []\n        for m_name, m_path in self._modules_map.items():\n            m_path = m_path.split('.')[0]\n            m_name = '.'.join([elm for elm in m_path.split(os.path.sep) if elm])\n            if pattern and fnmatch.fnmatch(m_name, pattern) or not pattern:\n                modules.append(m_name)\n        return sorted(modules)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalls an Ansible module by invoking it.", "response": "def call(self, module, *args, **kwargs):\n        '''\n        Call an Ansible module by invoking it.\n        :param module: the name of the module.\n        :param args: Arguments to the module\n        :param kwargs: keywords to the module\n        :return:\n        '''\n\n        module = self._resolver.load_module(module)\n        if not hasattr(module, 'main'):\n            raise CommandExecutionError('This module is not callable '\n                                        '(see \"ansible.help {0}\")'.format(module.__name__.replace('ansible.modules.',\n                                                                                                  '')))\n        if args:\n            kwargs['_raw_params'] = ' '.join(args)\n        js_args = str('{{\"ANSIBLE_MODULE_ARGS\": {args}}}')  # future lint: disable=blacklisted-function\n        js_args = js_args.format(args=salt.utils.json.dumps(kwargs))\n\n        proc_out = salt.utils.timed_subprocess.TimedProc(\n            [\"echo\", \"{0}\".format(js_args)],\n            stdout=subprocess.PIPE, timeout=self.timeout)\n        proc_out.run()\n        proc_exc = salt.utils.timed_subprocess.TimedProc(\n            ['python', module.__file__],\n            stdin=proc_out.stdout, stdout=subprocess.PIPE, timeout=self.timeout)\n        proc_exc.run()\n\n        try:\n            out = salt.utils.json.loads(proc_exc.stdout)\n        except ValueError as ex:\n            out = {'Error': (proc_exc.stderr and (proc_exc.stderr + '.') or six.text_type(ex))}\n            if proc_exc.stdout:\n                out['Given JSON output'] = proc_exc.stdout\n            return out\n\n        if 'invocation' in out:\n            del out['invocation']\n\n        out['timeout'] = self.timeout\n\n        return out"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncleans out a template temp file", "response": "def __clean_tmp(sfn):\n    '''\n    Clean out a template temp file\n    '''\n    if sfn.startswith(os.path.join(tempfile.gettempdir(),\n                                   salt.utils.files.TEMPFILE_PREFIX)):\n        # Don't remove if it exists in file_roots (any saltenv)\n        all_roots = itertools.chain.from_iterable(\n                six.itervalues(__opts__['file_roots']))\n        in_roots = any(sfn.startswith(root) for root in all_roots)\n        # Only clean up files that exist\n        if os.path.exists(sfn) and not in_roots:\n            os.remove(sfn)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _splitlines_preserving_trailing_newline(str):\n    '''\n    Returns a list of the lines in the string, breaking at line boundaries and\n    preserving a trailing newline (if present).\n\n    Essentially, this works like ``str.striplines(False)`` but preserves an\n    empty line at the end. This is equivalent to the following code:\n\n    .. code-block:: python\n\n        lines = str.splitlines()\n        if str.endswith('\\n') or str.endswith('\\r'):\n            lines.append('')\n    '''\n    lines = str.splitlines()\n    if str.endswith('\\n') or str.endswith('\\r'):\n        lines.append('')\n    return lines", "response": "Returns a list of the lines in the string preserving a trailing newline."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_gid(path, follow_symlinks=True):\n    '''\n    Return the id of the group that owns a given file\n\n    path\n        file or directory of which to get the gid\n\n    follow_symlinks\n        indicated if symlinks should be followed\n\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.get_gid /etc/passwd\n\n    .. versionchanged:: 0.16.4\n        ``follow_symlinks`` option added\n    '''\n    return stats(os.path.expanduser(path), follow_symlinks=follow_symlinks).get('gid', -1)", "response": "Get the gid of a given file or directory"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the group that owns a given file or directory", "response": "def get_group(path, follow_symlinks=True):\n    '''\n    Return the group that owns a given file\n\n    path\n        file or directory of which to get the group\n\n    follow_symlinks\n        indicated if symlinks should be followed\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.get_group /etc/passwd\n\n    .. versionchanged:: 0.16.4\n        ``follow_symlinks`` option added\n    '''\n    return stats(os.path.expanduser(path), follow_symlinks=follow_symlinks).get('group', False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef user_to_uid(user):\n    '''\n    Convert user name to a uid\n\n    user\n        user name to convert to its uid\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.user_to_uid root\n    '''\n    if user is None:\n        user = salt.utils.user.get_user()\n    try:\n        if isinstance(user, int):\n            return user\n        return pwd.getpwnam(user).pw_uid\n    except KeyError:\n        return ''", "response": "Convert user name to a uid"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the uid of a given file or directory", "response": "def get_uid(path, follow_symlinks=True):\n    '''\n    Return the id of the user that owns a given file\n\n    path\n        file or directory of which to get the uid\n\n    follow_symlinks\n        indicated if symlinks should be followed\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.get_uid /etc/passwd\n\n    .. versionchanged:: 0.16.4\n        ``follow_symlinks`` option added\n    '''\n    return stats(os.path.expanduser(path), follow_symlinks=follow_symlinks).get('uid', -1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the mode of a file or directory", "response": "def get_mode(path, follow_symlinks=True):\n    '''\n    Return the mode of a file\n\n    path\n        file or directory of which to get the mode\n\n    follow_symlinks\n        indicated if symlinks should be followed\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.get_mode /etc/passwd\n\n    .. versionchanged:: 2014.1.0\n        ``follow_symlinks`` option added\n    '''\n    return stats(os.path.expanduser(path), follow_symlinks=follow_symlinks).get('mode', '')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_mode(path, mode):\n    '''\n    Set the mode of a file\n\n    path\n        file or directory of which to set the mode\n\n    mode\n        mode to set the path to\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.set_mode /etc/passwd 0644\n    '''\n    path = os.path.expanduser(path)\n\n    mode = six.text_type(mode).lstrip('0Oo')\n    if not mode:\n        mode = '0'\n    if not os.path.exists(path):\n        raise CommandExecutionError('{0}: File not found'.format(path))\n    try:\n        os.chmod(path, int(mode, 8))\n    except Exception:\n        return 'Invalid Mode ' + mode\n    return get_mode(path)", "response": "Set the mode of a file or directory of which to set the mode\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef chown(path, user, group):\n    '''\n    Chown a file, pass the file the desired user and group\n\n    path\n        path to the file or directory\n\n    user\n        user owner\n\n    group\n        group owner\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.chown /etc/passwd root root\n    '''\n    path = os.path.expanduser(path)\n\n    uid = user_to_uid(user)\n    gid = group_to_gid(group)\n    err = ''\n    if uid == '':\n        if user:\n            err += 'User does not exist\\n'\n        else:\n            uid = -1\n    if gid == '':\n        if group:\n            err += 'Group does not exist\\n'\n        else:\n            gid = -1\n    if not os.path.exists(path):\n        try:\n            # Broken symlinks will return false, but still need to be chowned\n            return os.lchown(path, uid, gid)\n        except OSError:\n            pass\n        err += 'File not found'\n    if err:\n        return err\n    return os.chown(path, uid, gid)", "response": "Chown a file pass the file the desired user and group and return the result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef chgrp(path, group):\n    '''\n    Change the group of a file\n\n    path\n        path to the file or directory\n\n    group\n        group owner\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.chgrp /etc/passwd root\n    '''\n    path = os.path.expanduser(path)\n\n    user = get_user(path)\n    return chown(path, user, group)", "response": "Change the group of a file or directory"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncompares attributes of a given file with a given list of attributes.", "response": "def _cmp_attrs(path, attrs):\n    '''\n    .. versionadded:: 2018.3.0\n\n    Compare attributes of a given file to given attributes.\n    Returns a pair (list) where first item are attributes to\n    add and second item are to be removed.\n\n    Please take into account when using this function that some minions will\n    not have lsattr installed.\n\n    path\n        path to file to compare attributes with.\n\n    attrs\n        string of attributes to compare against a given file\n    '''\n    diff = [None, None]\n\n    # lsattr for AIX is not the same thing as lsattr for linux.\n    if salt.utils.platform.is_aix():\n        return None\n\n    try:\n        lattrs = lsattr(path).get(path, '')\n    except AttributeError:\n        # lsattr not installed\n        return None\n\n    old = [chr for chr in lattrs if chr not in attrs]\n    if old:\n        diff[1] = ''.join(old)\n\n    new = [chr for chr in attrs if chr not in lattrs]\n    if new:\n        diff[0] = ''.join(new)\n\n    return diff"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef lsattr(path):\n    '''\n    .. versionadded:: 2018.3.0\n    .. versionchanged:: 2018.3.1\n        If ``lsattr`` is not installed on the system, ``None`` is returned.\n    .. versionchanged:: 2018.3.4\n        If on ``AIX``, ``None`` is returned even if in filesystem as lsattr on ``AIX``\n        is not the same thing as the linux version.\n\n    Obtain the modifiable attributes of the given file. If path\n    is to a directory, an empty list is returned.\n\n    path\n        path to file to obtain attributes of. File/directory must exist.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.lsattr foo1.txt\n    '''\n    if not salt.utils.path.which('lsattr') or salt.utils.platform.is_aix():\n        return None\n\n    if not os.path.exists(path):\n        raise SaltInvocationError(\"File or directory does not exist: \" + path)\n\n    cmd = ['lsattr', path]\n    result = __salt__['cmd.run'](cmd, ignore_retcode=True, python_shell=False)\n\n    results = {}\n    for line in result.splitlines():\n        if not line.startswith('lsattr: '):\n            vals = line.split(None, 1)\n            results[vals[1]] = re.findall(r\"[aAcCdDeijPsStTu]\", vals[0])\n\n    return results", "response": "Return the modifiable attributes of a file or directory."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nchange the attributes of a file or directory.", "response": "def chattr(*files, **kwargs):\n    '''\n    .. versionadded:: 2018.3.0\n\n    Change the attributes of files. This function accepts one or more files and\n    the following options:\n\n    operator\n        Can be wither ``add`` or ``remove``. Determines whether attributes\n        should be added or removed from files\n\n    attributes\n        One or more of the following characters: ``aAcCdDeijPsStTu``,\n        representing attributes to add to/remove from files\n\n    version\n        a version number to assign to the file(s)\n\n    flags\n        One or more of the following characters: ``RVf``, representing\n        flags to assign to chattr (recurse, verbose, suppress most errors)\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.chattr foo1.txt foo2.txt operator=add attributes=ai\n        salt '*' file.chattr foo3.txt operator=remove attributes=i version=2\n    '''\n    operator = kwargs.pop('operator', None)\n    attributes = kwargs.pop('attributes', None)\n    flags = kwargs.pop('flags', None)\n    version = kwargs.pop('version', None)\n\n    if (operator is None) or (operator not in ('add', 'remove')):\n        raise SaltInvocationError(\n            \"Need an operator: 'add' or 'remove' to modify attributes.\")\n    if attributes is None:\n        raise SaltInvocationError(\"Need attributes: [aAcCdDeijPsStTu]\")\n\n    cmd = ['chattr']\n\n    if operator == \"add\":\n        attrs = '+{0}'.format(attributes)\n    elif operator == \"remove\":\n        attrs = '-{0}'.format(attributes)\n\n    cmd.append(attrs)\n\n    if flags is not None:\n        cmd.append('-{0}'.format(flags))\n\n    if version is not None:\n        cmd.extend(['-v', version])\n\n    cmd.extend(files)\n\n    result = __salt__['cmd.run'](cmd, python_shell=False)\n\n    if bool(result):\n        raise CommandExecutionError(\n            \"chattr failed to run, possibly due to bad parameters.\")\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_sum(path, form='sha256'):\n    '''\n    Return the checksum for the given file. The following checksum algorithms\n    are supported:\n\n    * md5\n    * sha1\n    * sha224\n    * sha256 **(default)**\n    * sha384\n    * sha512\n\n    path\n        path to the file or directory\n\n    form\n        desired sum format\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.get_sum /etc/passwd sha512\n    '''\n    path = os.path.expanduser(path)\n\n    if not os.path.isfile(path):\n        return 'File not found'\n    return salt.utils.hashutils.get_hash(path, form, 4096)", "response": "Return the checksum for the given file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the hash sum of a file", "response": "def get_hash(path, form='sha256', chunk_size=65536):\n    '''\n    Get the hash sum of a file\n\n    This is better than ``get_sum`` for the following reasons:\n        - It does not read the entire file into memory.\n        - It does not return a string on error. The returned value of\n            ``get_sum`` cannot really be trusted since it is vulnerable to\n            collisions: ``get_sum(..., 'xyz') == 'Hash xyz not supported'``\n\n    path\n        path to the file or directory\n\n    form\n        desired sum format\n\n    chunk_size\n        amount to sum at once\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.get_hash /etc/shadow\n    '''\n    return salt.utils.hashutils.get_hash(os.path.expanduser(path), form, chunk_size)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the source_sum for the file_name source_hash and source_hash_name", "response": "def get_source_sum(file_name='',\n                   source='',\n                   source_hash=None,\n                   source_hash_name=None,\n                   saltenv='base'):\n    '''\n    .. versionadded:: 2016.11.0\n\n    Used by :py:func:`file.get_managed <salt.modules.file.get_managed>` to\n    obtain the hash and hash type from the parameters specified below.\n\n    file_name\n        Optional file name being managed, for matching with\n        :py:func:`file.extract_hash <salt.modules.file.extract_hash>`.\n\n    source\n        Source file, as used in :py:mod:`file <salt.states.file>` and other\n        states. If ``source_hash`` refers to a file containing hashes, then\n        this filename will be used to match a filename in that file. If the\n        ``source_hash`` is a hash expression, then this argument will be\n        ignored.\n\n    source_hash\n        Hash file/expression, as used in :py:mod:`file <salt.states.file>` and\n        other states. If this value refers to a remote URL or absolute path to\n        a local file, it will be cached and :py:func:`file.extract_hash\n        <salt.modules.file.extract_hash>` will be used to obtain a hash from\n        it.\n\n    source_hash_name\n        Specific file name to look for when ``source_hash`` refers to a remote\n        file, used to disambiguate ambiguous matches.\n\n    saltenv : base\n        Salt fileserver environment from which to retrieve the source_hash. This\n        value will only be used when ``source_hash`` refers to a file on the\n        Salt fileserver (i.e. one beginning with ``salt://``).\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.get_source_sum /tmp/foo.tar.gz source=http://mydomain.tld/foo.tar.gz source_hash=499ae16dcae71eeb7c3a30c75ea7a1a6\n        salt '*' file.get_source_sum /tmp/foo.tar.gz source=http://mydomain.tld/foo.tar.gz source_hash=https://mydomain.tld/hashes.md5\n        salt '*' file.get_source_sum /tmp/foo.tar.gz source=http://mydomain.tld/foo.tar.gz source_hash=https://mydomain.tld/hashes.md5 source_hash_name=./dir2/foo.tar.gz\n    '''\n    def _invalid_source_hash_format():\n        '''\n        DRY helper for reporting invalid source_hash input\n        '''\n        raise CommandExecutionError(\n            'Source hash {0} format is invalid. The supported formats are: '\n            '1) a hash, 2) an expression in the format <hash_type>=<hash>, or '\n            '3) either a path to a local file containing hashes, or a URI of '\n            'a remote hash file. Supported protocols for remote hash files '\n            'are: {1}. The hash may also not be of a valid length, the '\n            'following are supported hash types and lengths: {2}.'.format(\n                source_hash,\n                ', '.join(salt.utils.files.VALID_PROTOS),\n                ', '.join(\n                    ['{0} ({1})'.format(HASHES_REVMAP[x], x)\n                     for x in sorted(HASHES_REVMAP)]\n                ),\n            )\n        )\n\n    hash_fn = None\n    if os.path.isabs(source_hash):\n        hash_fn = source_hash\n    else:\n        try:\n            proto = _urlparse(source_hash).scheme\n            if proto in salt.utils.files.VALID_PROTOS:\n                hash_fn = __salt__['cp.cache_file'](source_hash, saltenv)\n                if not hash_fn:\n                    raise CommandExecutionError(\n                        'Source hash file {0} not found'.format(source_hash)\n                    )\n            else:\n                if proto != '':\n                    # Some unsupported protocol (e.g. foo://) is being used.\n                    # We'll get into this else block if a hash expression\n                    # (like md5=<md5 checksum here>), but in those cases, the\n                    # protocol will be an empty string, in which case we avoid\n                    # this error condition.\n                    _invalid_source_hash_format()\n        except (AttributeError, TypeError):\n            _invalid_source_hash_format()\n\n    if hash_fn is not None:\n        ret = extract_hash(hash_fn, '', file_name, source, source_hash_name)\n        if ret is None:\n            _invalid_source_hash_format()\n        ret['hsum'] = ret['hsum'].lower()\n        return ret\n    else:\n        # The source_hash is a hash expression\n        ret = {}\n        try:\n            ret['hash_type'], ret['hsum'] = \\\n                [x.strip() for x in source_hash.split('=', 1)]\n        except AttributeError:\n            _invalid_source_hash_format()\n        except ValueError:\n            # No hash type, try to figure out by hash length\n            if not re.match('^[{0}]+$'.format(string.hexdigits), source_hash):\n                _invalid_source_hash_format()\n            ret['hsum'] = source_hash\n            source_hash_len = len(source_hash)\n            if source_hash_len in HASHES_REVMAP:\n                ret['hash_type'] = HASHES_REVMAP[source_hash_len]\n            else:\n                _invalid_source_hash_format()\n\n        if ret['hash_type'] not in HASHES:\n            raise CommandExecutionError(\n                'Invalid hash type \\'{0}\\'. Supported hash types are: {1}. '\n                'Either remove the hash type and simply use \\'{2}\\' as the '\n                'source_hash, or change the hash type to a supported type.'\n                .format(ret['hash_type'], ', '.join(HASHES), ret['hsum'])\n            )\n        else:\n            hsum_len = len(ret['hsum'])\n            if hsum_len not in HASHES_REVMAP:\n                _invalid_source_hash_format()\n            elif hsum_len != HASHES[ret['hash_type']]:\n                raise CommandExecutionError(\n                    'Invalid length ({0}) for hash type \\'{1}\\'. Either '\n                    'remove the hash type and simply use \\'{2}\\' as the '\n                    'source_hash, or change the hash type to \\'{3}\\''.format(\n                        hsum_len,\n                        ret['hash_type'],\n                        ret['hsum'],\n                        HASHES_REVMAP[hsum_len],\n                    )\n                )\n\n        ret['hsum'] = ret['hsum'].lower()\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if a file matches the given hash string.", "response": "def check_hash(path, file_hash):\n    '''\n    Check if a file matches the given hash string\n\n    Returns ``True`` if the hash matches, otherwise ``False``.\n\n    path\n        Path to a file local to the minion.\n\n    hash\n        The hash to check against the file specified in the ``path`` argument.\n\n        .. versionchanged:: 2016.11.4\n\n        For this and newer versions the hash can be specified without an\n        accompanying hash type (e.g. ``e138491e9d5b97023cea823fe17bac22``),\n        but for earlier releases it is necessary to also specify the hash type\n        in the format ``<hash_type>=<hash_value>`` (e.g.\n        ``md5=e138491e9d5b97023cea823fe17bac22``).\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.check_hash /etc/fstab e138491e9d5b97023cea823fe17bac22\n        salt '*' file.check_hash /etc/fstab md5=e138491e9d5b97023cea823fe17bac22\n    '''\n    path = os.path.expanduser(path)\n\n    if not isinstance(file_hash, six.string_types):\n        raise SaltInvocationError('hash must be a string')\n\n    for sep in (':', '='):\n        if sep in file_hash:\n            hash_type, hash_value = file_hash.split(sep, 1)\n            break\n    else:\n        hash_value = file_hash\n        hash_len = len(file_hash)\n        hash_type = HASHES_REVMAP.get(hash_len)\n        if hash_type is None:\n            raise SaltInvocationError(\n                'Hash {0} (length: {1}) could not be matched to a supported '\n                'hash type. The supported hash types and lengths are: '\n                '{2}'.format(\n                    file_hash,\n                    hash_len,\n                    ', '.join(\n                        ['{0} ({1})'.format(HASHES_REVMAP[x], x)\n                         for x in sorted(HASHES_REVMAP)]\n                    ),\n                )\n            )\n\n    return get_hash(path, hash_type) == hash_value"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\napproximate the Unix ``find(1)`` command and return a list of paths that meet the specified criteria. The options include match criteria: .. code-block:: text name = path-glob # case sensitive iname = path-glob # case insensitive regex = path-regex # case sensitive iregex = path-regex # case insensitive type = file-types # match any listed type user = users # match any listed user group = groups # match any listed group size = [+-]number[size-unit] # default unit = byte mtime = interval # modified since date grep = regex # search file contents and/or actions: .. code-block:: text delete [= file-types] # default type = 'f' exec = command [arg ...] # where {} is replaced by pathname print [= print-opts] and/or depth criteria: .. code-block:: text maxdepth = maximum depth to transverse in path mindepth = minimum depth to transverse before checking files or directories The default action is ``print=path`` ``path-glob``: .. code-block:: text * = match zero or more chars ? = match any char [abc] = match a, b, or c [!abc] or [^abc] = match anything except a, b, and c [x-y] = match chars x through y [!x-y] or [^x-y] = match anything except chars x through y {a,b,c} = match a or b or c ``path-regex``: a Python Regex (regular expression) pattern to match pathnames ``file-types``: a string of one or more of the following: .. code-block:: text a: all file types b: block device c: character device d: directory p: FIFO (named pipe) f: plain file l: symlink s: socket ``users``: a space and/or comma separated list of user names and/or uids ``groups``: a space and/or comma separated list of group names and/or gids ``size-unit``: .. code-block:: text b: bytes k: kilobytes m: megabytes g: gigabytes t: terabytes interval: .. code-block:: text [<num>w] [<num>d] [<num>h] [<num>m] [<num>s] where: w: week d: day h: hour m: minute s: second print-opts: a comma and/or space separated list of one or more of the following: .. code-block:: text group: group name md5: MD5 digest of file contents mode: file permissions (as integer) mtime: last modification time (as time_t) name: file basename path: file absolute path size: file size in bytes type: file type user: user name CLI Examples: .. code-block:: bash salt '*' file.find / type=f name=\\\\*.bak size=+10m salt '*' file.find /var mtime=+30d size=+10m print=path,size,mtime salt '*' file.find /var/log name=\\\\*.[0-9] mtime=+30d size=+10m delete", "response": "def find(path, *args, **kwargs):\n    '''\n    Approximate the Unix ``find(1)`` command and return a list of paths that\n    meet the specified criteria.\n\n    The options include match criteria:\n\n    .. code-block:: text\n\n        name    = path-glob                 # case sensitive\n        iname   = path-glob                 # case insensitive\n        regex   = path-regex                # case sensitive\n        iregex  = path-regex                # case insensitive\n        type    = file-types                # match any listed type\n        user    = users                     # match any listed user\n        group   = groups                    # match any listed group\n        size    = [+-]number[size-unit]     # default unit = byte\n        mtime   = interval                  # modified since date\n        grep    = regex                     # search file contents\n\n    and/or actions:\n\n    .. code-block:: text\n\n        delete [= file-types]               # default type = 'f'\n        exec    = command [arg ...]         # where {} is replaced by pathname\n        print  [= print-opts]\n\n    and/or depth criteria:\n\n    .. code-block:: text\n\n        maxdepth = maximum depth to transverse in path\n        mindepth = minimum depth to transverse before checking files or directories\n\n    The default action is ``print=path``\n\n    ``path-glob``:\n\n    .. code-block:: text\n\n        *                = match zero or more chars\n        ?                = match any char\n        [abc]            = match a, b, or c\n        [!abc] or [^abc] = match anything except a, b, and c\n        [x-y]            = match chars x through y\n        [!x-y] or [^x-y] = match anything except chars x through y\n        {a,b,c}          = match a or b or c\n\n    ``path-regex``: a Python Regex (regular expression) pattern to match pathnames\n\n    ``file-types``: a string of one or more of the following:\n\n    .. code-block:: text\n\n        a: all file types\n        b: block device\n        c: character device\n        d: directory\n        p: FIFO (named pipe)\n        f: plain file\n        l: symlink\n        s: socket\n\n    ``users``: a space and/or comma separated list of user names and/or uids\n\n    ``groups``: a space and/or comma separated list of group names and/or gids\n\n    ``size-unit``:\n\n    .. code-block:: text\n\n        b: bytes\n        k: kilobytes\n        m: megabytes\n        g: gigabytes\n        t: terabytes\n\n    interval:\n\n    .. code-block:: text\n\n        [<num>w] [<num>d] [<num>h] [<num>m] [<num>s]\n\n        where:\n            w: week\n            d: day\n            h: hour\n            m: minute\n            s: second\n\n    print-opts: a comma and/or space separated list of one or more of the\n    following:\n\n    .. code-block:: text\n\n        group: group name\n        md5:   MD5 digest of file contents\n        mode:  file permissions (as integer)\n        mtime: last modification time (as time_t)\n        name:  file basename\n        path:  file absolute path\n        size:  file size in bytes\n        type:  file type\n        user:  user name\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' file.find / type=f name=\\\\*.bak size=+10m\n        salt '*' file.find /var mtime=+30d size=+10m print=path,size,mtime\n        salt '*' file.find /var/log name=\\\\*.[0-9] mtime=+30d size=+10m delete\n    '''\n    if 'delete' in args:\n        kwargs['delete'] = 'f'\n    elif 'print' in args:\n        kwargs['print'] = 'path'\n\n    try:\n        finder = salt.utils.find.Finder(kwargs)\n    except ValueError as ex:\n        return 'error: {0}'.format(ex)\n\n    ret = [item for i in [finder.find(p) for p in glob.glob(os.path.expanduser(path))] for item in i]\n    ret.sort()\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _sed_esc(string, escape_all=False):\n    '''\n    Escape single quotes and forward slashes\n    '''\n    special_chars = \"^.[$()|*+?{\"\n    string = string.replace(\"'\", \"'\\\"'\\\"'\").replace(\"/\", \"\\\\/\")\n    if escape_all is True:\n        for char in special_chars:\n            string = string.replace(char, \"\\\\\" + char)\n    return string", "response": "Escape single quotes forward slashes and slashes and escape all special characters in a string."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sed_contains(path,\n                 text,\n                 limit='',\n                 flags='g'):\n    '''\n    .. deprecated:: 0.17.0\n       Use :func:`search` instead.\n\n    Return True if the file at ``path`` contains ``text``. Utilizes sed to\n    perform the search (line-wise search).\n\n    Note: the ``p`` flag will be added to any flags you pass in.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.contains /etc/crontab 'mymaintenance.sh'\n    '''\n    # Largely inspired by Fabric's contrib.files.contains()\n    path = os.path.expanduser(path)\n\n    if not os.path.exists(path):\n        return False\n\n    before = _sed_esc(six.text_type(text), False)\n    limit = _sed_esc(six.text_type(limit), False)\n    options = '-n -r -e'\n    if sys.platform == 'darwin':\n        options = options.replace('-r', '-E')\n\n    cmd = ['sed']\n    cmd.extend(salt.utils.args.shlex_split(options))\n    cmd.append(\n        r'{limit}s/{before}/$/{flags}'.format(\n            limit='/{0}/ '.format(limit) if limit else '',\n            before=before,\n            flags='p{0}'.format(flags)\n        )\n    )\n    cmd.append(path)\n\n    result = __salt__['cmd.run'](cmd, python_shell=False)\n\n    return bool(result)", "response": "Return True if the file at path contains text."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a copy of the file with the given pattern.", "response": "def psed(path,\n         before,\n         after,\n         limit='',\n         backup='.bak',\n         flags='gMS',\n         escape_all=False,\n         multi=False):\n    '''\n    .. deprecated:: 0.17.0\n       Use :py:func:`~salt.modules.file.replace` instead.\n\n    Make a simple edit to a file (pure Python version)\n\n    Equivalent to:\n\n    .. code-block:: bash\n\n        sed <backup> <options> \"/<limit>/ s/<before>/<after>/<flags> <file>\"\n\n    path\n        The full path to the file to be edited\n    before\n        A pattern to find in order to replace with ``after``\n    after\n        Text that will replace ``before``\n    limit : ``''``\n        An initial pattern to search for before searching for ``before``\n    backup : ``.bak``\n        The file will be backed up before edit with this file extension;\n        **WARNING:** each time ``sed``/``comment``/``uncomment`` is called will\n        overwrite this backup\n    flags : ``gMS``\n        Flags to modify the search. Valid values are:\n          - ``g``: Replace all occurrences of the pattern, not just the first.\n          - ``I``: Ignore case.\n          - ``L``: Make ``\\\\w``, ``\\\\W``, ``\\\\b``, ``\\\\B``, ``\\\\s`` and ``\\\\S``\n            dependent on the locale.\n          - ``M``: Treat multiple lines as a single line.\n          - ``S``: Make `.` match all characters, including newlines.\n          - ``U``: Make ``\\\\w``, ``\\\\W``, ``\\\\b``, ``\\\\B``, ``\\\\d``, ``\\\\D``,\n            ``\\\\s`` and ``\\\\S`` dependent on Unicode.\n          - ``X``: Verbose (whitespace is ignored).\n    multi: ``False``\n        If True, treat the entire file as a single line\n\n    Forward slashes and single quotes will be escaped automatically in the\n    ``before`` and ``after`` patterns.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.sed /etc/httpd/httpd.conf 'LogLevel warn' 'LogLevel info'\n    '''\n    # Largely inspired by Fabric's contrib.files.sed()\n    # XXX:dc: Do we really want to always force escaping?\n    #\n    # Mandate that before and after are strings\n    path = os.path.expanduser(path)\n\n    multi = bool(multi)\n\n    before = six.text_type(before)\n    after = six.text_type(after)\n    before = _sed_esc(before, escape_all)\n    # The pattern to replace with does not need to be escaped!!!\n    # after = _sed_esc(after, escape_all)\n    limit = _sed_esc(limit, escape_all)\n\n    shutil.copy2(path, '{0}{1}'.format(path, backup))\n\n    with salt.utils.files.fopen(path, 'w') as ofile:\n        with salt.utils.files.fopen('{0}{1}'.format(path, backup), 'r') as ifile:\n            if multi is True:\n                for line in ifile.readline():\n                    ofile.write(\n                        salt.utils.stringutils.to_str(\n                            _psed(\n                                salt.utils.stringutils.to_unicode(line),\n                                before,\n                                after,\n                                limit,\n                                flags\n                            )\n                        )\n                    )\n            else:\n                ofile.write(\n                    salt.utils.stringutils.to_str(\n                        _psed(\n                            salt.utils.stringutils.to_unicode(ifile.read()),\n                            before,\n                            after,\n                            limit,\n                            flags\n                        )\n                    )\n                )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndoing the actual work for file. psed", "response": "def _psed(text,\n          before,\n          after,\n          limit,\n          flags):\n    '''\n    Does the actual work for file.psed, so that single lines can be passed in\n    '''\n    atext = text\n    if limit:\n        limit = re.compile(limit)\n        comps = text.split(limit)\n        atext = ''.join(comps[1:])\n\n    count = 1\n    if 'g' in flags:\n        count = 0\n        flags = flags.replace('g', '')\n\n    aflags = 0\n    for flag in flags:\n        aflags |= RE_FLAG_TABLE[flag]\n\n    before = re.compile(before, flags=aflags)\n    text = re.sub(before, after, atext, count=count)\n\n    return text"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a copy of the file with the specified lines commented out.", "response": "def comment(path,\n            regex,\n            char='#',\n            backup='.bak'):\n    '''\n    .. deprecated:: 0.17.0\n       Use :py:func:`~salt.modules.file.replace` instead.\n\n    Comment out specified lines in a file\n\n    path\n        The full path to the file to be edited\n    regex\n        A regular expression used to find the lines that are to be commented;\n        this pattern will be wrapped in parenthesis and will move any\n        preceding/trailing ``^`` or ``$`` characters outside the parenthesis\n        (e.g., the pattern ``^foo$`` will be rewritten as ``^(foo)$``)\n    char : ``#``\n        The character to be inserted at the beginning of a line in order to\n        comment it out\n    backup : ``.bak``\n        The file will be backed up before edit with this file extension\n\n        .. warning::\n\n            This backup will be overwritten each time ``sed`` / ``comment`` /\n            ``uncomment`` is called. Meaning the backup will only be useful\n            after the first invocation.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.comment /etc/modules pcspkr\n    '''\n    return comment_line(path=path,\n                        regex=regex,\n                        char=char,\n                        cmnt=True,\n                        backup=backup)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_flags(flags):\n    '''\n    Return an integer appropriate for use as a flag for the re module from a\n    list of human-readable strings\n\n    .. code-block:: python\n\n        >>> _get_flags(['MULTILINE', 'IGNORECASE'])\n        10\n        >>> _get_flags('MULTILINE')\n        8\n        >>> _get_flags(2)\n        2\n    '''\n    if isinstance(flags, six.string_types):\n        flags = [flags]\n\n    if isinstance(flags, Iterable) and not isinstance(flags, Mapping):\n        _flags_acc = []\n        for flag in flags:\n            _flag = getattr(re, six.text_type(flag).upper())\n\n            if not isinstance(_flag, six.integer_types):\n                raise SaltInvocationError(\n                    'Invalid re flag given: {0}'.format(flag)\n                )\n\n            _flags_acc.append(_flag)\n\n        return reduce(operator.__or__, _flags_acc)\n    elif isinstance(flags, six.integer_types):\n        return flags\n    else:\n        raise SaltInvocationError(\n            'Invalid re flags: \"{0}\", must be given either as a single flag '\n            'string, a list of strings, or as an integer'.format(flags)\n        )", "response": "Return an integer appropriate for use as a flag for the re module from a\n    list of human - readable strings\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncombining flags and new_flags and return the result.", "response": "def _add_flags(flags, new_flags):\n    '''\n    Combine ``flags`` and ``new_flags``\n    '''\n    flags = _get_flags(flags)\n    new_flags = _get_flags(new_flags)\n    return flags | new_flags"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _mkstemp_copy(path,\n                  preserve_inode=True):\n    '''\n    Create a temp file and move/copy the contents of ``path`` to the temp file.\n    Return the path to the temp file.\n\n    path\n        The full path to the file whose contents will be moved/copied to a temp file.\n        Whether it's moved or copied depends on the value of ``preserve_inode``.\n    preserve_inode\n        Preserve the inode of the file, so that any hard links continue to share the\n        inode with the original filename. This works by *copying* the file, reading\n        from the copy, and writing to the file at the original inode. If ``False``, the\n        file will be *moved* rather than copied, and a new file will be written to a\n        new inode, but using the original filename. Hard links will then share an inode\n        with the backup, instead (if using ``backup`` to create a backup copy).\n        Default is ``True``.\n    '''\n    temp_file = None\n    # Create the temp file\n    try:\n        temp_file = salt.utils.files.mkstemp(prefix=salt.utils.files.TEMPFILE_PREFIX)\n    except (OSError, IOError) as exc:\n        raise CommandExecutionError(\n            \"Unable to create temp file. \"\n            \"Exception: {0}\".format(exc)\n            )\n    # use `copy` to preserve the inode of the\n    # original file, and thus preserve hardlinks\n    # to the inode. otherwise, use `move` to\n    # preserve prior behavior, which results in\n    # writing the file to a new inode.\n    if preserve_inode:\n        try:\n            shutil.copy2(path, temp_file)\n        except (OSError, IOError) as exc:\n            raise CommandExecutionError(\n                \"Unable to copy file '{0}' to the \"\n                \"temp file '{1}'. \"\n                \"Exception: {2}\".format(path, temp_file, exc)\n                )\n    else:\n        try:\n            shutil.move(path, temp_file)\n        except (OSError, IOError) as exc:\n            raise CommandExecutionError(\n                \"Unable to move file '{0}' to the \"\n                \"temp file '{1}'. \"\n                \"Exception: {2}\".format(path, temp_file, exc)\n                )\n\n    return temp_file", "response": "Create a temp file and move it to the temp file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn True if src and probe at least matches at the beginning till some point.", "response": "def _starts_till(src, probe, strip_comments=True):\n    '''\n    Returns True if src and probe at least matches at the beginning till some point.\n    '''\n    def _strip_comments(txt):\n        '''\n        Strip possible comments.\n        Usually comments are one or two symbols at the beginning of the line, separated with space\n        '''\n        buff = txt.split(\" \", 1)\n        return len(buff) == 2 and len(buff[0]) < 2 and buff[1] or txt\n\n    def _to_words(txt):\n        '''\n        Split by words\n        '''\n        return txt and [w for w in txt.strip().split(\" \") if w.strip()] or txt\n\n    no_match = -1\n    equal = 0\n    if not src or not probe:\n        return no_match\n\n    src = src.rstrip('\\n\\r')\n    probe = probe.rstrip('\\n\\r')\n    if src == probe:\n        return equal\n\n    src = _to_words(strip_comments and _strip_comments(src) or src)\n    probe = _to_words(strip_comments and _strip_comments(probe) or probe)\n\n    a_buff, b_buff = len(src) < len(probe) and (src, probe) or (probe, src)\n    b_buff = ' '.join(b_buff)\n    for idx in range(len(a_buff)):\n        prb = ' '.join(a_buff[:-(idx + 1)])\n        if prb and b_buff.startswith(prb):\n            return idx\n\n    return no_match"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nexpand regular expression to static match.", "response": "def _regex_to_static(src, regex):\n    '''\n    Expand regular expression to static match.\n    '''\n    if not src or not regex:\n        return None\n\n    try:\n        compiled = re.compile(regex, re.DOTALL)\n        src = [line for line in src if compiled.search(line) or line.count(regex)]\n    except Exception as ex:\n        raise CommandExecutionError(\"{0}: '{1}'\".format(_get_error_message(ex), regex))\n\n    return src and src or []"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _assert_occurrence(probe, target, amount=1):\n    '''\n    Raise an exception, if there are different amount of specified occurrences in src.\n    '''\n    occ = len(probe)\n    if occ > amount:\n        msg = 'more than'\n    elif occ < amount:\n        msg = 'less than'\n    elif not occ:\n        msg = 'no'\n    else:\n        msg = None\n\n    if msg:\n        raise CommandExecutionError('Found {0} expected occurrences in \"{1}\" expression'.format(msg, target))\n\n    return occ", "response": "Raise an exception if there are different amount of specified occurrences in src."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _set_line_indent(src, line, indent):\n    '''\n    Indent the line with the source line.\n    '''\n    if not indent:\n        return line\n\n    idt = []\n    for c in src:\n        if c not in ['\\t', ' ']:\n            break\n        idt.append(c)\n\n    return ''.join(idt) + line.lstrip()", "response": "Indent the line with the source line."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _set_line_eol(src, line):\n    '''\n    Add line ending\n    '''\n    line_ending = _get_eol(src) or os.linesep\n    return line.rstrip() + line_ending", "response": "Set the line ending of the line."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef line(path, content=None, match=None, mode=None, location=None,\n         before=None, after=None, show_changes=True, backup=False,\n         quiet=False, indent=True):\n    '''\n    .. versionadded:: 2015.8.0\n\n    Edit a line in the configuration file. The ``path`` and ``content``\n    arguments are required, as well as passing in one of the ``mode``\n    options.\n\n    path\n        Filesystem path to the file to be edited.\n\n    content\n        Content of the line. Allowed to be empty if mode=delete.\n\n    match\n        Match the target line for an action by\n        a fragment of a string or regular expression.\n\n        If neither ``before`` nor ``after`` are provided, and ``match``\n        is also ``None``, match becomes the ``content`` value.\n\n    mode\n        Defines how to edit a line. One of the following options is\n        required:\n\n        - ensure\n            If line does not exist, it will be added. This is based on the\n            ``content`` argument.\n        - replace\n            If line already exists, it will be replaced.\n        - delete\n            Delete the line, once found.\n        - insert\n            Insert a line.\n\n        .. note::\n\n            If ``mode=insert`` is used, at least one of the following\n            options must also be defined: ``location``, ``before``, or\n            ``after``. If ``location`` is used, it takes precedence\n            over the other two options.\n\n    location\n        Defines where to place content in the line. Note this option is only\n        used when ``mode=insert`` is specified. If a location is passed in, it\n        takes precedence over both the ``before`` and ``after`` kwargs. Valid\n        locations are:\n\n        - start\n            Place the content at the beginning of the file.\n        - end\n            Place the content at the end of the file.\n\n    before\n        Regular expression or an exact case-sensitive fragment of the string.\n        This option is only used when either the ``ensure`` or ``insert`` mode\n        is defined.\n\n    after\n        Regular expression or an exact case-sensitive fragment of the string.\n        This option is only used when either the ``ensure`` or ``insert`` mode\n        is defined.\n\n    show_changes\n        Output a unified diff of the old file and the new file.\n        If ``False`` return a boolean if any changes were made.\n        Default is ``True``\n\n        .. note::\n            Using this option will store two copies of the file in-memory\n            (the original version and the edited version) in order to generate the diff.\n\n    backup\n        Create a backup of the original file with the extension:\n        \"Year-Month-Day-Hour-Minutes-Seconds\".\n\n    quiet\n        Do not raise any exceptions. E.g. ignore the fact that the file that is\n        tried to be edited does not exist and nothing really happened.\n\n    indent\n        Keep indentation with the previous line. This option is not considered when\n        the ``delete`` mode is specified.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.line /etc/nsswitch.conf \"networks:\\tfiles dns\" after=\"hosts:.*?\" mode='ensure'\n\n    .. note::\n\n        If an equal sign (``=``) appears in an argument to a Salt command, it is\n        interpreted as a keyword argument in the format of ``key=val``. That\n        processing can be bypassed in order to pass an equal sign through to the\n        remote shell command by manually specifying the kwarg:\n\n        .. code-block:: bash\n\n            salt '*' file.line /path/to/file content=\"CREATEMAIL_SPOOL=no\" match=\"CREATE_MAIL_SPOOL=yes\" mode=\"replace\"\n    '''\n    path = os.path.realpath(os.path.expanduser(path))\n    if not os.path.isfile(path):\n        if not quiet:\n            raise CommandExecutionError('File \"{0}\" does not exists or is not a file.'.format(path))\n        return False  # No changes had happened\n\n    mode = mode and mode.lower() or mode\n    if mode not in ['insert', 'ensure', 'delete', 'replace']:\n        if mode is None:\n            raise CommandExecutionError('Mode was not defined. How to process the file?')\n        else:\n            raise CommandExecutionError('Unknown mode: \"{0}\"'.format(mode))\n\n    # We've set the content to be empty in the function params but we want to make sure\n    # it gets passed when needed. Feature #37092\n    empty_content_modes = ['delete']\n    if mode not in empty_content_modes and content is None:\n        raise CommandExecutionError('Content can only be empty if mode is \"{0}\"'.format(', '.join(empty_content_modes)))\n    del empty_content_modes\n\n    # Before/after has privilege. If nothing defined, match is used by content.\n    if before is None and after is None and not match:\n        match = content\n\n    with salt.utils.files.fopen(path, mode='r') as fp_:\n        body = salt.utils.data.decode_list(fp_.readlines())\n    body_before = hashlib.sha256(salt.utils.stringutils.to_bytes(''.join(body))).hexdigest()\n    # Add empty line at the end if last line ends with eol.\n    # Allows simpler code\n    if body and _get_eol(body[-1]):\n        body.append('')\n\n    after = _regex_to_static(body, after)\n    before = _regex_to_static(body, before)\n    match = _regex_to_static(body, match)\n\n    if os.stat(path).st_size == 0 and mode in ('delete', 'replace'):\n        log.warning('Cannot find text to %s. File \\'%s\\' is empty.', mode, path)\n        body = []\n    elif mode == 'delete' and match:\n        body = [line for line in body if line != match[0]]\n    elif mode == 'replace' and match:\n        idx = body.index(match[0])\n        file_line = body.pop(idx)\n        body.insert(idx, _set_line_indent(file_line, content, indent))\n    elif mode == 'insert':\n        if not location and not before and not after:\n            raise CommandExecutionError('On insert must be defined either \"location\" or \"before/after\" conditions.')\n\n        if not location:\n            if before and after:\n                _assert_occurrence(before, 'before')\n                _assert_occurrence(after, 'after')\n\n                out = []\n                in_range = False\n                for line in body:\n                    if line == after[0]:\n                        in_range = True\n                    elif line == before[0] and in_range:\n                        cnd = _set_line_indent(line, content, indent)\n                        out.append(cnd)\n                    out.append(line)\n                body = out\n\n            if before and not after:\n                _assert_occurrence(before, 'before')\n\n                idx = body.index(before[0])\n                body = _insert_line_before(idx, body, content, indent)\n\n            elif after and not before:\n                _assert_occurrence(after, 'after')\n\n                idx = body.index(after[0])\n                body = _insert_line_after(idx, body, content, indent)\n\n        else:\n            if location == 'start':\n                if body:\n                    body.insert(0, _set_line_eol(body[0], content))\n                else:\n                    body.append(content + os.linesep)\n            elif location == 'end':\n                body.append(_set_line_indent(body[-1], content, indent) if body else content)\n\n    elif mode == 'ensure':\n\n        if before and after:\n            _assert_occurrence(before, 'before')\n            _assert_occurrence(after, 'after')\n\n            is_there = bool([l for l in body if l.count(content)])\n            if not is_there:\n                idx = body.index(after[0])\n                if idx < (len(body) - 1) and body[idx + 1] == before[0]:\n                    cnd = _set_line_indent(body[idx], content, indent)\n                    body.insert(idx + 1, cnd)\n                else:\n                    raise CommandExecutionError('Found more than one line between '\n                                                'boundaries \"before\" and \"after\".')\n\n        elif before and not after:\n            _assert_occurrence(before, 'before')\n\n            idx = body.index(before[0])\n            body = _insert_line_before(idx, body, content, indent)\n\n        elif not before and after:\n            _assert_occurrence(after, 'after')\n\n            idx = body.index(after[0])\n            body = _insert_line_after(idx, body, content, indent)\n\n        else:\n            raise CommandExecutionError(\"Wrong conditions? \"\n                                        \"Unable to ensure line without knowing \"\n                                        \"where to put it before and/or after.\")\n\n    if body:\n        for idx, line in enumerate(body):\n            if not _get_eol(line) and idx+1 < len(body):\n                prev = idx and idx-1 or 1\n                body[idx] = _set_line_eol(body[prev], line)\n        # We do not need empty line at the end anymore\n        if '' == body[-1]:\n            body.pop()\n\n    changed = body_before != hashlib.sha256(salt.utils.stringutils.to_bytes(''.join(body))).hexdigest()\n\n    if backup and changed and __opts__['test'] is False:\n        try:\n            temp_file = _mkstemp_copy(path=path, preserve_inode=True)\n            shutil.move(temp_file, '{0}.{1}'.format(path, time.strftime('%Y-%m-%d-%H-%M-%S', time.localtime())))\n        except (OSError, IOError) as exc:\n            raise CommandExecutionError(\"Unable to create the backup file of {0}. Exception: {1}\".format(path, exc))\n\n    changes_diff = None\n\n    if changed:\n        if show_changes:\n            with salt.utils.files.fopen(path, 'r') as fp_:\n                path_content = salt.utils.data.decode_list(fp_.read().splitlines(True))\n            changes_diff = __utils__['stringutils.get_diff'](path_content, body)\n        if __opts__['test'] is False:\n            fh_ = None\n            try:\n                # Make sure we match the file mode from salt.utils.files.fopen\n                if six.PY2 and salt.utils.platform.is_windows():\n                    mode = 'wb'\n                    body = salt.utils.data.encode_list(body)\n                else:\n                    mode = 'w'\n                    body = salt.utils.data.decode_list(body, to_str=True)\n                fh_ = salt.utils.atomicfile.atomic_open(path, mode)\n                fh_.writelines(body)\n            finally:\n                if fh_:\n                    fh_.close()\n\n    return show_changes and changes_diff or changed", "response": "Edit a line in the configuration file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef replace(path,\n            pattern,\n            repl,\n            count=0,\n            flags=8,\n            bufsize=1,\n            append_if_not_found=False,\n            prepend_if_not_found=False,\n            not_found_content=None,\n            backup='.bak',\n            dry_run=False,\n            search_only=False,\n            show_changes=True,\n            ignore_if_missing=False,\n            preserve_inode=True,\n            backslash_literal=False,\n            ):\n    '''\n    .. versionadded:: 0.17.0\n\n    Replace occurrences of a pattern in a file. If ``show_changes`` is\n    ``True``, then a diff of what changed will be returned, otherwise a\n    ``True`` will be returned when changes are made, and ``False`` when\n    no changes are made.\n\n    This is a pure Python implementation that wraps Python's :py:func:`~re.sub`.\n\n    path\n        Filesystem path to the file to be edited. If a symlink is specified, it\n        will be resolved to its target.\n\n    pattern\n        A regular expression, to be matched using Python's\n        :py:func:`~re.search`.\n\n    repl\n        The replacement text\n\n    count : 0\n        Maximum number of pattern occurrences to be replaced. If count is a\n        positive integer ``n``, only ``n`` occurrences will be replaced,\n        otherwise all occurrences will be replaced.\n\n    flags (list or int)\n        A list of flags defined in the ``re`` module documentation from the\n        Python standard library. Each list item should be a string that will\n        correlate to the human-friendly flag name. E.g., ``['IGNORECASE',\n        'MULTILINE']``. Optionally, ``flags`` may be an int, with a value\n        corresponding to the XOR (``|``) of all the desired flags. Defaults to\n        8 (which supports 'MULTILINE').\n\n    bufsize (int or str)\n        How much of the file to buffer into memory at once. The\n        default value ``1`` processes one line at a time. The special value\n        ``file`` may be specified which will read the entire file into memory\n        before processing.\n\n    append_if_not_found : False\n        .. versionadded:: 2014.7.0\n\n        If set to ``True``, and pattern is not found, then the content will be\n        appended to the file.\n\n    prepend_if_not_found : False\n        .. versionadded:: 2014.7.0\n\n        If set to ``True`` and pattern is not found, then the content will be\n        prepended to the file.\n\n    not_found_content\n        .. versionadded:: 2014.7.0\n\n        Content to use for append/prepend if not found. If None (default), uses\n        ``repl``. Useful when ``repl`` uses references to group in pattern.\n\n    backup : .bak\n        The file extension to use for a backup of the file before editing. Set\n        to ``False`` to skip making a backup.\n\n    dry_run : False\n        If set to ``True``, no changes will be made to the file, the function\n        will just return the changes that would have been made (or a\n        ``True``/``False`` value if ``show_changes`` is set to ``False``).\n\n    search_only : False\n        If set to true, this no changes will be performed on the file, and this\n        function will simply return ``True`` if the pattern was matched, and\n        ``False`` if not.\n\n    show_changes : True\n        If ``True``, return a diff of changes made. Otherwise, return ``True``\n        if changes were made, and ``False`` if not.\n\n        .. note::\n            Using this option will store two copies of the file in memory (the\n            original version and the edited version) in order to generate the\n            diff. This may not normally be a concern, but could impact\n            performance if used with large files.\n\n    ignore_if_missing : False\n        .. versionadded:: 2015.8.0\n\n        If set to ``True``, this function will simply return ``False``\n        if the file doesn't exist. Otherwise, an error will be thrown.\n\n    preserve_inode : True\n        .. versionadded:: 2015.8.0\n\n        Preserve the inode of the file, so that any hard links continue to\n        share the inode with the original filename. This works by *copying* the\n        file, reading from the copy, and writing to the file at the original\n        inode. If ``False``, the file will be *moved* rather than copied, and a\n        new file will be written to a new inode, but using the original\n        filename. Hard links will then share an inode with the backup, instead\n        (if using ``backup`` to create a backup copy).\n\n    backslash_literal : False\n        .. versionadded:: 2016.11.7\n\n        Interpret backslashes as literal backslashes for the repl and not\n        escape characters.  This will help when using append/prepend so that\n        the backslashes are not interpreted for the repl on the second run of\n        the state.\n\n    If an equal sign (``=``) appears in an argument to a Salt command it is\n    interpreted as a keyword argument in the format ``key=val``. That\n    processing can be bypassed in order to pass an equal sign through to the\n    remote shell command by manually specifying the kwarg:\n\n    .. code-block:: bash\n\n        salt '*' file.replace /path/to/file pattern='=' repl=':'\n        salt '*' file.replace /path/to/file pattern=\"bind-address\\\\s*=\" repl='bind-address:'\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' file.replace /etc/httpd/httpd.conf pattern='LogLevel warn' repl='LogLevel info'\n        salt '*' file.replace /some/file pattern='before' repl='after' flags='[MULTILINE, IGNORECASE]'\n    '''\n    symlink = False\n    if is_link(path):\n        symlink = True\n        target_path = os.readlink(path)\n        given_path = os.path.expanduser(path)\n\n    path = os.path.realpath(os.path.expanduser(path))\n\n    if not os.path.exists(path):\n        if ignore_if_missing:\n            return False\n        else:\n            raise SaltInvocationError('File not found: {0}'.format(path))\n\n    if not __utils__['files.is_text'](path):\n        raise SaltInvocationError(\n            'Cannot perform string replacements on a binary file: {0}'\n            .format(path)\n        )\n\n    if search_only and (append_if_not_found or prepend_if_not_found):\n        raise SaltInvocationError(\n            'search_only cannot be used with append/prepend_if_not_found'\n        )\n\n    if append_if_not_found and prepend_if_not_found:\n        raise SaltInvocationError(\n            'Only one of append and prepend_if_not_found is permitted'\n        )\n\n    flags_num = _get_flags(flags)\n    cpattern = re.compile(salt.utils.stringutils.to_bytes(pattern), flags_num)\n    filesize = os.path.getsize(path)\n    if bufsize == 'file':\n        bufsize = filesize\n\n    # Search the file; track if any changes have been made for the return val\n    has_changes = False\n    orig_file = []  # used for show_changes and change detection\n    new_file = []  # used for show_changes and change detection\n    if not salt.utils.platform.is_windows():\n        pre_user = get_user(path)\n        pre_group = get_group(path)\n        pre_mode = salt.utils.files.normalize_mode(get_mode(path))\n\n    # Avoid TypeErrors by forcing repl to be bytearray related to mmap\n    # Replacement text may contains integer: 123 for example\n    repl = salt.utils.stringutils.to_bytes(six.text_type(repl))\n    if not_found_content:\n        not_found_content = salt.utils.stringutils.to_bytes(not_found_content)\n\n    found = False\n    temp_file = None\n    content = salt.utils.stringutils.to_unicode(not_found_content) \\\n        if not_found_content and (prepend_if_not_found or append_if_not_found) \\\n        else salt.utils.stringutils.to_unicode(repl)\n\n    try:\n        # First check the whole file, determine whether to make the replacement\n        # Searching first avoids modifying the time stamp if there are no changes\n        r_data = None\n        # Use a read-only handle to open the file\n        with salt.utils.files.fopen(path,\n                                    mode='rb',\n                                    buffering=bufsize) as r_file:\n            try:\n                # mmap throws a ValueError if the file is empty.\n                r_data = mmap.mmap(r_file.fileno(),\n                                   0,\n                                   access=mmap.ACCESS_READ)\n            except (ValueError, mmap.error):\n                # size of file in /proc is 0, but contains data\n                r_data = salt.utils.stringutils.to_bytes(\"\".join(r_file))\n            if search_only:\n                # Just search; bail as early as a match is found\n                if re.search(cpattern, r_data):\n                    return True  # `with` block handles file closure\n                else:\n                    return False\n            else:\n                result, nrepl = re.subn(cpattern,\n                                        repl.replace('\\\\', '\\\\\\\\') if backslash_literal else repl,\n                                        r_data,\n                                        count)\n\n                # found anything? (even if no change)\n                if nrepl > 0:\n                    found = True\n                    # Identity check the potential change\n                    has_changes = True if pattern != repl else has_changes\n\n                if prepend_if_not_found or append_if_not_found:\n                    # Search for content, to avoid pre/appending the\n                    # content if it was pre/appended in a previous run.\n                    if re.search(salt.utils.stringutils.to_bytes('^{0}($|(?=\\r\\n))'.format(re.escape(content))),\n                                 r_data,\n                                 flags=flags_num):\n                        # Content was found, so set found.\n                        found = True\n\n                orig_file = r_data.read(filesize).splitlines(True) \\\n                    if isinstance(r_data, mmap.mmap) \\\n                    else r_data.splitlines(True)\n                new_file = result.splitlines(True)\n\n    except (OSError, IOError) as exc:\n        raise CommandExecutionError(\n            \"Unable to open file '{0}'. \"\n            \"Exception: {1}\".format(path, exc)\n            )\n    finally:\n        if r_data and isinstance(r_data, mmap.mmap):\n            r_data.close()\n\n    if has_changes and not dry_run:\n        # Write the replacement text in this block.\n        try:\n            # Create a copy to read from and to use as a backup later\n            temp_file = _mkstemp_copy(path=path,\n                                      preserve_inode=preserve_inode)\n        except (OSError, IOError) as exc:\n            raise CommandExecutionError(\"Exception: {0}\".format(exc))\n\n        r_data = None\n        try:\n            # Open the file in write mode\n            with salt.utils.files.fopen(path,\n                        mode='w',\n                        buffering=bufsize) as w_file:\n                try:\n                    # Open the temp file in read mode\n                    with salt.utils.files.fopen(temp_file,\n                                          mode='r',\n                                          buffering=bufsize) as r_file:\n                        r_data = mmap.mmap(r_file.fileno(),\n                                           0,\n                                           access=mmap.ACCESS_READ)\n                        result, nrepl = re.subn(cpattern,\n                                                repl.replace('\\\\', '\\\\\\\\') if backslash_literal else repl,\n                                                r_data,\n                                                count)\n                        try:\n                            w_file.write(salt.utils.stringutils.to_str(result))\n                        except (OSError, IOError) as exc:\n                            raise CommandExecutionError(\n                                \"Unable to write file '{0}'. Contents may \"\n                                \"be truncated. Temporary file contains copy \"\n                                \"at '{1}'. \"\n                                \"Exception: {2}\".format(path, temp_file, exc)\n                                )\n                except (OSError, IOError) as exc:\n                    raise CommandExecutionError(\"Exception: {0}\".format(exc))\n                finally:\n                    if r_data and isinstance(r_data, mmap.mmap):\n                        r_data.close()\n        except (OSError, IOError) as exc:\n            raise CommandExecutionError(\"Exception: {0}\".format(exc))\n\n    if not found and (append_if_not_found or prepend_if_not_found):\n        if not_found_content is None:\n            not_found_content = repl\n        if prepend_if_not_found:\n            new_file.insert(0, not_found_content + salt.utils.stringutils.to_bytes(os.linesep))\n        else:\n            # append_if_not_found\n            # Make sure we have a newline at the end of the file\n            if new_file:\n                if not new_file[-1].endswith(salt.utils.stringutils.to_bytes(os.linesep)):\n                    new_file[-1] += salt.utils.stringutils.to_bytes(os.linesep)\n            new_file.append(not_found_content + salt.utils.stringutils.to_bytes(os.linesep))\n        has_changes = True\n        if not dry_run:\n            try:\n                # Create a copy to read from and for later use as a backup\n                temp_file = _mkstemp_copy(path=path,\n                                          preserve_inode=preserve_inode)\n            except (OSError, IOError) as exc:\n                raise CommandExecutionError(\"Exception: {0}\".format(exc))\n            # write new content in the file while avoiding partial reads\n            try:\n                fh_ = salt.utils.atomicfile.atomic_open(path, 'wb')\n                for line in new_file:\n                    fh_.write(salt.utils.stringutils.to_bytes(line))\n            finally:\n                fh_.close()\n\n    if backup and has_changes and not dry_run:\n        # keep the backup only if it was requested\n        # and only if there were any changes\n        backup_name = '{0}{1}'.format(path, backup)\n        try:\n            shutil.move(temp_file, backup_name)\n        except (OSError, IOError) as exc:\n            raise CommandExecutionError(\n                \"Unable to move the temp file '{0}' to the \"\n                \"backup file '{1}'. \"\n                \"Exception: {2}\".format(path, temp_file, exc)\n                )\n        if symlink:\n            symlink_backup = '{0}{1}'.format(given_path, backup)\n            target_backup = '{0}{1}'.format(target_path, backup)\n            # Always clobber any existing symlink backup\n            # to match the behaviour of the 'backup' option\n            try:\n                os.symlink(target_backup, symlink_backup)\n            except OSError:\n                os.remove(symlink_backup)\n                os.symlink(target_backup, symlink_backup)\n            except Exception:\n                raise CommandExecutionError(\n                    \"Unable create backup symlink '{0}'. \"\n                    \"Target was '{1}'. \"\n                    \"Exception: {2}\".format(symlink_backup, target_backup,\n                                            exc)\n                    )\n    elif temp_file:\n        try:\n            os.remove(temp_file)\n        except (OSError, IOError) as exc:\n            raise CommandExecutionError(\n                \"Unable to delete temp file '{0}'. \"\n                \"Exception: {1}\".format(temp_file, exc)\n                )\n\n    if not dry_run and not salt.utils.platform.is_windows():\n        check_perms(path, None, pre_user, pre_group, pre_mode)\n\n    differences = __utils__['stringutils.get_diff'](orig_file, new_file)\n\n    if show_changes:\n        return differences\n\n    # We may have found a regex line match but don't need to change the line\n    # (for situations where the pattern also matches the repl). Revert the\n    # has_changes flag to False if the final result is unchanged.\n    if not differences:\n        has_changes = False\n\n    return has_changes", "response": "Replace occurrences of a pattern in a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef search(path,\n        pattern,\n        flags=8,\n        bufsize=1,\n        ignore_if_missing=False,\n        multiline=False\n        ):\n    '''\n    .. versionadded:: 0.17.0\n\n    Search for occurrences of a pattern in a file\n\n    Except for multiline, params are identical to\n    :py:func:`~salt.modules.file.replace`.\n\n    multiline\n        If true, inserts 'MULTILINE' into ``flags`` and sets ``bufsize`` to\n        'file'.\n\n        .. versionadded:: 2015.8.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.search /etc/crontab 'mymaintenance.sh'\n    '''\n    if multiline:\n        flags = _add_flags(flags, 'MULTILINE')\n        bufsize = 'file'\n\n    # This function wraps file.replace on purpose in order to enforce\n    # consistent usage, compatible regex's, expected behavior, *and* bugs. :)\n    # Any enhancements or fixes to one should affect the other.\n    return replace(path,\n            pattern,\n            '',\n            flags=flags,\n            bufsize=bufsize,\n            dry_run=True,\n            search_only=True,\n            show_changes=False,\n            ignore_if_missing=ignore_if_missing)", "response": "Search for occurrences of a pattern in a file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef patch(originalfile, patchfile, options='', dry_run=False):\n    '''\n    .. versionadded:: 0.10.4\n\n    Apply a patch to a file or directory.\n\n    Equivalent to:\n\n    .. code-block:: bash\n\n        patch <options> -i <patchfile> <originalfile>\n\n    Or, when a directory is patched:\n\n    .. code-block:: bash\n\n        patch <options> -i <patchfile> -d <originalfile> -p0\n\n    originalfile\n        The full path to the file or directory to be patched\n    patchfile\n        A patch file to apply to ``originalfile``\n    options\n        Options to pass to patch.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.patch /opt/file.txt /tmp/file.txt.patch\n    '''\n    patchpath = salt.utils.path.which('patch')\n    if not patchpath:\n        raise CommandExecutionError(\n            'patch executable not found. Is the distribution\\'s patch '\n            'package installed?'\n        )\n\n    cmd = [patchpath]\n    cmd.extend(salt.utils.args.shlex_split(options))\n    if dry_run:\n        if __grains__['kernel'] in ('FreeBSD', 'OpenBSD'):\n            cmd.append('-C')\n        else:\n            cmd.append('--dry-run')\n\n    # this argument prevents interactive prompts when the patch fails to apply.\n    # the exit code will still be greater than 0 if that is the case.\n    if '-N' not in cmd and '--forward' not in cmd:\n        cmd.append('--forward')\n\n    has_rejectfile_option = False\n    for option in cmd:\n        if option == '-r' or option.startswith('-r ') \\\n                or option.startswith('--reject-file'):\n            has_rejectfile_option = True\n            break\n\n    # by default, patch will write rejected patch files to <filename>.rej.\n    # this option prevents that.\n    if not has_rejectfile_option:\n        cmd.append('--reject-file=-')\n\n    cmd.extend(['-i', patchfile])\n\n    if os.path.isdir(originalfile):\n        cmd.extend(['-d', originalfile])\n\n        has_strip_option = False\n        for option in cmd:\n            if option.startswith('-p') or option.startswith('--strip='):\n                has_strip_option = True\n                break\n\n        if not has_strip_option:\n            cmd.append('--strip=0')\n    else:\n        cmd.append(originalfile)\n\n    return __salt__['cmd.run_all'](cmd, python_shell=False)", "response": "Apply a patch to a file or directory."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef contains(path, text):\n    '''\n    .. deprecated:: 0.17.0\n       Use :func:`search` instead.\n\n    Return ``True`` if the file at ``path`` contains ``text``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.contains /etc/crontab 'mymaintenance.sh'\n    '''\n    path = os.path.expanduser(path)\n\n    if not os.path.exists(path):\n        return False\n\n    stripped_text = six.text_type(text).strip()\n    try:\n        with salt.utils.filebuffer.BufferedReader(path) as breader:\n            for chunk in breader:\n                if stripped_text in chunk:\n                    return True\n        return False\n    except (IOError, OSError):\n        return False", "response": "Return True if the file at path contains text."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef contains_regex(path, regex, lchar=''):\n    '''\n    .. deprecated:: 0.17.0\n       Use :func:`search` instead.\n\n    Return True if the given regular expression matches on any line in the text\n    of a given file.\n\n    If the lchar argument (leading char) is specified, it\n    will strip `lchar` from the left side of each line before trying to match\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.contains_regex /etc/crontab\n    '''\n    path = os.path.expanduser(path)\n\n    if not os.path.exists(path):\n        return False\n\n    try:\n        with salt.utils.files.fopen(path, 'r') as target:\n            for line in target:\n                line = salt.utils.stringutils.to_unicode(line)\n                if lchar:\n                    line = line.lstrip(lchar)\n                if re.search(regex, line):\n                    return True\n            return False\n    except (IOError, OSError):\n        return False", "response": "Return True if the given regular expression matches on any line in the text\naurus of a given file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef contains_glob(path, glob_expr):\n    '''\n    .. deprecated:: 0.17.0\n       Use :func:`search` instead.\n\n    Return ``True`` if the given glob matches a string in the named file\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.contains_glob /etc/foobar '*cheese*'\n    '''\n    path = os.path.expanduser(path)\n\n    if not os.path.exists(path):\n        return False\n\n    try:\n        with salt.utils.filebuffer.BufferedReader(path) as breader:\n            for chunk in breader:\n                if fnmatch.fnmatch(chunk, glob_expr):\n                    return True\n            return False\n    except (IOError, OSError):\n        return False", "response": "Return True if the given glob is contained in the named file\nTaxonomy"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nappend text to the end of a file.", "response": "def append(path, *args, **kwargs):\n    '''\n    .. versionadded:: 0.9.5\n\n    Append text to the end of a file\n\n    path\n        path to file\n\n    `*args`\n        strings to append to file\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.append /etc/motd \\\\\n                \"With all thine offerings thou shalt offer salt.\" \\\\\n                \"Salt is what makes things taste bad when it isn't in them.\"\n\n    .. admonition:: Attention\n\n        If you need to pass a string to append and that string contains\n        an equal sign, you **must** include the argument name, args.\n        For example:\n\n        .. code-block:: bash\n\n            salt '*' file.append /etc/motd args='cheese=spam'\n\n            salt '*' file.append /etc/motd args=\"['cheese=spam','spam=cheese']\"\n\n    '''\n    path = os.path.expanduser(path)\n\n    # Largely inspired by Fabric's contrib.files.append()\n\n    if 'args' in kwargs:\n        if isinstance(kwargs['args'], list):\n            args = kwargs['args']\n        else:\n            args = [kwargs['args']]\n\n    # Make sure we have a newline at the end of the file. Do this in binary\n    # mode so SEEK_END with nonzero offset will work.\n    with salt.utils.files.fopen(path, 'rb+') as ofile:\n        linesep = salt.utils.stringutils.to_bytes(os.linesep)\n        try:\n            ofile.seek(-len(linesep), os.SEEK_END)\n        except IOError as exc:\n            if exc.errno in (errno.EINVAL, errno.ESPIPE):\n                # Empty file, simply append lines at the beginning of the file\n                pass\n            else:\n                raise\n        else:\n            if ofile.read(len(linesep)) != linesep:\n                ofile.seek(0, os.SEEK_END)\n                ofile.write(linesep)\n\n    # Append lines in text mode\n    with salt.utils.files.fopen(path, 'a') as ofile:\n        for new_line in args:\n            ofile.write(\n                salt.utils.stringutils.to_str(\n                    '{0}{1}'.format(new_line, os.linesep)\n                )\n            )\n\n    return 'Wrote {0} lines to \"{1}\"'.format(len(args), path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef prepend(path, *args, **kwargs):\n    '''\n    .. versionadded:: 2014.7.0\n\n    Prepend text to the beginning of a file\n\n    path\n        path to file\n\n    `*args`\n        strings to prepend to the file\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.prepend /etc/motd \\\\\n                \"With all thine offerings thou shalt offer salt.\" \\\\\n                \"Salt is what makes things taste bad when it isn't in them.\"\n\n    .. admonition:: Attention\n\n        If you need to pass a string to append and that string contains\n        an equal sign, you **must** include the argument name, args.\n        For example:\n\n        .. code-block:: bash\n\n            salt '*' file.prepend /etc/motd args='cheese=spam'\n\n            salt '*' file.prepend /etc/motd args=\"['cheese=spam','spam=cheese']\"\n\n    '''\n    path = os.path.expanduser(path)\n\n    if 'args' in kwargs:\n        if isinstance(kwargs['args'], list):\n            args = kwargs['args']\n        else:\n            args = [kwargs['args']]\n\n    try:\n        with salt.utils.files.fopen(path) as fhr:\n            contents = [salt.utils.stringutils.to_unicode(line)\n                        for line in fhr.readlines()]\n    except IOError:\n        contents = []\n\n    preface = []\n    for line in args:\n        preface.append('{0}\\n'.format(line))\n\n    with salt.utils.files.fopen(path, 'w') as ofile:\n        contents = preface + contents\n        ofile.write(salt.utils.stringutils.to_str(''.join(contents)))\n    return 'Prepended {0} lines to \"{1}\"'.format(len(args), path)", "response": "Prepend text to the beginning of a file"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwrites text to a file", "response": "def write(path, *args, **kwargs):\n    '''\n    .. versionadded:: 2014.7.0\n\n    Write text to a file, overwriting any existing contents.\n\n    path\n        path to file\n\n    `*args`\n        strings to write to the file\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.write /etc/motd \\\\\n                \"With all thine offerings thou shalt offer salt.\"\n\n    .. admonition:: Attention\n\n        If you need to pass a string to append and that string contains\n        an equal sign, you **must** include the argument name, args.\n        For example:\n\n        .. code-block:: bash\n\n            salt '*' file.write /etc/motd args='cheese=spam'\n\n            salt '*' file.write /etc/motd args=\"['cheese=spam','spam=cheese']\"\n\n    '''\n    path = os.path.expanduser(path)\n\n    if 'args' in kwargs:\n        if isinstance(kwargs['args'], list):\n            args = kwargs['args']\n        else:\n            args = [kwargs['args']]\n\n    contents = []\n    for line in args:\n        contents.append('{0}\\n'.format(line))\n    with salt.utils.files.fopen(path, \"w\") as ofile:\n        ofile.write(salt.utils.stringutils.to_str(''.join(contents)))\n    return 'Wrote {0} lines to \"{1}\"'.format(len(contents), path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef touch(name, atime=None, mtime=None):\n    '''\n    .. versionadded:: 0.9.5\n\n    Just like the ``touch`` command, create a file if it doesn't exist or\n    simply update the atime and mtime if it already does.\n\n    atime:\n        Access time in Unix epoch time. Set it to 0 to set atime of the\n        file with Unix date of birth. If this parameter isn't set, atime\n        will be set with current time.\n    mtime:\n        Last modification in Unix epoch time. Set it to 0 to set mtime of\n        the file with Unix date of birth. If this parameter isn't set,\n        mtime will be set with current time.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.touch /var/log/emptyfile\n    '''\n    name = os.path.expanduser(name)\n\n    if atime and atime.isdigit():\n        atime = int(atime)\n    if mtime and mtime.isdigit():\n        mtime = int(mtime)\n    try:\n        if not os.path.exists(name):\n            with salt.utils.files.fopen(name, 'a'):\n                pass\n\n        if atime is None and mtime is None:\n            times = None\n        elif mtime is None and atime is not None:\n            times = (atime, time.time())\n        elif atime is None and mtime is not None:\n            times = (time.time(), mtime)\n        else:\n            times = (atime, mtime)\n        os.utime(name, times)\n\n    except TypeError:\n        raise SaltInvocationError('atime and mtime must be integers')\n    except (IOError, OSError) as exc:\n        raise CommandExecutionError(exc.strerror)\n\n    return os.path.exists(name)", "response": "Create a file in the specified directory and update the atime and mtime fields of the specified file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef tail(path, lines):\n    '''\n    .. versionadded:: Neon\n\n    Read the last n lines from a file\n\n    path\n        path to file\n\n    lines\n        number of lines to read\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.tail /path/to/file 10\n    '''\n    path = os.path.expanduser(path)\n    lines_found = []\n    buffer_size = 4098\n\n    if not os.path.isfile(path):\n        raise SaltInvocationError('File not found: {0}'.format(path))\n\n    if not __utils__['files.is_text'](path):\n        raise SaltInvocationError(\n            'Cannot tail a binary file: {0}'.format(path))\n\n    try:\n        lines = int(lines)\n    except ValueError:\n        raise SaltInvocationError('file.tail: \\'lines\\' value must be an integer')\n\n    try:\n        with salt.utils.fopen(path) as tail_fh:\n            blk_cnt = 1\n            size = os.stat(path).st_size\n\n            if size > buffer_size:\n                tail_fh.seek(-buffer_size * blk_cnt, os.SEEK_END)\n            data = string.split(tail_fh.read(buffer_size), os.linesep)\n\n            for i in range(lines):\n                while len(data) == 1 and ((blk_cnt * buffer_size) < size):\n                    blk_cnt += 1\n                    line = data[0]\n                    try:\n                        tail_fh.seek(-buffer_size * blk_cnt, os.SEEK_END)\n                        data = string.split(tail_fh.read(buffer_size) + line, os.linesep)\n                    except IOError:\n                        tail_fh.seek(0)\n                        data = string.split(tail_fh.read(size - (buffer_size * (blk_cnt - 1))) + line, os.linesep)\n\n                line = data[-1]\n                data.pop()\n                lines_found.append(line)\n\n        return lines_found[-lines:]\n    except (OSError, IOError):\n        raise CommandExecutionError('Could not tail \\'{0}\\''.format(path))", "response": "Read the last n lines from a file and return the list of lines found."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef seek_read(path, size, offset):\n    '''\n    .. versionadded:: 2014.1.0\n\n    Seek to a position on a file and read it\n\n    path\n        path to file\n\n    seek\n        amount to read at once\n\n    offset\n        offset to start into the file\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.seek_read /path/to/file 4096 0\n    '''\n    path = os.path.expanduser(path)\n    seek_fh = os.open(path, os.O_RDONLY)\n    try:\n        os.lseek(seek_fh, int(offset), 0)\n        data = os.read(seek_fh, int(size))\n    finally:\n        os.close(seek_fh)\n    return data", "response": "Reads the contents of the file at the specified offset and returns the contents as a string."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nseeks to a position on a file and write to it", "response": "def seek_write(path, data, offset):\n    '''\n    .. versionadded:: 2014.1.0\n\n    Seek to a position on a file and write to it\n\n    path\n        path to file\n\n    data\n        data to write to file\n\n    offset\n        position in file to start writing\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.seek_write /path/to/file 'some data' 4096\n    '''\n    path = os.path.expanduser(path)\n    seek_fh = os.open(path, os.O_WRONLY)\n    try:\n        os.lseek(seek_fh, int(offset), 0)\n        ret = os.write(seek_fh, data)\n        os.fsync(seek_fh)\n    finally:\n        os.close(seek_fh)\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef truncate(path, length):\n    '''\n    .. versionadded:: 2014.1.0\n\n    Seek to a position on a file and delete everything after that point\n\n    path\n        path to file\n\n    length\n        offset into file to truncate\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.truncate /path/to/file 512\n    '''\n    path = os.path.expanduser(path)\n    with salt.utils.files.fopen(path, 'rb+') as seek_fh:\n        seek_fh.truncate(int(length))", "response": "Truncate the file at the specified location"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a hard link to a file", "response": "def link(src, path):\n    '''\n    .. versionadded:: 2014.1.0\n\n    Create a hard link to a file\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.link /path/to/file /path/to/link\n    '''\n    src = os.path.expanduser(src)\n\n    if not os.path.isabs(src):\n        raise SaltInvocationError('File path must be absolute.')\n\n    try:\n        os.link(src, path)\n        return True\n    except (OSError, IOError):\n        raise CommandExecutionError('Could not create \\'{0}\\''.format(path))\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a symbolic link to a file Taxonomy CLI Example : bash Taxonomy salt '*' file. symlink src path", "response": "def symlink(src, path):\n    '''\n    Create a symbolic link (symlink, soft link) to a file\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.symlink /path/to/file /path/to/link\n    '''\n    path = os.path.expanduser(path)\n\n    try:\n        if os.path.normpath(os.readlink(path)) == os.path.normpath(src):\n            log.debug('link already in correct state: %s -> %s', path, src)\n            return True\n    except OSError:\n        pass\n\n    if not os.path.isabs(path):\n        raise SaltInvocationError('File path must be absolute.')\n\n    try:\n        os.symlink(src, path)\n        return True\n    except (OSError, IOError):\n        raise CommandExecutionError('Could not create \\'{0}\\''.format(path))\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrename a file or directory or directory of a single object.", "response": "def rename(src, dst):\n    '''\n    Rename a file or directory\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.rename /path/to/src /path/to/dst\n    '''\n    src = os.path.expanduser(src)\n    dst = os.path.expanduser(dst)\n\n    if not os.path.isabs(src):\n        raise SaltInvocationError('File path must be absolute.')\n\n    try:\n        os.rename(src, dst)\n        return True\n    except OSError:\n        raise CommandExecutionError(\n            'Could not rename \\'{0}\\' to \\'{1}\\''.format(src, dst)\n        )\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef copy(src, dst, recurse=False, remove_existing=False):\n    '''\n    Copy a file or directory from source to dst\n\n    In order to copy a directory, the recurse flag is required, and\n    will by default overwrite files in the destination with the same path,\n    and retain all other existing files. (similar to cp -r on unix)\n\n    remove_existing will remove all files in the target directory,\n    and then copy files from the source.\n\n    .. note::\n        The copy function accepts paths that are local to the Salt minion.\n        This function does not support salt://, http://, or the other\n        additional file paths that are supported by :mod:`states.file.managed\n        <salt.states.file.managed>` and :mod:`states.file.recurse\n        <salt.states.file.recurse>`.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.copy /path/to/src /path/to/dst\n        salt '*' file.copy /path/to/src_dir /path/to/dst_dir recurse=True\n        salt '*' file.copy /path/to/src_dir /path/to/dst_dir recurse=True remove_existing=True\n\n    '''\n    src = os.path.expanduser(src)\n    dst = os.path.expanduser(dst)\n\n    if not os.path.isabs(src):\n        raise SaltInvocationError('File path must be absolute.')\n\n    if not os.path.exists(src):\n        raise CommandExecutionError('No such file or directory \\'{0}\\''.format(src))\n\n    if not salt.utils.platform.is_windows():\n        pre_user = get_user(src)\n        pre_group = get_group(src)\n        pre_mode = salt.utils.files.normalize_mode(get_mode(src))\n\n    try:\n        if (os.path.exists(dst) and os.path.isdir(dst)) or os.path.isdir(src):\n            if not recurse:\n                raise SaltInvocationError(\n                    \"Cannot copy overwriting a directory without recurse flag set to true!\")\n            if remove_existing:\n                if os.path.exists(dst):\n                    shutil.rmtree(dst)\n                shutil.copytree(src, dst)\n            else:\n                salt.utils.files.recursive_copy(src, dst)\n        else:\n            shutil.copyfile(src, dst)\n    except OSError:\n        raise CommandExecutionError(\n            'Could not copy \\'{0}\\' to \\'{1}\\''.format(src, dst)\n        )\n\n    if not salt.utils.platform.is_windows():\n        check_perms(dst, None, pre_user, pre_group, pre_mode)\n    return True", "response": "Copy a file or directory from source to destination."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef lstat(path):\n    '''\n    .. versionadded:: 2014.1.0\n\n    Returns the lstat attributes for the given file or dir. Does not support\n    symbolic links.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.lstat /path/to/file\n    '''\n    path = os.path.expanduser(path)\n\n    if not os.path.isabs(path):\n        raise SaltInvocationError('Path to file must be absolute.')\n\n    try:\n        lst = os.lstat(path)\n        return dict((key, getattr(lst, key)) for key in ('st_atime', 'st_ctime',\n            'st_gid', 'st_mode', 'st_mtime', 'st_nlink', 'st_size', 'st_uid'))\n    except Exception:\n        return {}", "response": "Returns the lstat attributes for the given file or directory."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef access(path, mode):\n    '''\n    .. versionadded:: 2014.1.0\n\n    Test whether the Salt process has the specified access to the file. One of\n    the following modes must be specified:\n\n    .. code-block::text\n\n        f: Test the existence of the path\n        r: Test the readability of the path\n        w: Test the writability of the path\n        x: Test whether the path can be executed\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.access /path/to/file f\n        salt '*' file.access /path/to/file x\n    '''\n    path = os.path.expanduser(path)\n\n    if not os.path.isabs(path):\n        raise SaltInvocationError('Path to link must be absolute.')\n\n    modes = {'f': os.F_OK,\n             'r': os.R_OK,\n             'w': os.W_OK,\n             'x': os.X_OK}\n\n    if mode in modes:\n        return os.access(path, modes[mode])\n    elif mode in six.itervalues(modes):\n        return os.access(path, mode)\n    else:\n        raise SaltInvocationError('Invalid mode specified.')", "response": "Test whether the file is accessible to the salt process."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read(path, binary=False):\n    '''\n    .. versionadded:: 2017.7.0\n\n    Return the content of the file.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.read /path/to/file\n    '''\n    access_mode = 'r'\n    if binary is True:\n        access_mode += 'b'\n    with salt.utils.files.fopen(path, access_mode) as file_obj:\n        return salt.utils.stringutils.to_unicode(file_obj.read())", "response": "Reads the content of the file at the specified path"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef readlink(path, canonicalize=False):\n    '''\n    .. versionadded:: 2014.1.0\n\n    Return the path that a symlink points to\n    If canonicalize is set to True, then it return the final target\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.readlink /path/to/link\n    '''\n    path = os.path.expanduser(path)\n\n    if not os.path.isabs(path):\n        raise SaltInvocationError('Path to link must be absolute.')\n\n    if not os.path.islink(path):\n        raise SaltInvocationError('A valid link was not specified.')\n\n    if canonicalize:\n        return os.path.realpath(path)\n    else:\n        return os.readlink(path)", "response": "Reads a symlink from a path to a base directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a dict of statvfs attributes for the file", "response": "def statvfs(path):\n    '''\n    .. versionadded:: 2014.1.0\n\n    Perform a statvfs call against the filesystem that the file resides on\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.statvfs /path/to/file\n    '''\n    path = os.path.expanduser(path)\n\n    if not os.path.isabs(path):\n        raise SaltInvocationError('File path must be absolute.')\n\n    try:\n        stv = os.statvfs(path)\n        return dict((key, getattr(stv, key)) for key in ('f_bavail', 'f_bfree',\n            'f_blocks', 'f_bsize', 'f_favail', 'f_ffree', 'f_files', 'f_flag',\n            'f_frsize', 'f_namemax'))\n    except (OSError, IOError):\n        raise CommandExecutionError('Could not statvfs \\'{0}\\''.format(path))\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a dict containing the stats for a given file.", "response": "def stats(path, hash_type=None, follow_symlinks=True):\n    '''\n    Return a dict containing the stats for a given file\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.stats /etc/passwd\n    '''\n    path = os.path.expanduser(path)\n\n    ret = {}\n    if not os.path.exists(path):\n        try:\n            # Broken symlinks will return False for os.path.exists(), but still\n            # have a uid and gid\n            pstat = os.lstat(path)\n        except OSError:\n            # Not a broken symlink, just a nonexistent path\n            # NOTE: The file.directory state checks the content of the error\n            # message in this exception. Any changes made to the message for this\n            # exception will reflect the file.directory state as well, and will\n            # likely require changes there.\n            raise CommandExecutionError('Path not found: {0}'.format(path))\n    else:\n        if follow_symlinks:\n            pstat = os.stat(path)\n        else:\n            pstat = os.lstat(path)\n    ret['inode'] = pstat.st_ino\n    ret['uid'] = pstat.st_uid\n    ret['gid'] = pstat.st_gid\n    ret['group'] = gid_to_group(pstat.st_gid)\n    ret['user'] = uid_to_user(pstat.st_uid)\n    ret['atime'] = pstat.st_atime\n    ret['mtime'] = pstat.st_mtime\n    ret['ctime'] = pstat.st_ctime\n    ret['size'] = pstat.st_size\n    ret['mode'] = six.text_type(oct(stat.S_IMODE(pstat.st_mode)))\n    if hash_type:\n        ret['sum'] = get_hash(path, hash_type)\n    ret['type'] = 'file'\n    if stat.S_ISDIR(pstat.st_mode):\n        ret['type'] = 'dir'\n    if stat.S_ISCHR(pstat.st_mode):\n        ret['type'] = 'char'\n    if stat.S_ISBLK(pstat.st_mode):\n        ret['type'] = 'block'\n    if stat.S_ISREG(pstat.st_mode):\n        ret['type'] = 'file'\n    if stat.S_ISLNK(pstat.st_mode):\n        ret['type'] = 'link'\n    if stat.S_ISFIFO(pstat.st_mode):\n        ret['type'] = 'pipe'\n    if stat.S_ISSOCK(pstat.st_mode):\n        ret['type'] = 'socket'\n    ret['target'] = os.path.realpath(path)\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rmdir(path):\n    '''\n    .. versionadded:: 2014.1.0\n\n    Remove the specified directory. Fails if a directory is not empty.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.rmdir /tmp/foo/\n    '''\n    path = os.path.expanduser(path)\n\n    if not os.path.isabs(path):\n        raise SaltInvocationError('File path must be absolute.')\n\n    if not os.path.isdir(path):\n        raise SaltInvocationError('A valid directory was not specified.')\n\n    try:\n        os.rmdir(path)\n        return True\n    except OSError as exc:\n        return exc.strerror", "response": "Removes the specified directory. Fails if a directory is not empty."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nremove the named file or directory.", "response": "def remove(path, **kwargs):\n    '''\n    Remove the named file. If a directory is supplied, it will be recursively\n    deleted.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.remove /tmp/foo\n    '''\n    path = os.path.expanduser(path)\n\n    if not os.path.isabs(path):\n        raise SaltInvocationError('File path must be absolute: {0}'.format(path))\n\n    try:\n        if os.path.isfile(path) or os.path.islink(path):\n            os.remove(path)\n            return True\n        elif os.path.isdir(path):\n            shutil.rmtree(path)\n            return True\n    except (OSError, IOError) as exc:\n        raise CommandExecutionError(\n            'Could not remove \\'{0}\\': {1}'.format(path, exc)\n        )\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntest to see if a path exists in glob", "response": "def path_exists_glob(path):\n    '''\n    Tests to see if path after expansion is a valid path (file or directory).\n    Expansion allows usage of ? * and character ranges []. Tilde expansion\n    is not supported. Returns True/False.\n\n    .. versionadded:: 2014.7.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.path_exists_glob /etc/pam*/pass*\n\n    '''\n    return True if glob.glob(os.path.expanduser(path)) else False"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nresets SELinux context on a given path", "response": "def restorecon(path, recursive=False):\n    '''\n    Reset the SELinux context on a given path\n\n    CLI Example:\n\n    .. code-block:: bash\n\n         salt '*' file.restorecon /home/user/.ssh/authorized_keys\n    '''\n    if recursive:\n        cmd = ['restorecon', '-FR', path]\n    else:\n        cmd = ['restorecon', '-F', path]\n    return not __salt__['cmd.retcode'](cmd, python_shell=False)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_selinux_context(path):\n    '''\n    Get an SELinux context from a given path\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.get_selinux_context /etc/hosts\n    '''\n    out = __salt__['cmd.run'](['ls', '-Z', path], python_shell=False)\n\n    try:\n        ret = re.search(r'\\w+:\\w+:\\w+:\\w+', out).group(0)\n    except AttributeError:\n        ret = (\n            'No selinux context information is available for {0}'.format(path)\n        )\n\n    return ret", "response": "Get an SELinux context from a given path"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset a specific SELinux label on a given path", "response": "def set_selinux_context(path,\n                        user=None,\n                        role=None,\n                        type=None,    # pylint: disable=W0622\n                        range=None,   # pylint: disable=W0622\n                        persist=False):\n    '''\n    .. versionchanged:: Neon\n\n        Added persist option\n\n    Set a specific SELinux label on a given path\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.set_selinux_context path <user> <role> <type> <range>\n        salt '*' file.set_selinux_context /etc/yum.repos.d/epel.repo system_u object_r system_conf_t s0\n    '''\n    if not any((user, role, type, range)):\n        return False\n\n    if persist:\n        fcontext_result = __salt__['selinux.fcontext_add_policy'](path,\n            sel_type=type, sel_user=user, sel_level=range)\n        if fcontext_result.get('retcode', None) is not 0:\n            # Problem setting fcontext policy\n            raise CommandExecutionError(\n                'Problem setting fcontext: {0}'.format(fcontext_result)\n            )\n\n    cmd = ['chcon']\n    if user:\n        cmd.extend(['-u', user])\n    if role:\n        cmd.extend(['-r', role])\n    if type:\n        cmd.extend(['-t', type])\n    if range:\n        cmd.extend(['-l', range])\n    cmd.append(path)\n\n    ret = not __salt__['cmd.retcode'](cmd, python_shell=False)\n    if ret:\n        return get_selinux_context(path)\n    else:\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef source_list(source, source_hash, saltenv):\n    '''\n    Check the source list and return the source to use\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.source_list salt://http/httpd.conf '{hash_type: 'md5', 'hsum': <md5sum>}' base\n    '''\n    contextkey = '{0}_|-{1}_|-{2}'.format(source, source_hash, saltenv)\n    if contextkey in __context__:\n        return __context__[contextkey]\n\n    # get the master file list\n    if isinstance(source, list):\n        mfiles = [(f, saltenv) for f in __salt__['cp.list_master'](saltenv)]\n        mdirs = [(d, saltenv) for d in __salt__['cp.list_master_dirs'](saltenv)]\n        for single in source:\n            if isinstance(single, dict):\n                single = next(iter(single))\n\n            path, senv = salt.utils.url.parse(single)\n            if senv:\n                mfiles += [(f, senv) for f in __salt__['cp.list_master'](senv)]\n                mdirs += [(d, senv) for d in __salt__['cp.list_master_dirs'](senv)]\n\n        ret = None\n        for single in source:\n            if isinstance(single, dict):\n                # check the proto, if it is http or ftp then download the file\n                # to check, if it is salt then check the master list\n                # if it is a local file, check if the file exists\n                if len(single) != 1:\n                    continue\n                single_src = next(iter(single))\n                single_hash = single[single_src] if single[single_src] else source_hash\n                urlparsed_single_src = _urlparse(single_src)\n                # Fix this for Windows\n                if salt.utils.platform.is_windows():\n                    # urlparse doesn't handle a local Windows path without the\n                    # protocol indicator (file://). The scheme will be the\n                    # drive letter instead of the protocol. So, we'll add the\n                    # protocol and re-parse\n                    if urlparsed_single_src.scheme.lower() in string.ascii_lowercase:\n                        urlparsed_single_src = _urlparse('file://' + single_src)\n                proto = urlparsed_single_src.scheme\n                if proto == 'salt':\n                    path, senv = salt.utils.url.parse(single_src)\n                    if not senv:\n                        senv = saltenv\n                    if (path, saltenv) in mfiles or (path, saltenv) in mdirs:\n                        ret = (single_src, single_hash)\n                        break\n                elif proto.startswith('http') or proto == 'ftp':\n                    ret = (single_src, single_hash)\n                    break\n                elif proto == 'file' and (\n                         os.path.exists(urlparsed_single_src.netloc) or\n                         os.path.exists(urlparsed_single_src.path) or\n                         os.path.exists(os.path.join(\n                             urlparsed_single_src.netloc,\n                             urlparsed_single_src.path))):\n                    ret = (single_src, single_hash)\n                    break\n                elif single_src.startswith(os.sep) and os.path.exists(single_src):\n                    ret = (single_src, single_hash)\n                    break\n            elif isinstance(single, six.string_types):\n                path, senv = salt.utils.url.parse(single)\n                if not senv:\n                    senv = saltenv\n                if (path, senv) in mfiles or (path, senv) in mdirs:\n                    ret = (single, source_hash)\n                    break\n                urlparsed_src = _urlparse(single)\n                if salt.utils.platform.is_windows():\n                    # urlparse doesn't handle a local Windows path without the\n                    # protocol indicator (file://). The scheme will be the\n                    # drive letter instead of the protocol. So, we'll add the\n                    # protocol and re-parse\n                    if urlparsed_src.scheme.lower() in string.ascii_lowercase:\n                        urlparsed_src = _urlparse('file://' + single)\n                proto = urlparsed_src.scheme\n                if proto == 'file' and (\n                        os.path.exists(urlparsed_src.netloc) or\n                        os.path.exists(urlparsed_src.path) or\n                        os.path.exists(os.path.join(\n                            urlparsed_src.netloc,\n                            urlparsed_src.path))):\n                    ret = (single, source_hash)\n                    break\n                elif proto.startswith('http') or proto == 'ftp':\n                    ret = (single, source_hash)\n                    break\n                elif single.startswith(os.sep) and os.path.exists(single):\n                    ret = (single, source_hash)\n                    break\n        if ret is None:\n            # None of the list items matched\n            raise CommandExecutionError(\n                'none of the specified sources were found'\n            )\n    else:\n        ret = (source, source_hash)\n\n    __context__[contextkey] = ret\n    return ret", "response": "Return the source list"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the contents after applying the templating engine contents template string template format context defaults and saltenv", "response": "def apply_template_on_contents(\n        contents,\n        template,\n        context,\n        defaults,\n        saltenv):\n    '''\n    Return the contents after applying the templating engine\n\n    contents\n        template string\n\n    template\n        template format\n\n    context\n        Overrides default context variables passed to the template.\n\n    defaults\n        Default context passed to the template.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.apply_template_on_contents \\\\\n            contents='This is a {{ template }} string.' \\\\\n            template=jinja \\\\\n            \"context={}\" \"defaults={'template': 'cool'}\" \\\\\n            saltenv=base\n    '''\n    if template in salt.utils.templates.TEMPLATE_REGISTRY:\n        context_dict = defaults if defaults else {}\n        if context:\n            context_dict = salt.utils.dictupdate.merge(context_dict, context)\n        # Apply templating\n        contents = salt.utils.templates.TEMPLATE_REGISTRY[template](\n            contents,\n            from_str=True,\n            to_str=True,\n            context=context_dict,\n            saltenv=saltenv,\n            grains=__opts__['grains'],\n            pillar=__pillar__,\n            salt=__salt__,\n            opts=__opts__)['data']\n        if six.PY2:\n            contents = contents.encode('utf-8')\n        elif six.PY3 and isinstance(contents, bytes):\n            # bytes -> str\n            contents = contents.decode('utf-8')\n    else:\n        ret = {}\n        ret['result'] = False\n        ret['comment'] = ('Specified template format {0} is not supported'\n                          ).format(template)\n        return ret\n    return contents"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the file. managed file data for the specified file.", "response": "def get_managed(\n        name,\n        template,\n        source,\n        source_hash,\n        source_hash_name,\n        user,\n        group,\n        mode,\n        attrs,\n        saltenv,\n        context,\n        defaults,\n        skip_verify=False,\n        **kwargs):\n    '''\n    Return the managed file data for file.managed\n\n    name\n        location where the file lives on the server\n\n    template\n        template format\n\n    source\n        managed source file\n\n    source_hash\n        hash of the source file\n\n    source_hash_name\n        When ``source_hash`` refers to a remote file, this specifies the\n        filename to look for in that file.\n\n        .. versionadded:: 2016.3.5\n\n    user\n        Owner of file\n\n    group\n        Group owner of file\n\n    mode\n        Permissions of file\n\n    attrs\n        Attributes of file\n\n        .. versionadded:: 2018.3.0\n\n    context\n        Variables to add to the template context\n\n    defaults\n        Default values of for context_dict\n\n    skip_verify\n        If ``True``, hash verification of remote file sources (``http://``,\n        ``https://``, ``ftp://``) will be skipped, and the ``source_hash``\n        argument will be ignored.\n\n        .. versionadded:: 2016.3.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.get_managed /etc/httpd/conf.d/httpd.conf jinja salt://http/httpd.conf '{hash_type: 'md5', 'hsum': <md5sum>}' None root root '755' base None None\n    '''\n    # Copy the file to the minion and templatize it\n    sfn = ''\n    source_sum = {}\n\n    def _get_local_file_source_sum(path):\n        '''\n        DRY helper for getting the source_sum value from a locally cached\n        path.\n        '''\n        return {'hsum': get_hash(path, form='sha256'), 'hash_type': 'sha256'}\n\n    # If we have a source defined, let's figure out what the hash is\n    if source:\n        urlparsed_source = _urlparse(source)\n        if urlparsed_source.scheme in salt.utils.files.VALID_PROTOS:\n            parsed_scheme = urlparsed_source.scheme\n        else:\n            parsed_scheme = ''\n        parsed_path = os.path.join(\n                urlparsed_source.netloc, urlparsed_source.path).rstrip(os.sep)\n        unix_local_source = parsed_scheme in ('file', '')\n\n        if parsed_scheme == '':\n            parsed_path = sfn = source\n            if not os.path.exists(sfn):\n                msg = 'Local file source {0} does not exist'.format(sfn)\n                return '', {}, msg\n        elif parsed_scheme == 'file':\n            sfn = parsed_path\n            if not os.path.exists(sfn):\n                msg = 'Local file source {0} does not exist'.format(sfn)\n                return '', {}, msg\n\n        if parsed_scheme and parsed_scheme.lower() in string.ascii_lowercase:\n            parsed_path = ':'.join([parsed_scheme, parsed_path])\n            parsed_scheme = 'file'\n\n        if parsed_scheme == 'salt':\n            source_sum = __salt__['cp.hash_file'](source, saltenv)\n            if not source_sum:\n                return '', {}, 'Source file {0} not found in saltenv \\'{1}\\''.format(source, saltenv)\n        elif not source_hash and unix_local_source:\n            source_sum = _get_local_file_source_sum(parsed_path)\n        elif not source_hash and source.startswith(os.sep):\n            # This should happen on Windows\n            source_sum = _get_local_file_source_sum(source)\n        else:\n            if not skip_verify:\n                if source_hash:\n                    try:\n                        source_sum = get_source_sum(name,\n                                                    source,\n                                                    source_hash,\n                                                    source_hash_name,\n                                                    saltenv)\n                    except CommandExecutionError as exc:\n                        return '', {}, exc.strerror\n                else:\n                    msg = (\n                        'Unable to verify upstream hash of source file {0}, '\n                        'please set source_hash or set skip_verify to True'\n                        .format(salt.utils.url.redact_http_basic_auth(source))\n                    )\n                    return '', {}, msg\n\n    if source and (template or parsed_scheme in salt.utils.files.REMOTE_PROTOS):\n        # Check if we have the template or remote file cached\n        cache_refetch = False\n        cached_dest = __salt__['cp.is_cached'](source, saltenv)\n        if cached_dest and (source_hash or skip_verify):\n            htype = source_sum.get('hash_type', 'sha256')\n            cached_sum = get_hash(cached_dest, form=htype)\n            if skip_verify:\n                # prev: if skip_verify or cached_sum == source_sum['hsum']:\n                # but `cached_sum == source_sum['hsum']` is elliptical as prev if\n                sfn = cached_dest\n                source_sum = {'hsum': cached_sum, 'hash_type': htype}\n            elif cached_sum != source_sum.get('hsum', __opts__['hash_type']):\n                cache_refetch = True\n            else:\n                sfn = cached_dest\n\n        # If we didn't have the template or remote file, or the file has been\n        # updated and the cache has to be refreshed, download the file.\n        if not sfn or cache_refetch:\n            try:\n                sfn = __salt__['cp.cache_file'](\n                    source,\n                    saltenv,\n                    source_hash=source_sum.get('hsum'))\n            except Exception as exc:\n                # A 404 or other error code may raise an exception, catch it\n                # and return a comment that will fail the calling state.\n                _source = salt.utils.url.redact_http_basic_auth(source)\n                return '', {}, 'Failed to cache {0}: {1}'.format(_source, exc)\n\n        # If cache failed, sfn will be False, so do a truth check on sfn first\n        # as invoking os.path.exists() on a bool raises a TypeError.\n        if not sfn or not os.path.exists(sfn):\n            _source = salt.utils.url.redact_http_basic_auth(source)\n            return sfn, {}, 'Source file \\'{0}\\' not found'.format(_source)\n        if sfn == name:\n            raise SaltInvocationError(\n                'Source file cannot be the same as destination'\n            )\n\n        if template:\n            if template in salt.utils.templates.TEMPLATE_REGISTRY:\n                context_dict = defaults if defaults else {}\n                if context:\n                    context_dict = salt.utils.dictupdate.merge(context_dict, context)\n                data = salt.utils.templates.TEMPLATE_REGISTRY[template](\n                    sfn,\n                    name=name,\n                    source=source,\n                    user=user,\n                    group=group,\n                    mode=mode,\n                    attrs=attrs,\n                    saltenv=saltenv,\n                    context=context_dict,\n                    salt=__salt__,\n                    pillar=__pillar__,\n                    grains=__opts__['grains'],\n                    opts=__opts__,\n                    **kwargs)\n            else:\n                return sfn, {}, ('Specified template format {0} is not supported'\n                                 ).format(template)\n\n            if data['result']:\n                sfn = data['data']\n                hsum = get_hash(sfn, form='sha256')\n                source_sum = {'hash_type': 'sha256',\n                              'hsum': hsum}\n            else:\n                __clean_tmp(sfn)\n                return sfn, {}, data['data']\n\n    return sfn, source_sum, ''"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking permissions on a file", "response": "def check_perms(name, ret, user, group, mode, attrs=None, follow_symlinks=False, seuser=None,\n                serole=None, setype=None, serange=None):\n    '''\n    .. versionchanged:: Neon\n\n        Added selinux options\n\n    Check the permissions on files, modify attributes and chown if needed. File\n    attributes are only verified if lsattr(1) is installed.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.check_perms /etc/sudoers '{}' root root 400 ai\n\n    .. versionchanged:: 2014.1.3\n        ``follow_symlinks`` option added\n    '''\n    name = os.path.expanduser(name)\n\n    if not ret:\n        ret = {'name': name,\n               'changes': {},\n               'comment': [],\n               'result': True}\n        orig_comment = ''\n    else:\n        orig_comment = ret['comment']\n        ret['comment'] = []\n\n    # Check permissions\n    perms = {}\n    cur = stats(name, follow_symlinks=follow_symlinks)\n    perms['luser'] = cur['user']\n    perms['lgroup'] = cur['group']\n    perms['lmode'] = salt.utils.files.normalize_mode(cur['mode'])\n\n    is_dir = os.path.isdir(name)\n    is_link = os.path.islink(name)\n    if attrs is not None \\\n            and not salt.utils.platform.is_windows() \\\n            and not is_dir and not is_link:\n        try:\n            lattrs = lsattr(name)\n        except SaltInvocationError:\n            lattrs = None\n        if lattrs is not None:\n            # List attributes on file\n            perms['lattrs'] = ''.join(lattrs.get(name, ''))\n            # Remove attributes on file so changes can be enforced.\n            if perms['lattrs']:\n                chattr(name, operator='remove', attributes=perms['lattrs'])\n\n    # user/group changes if needed, then check if it worked\n    if user:\n        if isinstance(user, int):\n            user = uid_to_user(user)\n        if (salt.utils.platform.is_windows() and\n                user_to_uid(user) != user_to_uid(perms['luser'])\n            ) or (\n            not salt.utils.platform.is_windows() and user != perms['luser']\n        ):\n            perms['cuser'] = user\n\n    if group:\n        if isinstance(group, int):\n            group = gid_to_group(group)\n        if (salt.utils.platform.is_windows() and\n                group_to_gid(group) != group_to_gid(perms['lgroup'])\n            ) or (\n                not salt.utils.platform.is_windows() and group != perms['lgroup']\n        ):\n            perms['cgroup'] = group\n\n    if 'cuser' in perms or 'cgroup' in perms:\n        if not __opts__['test']:\n            if os.path.islink(name) and not follow_symlinks:\n                chown_func = lchown\n            else:\n                chown_func = chown\n            if user is None:\n                user = perms['luser']\n            if group is None:\n                group = perms['lgroup']\n            try:\n                chown_func(name, user, group)\n                # Python os.chown() does reset the suid and sgid,\n                # that's why setting the right mode again is needed here.\n                set_mode(name, mode)\n            except OSError:\n                ret['result'] = False\n\n    if user:\n        if isinstance(user, int):\n            user = uid_to_user(user)\n        if (salt.utils.platform.is_windows() and\n                user_to_uid(user) != user_to_uid(\n                    get_user(name, follow_symlinks=follow_symlinks)) and\n                user != ''\n            ) or (\n            not salt.utils.platform.is_windows() and\n                user != get_user(name, follow_symlinks=follow_symlinks) and\n                user != ''\n        ):\n            if __opts__['test'] is True:\n                ret['changes']['user'] = user\n            else:\n                ret['result'] = False\n                ret['comment'].append('Failed to change user to {0}'\n                                          .format(user))\n        elif 'cuser' in perms and user != '':\n            ret['changes']['user'] = user\n\n    if group:\n        if isinstance(group, int):\n            group = gid_to_group(group)\n        if (salt.utils.platform.is_windows() and\n                group_to_gid(group) != group_to_gid(\n                    get_group(name, follow_symlinks=follow_symlinks)) and\n                user != '') or (\n            not salt.utils.platform.is_windows() and\n                group != get_group(name, follow_symlinks=follow_symlinks) and\n                user != ''\n        ):\n            if __opts__['test'] is True:\n                ret['changes']['group'] = group\n            else:\n                ret['result'] = False\n                ret['comment'].append('Failed to change group to {0}'\n                                      .format(group))\n        elif 'cgroup' in perms and user != '':\n            ret['changes']['group'] = group\n\n    if not salt.utils.platform.is_windows() and not is_dir:\n        # Replace attributes on file if it had been removed\n        if perms.get('lattrs', ''):\n            chattr(name, operator='add', attributes=perms['lattrs'])\n\n    # Mode changes if needed\n    if mode is not None:\n        # File is a symlink, ignore the mode setting\n        # if follow_symlinks is False\n        if os.path.islink(name) and not follow_symlinks:\n            pass\n        else:\n            mode = salt.utils.files.normalize_mode(mode)\n            if mode != perms['lmode']:\n                if __opts__['test'] is True:\n                    ret['changes']['mode'] = mode\n                else:\n                    set_mode(name, mode)\n                    if mode != salt.utils.files.normalize_mode(get_mode(name)):\n                        ret['result'] = False\n                        ret['comment'].append(\n                            'Failed to change mode to {0}'.format(mode)\n                        )\n                    else:\n                        ret['changes']['mode'] = mode\n\n    # Modify attributes of file if needed\n    if attrs is not None and not is_dir:\n        # File is a symlink, ignore the mode setting\n        # if follow_symlinks is False\n        if os.path.islink(name) and not follow_symlinks:\n            pass\n        else:\n            diff_attrs = _cmp_attrs(name, attrs)\n            if diff_attrs is not None:\n                if diff_attrs[0] is not None or diff_attrs[1] is not None:\n                    if __opts__['test'] is True:\n                        ret['changes']['attrs'] = attrs\n                    else:\n                        if diff_attrs[0] is not None:\n                            chattr(name, operator=\"add\", attributes=diff_attrs[0])\n                        if diff_attrs[1] is not None:\n                            chattr(name, operator=\"remove\", attributes=diff_attrs[1])\n                        cmp_attrs = _cmp_attrs(name, attrs)\n                        if cmp_attrs[0] is not None or cmp_attrs[1] is not None:\n                            ret['result'] = False\n                            ret['comment'].append(\n                                'Failed to change attributes to {0}'.format(attrs)\n                            )\n                        else:\n                            ret['changes']['attrs'] = attrs\n\n    # Set selinux attributes if needed\n    if salt.utils.platform.is_linux() and (seuser or serole or setype or serange):\n        selinux_error = False\n        try:\n            current_seuser, current_serole, current_setype, current_serange = get_selinux_context(name).split(':')\n            log.debug(\n                'Current selinux context user:%s role:%s type:%s range:%s',\n                current_seuser, current_serole, current_setype, current_serange\n            )\n        except ValueError:\n            log.error('Unable to get current selinux attributes')\n            ret['result'] = False\n            ret['comment'].append(\n                'Failed to get selinux attributes'\n            )\n            selinux_error = True\n\n        if not selinux_error:\n            requested_seuser = None\n            requested_serole = None\n            requested_setype = None\n            requested_serange = None\n            # Only set new selinux variables if updates are needed\n            if seuser and seuser != current_seuser:\n                requested_seuser = seuser\n            if serole and serole != current_serole:\n                requested_serole = serole\n            if setype and setype != current_setype:\n                requested_setype = setype\n            if serange and serange != current_serange:\n                requested_serange = serange\n\n            if requested_seuser or requested_serole or requested_setype or requested_serange:\n                # selinux updates needed, prep changes output\n                selinux_change_new = ''\n                selinux_change_orig = ''\n                if requested_seuser:\n                    selinux_change_new += \"User: {0} \".format(requested_seuser)\n                    selinux_change_orig += \"User: {0} \".format(current_seuser)\n                if requested_serole:\n                    selinux_change_new += \"Role: {0} \".format(requested_serole)\n                    selinux_change_orig += \"Role: {0} \".format(current_serole)\n                if requested_setype:\n                    selinux_change_new += \"Type: {0} \".format(requested_setype)\n                    selinux_change_orig += \"Type: {0} \".format(current_setype)\n                if requested_serange:\n                    selinux_change_new += \"Range: {0} \".format(requested_serange)\n                    selinux_change_orig += \"Range: {0} \".format(current_serange)\n\n                if __opts__['test']:\n                    ret['comment'] = 'File {0} selinux context to be updated'.format(name)\n                    ret['result'] = None\n                    ret['changes']['selinux'] = {'Old': selinux_change_orig.strip(),\n                                                 'New': selinux_change_new.strip()}\n                else:\n                    try:\n                        # set_selinux_context requires type to be set on any other change\n                        if (requested_seuser or requested_serole or requested_serange) and not requested_setype:\n                            requested_setype = current_setype\n                        result = set_selinux_context(name, user=requested_seuser, role=requested_serole,\n                                                     type=requested_setype, range=requested_serange, persist=True)\n                        log.debug('selinux set result: %s', result)\n                        current_seuser, current_serole, current_setype, current_serange = result.split(':')\n                    except ValueError:\n                        log.error('Unable to set current selinux attributes')\n                        ret['result'] = False\n                        ret['comment'].append(\n                            'Failed to set selinux attributes'\n                        )\n                        selinux_error = True\n\n                    if not selinux_error:\n                        ret['comment'].append('The file {0} is set to be changed'.format(name))\n\n                        if requested_seuser:\n                            if current_seuser != requested_seuser:\n                                ret['comment'].append(\"Unable to update seuser context\")\n                                ret['result'] = False\n                        if requested_serole:\n                            if current_serole != requested_serole:\n                                ret['comment'].append(\"Unable to update serole context\")\n                                ret['result'] = False\n                        if requested_setype:\n                            if current_setype != requested_setype:\n                                ret['comment'].append(\"Unable to update setype context\")\n                                ret['result'] = False\n                        if requested_serange:\n                            if current_serange != requested_serange:\n                                ret['comment'].append(\"Unable to update serange context\")\n                                ret['result'] = False\n                        ret['changes']['selinux'] = {'Old': selinux_change_orig.strip(),\n                                                     'New': selinux_change_new.strip()}\n\n    # Only combine the comment list into a string\n    # after all comments are added above\n    if isinstance(orig_comment, six.string_types):\n        if orig_comment:\n            ret['comment'].insert(0, orig_comment)\n        ret['comment'] = '; '.join(ret['comment'])\n\n    # Set result to None at the very end of the function,\n    # after all changes have been recorded above\n    if __opts__['test'] is True and ret['changes']:\n        ret['result'] = None\n\n    return ret, perms"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check_managed(\n        name,\n        source,\n        source_hash,\n        source_hash_name,\n        user,\n        group,\n        mode,\n        attrs,\n        template,\n        context,\n        defaults,\n        saltenv,\n        contents=None,\n        skip_verify=False,\n        seuser=None,\n        serole=None,\n        setype=None,\n        serange=None,\n        **kwargs):\n    '''\n    Check to see what changes need to be made for a file\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.check_managed /etc/httpd/conf.d/httpd.conf salt://http/httpd.conf '{hash_type: 'md5', 'hsum': <md5sum>}' root, root, '755' jinja True None None base\n    '''\n    # If the source is a list then find which file exists\n    source, source_hash = source_list(source,           # pylint: disable=W0633\n                                      source_hash,\n                                      saltenv)\n\n    sfn = ''\n    source_sum = None\n\n    if contents is None:\n        # Gather the source file from the server\n        sfn, source_sum, comments = get_managed(\n            name,\n            template,\n            source,\n            source_hash,\n            source_hash_name,\n            user,\n            group,\n            mode,\n            attrs,\n            saltenv,\n            context,\n            defaults,\n            skip_verify,\n            **kwargs)\n        if comments:\n            __clean_tmp(sfn)\n            return False, comments\n    changes = check_file_meta(name, sfn, source, source_sum, user,\n                              group, mode, attrs, saltenv, contents,\n                              seuser=seuser, serole=serole, setype=setype,\n                              serange=serange)\n    # Ignore permission for files written temporary directories\n    # Files in any path will still be set correctly using get_managed()\n    if name.startswith(tempfile.gettempdir()):\n        for key in ['user', 'group', 'mode']:\n            changes.pop(key, None)\n    __clean_tmp(sfn)\n    if changes:\n        log.info(changes)\n        comments = ['The following values are set to be changed:\\n']\n        comments.extend('{0}: {1}\\n'.format(key, val)\n                        for key, val in six.iteritems(changes))\n        return None, ''.join(comments)\n    return True, 'The file {0} is in the correct state'.format(name)", "response": "Check if a file is managed by the server"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check_managed_changes(\n        name,\n        source,\n        source_hash,\n        source_hash_name,\n        user,\n        group,\n        mode,\n        attrs,\n        template,\n        context,\n        defaults,\n        saltenv,\n        contents=None,\n        skip_verify=False,\n        keep_mode=False,\n        seuser=None,\n        serole=None,\n        setype=None,\n        serange=None,\n        **kwargs):\n    '''\n    Return a dictionary of what changes need to be made for a file\n\n    .. versionchanged:: Neon\n\n        selinux attributes added\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.check_managed_changes /etc/httpd/conf.d/httpd.conf salt://http/httpd.conf '{hash_type: 'md5', 'hsum': <md5sum>}' root, root, '755' jinja True None None base\n    '''\n    # If the source is a list then find which file exists\n    source, source_hash = source_list(source,           # pylint: disable=W0633\n                                      source_hash,\n                                      saltenv)\n\n    sfn = ''\n    source_sum = None\n\n    if contents is None:\n        # Gather the source file from the server\n        sfn, source_sum, comments = get_managed(\n            name,\n            template,\n            source,\n            source_hash,\n            source_hash_name,\n            user,\n            group,\n            mode,\n            attrs,\n            saltenv,\n            context,\n            defaults,\n            skip_verify,\n            **kwargs)\n\n        # Ensure that user-provided hash string is lowercase\n        if source_sum and ('hsum' in source_sum):\n            source_sum['hsum'] = source_sum['hsum'].lower()\n\n        if comments:\n            __clean_tmp(sfn)\n            raise CommandExecutionError(comments)\n        if sfn and source and keep_mode:\n            if _urlparse(source).scheme in ('salt', 'file') \\\n                    or source.startswith('/'):\n                try:\n                    mode = __salt__['cp.stat_file'](source, saltenv=saltenv, octal=True)\n                except Exception as exc:\n                    log.warning('Unable to stat %s: %s', sfn, exc)\n    changes = check_file_meta(name, sfn, source, source_sum, user,\n                              group, mode, attrs, saltenv, contents,\n                              seuser=seuser, serole=serole, setype=setype, serange=serange)\n    __clean_tmp(sfn)\n    return changes", "response": "Check if a file or a list of files is managed and return a dictionary of what changes need to be made for a file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck for the changes in the file metadata.", "response": "def check_file_meta(\n        name,\n        sfn,\n        source,\n        source_sum,\n        user,\n        group,\n        mode,\n        attrs,\n        saltenv,\n        contents=None,\n        seuser=None,\n        serole=None,\n        setype=None,\n        serange=None):\n    '''\n    Check for the changes in the file metadata.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.check_file_meta /etc/httpd/conf.d/httpd.conf salt://http/httpd.conf '{hash_type: 'md5', 'hsum': <md5sum>}' root, root, '755' base\n\n    .. note::\n\n        Supported hash types include sha512, sha384, sha256, sha224, sha1, and\n        md5.\n\n    name\n        Path to file destination\n\n    sfn\n        Template-processed source file contents\n\n    source\n        URL to file source\n\n    source_sum\n        File checksum information as a dictionary\n\n        .. code-block:: yaml\n\n            {hash_type: md5, hsum: <md5sum>}\n\n    user\n        Destination file user owner\n\n    group\n        Destination file group owner\n\n    mode\n        Destination file permissions mode\n\n    attrs\n        Destination file attributes\n\n        .. versionadded:: 2018.3.0\n\n    saltenv\n        Salt environment used to resolve source files\n\n    contents\n        File contents\n\n    seuser\n        selinux user attribute\n\n        .. versionadded:: Neon\n\n    serole\n        selinux role attribute\n\n        .. versionadded:: Neon\n\n    setype\n        selinux type attribute\n\n        .. versionadded:: Neon\n\n    serange\n        selinux range attribute\n\n        .. versionadded:: Neon\n    '''\n    changes = {}\n    if not source_sum:\n        source_sum = dict()\n\n    try:\n        lstats = stats(name, hash_type=source_sum.get('hash_type', None),\n                       follow_symlinks=False)\n    except CommandExecutionError:\n        lstats = {}\n\n    if not lstats:\n        changes['newfile'] = name\n        return changes\n\n    if 'hsum' in source_sum:\n        if source_sum['hsum'] != lstats['sum']:\n            if not sfn and source:\n                sfn = __salt__['cp.cache_file'](\n                    source,\n                    saltenv,\n                    source_hash=source_sum['hsum'])\n            if sfn:\n                try:\n                    changes['diff'] = get_diff(\n                        name, sfn, template=True, show_filenames=False)\n                except CommandExecutionError as exc:\n                    changes['diff'] = exc.strerror\n            else:\n                changes['sum'] = 'Checksum differs'\n\n    if contents is not None:\n        # Write a tempfile with the static contents\n        tmp = salt.utils.files.mkstemp(prefix=salt.utils.files.TEMPFILE_PREFIX,\n                                       text=True)\n        if salt.utils.platform.is_windows():\n            contents = os.linesep.join(\n                _splitlines_preserving_trailing_newline(contents))\n        with salt.utils.files.fopen(tmp, 'w') as tmp_:\n            tmp_.write(salt.utils.stringutils.to_str(contents))\n        # Compare the static contents with the named file\n        try:\n            differences = get_diff(name, tmp, show_filenames=False)\n        except CommandExecutionError as exc:\n            log.error('Failed to diff files: %s', exc)\n            differences = exc.strerror\n        __clean_tmp(tmp)\n        if differences:\n            if __salt__['config.option']('obfuscate_templates'):\n                changes['diff'] = '<Obfuscated Template>'\n            else:\n                changes['diff'] = differences\n\n    if not salt.utils.platform.is_windows():\n        # Check owner\n        if (user is not None\n                and user != lstats['user']\n                and user != lstats['uid']):\n            changes['user'] = user\n\n        # Check group\n        if (group is not None\n                and group != lstats['group']\n                and group != lstats['gid']):\n            changes['group'] = group\n\n        # Normalize the file mode\n        smode = salt.utils.files.normalize_mode(lstats['mode'])\n        mode = salt.utils.files.normalize_mode(mode)\n        if mode is not None and mode != smode:\n            changes['mode'] = mode\n\n        if attrs:\n            diff_attrs = _cmp_attrs(name, attrs)\n            if diff_attrs is not None:\n                if attrs is not None \\\n                        and (diff_attrs[0] is not None\n                             or diff_attrs[1] is not None):\n                    changes['attrs'] = attrs\n\n        # Check selinux\n        if seuser or serole or setype or serange:\n            try:\n                current_seuser, current_serole, current_setype, current_serange = get_selinux_context(name).split(':')\n                log.debug(\n                    'Current selinux context user:%s role:%s type:%s range:%s',\n                    current_seuser, current_serole, current_setype, current_serange\n                )\n            except ValueError as exc:\n                log.error('Unable to get current selinux attributes')\n                changes['selinux'] = exc.strerror\n\n            if seuser and seuser != current_seuser:\n                changes['selinux'] = {\"user\": seuser}\n            if serole and serole != current_serole:\n                changes['selinux'] = {\"role\": serole}\n            if setype and setype != current_setype:\n                changes['selinux'] = {\"type\": setype}\n            if serange and serange != current_serange:\n                changes['selinux'] = {\"range\": serange}\n\n    return changes"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_diff(file1,\n             file2,\n             saltenv='base',\n             show_filenames=True,\n             show_changes=True,\n             template=False,\n             source_hash_file1=None,\n             source_hash_file2=None):\n    '''\n    Return unified diff of two files\n\n    file1\n        The first file to feed into the diff utility\n\n        .. versionchanged:: 2018.3.0\n            Can now be either a local or remote file. In earlier releases,\n            thuis had to be a file local to the minion.\n\n    file2\n        The second file to feed into the diff utility\n\n        .. versionchanged:: 2018.3.0\n            Can now be either a local or remote file. In earlier releases, this\n            had to be a file on the salt fileserver (i.e.\n            ``salt://somefile.txt``)\n\n    show_filenames : True\n        Set to ``False`` to hide the filenames in the top two lines of the\n        diff.\n\n    show_changes : True\n        If set to ``False``, and there are differences, then instead of a diff\n        a simple message stating that show_changes is set to ``False`` will be\n        returned.\n\n    template : False\n        Set to ``True`` if two templates are being compared. This is not useful\n        except for within states, with the ``obfuscate_templates`` option set\n        to ``True``.\n\n        .. versionadded:: 2018.3.0\n\n    source_hash_file1\n        If ``file1`` is an http(s)/ftp URL and the file exists in the minion's\n        file cache, this option can be passed to keep the minion from\n        re-downloading the archive if the cached copy matches the specified\n        hash.\n\n        .. versionadded:: 2018.3.0\n\n    source_hash_file2\n        If ``file2`` is an http(s)/ftp URL and the file exists in the minion's\n        file cache, this option can be passed to keep the minion from\n        re-downloading the archive if the cached copy matches the specified\n        hash.\n\n        .. versionadded:: 2018.3.0\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' file.get_diff /home/fred/.vimrc salt://users/fred/.vimrc\n        salt '*' file.get_diff /tmp/foo.txt /tmp/bar.txt\n    '''\n    files = (file1, file2)\n    source_hashes = (source_hash_file1, source_hash_file2)\n    paths = []\n    errors = []\n\n    for filename, source_hash in zip(files, source_hashes):\n        try:\n            # Local file paths will just return the same path back when passed\n            # to cp.cache_file.\n            cached_path = __salt__['cp.cache_file'](filename,\n                                                    saltenv,\n                                                    source_hash=source_hash)\n            if cached_path is False:\n                errors.append(\n                    'File {0} not found'.format(\n                        salt.utils.stringutils.to_unicode(filename)\n                    )\n                )\n                continue\n            paths.append(cached_path)\n        except MinionError as exc:\n            errors.append(salt.utils.stringutils.to_unicode(exc.__str__()))\n            continue\n\n    if errors:\n        raise CommandExecutionError(\n            'Failed to cache one or more files',\n            info=errors\n        )\n\n    args = []\n    for filename in paths:\n        try:\n            with salt.utils.files.fopen(filename, 'rb') as fp_:\n                args.append(fp_.readlines())\n        except (IOError, OSError) as exc:\n            raise CommandExecutionError(\n                'Failed to read {0}: {1}'.format(\n                    salt.utils.stringutils.to_unicode(filename),\n                    exc.strerror\n                )\n            )\n\n    if args[0] != args[1]:\n        if template and __salt__['config.option']('obfuscate_templates'):\n            ret = '<Obfuscated Template>'\n        elif not show_changes:\n            ret = '<show_changes=False>'\n        else:\n            bdiff = _binary_replace(*paths)  # pylint: disable=no-value-for-parameter\n            if bdiff:\n                ret = bdiff\n            else:\n                if show_filenames:\n                    args.extend(paths)\n                ret = __utils__['stringutils.get_diff'](*args)\n        return ret\n    return ''", "response": "Return the unified diff of two files in a single environment"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmanage a file from the master and return the file contents.", "response": "def manage_file(name,\n                sfn,\n                ret,\n                source,\n                source_sum,\n                user,\n                group,\n                mode,\n                attrs,\n                saltenv,\n                backup,\n                makedirs=False,\n                template=None,   # pylint: disable=W0613\n                show_changes=True,\n                contents=None,\n                dir_mode=None,\n                follow_symlinks=True,\n                skip_verify=False,\n                keep_mode=False,\n                encoding=None,\n                encoding_errors='strict',\n                seuser=None,\n                serole=None,\n                setype=None,\n                serange=None,\n                **kwargs):\n    '''\n    Checks the destination against what was retrieved with get_managed and\n    makes the appropriate modifications (if necessary).\n\n    name\n        location to place the file\n\n    sfn\n        location of cached file on the minion\n\n        This is the path to the file stored on the minion. This file is placed\n        on the minion using cp.cache_file.  If the hash sum of that file\n        matches the source_sum, we do not transfer the file to the minion\n        again.\n\n        This file is then grabbed and if it has template set, it renders the\n        file to be placed into the correct place on the system using\n        salt.files.utils.copyfile()\n\n    ret\n        The initial state return data structure. Pass in ``None`` to use the\n        default structure.\n\n    source\n        file reference on the master\n\n    source_sum\n        sum hash for source\n\n    user\n        user owner\n\n    group\n        group owner\n\n    backup\n        backup_mode\n\n    attrs\n        attributes to be set on file: '' means remove all of them\n\n        .. versionadded:: 2018.3.0\n\n    makedirs\n        make directories if they do not exist\n\n    template\n        format of templating\n\n    show_changes\n        Include diff in state return\n\n    contents:\n        contents to be placed in the file\n\n    dir_mode\n        mode for directories created with makedirs\n\n    skip_verify : False\n        If ``True``, hash verification of remote file sources (``http://``,\n        ``https://``, ``ftp://``) will be skipped, and the ``source_hash``\n        argument will be ignored.\n\n        .. versionadded:: 2016.3.0\n\n    keep_mode : False\n        If ``True``, and the ``source`` is a file from the Salt fileserver (or\n        a local file on the minion), the mode of the destination file will be\n        set to the mode of the source file.\n\n        .. note:: keep_mode does not work with salt-ssh.\n\n            As a consequence of how the files are transferred to the minion, and\n            the inability to connect back to the master with salt-ssh, salt is\n            unable to stat the file as it exists on the fileserver and thus\n            cannot mirror the mode on the salt-ssh minion\n\n    encoding\n        If specified, then the specified encoding will be used. Otherwise, the\n        file will be encoded using the system locale (usually UTF-8). See\n        https://docs.python.org/3/library/codecs.html#standard-encodings for\n        the list of available encodings.\n\n        .. versionadded:: 2017.7.0\n\n    encoding_errors : 'strict'\n        Default is ```'strict'```.\n        See https://docs.python.org/2/library/codecs.html#codec-base-classes\n        for the error handling schemes.\n\n        .. versionadded:: 2017.7.0\n\n    seuser\n        selinux user attribute\n\n        .. versionadded:: Neon\n\n    serange\n        selinux range attribute\n\n        .. versionadded:: Neon\n\n    setype\n        selinux type attribute\n\n        .. versionadded:: Neon\n\n    serange\n        selinux range attribute\n\n        .. versionadded:: Neon\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.manage_file /etc/httpd/conf.d/httpd.conf '' '{}' salt://http/httpd.conf '{hash_type: 'md5', 'hsum': <md5sum>}' root root '755' '' base ''\n\n    .. versionchanged:: 2014.7.0\n        ``follow_symlinks`` option added\n\n    '''\n    name = os.path.expanduser(name)\n\n    if not ret:\n        ret = {'name': name,\n               'changes': {},\n               'comment': '',\n               'result': True}\n    # Ensure that user-provided hash string is lowercase\n    if source_sum and ('hsum' in source_sum):\n        source_sum['hsum'] = source_sum['hsum'].lower()\n\n    if source:\n        if not sfn:\n            # File is not present, cache it\n            sfn = __salt__['cp.cache_file'](source, saltenv)\n            if not sfn:\n                return _error(\n                    ret, 'Source file \\'{0}\\' not found'.format(source))\n            htype = source_sum.get('hash_type', __opts__['hash_type'])\n            # Recalculate source sum now that file has been cached\n            source_sum = {\n                'hash_type': htype,\n                'hsum': get_hash(sfn, form=htype)\n            }\n\n        if keep_mode:\n            if _urlparse(source).scheme in ('salt', 'file', ''):\n                try:\n                    mode = __salt__['cp.stat_file'](source, saltenv=saltenv, octal=True)\n                except Exception as exc:\n                    log.warning('Unable to stat %s: %s', sfn, exc)\n\n    # Check changes if the target file exists\n    if os.path.isfile(name) or os.path.islink(name):\n        if os.path.islink(name) and follow_symlinks:\n            real_name = os.path.realpath(name)\n        else:\n            real_name = name\n\n        # Only test the checksums on files with managed contents\n        if source and not (not follow_symlinks and os.path.islink(real_name)):\n            name_sum = get_hash(real_name, source_sum.get('hash_type', __opts__['hash_type']))\n        else:\n            name_sum = None\n\n        # Check if file needs to be replaced\n        if source and (name_sum is None or source_sum.get('hsum', __opts__['hash_type']) != name_sum):\n            if not sfn:\n                sfn = __salt__['cp.cache_file'](source, saltenv)\n            if not sfn:\n                return _error(\n                    ret, 'Source file \\'{0}\\' not found'.format(source))\n            # If the downloaded file came from a non salt server or local\n            # source, and we are not skipping checksum verification, then\n            # verify that it matches the specified checksum.\n            if not skip_verify \\\n                    and _urlparse(source).scheme != 'salt':\n                dl_sum = get_hash(sfn, source_sum['hash_type'])\n                if dl_sum != source_sum['hsum']:\n                    ret['comment'] = (\n                        'Specified {0} checksum for {1} ({2}) does not match '\n                        'actual checksum ({3}). If the \\'source_hash\\' value '\n                        'refers to a remote file with multiple possible '\n                        'matches, then it may be necessary to set '\n                        '\\'source_hash_name\\'.'.format(\n                            source_sum['hash_type'],\n                            source,\n                            source_sum['hsum'],\n                            dl_sum\n                        )\n                    )\n                    ret['result'] = False\n                    return ret\n\n            # Print a diff equivalent to diff -u old new\n            if __salt__['config.option']('obfuscate_templates'):\n                ret['changes']['diff'] = '<Obfuscated Template>'\n            elif not show_changes:\n                ret['changes']['diff'] = '<show_changes=False>'\n            else:\n                try:\n                    ret['changes']['diff'] = get_diff(\n                        real_name, sfn, show_filenames=False)\n                except CommandExecutionError as exc:\n                    ret['changes']['diff'] = exc.strerror\n\n            # Pre requisites are met, and the file needs to be replaced, do it\n            try:\n                salt.utils.files.copyfile(sfn,\n                                          real_name,\n                                          __salt__['config.backup_mode'](backup),\n                                          __opts__['cachedir'])\n            except IOError as io_error:\n                __clean_tmp(sfn)\n                return _error(\n                    ret, 'Failed to commit change: {0}'.format(io_error))\n\n        if contents is not None:\n            # Write the static contents to a temporary file\n            tmp = salt.utils.files.mkstemp(prefix=salt.utils.files.TEMPFILE_PREFIX,\n                                           text=True)\n            if salt.utils.platform.is_windows():\n                contents = os.linesep.join(\n                    _splitlines_preserving_trailing_newline(contents))\n            with salt.utils.files.fopen(tmp, 'wb') as tmp_:\n                if encoding:\n                    log.debug('File will be encoded with %s', encoding)\n                    tmp_.write(contents.encode(encoding=encoding, errors=encoding_errors))\n                else:\n                    tmp_.write(salt.utils.stringutils.to_bytes(contents))\n\n            try:\n                differences = get_diff(\n                    real_name, tmp, show_filenames=False,\n                    show_changes=show_changes, template=True)\n\n            except CommandExecutionError as exc:\n                ret.setdefault('warnings', []).append(\n                    'Failed to detect changes to file: {0}'.format(exc.strerror)\n                )\n                differences = ''\n\n            if differences:\n                ret['changes']['diff'] = differences\n\n                # Pre requisites are met, the file needs to be replaced, do it\n                try:\n                    salt.utils.files.copyfile(tmp,\n                                              real_name,\n                                              __salt__['config.backup_mode'](backup),\n                                              __opts__['cachedir'])\n                except IOError as io_error:\n                    __clean_tmp(tmp)\n                    return _error(\n                        ret, 'Failed to commit change: {0}'.format(io_error))\n            __clean_tmp(tmp)\n\n        # Check for changing symlink to regular file here\n        if os.path.islink(name) and not follow_symlinks:\n            if not sfn:\n                sfn = __salt__['cp.cache_file'](source, saltenv)\n            if not sfn:\n                return _error(\n                    ret, 'Source file \\'{0}\\' not found'.format(source))\n            # If the downloaded file came from a non salt server source verify\n            # that it matches the intended sum value\n            if not skip_verify and _urlparse(source).scheme != 'salt':\n                dl_sum = get_hash(sfn, source_sum['hash_type'])\n                if dl_sum != source_sum['hsum']:\n                    ret['comment'] = (\n                        'Specified {0} checksum for {1} ({2}) does not match '\n                        'actual checksum ({3})'.format(\n                            source_sum['hash_type'],\n                            name,\n                            source_sum['hsum'],\n                            dl_sum\n                        )\n                    )\n                    ret['result'] = False\n                    return ret\n\n            try:\n                salt.utils.files.copyfile(sfn,\n                                    name,\n                                    __salt__['config.backup_mode'](backup),\n                                    __opts__['cachedir'])\n            except IOError as io_error:\n                __clean_tmp(sfn)\n                return _error(\n                    ret, 'Failed to commit change: {0}'.format(io_error))\n\n            ret['changes']['diff'] = \\\n                'Replace symbolic link with regular file'\n\n        if salt.utils.platform.is_windows():\n            # This function resides in win_file.py and will be available\n            # on Windows. The local function will be overridden\n            # pylint: disable=E1120,E1121,E1123\n            ret = check_perms(\n                path=name,\n                ret=ret,\n                owner=kwargs.get('win_owner'),\n                grant_perms=kwargs.get('win_perms'),\n                deny_perms=kwargs.get('win_deny_perms'),\n                inheritance=kwargs.get('win_inheritance', True),\n                reset=kwargs.get('win_perms_reset', False))\n            # pylint: enable=E1120,E1121,E1123\n        else:\n            ret, _ = check_perms(name, ret, user, group, mode, attrs, follow_symlinks,\n                                 seuser=seuser, serole=serole, setype=setype, serange=serange)\n\n        if ret['changes']:\n            ret['comment'] = 'File {0} updated'.format(\n                salt.utils.data.decode(name)\n            )\n\n        elif not ret['changes'] and ret['result']:\n            ret['comment'] = 'File {0} is in the correct state'.format(\n                salt.utils.data.decode(name)\n            )\n        if sfn:\n            __clean_tmp(sfn)\n        return ret\n    else:  # target file does not exist\n        contain_dir = os.path.dirname(name)\n\n        def _set_mode_and_make_dirs(name, dir_mode, mode, user, group):\n            # check for existence of windows drive letter\n            if salt.utils.platform.is_windows():\n                drive, _ = os.path.splitdrive(name)\n                if drive and not os.path.exists(drive):\n                    __clean_tmp(sfn)\n                    return _error(ret,\n                                  '{0} drive not present'.format(drive))\n            if dir_mode is None and mode is not None:\n                # Add execute bit to each nonzero digit in the mode, if\n                # dir_mode was not specified. Otherwise, any\n                # directories created with makedirs_() below can't be\n                # listed via a shell.\n                mode_list = [x for x in six.text_type(mode)][-3:]\n                for idx in range(len(mode_list)):\n                    if mode_list[idx] != '0':\n                        mode_list[idx] = six.text_type(int(mode_list[idx]) | 1)\n                dir_mode = ''.join(mode_list)\n\n            if salt.utils.platform.is_windows():\n                # This function resides in win_file.py and will be available\n                # on Windows. The local function will be overridden\n                # pylint: disable=E1120,E1121,E1123\n                makedirs_(\n                    path=name,\n                    owner=kwargs.get('win_owner'),\n                    grant_perms=kwargs.get('win_perms'),\n                    deny_perms=kwargs.get('win_deny_perms'),\n                    inheritance=kwargs.get('win_inheritance', True),\n                    reset=kwargs.get('win_perms_reset', False))\n                # pylint: enable=E1120,E1121,E1123\n            else:\n                makedirs_(name, user=user, group=group, mode=dir_mode)\n\n        if source:\n            # Apply the new file\n            if not sfn:\n                sfn = __salt__['cp.cache_file'](source, saltenv)\n            if not sfn:\n                return _error(\n                    ret, 'Source file \\'{0}\\' not found'.format(source))\n            # If the downloaded file came from a non salt server source verify\n            # that it matches the intended sum value\n            if not skip_verify \\\n                    and _urlparse(source).scheme != 'salt':\n                dl_sum = get_hash(sfn, source_sum['hash_type'])\n                if dl_sum != source_sum['hsum']:\n                    ret['comment'] = (\n                        'Specified {0} checksum for {1} ({2}) does not match '\n                        'actual checksum ({3})'.format(\n                            source_sum['hash_type'],\n                            name,\n                            source_sum['hsum'],\n                            dl_sum\n                        )\n                    )\n                    ret['result'] = False\n                    return ret\n            # It is a new file, set the diff accordingly\n            ret['changes']['diff'] = 'New file'\n            if not os.path.isdir(contain_dir):\n                if makedirs:\n                    _set_mode_and_make_dirs(name, dir_mode, mode, user, group)\n                else:\n                    __clean_tmp(sfn)\n                    # No changes actually made\n                    ret['changes'].pop('diff', None)\n                    return _error(ret, 'Parent directory not present')\n        else:  # source != True\n            if not os.path.isdir(contain_dir):\n                if makedirs:\n                    _set_mode_and_make_dirs(name, dir_mode, mode, user, group)\n                else:\n                    __clean_tmp(sfn)\n                    # No changes actually made\n                    ret['changes'].pop('diff', None)\n                    return _error(ret, 'Parent directory not present')\n\n            # Create the file, user rw-only if mode will be set to prevent\n            # a small security race problem before the permissions are set\n            with salt.utils.files.set_umask(0o077 if mode else None):\n                # Create a new file when test is False and source is None\n                if contents is None:\n                    if not __opts__['test']:\n                        if touch(name):\n                            ret['changes']['new'] = 'file {0} created'.format(name)\n                            ret['comment'] = 'Empty file'\n                        else:\n                            return _error(\n                                ret, 'Empty file {0} not created'.format(name)\n                            )\n                else:\n                    if not __opts__['test']:\n                        if touch(name):\n                            ret['changes']['diff'] = 'New file'\n                        else:\n                            return _error(\n                                ret, 'File {0} not created'.format(name)\n                            )\n\n        if contents is not None:\n            # Write the static contents to a temporary file\n            tmp = salt.utils.files.mkstemp(prefix=salt.utils.files.TEMPFILE_PREFIX,\n                                           text=True)\n            with salt.utils.files.fopen(tmp, 'wb') as tmp_:\n                if encoding:\n                    if salt.utils.platform.is_windows():\n                        contents = os.linesep.join(\n                            _splitlines_preserving_trailing_newline(contents))\n                    log.debug('File will be encoded with %s', encoding)\n                    tmp_.write(contents.encode(encoding=encoding, errors=encoding_errors))\n                else:\n                    tmp_.write(salt.utils.stringutils.to_bytes(contents))\n\n            # Copy into place\n            salt.utils.files.copyfile(tmp,\n                                name,\n                                __salt__['config.backup_mode'](backup),\n                                __opts__['cachedir'])\n            __clean_tmp(tmp)\n        # Now copy the file contents if there is a source file\n        elif sfn:\n            salt.utils.files.copyfile(sfn,\n                                name,\n                                __salt__['config.backup_mode'](backup),\n                                __opts__['cachedir'])\n            __clean_tmp(sfn)\n\n        # This is a new file, if no mode specified, use the umask to figure\n        # out what mode to use for the new file.\n        if mode is None and not salt.utils.platform.is_windows():\n            # Get current umask\n            mask = salt.utils.files.get_umask()\n            # Calculate the mode value that results from the umask\n            mode = oct((0o777 ^ mask) & 0o666)\n\n        if salt.utils.platform.is_windows():\n            # This function resides in win_file.py and will be available\n            # on Windows. The local function will be overridden\n            # pylint: disable=E1120,E1121,E1123\n            ret = check_perms(\n                path=name,\n                ret=ret,\n                owner=kwargs.get('win_owner'),\n                grant_perms=kwargs.get('win_perms'),\n                deny_perms=kwargs.get('win_deny_perms'),\n                inheritance=kwargs.get('win_inheritance', True),\n                reset=kwargs.get('win_perms_reset', False))\n            # pylint: enable=E1120,E1121,E1123\n        else:\n            ret, _ = check_perms(name, ret, user, group, mode, attrs,\n                                 seuser=seuser, serole=serole, setype=setype, serange=serange)\n\n        if not ret['comment']:\n            ret['comment'] = 'File ' + name + ' updated'\n\n        if __opts__['test']:\n            ret['comment'] = 'File ' + name + ' not updated'\n        elif not ret['changes'] and ret['result']:\n            ret['comment'] = 'File ' + name + ' is in the correct state'\n        if sfn:\n            __clean_tmp(sfn)\n\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef mkdir(dir_path,\n          user=None,\n          group=None,\n          mode=None):\n    '''\n    Ensure that a directory is available.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.mkdir /opt/jetty/context\n    '''\n    dir_path = os.path.expanduser(dir_path)\n\n    directory = os.path.normpath(dir_path)\n\n    if not os.path.isdir(directory):\n        # If a caller such as managed() is invoked  with makedirs=True, make\n        # sure that any created dirs are created with the same user and group\n        # to follow the principal of least surprise method.\n        makedirs_perms(directory, user, group, mode)\n\n    return True", "response": "Ensure that a directory exists."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a new directory containing the specified path.", "response": "def makedirs_(path,\n              user=None,\n              group=None,\n              mode=None):\n    '''\n    Ensure that the directory containing this path is available.\n\n    .. note::\n\n        The path must end with a trailing slash otherwise the directory/directories\n        will be created up to the parent directory. For example if path is\n        ``/opt/code``, then it would be treated as ``/opt/`` but if the path\n        ends with a trailing slash like ``/opt/code/``, then it would be\n        treated as ``/opt/code/``.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.makedirs /opt/code/\n    '''\n    path = os.path.expanduser(path)\n\n    if mode:\n        mode = salt.utils.files.normalize_mode(mode)\n\n    # walk up the directory structure until we find the first existing\n    # directory\n    dirname = os.path.normpath(os.path.dirname(path))\n\n    if os.path.isdir(dirname):\n        # There's nothing for us to do\n        msg = 'Directory \\'{0}\\' already exists'.format(dirname)\n        log.debug(msg)\n        return msg\n\n    if os.path.exists(dirname):\n        msg = 'The path \\'{0}\\' already exists and is not a directory'.format(\n            dirname\n        )\n        log.debug(msg)\n        return msg\n\n    directories_to_create = []\n    while True:\n        if os.path.isdir(dirname):\n            break\n\n        directories_to_create.append(dirname)\n        current_dirname = dirname\n        dirname = os.path.dirname(dirname)\n\n        if current_dirname == dirname:\n            raise SaltInvocationError(\n                'Recursive creation for path \\'{0}\\' would result in an '\n                'infinite loop. Please use an absolute path.'.format(dirname)\n            )\n\n    # create parent directories from the topmost to the most deeply nested one\n    directories_to_create.reverse()\n    for directory_to_create in directories_to_create:\n        # all directories have the user, group and mode set!!\n        log.debug('Creating directory: %s', directory_to_create)\n        mkdir(directory_to_create, user=user, group=group, mode=mode)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef makedirs_perms(name,\n                   user=None,\n                   group=None,\n                   mode='0755'):\n    '''\n    Taken and modified from os.makedirs to set user, group and mode for each\n    directory created.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.makedirs_perms /opt/code\n    '''\n    name = os.path.expanduser(name)\n\n    path = os.path\n    head, tail = path.split(name)\n    if not tail:\n        head, tail = path.split(head)\n    if head and tail and not path.exists(head):\n        try:\n            makedirs_perms(head, user, group, mode)\n        except OSError as exc:\n            # be happy if someone already created the path\n            if exc.errno != errno.EEXIST:\n                raise\n        if tail == os.curdir:  # xxx/newdir/. exists if xxx/newdir exists\n            return\n    os.mkdir(name)\n    check_perms(name,\n                None,\n                user,\n                group,\n                int('{0}'.format(mode)) if mode else None)", "response": "Create the named directory and all its permissions."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if a file exists and is a character device.", "response": "def is_chrdev(name):\n    '''\n    Check if a file exists and is a character device.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n       salt '*' file.is_chrdev /dev/chr\n    '''\n    name = os.path.expanduser(name)\n\n    stat_structure = None\n    try:\n        stat_structure = os.stat(name)\n    except OSError as exc:\n        if exc.errno == errno.ENOENT:\n            # If the character device does not exist in the first place\n            return False\n        else:\n            raise\n    return stat.S_ISCHR(stat_structure.st_mode)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a new character device.", "response": "def mknod_chrdev(name,\n                 major,\n                 minor,\n                 user=None,\n                 group=None,\n                 mode='0660'):\n    '''\n    .. versionadded:: 0.17.0\n\n    Create a character device.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n       salt '*' file.mknod_chrdev /dev/chr 180 31\n    '''\n    name = os.path.expanduser(name)\n\n    ret = {'name': name,\n           'changes': {},\n           'comment': '',\n           'result': False}\n    log.debug('Creating character device name:%s major:%s minor:%s mode:%s',\n              name, major, minor, mode)\n    try:\n        if __opts__['test']:\n            ret['changes'] = {'new': 'Character device {0} created.'.format(name)}\n            ret['result'] = None\n        else:\n            if os.mknod(name,\n                        int(six.text_type(mode).lstrip('0Oo'), 8) | stat.S_IFCHR,\n                        os.makedev(major, minor)) is None:\n                ret['changes'] = {'new': 'Character device {0} created.'.format(name)}\n                ret['result'] = True\n    except OSError as exc:\n        # be happy it is already there....however, if you are trying to change the\n        # major/minor, you will need to unlink it first as os.mknod will not overwrite\n        if exc.errno != errno.EEXIST:\n            raise\n        else:\n            ret['comment'] = 'File {0} exists and cannot be overwritten'.format(name)\n    # quick pass at verifying the permissions of the newly created character device\n    check_perms(name,\n                None,\n                user,\n                group,\n                int('{0}'.format(mode)) if mode else None)\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if a file exists and is a block device.", "response": "def is_blkdev(name):\n    '''\n    Check if a file exists and is a block device.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n       salt '*' file.is_blkdev /dev/blk\n    '''\n    name = os.path.expanduser(name)\n\n    stat_structure = None\n    try:\n        stat_structure = os.stat(name)\n    except OSError as exc:\n        if exc.errno == errno.ENOENT:\n            # If the block device does not exist in the first place\n            return False\n        else:\n            raise\n    return stat.S_ISBLK(stat_structure.st_mode)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_fifo(name):\n    '''\n    Check if a file exists and is a FIFO.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n       salt '*' file.is_fifo /dev/fifo\n    '''\n    name = os.path.expanduser(name)\n\n    stat_structure = None\n    try:\n        stat_structure = os.stat(name)\n    except OSError as exc:\n        if exc.errno == errno.ENOENT:\n            # If the fifo does not exist in the first place\n            return False\n        else:\n            raise\n    return stat.S_ISFIFO(stat_structure.st_mode)", "response": "Check if a file exists and is a FIFO."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a new FIFO.", "response": "def mknod_fifo(name,\n               user=None,\n               group=None,\n               mode='0660'):\n    '''\n    .. versionadded:: 0.17.0\n\n    Create a FIFO pipe.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n       salt '*' file.mknod_fifo /dev/fifo\n    '''\n    name = os.path.expanduser(name)\n\n    ret = {'name': name,\n           'changes': {},\n           'comment': '',\n           'result': False}\n    log.debug('Creating FIFO name: %s', name)\n    try:\n        if __opts__['test']:\n            ret['changes'] = {'new': 'Fifo pipe {0} created.'.format(name)}\n            ret['result'] = None\n        else:\n            if os.mkfifo(name, int(six.text_type(mode).lstrip('0Oo'), 8)) is None:\n                ret['changes'] = {'new': 'Fifo pipe {0} created.'.format(name)}\n                ret['result'] = True\n    except OSError as exc:\n        # be happy it is already there\n        if exc.errno != errno.EEXIST:\n            raise\n        else:\n            ret['comment'] = 'File {0} exists and cannot be overwritten'.format(name)\n    # quick pass at verifying the permissions of the newly created fifo\n    check_perms(name,\n                None,\n                user,\n                group,\n                int('{0}'.format(mode)) if mode else None)\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a node in a block device character device or fifo pipe.", "response": "def mknod(name,\n          ntype,\n          major=0,\n          minor=0,\n          user=None,\n          group=None,\n          mode='0600'):\n    '''\n    .. versionadded:: 0.17.0\n\n    Create a block device, character device, or fifo pipe.\n    Identical to the gnu mknod.\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' file.mknod /dev/chr c 180 31\n        salt '*' file.mknod /dev/blk b 8 999\n        salt '*' file.nknod /dev/fifo p\n    '''\n    ret = False\n    makedirs_(name, user, group)\n    if ntype == 'c':\n        ret = mknod_chrdev(name, major, minor, user, group, mode)\n    elif ntype == 'b':\n        ret = mknod_blkdev(name, major, minor, user, group, mode)\n    elif ntype == 'p':\n        ret = mknod_fifo(name, user, group, mode)\n    else:\n        raise SaltInvocationError(\n            'Node type unavailable: \\'{0}\\'. Available node types are '\n            'character (\\'c\\'), block (\\'b\\'), and pipe (\\'p\\').'.format(ntype)\n        )\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef list_backups(path, limit=None):\n    '''\n    .. versionadded:: 0.17.0\n\n    Lists the previous versions of a file backed up using Salt's :ref:`file\n    state backup <file-state-backups>` system.\n\n    path\n        The path on the minion to check for backups\n    limit\n        Limit the number of results to the most recent N backups\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.list_backups /foo/bar/baz.txt\n    '''\n    path = os.path.expanduser(path)\n\n    try:\n        limit = int(limit)\n    except TypeError:\n        pass\n    except ValueError:\n        log.error('file.list_backups: \\'limit\\' value must be numeric')\n        limit = None\n\n    bkroot = _get_bkroot()\n    parent_dir, basename = os.path.split(path)\n    if salt.utils.platform.is_windows():\n        # ':' is an illegal filesystem path character on Windows\n        src_dir = parent_dir.replace(':', '_')\n    else:\n        src_dir = parent_dir[1:]\n    # Figure out full path of location of backup file in minion cache\n    bkdir = os.path.join(bkroot, src_dir)\n\n    if not os.path.isdir(bkdir):\n        return {}\n\n    files = {}\n    for fname in [x for x in os.listdir(bkdir)\n                  if os.path.isfile(os.path.join(bkdir, x))]:\n        if salt.utils.platform.is_windows():\n            # ':' is an illegal filesystem path character on Windows\n            strpfmt = '{0}_%a_%b_%d_%H-%M-%S_%f_%Y'.format(basename)\n        else:\n            strpfmt = '{0}_%a_%b_%d_%H:%M:%S_%f_%Y'.format(basename)\n        try:\n            timestamp = datetime.datetime.strptime(fname, strpfmt)\n        except ValueError:\n            # File didn't match the strp format string, so it's not a backup\n            # for this file. Move on to the next one.\n            continue\n        if salt.utils.platform.is_windows():\n            str_format = '%a %b %d %Y %H-%M-%S.%f'\n        else:\n            str_format = '%a %b %d %Y %H:%M:%S.%f'\n        files.setdefault(timestamp, {})['Backup Time'] = \\\n            timestamp.strftime(str_format)\n        location = os.path.join(bkdir, fname)\n        files[timestamp]['Size'] = os.stat(location).st_size\n        files[timestamp]['Location'] = location\n\n    return dict(list(zip(\n        list(range(len(files))),\n        [files[x] for x in sorted(files, reverse=True)[:limit]]\n    )))", "response": "Return a list of all backup files in the specified path"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nlist the previous versions of a directory backed up using Salt s state backup system.", "response": "def list_backups_dir(path, limit=None):\n    '''\n    Lists the previous versions of a directory backed up using Salt's :ref:`file\n    state backup <file-state-backups>` system.\n\n    path\n        The directory on the minion to check for backups\n    limit\n        Limit the number of results to the most recent N backups\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.list_backups_dir /foo/bar/baz/\n    '''\n    path = os.path.expanduser(path)\n\n    try:\n        limit = int(limit)\n    except TypeError:\n        pass\n    except ValueError:\n        log.error('file.list_backups_dir: \\'limit\\' value must be numeric')\n        limit = None\n\n    bkroot = _get_bkroot()\n    parent_dir, basename = os.path.split(path)\n    # Figure out full path of location of backup folder in minion cache\n    bkdir = os.path.join(bkroot, parent_dir[1:])\n\n    if not os.path.isdir(bkdir):\n        return {}\n\n    files = {}\n    f = dict([(i, len(list(n))) for i, n in itertools.groupby([x.split(\"_\")[0] for x in sorted(os.listdir(bkdir))])])\n    ff = os.listdir(bkdir)\n    for i, n in six.iteritems(f):\n        ssfile = {}\n        for x in sorted(ff):\n            basename = x.split('_')[0]\n            if i == basename:\n                strpfmt = '{0}_%a_%b_%d_%H:%M:%S_%f_%Y'.format(basename)\n                try:\n                    timestamp = datetime.datetime.strptime(x, strpfmt)\n                except ValueError:\n                    # Folder didn't match the strp format string, so it's not a backup\n                    # for this folder. Move on to the next one.\n                    continue\n                ssfile.setdefault(timestamp, {})['Backup Time'] = \\\n                    timestamp.strftime('%a %b %d %Y %H:%M:%S.%f')\n                location = os.path.join(bkdir, x)\n                ssfile[timestamp]['Size'] = os.stat(location).st_size\n                ssfile[timestamp]['Location'] = location\n\n        sfiles = dict(list(zip(list(range(n)), [ssfile[x] for x in sorted(ssfile, reverse=True)[:limit]])))\n        sefiles = {i: sfiles}\n        files.update(sefiles)\n    return files"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef restore_backup(path, backup_id):\n    '''\n    .. versionadded:: 0.17.0\n\n    Restore a previous version of a file that was backed up using Salt's\n    :ref:`file state backup <file-state-backups>` system.\n\n    path\n        The path on the minion to check for backups\n    backup_id\n        The numeric id for the backup you wish to restore, as found using\n        :mod:`file.list_backups <salt.modules.file.list_backups>`\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.restore_backup /foo/bar/baz.txt 0\n    '''\n    path = os.path.expanduser(path)\n\n    # Note: This only supports minion backups, so this function will need to be\n    # modified if/when master backups are implemented.\n    ret = {'result': False,\n           'comment': 'Invalid backup_id \\'{0}\\''.format(backup_id)}\n    try:\n        if len(six.text_type(backup_id)) == len(six.text_type(int(backup_id))):\n            backup = list_backups(path)[int(backup_id)]\n        else:\n            return ret\n    except ValueError:\n        return ret\n    except KeyError:\n        ret['comment'] = 'backup_id \\'{0}\\' does not exist for ' \\\n                         '{1}'.format(backup_id, path)\n        return ret\n\n    salt.utils.files.backup_minion(path, _get_bkroot())\n    try:\n        shutil.copyfile(backup['Location'], path)\n    except IOError as exc:\n        ret['comment'] = \\\n            'Unable to restore {0} to {1}: ' \\\n            '{2}'.format(backup['Location'], path, exc)\n        return ret\n    else:\n        ret['result'] = True\n        ret['comment'] = 'Successfully restored {0} to ' \\\n                         '{1}'.format(backup['Location'], path)\n\n    # Try to set proper ownership\n    if not salt.utils.platform.is_windows():\n        try:\n            fstat = os.stat(path)\n        except (OSError, IOError):\n            ret['comment'] += ', but was unable to set ownership'\n        else:\n            os.chown(path, fstat.st_uid, fstat.st_gid)\n\n    return ret", "response": "Restore a backup of a file"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef delete_backup(path, backup_id):\n    '''\n    .. versionadded:: 0.17.0\n\n    Delete a previous version of a file that was backed up using Salt's\n    :ref:`file state backup <file-state-backups>` system.\n\n    path\n        The path on the minion to check for backups\n    backup_id\n        The numeric id for the backup you wish to delete, as found using\n        :mod:`file.list_backups <salt.modules.file.list_backups>`\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.delete_backup /var/cache/salt/minion/file_backup/home/foo/bar/baz.txt 0\n    '''\n    path = os.path.expanduser(path)\n\n    ret = {'result': False,\n           'comment': 'Invalid backup_id \\'{0}\\''.format(backup_id)}\n    try:\n        if len(six.text_type(backup_id)) == len(six.text_type(int(backup_id))):\n            backup = list_backups(path)[int(backup_id)]\n        else:\n            return ret\n    except ValueError:\n        return ret\n    except KeyError:\n        ret['comment'] = 'backup_id \\'{0}\\' does not exist for ' \\\n                         '{1}'.format(backup_id, path)\n        return ret\n\n    try:\n        os.remove(backup['Location'])\n    except IOError as exc:\n        ret['comment'] = 'Unable to remove {0}: {1}'.format(backup['Location'],\n                                                            exc)\n    else:\n        ret['result'] = True\n        ret['comment'] = 'Successfully removed {0}'.format(backup['Location'])\n\n    return ret", "response": "Delete a backup from the file system"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a string in the specified file", "response": "def grep(path,\n         pattern,\n         *opts):\n    '''\n    Grep for a string in the specified file\n\n    .. note::\n        This function's return value is slated for refinement in future\n        versions of Salt\n\n    path\n        Path to the file to be searched\n\n        .. note::\n            Globbing is supported (i.e. ``/var/log/foo/*.log``, but if globbing\n            is being used then the path should be quoted to keep the shell from\n            attempting to expand the glob expression.\n\n    pattern\n        Pattern to match. For example: ``test``, or ``a[0-5]``\n\n    opts\n        Additional command-line flags to pass to the grep command. For example:\n        ``-v``, or ``-i -B2``\n\n        .. note::\n            The options should come after a double-dash (as shown in the\n            examples below) to keep Salt's own argument parser from\n            interpreting them.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.grep /etc/passwd nobody\n        salt '*' file.grep /etc/sysconfig/network-scripts/ifcfg-eth0 ipaddr -- -i\n        salt '*' file.grep /etc/sysconfig/network-scripts/ifcfg-eth0 ipaddr -- -i -B2\n        salt '*' file.grep \"/etc/sysconfig/network-scripts/*\" ipaddr -- -i -l\n    '''\n    path = os.path.expanduser(path)\n\n    # Backup the path in case the glob returns nothing\n    _path = path\n    path = glob.glob(path)\n\n    # If the list is empty no files exist\n    # so we revert back to the original path\n    # so the result is an error.\n    if not path:\n        path = _path\n\n    split_opts = []\n    for opt in opts:\n        try:\n            split = salt.utils.args.shlex_split(opt)\n        except AttributeError:\n            split = salt.utils.args.shlex_split(six.text_type(opt))\n        if len(split) > 1:\n            raise SaltInvocationError(\n                'Passing multiple command line arguments in a single string '\n                'is not supported, please pass the following arguments '\n                'separately: {0}'.format(opt)\n            )\n        split_opts.extend(split)\n\n    if isinstance(path, list):\n        cmd = ['grep'] + split_opts + [pattern] + path\n    else:\n        cmd = ['grep'] + split_opts + [pattern, path]\n    try:\n        ret = __salt__['cmd.run_all'](cmd, python_shell=False)\n    except (IOError, OSError) as exc:\n        raise CommandExecutionError(exc.strerror)\n\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef open_files(by_pid=False):\n    '''\n    Return a list of all physical open files on the system.\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' file.open_files\n        salt '*' file.open_files by_pid=True\n    '''\n    # First we collect valid PIDs\n    pids = {}\n    procfs = os.listdir('/proc/')\n    for pfile in procfs:\n        try:\n            pids[int(pfile)] = []\n        except ValueError:\n            # Not a valid PID, move on\n            pass\n\n    # Then we look at the open files for each PID\n    files = {}\n    for pid in pids:\n        ppath = '/proc/{0}'.format(pid)\n        try:\n            tids = os.listdir('{0}/task'.format(ppath))\n        except OSError:\n            continue\n\n        # Collect the names of all of the file descriptors\n        fd_ = []\n\n        #try:\n        #    fd_.append(os.path.realpath('{0}/task/{1}exe'.format(ppath, tid)))\n        #except Exception:\n        #    pass\n\n        for fpath in os.listdir('{0}/fd'.format(ppath)):\n            fd_.append('{0}/fd/{1}'.format(ppath, fpath))\n\n        for tid in tids:\n            try:\n                fd_.append(\n                    os.path.realpath('{0}/task/{1}/exe'.format(ppath, tid))\n                )\n            except OSError:\n                continue\n\n            for tpath in os.listdir('{0}/task/{1}/fd'.format(ppath, tid)):\n                fd_.append('{0}/task/{1}/fd/{2}'.format(ppath, tid, tpath))\n\n        fd_ = sorted(set(fd_))\n\n        # Loop through file descriptors and return useful data for each file\n        for fdpath in fd_:\n            # Sometimes PIDs and TIDs disappear before we can query them\n            try:\n                name = os.path.realpath(fdpath)\n                # Running stat on the file cuts out all of the sockets and\n                # deleted files from the list\n                os.stat(name)\n            except OSError:\n                continue\n\n            if name not in files:\n                files[name] = [pid]\n            else:\n                # We still want to know which PIDs are using each file\n                files[name].append(pid)\n                files[name] = sorted(set(files[name]))\n\n            pids[pid].append(name)\n            pids[pid] = sorted(set(pids[pid]))\n\n    if by_pid:\n        return pids\n    return files", "response": "Return a list of all physical open files on the system."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmove a file or directory or directory containing a single node.", "response": "def move(src, dst):\n    '''\n    Move a file or directory\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.move /path/to/src /path/to/dst\n    '''\n    src = os.path.expanduser(src)\n    dst = os.path.expanduser(dst)\n\n    if not os.path.isabs(src):\n        raise SaltInvocationError('Source path must be absolute.')\n\n    if not os.path.isabs(dst):\n        raise SaltInvocationError('Destination path must be absolute.')\n\n    ret = {\n        'result': True,\n        'comment': \"'{0}' moved to '{1}'\".format(src, dst),\n    }\n\n    try:\n        shutil.move(src, dst)\n    except (OSError, IOError) as exc:\n        raise CommandExecutionError(\n            \"Unable to move '{0}' to '{1}': {2}\".format(src, dst, exc)\n        )\n\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_(key, value, profile=None):\n    '''\n    Set a key/value pair in sqlite3\n    '''\n    if not profile:\n        return False\n    conn, cur, table = _connect(profile)\n    if six.PY2:\n        value = buffer(salt.utils.msgpack.packb(value))\n    else:\n        value = memoryview(salt.utils.msgpack.packb(value))\n    q = profile.get('set_query', ('INSERT OR REPLACE INTO {0} VALUES '\n                                  '(:key, :value)').format(table))\n    conn.execute(q, {'key': key, 'value': value})\n    conn.commit()\n    return True", "response": "Set a key value pair in sqlite3\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get(key, profile=None):\n    '''\n    Get a value from sqlite3\n    '''\n    if not profile:\n        return None\n    _, cur, table = _connect(profile)\n    q = profile.get('get_query', ('SELECT value FROM {0} WHERE '\n                                  'key=:key'.format(table)))\n    res = cur.execute(q, {'key': key})\n    res = res.fetchone()\n    if not res:\n        return None\n    return salt.utils.msgpack.unpackb(res[0])", "response": "Get a value from sqlite3\n   "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef delete(key, profile=None):\n    '''\n    Delete a key/value pair from sqlite3\n    '''\n    if not profile:\n        return None\n    conn, cur, table = _connect(profile)\n    q = profile.get('delete_query', ('DELETE FROM {0} WHERE '\n                                     'key=:key'.format(table)))\n    res = cur.execute(q, {'key': key})\n    conn.commit()\n    return cur.rowcount", "response": "Delete a key value pair from sqlite3\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef show(destination, protocol=None, **kwargs):  # pylint: disable=unused-argument\n\n    '''\n    Displays all details for a certain route learned via a specific protocol.\n    If the protocol is not specified, will return all possible routes.\n\n    .. note::\n\n        This function return the routes from the RIB.\n        In case the destination prefix is too short,\n        there may be too many routes matched.\n        Therefore in cases of devices having a very high number of routes\n        it may be necessary to adjust the prefix length and request\n        using a longer prefix.\n\n    destination\n        destination prefix.\n\n    protocol (optional)\n        protocol used to learn the routes to the destination.\n\n    .. versionchanged:: 2017.7.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt 'my_router' route.show 172.16.0.0/25\n        salt 'my_router' route.show 172.16.0.0/25 bgp\n\n    Output example:\n\n    .. code-block:: python\n\n        {\n            '172.16.0.0/25': [\n                {\n                    'protocol': 'BGP',\n                    'last_active': True,\n                    'current_active': True,\n                    'age': 1178693,\n                    'routing_table': 'inet.0',\n                    'next_hop': '192.168.0.11',\n                    'outgoing_interface': 'xe-1/1/1.100',\n                    'preference': 170,\n                    'selected_next_hop': False,\n                    'protocol_attributes': {\n                        'remote_as': 65001,\n                        'metric': 5,\n                        'local_as': 13335,\n                        'as_path': '',\n                        'remote_address': '192.168.0.11',\n                        'metric2': 0,\n                        'local_preference': 0,\n                        'communities': [\n                            '0:2',\n                            'no-export'\n                        ],\n                        'preference2': -1\n                    },\n                    'inactive_reason': ''\n                },\n                {\n                    'protocol': 'BGP',\n                    'last_active': False,\n                    'current_active': False,\n                    'age': 2359429,\n                    'routing_table': 'inet.0',\n                    'next_hop': '192.168.0.17',\n                    'outgoing_interface': 'xe-1/1/1.100',\n                    'preference': 170,\n                    'selected_next_hop': True,\n                    'protocol_attributes': {\n                        'remote_as': 65001,\n                        'metric': 5,\n                        'local_as': 13335,\n                        'as_path': '',\n                        'remote_address': '192.168.0.17',\n                        'metric2': 0,\n                        'local_preference': 0,\n                        'communities': [\n                            '0:3',\n                            'no-export'\n                        ],\n                        'preference2': -1\n                    },\n                    'inactive_reason': 'Not Best in its group - Router ID'\n                }\n            ]\n        }\n    '''\n\n    return salt.utils.napalm.call(\n        napalm_device,  # pylint: disable=undefined-variable\n        'get_route_to',\n        **{\n            'destination': destination,\n            'protocol': protocol\n        }\n    )", "response": "Show the details of a certain route learned via a specific protocol."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nensuring that a network is present.", "response": "def present(name,\n            skip_translate=None,\n            ignore_collisions=False,\n            validate_ip_addrs=True,\n            containers=None,\n            reconnect=True,\n            **kwargs):\n    '''\n    .. versionchanged:: 2018.3.0\n        Support added for network configuration options other than ``driver``\n        and ``driver_opts``, as well as IPAM configuration.\n\n    Ensure that a network is present\n\n    .. note::\n        This state supports all arguments for network and IPAM pool\n        configuration which are available for the release of docker-py\n        installed on the minion. For that reason, the arguments described below\n        in the :ref:`NETWORK CONFIGURATION\n        <salt-states-docker-network-present-netconf>` and :ref:`IP ADDRESS\n        MANAGEMENT (IPAM) <salt-states-docker-network-present-ipam>` sections\n        may not accurately reflect what is available on the minion. The\n        :py:func:`docker.get_client_args\n        <salt.modules.dockermod.get_client_args>` function can be used to check\n        the available arguments for the installed version of docker-py (they\n        are found in the ``network_config`` and ``ipam_config`` sections of the\n        return data), but Salt will not prevent a user from attempting to use\n        an argument which is unsupported in the release of Docker which is\n        installed. In those cases, network creation be attempted but will fail.\n\n    name\n        Network name\n\n    skip_translate\n        This function translates Salt SLS input into the format which\n        docker-py expects. However, in the event that Salt's translation logic\n        fails (due to potential changes in the Docker Remote API, or to bugs in\n        the translation code), this argument can be used to exert granular\n        control over which arguments are translated and which are not.\n\n        Pass this argument as a comma-separated list (or Python list) of\n        arguments, and translation for each passed argument name will be\n        skipped. Alternatively, pass ``True`` and *all* translation will be\n        skipped.\n\n        Skipping tranlsation allows for arguments to be formatted directly in\n        the format which docker-py expects. This allows for API changes and\n        other issues to be more easily worked around. See the following links\n        for more information:\n\n        - `docker-py Low-level API`_\n        - `Docker Engine API`_\n\n        .. versionadded:: 2018.3.0\n\n    .. _`docker-py Low-level API`: http://docker-py.readthedocs.io/en/stable/api.html#docker.api.container.ContainerApiMixin.create_container\n    .. _`Docker Engine API`: https://docs.docker.com/engine/api/v1.33/#operation/ContainerCreate\n\n    ignore_collisions : False\n        Since many of docker-py's arguments differ in name from their CLI\n        counterparts (with which most Docker users are more familiar), Salt\n        detects usage of these and aliases them to the docker-py version of\n        that argument. However, if both the alias and the docker-py version of\n        the same argument (e.g. ``options`` and ``driver_opts``) are used, an error\n        will be raised. Set this argument to ``True`` to suppress these errors\n        and keep the docker-py version of the argument.\n\n        .. versionadded:: 2018.3.0\n\n    validate_ip_addrs : True\n        For parameters which accept IP addresses/subnets as input, validation\n        will be performed. To disable, set this to ``False``.\n\n        .. versionadded:: 2018.3.0\n\n    containers\n        A list of containers which should be connected to this network.\n\n        .. note::\n            As of the 2018.3.0 release, this is not the recommended way of\n            managing a container's membership in a network, for a couple\n            reasons:\n\n            1. It does not support setting static IPs, aliases, or links in the\n               container's IP configuration.\n            2. If a :py:func:`docker_container.running\n               <salt.states.docker_container.running>` state replaces a\n               container, it will not be reconnected to the network until the\n               ``docker_network.present`` state is run again. Since containers\n               often have ``require`` requisites to ensure that the network\n               is present, this means that the ``docker_network.present`` state\n               ends up being run *before* the :py:func:`docker_container.running\n               <salt.states.docker_container.running>`, leaving the container\n               unattached at the end of the Salt run.\n\n            For these reasons, it is recommended to use\n            :ref:`docker_container.running's network management support\n            <salt-states-docker-container-network-management>`.\n\n    reconnect : True\n        If ``containers`` is not used, and the network is replaced, then Salt\n        will keep track of the containers which were connected to the network\n        and reconnect them to the network after it is replaced. Salt will first\n        attempt to reconnect using the same IP the container had before the\n        network was replaced. If that fails (for instance, if the network was\n        replaced because the subnet was modified), then the container will be\n        reconnected without an explicit IP address, and its IP will be assigned\n        by Docker.\n\n        Set this option to ``False`` to keep Salt from trying to reconnect\n        containers. This can be useful in some cases when :ref:`managing static\n        IPs in docker_container.running\n        <salt-states-docker-container-network-management>`. For instance, if a\n        network's subnet is modified, it is likely that the static IP will need\n        to be updated in the ``docker_container.running`` state as well. When\n        the network is replaced, the initial reconnect attempt would fail, and\n        the container would be reconnected with an automatically-assigned IP\n        address. Then, when the ``docker_container.running`` state executes, it\n        would disconnect the network *again* and reconnect using the new static\n        IP. Disabling the reconnect behavior in these cases would prevent the\n        unnecessary extra reconnection.\n\n        .. versionadded:: 2018.3.0\n\n    .. _salt-states-docker-network-present-netconf:\n\n    **NETWORK CONFIGURATION ARGUMENTS**\n\n    driver\n        Network driver\n\n        .. code-block:: yaml\n\n            mynet:\n              docker_network.present:\n                - driver: macvlan\n\n    driver_opts (or *driver_opt*, or *options*)\n        Options for the network driver. Either a dictionary of option names and\n        values or a Python list of strings in the format ``varname=value``. The\n        below three examples are equivalent:\n\n        .. code-block:: yaml\n\n            mynet:\n              docker_network.present:\n                - driver: macvlan\n                - driver_opts: macvlan_mode=bridge,parent=eth0\n\n        .. code-block:: yaml\n\n            mynet:\n              docker_network.present:\n                - driver: macvlan\n                - driver_opts:\n                  - macvlan_mode=bridge\n                  - parent=eth0\n\n        .. code-block:: yaml\n\n            mynet:\n              docker_network.present:\n                - driver: macvlan\n                - driver_opts:\n                  - macvlan_mode: bridge\n                  - parent: eth0\n\n        The options can also simply be passed as a dictionary, though this can\n        be error-prone due to some :ref:`idiosyncrasies <yaml-idiosyncrasies>`\n        with how PyYAML loads nested data structures:\n\n        .. code-block:: yaml\n\n            mynet:\n              docker_network.present:\n                - driver: macvlan\n                - driver_opts:\n                    macvlan_mode: bridge\n                    parent: eth0\n\n    check_duplicate : True\n        If ``True``, checks for networks with duplicate names. Since networks\n        are primarily keyed based on a random ID and not on the name, and\n        network name is strictly a user-friendly alias to the network which is\n        uniquely identified using ID, there is no guaranteed way to check for\n        duplicates. This option providess a best effort, checking for any\n        networks which have the same name, but it is not guaranteed to catch\n        all name collisions.\n\n        .. code-block:: yaml\n\n            mynet:\n              docker_network.present:\n                - check_duplicate: False\n\n    internal : False\n        If ``True``, restricts external access to the network\n\n        .. code-block:: yaml\n\n            mynet:\n              docker_network.present:\n                - internal: True\n\n    labels\n        Add metadata to the network. Labels can be set both with and without\n        values, and labels with values can be passed either as ``key=value`` or\n        ``key: value`` pairs. For example, while the below would be very\n        confusing to read, it is technically valid, and demonstrates the\n        different ways in which labels can be passed:\n\n        .. code-block:: yaml\n\n            mynet:\n              docker_network.present:\n                - labels:\n                  - foo\n                  - bar=baz\n                  - hello: world\n\n        The labels can also simply be passed as a YAML dictionary, though this\n        can be error-prone due to some :ref:`idiosyncrasies\n        <yaml-idiosyncrasies>` with how PyYAML loads nested data structures:\n\n        .. code-block:: yaml\n\n            foo:\n              docker_network.present:\n                - labels:\n                    foo: ''\n                    bar: baz\n                    hello: world\n\n        .. versionchanged:: 2018.3.0\n            Methods for specifying labels can now be mixed. Earlier releases\n            required either labels with or without values.\n\n    enable_ipv6 (or *ipv6*) : False\n        Enable IPv6 on the network\n\n        .. code-block:: yaml\n\n            mynet:\n              docker_network.present:\n                - enable_ipv6: True\n\n        .. note::\n            While it should go without saying, this argument must be set to\n            ``True`` to :ref:`configure an IPv6 subnet\n            <salt-states-docker-network-present-ipam>`. Also, if this option is\n            turned on without an IPv6 subnet explicitly configured, you will\n            get an error unless you have set up a fixed IPv6 subnet. Consult\n            the `Docker IPv6 docs`_ for information on how to do this.\n\n            .. _`Docker IPv6 docs`: https://docs.docker.com/v17.09/engine/userguide/networking/default_network/ipv6/\n\n    attachable : False\n        If ``True``, and the network is in the global scope, non-service\n        containers on worker nodes will be able to connect to the network.\n\n        .. code-block:: yaml\n\n            mynet:\n              docker_network.present:\n                - attachable: True\n\n        .. note::\n            This option cannot be reliably managed on CentOS 7. This is because\n            while support for this option was added in API version 1.24, its\n            value was not added to the inpsect results until API version 1.26.\n            The version of Docker which is available for CentOS 7 runs API\n            version 1.24, meaning that while Salt can pass this argument to the\n            API, it has no way of knowing the value of this config option in an\n            existing Docker network.\n\n    scope\n        Specify the network's scope (``local``, ``global`` or ``swarm``)\n\n        .. code-block:: yaml\n\n            mynet:\n              docker_network.present:\n                - scope: local\n\n    ingress : False\n        If ``True``, create an ingress network which provides the routing-mesh in\n        swarm mode\n\n        .. code-block:: yaml\n\n            mynet:\n              docker_network.present:\n                - ingress: True\n\n    .. _salt-states-docker-network-present-ipam:\n\n    **IP ADDRESS MANAGEMENT (IPAM)**\n\n    This state supports networks with either IPv4, or both IPv4 and IPv6. If\n    configuring IPv4, then you can pass the :ref:`IPAM pool arguments\n    <salt-states-docker-network-present-ipam-pool-arguments>` below as\n    individual arguments. However, if configuring IPv4 and IPv6, the arguments\n    must be passed as a list of dictionaries, in the ``ipam_pools`` argument\n    (click :ref:`here <salt-states-docker-network-present-ipam-examples>` for\n    some examples). `These docs`_ also have more information on these\n    arguments.\n\n    .. _`These docs`: http://docker-py.readthedocs.io/en/stable/api.html#docker.types.IPAMPool\n\n    *IPAM ARGUMENTS*\n\n    ipam_driver\n        IPAM driver to use, if different from the default one\n\n        .. code-block:: yaml\n\n            mynet:\n              docker_network.present:\n                - ipam_driver: foo\n\n    ipam_opts\n        Options for the IPAM driver. Either a dictionary of option names and\n        values or a Python list of strings in the format ``varname=value``. The\n        below three examples are equivalent:\n\n        .. code-block:: yaml\n\n            mynet:\n              docker_network.present:\n                - ipam_driver: foo\n                - ipam_opts: foo=bar,baz=qux\n\n        .. code-block:: yaml\n\n            mynet:\n              docker_network.present:\n                - ipam_driver: foo\n                - ipam_opts:\n                  - foo=bar\n                  - baz=qux\n\n        .. code-block:: yaml\n\n            mynet:\n              docker_network.present:\n                - ipam_driver: foo\n                - ipam_opts:\n                  - foo: bar\n                  - baz: qux\n\n        The options can also simply be passed as a dictionary, though this can\n        be error-prone due to some :ref:`idiosyncrasies <yaml-idiosyncrasies>`\n        with how PyYAML loads nested data structures:\n\n        .. code-block:: yaml\n\n            mynet:\n              docker_network.present:\n                - ipam_driver: macvlan\n                - ipam_opts:\n                    foo: bar\n                    baz: qux\n\n    .. _salt-states-docker-network-present-ipam-pool-arguments:\n\n    *IPAM POOL ARGUMENTS*\n\n    subnet\n        Subnet in CIDR format that represents a network segment\n\n    iprange (or *ip_range*)\n        Allocate container IP from a sub-range within the subnet\n\n        Subnet in CIDR format that represents a network segment\n\n    gateway\n        IPv4 or IPv6 gateway for the master subnet\n\n    aux_addresses (or *aux_address*)\n        A dictionary of mapping container names to IP addresses which should be\n        allocated for them should they connect to the network. Either a\n        dictionary of option names and values or a Python list of strings in\n        the format ``host=ipaddr``.\n\n    .. _salt-states-docker-network-present-ipam-examples:\n\n    *IPAM CONFIGURATION EXAMPLES*\n\n    Below is an example of an IPv4-only network (keep in mind that ``subnet``\n    is the only required argument).\n\n    .. code-block:: yaml\n\n        mynet:\n          docker_network.present:\n            - subnet: 10.0.20.0/24\n            - iprange: 10.0.20.128/25\n            - gateway: 10.0.20.254\n            - aux_addresses:\n              - foo.bar.tld: 10.0.20.50\n              - hello.world.tld: 10.0.20.51\n\n    .. note::\n        The ``aux_addresses`` can be passed differently, in the same way that\n        ``driver_opts`` and ``ipam_opts`` can.\n\n    This same network could also be configured this way:\n\n    .. code-block:: yaml\n\n        mynet:\n          docker_network.present:\n            - ipam_pools:\n              - subnet: 10.0.20.0/24\n                iprange: 10.0.20.128/25\n                gateway: 10.0.20.254\n                aux_addresses:\n                  foo.bar.tld: 10.0.20.50\n                  hello.world.tld: 10.0.20.51\n\n    Here is an example of a mixed IPv4/IPv6 subnet.\n\n    .. code-block:: yaml\n\n        mynet:\n          docker_network.present:\n            - ipam_pools:\n              - subnet: 10.0.20.0/24\n                gateway: 10.0.20.1\n              - subnet: fe3f:2180:26:1::/123\n                gateway: fe3f:2180:26:1::1\n    '''\n    ret = {'name': name,\n           'changes': {},\n           'result': False,\n           'comment': ''}\n\n    try:\n        network = __salt__['docker.inspect_network'](name)\n    except CommandExecutionError as exc:\n        msg = exc.__str__()\n        if '404' in msg:\n            # Network not present\n            network = None\n        else:\n            ret['comment'] = msg\n            return ret\n\n    # map container's IDs to names\n    to_connect = {}\n    missing_containers = []\n    stopped_containers = []\n    for cname in __utils__['args.split_input'](containers or []):\n        try:\n            cinfo = __salt__['docker.inspect_container'](cname)\n        except CommandExecutionError:\n            missing_containers.append(cname)\n        else:\n            try:\n                cid = cinfo['Id']\n            except KeyError:\n                missing_containers.append(cname)\n            else:\n                if not cinfo.get('State', {}).get('Running', False):\n                    stopped_containers.append(cname)\n                else:\n                    to_connect[cid] = {'Name': cname}\n\n    if missing_containers:\n        ret.setdefault('warnings', []).append(\n            'The following containers do not exist: {0}.'.format(\n                ', '.join(missing_containers)\n            )\n        )\n\n    if stopped_containers:\n        ret.setdefault('warnings', []).append(\n            'The following containers are not running: {0}.'.format(\n                ', '.join(stopped_containers)\n            )\n        )\n\n    # We might disconnect containers in the process of recreating the network,\n    # we'll need to keep track these containers so we can reconnect them later.\n    disconnected_containers = {}\n\n    try:\n        kwargs = __utils__['docker.translate_input'](\n            salt.utils.docker.translate.network,\n            skip_translate=skip_translate,\n            ignore_collisions=ignore_collisions,\n            validate_ip_addrs=validate_ip_addrs,\n            **__utils__['args.clean_kwargs'](**kwargs))\n    except Exception as exc:\n        ret['comment'] = exc.__str__()\n        return ret\n\n    # Separate out the IPAM config options and build the IPAM config dict\n    ipam_kwargs = {}\n    ipam_kwarg_names = ['ipam', 'ipam_driver', 'ipam_opts', 'ipam_pools']\n    ipam_kwarg_names.extend(\n        __salt__['docker.get_client_args']('ipam_config')['ipam_config'])\n    for key in ipam_kwarg_names:\n        try:\n            ipam_kwargs[key] = kwargs.pop(key)\n        except KeyError:\n            pass\n    if 'ipam' in ipam_kwargs:\n        if len(ipam_kwargs) > 1:\n            ret['comment'] = (\n                'Cannot mix the \\'ipam\\' argument with any of the IPAM config '\n                'arguments. See documentation for details.'\n            )\n            return ret\n        ipam_config = ipam_kwargs['ipam']\n    else:\n        ipam_pools = ipam_kwargs.pop('ipam_pools', ())\n        try:\n            ipam_config = __utils__['docker.create_ipam_config'](\n                *ipam_pools,\n                **ipam_kwargs)\n        except Exception as exc:\n            ret['comment'] = exc.__str__()\n            return ret\n\n    # We'll turn this off if we decide below that creating the network is not\n    # necessary.\n    create_network = True\n\n    if network is not None:\n        log.debug('Docker network \\'%s\\' already exists', name)\n\n        # Set the comment now to say that it already exists, if we need to\n        # recreate the network with new config we'll update the comment later.\n        ret['comment'] = (\n            'Network \\'{0}\\' already exists, and is configured '\n            'as specified'.format(name)\n        )\n        log.trace('Details of docker network \\'%s\\': %s', name, network)\n\n        temp_net_name = ''.join(\n            random.choice(string.ascii_lowercase) for _ in range(20))\n\n        try:\n            # When using enable_ipv6, you *must* provide a subnet. But we don't\n            # care about the subnet when we make our temp network, we only care\n            # about the non-IPAM values in the network. And we also do not want\n            # to try some hacky workaround where we choose a small IPv6 subnet\n            # to pass when creating the temp network, that may end up\n            # overlapping with a large IPv6 subnet already in use by Docker.\n            # So, for purposes of comparison we will create the temp network\n            # with enable_ipv6=False and then munge the inspect results before\n            # performing the comparison. Note that technically it is not\n            # required that one specify both v4 and v6 subnets when creating a\n            # network, but not specifying IPv4 makes it impossible for us to\n            # reliably compare the SLS input to the existing network, as we\n            # wouldng't know if the IPv4 subnet in the existing network was\n            # explicitly configured or was automatically assigned by Docker.\n            enable_ipv6 = kwargs.pop('enable_ipv6', None)\n            __salt__['docker.create_network'](\n                temp_net_name,\n                skip_translate=True,  # No need to translate (already did)\n                enable_ipv6=False,\n                **kwargs)\n        except CommandExecutionError as exc:\n            ret['comment'] = (\n                'Failed to create temp network for comparison: {0}'.format(\n                    exc.__str__()\n                )\n            )\n            return ret\n        else:\n            # Replace the value so we can use it later\n            if enable_ipv6 is not None:\n                kwargs['enable_ipv6'] = enable_ipv6\n\n        try:\n            try:\n                temp_net_info = __salt__['docker.inspect_network'](temp_net_name)\n            except CommandExecutionError as exc:\n                ret['comment'] = 'Failed to inspect temp network: {0}'.format(\n                    exc.__str__()\n                )\n                return ret\n            else:\n                temp_net_info['EnableIPv6'] = bool(enable_ipv6)\n\n            # Replace the IPAM configuration in the temp network with the IPAM\n            # config dict we created earlier, for comparison purposes. This is\n            # necessary because we cannot create two networks that have\n            # overlapping subnets (the Docker Engine will throw an error).\n            temp_net_info['IPAM'] = ipam_config\n\n            existing_pool_count = len(network['IPAM']['Config'])\n            desired_pool_count = len(temp_net_info['IPAM']['Config'])\n\n            is_default_pool = lambda x: True \\\n                if sorted(x) == ['Gateway', 'Subnet'] \\\n                else False\n\n            if desired_pool_count == 0 \\\n                    and existing_pool_count == 1 \\\n                    and is_default_pool(network['IPAM']['Config'][0]):\n                # If we're not explicitly configuring an IPAM pool, then we\n                # don't care what the subnet is. Docker networks created with\n                # no explicit IPAM configuration are assigned a single IPAM\n                # pool containing just a subnet and gateway. If the above if\n                # statement resolves as True, then we know that both A) we\n                # aren't explicitly configuring IPAM, and B) the existing\n                # network appears to be one that was created without an\n                # explicit IPAM configuration (since it has the default pool\n                # config values). Of course, it could be possible that the\n                # existing network was created with a single custom IPAM pool,\n                # with just a subnet and gateway. But even if this was the\n                # case, the fact that we aren't explicitly enforcing IPAM\n                # configuration means we don't really care what the existing\n                # IPAM configuration is. At any rate, to avoid IPAM differences\n                # when comparing the existing network to the temp network, we\n                # need to clear the existing network's IPAM configuration.\n                network['IPAM']['Config'] = []\n\n            changes = __salt__['docker.compare_networks'](\n                network,\n                temp_net_info,\n                ignore='Name,Id,Created,Containers')\n\n            if not changes:\n                # No changes to the network, so we'll be keeping the existing\n                # network and at most just connecting containers to it.\n                create_network = False\n\n            else:\n                ret['changes'][name] = changes\n                if __opts__['test']:\n                    ret['result'] = None\n                    ret['comment'] = 'Network would be recreated with new config'\n                    return ret\n\n                if network['Containers']:\n                    # We've removed the network, so there are now no containers\n                    # attached to it. However, once we recreate the network\n                    # with the new configuration we may need to reconnect the\n                    # containers that were previously connected. Even if we're\n                    # not reconnecting, we still need to track the containers\n                    # so that we can report on which were disconnected.\n                    disconnected_containers = copy.deepcopy(network['Containers'])\n                    if not containers and reconnect:\n                        # Grab the links and aliases from each connected\n                        # container so that we have them when we attempt to\n                        # reconnect later\n                        for cid in disconnected_containers:\n                            try:\n                                cinfo = __salt__['docker.inspect_container'](cid)\n                                netinfo = cinfo['NetworkSettings']['Networks'][name]\n                                # Links and Aliases will be None if not\n                                # explicitly set, hence using \"or\" instead of\n                                # placing the empty list inside the dict.get\n                                net_links = netinfo.get('Links') or []\n                                net_aliases = netinfo.get('Aliases') or []\n                                if net_links:\n                                    disconnected_containers[cid]['Links'] = net_links\n                                if net_aliases:\n                                    disconnected_containers[cid]['Aliases'] = net_aliases\n                            except (CommandExecutionError, KeyError, ValueError):\n                                continue\n\n                remove_result = _remove_network(network)\n                if not remove_result['result']:\n                    return remove_result\n\n                # Replace the Containers key with an empty dict so that when we\n                # check for connnected containers below, we correctly see that\n                # there are none connected.\n                network['Containers'] = {}\n        finally:\n            try:\n                __salt__['docker.remove_network'](temp_net_name)\n            except CommandExecutionError as exc:\n                ret.setdefault('warnings', []).append(\n                    'Failed to remove temp network \\'{0}\\': {1}.'.format(\n                        temp_net_name, exc.__str__()\n                    )\n                )\n\n    if create_network:\n        log.debug('Network \\'%s\\' will be created', name)\n        if __opts__['test']:\n            # NOTE: if the container already existed and needed to be\n            # recreated, and we were in test mode, we would have already exited\n            # above with a comment about the network needing to be recreated.\n            # So, even though the below block to create the network would be\n            # executed to create the network both when it's being recreated and\n            # when it's being created for the first time, the below comment is\n            # still accurate.\n            ret['result'] = None\n            ret['comment'] = 'Network will be created'\n            return ret\n\n        kwargs['ipam'] = ipam_config\n        try:\n            __salt__['docker.create_network'](\n                name,\n                skip_translate=True,  # No need to translate (already did)\n                **kwargs)\n        except Exception as exc:\n            ret['comment'] = 'Failed to create network \\'{0}\\': {1}'.format(\n                name, exc.__str__())\n            return ret\n        else:\n            action = 'recreated' if network is not None else 'created'\n            ret['changes'][action] = True\n            ret['comment'] = 'Network \\'{0}\\' {1}'.format(\n                name,\n                'created' if network is None\n                    else 'was replaced with updated config'\n            )\n            # Make sure the \"Containers\" key exists for logic below\n            network = {'Containers': {}}\n\n    # If no containers were specified in the state but we have disconnected\n    # some in the process of recreating the network, we should reconnect those\n    # containers.\n    if containers is None and reconnect and disconnected_containers:\n        to_connect = disconnected_containers\n\n    # Don't try to connect any containers which are already connected. If we\n    # created/re-created the network, then network['Containers'] will be empty\n    # and no containers will be deleted from the to_connect dict (the result\n    # being that we will reconnect all containers in the to_connect dict).\n    # list() is used here because we will potentially be modifying the\n    # dictionary during iteration.\n    for cid in list(to_connect):\n        if cid in network['Containers']:\n            del to_connect[cid]\n\n    errors = []\n    if to_connect:\n        for cid, connect_info in six.iteritems(to_connect):\n            connect_kwargs = {}\n            if cid in disconnected_containers:\n                for key_name, arg_name in (('IPv4Address', 'ipv4_address'),\n                                           ('IPV6Address', 'ipv6_address'),\n                                           ('Links', 'links'),\n                                           ('Aliases', 'aliases')):\n                    try:\n                        connect_kwargs[arg_name] = connect_info[key_name]\n                    except (KeyError, AttributeError):\n                        continue\n                    else:\n                        if key_name.endswith('Address'):\n                            connect_kwargs[arg_name] = \\\n                                connect_kwargs[arg_name].rsplit('/', 1)[0]\n            try:\n                __salt__['docker.connect_container_to_network'](\n                    cid, name, **connect_kwargs)\n            except CommandExecutionError as exc:\n                if not connect_kwargs:\n                    errors.append(exc.__str__())\n                else:\n                    # We failed to reconnect with the container's old IP\n                    # configuration. Reconnect using automatic IP config.\n                    try:\n                        __salt__['docker.connect_container_to_network'](\n                            cid, name)\n                    except CommandExecutionError as exc:\n                        errors.append(exc.__str__())\n                    else:\n                        ret['changes'].setdefault(\n                            'reconnected' if cid in disconnected_containers\n                                else 'connected',\n                            []\n                        ).append(connect_info['Name'])\n            else:\n                ret['changes'].setdefault(\n                    'reconnected' if cid in disconnected_containers\n                        else 'connected',\n                    []\n                ).append(connect_info['Name'])\n\n    if errors:\n        if ret['comment']:\n            ret['comment'] += '. '\n        ret['comment'] += '. '.join(errors) + '.'\n    else:\n        ret['result'] = True\n\n    # Figure out if we removed any containers as a result of replacing the\n    # network and did not reconnect them. We only would not have reconnected if\n    # a list of containers was passed in the \"containers\" argument, and there\n    # were containers connected to the network prior to its replacement which\n    # were not part of that list.\n    for cid, c_info in six.iteritems(disconnected_containers):\n        if cid not in to_connect:\n            ret['changes'].setdefault('disconnected', []).append(c_info['Name'])\n\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nensuring that a network is absent.", "response": "def absent(name):\n    '''\n    Ensure that a network is absent.\n\n    name\n        Name of the network\n\n    Usage Example:\n\n    .. code-block:: yaml\n\n        network_foo:\n          docker_network.absent\n    '''\n    ret = {'name': name,\n           'changes': {},\n           'result': False,\n           'comment': ''}\n\n    try:\n        network = __salt__['docker.inspect_network'](name)\n    except CommandExecutionError as exc:\n        msg = exc.__str__()\n        if '404' in msg:\n            # Network not present\n            network = None\n        else:\n            ret['comment'] = msg\n            return ret\n\n    if network is None:\n        ret['result'] = True\n        ret['comment'] = 'Network \\'{0}\\' already absent'.format(name)\n        return ret\n\n    if __opts__['test']:\n        ret['result'] = None\n        ret['comment'] = 'Network \\'{0}\\' will be removed'.format(name)\n        return ret\n\n    return _remove_network(network)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nremoving a network including all connected containers", "response": "def _remove_network(network):\n    '''\n    Remove network, including all connected containers\n    '''\n    ret = {'name': network['Name'],\n           'changes': {},\n           'result': False,\n           'comment': ''}\n\n    errors = []\n    for cid in network['Containers']:\n        try:\n            cinfo = __salt__['docker.inspect_container'](cid)\n        except CommandExecutionError:\n            # Fall back to container ID\n            cname = cid\n        else:\n            cname = cinfo.get('Name', '').lstrip('/')\n\n        try:\n            __salt__['docker.disconnect_container_from_network'](cid, network['Name'])\n        except CommandExecutionError as exc:\n            errors = \\\n                'Failed to disconnect container \\'{0}\\' : {1}'.format(\n                    cname, exc\n                )\n        else:\n            ret['changes'].setdefault('disconnected', []).append(cname)\n\n    if errors:\n        ret['comment'] = '\\n'.join(errors)\n        return ret\n\n    try:\n        __salt__['docker.remove_network'](network['Name'])\n    except CommandExecutionError as exc:\n        ret['comment'] = 'Failed to remove network: {0}'.format(exc)\n    else:\n        ret['changes']['removed'] = True\n        ret['result'] = True\n        ret['comment'] = 'Removed network \\'{0}\\''.format(network['Name'])\n\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _encrypt_private(self, ret, dictkey, target):\n        '''\n        The server equivalent of ReqChannel.crypted_transfer_decode_dictentry\n        '''\n        # encrypt with a specific AES key\n        pubfn = os.path.join(self.opts['pki_dir'],\n                             'minions',\n                             target)\n        key = salt.crypt.Crypticle.generate_key_string()\n        pcrypt = salt.crypt.Crypticle(\n            self.opts,\n            key)\n        try:\n            pub = salt.crypt.get_rsa_pub_key(pubfn)\n        except (ValueError, IndexError, TypeError):\n            return self.crypticle.dumps({})\n        except IOError:\n            log.error('AES key not found')\n            return {'error': 'AES key not found'}\n\n        pret = {}\n        if not six.PY2:\n            key = salt.utils.stringutils.to_bytes(key)\n        if HAS_M2:\n            pret['key'] = pub.public_encrypt(key, RSA.pkcs1_oaep_padding)\n        else:\n            cipher = PKCS1_OAEP.new(pub)\n            pret['key'] = cipher.encrypt(key)\n        pret[dictkey] = pcrypt.dumps(\n            ret if ret is not False else {}\n        )\n        return pret", "response": "encrypt the dictentry with the private key"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _update_aes(self):\n        '''\n        Check to see if a fresh AES key is available and update the components\n        of the worker\n        '''\n        if salt.master.SMaster.secrets['aes']['secret'].value != self.crypticle.key_string:\n            self.crypticle = salt.crypt.Crypticle(self.opts, salt.master.SMaster.secrets['aes']['secret'].value)\n            return True\n        return False", "response": "Check to see if a fresh AES key is available and update the components\n        of the worker"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nauthenticate the client and return the key we are receiving", "response": "def _auth(self, load):\n        '''\n        Authenticate the client, use the sent public key to encrypt the AES key\n        which was generated at start up.\n\n        This method fires an event over the master event manager. The event is\n        tagged \"auth\" and returns a dict with information about the auth\n        event\n\n        # Verify that the key we are receiving matches the stored key\n        # Store the key if it is not there\n        # Make an RSA key with the pub key\n        # Encrypt the AES key as an encrypted salt.payload\n        # Package the return and return it\n        '''\n\n        if not salt.utils.verify.valid_id(self.opts, load['id']):\n            log.info('Authentication request from invalid id %s', load['id'])\n            return {'enc': 'clear',\n                    'load': {'ret': False}}\n        log.info('Authentication request from %s', load['id'])\n\n        # 0 is default which should be 'unlimited'\n        if self.opts['max_minions'] > 0:\n            # use the ConCache if enabled, else use the minion utils\n            if self.cache_cli:\n                minions = self.cache_cli.get_cached()\n            else:\n                minions = self.ckminions.connected_ids()\n                if len(minions) > 1000:\n                    log.info('With large numbers of minions it is advised '\n                             'to enable the ConCache with \\'con_cache: True\\' '\n                             'in the masters configuration file.')\n\n            if not len(minions) <= self.opts['max_minions']:\n                # we reject new minions, minions that are already\n                # connected must be allowed for the mine, highstate, etc.\n                if load['id'] not in minions:\n                    msg = ('Too many minions connected (max_minions={0}). '\n                           'Rejecting connection from id '\n                           '{1}'.format(self.opts['max_minions'],\n                                        load['id']))\n                    log.info(msg)\n                    eload = {'result': False,\n                             'act': 'full',\n                             'id': load['id'],\n                             'pub': load['pub']}\n\n                    if self.opts.get('auth_events') is True:\n                        self.event.fire_event(eload, salt.utils.event.tagify(prefix='auth'))\n                    return {'enc': 'clear',\n                            'load': {'ret': 'full'}}\n\n        # Check if key is configured to be auto-rejected/signed\n        auto_reject = self.auto_key.check_autoreject(load['id'])\n        auto_sign = self.auto_key.check_autosign(load['id'], load.get(u'autosign_grains', None))\n\n        pubfn = os.path.join(self.opts['pki_dir'],\n                             'minions',\n                             load['id'])\n        pubfn_pend = os.path.join(self.opts['pki_dir'],\n                                  'minions_pre',\n                                  load['id'])\n        pubfn_rejected = os.path.join(self.opts['pki_dir'],\n                                      'minions_rejected',\n                                      load['id'])\n        pubfn_denied = os.path.join(self.opts['pki_dir'],\n                                    'minions_denied',\n                                    load['id'])\n        if self.opts['open_mode']:\n            # open mode is turned on, nuts to checks and overwrite whatever\n            # is there\n            pass\n        elif os.path.isfile(pubfn_rejected):\n            # The key has been rejected, don't place it in pending\n            log.info('Public key rejected for %s. Key is present in '\n                     'rejection key dir.', load['id'])\n            eload = {'result': False,\n                     'id': load['id'],\n                     'pub': load['pub']}\n            if self.opts.get('auth_events') is True:\n                self.event.fire_event(eload, salt.utils.event.tagify(prefix='auth'))\n            return {'enc': 'clear',\n                    'load': {'ret': False}}\n\n        elif os.path.isfile(pubfn):\n            # The key has been accepted, check it\n            with salt.utils.files.fopen(pubfn, 'r') as pubfn_handle:\n                if pubfn_handle.read().strip() != load['pub'].strip():\n                    log.error(\n                        'Authentication attempt from %s failed, the public '\n                        'keys did not match. This may be an attempt to compromise '\n                        'the Salt cluster.', load['id']\n                    )\n                    # put denied minion key into minions_denied\n                    with salt.utils.files.fopen(pubfn_denied, 'w+') as fp_:\n                        fp_.write(load['pub'])\n                    eload = {'result': False,\n                             'id': load['id'],\n                             'act': 'denied',\n                             'pub': load['pub']}\n                    if self.opts.get('auth_events') is True:\n                        self.event.fire_event(eload, salt.utils.event.tagify(prefix='auth'))\n                    return {'enc': 'clear',\n                            'load': {'ret': False}}\n\n        elif not os.path.isfile(pubfn_pend):\n            # The key has not been accepted, this is a new minion\n            if os.path.isdir(pubfn_pend):\n                # The key path is a directory, error out\n                log.info('New public key %s is a directory', load['id'])\n                eload = {'result': False,\n                         'id': load['id'],\n                         'pub': load['pub']}\n                if self.opts.get('auth_events') is True:\n                    self.event.fire_event(eload, salt.utils.event.tagify(prefix='auth'))\n                return {'enc': 'clear',\n                        'load': {'ret': False}}\n\n            if auto_reject:\n                key_path = pubfn_rejected\n                log.info('New public key for %s rejected via autoreject_file', load['id'])\n                key_act = 'reject'\n                key_result = False\n            elif not auto_sign:\n                key_path = pubfn_pend\n                log.info('New public key for %s placed in pending', load['id'])\n                key_act = 'pend'\n                key_result = True\n            else:\n                # The key is being automatically accepted, don't do anything\n                # here and let the auto accept logic below handle it.\n                key_path = None\n\n            if key_path is not None:\n                # Write the key to the appropriate location\n                with salt.utils.files.fopen(key_path, 'w+') as fp_:\n                    fp_.write(load['pub'])\n                ret = {'enc': 'clear',\n                       'load': {'ret': key_result}}\n                eload = {'result': key_result,\n                         'act': key_act,\n                         'id': load['id'],\n                         'pub': load['pub']}\n                if self.opts.get('auth_events') is True:\n                    self.event.fire_event(eload, salt.utils.event.tagify(prefix='auth'))\n                return ret\n\n        elif os.path.isfile(pubfn_pend):\n            # This key is in the pending dir and is awaiting acceptance\n            if auto_reject:\n                # We don't care if the keys match, this minion is being\n                # auto-rejected. Move the key file from the pending dir to the\n                # rejected dir.\n                try:\n                    shutil.move(pubfn_pend, pubfn_rejected)\n                except (IOError, OSError):\n                    pass\n                log.info('Pending public key for %s rejected via '\n                         'autoreject_file', load['id'])\n                ret = {'enc': 'clear',\n                       'load': {'ret': False}}\n                eload = {'result': False,\n                         'act': 'reject',\n                         'id': load['id'],\n                         'pub': load['pub']}\n                if self.opts.get('auth_events') is True:\n                    self.event.fire_event(eload, salt.utils.event.tagify(prefix='auth'))\n                return ret\n\n            elif not auto_sign:\n                # This key is in the pending dir and is not being auto-signed.\n                # Check if the keys are the same and error out if this is the\n                # case. Otherwise log the fact that the minion is still\n                # pending.\n                with salt.utils.files.fopen(pubfn_pend, 'r') as pubfn_handle:\n                    if pubfn_handle.read() != load['pub']:\n                        log.error(\n                            'Authentication attempt from %s failed, the public '\n                            'key in pending did not match. This may be an '\n                            'attempt to compromise the Salt cluster.', load['id']\n                        )\n                        # put denied minion key into minions_denied\n                        with salt.utils.files.fopen(pubfn_denied, 'w+') as fp_:\n                            fp_.write(load['pub'])\n                        eload = {'result': False,\n                                 'id': load['id'],\n                                 'act': 'denied',\n                                 'pub': load['pub']}\n                        if self.opts.get('auth_events') is True:\n                            self.event.fire_event(eload, salt.utils.event.tagify(prefix='auth'))\n                        return {'enc': 'clear',\n                                'load': {'ret': False}}\n                    else:\n                        log.info(\n                            'Authentication failed from host %s, the key is in '\n                            'pending and needs to be accepted with salt-key '\n                            '-a %s', load['id'], load['id']\n                        )\n                        eload = {'result': True,\n                                 'act': 'pend',\n                                 'id': load['id'],\n                                 'pub': load['pub']}\n                        if self.opts.get('auth_events') is True:\n                            self.event.fire_event(eload, salt.utils.event.tagify(prefix='auth'))\n                        return {'enc': 'clear',\n                                'load': {'ret': True}}\n            else:\n                # This key is in pending and has been configured to be\n                # auto-signed. Check to see if it is the same key, and if\n                # so, pass on doing anything here, and let it get automatically\n                # accepted below.\n                with salt.utils.files.fopen(pubfn_pend, 'r') as pubfn_handle:\n                    if pubfn_handle.read() != load['pub']:\n                        log.error(\n                            'Authentication attempt from %s failed, the public '\n                            'keys in pending did not match. This may be an '\n                            'attempt to compromise the Salt cluster.', load['id']\n                        )\n                        # put denied minion key into minions_denied\n                        with salt.utils.files.fopen(pubfn_denied, 'w+') as fp_:\n                            fp_.write(load['pub'])\n                        eload = {'result': False,\n                                 'id': load['id'],\n                                 'pub': load['pub']}\n                        if self.opts.get('auth_events') is True:\n                            self.event.fire_event(eload, salt.utils.event.tagify(prefix='auth'))\n                        return {'enc': 'clear',\n                                'load': {'ret': False}}\n                    else:\n                        os.remove(pubfn_pend)\n\n        else:\n            # Something happened that I have not accounted for, FAIL!\n            log.warning('Unaccounted for authentication failure')\n            eload = {'result': False,\n                     'id': load['id'],\n                     'pub': load['pub']}\n            if self.opts.get('auth_events') is True:\n                self.event.fire_event(eload, salt.utils.event.tagify(prefix='auth'))\n            return {'enc': 'clear',\n                    'load': {'ret': False}}\n\n        log.info('Authentication accepted from %s', load['id'])\n        # only write to disk if you are adding the file, and in open mode,\n        # which implies we accept any key from a minion.\n        if not os.path.isfile(pubfn) and not self.opts['open_mode']:\n            with salt.utils.files.fopen(pubfn, 'w+') as fp_:\n                fp_.write(load['pub'])\n        elif self.opts['open_mode']:\n            disk_key = ''\n            if os.path.isfile(pubfn):\n                with salt.utils.files.fopen(pubfn, 'r') as fp_:\n                    disk_key = fp_.read()\n            if load['pub'] and load['pub'] != disk_key:\n                log.debug('Host key change detected in open mode.')\n                with salt.utils.files.fopen(pubfn, 'w+') as fp_:\n                    fp_.write(load['pub'])\n            elif not load['pub']:\n                log.error('Public key is empty: %s', load['id'])\n                return {'enc': 'clear',\n                        'load': {'ret': False}}\n\n        pub = None\n\n        # the con_cache is enabled, send the minion id to the cache\n        if self.cache_cli:\n            self.cache_cli.put_cache([load['id']])\n\n        # The key payload may sometimes be corrupt when using auto-accept\n        # and an empty request comes in\n        try:\n            pub = salt.crypt.get_rsa_pub_key(pubfn)\n        except (ValueError, IndexError, TypeError) as err:\n            log.error('Corrupt public key \"%s\": %s', pubfn, err)\n            return {'enc': 'clear',\n                    'load': {'ret': False}}\n\n        if not HAS_M2:\n            cipher = PKCS1_OAEP.new(pub)\n        ret = {'enc': 'pub',\n               'pub_key': self.master_key.get_pub_str(),\n               'publish_port': self.opts['publish_port']}\n\n        # sign the master's pubkey (if enabled) before it is\n        # sent to the minion that was just authenticated\n        if self.opts['master_sign_pubkey']:\n            # append the pre-computed signature to the auth-reply\n            if self.master_key.pubkey_signature():\n                log.debug('Adding pubkey signature to auth-reply')\n                log.debug(self.master_key.pubkey_signature())\n                ret.update({'pub_sig': self.master_key.pubkey_signature()})\n            else:\n                # the master has its own signing-keypair, compute the master.pub's\n                # signature and append that to the auth-reply\n\n                # get the key_pass for the signing key\n                key_pass = salt.utils.sdb.sdb_get(self.opts['signing_key_pass'], self.opts)\n\n                log.debug(\"Signing master public key before sending\")\n                pub_sign = salt.crypt.sign_message(self.master_key.get_sign_paths()[1],\n                                                   ret['pub_key'], key_pass)\n                ret.update({'pub_sig': binascii.b2a_base64(pub_sign)})\n\n        if not HAS_M2:\n            mcipher = PKCS1_OAEP.new(self.master_key.key)\n        if self.opts['auth_mode'] >= 2:\n            if 'token' in load:\n                try:\n                    if HAS_M2:\n                        mtoken = self.master_key.key.private_decrypt(load['token'],\n                                                                     RSA.pkcs1_oaep_padding)\n                    else:\n                        mtoken = mcipher.decrypt(load['token'])\n                    aes = '{0}_|-{1}'.format(salt.master.SMaster.secrets['aes']['secret'].value, mtoken)\n                except Exception:\n                    # Token failed to decrypt, send back the salty bacon to\n                    # support older minions\n                    pass\n            else:\n                aes = salt.master.SMaster.secrets['aes']['secret'].value\n\n            if HAS_M2:\n                ret['aes'] = pub.public_encrypt(aes, RSA.pkcs1_oaep_padding)\n            else:\n                ret['aes'] = cipher.encrypt(aes)\n        else:\n            if 'token' in load:\n                try:\n                    if HAS_M2:\n                        mtoken = self.master_key.key.private_decrypt(load['token'],\n                                                                     RSA.pkcs1_oaep_padding)\n                        ret['token'] = pub.public_encrypt(mtoken, RSA.pkcs1_oaep_padding)\n                    else:\n                        mtoken = mcipher.decrypt(load['token'])\n                        ret['token'] = cipher.encrypt(mtoken)\n                except Exception:\n                    # Token failed to decrypt, send back the salty bacon to\n                    # support older minions\n                    pass\n\n            aes = salt.master.SMaster.secrets['aes']['secret'].value\n            if HAS_M2:\n                ret['aes'] = pub.public_encrypt(aes,\n                                                RSA.pkcs1_oaep_padding)\n            else:\n                ret['aes'] = cipher.encrypt(aes)\n        # Be aggressive about the signature\n        digest = salt.utils.stringutils.to_bytes(hashlib.sha256(aes).hexdigest())\n        ret['sig'] = salt.crypt.private_encrypt(self.master_key.key, digest)\n        eload = {'result': True,\n                 'act': 'accept',\n                 'id': load['id'],\n                 'pub': load['pub']}\n        if self.opts.get('auth_events') is True:\n            self.event.fire_event(eload, salt.utils.event.tagify(prefix='auth'))\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef which(exe):\n    '''\n    Decorator wrapper for salt.utils.path.which\n    '''\n    def wrapper(function):\n        def wrapped(*args, **kwargs):\n            if salt.utils.path.which(exe) is None:\n                raise CommandNotFoundError(\n                    'The \\'{0}\\' binary was not found in $PATH.'.format(exe)\n                )\n            return function(*args, **kwargs)\n        return identical_signature_wrapper(function, wrapped)\n    return wrapper", "response": "Decorator for salt. utils. path. which"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef which_bin(exes):\n    '''\n    Decorator wrapper for salt.utils.path.which_bin\n    '''\n    def wrapper(function):\n        def wrapped(*args, **kwargs):\n            if salt.utils.path.which_bin(exes) is None:\n                raise CommandNotFoundError(\n                    'None of provided binaries({0}) was not found '\n                    'in $PATH.'.format(\n                        ['\\'{0}\\''.format(exe) for exe in exes]\n                    )\n                )\n            return function(*args, **kwargs)\n        return identical_signature_wrapper(function, wrapped)\n    return wrapper", "response": "Decorator for salt. utils. path. which_bin"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn all inline documentation for runner modules CLI Example : bash salt - run doc. runner", "response": "def runner():\n    '''\n    Return all inline documentation for runner modules\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run doc.runner\n    '''\n    client = salt.runner.RunnerClient(__opts__)\n    ret = client.get_docs()\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef wheel():\n    '''\n    Return all inline documentation for wheel modules\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run doc.wheel\n    '''\n    client = salt.wheel.Wheel(__opts__)\n    ret = client.get_docs()\n    return ret", "response": "Return all inline documentation for wheel modules\n    CLI Example : bash\n        salt - run doc. wheel\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef execution():\n    '''\n    Collect all the sys.doc output from each minion and return the aggregate\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run doc.execution\n    '''\n    client = salt.client.get_local_client(__opts__['conf_file'])\n\n    docs = {}\n    try:\n        for ret in client.cmd_iter('*', 'sys.doc', timeout=__opts__['timeout']):\n            for v in six.itervalues(ret):\n                docs.update(v)\n    except SaltClientError as exc:\n        print(exc)\n        return []\n\n    i = itertools.chain.from_iterable([six.iteritems(docs['ret'])])\n    ret = dict(list(i))\n\n    return ret", "response": "Return the aggregate\n    CLI Example"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef file(suffix='', prefix='tmp', parent=None):\n    '''\n    Create a temporary file\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' temp.file\n        salt '*' temp.file prefix='mytemp-' parent='/var/run/'\n    '''\n    fh_, tmp_ = tempfile.mkstemp(suffix, prefix, parent)\n    os.close(fh_)\n    return tmp_", "response": "Create a temporary file with the specified suffix and prefix prefix."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _reformat_low(self, low):\n        '''\n        Format the low data for RunnerClient()'s master_call() function\n\n        This also normalizes the following low data formats to a single, common\n        low data structure.\n\n        Old-style low: ``{'fun': 'jobs.lookup_jid', 'jid': '1234'}``\n        New-style: ``{'fun': 'jobs.lookup_jid', 'kwarg': {'jid': '1234'}}``\n        CLI-style: ``{'fun': 'jobs.lookup_jid', 'arg': ['jid=\"1234\"']}``\n        '''\n        fun = low.pop('fun')\n        verify_fun(self.functions, fun)\n\n        eauth_creds = dict([(i, low.pop(i)) for i in [\n            'username', 'password', 'eauth', 'token', 'client', 'user', 'key',\n        ] if i in low])\n\n        # Run name=value args through parse_input. We don't need to run kwargs\n        # through because there is no way to send name=value strings in the low\n        # dict other than by including an `arg` array.\n        _arg, _kwarg = salt.utils.args.parse_input(\n                low.pop('arg', []), condition=False)\n        _kwarg.update(low.pop('kwarg', {}))\n\n        # If anything hasn't been pop()'ed out of low by this point it must be\n        # an old-style kwarg.\n        _kwarg.update(low)\n\n        # Finally, mung our kwargs to a format suitable for the byzantine\n        # load_args_and_kwargs so that we can introspect the function being\n        # called and fish for invalid kwargs.\n        munged = []\n        munged.extend(_arg)\n        munged.append(dict(__kwarg__=True, **_kwarg))\n        arg, kwarg = salt.minion.load_args_and_kwargs(\n            self.functions[fun],\n            munged,\n            ignore_invalid=True)\n\n        return dict(fun=fun, kwarg={'kwarg': kwarg, 'arg': arg},\n                **eauth_creds)", "response": "Reformat low data for RunnerClient s master_call function."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nexecuting a runner function asynchronously ; eauth is respected", "response": "def cmd_async(self, low):\n        '''\n        Execute a runner function asynchronously; eauth is respected\n\n        This function requires that :conf_master:`external_auth` is configured\n        and the user is authorized to execute runner functions: (``@runner``).\n\n        .. code-block:: python\n\n            runner.eauth_async({\n                'fun': 'jobs.list_jobs',\n                'username': 'saltdev',\n                'password': 'saltdev',\n                'eauth': 'pam',\n            })\n        '''\n        reformatted_low = self._reformat_low(low)\n\n        return mixins.AsyncClientMixin.cmd_async(self, reformatted_low)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cmd_sync(self, low, timeout=None, full_return=False):\n        '''\n        Execute a runner function synchronously; eauth is respected\n\n        This function requires that :conf_master:`external_auth` is configured\n        and the user is authorized to execute runner functions: (``@runner``).\n\n        .. code-block:: python\n\n            runner.eauth_sync({\n                'fun': 'jobs.list_jobs',\n                'username': 'saltdev',\n                'password': 'saltdev',\n                'eauth': 'pam',\n            })\n        '''\n        reformatted_low = self._reformat_low(low)\n        return mixins.SyncClientMixin.cmd_sync(self, reformatted_low, timeout, full_return)", "response": "Execute a runner function synchronously ; eauth is respected\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef run(self):\n        '''\n        Execute the runner sequence\n        '''\n        # Print documentation only\n        if self.opts.get('doc', False):\n            self.print_docs()\n        else:\n            return self._run_runner()", "response": "Execute the runner sequence\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nactivating this register to turn on a minion status tracking register", "response": "def reg(name):\n    '''\n    Activate this register to turn on a minion status tracking register, this\n    register keeps the current status beacon data and the time that each beacon\n    was last checked in.\n    '''\n    ret = {'name': name,\n           'changes': {},\n           'comment': '',\n           'result': True}\n    now = time.time()\n    if 'status' not in __reg__:\n        __reg__['status'] = {}\n        __reg__['status']['val'] = {}\n    for event in __events__:\n        if fnmatch.fnmatch(event['tag'], 'salt/beacon/*/status/*'):\n            # Got one!\n            idata = {'recv_time': now}\n            for key in event['data']['data']:\n                if key in ('id', 'recv_time'):\n                    continue\n                idata[key] = event['data']['data'][key]\n            __reg__['status']['val'][event['data']['id']] = idata\n            ret['changes'][event['data']['id']] = True\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the top data of the minions", "response": "def top(**kwargs):\n    '''\n    Query |reclass| for the top data (states of the minions).\n    '''\n\n    # If reclass is installed, __virtual__ put it onto the search path, so we\n    # don't need to protect against ImportError:\n    # pylint: disable=3rd-party-module-not-gated\n    from reclass.adapters.salt import top as reclass_top\n    from reclass.errors import ReclassException\n    # pylint: enable=3rd-party-module-not-gated\n\n    try:\n        # Salt's top interface is inconsistent with ext_pillar (see #5786) and\n        # one is expected to extract the arguments to the master_tops plugin\n        # by parsing the configuration file data. I therefore use this adapter\n        # to hide this internality.\n        reclass_opts = __opts__['master_tops']['reclass']\n\n        # the source path we used above isn't something reclass needs to care\n        # about, so filter it:\n        filter_out_source_path_option(reclass_opts)\n\n        # if no inventory_base_uri was specified, initialise it to the first\n        # file_roots of class 'base' (if that exists):\n        set_inventory_base_uri_default(__opts__, kwargs)\n\n        # Salt expects the top data to be filtered by minion_id, so we better\n        # let it know which minion it is dealing with. Unfortunately, we must\n        # extract these data (see #6930):\n        minion_id = kwargs['opts']['id']\n\n        # I purposely do not pass any of __opts__ or __salt__ or __grains__\n        # to reclass, as I consider those to be Salt-internal and reclass\n        # should not make any assumptions about it. Reclass only needs to know\n        # how it's configured, so:\n        return reclass_top(minion_id, **reclass_opts)\n\n    except ImportError as e:\n        if 'reclass' in six.text_type(e):\n            raise SaltInvocationError(\n                'master_tops.reclass: cannot find reclass module '\n                'in {0}'.format(sys.path)\n            )\n        else:\n            raise\n\n    except TypeError as e:\n        if 'unexpected keyword argument' in six.text_type(e):\n            arg = six.text_type(e).split()[-1]\n            raise SaltInvocationError(\n                'master_tops.reclass: unexpected option: {0}'.format(arg)\n            )\n        else:\n            raise\n\n    except KeyError as e:\n        if 'reclass' in six.text_type(e):\n            raise SaltInvocationError('master_tops.reclass: no configuration '\n                                      'found in master config')\n        else:\n            raise\n\n    except ReclassException as e:\n        raise SaltInvocationError('master_tops.reclass: {0}'.format(six.text_type(e)))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_pipeline(name, unique_id, description='', region=None, key=None, keyid=None,\n                    profile=None):\n    '''\n    Create a new, empty pipeline. This function is idempotent.\n\n    CLI example:\n\n    .. code-block:: bash\n\n        salt myminion boto_datapipeline.create_pipeline my_name my_unique_id\n    '''\n    client = _get_client(region, key, keyid, profile)\n    r = {}\n    try:\n        response = client.create_pipeline(\n            name=name,\n            uniqueId=unique_id,\n            description=description,\n        )\n        r['result'] = response['pipelineId']\n    except (botocore.exceptions.BotoCoreError, botocore.exceptions.ClientError) as e:\n        r['error'] = six.text_type(e)\n    return r", "response": "Create a new empty pipeline. This function is idempotent."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef delete_pipeline(pipeline_id, region=None, key=None, keyid=None, profile=None):\n    '''\n    Delete a pipeline, its pipeline definition, and its run history. This function is idempotent.\n\n    CLI example:\n\n    .. code-block:: bash\n\n        salt myminion boto_datapipeline.delete_pipeline my_pipeline_id\n    '''\n    client = _get_client(region, key, keyid, profile)\n    r = {}\n    try:\n        client.delete_pipeline(pipelineId=pipeline_id)\n        r['result'] = True\n    except (botocore.exceptions.BotoCoreError, botocore.exceptions.ClientError) as e:\n        r['error'] = six.text_type(e)\n    return r", "response": "Delete a pipeline, its pipeline definition, and its run history. This function is idempotent.\n\n    CLI example:\n\n    .. code-block:: bash\n\n        salt myminion boto_datapipeline.delete_pipeline my_pipeline_id"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nretrieve metadata about one or more pipelines.", "response": "def describe_pipelines(pipeline_ids, region=None, key=None, keyid=None, profile=None):\n    '''\n    Retrieve metadata about one or more pipelines.\n\n    CLI example:\n\n    .. code-block:: bash\n\n        salt myminion boto_datapipeline.describe_pipelines ['my_pipeline_id']\n    '''\n    client = _get_client(region, key, keyid, profile)\n    r = {}\n    try:\n        r['result'] = client.describe_pipelines(pipelineIds=pipeline_ids)\n    except (botocore.exceptions.BotoCoreError, botocore.exceptions.ClientError) as e:\n        r['error'] = six.text_type(e)\n    return r"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef list_pipelines(region=None, key=None, keyid=None, profile=None):\n    '''\n    Get a list of pipeline ids and names for all pipelines.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_datapipeline.list_pipelines profile=myprofile\n    '''\n    client = _get_client(region, key, keyid, profile)\n    r = {}\n    try:\n        paginator = client.get_paginator('list_pipelines')\n        pipelines = []\n        for page in paginator.paginate():\n            pipelines += page['pipelineIdList']\n        r['result'] = pipelines\n    except (botocore.exceptions.BotoCoreError, botocore.exceptions.ClientError) as e:\n        r['error'] = six.text_type(e)\n    return r", "response": "Get a list of pipeline ids and names for all pipelines."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef pipeline_id_from_name(name, region=None, key=None, keyid=None, profile=None):\n    '''\n    Get the pipeline id, if it exists, for the given name.\n\n    CLI example:\n\n    .. code-block:: bash\n\n        salt myminion boto_datapipeline.pipeline_id_from_name my_pipeline_name\n    '''\n    r = {}\n    result_pipelines = list_pipelines()\n    if 'error' in result_pipelines:\n        return result_pipelines\n\n    for pipeline in result_pipelines['result']:\n        if pipeline['name'] == name:\n            r['result'] = pipeline['id']\n            return r\n    r['error'] = 'No pipeline found with name={0}'.format(name)\n    return r", "response": "Get the pipeline id for the given name."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding tasks, schedules, and preconditions to the specified pipeline. This function is idempotent and will replace an existing definition. CLI example: .. code-block:: bash salt myminion boto_datapipeline.put_pipeline_definition my_pipeline_id my_pipeline_objects", "response": "def put_pipeline_definition(pipeline_id, pipeline_objects, parameter_objects=None,\n                            parameter_values=None, region=None, key=None, keyid=None, profile=None):\n    '''\n    Add tasks, schedules, and preconditions to the specified pipeline. This function is\n    idempotent and will replace an existing definition.\n\n    CLI example:\n\n    .. code-block:: bash\n\n        salt myminion boto_datapipeline.put_pipeline_definition my_pipeline_id my_pipeline_objects\n    '''\n    parameter_objects = parameter_objects or []\n    parameter_values = parameter_values or []\n    client = _get_client(region, key, keyid, profile)\n    r = {}\n    try:\n        response = client.put_pipeline_definition(\n            pipelineId=pipeline_id,\n            pipelineObjects=pipeline_objects,\n            parameterObjects=parameter_objects,\n            parameterValues=parameter_values,\n        )\n        if response['errored']:\n            r['error'] = response['validationErrors']\n        else:\n            r['result'] = response\n    except (botocore.exceptions.BotoCoreError, botocore.exceptions.ClientError) as e:\n        r['error'] = six.text_type(e)\n    return r"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_client(region, key, keyid, profile):\n    '''\n    Get a boto connection to Data Pipeline.\n    '''\n    session = _get_session(region, key, keyid, profile)\n    if not session:\n        log.error(\"Failed to get datapipeline client.\")\n        return None\n\n    return session.client('datapipeline')", "response": "Get a boto connection to the data pipeline client."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a boto3 session", "response": "def _get_session(region, key, keyid, profile):\n    '''\n    Get a boto3 session\n    '''\n    if profile:\n        if isinstance(profile, six.string_types):\n            _profile = __salt__['config.option'](profile)\n        elif isinstance(profile, dict):\n            _profile = profile\n        key = _profile.get('key', None)\n        keyid = _profile.get('keyid', None)\n        region = _profile.get('region', None)\n\n    if not region and __salt__['config.option']('datapipeline.region'):\n        region = __salt__['config.option']('datapipeline.region')\n\n    if not region:\n        region = 'us-east-1'\n\n    return boto3.session.Session(\n        region_name=region,\n        aws_secret_access_key=key,\n        aws_access_key_id=keyid,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a blank virtual machine image file of the specified size in megabytes.", "response": "def make_image(location, size, fmt):\n    '''\n    Create a blank virtual machine image file of the specified size in\n    megabytes. The image can be created in any format supported by qemu\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' qemu_img.make_image /tmp/image.qcow 2048 qcow2\n        salt '*' qemu_img.make_image /tmp/image.raw 10240 raw\n    '''\n    if not os.path.isabs(location):\n        return ''\n    if not os.path.isdir(os.path.dirname(location)):\n        return ''\n    if not __salt__['cmd.retcode'](\n            'qemu-img create -f {0} {1} {2}M'.format(\n                fmt,\n                location,\n                size),\n                python_shell=False):\n        return location\n    return ''"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts an existing disk image to another format using qemu - img. convert", "response": "def convert(orig, dest, fmt):\n    '''\n    Convert an existing disk image to another format using qemu-img\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' qemu_img.convert /path/to/original.img /path/to/new.img qcow2\n    '''\n    cmd = ('qemu-img', 'convert', '-O', fmt, orig, dest)\n    ret = __salt__['cmd.run_all'](cmd, python_shell=False)\n    if ret['retcode'] == 0:\n        return True\n    else:\n        return ret['stderr']"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ping(host, timeout=False, return_boolean=False):\n    '''\n    Performs a ping to a host\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' network.ping archlinux.org\n\n    .. versionadded:: 2016.11.0\n\n    Return a True or False instead of ping output.\n\n    .. code-block:: bash\n\n        salt '*' network.ping archlinux.org return_boolean=True\n\n    Set the time to wait for a response in seconds.\n\n    .. code-block:: bash\n\n        salt '*' network.ping archlinux.org timeout=3\n    '''\n    if timeout:\n        # Windows ping differs by having timeout be for individual echo requests.'\n        # Divide timeout by tries to mimic BSD behaviour.\n        timeout = int(timeout) * 1000 // 4\n        cmd = ['ping', '-n', '4', '-w', six.text_type(timeout), salt.utils.network.sanitize_host(host)]\n    else:\n        cmd = ['ping', '-n', '4', salt.utils.network.sanitize_host(host)]\n    if return_boolean:\n        ret = __salt__['cmd.run_all'](cmd, python_shell=False)\n        if ret['retcode'] != 0:\n            return False\n        else:\n            return True\n    else:\n        return __salt__['cmd.run'](cmd, python_shell=False)", "response": "Performs a ping to a host and returns a boolean or False."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef netstat():\n    '''\n    Return information on open ports and states\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' network.netstat\n    '''\n    ret = []\n    cmd = ['netstat', '-nao']\n    lines = __salt__['cmd.run'](cmd, python_shell=False).splitlines()\n    for line in lines:\n        comps = line.split()\n        if line.startswith('  TCP'):\n            ret.append({\n                'local-address': comps[1],\n                'proto': comps[0],\n                'remote-address': comps[2],\n                'state': comps[3],\n                'program': comps[4]})\n        if line.startswith('  UDP'):\n            ret.append({\n                'local-address': comps[1],\n                'proto': comps[0],\n                'remote-address': comps[2],\n                'state': None,\n                'program': comps[3]})\n    return ret", "response": "Return information on open ports and states of the\nridge network"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nperform a traceroute to a 3rd party host getTotalLanguage", "response": "def traceroute(host):\n    '''\n    Performs a traceroute to a 3rd party host\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' network.traceroute archlinux.org\n    '''\n    ret = []\n    cmd = ['tracert', salt.utils.network.sanitize_host(host)]\n    lines = __salt__['cmd.run'](cmd, python_shell=False).splitlines()\n    for line in lines:\n        if ' ' not in line:\n            continue\n        if line.startswith('Trac'):\n            continue\n        if line.startswith('over'):\n            continue\n        comps = line.split()\n        complength = len(comps)\n        # This method still needs to better catch rows of other lengths\n        # For example if some of the ms returns are '*'\n        if complength == 9:\n            result = {\n                'count': comps[0],\n                'hostname': comps[7],\n                'ip': comps[8],\n                'ms1': comps[1],\n                'ms2': comps[3],\n                'ms3': comps[5]}\n            ret.append(result)\n        elif complength == 8:\n            result = {\n                'count': comps[0],\n                'hostname': None,\n                'ip': comps[7],\n                'ms1': comps[1],\n                'ms2': comps[3],\n                'ms3': comps[5]}\n            ret.append(result)\n        else:\n            result = {\n                'count': comps[0],\n                'hostname': None,\n                'ip': None,\n                'ms1': None,\n                'ms2': None,\n                'ms3': None}\n            ret.append(result)\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nquery DNS for information about a domain or ip address", "response": "def nslookup(host):\n    '''\n    Query DNS for information about a domain or ip address\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' network.nslookup archlinux.org\n    '''\n    ret = []\n    addresses = []\n    cmd = ['nslookup', salt.utils.network.sanitize_host(host)]\n    lines = __salt__['cmd.run'](cmd, python_shell=False).splitlines()\n    for line in lines:\n        if addresses:\n            # We're in the last block listing addresses\n            addresses.append(line.strip())\n            continue\n        if line.startswith('Non-authoritative'):\n            continue\n        if 'Addresses' in line:\n            comps = line.split(\":\", 1)\n            addresses.append(comps[1].strip())\n            continue\n        if \":\" in line:\n            comps = line.split(\":\", 1)\n            ret.append({comps[0].strip(): comps[1].strip()})\n    if addresses:\n        ret.append({'Addresses': addresses})\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn routing information for given destination ip", "response": "def get_route(ip):\n    '''\n    Return routing information for given destination ip\n\n    .. versionadded:: 2016.11.5\n\n    CLI Example::\n\n        salt '*' network.get_route 10.10.10.10\n    '''\n    cmd = 'Find-NetRoute -RemoteIPAddress {0}'.format(ip)\n    out = __salt__['cmd.run'](cmd, shell='powershell', python_shell=True)\n    regexp = re.compile(\n        r\"^IPAddress\\s+:\\s(?P<source>[\\d\\.:]+)?.*\"\n        r\"^InterfaceAlias\\s+:\\s(?P<interface>[\\w\\.\\:\\-\\ ]+)?.*\"\n        r\"^NextHop\\s+:\\s(?P<gateway>[\\d\\.:]+)\",\n        flags=re.MULTILINE | re.DOTALL\n    )\n    m = regexp.search(out)\n    ret = {\n        'destination': ip,\n        'gateway': m.group('gateway'),\n        'interface': m.group('interface'),\n        'source': m.group('source')\n    }\n\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nperform a DNS lookup with dig", "response": "def dig(host):\n    '''\n    Performs a DNS lookup with dig\n\n    Note: dig must be installed on the Windows minion\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' network.dig archlinux.org\n    '''\n    cmd = ['dig', salt.utils.network.sanitize_host(host)]\n    return __salt__['cmd.run'](cmd, python_shell=False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef interfaces_names():\n    '''\n    Return a list of all the interfaces names\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' network.interfaces_names\n    '''\n\n    ret = []\n    with salt.utils.winapi.Com():\n        c = wmi.WMI()\n        for iface in c.Win32_NetworkAdapter(NetEnabled=True):\n            ret.append(iface.NetConnectionID)\n    return ret", "response": "Return a list of all the interfaces names\n    CLI Example : bash\nCTYPE network. interfaces_names\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of IPv4 addresses assigned to the host.", "response": "def ip_addrs(interface=None, include_loopback=False, cidr=None, type=None):\n    '''\n    Returns a list of IPv4 addresses assigned to the host.\n\n    interface\n        Only IP addresses from that interface will be returned.\n\n    include_loopback : False\n        Include loopback 127.0.0.1 IPv4 address.\n\n    cidr\n        Describes subnet using CIDR notation and only IPv4 addresses that belong\n        to this subnet will be returned.\n\n      .. versionchanged:: 2019.2.0\n\n    type\n        If option set to 'public' then only public addresses will be returned.\n        Ditto for 'private'.\n\n        .. versionchanged:: 2019.2.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' network.ip_addrs\n        salt '*' network.ip_addrs cidr=10.0.0.0/8\n        salt '*' network.ip_addrs cidr=192.168.0.0/16 type=private\n    '''\n    addrs = salt.utils.network.ip_addrs(interface=interface,\n                                        include_loopback=include_loopback)\n    if cidr:\n        return [i for i in addrs if salt.utils.network.in_subnet(cidr, [i])]\n    else:\n        if type == 'public':\n            return [i for i in addrs if not is_private(i)]\n        elif type == 'private':\n            return [i for i in addrs if is_private(i)]\n        else:\n            return addrs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ip_addrs6(interface=None, include_loopback=False, cidr=None):\n    '''\n    Returns a list of IPv6 addresses assigned to the host.\n\n    interface\n        Only IP addresses from that interface will be returned.\n\n    include_loopback : False\n        Include loopback ::1 IPv6 address.\n\n    cidr\n        Describes subnet using CIDR notation and only IPv6 addresses that belong\n        to this subnet will be returned.\n\n        .. versionchanged:: 2019.2.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' network.ip_addrs6\n        salt '*' network.ip_addrs6 cidr=2000::/3\n    '''\n    addrs = salt.utils.network.ip_addrs6(interface=interface,\n                                        include_loopback=include_loopback)\n    if cidr:\n        return [i for i in addrs if salt.utils.network.in_subnet(cidr, [i])]\n    else:\n        return addrs", "response": "Returns a list of IPv6 addresses assigned to the host."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef connect(host, port=None, **kwargs):\n    '''\n    Test connectivity to a host using a particular\n    port from the minion.\n\n    .. versionadded:: 2016.3.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' network.connect archlinux.org 80\n\n        salt '*' network.connect archlinux.org 80 timeout=3\n\n        salt '*' network.connect archlinux.org 80 timeout=3 family=ipv4\n\n        salt '*' network.connect google-public-dns-a.google.com port=53 proto=udp timeout=3\n    '''\n\n    ret = {'result': None,\n           'comment': ''}\n\n    if not host:\n        ret['result'] = False\n        ret['comment'] = 'Required argument, host, is missing.'\n        return ret\n\n    if not port:\n        ret['result'] = False\n        ret['comment'] = 'Required argument, port, is missing.'\n        return ret\n\n    proto = kwargs.get('proto', 'tcp')\n    timeout = kwargs.get('timeout', 5)\n    family = kwargs.get('family', None)\n\n    if salt.utils.validate.net.ipv4_addr(host) or salt.utils.validate.net.ipv6_addr(host):\n        address = host\n    else:\n        address = '{0}'.format(salt.utils.network.sanitize_host(host))\n\n    try:\n        if proto == 'udp':\n            __proto = socket.SOL_UDP\n        else:\n            __proto = socket.SOL_TCP\n            proto = 'tcp'\n\n        if family:\n            if family == 'ipv4':\n                __family = socket.AF_INET\n            elif family == 'ipv6':\n                __family = socket.AF_INET6\n            else:\n                __family = 0\n        else:\n            __family = 0\n\n        (family,\n         socktype,\n         _proto,\n         garbage,\n         _address) = socket.getaddrinfo(address, port, __family, 0, __proto)[0]\n\n        skt = socket.socket(family, socktype, _proto)\n        skt.settimeout(timeout)\n\n        if proto == 'udp':\n            # Generate a random string of a\n            # decent size to test UDP connection\n            md5h = hashlib.md5()\n            md5h.update(datetime.datetime.now().strftime('%s'))\n            msg = md5h.hexdigest()\n            skt.sendto(msg, _address)\n            recv, svr = skt.recvfrom(255)\n            skt.close()\n        else:\n            skt.connect(_address)\n            skt.shutdown(2)\n    except Exception as exc:\n        ret['result'] = False\n        ret['comment'] = 'Unable to connect to {0} ({1}) on {2} port {3}'\\\n            .format(host, _address[0], proto, port)\n        return ret\n\n    ret['result'] = True\n    ret['comment'] = 'Successfully connected to {0} ({1}) on {2} port {3}'\\\n        .format(host, _address[0], proto, port)\n    return ret", "response": "Test connectivity to a specific host using a particular port from the minion."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _post_message(user,\n                  device,\n                  message,\n                  title,\n                  priority,\n                  expire,\n                  retry,\n                  sound,\n                  api_version=1,\n                  token=None):\n    '''\n    Send a message to a Pushover user or group.\n    :param user:        The user or group to send to, must be key of user or group not email address.\n    :param message:     The message to send to the PushOver user or group.\n    :param title:       Specify who the message is from.\n    :param priority     The priority of the message, defaults to 0.\n    :param api_version: The PushOver API version, if not specified in the configuration.\n    :param notify:      Whether to notify the room, default: False.\n    :param token:       The PushOver token, if not specified in the configuration.\n    :return:            Boolean if message was sent successfully.\n    '''\n\n    user_validate = salt.utils.pushover.validate_user(user, device, token)\n    if not user_validate['result']:\n        return user_validate\n\n    parameters = dict()\n    parameters['user'] = user\n    parameters['device'] = device\n    parameters['token'] = token\n    parameters['title'] = title\n    parameters['priority'] = priority\n    parameters['expire'] = expire\n    parameters['retry'] = retry\n    parameters['message'] = message\n\n    if sound:\n        sound_validate = salt.utils.pushover.validate_sound(sound, token)\n        if sound_validate['res']:\n            parameters['sound'] = sound\n\n    result = salt.utils.pushover.query(function='message',\n                                       method='POST',\n                                       header_dict={'Content-Type': 'application/x-www-form-urlencoded'},\n                                       data=_urlencode(parameters),\n                                       opts=__opts__)\n\n    return result", "response": "Send a message to a PushOver user or group."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef returner(ret):\n    '''\n    Send an PushOver message with the data\n    '''\n\n    _options = _get_options(ret)\n\n    user = _options.get('user')\n    device = _options.get('device')\n    token = _options.get('token')\n    title = _options.get('title')\n    priority = _options.get('priority')\n    expire = _options.get('expire')\n    retry = _options.get('retry')\n    sound = _options.get('sound')\n\n    if not token:\n        raise SaltInvocationError('Pushover token is unavailable.')\n\n    if not user:\n        raise SaltInvocationError('Pushover user key is unavailable.')\n\n    if priority and priority == 2:\n        if not expire and not retry:\n            raise SaltInvocationError('Priority 2 requires pushover.expire and pushover.retry options.')\n\n    message = ('id: {0}\\r\\n'\n               'function: {1}\\r\\n'\n               'function args: {2}\\r\\n'\n               'jid: {3}\\r\\n'\n               'return: {4}\\r\\n').format(\n                    ret.get('id'),\n                    ret.get('fun'),\n                    ret.get('fun_args'),\n                    ret.get('jid'),\n                    pprint.pformat(ret.get('return')))\n\n    result = _post_message(user=user,\n                           device=device,\n                           message=message,\n                           title=title,\n                           priority=priority,\n                           expire=expire,\n                           retry=retry,\n                           sound=sound,\n                           token=token)\n\n    log.debug('pushover result %s', result)\n    if not result['res']:\n        log.info('Error: %s', result['message'])\n    return", "response": "Send a pushover message with the data"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _format_entrypoint_target(ep):\n    '''\n    Makes a string describing the target of an EntryPoint object.\n\n    Base strongly on EntryPoint.__str__().\n    '''\n    s = ep.module_name\n    if ep.attrs:\n        s += ':' + '.'.join(ep.attrs)\n    return s", "response": "Formats the EntryPoint target as a string."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef minion_mods(\n        opts,\n        context=None,\n        utils=None,\n        whitelist=None,\n        initial_load=False,\n        loaded_base_name=None,\n        notify=False,\n        static_modules=None,\n        proxy=None):\n    '''\n    Load execution modules\n\n    Returns a dictionary of execution modules appropriate for the current\n    system by evaluating the __virtual__() function in each module.\n\n    :param dict opts: The Salt options dictionary\n\n    :param dict context: A Salt context that should be made present inside\n                            generated modules in __context__\n\n    :param dict utils: Utility functions which should be made available to\n                            Salt modules in __utils__. See `utils_dirs` in\n                            salt.config for additional information about\n                            configuration.\n\n    :param list whitelist: A list of modules which should be whitelisted.\n    :param bool initial_load: Deprecated flag! Unused.\n    :param str loaded_base_name: A string marker for the loaded base name.\n    :param bool notify: Flag indicating that an event should be fired upon\n                        completion of module loading.\n\n    .. code-block:: python\n\n        import salt.config\n        import salt.loader\n\n        __opts__ = salt.config.minion_config('/etc/salt/minion')\n        __grains__ = salt.loader.grains(__opts__)\n        __opts__['grains'] = __grains__\n        __utils__ = salt.loader.utils(__opts__)\n        __salt__ = salt.loader.minion_mods(__opts__, utils=__utils__)\n        __salt__['test.ping']()\n    '''\n    # TODO Publish documentation for module whitelisting\n    if not whitelist:\n        whitelist = opts.get('whitelist_modules', None)\n    ret = LazyLoader(\n        _module_dirs(opts, 'modules', 'module'),\n        opts,\n        tag='module',\n        pack={'__context__': context, '__utils__': utils, '__proxy__': proxy},\n        whitelist=whitelist,\n        loaded_base_name=loaded_base_name,\n        static_modules=static_modules,\n    )\n\n    ret.pack['__salt__'] = ret\n\n    # Load any provider overrides from the configuration file providers option\n    #  Note: Providers can be pkg, service, user or group - not to be confused\n    #        with cloud providers.\n    providers = opts.get('providers', False)\n    if providers and isinstance(providers, dict):\n        for mod in providers:\n            # sometimes providers opts is not to diverge modules but\n            # for other configuration\n            try:\n                funcs = raw_mod(opts, providers[mod], ret)\n            except TypeError:\n                break\n            else:\n                if funcs:\n                    for func in funcs:\n                        f_key = '{0}{1}'.format(mod, func[func.rindex('.'):])\n                        ret[f_key] = funcs[func]\n\n    if notify:\n        evt = salt.utils.event.get_event('minion', opts=opts, listen=False)\n        evt.fire_event({'complete': True},\n                       tag=salt.defaults.events.MINION_MOD_COMPLETE)\n\n    return ret", "response": "Load execution modules for the current salt master"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef raw_mod(opts, name, functions, mod='modules'):\n    '''\n    Returns a single module loaded raw and bypassing the __virtual__ function\n\n    .. code-block:: python\n\n        import salt.config\n        import salt.loader\n\n        __opts__ = salt.config.minion_config('/etc/salt/minion')\n        testmod = salt.loader.raw_mod(__opts__, 'test', None)\n        testmod['test.ping']()\n    '''\n    loader = LazyLoader(\n        _module_dirs(opts, mod, 'module'),\n        opts,\n        tag='rawmodule',\n        virtual_enable=False,\n        pack={'__salt__': functions},\n    )\n    # if we don't have the module, return an empty dict\n    if name not in loader.file_mapping:\n        return {}\n\n    loader._load_module(name)  # load a single module (the one passed in)\n    return dict(loader._dict)", "response": "Load a single module from raw mode"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef engines(opts, functions, runners, utils, proxy=None):\n    '''\n    Return the master services plugins\n    '''\n    pack = {'__salt__': functions,\n            '__runners__': runners,\n            '__proxy__': proxy,\n            '__utils__': utils}\n    return LazyLoader(\n        _module_dirs(opts, 'engines'),\n        opts,\n        tag='engines',\n        pack=pack,\n    )", "response": "Return the master services plugins\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the proxy module for this salt - proxy - minion", "response": "def proxy(opts, functions=None, returners=None, whitelist=None, utils=None):\n    '''\n    Returns the proxy module for this salt-proxy-minion\n    '''\n    ret = LazyLoader(\n        _module_dirs(opts, 'proxy'),\n        opts,\n        tag='proxy',\n        pack={'__salt__': functions, '__ret__': returners, '__utils__': utils},\n    )\n\n    ret.pack['__proxy__'] = ret\n\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the returner modules", "response": "def returners(opts, functions, whitelist=None, context=None, proxy=None):\n    '''\n    Returns the returner modules\n    '''\n    return LazyLoader(\n        _module_dirs(opts, 'returners', 'returner'),\n        opts,\n        tag='returner',\n        whitelist=whitelist,\n        pack={'__salt__': functions, '__context__': context, '__proxy__': proxy or {}},\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef utils(opts, whitelist=None, context=None, proxy=proxy):\n    '''\n    Returns the utility modules\n    '''\n    return LazyLoader(\n        _module_dirs(opts, 'utils', ext_type_dirs='utils_dirs'),\n        opts,\n        tag='utils',\n        whitelist=whitelist,\n        pack={'__context__': context, '__proxy__': proxy or {}},\n    )", "response": "Returns the utility modules\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the pillars modules", "response": "def pillars(opts, functions, context=None):\n    '''\n    Returns the pillars modules\n    '''\n    ret = LazyLoader(_module_dirs(opts, 'pillar'),\n                     opts,\n                     tag='pillar',\n                     pack={'__salt__': functions,\n                           '__context__': context,\n                           '__utils__': utils(opts)})\n    ret.pack['__ext_pillar__'] = ret\n    return FilterDictWrapper(ret, '.ext_pillar')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the tops modules", "response": "def tops(opts):\n    '''\n    Returns the tops modules\n    '''\n    if 'master_tops' not in opts:\n        return {}\n    whitelist = list(opts['master_tops'].keys())\n    ret = LazyLoader(\n        _module_dirs(opts, 'tops', 'top'),\n        opts,\n        tag='top',\n        whitelist=whitelist,\n    )\n    return FilterDictWrapper(ret, '.top')"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the wheels modules", "response": "def wheels(opts, whitelist=None, context=None):\n    '''\n    Returns the wheels modules\n    '''\n    if context is None:\n        context = {}\n    return LazyLoader(\n        _module_dirs(opts, 'wheel'),\n        opts,\n        tag='wheel',\n        whitelist=whitelist,\n        pack={'__context__': context},\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the outputters modules", "response": "def outputters(opts):\n    '''\n    Returns the outputters modules\n\n    :param dict opts: The Salt options dictionary\n    :returns: LazyLoader instance, with only outputters present in the keyspace\n    '''\n    ret = LazyLoader(\n        _module_dirs(opts, 'output', ext_type_dirs='outputter_dirs'),\n        opts,\n        tag='output',\n    )\n    wrapped_ret = FilterDictWrapper(ret, '.output')\n    # TODO: this name seems terrible... __salt__ should always be execution mods\n    ret.pack['__salt__'] = wrapped_ret\n    return wrapped_ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef auth(opts, whitelist=None):\n    '''\n    Returns the auth modules\n\n    :param dict opts: The Salt options dictionary\n    :returns: LazyLoader\n    '''\n    return LazyLoader(\n        _module_dirs(opts, 'auth'),\n        opts,\n        tag='auth',\n        whitelist=whitelist,\n        pack={'__salt__': minion_mods(opts)},\n    )", "response": "Returns the auth modules"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the file server modules", "response": "def fileserver(opts, backends):\n    '''\n    Returns the file server modules\n    '''\n    return LazyLoader(_module_dirs(opts, 'fileserver'),\n                      opts,\n                      tag='fileserver',\n                      whitelist=backends,\n                      pack={'__utils__': utils(opts)})"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the roster modules", "response": "def roster(opts, runner=None, utils=None, whitelist=None):\n    '''\n    Returns the roster modules\n    '''\n    return LazyLoader(\n        _module_dirs(opts, 'roster'),\n        opts,\n        tag='roster',\n        whitelist=whitelist,\n        pack={\n            '__runner__': runner,\n            '__utils__': utils,\n        },\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads the thorium runtime modules", "response": "def thorium(opts, functions, runners):\n    '''\n    Load the thorium runtime modules\n    '''\n    pack = {'__salt__': functions, '__runner__': runners, '__context__': {}}\n    ret = LazyLoader(_module_dirs(opts, 'thorium'),\n            opts,\n            tag='thorium',\n            pack=pack)\n    ret.pack['__thorium__'] = ret\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef states(opts, functions, utils, serializers, whitelist=None, proxy=None, context=None):\n    '''\n    Returns the state modules\n\n    :param dict opts: The Salt options dictionary\n    :param dict functions: A dictionary of minion modules, with module names as\n                            keys and funcs as values.\n\n    .. code-block:: python\n\n        import salt.config\n        import salt.loader\n\n        __opts__ = salt.config.minion_config('/etc/salt/minion')\n        statemods = salt.loader.states(__opts__, None, None)\n    '''\n    if context is None:\n        context = {}\n\n    ret = LazyLoader(\n        _module_dirs(opts, 'states'),\n        opts,\n        tag='states',\n        pack={'__salt__': functions, '__proxy__': proxy or {}},\n        whitelist=whitelist,\n    )\n    ret.pack['__states__'] = ret\n    ret.pack['__utils__'] = utils\n    ret.pack['__serializers__'] = serializers\n    ret.pack['__context__'] = context\n    return ret", "response": "Returns the state modules"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload the beacon modules", "response": "def beacons(opts, functions, context=None, proxy=None):\n    '''\n    Load the beacon modules\n\n    :param dict opts: The Salt options dictionary\n    :param dict functions: A dictionary of minion modules, with module names as\n                            keys and funcs as values.\n    '''\n    return LazyLoader(\n        _module_dirs(opts, 'beacons'),\n        opts,\n        tag='beacons',\n        pack={'__context__': context, '__salt__': functions, '__proxy__': proxy or {}},\n        virtual_funcs=[],\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef log_handlers(opts):\n    '''\n    Returns the custom logging handler modules\n\n    :param dict opts: The Salt options dictionary\n    '''\n    ret = LazyLoader(\n        _module_dirs(\n            opts,\n            'log_handlers',\n            int_type='handlers',\n            base_path=os.path.join(SALT_BASE_PATH, 'log'),\n        ),\n        opts,\n        tag='log_handlers',\n    )\n    return FilterDictWrapper(ret, '.setup_handlers')", "response": "Returns the custom logging handler modules\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ssh_wrapper(opts, functions=None, context=None):\n    '''\n    Returns the custom logging handler modules\n    '''\n    return LazyLoader(\n        _module_dirs(\n            opts,\n            'wrapper',\n            base_path=os.path.join(SALT_BASE_PATH, os.path.join('client', 'ssh')),\n        ),\n        opts,\n        tag='wrapper',\n        pack={\n            '__salt__': functions,\n            '__grains__': opts.get('grains', {}),\n            '__pillar__': opts.get('pillar', {}),\n            '__context__': context,\n            },\n    )", "response": "Returns the custom logging handler modules\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the render modules", "response": "def render(opts, functions, states=None, proxy=None, context=None):\n    '''\n    Returns the render modules\n    '''\n    if context is None:\n        context = {}\n\n    pack = {'__salt__': functions,\n            '__grains__': opts.get('grains', {}),\n            '__context__': context}\n\n    if states:\n        pack['__states__'] = states\n    pack['__proxy__'] = proxy or {}\n    ret = LazyLoader(\n        _module_dirs(\n            opts,\n            'renderers',\n            'render',\n            ext_type_dirs='render_dirs',\n        ),\n        opts,\n        tag='render',\n        pack=pack,\n    )\n    rend = FilterDictWrapper(ret, '.render')\n\n    if not check_render_pipe_str(opts['renderer'], rend, opts['renderer_blacklist'], opts['renderer_whitelist']):\n        err = ('The renderer {0} is unavailable, this error is often because '\n               'the needed software is unavailable'.format(opts['renderer']))\n        log.critical(err)\n        raise LoaderError(err)\n    return rend"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef grain_funcs(opts, proxy=None):\n    '''\n    Returns the grain functions\n\n      .. code-block:: python\n\n          import salt.config\n          import salt.loader\n\n          __opts__ = salt.config.minion_config('/etc/salt/minion')\n          grainfuncs = salt.loader.grain_funcs(__opts__)\n    '''\n    ret = LazyLoader(\n        _module_dirs(\n            opts,\n            'grains',\n            'grain',\n            ext_type_dirs='grains_dirs',\n        ),\n        opts,\n        tag='grains',\n    )\n    ret.pack['__utils__'] = utils(opts, proxy=proxy)\n    return ret", "response": "Returns the grain functions for the current node"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _load_cached_grains(opts, cfn):\n    '''\n    Returns the grains cached in cfn, or None if the cache is too old or is\n    corrupted.\n    '''\n    if not os.path.isfile(cfn):\n        log.debug('Grains cache file does not exist.')\n        return None\n\n    grains_cache_age = int(time.time() - os.path.getmtime(cfn))\n    if grains_cache_age > opts.get('grains_cache_expiration', 300):\n        log.debug(\n            'Grains cache last modified %s seconds ago and cache '\n            'expiration is set to %s. Grains cache expired. '\n            'Refreshing.',\n            grains_cache_age, opts.get('grains_cache_expiration', 300)\n        )\n        return None\n\n    if opts.get('refresh_grains_cache', False):\n        log.debug('refresh_grains_cache requested, Refreshing.')\n        return None\n\n    log.debug('Retrieving grains from cache')\n    try:\n        serial = salt.payload.Serial(opts)\n        with salt.utils.files.fopen(cfn, 'rb') as fp_:\n            cached_grains = salt.utils.data.decode(serial.load(fp_), preserve_tuples=True)\n        if not cached_grains:\n            log.debug('Cached grains are empty, cache might be corrupted. Refreshing.')\n            return None\n\n        return cached_grains\n    except (IOError, OSError):\n        return None", "response": "Load the grains from a cached file."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the functions for the dynamic grains and the values for the static grains.", "response": "def grains(opts, force_refresh=False, proxy=None):\n    '''\n    Return the functions for the dynamic grains and the values for the static\n    grains.\n\n    Since grains are computed early in the startup process, grains functions\n    do not have __salt__ or __proxy__ available.  At proxy-minion startup,\n    this function is called with the proxymodule LazyLoader object so grains\n    functions can communicate with their controlled device.\n\n    .. code-block:: python\n\n        import salt.config\n        import salt.loader\n\n        __opts__ = salt.config.minion_config('/etc/salt/minion')\n        __grains__ = salt.loader.grains(__opts__)\n        print __grains__['id']\n    '''\n    # Need to re-import salt.config, somehow it got lost when a minion is starting\n    import salt.config\n    # if we have no grains, lets try loading from disk (TODO: move to decorator?)\n    cfn = os.path.join(\n        opts['cachedir'],\n        'grains.cache.p'\n    )\n    if not force_refresh and opts.get('grains_cache', False):\n        cached_grains = _load_cached_grains(opts, cfn)\n        if cached_grains:\n            return cached_grains\n    else:\n        log.debug('Grains refresh requested. Refreshing grains.')\n\n    if opts.get('skip_grains', False):\n        return {}\n    grains_deep_merge = opts.get('grains_deep_merge', False) is True\n    if 'conf_file' in opts:\n        pre_opts = {}\n        pre_opts.update(salt.config.load_config(\n            opts['conf_file'], 'SALT_MINION_CONFIG',\n            salt.config.DEFAULT_MINION_OPTS['conf_file']\n        ))\n        default_include = pre_opts.get(\n            'default_include', opts['default_include']\n        )\n        include = pre_opts.get('include', [])\n        pre_opts.update(salt.config.include_config(\n            default_include, opts['conf_file'], verbose=False\n        ))\n        pre_opts.update(salt.config.include_config(\n            include, opts['conf_file'], verbose=True\n        ))\n        if 'grains' in pre_opts:\n            opts['grains'] = pre_opts['grains']\n        else:\n            opts['grains'] = {}\n    else:\n        opts['grains'] = {}\n\n    grains_data = {}\n    blist = opts.get('grains_blacklist', [])\n    funcs = grain_funcs(opts, proxy=proxy)\n    if force_refresh:  # if we refresh, lets reload grain modules\n        funcs.clear()\n    # Run core grains\n    for key in funcs:\n        if not key.startswith('core.'):\n            continue\n        log.trace('Loading %s grain', key)\n        ret = funcs[key]()\n        if not isinstance(ret, dict):\n            continue\n        if blist:\n            for key in list(ret):\n                for block in blist:\n                    if salt.utils.stringutils.expr_match(key, block):\n                        del ret[key]\n                        log.trace('Filtering %s grain', key)\n            if not ret:\n                continue\n        if grains_deep_merge:\n            salt.utils.dictupdate.update(grains_data, ret)\n        else:\n            grains_data.update(ret)\n\n    # Run the rest of the grains\n    for key in funcs:\n        if key.startswith('core.') or key == '_errors':\n            continue\n        try:\n            # Grains are loaded too early to take advantage of the injected\n            # __proxy__ variable.  Pass an instance of that LazyLoader\n            # here instead to grains functions if the grains functions take\n            # one parameter.  Then the grains can have access to the\n            # proxymodule for retrieving information from the connected\n            # device.\n            log.trace('Loading %s grain', key)\n            parameters = salt.utils.args.get_function_argspec(funcs[key]).args\n            kwargs = {}\n            if 'proxy' in parameters:\n                kwargs['proxy'] = proxy\n            if 'grains' in parameters:\n                kwargs['grains'] = grains_data\n            ret = funcs[key](**kwargs)\n        except Exception:\n            if salt.utils.platform.is_proxy():\n                log.info('The following CRITICAL message may not be an error; the proxy may not be completely established yet.')\n            log.critical(\n                'Failed to load grains defined in grain file %s in '\n                'function %s, error:\\n', key, funcs[key],\n                exc_info=True\n            )\n            continue\n        if not isinstance(ret, dict):\n            continue\n        if blist:\n            for key in list(ret):\n                for block in blist:\n                    if salt.utils.stringutils.expr_match(key, block):\n                        del ret[key]\n                        log.trace('Filtering %s grain', key)\n            if not ret:\n                continue\n        if grains_deep_merge:\n            salt.utils.dictupdate.update(grains_data, ret)\n        else:\n            grains_data.update(ret)\n\n    if opts.get('proxy_merge_grains_in_module', True) and proxy:\n        try:\n            proxytype = proxy.opts['proxy']['proxytype']\n            if proxytype + '.grains' in proxy:\n                if proxytype + '.initialized' in proxy and proxy[proxytype + '.initialized']():\n                    try:\n                        proxytype = proxy.opts['proxy']['proxytype']\n                        ret = proxy[proxytype + '.grains']()\n                        if grains_deep_merge:\n                            salt.utils.dictupdate.update(grains_data, ret)\n                        else:\n                            grains_data.update(ret)\n                    except Exception:\n                        log.critical('Failed to run proxy\\'s grains function!',\n                            exc_info=True\n                        )\n        except KeyError:\n            pass\n\n    grains_data.update(opts['grains'])\n    # Write cache if enabled\n    if opts.get('grains_cache', False):\n        with salt.utils.files.set_umask(0o077):\n            try:\n                if salt.utils.platform.is_windows():\n                    # Late import\n                    import salt.modules.cmdmod\n                    # Make sure cache file isn't read-only\n                    salt.modules.cmdmod._run_quiet('attrib -R \"{0}\"'.format(cfn))\n                with salt.utils.files.fopen(cfn, 'w+b') as fp_:\n                    try:\n                        serial = salt.payload.Serial(opts)\n                        serial.dump(grains_data, fp_)\n                    except TypeError as e:\n                        log.error('Failed to serialize grains cache: %s', e)\n                        raise  # re-throw for cleanup\n            except Exception as e:\n                log.error('Unable to write to grains cache file %s: %s', cfn, e)\n                # Based on the original exception, the file may or may not have been\n                # created. If it was, we will remove it now, as the exception means\n                # the serialized data is not to be trusted, no matter what the\n                # exception is.\n                if os.path.isfile(cfn):\n                    os.unlink(cfn)\n\n    if grains_deep_merge:\n        salt.utils.dictupdate.update(grains_data, opts['grains'])\n    else:\n        grains_data.update(opts['grains'])\n    return salt.utils.data.decode(grains_data, preserve_tuples=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef call(fun, **kwargs):\n    '''\n    Directly call a function inside a loader directory\n    '''\n    args = kwargs.get('args', [])\n    dirs = kwargs.get('dirs', [])\n\n    funcs = LazyLoader(\n        [os.path.join(SALT_BASE_PATH, 'modules')] + dirs,\n        None,\n        tag='modules',\n        virtual_enable=False,\n    )\n    return funcs[fun](*args)", "response": "Directly call a function inside a loader directory\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a lazy loader for the runners module", "response": "def runner(opts, utils=None, context=None, whitelist=None):\n    '''\n    Directly call a function inside a loader directory\n    '''\n    if utils is None:\n        utils = {}\n    if context is None:\n        context = {}\n    ret = LazyLoader(\n        _module_dirs(opts, 'runners', 'runner', ext_type_dirs='runner_dirs'),\n        opts,\n        tag='runners',\n        pack={'__utils__': utils, '__context__': context},\n        whitelist=whitelist,\n    )\n    # TODO: change from __salt__ to something else, we overload __salt__ too much\n    ret.pack['__salt__'] = ret\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sdb(opts, functions=None, whitelist=None, utils=None):\n    '''\n    Make a very small database call\n    '''\n    if utils is None:\n        utils = {}\n\n    return LazyLoader(\n        _module_dirs(opts, 'sdb'),\n        opts,\n        tag='sdb',\n        pack={\n            '__sdb__': functions,\n            '__opts__': opts,\n            '__utils__': utils,\n            '__salt__': minion_mods(opts, utils),\n        },\n        whitelist=whitelist,\n    )", "response": "Make a very small database call"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns modules for SPM s package database", "response": "def pkgdb(opts):\n    '''\n    Return modules for SPM's package database\n\n    .. versionadded:: 2015.8.0\n    '''\n    return LazyLoader(\n        _module_dirs(\n            opts,\n            'pkgdb',\n            base_path=os.path.join(SALT_BASE_PATH, 'spm')\n        ),\n        opts,\n        tag='pkgdb'\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef clouds(opts):\n    '''\n    Return the cloud functions\n    '''\n    # Let's bring __active_provider_name__, defaulting to None, to all cloud\n    # drivers. This will get temporarily updated/overridden with a context\n    # manager when needed.\n    functions = LazyLoader(\n        _module_dirs(opts,\n                     'clouds',\n                     'cloud',\n                     base_path=os.path.join(SALT_BASE_PATH, 'cloud'),\n                     int_type='clouds'),\n        opts,\n        tag='clouds',\n        pack={'__utils__': salt.loader.utils(opts),\n              '__active_provider_name__': None},\n    )\n    for funcname in LIBCLOUD_FUNCS_NOT_SUPPORTED:\n        log.trace(\n            '\\'%s\\' has been marked as not supported. Removing from the '\n            'list of supported cloud functions', funcname\n        )\n        functions.pop(funcname, None)\n    return functions", "response": "Return the cloud functions\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef executors(opts, functions=None, context=None, proxy=None):\n    '''\n    Returns the executor modules\n    '''\n    executors = LazyLoader(\n        _module_dirs(opts, 'executors', 'executor'),\n        opts,\n        tag='executor',\n        pack={'__salt__': functions, '__context__': context or {}, '__proxy__': proxy or {}},\n    )\n    executors.pack['__executors__'] = executors\n    return executors", "response": "Returns the executor modules\n   "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the returner modules", "response": "def cache(opts, serial):\n    '''\n    Returns the returner modules\n    '''\n    return LazyLoader(\n        _module_dirs(opts, 'cache', 'cache'),\n        opts,\n        tag='cache',\n        pack={'__opts__': opts, '__context__': {'serial': serial}},\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninjects a variable into a module.", "response": "def _inject_into_mod(mod, name, value, force_lock=False):\n    '''\n    Inject a variable into a module. This is used to inject \"globals\" like\n    ``__salt__``, ``__pillar``, or ``grains``.\n\n    Instead of injecting the value directly, a ``ThreadLocalProxy`` is created.\n    If such a proxy is already present under the specified name, it is updated\n    with the new value. This update only affects the current thread, so that\n    the same name can refer to different values depending on the thread of\n    execution.\n\n    This is important for data that is not truly global. For example, pillar\n    data might be dynamically overriden through function parameters and thus\n    the actual values available in pillar might depend on the thread that is\n    calling a module.\n\n    mod:\n        module object into which the value is going to be injected.\n\n    name:\n        name of the variable that is injected into the module.\n\n    value:\n        value that is injected into the variable. The value is not injected\n        directly, but instead set as the new reference of the proxy that has\n        been created for the variable.\n\n    force_lock:\n        whether the lock should be acquired before checking whether a proxy\n        object for the specified name has already been injected into the\n        module. If ``False`` (the default), this function checks for the\n        module's variable without acquiring the lock and only acquires the lock\n        if a new proxy has to be created and injected.\n    '''\n    old_value = getattr(mod, name, None)\n    # We use a double-checked locking scheme in order to avoid taking the lock\n    # when a proxy object has already been injected.\n    # In most programming languages, double-checked locking is considered\n    # unsafe when used without explicit memory barriers because one might read\n    # an uninitialized value. In CPython it is safe due to the global\n    # interpreter lock (GIL). In Python implementations that do not have the\n    # GIL, it could be unsafe, but at least Jython also guarantees that (for\n    # Python objects) memory is not corrupted when writing and reading without\n    # explicit synchronization\n    # (http://www.jython.org/jythonbook/en/1.0/Concurrency.html).\n    # Please note that in order to make this code safe in a runtime environment\n    # that does not make this guarantees, it is not sufficient. The\n    # ThreadLocalProxy must also be created with fallback_to_shared set to\n    # False or a lock must be added to the ThreadLocalProxy.\n    if force_lock:\n        with _inject_into_mod.lock:\n            if isinstance(old_value, ThreadLocalProxy):\n                ThreadLocalProxy.set_reference(old_value, value)\n            else:\n                setattr(mod, name, ThreadLocalProxy(value, True))\n    else:\n        if isinstance(old_value, ThreadLocalProxy):\n            ThreadLocalProxy.set_reference(old_value, value)\n        else:\n            _inject_into_mod(mod, name, value, True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef global_injector_decorator(inject_globals):\n    '''\n    Decorator used by the LazyLoader to inject globals into a function at\n    execute time.\n\n    globals\n        Dictionary with global variables to inject\n    '''\n    def inner_decorator(f):\n        @functools.wraps(f)\n        def wrapper(*args, **kwargs):\n            with salt.utils.context.func_globals_inject(f, **inject_globals):\n                return f(*args, **kwargs)\n        return wrapper\n    return inner_decorator", "response": "Decorator used by the LazyLoader to inject globals into a function at\n    execute time."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the error string for a missing function.", "response": "def missing_fun_string(self, function_name):\n        '''\n        Return the error string for a missing function.\n\n        This can range from \"not available' to \"__virtual__\" returned False\n        '''\n        mod_name = function_name.split('.')[0]\n        if mod_name in self.loaded_modules:\n            return '\\'{0}\\' is not available.'.format(function_name)\n        else:\n            try:\n                reason = self.missing_modules[mod_name]\n            except KeyError:\n                return '\\'{0}\\' is not available.'.format(function_name)\n            else:\n                if reason is not None:\n                    return '\\'{0}\\' __virtual__ returned False: {1}'.format(mod_name, reason)\n                else:\n                    return '\\'{0}\\' __virtual__ returned False'.format(mod_name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nrefreshes the file mapping of the FS on disk.", "response": "def _refresh_file_mapping(self):\n        '''\n        refresh the mapping of the FS on disk\n        '''\n        # map of suffix to description for imp\n        if self.opts.get('cython_enable', True) is True:\n            try:\n                global pyximport\n                pyximport = __import__('pyximport')  # pylint: disable=import-error\n                pyximport.install()\n                # add to suffix_map so file_mapping will pick it up\n                self.suffix_map['.pyx'] = tuple()\n            except ImportError:\n                log.info('Cython is enabled in the options but not present '\n                    'in the system path. Skipping Cython modules.')\n        # Allow for zipimport of modules\n        if self.opts.get('enable_zip_modules', True) is True:\n            self.suffix_map['.zip'] = tuple()\n        # allow for module dirs\n        if USE_IMPORTLIB:\n            self.suffix_map[''] = ('', '', MODULE_KIND_PKG_DIRECTORY)\n        else:\n            self.suffix_map[''] = ('', '', imp.PKG_DIRECTORY)\n\n        # create mapping of filename (without suffix) to (path, suffix)\n        # The files are added in order of priority, so order *must* be retained.\n        self.file_mapping = salt.utils.odict.OrderedDict()\n\n        opt_match = []\n\n        def _replace_pre_ext(obj):\n            '''\n            Hack so we can get the optimization level that we replaced (if\n            any) out of the re.sub call below. We use a list here because\n            it is a persistent data structure that we will be able to\n            access after re.sub is called.\n            '''\n            opt_match.append(obj)\n            return ''\n\n        for mod_dir in self.module_dirs:\n            try:\n                # Make sure we have a sorted listdir in order to have\n                # expectable override results\n                files = sorted(\n                    x for x in os.listdir(mod_dir) if x != '__pycache__'\n                )\n            except OSError:\n                continue  # Next mod_dir\n            if six.PY3:\n                try:\n                    pycache_files = [\n                        os.path.join('__pycache__', x) for x in\n                        sorted(os.listdir(os.path.join(mod_dir, '__pycache__')))\n                    ]\n                except OSError:\n                    pass\n                else:\n                    files.extend(pycache_files)\n\n            for filename in files:\n                try:\n                    dirname, basename = os.path.split(filename)\n                    if basename.startswith('_'):\n                        # skip private modules\n                        # log messages omitted for obviousness\n                        continue  # Next filename\n                    f_noext, ext = os.path.splitext(basename)\n                    if six.PY3:\n                        f_noext = PY3_PRE_EXT.sub(_replace_pre_ext, f_noext)\n                        try:\n                            opt_level = int(\n                                opt_match.pop().group(1).rsplit('-', 1)[-1]\n                            )\n                        except (AttributeError, IndexError, ValueError):\n                            # No regex match or no optimization level matched\n                            opt_level = 0\n                        try:\n                            opt_index = self.opts['optimization_order'].index(opt_level)\n                        except KeyError:\n                            log.trace(\n                                'Disallowed optimization level %d for module '\n                                'name \\'%s\\', skipping. Add %d to the '\n                                '\\'optimization_order\\' config option if you '\n                                'do not want to ignore this optimization '\n                                'level.', opt_level, f_noext, opt_level\n                            )\n                            continue\n                    else:\n                        # Optimization level not reflected in filename on PY2\n                        opt_index = 0\n\n                    # make sure it is a suffix we support\n                    if ext not in self.suffix_map:\n                        continue  # Next filename\n                    if f_noext in self.disabled:\n                        log.trace(\n                            'Skipping %s, it is disabled by configuration',\n                            filename\n                        )\n                        continue  # Next filename\n                    fpath = os.path.join(mod_dir, filename)\n                    # if its a directory, lets allow us to load that\n                    if ext == '':\n                        # is there something __init__?\n                        subfiles = os.listdir(fpath)\n                        for suffix in self.suffix_order:\n                            if '' == suffix:\n                                continue  # Next suffix (__init__ must have a suffix)\n                            init_file = '__init__{0}'.format(suffix)\n                            if init_file in subfiles:\n                                break\n                        else:\n                            continue  # Next filename\n\n                    try:\n                        curr_ext = self.file_mapping[f_noext][1]\n                        curr_opt_index = self.file_mapping[f_noext][2]\n                    except KeyError:\n                        pass\n                    else:\n                        if '' in (curr_ext, ext) and curr_ext != ext:\n                            log.error(\n                                'Module/package collision: \\'%s\\' and \\'%s\\'',\n                                fpath,\n                                self.file_mapping[f_noext][0]\n                            )\n\n                        if six.PY3 and ext == '.pyc' and curr_ext == '.pyc':\n                            # Check the optimization level\n                            if opt_index >= curr_opt_index:\n                                # Module name match, but a higher-priority\n                                # optimization level was already matched, skipping.\n                                continue\n                        elif not curr_ext or self.suffix_order.index(ext) >= self.suffix_order.index(curr_ext):\n                            # Match found but a higher-priorty match already\n                            # exists, so skip this.\n                            continue\n\n                    if six.PY3 and not dirname and ext == '.pyc':\n                        # On Python 3, we should only load .pyc files from the\n                        # __pycache__ subdirectory (i.e. when dirname is not an\n                        # empty string).\n                        continue\n\n                    # Made it this far - add it\n                    self.file_mapping[f_noext] = (fpath, ext, opt_index)\n\n                except OSError:\n                    continue\n        for smod in self.static_modules:\n            f_noext = smod.split('.')[-1]\n            self.file_mapping[f_noext] = (smod, '.o', 0)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef __prep_mod_opts(self, opts):\n        '''\n        Strip out of the opts any logger instance\n        '''\n        if '__grains__' not in self.pack:\n            grains = opts.get('grains', {})\n\n            if isinstance(grains, ThreadLocalProxy):\n                grains = ThreadLocalProxy.unproxy(grains)\n\n            self.context_dict['grains'] = grains\n            self.pack['__grains__'] = salt.utils.context.NamespacedDictWrapper(self.context_dict, 'grains')\n\n        if '__pillar__' not in self.pack:\n            pillar = opts.get('pillar', {})\n\n            if isinstance(pillar, ThreadLocalProxy):\n                pillar = ThreadLocalProxy.unproxy(pillar)\n\n            self.context_dict['pillar'] = pillar\n            self.pack['__pillar__'] = salt.utils.context.NamespacedDictWrapper(self.context_dict, 'pillar')\n\n        mod_opts = {}\n        for key, val in list(opts.items()):\n            if key == 'logger':\n                continue\n            mod_opts[key] = val\n        return mod_opts", "response": "Prepare the mod options for the mod module"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _iter_files(self, mod_name):\n        '''\n        Iterate over all file_mapping files in order of closeness to mod_name\n        '''\n        # do we have an exact match?\n        if mod_name in self.file_mapping:\n            yield mod_name\n\n        # do we have a partial match?\n        for k in self.file_mapping:\n            if mod_name in k:\n                yield k\n\n        # anyone else? Bueller?\n        for k in self.file_mapping:\n            if mod_name not in k:\n                yield k", "response": "Iterate over all files in the file_mapping for a given mod_name."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _load(self, key):\n        '''\n        Load a single item if you have it\n        '''\n        # if the key doesn't have a '.' then it isn't valid for this mod dict\n        if not isinstance(key, six.string_types):\n            raise KeyError('The key must be a string.')\n        if '.' not in key:\n            raise KeyError('The key \\'{0}\\' should contain a \\'.\\''.format(key))\n        mod_name, _ = key.split('.', 1)\n        with self._lock:\n            # It is possible that the key is in the dictionary after\n            # acquiring the lock due to another thread loading it.\n            if mod_name in self.missing_modules or key in self._dict:\n                return True\n            # if the modulename isn't in the whitelist, don't bother\n            if self.whitelist and mod_name not in self.whitelist:\n                log.error(\n                    'Failed to load function %s because its module (%s) is '\n                    'not in the whitelist: %s', key, mod_name, self.whitelist\n                )\n                raise KeyError(key)\n\n            def _inner_load(mod_name):\n                for name in self._iter_files(mod_name):\n                    if name in self.loaded_files:\n                        continue\n                    # if we got what we wanted, we are done\n                    if self._load_module(name) and key in self._dict:\n                        return True\n                return False\n\n            # try to load the module\n            ret = None\n            reloaded = False\n            # re-scan up to once, IOErrors or a failed load cause re-scans of the\n            # filesystem\n            while True:\n                try:\n                    ret = _inner_load(mod_name)\n                    if not reloaded and ret is not True:\n                        self._refresh_file_mapping()\n                        reloaded = True\n                        continue\n                    break\n                except IOError:\n                    if not reloaded:\n                        self._refresh_file_mapping()\n                        reloaded = True\n                    continue\n\n        return ret", "response": "Load a single item from the cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads all of them", "response": "def _load_all(self):\n        '''\n        Load all of them\n        '''\n        with self._lock:\n            for name in self.file_mapping:\n                if name in self.loaded_files or name in self.missing_modules:\n                    continue\n                self._load_module(name)\n\n            self.loaded = True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _apply_outputter(self, func, mod):\n        '''\n        Apply the __outputter__ variable to the functions\n        '''\n        if hasattr(mod, '__outputter__'):\n            outp = mod.__outputter__\n            if func.__name__ in outp:\n                func.__outputter__ = outp[func.__name__]", "response": "Apply the outputter variable to the functions\n            variable"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngives a loaded module and its default name determine its virtual name This function returns a tuple. The first value will be either True or False and will indicate if the module should be loaded or not (i.e. if it threw and exception while processing its __virtual__ function). The second value is the determined virtual name, which may be the same as the value provided. The default name can be calculated as follows:: module_name = mod.__name__.rsplit('.', 1)[-1]", "response": "def _process_virtual(self, mod, module_name, virtual_func='__virtual__'):\n        '''\n        Given a loaded module and its default name determine its virtual name\n\n        This function returns a tuple. The first value will be either True or\n        False and will indicate if the module should be loaded or not (i.e. if\n        it threw and exception while processing its __virtual__ function). The\n        second value is the determined virtual name, which may be the same as\n        the value provided.\n\n        The default name can be calculated as follows::\n\n            module_name = mod.__name__.rsplit('.', 1)[-1]\n        '''\n\n        # The __virtual__ function will return either a True or False value.\n        # If it returns a True value it can also set a module level attribute\n        # named __virtualname__ with the name that the module should be\n        # referred to as.\n        #\n        # This allows us to have things like the pkg module working on all\n        # platforms under the name 'pkg'. It also allows for modules like\n        # augeas_cfg to be referred to as 'augeas', which would otherwise have\n        # namespace collisions. And finally it allows modules to return False\n        # if they are not intended to run on the given platform or are missing\n        # dependencies.\n        virtual_aliases = getattr(mod, '__virtual_aliases__', tuple())\n        try:\n            error_reason = None\n            if hasattr(mod, '__virtual__') and inspect.isfunction(mod.__virtual__):\n                try:\n                    start = time.time()\n                    virtual = getattr(mod, virtual_func)()\n                    if isinstance(virtual, tuple):\n                        error_reason = virtual[1]\n                        virtual = virtual[0]\n                    if self.opts.get('virtual_timer', False):\n                        end = time.time() - start\n                        msg = 'Virtual function took {0} seconds for {1}'.format(\n                                end, module_name)\n                        log.warning(msg)\n                except Exception as exc:\n                    error_reason = (\n                        'Exception raised when processing __virtual__ function'\n                        ' for {0}. Module will not be loaded: {1}'.format(\n                            mod.__name__, exc))\n                    log.error(error_reason, exc_info_on_loglevel=logging.DEBUG)\n                    virtual = None\n                # Get the module's virtual name\n                virtualname = getattr(mod, '__virtualname__', virtual)\n                if not virtual:\n                    # if __virtual__() evaluates to False then the module\n                    # wasn't meant for this platform or it's not supposed to\n                    # load for some other reason.\n\n                    # Some modules might accidentally return None and are\n                    # improperly loaded\n                    if virtual is None:\n                        log.warning(\n                            '%s.__virtual__() is wrongly returning `None`. '\n                            'It should either return `True`, `False` or a new '\n                            'name. If you\\'re the developer of the module '\n                            '\\'%s\\', please fix this.', mod.__name__, module_name\n                        )\n\n                    return (False, module_name, error_reason, virtual_aliases)\n\n                # At this point, __virtual__ did not return a\n                # boolean value, let's check for deprecated usage\n                # or module renames\n                if virtual is not True and module_name != virtual:\n                    # The module is renaming itself. Updating the module name\n                    # with the new name\n                    log.trace('Loaded %s as virtual %s', module_name, virtual)\n\n                    if virtualname != virtual:\n                        # The __virtualname__ attribute does not match what's\n                        # being returned by the __virtual__() function. This\n                        # should be considered an error.\n                        log.error(\n                            'The module \\'%s\\' is showing some bad usage. Its '\n                            '__virtualname__ attribute is set to \\'%s\\' yet the '\n                            '__virtual__() function is returning \\'%s\\'. These '\n                            'values should match!',\n                            mod.__name__, virtualname, virtual\n                        )\n\n                    module_name = virtualname\n\n                # If the __virtual__ function returns True and __virtualname__\n                # is set then use it\n                elif virtual is True and virtualname != module_name:\n                    if virtualname is not True:\n                        module_name = virtualname\n\n        except KeyError:\n            # Key errors come out of the virtual function when passing\n            # in incomplete grains sets, these can be safely ignored\n            # and logged to debug, still, it includes the traceback to\n            # help debugging.\n            log.debug('KeyError when loading %s', module_name, exc_info=True)\n\n        except Exception:\n            # If the module throws an exception during __virtual__()\n            # then log the information and continue to the next.\n            log.error(\n                'Failed to read the virtual function for %s: %s',\n                self.tag, module_name, exc_info=True\n            )\n            return (False, module_name, error_reason, virtual_aliases)\n\n        return (True, module_name, None, virtual_aliases)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget an individual DMI string from SMBIOS info", "response": "def get(string, clean=True):\n    '''\n    Get an individual DMI string from SMBIOS info\n\n    string\n        The string to fetch. DMIdecode supports:\n          - ``bios-vendor``\n          - ``bios-version``\n          - ``bios-release-date``\n          - ``system-manufacturer``\n          - ``system-product-name``\n          - ``system-version``\n          - ``system-serial-number``\n          - ``system-uuid``\n          - ``baseboard-manufacturer``\n          - ``baseboard-product-name``\n          - ``baseboard-version``\n          - ``baseboard-serial-number``\n          - ``baseboard-asset-tag``\n          - ``chassis-manufacturer``\n          - ``chassis-type``\n          - ``chassis-version``\n          - ``chassis-serial-number``\n          - ``chassis-asset-tag``\n          - ``processor-family``\n          - ``processor-manufacturer``\n          - ``processor-version``\n          - ``processor-frequency``\n\n    clean\n      | Don't return well-known false information\n      | (invalid UUID's, serial 000000000's, etcetera)\n      | Defaults to ``True``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' smbios.get system-uuid clean=False\n    '''\n\n    val = _dmidecoder('-s {0}'.format(string)).strip()\n\n    # Cleanup possible comments in strings.\n    val = '\\n'.join([v for v in val.split('\\n') if not v.startswith('#')])\n    if val.startswith('/dev/mem') or clean and not _dmi_isclean(string, val):\n        val = None\n\n    return val"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns only SMBIOS records of the given type.", "response": "def records(rec_type=None, fields=None, clean=True):\n    '''\n    Return DMI records from SMBIOS\n\n    type\n        Return only records of type(s)\n        The SMBIOS specification defines the following DMI types:\n\n        ====  ======================================\n        Type  Information\n        ====  ======================================\n         0    BIOS\n         1    System\n         2    Baseboard\n         3    Chassis\n         4    Processor\n         5    Memory Controller\n         6    Memory Module\n         7    Cache\n         8    Port Connector\n         9    System Slots\n        10    On Board Devices\n        11    OEM Strings\n        12    System Configuration Options\n        13    BIOS Language\n        14    Group Associations\n        15    System Event Log\n        16    Physical Memory Array\n        17    Memory Device\n        18    32-bit Memory Error\n        19    Memory Array Mapped Address\n        20    Memory Device Mapped Address\n        21    Built-in Pointing Device\n        22    Portable Battery\n        23    System Reset\n        24    Hardware Security\n        25    System Power Controls\n        26    Voltage Probe\n        27    Cooling Device\n        28    Temperature Probe\n        29    Electrical Current Probe\n        30    Out-of-band Remote Access\n        31    Boot Integrity Services\n        32    System Boot\n        33    64-bit Memory Error\n        34    Management Device\n        35    Management Device Component\n        36    Management Device Threshold Data\n        37    Memory Channel\n        38    IPMI Device\n        39    Power Supply\n        40    Additional Information\n        41    Onboard Devices Extended Information\n        42    Management Controller Host Interface\n        ====  ======================================\n\n    clean\n      | Don't return well-known false information\n      | (invalid UUID's, serial 000000000's, etcetera)\n      | Defaults to ``True``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' smbios.records clean=False\n        salt '*' smbios.records 14\n        salt '*' smbios.records 4 core_count,thread_count,current_speed\n\n    '''\n    if rec_type is None:\n        smbios = _dmi_parse(_dmidecoder(), clean, fields)\n    else:\n        smbios = _dmi_parse(_dmidecoder('-t {0}'.format(rec_type)), clean, fields)\n\n    return smbios"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _dmi_parse(data, clean=True, fields=None):\n    '''\n    Structurize DMI records into a nice list\n    Optionally trash bogus entries and filter output\n    '''\n    dmi = []\n\n    # Detect & split Handle records\n    dmi_split = re.compile('(handle [0-9]x[0-9a-f]+[^\\n]+)\\n', re.MULTILINE+re.IGNORECASE)\n    dmi_raw = iter(re.split(dmi_split, data)[1:])\n    for handle, dmi_raw in zip(dmi_raw, dmi_raw):\n        handle, htype = [hline.split()[-1] for hline in handle.split(',')][0:2]\n        dmi_raw = dmi_raw.split('\\n')\n        # log.debug('%s record contains %s', handle, dmi_raw)\n        log.debug('Parsing handle %s', handle)\n\n        # The first line of a handle is a description of the type\n        record = {\n            'handle':      handle,\n            'description': dmi_raw.pop(0).strip(),\n            'type':        int(htype)\n        }\n\n        if not dmi_raw:\n            # empty record\n            if not clean:\n                dmi.append(record)\n            continue\n\n        # log.debug('%s record contains %s', record, dmi_raw)\n        dmi_data = _dmi_data(dmi_raw, clean, fields)\n        if dmi_data:\n            record['data'] = dmi_data\n            dmi.append(record)\n        elif not clean:\n            dmi.append(record)\n\n    return dmi", "response": "Parse DMI records into a nice list of dicts."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _dmi_data(dmi_raw, clean, fields):\n    '''\n    Parse the raw DMIdecode output of a single handle\n    into a nice dict\n    '''\n    dmi_data = {}\n\n    key = None\n    key_data = [None, []]\n    for line in dmi_raw:\n        if re.match(r'\\t[^\\s]+', line):\n            # Finish previous key\n            if key is not None:\n                # log.debug('Evaluating DMI key {0}: {1}'.format(key, key_data))\n                value, vlist = key_data\n                if vlist:\n                    if value is not None:\n                        # On the rare occasion\n                        # (I counted 1 on all systems we have)\n                        # that there's both a value <and> a list\n                        # just insert the value on top of the list\n                        vlist.insert(0, value)\n                    dmi_data[key] = vlist\n                elif value is not None:\n                    dmi_data[key] = value\n\n            # Family: Core i5\n            # Keyboard Password Status: Not Implemented\n            key, val = line.split(':', 1)\n            key = key.strip().lower().replace(' ', '_')\n            if (clean and key == 'header_and_data') \\\n                    or (fields and key not in fields):\n                key = None\n                continue\n            else:\n                key_data = [_dmi_cast(key, val.strip(), clean), []]\n        elif key is None:\n            continue\n        elif re.match(r'\\t\\t[^\\s]+', line):\n            # Installable Languages: 1\n            #        en-US\n            # Characteristics:\n            #        PCI is supported\n            #        PNP is supported\n            val = _dmi_cast(key, line.strip(), clean)\n            if val is not None:\n                # log.debug('DMI key %s gained list item %s', key, val)\n                key_data[1].append(val)\n\n    return dmi_data", "response": "Parse the DMI output of a single handle\n            into a nice dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _dmi_cast(key, val, clean=True):\n    '''\n    Simple caster thingy for trying to fish out at least ints & lists from strings\n    '''\n    if clean and not _dmi_isclean(key, val):\n        return\n    elif not re.match(r'serial|part|asset|product', key, flags=re.IGNORECASE):\n        if ',' in val:\n            val = [el.strip() for el in val.split(',')]\n        else:\n            try:\n                val = int(val)\n            except Exception:\n                pass\n\n    return val", "response": "Simple caster thingy for trying to fish out at least ints & lists from strings\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef construct_mapping(self, node, deep=False):\n        '''\n        Build the mapping for YAML\n        '''\n        if not isinstance(node, MappingNode):\n            raise ConstructorError(\n                None,\n                None,\n                'expected a mapping node, but found {0}'.format(node.id),\n                node.start_mark)\n\n        self.flatten_mapping(node)\n\n        context = 'while constructing a mapping'\n        mapping = self.dictclass()\n        for key_node, value_node in node.value:\n            key = self.construct_object(key_node, deep=deep)\n            try:\n                hash(key)\n            except TypeError:\n                raise ConstructorError(\n                    context,\n                    node.start_mark,\n                    \"found unacceptable key {0}\".format(key_node.value),\n                    key_node.start_mark)\n            value = self.construct_object(value_node, deep=deep)\n            if key in mapping:\n                raise ConstructorError(\n                    context,\n                    node.start_mark,\n                    \"found conflicting ID '{0}'\".format(key),\n                    key_node.start_mark)\n            mapping[key] = value\n        return mapping", "response": "Builds the mapping for YAML"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef construct_scalar(self, node):\n        '''\n        Verify integers and pass them in correctly is they are declared\n        as octal\n        '''\n        if node.tag == 'tag:yaml.org,2002:int':\n            if node.value == '0':\n                pass\n            elif node.value.startswith('0') and not node.value.startswith(('0b', '0x')):\n                node.value = node.value.lstrip('0')\n                # If value was all zeros, node.value would have been reduced to\n                # an empty string. Change it to '0'.\n                if node.value == '':\n                    node.value = '0'\n        return super(SaltYamlSafeLoader, self).construct_scalar(node)", "response": "Verify integers and pass them in correctly is they are declared\n        as octal\n           "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ip_address(address):\n    try:\n        return IPv4Address(address)\n    except (AddressValueError, NetmaskValueError):\n        pass\n\n    try:\n        return IPv6Address(address)\n    except (AddressValueError, NetmaskValueError):\n        pass\n\n    raise ValueError('%r does not appear to be an IPv4 or IPv6 address' %\n                     address)", "response": "Take an IP string or integer and return an object of the correct type."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ip_network(address, strict=True):\n    try:\n        return IPv4Network(address, strict)\n    except (AddressValueError, NetmaskValueError):\n        pass\n\n    try:\n        return IPv6Network(address, strict)\n    except (AddressValueError, NetmaskValueError):\n        pass\n\n    raise ValueError('%r does not appear to be an IPv4 or IPv6 network' %\n                     address)", "response": "Take an IP string or integer and return an object of the correct type."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _split_optional_netmask(address):\n    addr = str(address).split('/')\n    if len(addr) > 2:\n        raise AddressValueError(\"Only one '/' permitted in %r\" % address)\n    return addr", "response": "Helper to split the netmask and raise AddressValueError if needed"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncounting the number of zero bits on the right hand side of the number.", "response": "def _count_righthand_zero_bits(number, bits):\n    \"\"\"Count the number of zero bits on the right hand side.\n\n    Args:\n        number: an integer.\n        bits: maximum number of bits to count.\n\n    Returns:\n        The number of zero bits on the right hand side of the number.\n\n    \"\"\"\n    if number == 0:\n        return bits\n    for i in range(bits):\n        if (number >> i) & 1:\n            return i\n    # All bits of interest were zero, even if there are more in the number\n    return bits"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nloops through the addresses, collapsing concurrent netblocks. Example: ip1 = IPv4Network('192.0.2.0/26') ip2 = IPv4Network('192.0.2.64/26') ip3 = IPv4Network('192.0.2.128/26') ip4 = IPv4Network('192.0.2.192/26') _collapse_addresses_recursive([ip1, ip2, ip3, ip4]) -> [IPv4Network('192.0.2.0/24')] This shouldn't be called directly; it is called via collapse_addresses([]). Args: addresses: A list of IPv4Network's or IPv6Network's Returns: A list of IPv4Network's or IPv6Network's depending on what we were passed.", "response": "def _collapse_addresses_recursive(addresses):\n    \"\"\"Loops through the addresses, collapsing concurrent netblocks.\n\n    Example:\n\n        ip1 = IPv4Network('192.0.2.0/26')\n        ip2 = IPv4Network('192.0.2.64/26')\n        ip3 = IPv4Network('192.0.2.128/26')\n        ip4 = IPv4Network('192.0.2.192/26')\n\n        _collapse_addresses_recursive([ip1, ip2, ip3, ip4]) ->\n          [IPv4Network('192.0.2.0/24')]\n\n        This shouldn't be called directly; it is called via\n          collapse_addresses([]).\n\n    Args:\n        addresses: A list of IPv4Network's or IPv6Network's\n\n    Returns:\n        A list of IPv4Network's or IPv6Network's depending on what we were\n        passed.\n\n    \"\"\"\n    while True:\n        last_addr = None\n        ret_array = []\n        optimized = False\n\n        for cur_addr in addresses:\n            if not ret_array:\n                last_addr = cur_addr\n                ret_array.append(cur_addr)\n            elif (cur_addr.network_address >= last_addr.network_address and\n                cur_addr.broadcast_address <= last_addr.broadcast_address):\n                optimized = True\n            elif cur_addr == list(last_addr.supernet().subnets())[1]:\n                ret_array[-1] = last_addr = last_addr.supernet()\n                optimized = True\n            else:\n                last_addr = cur_addr\n                ret_array.append(cur_addr)\n\n        addresses = ret_array\n        if not optimized:\n            return addresses"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncollapsing a list of IPv4Network or IPv6Network objects.", "response": "def collapse_addresses(addresses):\n    \"\"\"Collapse a list of IP objects.\n\n    Example:\n        collapse_addresses([IPv4Network('192.0.2.0/25'),\n                            IPv4Network('192.0.2.128/25')]) ->\n                           [IPv4Network('192.0.2.0/24')]\n\n    Args:\n        addresses: An iterator of IPv4Network or IPv6Network objects.\n\n    Returns:\n        An iterator of the collapsed IPv(4|6)Network objects.\n\n    Raises:\n        TypeError: If passed a list of mixed version objects.\n\n    \"\"\"\n    i = 0\n    addrs = []\n    ips = []\n    nets = []\n\n    # split IP addresses and networks\n    for ip in addresses:\n        if isinstance(ip, _BaseAddress):\n            if ips and ips[-1]._version != ip._version:\n                raise TypeError(\"%s and %s are not of the same version\" % (\n                                 ip, ips[-1]))\n            ips.append(ip)\n        elif ip._prefixlen == ip._max_prefixlen:\n            if ips and ips[-1]._version != ip._version:\n                raise TypeError(\"%s and %s are not of the same version\" % (\n                                 ip, ips[-1]))\n            try:\n                ips.append(ip.ip)\n            except AttributeError:\n                ips.append(ip.network_address)\n        else:\n            if nets and nets[-1]._version != ip._version:\n                raise TypeError(\"%s and %s are not of the same version\" % (\n                                 ip, nets[-1]))\n            nets.append(ip)\n\n    # sort and dedup\n    ips = sorted(set(ips))\n    nets = sorted(set(nets))\n\n    while i < len(ips):\n        (first, last) = _find_address_range(ips[i:])\n        i = ips.index(last) + 1\n        addrs.extend(summarize_address_range(first, last))\n\n    return iter(_collapse_addresses_recursive(sorted(\n        addrs + nets, key=_BaseNetwork._get_networks_key)))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the prefix length from the bitwise netmask.", "response": "def _prefix_from_ip_int(self, ip_int):\n        \"\"\"Return prefix length from the bitwise netmask.\n\n        Args:\n            ip_int: An integer, the netmask in expanded bitwise format\n\n        Returns:\n            An integer, the prefix length.\n\n        Raises:\n            ValueError: If the input intermingles zeroes & ones\n        \"\"\"\n        trailing_zeroes = _count_righthand_zero_bits(ip_int,\n                                                     self._max_prefixlen)\n        prefixlen = self._max_prefixlen - trailing_zeroes\n        leading_ones = ip_int >> trailing_zeroes\n        all_ones = (1 << prefixlen) - 1\n        if leading_ones != all_ones:\n            byteslen = self._max_prefixlen // 8\n            details = _int_to_bytes(ip_int, byteslen, 'big')\n            msg = 'Netmask pattern %r mixes zeroes & ones'\n            raise ValueError(msg % details)\n        return prefixlen"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _prefix_from_prefix_string(self, prefixlen_str):\n        # int allows a leading +/- as well as surrounding whitespace,\n        # so we ensure that isn't the case\n        if not _BaseV4._DECIMAL_DIGITS.issuperset(prefixlen_str):\n            self._report_invalid_netmask(prefixlen_str)\n        try:\n            prefixlen = int(prefixlen_str)\n        except ValueError:\n            self._report_invalid_netmask(prefixlen_str)\n        if not (0 <= prefixlen <= self._max_prefixlen):\n            self._report_invalid_netmask(prefixlen_str)\n        return prefixlen", "response": "Convert a numeric string to a prefix length."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _prefix_from_ip_string(self, ip_str):\n        # Parse the netmask/hostmask like an IP address.\n        try:\n            ip_int = self._ip_int_from_string(ip_str)\n        except AddressValueError:\n            self._report_invalid_netmask(ip_str)\n\n        # Try matching a netmask (this would be /1*0*/ as a bitwise regexp).\n        # Note that the two ambiguous cases (all-ones and all-zeroes) are\n        # treated as netmasks.\n        try:\n            return self._prefix_from_ip_int(ip_int)\n        except ValueError:\n            pass\n\n        # Invert the bits, and try matching a /0+1+/ hostmask instead.\n        ip_int ^= self._ALL_ONES\n        try:\n            return self._prefix_from_ip_int(ip_int)\n        except ValueError:\n            self._report_invalid_netmask(ip_str)", "response": "Turn a netmask or hostmask string into a prefix length."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef supernet(self, prefixlen_diff=1, new_prefix=None):\n        if self._prefixlen == 0:\n            return self\n\n        if new_prefix is not None:\n            if new_prefix > self._prefixlen:\n                raise ValueError('new prefix must be shorter')\n            if prefixlen_diff != 1:\n                raise ValueError('cannot set prefixlen_diff and new_prefix')\n            prefixlen_diff = self._prefixlen - new_prefix\n\n        if self.prefixlen - prefixlen_diff < 0:\n            raise ValueError(\n                'current prefixlen is %d, cannot have a prefixlen_diff of %d' %\n                (self.prefixlen, prefixlen_diff))\n        # TODO (pmoody): optimize this.\n        t = self.__class__('%s/%d' % (self.network_address,\n                                      self.prefixlen - prefixlen_diff),\n                                     strict=False)\n        return t.__class__('%s/%d' % (t.network_address, t.prefixlen))", "response": "Returns a new IPv4 network object containing the current network."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nturn the given string into an integer for comparison.", "response": "def _ip_int_from_string(self, ip_str):\n        \"\"\"Turn the given IP string into an integer for comparison.\n\n        Args:\n            ip_str: A string, the IP ip_str.\n\n        Returns:\n            The IP ip_str as an integer.\n\n        Raises:\n            AddressValueError: if ip_str isn't a valid IPv4 Address.\n\n        \"\"\"\n        if not ip_str:\n            raise AddressValueError('Address cannot be empty')\n\n        octets = ip_str.split('.')\n        if len(octets) != 4:\n            raise AddressValueError(\"Expected 4 octets in %r\" % ip_str)\n\n        try:\n            return _int_from_bytes(map(self._parse_octet, octets), 'big')\n        except ValueError as exc:\n            raise AddressValueError(\"%s in %r\" % (exc, ip_str))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts a decimal octet into an integer.", "response": "def _parse_octet(self, octet_str):\n        \"\"\"Convert a decimal octet into an integer.\n\n        Args:\n            octet_str: A string, the number to parse.\n\n        Returns:\n            The octet as an integer.\n\n        Raises:\n            ValueError: if the octet isn't strictly a decimal from [0..255].\n\n        \"\"\"\n        if not octet_str:\n            raise ValueError(\"Empty octet not permitted\")\n        # Whitelist the characters, since int() allows a lot of bizarre stuff.\n        if not self._DECIMAL_DIGITS.issuperset(octet_str):\n            msg = \"Only decimal digits permitted in %r\"\n            raise ValueError(msg % octet_str)\n        # We do the length check second, since the invalid character error\n        # is likely to be more informative for the user\n        if len(octet_str) > 3:\n            msg = \"At most 3 characters permitted in %r\"\n            raise ValueError(msg % octet_str)\n        # Convert to integer (we know digits are legal)\n        octet_int = int(octet_str, 10)\n        # Any octets that look like they *might* be written in octal,\n        # and which don't look exactly the same in both octal and\n        # decimal are rejected as ambiguous\n        if octet_int > 7 and octet_str[0] == '0':\n            msg = \"Ambiguous (octal/decimal) value in %r not permitted\"\n            raise ValueError(msg % octet_str)\n        if octet_int > 255:\n            raise ValueError(\"Octet %d (> 255) not permitted\" % octet_int)\n        return octet_int"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nverifying that the netmask is a valid IPv4 or dotted decimal netmask.", "response": "def _is_valid_netmask(self, netmask):\n        \"\"\"Verify that the netmask is valid.\n\n        Args:\n            netmask: A string, either a prefix or dotted decimal\n              netmask.\n\n        Returns:\n            A boolean, True if the prefix represents a valid IPv4\n            netmask.\n\n        \"\"\"\n        mask = netmask.split('.')\n        if len(mask) == 4:\n            try:\n                for x in mask:\n                    if int(x) not in self._valid_mask_octets:\n                        return False\n            except ValueError:\n                # Found something that isn't an integer or isn't valid\n                return False\n            for idx, y in enumerate(mask):\n                if idx > 0 and y > mask[idx - 1]:\n                    return False\n            return True\n        try:\n            netmask = int(netmask)\n        except ValueError:\n            return False\n        return 0 <= netmask <= self._max_prefixlen"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_private(self):\n        return (self in IPv4Network('0.0.0.0/8') or\n                self in IPv4Network('10.0.0.0/8') or\n                self in IPv4Network('127.0.0.0/8') or\n                self in IPv4Network('169.254.0.0/16') or\n                self in IPv4Network('172.16.0.0/12') or\n                self in IPv4Network('192.0.0.0/29') or\n                self in IPv4Network('192.0.0.170/31') or\n                self in IPv4Network('192.0.2.0/24') or\n                self in IPv4Network('192.168.0.0/16') or\n                self in IPv4Network('198.18.0.0/15') or\n                self in IPv4Network('198.51.100.0/24') or\n                self in IPv4Network('203.0.113.0/24') or\n                self in IPv4Network('240.0.0.0/4') or\n                self in IPv4Network('255.255.255.255/32'))", "response": "Tests if this address is allocated for private networks."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nexpanding a shortened IPv6 address.", "response": "def _explode_shorthand_ip_string(self):\n        \"\"\"Expand a shortened IPv6 address.\n\n        Args:\n            ip_str: A string, the IPv6 address.\n\n        Returns:\n            A string, the expanded IPv6 address.\n\n        \"\"\"\n        if isinstance(self, IPv6Network):\n            ip_str = str(self.network_address)\n        elif isinstance(self, IPv6Interface):\n            ip_str = str(self.ip)\n        else:\n            ip_str = str(self)\n\n        ip_int = self._ip_int_from_string(ip_str)\n        hex_str = '%032x' % ip_int\n        parts = [hex_str[x:x+4] for x in range(0, 32, 4)]\n        if isinstance(self, (_BaseNetwork, IPv6Interface)):\n            return '%s/%d' % (':'.join(parts), self._prefixlen)\n        return ':'.join(parts)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntest if the address is otherwise IETF reserved.", "response": "def is_reserved(self):\n        \"\"\"Test if the address is otherwise IETF reserved.\n\n        Returns:\n            A boolean, True if the address is within one of the\n            reserved IPv6 Network ranges.\n\n        \"\"\"\n        reserved_networks = [IPv6Network('::/8'), IPv6Network('100::/8'),\n                             IPv6Network('200::/7'), IPv6Network('400::/6'),\n                             IPv6Network('800::/5'), IPv6Network('1000::/4'),\n                             IPv6Network('4000::/3'), IPv6Network('6000::/3'),\n                             IPv6Network('8000::/3'), IPv6Network('A000::/3'),\n                             IPv6Network('C000::/3'), IPv6Network('E000::/4'),\n                             IPv6Network('F000::/5'), IPv6Network('F800::/6'),\n                             IPv6Network('FE00::/9')]\n\n        return any(self in x for x in reserved_networks)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_private(self):\n        return (self in IPv6Network('::1/128') or\n                self in IPv6Network('::/128') or\n                self in IPv6Network('::ffff:0:0/96') or\n                self in IPv6Network('100::/64') or\n                self in IPv6Network('2001::/23') or\n                self in IPv6Network('2001:2::/48') or\n                self in IPv6Network('2001:db8::/32') or\n                self in IPv6Network('2001:10::/28') or\n                self in IPv6Network('fc00::/7') or\n                self in IPv6Network('fe80::/10'))", "response": "Test if this address is allocated for private networks."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating an iterator over usable hosts in a network.", "response": "def hosts(self):\n        \"\"\"Generate Iterator over usable hosts in a network.\n\n          This is like __iter__ except it doesn't return the\n          Subnet-Router anycast address.\n\n        \"\"\"\n        network = int(self.network_address)\n        broadcast = int(self.broadcast_address)\n        for x in long_range(1, broadcast - network + 1):\n            yield self._address_class(network + x)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert snetinfo object to list", "response": "def _to_list(obj):\n    '''\n    Convert snetinfo object to list\n    '''\n    ret = {}\n\n    for attr in __attrs:\n        if hasattr(obj, attr):\n            ret[attr] = getattr(obj, attr)\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef validate(config):\n    '''\n    Validate the beacon configuration\n    '''\n\n    VALID_ITEMS = [\n        'type', 'bytes_sent', 'bytes_recv', 'packets_sent',\n        'packets_recv', 'errin', 'errout', 'dropin',\n        'dropout'\n    ]\n\n    # Configuration for load beacon should be a list of dicts\n    if not isinstance(config, list):\n        return False, ('Configuration for network_info beacon must be a list.')\n    else:\n\n        _config = {}\n        list(map(_config.update, config))\n\n        for item in _config.get('interfaces', {}):\n            if not isinstance(_config['interfaces'][item], dict):\n                return False, ('Configuration for network_info beacon must '\n                               'be a list of dictionaries.')\n            else:\n                if not any(j in VALID_ITEMS for j in _config['interfaces'][item]):\n                    return False, ('Invalid configuration item in '\n                                   'Beacon configuration.')\n    return True, 'Valid beacon configuration'", "response": "Validate the beacon configuration for load beacon"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef beacon(config):\n    '''\n    Emit the network statistics of this host.\n\n    Specify thresholds for each network stat\n    and only emit a beacon if any of them are\n    exceeded.\n\n    Emit beacon when any values are equal to\n    configured values.\n\n    .. code-block:: yaml\n\n        beacons:\n          network_info:\n            - interfaces:\n                eth0:\n                  type: equal\n                  bytes_sent: 100000\n                  bytes_recv: 100000\n                  packets_sent: 100000\n                  packets_recv: 100000\n                  errin: 100\n                  errout: 100\n                  dropin: 100\n                  dropout: 100\n\n    Emit beacon when any values are greater\n    than configured values.\n\n    .. code-block:: yaml\n\n        beacons:\n          network_info:\n            - interfaces:\n                eth0:\n                  type: greater\n                  bytes_sent: 100000\n                  bytes_recv: 100000\n                  packets_sent: 100000\n                  packets_recv: 100000\n                  errin: 100\n                  errout: 100\n                  dropin: 100\n                  dropout: 100\n\n\n    '''\n    ret = []\n\n    _config = {}\n    list(map(_config.update, config))\n\n    log.debug('psutil.net_io_counters %s', psutil.net_io_counters)\n\n    _stats = psutil.net_io_counters(pernic=True)\n\n    log.debug('_stats %s', _stats)\n    for interface in _config.get('interfaces', {}):\n        if interface in _stats:\n            interface_config = _config['interfaces'][interface]\n            _if_stats = _stats[interface]\n            _diff = False\n            for attr in __attrs:\n                if attr in interface_config:\n                    if 'type' in interface_config and \\\n                            interface_config['type'] == 'equal':\n                        if getattr(_if_stats, attr, None) == \\\n                                int(interface_config[attr]):\n                            _diff = True\n                    elif 'type' in interface_config and \\\n                            interface_config['type'] == 'greater':\n                        if getattr(_if_stats, attr, None) > \\\n                                int(interface_config[attr]):\n                            _diff = True\n                        else:\n                            log.debug('attr %s', getattr(_if_stats,\n                                                         attr, None))\n                    else:\n                        if getattr(_if_stats, attr, None) == \\\n                                int(interface_config[attr]):\n                            _diff = True\n            if _diff:\n                ret.append({'interface': interface,\n                            'network_info': _to_list(_if_stats)})\n    return ret", "response": "Emit the network statistics of this host."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn apcaccess output CLI Example: .. code-block:: bash salt '*' apcups.status", "response": "def status():\n    '''\n    Return apcaccess output\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' apcups.status\n    '''\n    ret = {}\n    apcaccess = _check_apcaccess()\n    res = __salt__['cmd.run_all'](apcaccess)\n    retcode = res['retcode']\n    if retcode != 0:\n        ret['Error'] = 'Something with wrong executing apcaccess, is apcupsd running?'\n        return ret\n\n    for line in res['stdout'].splitlines():\n        line = line.split(':')\n        ret[line[0].strip()] = line[1].strip()\n\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns load of the CLI Example", "response": "def status_load():\n    '''\n    Return load\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' apcups.status_load\n    '''\n    data = status()\n    if 'LOADPCT' in data:\n        load = data['LOADPCT'].split()\n        if load[1].lower() == 'percent':\n            return float(load[0])\n\n    return {'Error': 'Load not available.'}"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef coredump_configured(name, enabled, dump_ip, host_vnic='vmk0', dump_port=6500):\n    '''\n    Ensures a host's core dump configuration.\n\n    name\n        Name of the state.\n\n    enabled\n        Sets whether or not ESXi core dump collection should be enabled.\n        This is a boolean value set to ``True`` or ``False`` to enable\n        or disable core dumps.\n\n        Note that ESXi requires that the core dump must be enabled before\n        any other parameters may be set. This also affects the ``changes``\n        results in the state return dictionary. If ``enabled`` is ``False``,\n        we can't obtain any previous settings to compare other state variables,\n        resulting in many ``old`` references returning ``None``.\n\n        Once ``enabled`` is ``True`` the ``changes`` dictionary comparisons\n        will be more accurate. This is due to the way the system coredemp\n        network configuration command returns data.\n\n    dump_ip\n        The IP address of host that will accept the dump.\n\n    host_vnic\n        Host VNic port through which to communicate. Defaults to ``vmk0``.\n\n    dump_port\n        TCP port to use for the dump. Defaults to ``6500``.\n\n    Example:\n\n    .. code-block:: yaml\n\n        configure-host-coredump:\n          esxi.coredump_configured:\n            - enabled: True\n            - dump_ip: 'my-coredump-ip.example.com'\n\n    '''\n    ret = {'name': name,\n           'result': False,\n           'changes': {},\n           'comment': ''}\n    esxi_cmd = 'esxi.cmd'\n    enabled_msg = 'ESXi requires that the core dump must be enabled ' \\\n                  'before any other parameters may be set.'\n    host = __pillar__['proxy']['host']\n\n    current_config = __salt__[esxi_cmd]('get_coredump_network_config').get(host)\n    error = current_config.get('Error')\n    if error:\n        ret['comment'] = 'Error: {0}'.format(error)\n        return ret\n\n    current_config = current_config.get('Coredump Config')\n    current_enabled = current_config.get('enabled')\n\n    # Configure coredump enabled state, if there are changes.\n    if current_enabled != enabled:\n        enabled_changes = {'enabled': {'old': current_enabled, 'new': enabled}}\n        # Only run the command if not using test=True\n        if not __opts__['test']:\n            response = __salt__[esxi_cmd]('coredump_network_enable',\n                                          enabled=enabled).get(host)\n            error = response.get('Error')\n            if error:\n                ret['comment'] = 'Error: {0}'.format(error)\n                return ret\n\n            # Allow users to disable core dump, but then return since\n            # nothing else can be set if core dump is disabled.\n            if not enabled:\n                ret['result'] = True\n                ret['comment'] = enabled_msg\n                ret['changes'].update(enabled_changes)\n                return ret\n\n        ret['changes'].update(enabled_changes)\n\n    elif not enabled:\n        # If current_enabled and enabled match, but are both False,\n        # We must return before configuring anything. This isn't a\n        # failure as core dump may be disabled intentionally.\n        ret['result'] = True\n        ret['comment'] = enabled_msg\n        return ret\n\n    # Test for changes with all remaining configurations. The changes flag is used\n    # To detect changes, and then set_coredump_network_config is called one time.\n    changes = False\n    current_ip = current_config.get('ip')\n    if current_ip != dump_ip:\n        ret['changes'].update({'dump_ip':\n                              {'old': current_ip,\n                               'new': dump_ip}})\n        changes = True\n\n    current_vnic = current_config.get('host_vnic')\n    if current_vnic != host_vnic:\n        ret['changes'].update({'host_vnic':\n                              {'old': current_vnic,\n                               'new': host_vnic}})\n        changes = True\n\n    current_port = current_config.get('port')\n    if current_port != six.text_type(dump_port):\n        ret['changes'].update({'dump_port':\n                              {'old': current_port,\n                               'new': six.text_type(dump_port)}})\n        changes = True\n\n    # Only run the command if not using test=True and changes were detected.\n    if not __opts__['test'] and changes is True:\n        response = __salt__[esxi_cmd]('set_coredump_network_config',\n                                      dump_ip=dump_ip,\n                                      host_vnic=host_vnic,\n                                      dump_port=dump_port).get(host)\n        if response.get('success') is False:\n            msg = response.get('stderr')\n            if not msg:\n                msg = response.get('stdout')\n            ret['comment'] = 'Error: {0}'.format(msg)\n            return ret\n\n    ret['result'] = True\n    if ret['changes'] == {}:\n        ret['comment'] = 'Core Dump configuration is already in the desired state.'\n        return ret\n\n    if __opts__['test']:\n        ret['result'] = None\n        ret['comment'] = 'Core dump configuration will change.'\n\n    return ret", "response": "Ensures that the core dump collection is configured for the given host."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nensure that the given password is set on the state.", "response": "def password_present(name, password):\n    '''\n    Ensures the given password is set on the ESXi host. Passwords cannot be obtained from\n    host, so if a password is set in this state, the ``vsphere.update_host_password``\n    function will always run (except when using test=True functionality) and the state's\n    changes dictionary will always be populated.\n\n    The username for which the password will change is the same username that is used to\n    authenticate against the ESXi host via the Proxy Minion. For example, if the pillar\n    definition for the proxy username is defined as ``root``, then the username that the\n    password will be updated for via this state is ``root``.\n\n    name\n        Name of the state.\n\n    password\n        The new password to change on the host.\n\n    Example:\n\n    .. code-block:: yaml\n\n        configure-host-password:\n          esxi.password_present:\n            - password: 'new-bad-password'\n    '''\n    ret = {'name': name,\n           'result': True,\n           'changes': {'old': 'unknown',\n                       'new': '********'},\n           'comment': 'Host password was updated.'}\n    esxi_cmd = 'esxi.cmd'\n\n    if __opts__['test']:\n        ret['result'] = None\n        ret['comment'] = 'Host password will change.'\n        return ret\n    else:\n        try:\n            __salt__[esxi_cmd]('update_host_password',\n                               new_password=password)\n        except CommandExecutionError as err:\n            ret['result'] = False\n            ret['comment'] = 'Error: {0}'.format(err)\n            return ret\n\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconfigures the NTP server configuration for a given host.", "response": "def ntp_configured(name,\n                   service_running,\n                   ntp_servers=None,\n                   service_policy=None,\n                   service_restart=False,\n                   update_datetime=False):\n    '''\n    Ensures a host's NTP server configuration such as setting NTP servers, ensuring the\n    NTP daemon is running or stopped, or restarting the NTP daemon for the ESXi host.\n\n    name\n        Name of the state.\n\n    service_running\n        Ensures the running state of the ntp daemon for the host. Boolean value where\n        ``True`` indicates that ntpd should be running and ``False`` indicates that it\n        should be stopped.\n\n    ntp_servers\n        A list of servers that should be added to the ESXi host's NTP configuration.\n\n    service_policy\n        The policy to set for the NTP service.\n\n        .. note::\n\n            When setting the service policy to ``off`` or ``on``, you *must* quote the\n            setting. If you don't, the yaml parser will set the string to a boolean,\n            which will cause trouble checking for stateful changes and will error when\n            trying to set the policy on the ESXi host.\n\n\n    service_restart\n        If set to ``True``, the ntp daemon will be restarted, regardless of its previous\n        running state. Default is ``False``.\n\n    update_datetime\n        If set to ``True``, the date/time on the given host will be updated to UTC.\n        Default setting is ``False``. This option should be used with caution since\n        network delays and execution delays can result in time skews.\n\n    Example:\n\n    .. code-block:: yaml\n\n        configure-host-ntp:\n          esxi.ntp_configured:\n            - service_running: True\n            - ntp_servers:\n              - 192.174.1.100\n              - 192.174.1.200\n            - service_policy: 'on'\n            - service_restart: True\n\n    '''\n    ret = {'name': name,\n           'result': False,\n           'changes': {},\n           'comment': ''}\n    esxi_cmd = 'esxi.cmd'\n    host = __pillar__['proxy']['host']\n    ntpd = 'ntpd'\n\n    ntp_config = __salt__[esxi_cmd]('get_ntp_config').get(host)\n    ntp_running = __salt__[esxi_cmd]('get_service_running',\n                                     service_name=ntpd).get(host)\n    error = ntp_running.get('Error')\n    if error:\n        ret['comment'] = 'Error: {0}'.format(error)\n        return ret\n    ntp_running = ntp_running.get(ntpd)\n\n    # Configure NTP Servers for the Host\n    if ntp_servers and set(ntp_servers) != set(ntp_config):\n        # Only run the command if not using test=True\n        if not __opts__['test']:\n            response = __salt__[esxi_cmd]('set_ntp_config',\n                                          ntp_servers=ntp_servers).get(host)\n            error = response.get('Error')\n            if error:\n                ret['comment'] = 'Error: {0}'.format(error)\n                return ret\n        # Set changes dictionary for ntp_servers\n        ret['changes'].update({'ntp_servers':\n                              {'old': ntp_config,\n                               'new': ntp_servers}})\n\n    # Configure service_running state\n    if service_running != ntp_running:\n        # Only run the command if not using test=True\n        if not __opts__['test']:\n            # Start ntdp if service_running=True\n            if ntp_running is True:\n                response = __salt__[esxi_cmd]('service_start',\n                                              service_name=ntpd).get(host)\n                error = response.get('Error')\n                if error:\n                    ret['comment'] = 'Error: {0}'.format(error)\n                    return ret\n            # Stop ntpd if service_running=False\n            else:\n                response = __salt__[esxi_cmd]('service_stop',\n                                              service_name=ntpd).get(host)\n                error = response.get('Error')\n                if error:\n                    ret['comment'] = 'Error: {0}'.format(error)\n                    return ret\n        ret['changes'].update({'service_running':\n                              {'old': ntp_running,\n                               'new': service_running}})\n\n    # Configure service_policy\n    if service_policy:\n        current_service_policy = __salt__[esxi_cmd]('get_service_policy',\n                                                    service_name=ntpd).get(host)\n        error = current_service_policy.get('Error')\n        if error:\n            ret['comment'] = 'Error: {0}'.format(error)\n            return ret\n        current_service_policy = current_service_policy.get(ntpd)\n\n        if service_policy != current_service_policy:\n            # Only run the command if not using test=True\n            if not __opts__['test']:\n                response = __salt__[esxi_cmd]('set_service_policy',\n                                              service_name=ntpd,\n                                              service_policy=service_policy).get(host)\n                error = response.get('Error')\n                if error:\n                    ret['comment'] = 'Error: {0}'.format(error)\n                    return ret\n            ret['changes'].update({'service_policy':\n                                  {'old': current_service_policy,\n                                   'new': service_policy}})\n\n    # Update datetime, if requested.\n    if update_datetime:\n        # Only run the command if not using test=True\n        if not __opts__['test']:\n            response = __salt__[esxi_cmd]('update_host_datetime').get(host)\n            error = response.get('Error')\n            if error:\n                ret['comment'] = 'Error: {0}'.format(error)\n                return ret\n        ret['changes'].update({'update_datetime':\n                              {'old': '',\n                               'new': 'Host datetime was updated.'}})\n\n    # Restart ntp_service if service_restart=True\n    if service_restart:\n        # Only run the command if not using test=True\n        if not __opts__['test']:\n            response = __salt__[esxi_cmd]('service_restart',\n                                          service_name=ntpd).get(host)\n            error = response.get('Error')\n            if error:\n                ret['comment'] = 'Error: {0}'.format(error)\n                return ret\n        ret['changes'].update({'service_restart':\n                              {'old': '',\n                               'new': 'NTP Daemon Restarted.'}})\n\n    ret['result'] = True\n    if ret['changes'] == {}:\n        ret['comment'] = 'NTP is already in the desired state.'\n        return ret\n\n    if __opts__['test']:\n        ret['result'] = None\n        ret['comment'] = 'NTP state will change.'\n\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef vmotion_configured(name, enabled, device='vmk0'):\n    '''\n    Configures a host's VMotion properties such as enabling VMotion and setting\n    the device VirtualNic that VMotion will use.\n\n    name\n        Name of the state.\n\n    enabled\n        Ensures whether or not VMotion should be enabled on a host as a boolean\n        value where ``True`` indicates that VMotion should be enabled and ``False``\n        indicates that VMotion should be disabled.\n\n    device\n        The device that uniquely identifies the VirtualNic that will be used for\n        VMotion for the host. Defaults to ``vmk0``.\n\n    Example:\n\n    .. code-block:: yaml\n\n        configure-vmotion:\n          esxi.vmotion_configured:\n            - enabled: True\n            - device: sample-device\n\n    '''\n    ret = {'name': name,\n           'result': False,\n           'changes': {},\n           'comment': ''}\n    esxi_cmd = 'esxi.cmd'\n    host = __pillar__['proxy']['host']\n\n    current_vmotion_enabled = __salt__[esxi_cmd]('get_vmotion_enabled').get(host)\n    current_vmotion_enabled = current_vmotion_enabled.get('VMotion Enabled')\n\n    # Configure VMotion Enabled state, if changed.\n    if enabled != current_vmotion_enabled:\n        # Only run the command if not using test=True\n        if not __opts__['test']:\n            # Enable VMotion if enabled=True\n            if enabled is True:\n                response = __salt__[esxi_cmd]('vmotion_enable',\n                                              device=device).get(host)\n                error = response.get('Error')\n                if error:\n                    ret['comment'] = 'Error: {0}'.format(error)\n                    return ret\n            # Disable VMotion if enabled=False\n            else:\n                response = __salt__[esxi_cmd]('vmotion_disable').get(host)\n                error = response.get('Error')\n                if error:\n                    ret['comment'] = 'Error: {0}'.format(error)\n                    return ret\n        ret['changes'].update({'enabled':\n                              {'old': current_vmotion_enabled,\n                               'new': enabled}})\n\n    ret['result'] = True\n    if ret['changes'] == {}:\n        ret['comment'] = 'VMotion configuration is already in the desired state.'\n        return ret\n\n    if __opts__['test']:\n        ret['result'] = None\n        ret['comment'] = 'VMotion configuration will change.'\n\n    return ret", "response": "Configure a host s VMotion properties such as enabling VMotion and setting the VirtualNic that VMotion will use."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef vsan_configured(name, enabled, add_disks_to_vsan=False):\n    '''\n    Configures a host's VSAN properties such as enabling or disabling VSAN, or\n    adding VSAN-eligible disks to the VSAN system for the host.\n\n    name\n        Name of the state.\n\n    enabled\n        Ensures whether or not VSAN should be enabled on a host as a boolean\n        value where ``True`` indicates that VSAN should be enabled and ``False``\n        indicates that VSAN should be disabled.\n\n    add_disks_to_vsan\n        If set to ``True``, any VSAN-eligible disks for the given host will be added\n        to the host's VSAN system. Default is ``False``.\n\n    Example:\n\n    .. code-block:: yaml\n\n        configure-host-vsan:\n          esxi.vsan_configured:\n            - enabled: True\n            - add_disks_to_vsan: True\n\n    '''\n    ret = {'name': name,\n           'result': False,\n           'changes': {},\n           'comment': ''}\n    esxi_cmd = 'esxi.cmd'\n    host = __pillar__['proxy']['host']\n\n    current_vsan_enabled = __salt__[esxi_cmd]('get_vsan_enabled').get(host)\n    error = current_vsan_enabled.get('Error')\n    if error:\n        ret['comment'] = 'Error: {0}'.format(error)\n        return ret\n    current_vsan_enabled = current_vsan_enabled.get('VSAN Enabled')\n\n    # Configure VSAN Enabled state, if changed.\n    if enabled != current_vsan_enabled:\n        # Only run the command if not using test=True\n        if not __opts__['test']:\n            # Enable VSAN if enabled=True\n            if enabled is True:\n                response = __salt__[esxi_cmd]('vsan_enable').get(host)\n                error = response.get('Error')\n                if error:\n                    ret['comment'] = 'Error: {0}'.format(error)\n                    return ret\n            # Disable VSAN if enabled=False\n            else:\n                response = __salt__[esxi_cmd]('vsan_disable').get(host)\n                error = response.get('Error')\n                if error:\n                    ret['comment'] = 'Error: {0}'.format(error)\n                    return ret\n        ret['changes'].update({'enabled':\n                              {'old': current_vsan_enabled,\n                               'new': enabled}})\n\n    # Add any eligible disks to VSAN, if requested.\n    if add_disks_to_vsan:\n        current_eligible_disks = __salt__[esxi_cmd]('get_vsan_eligible_disks').get(host)\n        error = current_eligible_disks.get('Error')\n        if error:\n            ret['comment'] = 'Error: {0}'.format(error)\n            return ret\n\n        disks = current_eligible_disks.get('Eligible')\n        if disks and isinstance(disks, list):\n            # Only run the command if not using test=True\n            if not __opts__['test']:\n                response = __salt__[esxi_cmd]('vsan_add_disks').get(host)\n                error = response.get('Error')\n                if error:\n                    ret['comment'] = 'Error: {0}'.format(error)\n                    return ret\n\n            ret['changes'].update({'add_disks_to_vsan':\n                                  {'old': '',\n                                   'new': disks}})\n\n    ret['result'] = True\n    if ret['changes'] == {}:\n        ret['comment'] = 'VSAN configuration is already in the desired state.'\n        return ret\n\n    if __opts__['test']:\n        ret['result'] = None\n        ret['comment'] = 'VSAN configuration will change.'\n\n    return ret", "response": "Configure a host s VSAN properties such as enabling or disabling VSAN and adding VSAN - eligible disks to VSAN system for the given host."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmanages the SSH configuration for a given host.", "response": "def ssh_configured(name,\n                   service_running,\n                   ssh_key=None,\n                   ssh_key_file=None,\n                   service_policy=None,\n                   service_restart=False,\n                   certificate_verify=False):\n    '''\n    Manage the SSH configuration for a host including whether or not SSH is running or\n    the presence of a given SSH key. Note: Only one ssh key can be uploaded for root.\n    Uploading a second key will replace any existing key.\n\n    name\n        Name of the state.\n\n    service_running\n        Ensures whether or not the SSH service should be running on a host. Represented\n        as a boolean value where ``True`` indicates that SSH should be running and\n        ``False`` indicates that SSH should stopped.\n\n        In order to update SSH keys, the SSH service must be running.\n\n    ssh_key\n        Public SSH key to added to the authorized_keys file on the ESXi host. You can\n        use ``ssh_key`` or ``ssh_key_file``, but not both.\n\n    ssh_key_file\n        File containing the public SSH key to be added to the authorized_keys file on\n        the ESXi host. You can use ``ssh_key_file`` or ``ssh_key``, but not both.\n\n    service_policy\n        The policy to set for the NTP service.\n\n        .. note::\n\n            When setting the service policy to ``off`` or ``on``, you *must* quote the\n            setting. If you don't, the yaml parser will set the string to a boolean,\n            which will cause trouble checking for stateful changes and will error when\n            trying to set the policy on the ESXi host.\n\n    service_restart\n        If set to ``True``, the SSH service will be restarted, regardless of its\n        previous running state. Default is ``False``.\n\n    certificate_verify\n        If set to ``True``, the SSL connection must present a valid certificate.\n        Default is ``False``.\n\n    Example:\n\n    .. code-block:: yaml\n\n        configure-host-ssh:\n          esxi.ssh_configured:\n            - service_running: True\n            - ssh_key_file: /etc/salt/ssh_keys/my_key.pub\n            - service_policy: 'on'\n            - service_restart: True\n            - certificate_verify: True\n\n    '''\n    ret = {'name': name,\n           'result': False,\n           'changes': {},\n           'comment': ''}\n    esxi_cmd = 'esxi.cmd'\n    host = __pillar__['proxy']['host']\n    ssh = 'ssh'\n\n    ssh_running = __salt__[esxi_cmd]('get_service_running',\n                                     service_name=ssh).get(host)\n    error = ssh_running.get('Error')\n    if error:\n        ret['comment'] = 'Error: {0}'.format(error)\n        return ret\n    ssh_running = ssh_running.get(ssh)\n\n    # Configure SSH service_running state, if changed.\n    if service_running != ssh_running:\n        # Only actually run the command if not using test=True\n        if not __opts__['test']:\n            # Start SSH if service_running=True\n            if service_running is True:\n                enable = __salt__[esxi_cmd]('service_start',\n                                            service_name=ssh).get(host)\n                error = enable.get('Error')\n                if error:\n                    ret['comment'] = 'Error: {0}'.format(error)\n                    return ret\n            # Disable SSH if service_running=False\n            else:\n                disable = __salt__[esxi_cmd]('service_stop',\n                                             service_name=ssh).get(host)\n                error = disable.get('Error')\n                if error:\n                    ret['comment'] = 'Error: {0}'.format(error)\n                    return ret\n\n        ret['changes'].update({'service_running':\n                              {'old': ssh_running,\n                               'new': service_running}})\n\n    # If uploading an SSH key or SSH key file, see if there's a current\n    # SSH key and compare the current key to the key set in the state.\n    current_ssh_key, ssh_key_changed = None, False\n    if ssh_key or ssh_key_file:\n        current_ssh_key = __salt__[esxi_cmd]('get_ssh_key',\n                                             certificate_verify=certificate_verify)\n        error = current_ssh_key.get('Error')\n        if error:\n            ret['comment'] = 'Error: {0}'.format(error)\n            return ret\n        current_ssh_key = current_ssh_key.get('key')\n        if current_ssh_key:\n            clean_current_key = _strip_key(current_ssh_key).split(' ')\n            if not ssh_key:\n                ssh_key = ''\n                # Open ssh key file and read in contents to create one key string\n                with salt.utils.files.fopen(ssh_key_file, 'r') as key_file:\n                    for line in key_file:\n                        if line.startswith('#'):\n                            # Commented line\n                            continue\n                        ssh_key = ssh_key + line\n\n            clean_ssh_key = _strip_key(ssh_key).split(' ')\n            # Check that the first two list items of clean key lists are equal.\n            if clean_current_key[0] != clean_ssh_key[0] or clean_current_key[1] != clean_ssh_key[1]:\n                ssh_key_changed = True\n        else:\n            # If current_ssh_key is None, but we're setting a new key with\n            # either ssh_key or ssh_key_file, then we need to flag the change.\n            ssh_key_changed = True\n\n    # Upload SSH key, if changed.\n    if ssh_key_changed:\n        if not __opts__['test']:\n            # Upload key\n            response = __salt__[esxi_cmd]('upload_ssh_key',\n                                          ssh_key=ssh_key,\n                                          ssh_key_file=ssh_key_file,\n                                          certificate_verify=certificate_verify)\n            error = response.get('Error')\n            if error:\n                ret['comment'] = 'Error: {0}'.format(error)\n                return ret\n        ret['changes'].update({'SSH Key':\n                              {'old': current_ssh_key,\n                               'new': ssh_key if ssh_key else ssh_key_file}})\n\n    # Configure service_policy\n    if service_policy:\n        current_service_policy = __salt__[esxi_cmd]('get_service_policy',\n                                                    service_name=ssh).get(host)\n        error = current_service_policy.get('Error')\n        if error:\n            ret['comment'] = 'Error: {0}'.format(error)\n            return ret\n        current_service_policy = current_service_policy.get(ssh)\n\n        if service_policy != current_service_policy:\n            # Only run the command if not using test=True\n            if not __opts__['test']:\n                response = __salt__[esxi_cmd]('set_service_policy',\n                                              service_name=ssh,\n                                              service_policy=service_policy).get(host)\n                error = response.get('Error')\n                if error:\n                    ret['comment'] = 'Error: {0}'.format(error)\n                    return ret\n            ret['changes'].update({'service_policy':\n                                  {'old': current_service_policy,\n                                   'new': service_policy}})\n\n    # Restart ssh_service if service_restart=True\n    if service_restart:\n        # Only run the command if not using test=True\n        if not __opts__['test']:\n            response = __salt__[esxi_cmd]('service_restart',\n                                          service_name=ssh).get(host)\n            error = response.get('Error')\n            if error:\n                ret['comment'] = 'Error: {0}'.format(error)\n                return ret\n        ret['changes'].update({'service_restart':\n                              {'old': '',\n                               'new': 'SSH service restarted.'}})\n\n    ret['result'] = True\n    if ret['changes'] == {}:\n        ret['comment'] = 'SSH service is already in the desired state.'\n        return ret\n\n    if __opts__['test']:\n        ret['result'] = None\n        ret['comment'] = 'SSH service state will change.'\n\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a new state where syslog configuration parameters are set.", "response": "def syslog_configured(name,\n                      syslog_configs,\n                      firewall=True,\n                      reset_service=True,\n                      reset_syslog_config=False,\n                      reset_configs=None):\n    '''\n    Ensures the specified syslog configuration parameters. By default,\n    this state will reset the syslog service after any new or changed\n    parameters are set successfully.\n\n    name\n        Name of the state.\n\n    syslog_configs\n        Name of parameter to set (corresponds to the command line switch for\n        esxcli without the double dashes (--))\n\n        Valid syslog_config values are ``logdir``, ``loghost``, ``logdir-unique``,\n        ``default-rotate``, ``default-size``, and ``default-timeout``.\n\n        Each syslog_config option also needs a configuration value to set.\n        For example, ``loghost`` requires URLs or IP addresses to use for\n        logging. Multiple log servers can be specified by listing them,\n        comma-separated, but without spaces before or after commas\n\n        (reference: https://blogs.vmware.com/vsphere/2012/04/configuring-multiple-syslog-servers-for-esxi-5.html)\n\n    firewall\n        Enable the firewall rule set for syslog. Defaults to ``True``.\n\n    reset_service\n        After a successful parameter set, reset the service. Defaults to ``True``.\n\n    reset_syslog_config\n        Resets the syslog service to it's default settings. Defaults to ``False``.\n        If set to ``True``, default settings defined by the list of syslog configs\n        in ``reset_configs`` will be reset before running any other syslog settings.\n\n    reset_configs\n        A comma-delimited list of parameters to reset. Only runs if\n        ``reset_syslog_config`` is set to ``True``. If ``reset_syslog_config`` is set\n        to ``True``, but no syslog configs are listed in ``reset_configs``, then\n        ``reset_configs`` will be set to ``all`` by default.\n\n        See ``syslog_configs`` parameter above for a list of valid options.\n\n    Example:\n\n    .. code-block:: yaml\n\n        configure-host-syslog:\n          esxi.syslog_configured:\n            - syslog_configs:\n                loghost: ssl://localhost:5432,tcp://10.1.0.1:1514\n                default-timeout: 120\n            - firewall: True\n            - reset_service: True\n            - reset_syslog_config: True\n            - reset_configs: loghost,default-timeout\n    '''\n    ret = {'name': name,\n           'result': False,\n           'changes': {},\n           'comment': ''}\n    esxi_cmd = 'esxi.cmd'\n    host = __pillar__['proxy']['host']\n\n    if reset_syslog_config:\n        if not reset_configs:\n            reset_configs = 'all'\n        # Only run the command if not using test=True\n        if not __opts__['test']:\n            reset = __salt__[esxi_cmd]('reset_syslog_config',\n                                       syslog_config=reset_configs).get(host)\n            for key, val in six.iteritems(reset):\n                if isinstance(val, bool):\n                    continue\n                if not val.get('success'):\n                    msg = val.get('message')\n                    if not msg:\n                        msg = 'There was an error resetting a syslog config \\'{0}\\'.' \\\n                              'Please check debug logs.'.format(val)\n                    ret['comment'] = 'Error: {0}'.format(msg)\n                    return ret\n\n        ret['changes'].update({'reset_syslog_config':\n                              {'old': '',\n                               'new': reset_configs}})\n\n    current_firewall = __salt__[esxi_cmd]('get_firewall_status').get(host)\n    error = current_firewall.get('Error')\n    if error:\n        ret['comment'] = 'Error: {0}'.format(error)\n        return ret\n\n    current_firewall = current_firewall.get('rulesets').get('syslog')\n    if current_firewall != firewall:\n        # Only run the command if not using test=True\n        if not __opts__['test']:\n            enabled = __salt__[esxi_cmd]('enable_firewall_ruleset',\n                                         ruleset_enable=firewall,\n                                         ruleset_name='syslog').get(host)\n            if enabled.get('retcode') != 0:\n                err = enabled.get('stderr')\n                out = enabled.get('stdout')\n                ret['comment'] = 'Error: {0}'.format(err if err else out)\n                return ret\n\n        ret['changes'].update({'firewall':\n                              {'old': current_firewall,\n                               'new': firewall}})\n\n    current_syslog_config = __salt__[esxi_cmd]('get_syslog_config').get(host)\n    for key, val in six.iteritems(syslog_configs):\n        # The output of get_syslog_config has different keys than the keys\n        # Used to set syslog_config values. We need to look them up first.\n        try:\n            lookup_key = _lookup_syslog_config(key)\n        except KeyError:\n            ret['comment'] = '\\'{0}\\' is not a valid config variable.'.format(key)\n            return ret\n\n        current_val = current_syslog_config[lookup_key]\n        if six.text_type(current_val) != six.text_type(val):\n            # Only run the command if not using test=True\n            if not __opts__['test']:\n                response = __salt__[esxi_cmd]('set_syslog_config',\n                                              syslog_config=key,\n                                              config_value=val,\n                                              firewall=firewall,\n                                              reset_service=reset_service).get(host)\n                success = response.get(key).get('success')\n                if not success:\n                    msg = response.get(key).get('message')\n                    if not msg:\n                        msg = 'There was an error setting syslog config \\'{0}\\'. ' \\\n                              'Please check debug logs.'.format(key)\n                    ret['comment'] = msg\n                    return ret\n\n            if not ret['changes'].get('syslog_config'):\n                ret['changes'].update({'syslog_config': {}})\n            ret['changes']['syslog_config'].update({key:\n                                                   {'old': current_val,\n                                                    'new': val}})\n\n    ret['result'] = True\n    if ret['changes'] == {}:\n        ret['comment'] = 'Syslog is already in the desired state.'\n        return ret\n\n    if __opts__['test']:\n        ret['result'] = None\n        ret['comment'] = 'Syslog state will change.'\n\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef diskgroups_configured(name, diskgroups, erase_disks=False):\n    '''\n    Configures the disk groups to use for vsan.\n\n    This function will do the following:\n\n    1. Check whether or not all disks in the diskgroup spec exist, and raises\n       and errors if they do not.\n\n    2. Create diskgroups with the correct disk configurations if diskgroup\n       (identified by the cache disk canonical name) doesn't exist\n\n    3. Adds extra capacity disks to the existing diskgroup\n\n    Example:\n\n    .. code:: python\n\n        {\n            'cache_scsi_addr': 'vmhba1:C0:T0:L0',\n            'capacity_scsi_addrs': [\n                'vmhba2:C0:T0:L0',\n                'vmhba3:C0:T0:L0',\n                'vmhba4:C0:T0:L0',\n            ]\n        }\n\n    name\n        Mandatory state name\n\n    diskgroups\n        Disk group representation containing scsi disk addresses.\n        Scsi addresses are expected for disks in the diskgroup:\n\n    erase_disks\n        Specifies whether to erase all partitions on all disks member of the\n        disk group before the disk group is created. Default value is False.\n    '''\n    proxy_details = __salt__['esxi.get_details']()\n    hostname = proxy_details['host'] if not proxy_details.get('vcenter') \\\n               else proxy_details['esxi_host']\n    log.info('Running state %s for host \\'%s\\'', name, hostname)\n    # Variable used to return the result of the invocation\n    ret = {'name': name,\n           'result': None,\n           'changes': {},\n           'comments': None}\n    # Signals if errors have been encountered\n    errors = False\n    # Signals if changes are required\n    changes = False\n    comments = []\n    diskgroup_changes = {}\n    si = None\n    try:\n        log.trace('Validating diskgroups_configured input')\n        schema = DiskGroupsDiskScsiAddressSchema.serialize()\n        try:\n            jsonschema.validate({'diskgroups': diskgroups,\n                                 'erase_disks': erase_disks}, schema)\n        except jsonschema.exceptions.ValidationError as exc:\n            raise InvalidConfigError(exc)\n        si = __salt__['vsphere.get_service_instance_via_proxy']()\n        host_disks = __salt__['vsphere.list_disks'](service_instance=si)\n        if not host_disks:\n            raise VMwareObjectRetrievalError(\n                'No disks retrieved from host \\'{0}\\''.format(hostname))\n        scsi_addr_to_disk_map = {d['scsi_address']: d for d in host_disks}\n        log.trace('scsi_addr_to_disk_map = %s', scsi_addr_to_disk_map)\n        existing_diskgroups = \\\n                __salt__['vsphere.list_diskgroups'](service_instance=si)\n        cache_disk_to_existing_diskgroup_map = \\\n                {dg['cache_disk']: dg for dg in existing_diskgroups}\n    except CommandExecutionError as err:\n        log.error('Error: %s', err)\n        if si:\n            __salt__['vsphere.disconnect'](si)\n        ret.update({\n            'result': False if not __opts__['test'] else None,\n            'comment': six.text_type(err)})\n        return ret\n\n    # Iterate through all of the disk groups\n    for idx, dg in enumerate(diskgroups):\n        # Check for cache disk\n        if not dg['cache_scsi_addr'] in scsi_addr_to_disk_map:\n            comments.append('No cache disk with scsi address \\'{0}\\' was '\n                            'found.'.format(dg['cache_scsi_addr']))\n            log.error(comments[-1])\n            errors = True\n            continue\n\n        # Check for capacity disks\n        cache_disk_id = scsi_addr_to_disk_map[dg['cache_scsi_addr']]['id']\n        cache_disk_display = '{0} (id:{1})'.format(dg['cache_scsi_addr'],\n                                                   cache_disk_id)\n        bad_scsi_addrs = []\n        capacity_disk_ids = []\n        capacity_disk_displays = []\n        for scsi_addr in dg['capacity_scsi_addrs']:\n            if scsi_addr not in scsi_addr_to_disk_map:\n                bad_scsi_addrs.append(scsi_addr)\n                continue\n            capacity_disk_ids.append(scsi_addr_to_disk_map[scsi_addr]['id'])\n            capacity_disk_displays.append(\n                '{0} (id:{1})'.format(scsi_addr, capacity_disk_ids[-1]))\n        if bad_scsi_addrs:\n            comments.append('Error in diskgroup #{0}: capacity disks with '\n                            'scsi addresses {1} were not found.'\n                            ''.format(idx,\n                                      ', '.join(['\\'{0}\\''.format(a)\n                                                 for a in bad_scsi_addrs])))\n            log.error(comments[-1])\n            errors = True\n            continue\n\n        if not cache_disk_to_existing_diskgroup_map.get(cache_disk_id):\n            # A new diskgroup needs to be created\n            log.trace('erase_disks = %s', erase_disks)\n            if erase_disks:\n                if __opts__['test']:\n                    comments.append('State {0} will '\n                                    'erase all disks of disk group #{1}; '\n                                    'cache disk: \\'{2}\\', '\n                                    'capacity disk(s): {3}.'\n                                    ''.format(name, idx, cache_disk_display,\n                                              ', '.join(\n                                                  ['\\'{}\\''.format(a) for a in\n                                                   capacity_disk_displays])))\n                else:\n                    # Erase disk group disks\n                    for disk_id in [cache_disk_id] + capacity_disk_ids:\n                        __salt__['vsphere.erase_disk_partitions'](\n                            disk_id=disk_id, service_instance=si)\n                    comments.append('Erased disks of diskgroup #{0}; '\n                                    'cache disk: \\'{1}\\', capacity disk(s): '\n                                    '{2}'.format(\n                                        idx, cache_disk_display,\n                                        ', '.join(['\\'{0}\\''.format(a) for a in\n                                                  capacity_disk_displays])))\n                    log.info(comments[-1])\n\n            if __opts__['test']:\n                comments.append('State {0} will create '\n                                'the disk group #{1}; cache disk: \\'{2}\\', '\n                                'capacity disk(s): {3}.'\n                                .format(name, idx, cache_disk_display,\n                                        ', '.join(['\\'{0}\\''.format(a) for a in\n                                                   capacity_disk_displays])))\n                log.info(comments[-1])\n                changes = True\n                continue\n            try:\n                __salt__['vsphere.create_diskgroup'](cache_disk_id,\n                                                     capacity_disk_ids,\n                                                     safety_checks=False,\n                                                     service_instance=si)\n            except VMwareSaltError as err:\n                comments.append('Error creating disk group #{0}: '\n                                '{1}.'.format(idx, err))\n                log.error(comments[-1])\n                errors = True\n                continue\n\n            comments.append('Created disk group #\\'{0}\\'.'.format(idx))\n            log.info(comments[-1])\n            diskgroup_changes[six.text_type(idx)] = \\\n                 {'new': {'cache': cache_disk_display,\n                          'capacity': capacity_disk_displays}}\n            changes = True\n            continue\n\n        # The diskgroup exists; checking the capacity disks\n        log.debug('Disk group #%s exists. Checking capacity disks: %s.',\n                  idx, capacity_disk_displays)\n        existing_diskgroup = \\\n                cache_disk_to_existing_diskgroup_map.get(cache_disk_id)\n        existing_capacity_disk_displays = \\\n                ['{0} (id:{1})'.format([d['scsi_address'] for d in host_disks\n                                        if d['id'] == disk_id][0], disk_id)\n                 for disk_id in existing_diskgroup['capacity_disks']]\n        # Populate added disks and removed disks and their displays\n        added_capacity_disk_ids = []\n        added_capacity_disk_displays = []\n        removed_capacity_disk_ids = []\n        removed_capacity_disk_displays = []\n        for disk_id in capacity_disk_ids:\n            if disk_id not in existing_diskgroup['capacity_disks']:\n                disk_scsi_addr = [d['scsi_address'] for d in host_disks\n                                  if d['id'] == disk_id][0]\n                added_capacity_disk_ids.append(disk_id)\n                added_capacity_disk_displays.append(\n                    '{0} (id:{1})'.format(disk_scsi_addr, disk_id))\n        for disk_id in existing_diskgroup['capacity_disks']:\n            if disk_id not in capacity_disk_ids:\n                disk_scsi_addr = [d['scsi_address'] for d in host_disks\n                                  if d['id'] == disk_id][0]\n                removed_capacity_disk_ids.append(disk_id)\n                removed_capacity_disk_displays.append(\n                    '{0} (id:{1})'.format(disk_scsi_addr, disk_id))\n\n        log.debug('Disk group #%s: existing capacity disk ids: %s; added '\n                  'capacity disk ids: %s; removed capacity disk ids: %s',\n                  idx, existing_capacity_disk_displays,\n                  added_capacity_disk_displays, removed_capacity_disk_displays)\n\n        #TODO revisit this when removing capacity disks is supported\n        if removed_capacity_disk_ids:\n            comments.append(\n                'Error removing capacity disk(s) {0} from disk group #{1}; '\n                'operation is not supported.'\n                ''.format(', '.join(['\\'{0}\\''.format(id) for id in\n                                     removed_capacity_disk_displays]), idx))\n            log.error(comments[-1])\n            errors = True\n            continue\n\n        if added_capacity_disk_ids:\n            # Capacity disks need to be added to disk group\n\n            # Building a string representation of the capacity disks\n            # that need to be added\n            s = ', '.join(['\\'{0}\\''.format(id) for id in\n                           added_capacity_disk_displays])\n            if __opts__['test']:\n                comments.append('State {0} will add '\n                                'capacity disk(s) {1} to disk group #{2}.'\n                                ''.format(name, s, idx))\n                log.info(comments[-1])\n                changes = True\n                continue\n            try:\n                __salt__['vsphere.add_capacity_to_diskgroup'](\n                    cache_disk_id,\n                    added_capacity_disk_ids,\n                    safety_checks=False,\n                    service_instance=si)\n            except VMwareSaltError as err:\n                comments.append('Error adding capacity disk(s) {0} to '\n                                'disk group #{1}: {2}.'.format(s, idx, err))\n                log.error(comments[-1])\n                errors = True\n                continue\n\n            com = ('Added capacity disk(s) {0} to disk group #{1}'\n                   ''.format(s, idx))\n            log.info(com)\n            comments.append(com)\n            diskgroup_changes[six.text_type(idx)] = \\\n                 {'new': {'cache': cache_disk_display,\n                          'capacity': capacity_disk_displays},\n                  'old': {'cache': cache_disk_display,\n                          'capacity': existing_capacity_disk_displays}}\n            changes = True\n            continue\n\n        # No capacity needs to be added\n        s = ('Disk group #{0} is correctly configured. Nothing to be done.'\n             ''.format(idx))\n        log.info(s)\n        comments.append(s)\n    __salt__['vsphere.disconnect'](si)\n\n    #Build the final return message\n    result = (True if not (changes or errors) else  # no changes/errors\n              None if __opts__['test'] else  # running in test mode\n              False if errors else True)  # found errors; defaults to True\n    ret.update({'result': result,\n                'comment': '\\n'.join(comments),\n                'changes': diskgroup_changes})\n    return ret", "response": "Configure the disk groups for a given host."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconfiguring the host cache used for swapping. It will do the following: 1. Checks if backing disk exists 2. Creates the VMFS datastore if doesn't exist (datastore partition will be created and use the entire disk) 3. Raises an error if ``dedicated_backing_disk`` is ``True`` and partitions already exist on the backing disk 4. Configures host_cache to use a portion of the datastore for caching (either a specific size or a percentage of the datastore) Examples Percentage swap size (can't be 100%) .. code:: python { 'enabled': true, 'datastore': { 'backing_disk_scsi_addr': 'vmhba0:C0:T0:L0', 'vmfs_version': 5, 'name': 'hostcache' } 'dedicated_backing_disk': false 'swap_size': '98%', } Fixed sized swap size .. code:: python { 'enabled': true, 'datastore': { 'backing_disk_scsi_addr': 'vmhba0:C0:T0:L0', 'vmfs_version': 5, 'name': 'hostcache' } 'dedicated_backing_disk': true 'swap_size': '10GiB', } name Mandatory state name. enabled Specifies whether the host cache is enabled. datastore Specifies the host cache datastore. swap_size Specifies the size of the host cache swap. Can be a percentage or a value in GiB. Default value is ``100%``. dedicated_backing_disk Specifies whether the backing disk is dedicated to the host cache which means it must have no other partitions. Default is False erase_backing_disk Specifies whether to erase all partitions on the backing disk before the datastore is created. Default value is False.", "response": "def host_cache_configured(name, enabled, datastore, swap_size='100%',\n                          dedicated_backing_disk=False,\n                          erase_backing_disk=False):\n    '''\n    Configures the host cache used for swapping.\n\n    It will do the following:\n\n    1. Checks if backing disk exists\n\n    2. Creates the VMFS datastore if doesn't exist (datastore partition will be\n       created and use the entire disk)\n\n    3. Raises an error if ``dedicated_backing_disk`` is ``True`` and partitions\n       already exist on the backing disk\n\n    4. Configures host_cache to use a portion of the datastore for caching\n       (either a specific size or a percentage of the datastore)\n\n    Examples\n\n    Percentage swap size (can't be 100%)\n\n    .. code:: python\n\n        {\n            'enabled': true,\n            'datastore': {\n                'backing_disk_scsi_addr': 'vmhba0:C0:T0:L0',\n                'vmfs_version': 5,\n                'name': 'hostcache'\n                }\n            'dedicated_backing_disk': false\n            'swap_size': '98%',\n        }\n\n    Fixed sized swap size\n\n    .. code:: python\n\n        {\n            'enabled': true,\n            'datastore': {\n                'backing_disk_scsi_addr': 'vmhba0:C0:T0:L0',\n                'vmfs_version': 5,\n                'name': 'hostcache'\n                }\n            'dedicated_backing_disk': true\n            'swap_size': '10GiB',\n        }\n\n    name\n        Mandatory state name.\n\n    enabled\n        Specifies whether the host cache is enabled.\n\n    datastore\n        Specifies the host cache datastore.\n\n    swap_size\n        Specifies the size of the host cache swap. Can be a percentage or a\n        value in GiB. Default value is ``100%``.\n\n    dedicated_backing_disk\n        Specifies whether the backing disk is dedicated to the host cache which\n        means it must have no other partitions. Default is False\n\n    erase_backing_disk\n        Specifies whether to erase all partitions on the backing disk before\n        the datastore is created. Default value is False.\n    '''\n    log.trace('enabled = %s', enabled)\n    log.trace('datastore = %s', datastore)\n    log.trace('swap_size = %s', swap_size)\n    log.trace('erase_backing_disk = %s', erase_backing_disk)\n    # Variable used to return the result of the invocation\n    proxy_details = __salt__['esxi.get_details']()\n    hostname = proxy_details['host'] if not proxy_details.get('vcenter') \\\n               else proxy_details['esxi_host']\n    log.trace('hostname = %s', hostname)\n    log.info('Running host_cache_swap_configured for host \\'%s\\'', hostname)\n    ret = {'name': hostname,\n           'comment': 'Default comments',\n           'result': None,\n           'changes': {}}\n    result = None if __opts__['test'] else True  # We assume success\n    needs_setting = False\n    comments = []\n    changes = {}\n    si = None\n    try:\n        log.debug('Validating host_cache_configured input')\n        schema = HostCacheSchema.serialize()\n        try:\n            jsonschema.validate({'enabled': enabled,\n                                 'datastore': datastore,\n                                 'swap_size': swap_size,\n                                 'erase_backing_disk': erase_backing_disk},\n                                schema)\n        except jsonschema.exceptions.ValidationError as exc:\n            raise InvalidConfigError(exc)\n        m = re.match(r'(\\d+)(%|GiB)', swap_size)\n        swap_size_value = int(m.group(1))\n        swap_type = m.group(2)\n        log.trace('swap_size_value = %s; swap_type = %s', swap_size_value, swap_type)\n        si = __salt__['vsphere.get_service_instance_via_proxy']()\n        host_cache = __salt__['vsphere.get_host_cache'](service_instance=si)\n\n        # Check enabled\n        if host_cache['enabled'] != enabled:\n            changes.update({'enabled': {'old': host_cache['enabled'],\n                                        'new': enabled}})\n            needs_setting = True\n\n        # Check datastores\n        existing_datastores = None\n        if host_cache.get('datastore'):\n            existing_datastores = \\\n                    __salt__['vsphere.list_datastores_via_proxy'](\n                        datastore_names=[datastore['name']],\n                        service_instance=si)\n        # Retrieve backing disks\n        existing_disks = __salt__['vsphere.list_disks'](\n            scsi_addresses=[datastore['backing_disk_scsi_addr']],\n            service_instance=si)\n        if not existing_disks:\n            raise VMwareObjectRetrievalError(\n                'Disk with scsi address \\'{0}\\' was not found in host \\'{1}\\''\n                ''.format(datastore['backing_disk_scsi_addr'], hostname))\n        backing_disk = existing_disks[0]\n        backing_disk_display = '{0} (id:{1})'.format(\n            backing_disk['scsi_address'], backing_disk['id'])\n        log.trace('backing_disk = %s', backing_disk_display)\n\n        existing_datastore = None\n        if not existing_datastores:\n            # Check if disk needs to be erased\n            if erase_backing_disk:\n                if __opts__['test']:\n                    comments.append('State {0} will erase '\n                                    'the backing disk \\'{1}\\' on host \\'{2}\\'.'\n                                    ''.format(name, backing_disk_display,\n                                              hostname))\n                    log.info(comments[-1])\n                else:\n                    # Erase disk\n                    __salt__['vsphere.erase_disk_partitions'](\n                        disk_id=backing_disk['id'], service_instance=si)\n                    comments.append('Erased backing disk \\'{0}\\' on host '\n                                    '\\'{1}\\'.'.format(backing_disk_display,\n                                                      hostname))\n                    log.info(comments[-1])\n            # Create the datastore\n            if __opts__['test']:\n                comments.append('State {0} will create '\n                                'the datastore \\'{1}\\', with backing disk '\n                                '\\'{2}\\', on host \\'{3}\\'.'\n                                ''.format(name, datastore['name'],\n                                          backing_disk_display, hostname))\n                log.info(comments[-1])\n            else:\n                if dedicated_backing_disk:\n                    # Check backing disk doesn't already have partitions\n                    partitions = __salt__['vsphere.list_disk_partitions'](\n                        disk_id=backing_disk['id'], service_instance=si)\n                    log.trace('partitions = %s', partitions)\n                    # We will ignore the mbr partitions\n                    non_mbr_partitions = [p for p in partitions\n                                          if p['format'] != 'mbr']\n                    if non_mbr_partitions:\n                        raise VMwareApiError(\n                            'Backing disk \\'{0}\\' has unexpected partitions'\n                            ''.format(backing_disk_display))\n                __salt__['vsphere.create_vmfs_datastore'](\n                    datastore['name'], existing_disks[0]['id'],\n                    datastore['vmfs_version'], service_instance=si)\n                comments.append('Created vmfs datastore \\'{0}\\', backed by '\n                                'disk \\'{1}\\', on host \\'{2}\\'.'\n                                ''.format(datastore['name'],\n                                          backing_disk_display, hostname))\n                log.info(comments[-1])\n                changes.update(\n                    {'datastore':\n                     {'new': {'name': datastore['name'],\n                              'backing_disk': backing_disk_display}}})\n                existing_datastore = \\\n                        __salt__['vsphere.list_datastores_via_proxy'](\n                            datastore_names=[datastore['name']],\n                            service_instance=si)[0]\n            needs_setting = True\n        else:\n            # Check datastore is backed by the correct disk\n            if not existing_datastores[0].get('backing_disk_ids'):\n                raise VMwareSaltError('Datastore \\'{0}\\' doesn\\'t have a '\n                                           'backing disk'\n                                           ''.format(datastore['name']))\n            if backing_disk['id'] not in \\\n               existing_datastores[0]['backing_disk_ids']:\n\n                raise VMwareSaltError(\n                    'Datastore \\'{0}\\' is not backed by the correct disk: '\n                    'expected \\'{1}\\'; got {2}'\n                    ''.format(\n                        datastore['name'], backing_disk['id'],\n                        ', '.join(\n                            ['\\'{0}\\''.format(disk) for disk in\n                             existing_datastores[0]['backing_disk_ids']])))\n\n            comments.append('Datastore \\'{0}\\' already exists on host \\'{1}\\' '\n                            'and is backed by disk \\'{2}\\'. Nothing to be '\n                            'done.'.format(datastore['name'], hostname,\n                                           backing_disk_display))\n            existing_datastore = existing_datastores[0]\n            log.trace('existing_datastore = %s', existing_datastore)\n            log.info(comments[-1])\n\n        if existing_datastore:\n            # The following comparisons can be done if the existing_datastore\n            # is set; it may not be set if running in test mode\n            #\n            # We support percent, as well as MiB, we will convert the size\n            # to MiB, multiples of 1024 (VMware SDK limitation)\n            if swap_type == '%':\n                # Percentage swap size\n                # Convert from bytes to MiB\n                raw_size_MiB = (swap_size_value/100.0) * \\\n                        (existing_datastore['capacity']/1024/1024)\n            else:\n                raw_size_MiB = swap_size_value * 1024\n            log.trace('raw_size = %sMiB', raw_size_MiB)\n            swap_size_MiB = int(raw_size_MiB/1024)*1024\n            log.trace('adjusted swap_size = %sMiB', swap_size_MiB)\n            existing_swap_size_MiB = 0\n            m = re.match(r'(\\d+)MiB', host_cache.get('swap_size')) if \\\n                    host_cache.get('swap_size') else None\n            if m:\n                # if swap_size from the host is set and has an expected value\n                # we are going to parse it to get the number of MiBs\n                existing_swap_size_MiB = int(m.group(1))\n            if not existing_swap_size_MiB == swap_size_MiB:\n                needs_setting = True\n                changes.update(\n                    {'swap_size':\n                     {'old': '{}GiB'.format(existing_swap_size_MiB/1024),\n                      'new': '{}GiB'.format(swap_size_MiB/1024)}})\n\n        if needs_setting:\n            if __opts__['test']:\n                comments.append('State {0} will configure '\n                                'the host cache on host \\'{1}\\' to: {2}.'\n                                ''.format(name, hostname,\n                                          {'enabled': enabled,\n                                           'datastore_name': datastore['name'],\n                                           'swap_size': swap_size}))\n            else:\n                if (existing_datastore['capacity'] / 1024.0**2) < \\\n                   swap_size_MiB:\n\n                    raise ArgumentValueError(\n                        'Capacity of host cache datastore \\'{0}\\' ({1} MiB) is '\n                        'smaller than the required swap size ({2} MiB)'\n                        ''.format(existing_datastore['name'],\n                                  existing_datastore['capacity'] / 1024.0**2,\n                                  swap_size_MiB))\n                __salt__['vsphere.configure_host_cache'](\n                    enabled,\n                    datastore['name'],\n                    swap_size_MiB=swap_size_MiB,\n                    service_instance=si)\n                comments.append('Host cache configured on host '\n                                '\\'{0}\\'.'.format(hostname))\n        else:\n            comments.append('Host cache on host \\'{0}\\' is already correctly '\n                            'configured. Nothing to be done.'.format(hostname))\n            result = True\n        __salt__['vsphere.disconnect'](si)\n        log.info(comments[-1])\n        ret.update({'comment': '\\n'.join(comments),\n                    'result': result,\n                    'changes': changes})\n        return ret\n    except CommandExecutionError as err:\n        log.error('Error: %s.', err)\n        if si:\n            __salt__['vsphere.disconnect'](si)\n        ret.update({\n            'result': False if not __opts__['test'] else None,\n            'comment': '{}.'.format(err)})\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef hostname(name, hostname=None):\n    '''\n    Ensures that the hostname is set to the specified value.\n\n    .. versionadded:: 2019.2.0\n\n    name: The name of the module function to execute.\n\n    hostname(str): The hostname of the server.\n\n    SLS Example:\n\n    .. code-block:: yaml\n\n        set_name:\n          cimc.hostname:\n            - hostname: foobar\n\n    '''\n\n    ret = _default_ret(name)\n\n    current_name = __salt__['cimc.get_hostname']()\n\n    req_change = False\n\n    try:\n\n        if current_name != hostname:\n            req_change = True\n\n        if req_change:\n\n            update = __salt__['cimc.set_hostname'](hostname)\n\n            if not update:\n                ret['result'] = False\n                ret['comment'] = \"Error setting hostname.\"\n                return ret\n\n            ret['changes']['before'] = current_name\n            ret['changes']['after'] = hostname\n            ret['comment'] = \"Hostname modified.\"\n        else:\n            ret['comment'] = \"Hostname already configured. No changes required.\"\n\n    except Exception as err:\n        ret['result'] = False\n        ret['comment'] = \"Error occurred setting hostname.\"\n        log.error(err)\n        return ret\n\n    ret['result'] = True\n\n    return ret", "response": "Ensures that the hostname of the specified module function is set to the specified value."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef logging_levels(name, remote=None, local=None):\n    '''\n    Ensures that the logging levels are set on the device. The logging levels\n    must match the following options: emergency, alert, critical, error, warning,\n    notice, informational, debug.\n\n    .. versionadded:: 2019.2.0\n\n    name: The name of the module function to execute.\n\n    remote(str): The logging level for SYSLOG logs.\n\n    local(str): The logging level for the local device.\n\n    SLS Example:\n\n    .. code-block:: yaml\n\n        logging_levels:\n          cimc.logging_levels:\n            - remote: informational\n            - local: notice\n\n    '''\n\n    ret = _default_ret(name)\n\n    syslog_conf = __salt__['cimc.get_syslog_settings']()\n\n    req_change = False\n\n    try:\n        syslog_dict = syslog_conf['outConfigs']['commSyslog'][0]\n\n        if remote and syslog_dict['remoteSeverity'] != remote:\n            req_change = True\n        elif local and syslog_dict['localSeverity'] != local:\n            req_change = True\n\n        if req_change:\n\n            update = __salt__['cimc.set_logging_levels'](remote, local)\n\n            if update['outConfig']['commSyslog'][0]['status'] != 'modified':\n                ret['result'] = False\n                ret['comment'] = \"Error setting logging levels.\"\n                return ret\n\n            ret['changes']['before'] = syslog_conf\n            ret['changes']['after'] = __salt__['cimc.get_syslog_settings']()\n            ret['comment'] = \"Logging level settings modified.\"\n        else:\n            ret['comment'] = \"Logging level already configured. No changes required.\"\n\n    except Exception as err:\n        ret['result'] = False\n        ret['comment'] = \"Error occurred setting logging level settings.\"\n        log.error(err)\n        return ret\n\n    ret['result'] = True\n\n    return ret", "response": "Sets the logging levels on the specified device."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a NTP server list for the named module function.", "response": "def ntp(name, servers):\n    '''\n    Ensures that the NTP servers are configured. Servers are provided as an individual string or list format. Only four\n    NTP servers will be reviewed. Any entries past four will be ignored.\n\n    name: The name of the module function to execute.\n\n    servers(str, list): The IP address or FQDN of the NTP servers.\n\n    SLS Example:\n\n    .. code-block:: yaml\n\n        ntp_configuration_list:\n          cimc.ntp:\n            - servers:\n              - foo.bar.com\n              - 10.10.10.10\n\n        ntp_configuration_str:\n          cimc.ntp:\n            - servers: foo.bar.com\n\n    '''\n    ret = _default_ret(name)\n\n    ntp_servers = ['', '', '', '']\n\n    # Parse our server arguments\n    if isinstance(servers, list):\n        i = 0\n        for x in servers:\n            ntp_servers[i] = x\n            i += 1\n    else:\n        ntp_servers[0] = servers\n\n    conf = __salt__['cimc.get_ntp']()\n\n    # Check if our NTP configuration is already set\n    req_change = False\n    try:\n        if conf['outConfigs']['commNtpProvider'][0]['ntpEnable'] != 'yes' \\\n                or ntp_servers[0] != conf['outConfigs']['commNtpProvider'][0]['ntpServer1'] \\\n                or ntp_servers[1] != conf['outConfigs']['commNtpProvider'][0]['ntpServer2'] \\\n                or ntp_servers[2] != conf['outConfigs']['commNtpProvider'][0]['ntpServer3'] \\\n                or ntp_servers[3] != conf['outConfigs']['commNtpProvider'][0]['ntpServer4']:\n            req_change = True\n    except KeyError as err:\n        ret['result'] = False\n        ret['comment'] = \"Unable to confirm current NTP settings.\"\n        log.error(err)\n        return ret\n\n    if req_change:\n\n        try:\n            update = __salt__['cimc.set_ntp_server'](ntp_servers[0],\n                                                     ntp_servers[1],\n                                                     ntp_servers[2],\n                                                     ntp_servers[3])\n            if update['outConfig']['commNtpProvider'][0]['status'] != 'modified':\n                ret['result'] = False\n                ret['comment'] = \"Error setting NTP configuration.\"\n                return ret\n        except Exception as err:\n            ret['result'] = False\n            ret['comment'] = \"Error setting NTP configuration.\"\n            log.error(err)\n            return ret\n\n        ret['changes']['before'] = conf\n        ret['changes']['after'] = __salt__['cimc.get_ntp']()\n        ret['comment'] = \"NTP settings modified.\"\n    else:\n        ret['comment'] = \"NTP already configured. No changes required.\"\n\n    ret['result'] = True\n\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nensures that the power configuration of a Chassis is configured on the system.", "response": "def power_configuration(name, policy=None, delayType=None, delayValue=None):\n    '''\n    Ensures that the power configuration is configured on the system. This is\n    only available on some C-Series servers.\n\n    .. versionadded:: 2019.2.0\n\n    name: The name of the module function to execute.\n\n    policy(str): The action to be taken when chassis power is restored after\n    an unexpected power loss. This can be one of the following:\n\n        reset: The server is allowed to boot up normally when power is\n        restored. The server can restart immediately or, optionally, after a\n        fixed or random delay.\n\n        stay-off: The server remains off until it is manually restarted.\n\n        last-state: The server restarts and the system attempts to restore\n        any processes that were running before power was lost.\n\n    delayType(str): If the selected policy is reset, the restart can be\n    delayed with this option. This can be one of the following:\n\n        fixed: The server restarts after a fixed delay.\n\n        random: The server restarts after a random delay.\n\n    delayValue(int): If a fixed delay is selected, once chassis power is\n    restored and the Cisco IMC has finished rebooting, the system waits for\n    the specified number of seconds before restarting the server. Enter an\n    integer between 0 and 240.\n\n\n    SLS Example:\n\n    .. code-block:: yaml\n\n        reset_power:\n          cimc.power_configuration:\n            - policy: reset\n            - delayType: fixed\n            - delayValue: 0\n\n        power_off:\n          cimc.power_configuration:\n            - policy: stay-off\n\n\n    '''\n\n    ret = _default_ret(name)\n\n    power_conf = __salt__['cimc.get_power_configuration']()\n\n    req_change = False\n\n    try:\n        power_dict = power_conf['outConfigs']['biosVfResumeOnACPowerLoss'][0]\n\n        if policy and power_dict['vpResumeOnACPowerLoss'] != policy:\n            req_change = True\n        elif policy == \"reset\":\n            if power_dict['delayType'] != delayType:\n                req_change = True\n            elif power_dict['delayType'] == \"fixed\":\n                if str(power_dict['delay']) != str(delayValue):\n                    req_change = True\n        else:\n            ret['result'] = False\n            ret['comment'] = \"The power policy must be specified.\"\n            return ret\n\n        if req_change:\n\n            update = __salt__['cimc.set_power_configuration'](policy,\n                                                              delayType,\n                                                              delayValue)\n\n            if update['outConfig']['biosVfResumeOnACPowerLoss'][0]['status'] != 'modified':\n                ret['result'] = False\n                ret['comment'] = \"Error setting power configuration.\"\n                return ret\n\n            ret['changes']['before'] = power_conf\n            ret['changes']['after'] = __salt__['cimc.get_power_configuration']()\n            ret['comment'] = \"Power settings modified.\"\n        else:\n            ret['comment'] = \"Power settings already configured. No changes required.\"\n\n    except Exception as err:\n        ret['result'] = False\n        ret['comment'] = \"Error occurred setting power settings.\"\n        log.error(err)\n        return ret\n\n    ret['result'] = True\n\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef syslog(name, primary=None, secondary=None):\n    '''\n    Ensures that the syslog servers are set to the specified values. A value of None will be ignored.\n\n    name: The name of the module function to execute.\n\n    primary(str): The IP address or FQDN of the primary syslog server.\n\n    secondary(str): The IP address or FQDN of the secondary syslog server.\n\n    SLS Example:\n\n    .. code-block:: yaml\n\n        syslog_configuration:\n          cimc.syslog:\n            - primary: 10.10.10.10\n            - secondary: foo.bar.com\n\n    '''\n    ret = _default_ret(name)\n\n    conf = __salt__['cimc.get_syslog']()\n\n    req_change = False\n\n    if primary:\n        prim_change = True\n        if 'outConfigs' in conf and 'commSyslogClient' in conf['outConfigs']:\n            for entry in conf['outConfigs']['commSyslogClient']:\n                if entry['name'] != 'primary':\n                    continue\n                if entry['adminState'] == 'enabled' and entry['hostname'] == primary:\n                    prim_change = False\n\n        if prim_change:\n            try:\n                update = __salt__['cimc.set_syslog_server'](primary, \"primary\")\n                if update['outConfig']['commSyslogClient'][0]['status'] == 'modified':\n                    req_change = True\n                else:\n                    ret['result'] = False\n                    ret['comment'] = \"Error setting primary SYSLOG server.\"\n                    return ret\n            except Exception as err:\n                ret['result'] = False\n                ret['comment'] = \"Error setting primary SYSLOG server.\"\n                log.error(err)\n                return ret\n\n    if secondary:\n        sec_change = True\n        if 'outConfig' in conf and 'commSyslogClient' in conf['outConfig']:\n            for entry in conf['outConfig']['commSyslogClient']:\n                if entry['name'] != 'secondary':\n                    continue\n                if entry['adminState'] == 'enabled' and entry['hostname'] == secondary:\n                    sec_change = False\n\n        if sec_change:\n            try:\n                update = __salt__['cimc.set_syslog_server'](secondary, \"secondary\")\n                if update['outConfig']['commSyslogClient'][0]['status'] == 'modified':\n                    req_change = True\n                else:\n                    ret['result'] = False\n                    ret['comment'] = \"Error setting secondary SYSLOG server.\"\n                    return ret\n            except Exception as err:\n                ret['result'] = False\n                ret['comment'] = \"Error setting secondary SYSLOG server.\"\n                log.error(err)\n                return ret\n\n    if req_change:\n        ret['changes']['before'] = conf\n        ret['changes']['after'] = __salt__['cimc.get_syslog']()\n        ret['comment'] = \"SYSLOG settings modified.\"\n    else:\n        ret['comment'] = \"SYSLOG already configured. No changes required.\"\n\n    ret['result'] = True\n\n    return ret", "response": "Sets the syslog servers for the specified module function."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef user(name, id='', user='', priv='', password='', status='active'):\n    '''\n    Ensures that a user is configured on the device. Due to being unable to\n    verify the user password. This is a forced operation.\n\n    .. versionadded:: 2019.2.0\n\n    name: The name of the module function to execute.\n\n    id(int): The user ID slot on the device.\n\n    user(str): The username of the user.\n\n    priv(str): The privilege level of the user.\n\n    password(str): The password of the user.\n\n    status(str): The status of the user. Can be either active or inactive.\n\n    SLS Example:\n\n    .. code-block:: yaml\n\n        user_configuration:\n          cimc.user:\n            - id: 11\n            - user: foo\n            - priv: admin\n            - password: mypassword\n            - status: active\n\n    '''\n\n    ret = _default_ret(name)\n\n    user_conf = __salt__['cimc.get_users']()\n\n    try:\n        for entry in user_conf['outConfigs']['aaaUser']:\n            if entry['id'] == str(id):\n                conf = entry\n\n        if not conf:\n            ret['result'] = False\n            ret['comment'] = \"Unable to find requested user id on device. Please verify id is valid.\"\n            return ret\n\n        updates = __salt__['cimc.set_user'](str(id), user, password, priv, status)\n\n        if 'outConfig' in updates:\n            ret['changes']['before'] = conf\n            ret['changes']['after'] = updates['outConfig']['aaaUser']\n            ret['comment'] = \"User settings modified.\"\n        else:\n            ret['result'] = False\n            ret['comment'] = \"Error setting user configuration.\"\n            return ret\n\n    except Exception as err:\n        ret['result'] = False\n        ret['comment'] = \"Error setting user configuration.\"\n        log.error(err)\n        return ret\n\n    ret['result'] = True\n\n    return ret", "response": "Ensures that a user is configured on the device."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninitializing the esxcluster proxy module", "response": "def init(opts):\n    '''\n    This function gets called when the proxy starts up. For\n    login\n    the protocol and port are cached.\n    '''\n    log.debug('Initting esxcluster proxy module in process %s', os.getpid())\n    log.debug('Validating esxcluster proxy input')\n    schema = EsxclusterProxySchema.serialize()\n    log.trace('schema = %s', schema)\n    proxy_conf = merge(opts.get('proxy', {}), __pillar__.get('proxy', {}))\n    log.trace('proxy_conf = %s', proxy_conf)\n    try:\n        jsonschema.validate(proxy_conf, schema)\n    except jsonschema.exceptions.ValidationError as exc:\n        raise salt.exceptions.InvalidConfigError(exc)\n\n    # Save mandatory fields in cache\n    for key in ('vcenter', 'datacenter', 'cluster', 'mechanism'):\n        DETAILS[key] = proxy_conf[key]\n\n    # Additional validation\n    if DETAILS['mechanism'] == 'userpass':\n        if 'username' not in proxy_conf:\n            raise salt.exceptions.InvalidConfigError(\n                'Mechanism is set to \\'userpass\\', but no '\n                '\\'username\\' key found in proxy config.')\n        if 'passwords' not in proxy_conf:\n            raise salt.exceptions.InvalidConfigError(\n                'Mechanism is set to \\'userpass\\', but no '\n                '\\'passwords\\' key found in proxy config.')\n        for key in ('username', 'passwords'):\n            DETAILS[key] = proxy_conf[key]\n    else:\n        if 'domain' not in proxy_conf:\n            raise salt.exceptions.InvalidConfigError(\n                'Mechanism is set to \\'sspi\\', but no '\n                '\\'domain\\' key found in proxy config.')\n        if 'principal' not in proxy_conf:\n            raise salt.exceptions.InvalidConfigError(\n                'Mechanism is set to \\'sspi\\', but no '\n                '\\'principal\\' key found in proxy config.')\n        for key in ('domain', 'principal'):\n            DETAILS[key] = proxy_conf[key]\n\n    # Save optional\n    DETAILS['protocol'] = proxy_conf.get('protocol')\n    DETAILS['port'] = proxy_conf.get('port')\n\n    # Test connection\n    if DETAILS['mechanism'] == 'userpass':\n        # Get the correct login details\n        log.debug('Retrieving credentials and testing vCenter connection for '\n                  'mehchanism \\'userpass\\'')\n        try:\n            username, password = find_credentials()\n            DETAILS['password'] = password\n        except salt.exceptions.SaltSystemExit as err:\n            log.critical('Error: %s', err)\n            return False\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef find_credentials():\n    '''\n    Cycle through all the possible credentials and return the first one that\n    works.\n    '''\n\n    # if the username and password were already found don't fo though the\n    # connection process again\n    if 'username' in DETAILS and 'password' in DETAILS:\n        return DETAILS['username'], DETAILS['password']\n\n    passwords = DETAILS['passwords']\n    for password in passwords:\n        DETAILS['password'] = password\n        if not __salt__['vsphere.test_vcenter_connection']():\n            # We are unable to authenticate\n            continue\n        # If we have data returned from above, we've successfully authenticated.\n        return DETAILS['username'], password\n    # We've reached the end of the list without successfully authenticating.\n    raise salt.exceptions.VMwareConnectionError('Cannot complete login due to '\n                                                'incorrect credentials.')", "response": "Find the first credentials that can be used for the virtual machine."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef targets(tgt, tgt_type='glob', **kwargs):\n    '''\n    Return the targets from the flat yaml file, checks opts for location but\n    defaults to /etc/salt/roster\n    '''\n    template = get_roster_file(__opts__)\n\n    rend = salt.loader.render(__opts__, {})\n    raw = compile_template(template,\n                           rend,\n                           __opts__['renderer'],\n                           __opts__['renderer_blacklist'],\n                           __opts__['renderer_whitelist'],\n                           mask_value='passw*',\n                           **kwargs)\n    conditioned_raw = {}\n    for minion in raw:\n        conditioned_raw[six.text_type(minion)] = salt.config.apply_sdb(raw[minion])\n    return __utils__['roster_matcher.targets'](conditioned_raw, tgt, tgt_type, 'ipv4')", "response": "Return the targets from the flat yaml file"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nexecuting salt - key", "response": "def run(self):\n        '''\n        Execute salt-key\n        '''\n        import salt.key\n        self.parse_args()\n\n        self.setup_logfile_logger()\n        verify_log(self.config)\n\n        key = salt.key.KeyCLI(self.config)\n        if check_user(self.config['user']):\n            key.run()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets a decrypted secret from the tISMd API", "response": "def get(key, service=None, profile=None):  # pylint: disable=W0613\n    '''\n    Get a decrypted secret from the tISMd API\n    '''\n\n    if not profile.get('url') or not profile.get('token'):\n        raise SaltConfigurationError(\"url and/or token missing from the tism sdb profile\")\n\n    request = {\"token\": profile['token'], \"encsecret\": key}\n\n    result = http.query(\n        profile['url'],\n        method='POST',\n        data=salt.utils.json.dumps(request),\n    )\n\n    decrypted = result.get('body')\n\n    if not decrypted:\n        log.warning(\n            'tism.get sdb decryption request failed with error %s',\n            result.get('error', 'unknown')\n        )\n        return 'ERROR' + six.text_type(result.get('status', 'unknown'))\n\n    return decrypted"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ext_pillar(minion_id,\n               pillar,\n               *args,\n               **kwargs):\n    '''\n    Execute queries against POSTGRES, merge and return as a dict\n    '''\n    return POSTGRESExtPillar().fetch(minion_id, pillar, *args, **kwargs)", "response": "Execute queries against POSTGRES merge and return as a dict\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nyields a POSTGRES cursor", "response": "def _get_cursor(self):\n        '''\n        Yield a POSTGRES cursor\n        '''\n        _options = self._get_options()\n        conn = psycopg2.connect(host=_options['host'],\n                                user=_options['user'],\n                                password=_options['pass'],\n                                dbname=_options['db'],\n                                port=_options['port'])\n        cursor = conn.cursor()\n        try:\n            yield cursor\n            log.debug('Connected to POSTGRES DB')\n        except psycopg2.DatabaseError as err:\n            log.exception('Error in ext_pillar POSTGRES: %s', err.args)\n        finally:\n            conn.close()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef extract_queries(self, args, kwargs):\n        '''\n            This function normalizes the config block into a set of queries we\n            can use.  The return is a list of consistently laid out dicts.\n        '''\n        return super(POSTGRESExtPillar, self).extract_queries(args, kwargs)", "response": "This function normalizes the config block into a set of queries we can use."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the cmdline of a Process instance.", "response": "def _get_proc_cmdline(proc):\n    '''\n    Returns the cmdline of a Process instance.\n\n    It's backward compatible with < 2.0 versions of psutil.\n    '''\n    try:\n        return salt.utils.data.decode(proc.cmdline() if PSUTIL2 else proc.cmdline)\n    except (psutil.NoSuchProcess, psutil.AccessDenied):\n        return []"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_proc_create_time(proc):\n    '''\n    Returns the create_time of a Process instance.\n\n    It's backward compatible with < 2.0 versions of psutil.\n    '''\n    try:\n        return salt.utils.data.decode(proc.create_time() if PSUTIL2 else proc.create_time)\n    except (psutil.NoSuchProcess, psutil.AccessDenied):\n        return None", "response": "Returns the create_time of a Process instance."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_proc_name(proc):\n    '''\n    Returns the name of a Process instance.\n\n    It's backward compatible with < 2.0 versions of psutil.\n    '''\n    try:\n        return salt.utils.data.decode(proc.name() if PSUTIL2 else proc.name)\n    except (psutil.NoSuchProcess, psutil.AccessDenied):\n        return []", "response": "Returns the name of a Process instance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the status of a Process instance.", "response": "def _get_proc_status(proc):\n    '''\n    Returns the status of a Process instance.\n\n    It's backward compatible with < 2.0 versions of psutil.\n    '''\n    try:\n        return salt.utils.data.decode(proc.status() if PSUTIL2 else proc.status)\n    except (psutil.NoSuchProcess, psutil.AccessDenied):\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_proc_username(proc):\n    '''\n    Returns the username of a Process instance.\n\n    It's backward compatible with < 2.0 versions of psutil.\n    '''\n    try:\n        return salt.utils.data.decode(proc.username() if PSUTIL2 else proc.username)\n    except (psutil.NoSuchProcess, psutil.AccessDenied, KeyError):\n        return None", "response": "Returns the username of a Process instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef top(num_processes=5, interval=3):\n    '''\n    Return a list of top CPU consuming processes during the interval.\n    num_processes = return the top N CPU consuming processes\n    interval = the number of seconds to sample CPU usage over\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' ps.top\n\n        salt '*' ps.top 5 10\n    '''\n    result = []\n    start_usage = {}\n    for pid in psutil.pids():\n        try:\n            process = psutil.Process(pid)\n            user, system = process.cpu_times()\n        except ValueError:\n            user, system, _, _ = process.cpu_times()\n        except psutil.NoSuchProcess:\n            continue\n        start_usage[process] = user + system\n    time.sleep(interval)\n    usage = set()\n    for process, start in six.iteritems(start_usage):\n        try:\n            user, system = process.cpu_times()\n        except ValueError:\n            user, system, _, _ = process.cpu_times()\n        except psutil.NoSuchProcess:\n            continue\n        now = user + system\n        diff = now - start\n        usage.add((diff, process))\n\n    for idx, (diff, process) in enumerate(reversed(sorted(usage))):\n        if num_processes and idx >= num_processes:\n            break\n        if not _get_proc_cmdline(process):\n            cmdline = _get_proc_name(process)\n        else:\n            cmdline = _get_proc_cmdline(process)\n        info = {'cmd': cmdline,\n                'user': _get_proc_username(process),\n                'status': _get_proc_status(process),\n                'pid': _get_proc_pid(process),\n                'create_time': _get_proc_create_time(process),\n                'cpu': {},\n                'mem': {},\n        }\n        for key, value in six.iteritems(process.cpu_times()._asdict()):\n            info['cpu'][key] = value\n        for key, value in six.iteritems(process.memory_info()._asdict()):\n            info['mem'][key] = value\n        result.append(info)\n\n    return result", "response": "Return a list of top N CPU consuming processes during the interval."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef proc_info(pid, attrs=None):\n    '''\n    Return a dictionary of information for a process id (PID).\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' ps.proc_info 2322\n        salt '*' ps.proc_info 2322 attrs='[\"pid\", \"name\"]'\n\n    pid\n        PID of process to query.\n\n    attrs\n        Optional list of desired process attributes.  The list of possible\n        attributes can be found here:\n        http://pythonhosted.org/psutil/#psutil.Process\n    '''\n    try:\n        proc = psutil.Process(pid)\n        return proc.as_dict(attrs)\n    except (psutil.NoSuchProcess, psutil.AccessDenied, AttributeError) as exc:\n        raise CommandExecutionError(exc)", "response": "Return a dictionary of information for a process id."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef kill_pid(pid, signal=15):\n    '''\n    Kill a process by PID.\n\n    .. code-block:: bash\n\n        salt 'minion' ps.kill_pid pid [signal=signal_number]\n\n    pid\n        PID of process to kill.\n\n    signal\n        Signal to send to the process. See manpage entry for kill\n        for possible values. Default: 15 (SIGTERM).\n\n    **Example:**\n\n    Send SIGKILL to process with PID 2000:\n\n    .. code-block:: bash\n\n        salt 'minion' ps.kill_pid 2000 signal=9\n    '''\n    try:\n        psutil.Process(pid).send_signal(signal)\n        return True\n    except psutil.NoSuchProcess:\n        return False", "response": "Kill a process by PID."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nkills processes matching a pattern and return a dict of pid and status.", "response": "def pkill(pattern, user=None, signal=15, full=False):\n    '''\n    Kill processes matching a pattern.\n\n    .. code-block:: bash\n\n        salt '*' ps.pkill pattern [user=username] [signal=signal_number] \\\\\n                [full=(true|false)]\n\n    pattern\n        Pattern to search for in the process list.\n\n    user\n        Limit matches to the given username. Default: All users.\n\n    signal\n        Signal to send to the process(es). See manpage entry for kill\n        for possible values. Default: 15 (SIGTERM).\n\n    full\n        A boolean value indicating whether only the name of the command or\n        the full command line should be matched against the pattern.\n\n    **Examples:**\n\n    Send SIGHUP to all httpd processes on all 'www' minions:\n\n    .. code-block:: bash\n\n        salt 'www.*' ps.pkill httpd signal=1\n\n    Send SIGKILL to all bash processes owned by user 'tom':\n\n    .. code-block:: bash\n\n        salt '*' ps.pkill bash signal=9 user=tom\n    '''\n\n    killed = []\n    for proc in psutil.process_iter():\n        name_match = pattern in ' '.join(_get_proc_cmdline(proc)) if full \\\n            else pattern in _get_proc_name(proc)\n        user_match = True if user is None else user == _get_proc_username(proc)\n        if name_match and user_match:\n            try:\n                proc.send_signal(signal)\n                killed.append(_get_proc_pid(proc))\n            except psutil.NoSuchProcess:\n                pass\n    if not killed:\n        return None\n    else:\n        return {'killed': killed}"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pgrep(pattern, user=None, full=False, pattern_is_regex=False):\n    '''\n    Return the pids for processes matching a pattern.\n\n    If full is true, the full command line is searched for a match,\n    otherwise only the name of the command is searched.\n\n    .. code-block:: bash\n\n        salt '*' ps.pgrep pattern [user=username] [full=(true|false)]\n\n    pattern\n        Pattern to search for in the process list.\n\n    user\n        Limit matches to the given username. Default: All users.\n\n    full\n        A boolean value indicating whether only the name of the command or\n        the full command line should be matched against the pattern.\n\n    pattern_is_regex\n        This flag enables ps.pgrep to mirror the regex search functionality\n         found in the pgrep command line utility.\n\n        .. versionadded:: Neon\n\n    **Examples:**\n\n    Find all httpd processes on all 'www' minions:\n\n    .. code-block:: bash\n\n        salt 'www.*' ps.pgrep httpd\n\n    Find all bash processes owned by user 'tom':\n\n    .. code-block:: bash\n\n        salt '*' ps.pgrep bash user=tom\n    '''\n    procs = []\n\n    if pattern_is_regex:\n        pattern = re.compile(str(pattern))\n\n    procs = []\n    for proc in psutil.process_iter():\n        if full:\n            process_line = ' '.join(_get_proc_cmdline(proc))\n        else:\n            process_line = _get_proc_name(proc)\n\n        if pattern_is_regex:\n            name_match = re.search(pattern, process_line)\n        else:\n            name_match = pattern in process_line\n\n        user_match = True if user is None else user == _get_proc_username(proc)\n\n        if name_match and user_match:\n            procs.append(_get_proc_pid(proc))\n\n    return procs or None", "response": "Return the pids for processes matching a pattern."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the percent of time the CPU is busy for each CPU.", "response": "def cpu_percent(interval=0.1, per_cpu=False):\n    '''\n    Return the percent of time the CPU is busy.\n\n    interval\n        the number of seconds to sample CPU usage over\n    per_cpu\n        if True return an array of CPU percent busy for each CPU, otherwise\n        aggregate all percents into one number\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' ps.cpu_percent\n    '''\n    if per_cpu:\n        result = list(psutil.cpu_percent(interval, True))\n    else:\n        result = psutil.cpu_percent(interval)\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cpu_times(per_cpu=False):\n    '''\n    Return the percent of time the CPU spends in each state,\n    e.g. user, system, idle, nice, iowait, irq, softirq.\n\n    per_cpu\n        if True return an array of percents for each CPU, otherwise aggregate\n        all percents into one number\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' ps.cpu_times\n    '''\n    if per_cpu:\n        result = [dict(times._asdict()) for times in psutil.cpu_times(True)]\n    else:\n        result = dict(psutil.cpu_times(per_cpu)._asdict())\n    return result", "response": "Return the percent of time the CPU spends in each state"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a dict that describes statistics about system memory usage.", "response": "def virtual_memory():\n    '''\n    .. versionadded:: 2014.7.0\n\n    Return a dict that describes statistics about system memory usage.\n\n    .. note::\n\n        This function is only available in psutil version 0.6.0 and above.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' ps.virtual_memory\n    '''\n    if psutil.version_info < (0, 6, 0):\n        msg = 'virtual_memory is only available in psutil 0.6.0 or greater'\n        raise CommandExecutionError(msg)\n    return dict(psutil.virtual_memory()._asdict())"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef swap_memory():\n    '''\n    .. versionadded:: 2014.7.0\n\n    Return a dict that describes swap memory statistics.\n\n    .. note::\n\n        This function is only available in psutil version 0.6.0 and above.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' ps.swap_memory\n    '''\n    if psutil.version_info < (0, 6, 0):\n        msg = 'swap_memory is only available in psutil 0.6.0 or greater'\n        raise CommandExecutionError(msg)\n    return dict(psutil.swap_memory()._asdict())", "response": "Return a dict that describes the current swap memory statistics."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef disk_partitions(all=False):\n    '''\n    Return a list of disk partitions and their device, mount point, and\n    filesystem type.\n\n    all\n        if set to False, only return local, physical partitions (hard disk,\n        USB, CD/DVD partitions).  If True, return all filesystems.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' ps.disk_partitions\n    '''\n    result = [dict(partition._asdict()) for partition in\n              psutil.disk_partitions(all)]\n    return result", "response": "Return a list of disk partitions and their device mount point and filesystem type."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef disk_partition_usage(all=False):\n    '''\n    Return a list of disk partitions plus the mount point, filesystem and usage\n    statistics.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' ps.disk_partition_usage\n    '''\n    result = disk_partitions(all)\n    for partition in result:\n        partition.update(disk_usage(partition['mountpoint']))\n    return result", "response": "Return a list of disk partitions plus the mount point filesystem and usage statistics."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef total_physical_memory():\n    '''\n    Return the total number of bytes of physical memory.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' ps.total_physical_memory\n    '''\n    if psutil.version_info < (0, 6, 0):\n        msg = 'virtual_memory is only available in psutil 0.6.0 or greater'\n        raise CommandExecutionError(msg)\n    try:\n        return psutil.virtual_memory().total\n    except AttributeError:\n        # TOTAL_PHYMEM is deprecated but with older psutil versions this is\n        # needed as a fallback.\n        return psutil.TOTAL_PHYMEM", "response": "Return the total number of bytes of physical memory."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the boot time in seconds since the epoch began.", "response": "def boot_time(time_format=None):\n    '''\n    Return the boot time in number of seconds since the epoch began.\n\n    CLI Example:\n\n    time_format\n        Optionally specify a `strftime`_ format string. Use\n        ``time_format='%c'`` to get a nicely-formatted locale specific date and\n        time (i.e. ``Fri May  2 19:08:32 2014``).\n\n        .. _strftime: https://docs.python.org/2/library/datetime.html#strftime-strptime-behavior\n\n        .. versionadded:: 2014.1.4\n\n    .. code-block:: bash\n\n        salt '*' ps.boot_time\n    '''\n    try:\n        b_time = int(psutil.boot_time())\n    except AttributeError:\n        # get_boot_time() has been removed in newer psutil versions, and has\n        # been replaced by boot_time() which provides the same information.\n        b_time = int(psutil.boot_time())\n    if time_format:\n        # Load epoch timestamp as a datetime.datetime object\n        b_time = datetime.datetime.fromtimestamp(b_time)\n        try:\n            return b_time.strftime(time_format)\n        except TypeError as exc:\n            raise SaltInvocationError('Invalid format string: {0}'.format(exc))\n    return b_time"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn network I/O statistics.", "response": "def network_io_counters(interface=None):\n    '''\n    Return network I/O statistics.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' ps.network_io_counters\n\n        salt '*' ps.network_io_counters interface=eth0\n    '''\n    if not interface:\n        return dict(psutil.net_io_counters()._asdict())\n    else:\n        stats = psutil.net_io_counters(pernic=True)\n        if interface in stats:\n            return dict(stats[interface]._asdict())\n        else:\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef disk_io_counters(device=None):\n    '''\n    Return disk I/O statistics.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' ps.disk_io_counters\n\n        salt '*' ps.disk_io_counters device=sda1\n    '''\n    if not device:\n        return dict(psutil.disk_io_counters()._asdict())\n    else:\n        stats = psutil.disk_io_counters(perdisk=True)\n        if device in stats:\n            return dict(stats[device]._asdict())\n        else:\n            return False", "response": "Return disk I/O statistics."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_users():\n    '''\n    Return logged-in users.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' ps.get_users\n    '''\n    try:\n        recs = psutil.users()\n        return [dict(x._asdict()) for x in recs]\n    except AttributeError:\n        # get_users is only present in psutil > v0.5.0\n        # try utmp\n        try:\n            import utmp  # pylint: disable=import-error\n\n            result = []\n            while True:\n                rec = utmp.utmpaccess.getutent()\n                if rec is None:\n                    return result\n                elif rec[0] == 7:\n                    started = rec[8]\n                    if isinstance(started, tuple):\n                        started = started[0]\n                    result.append({'name': rec[4], 'terminal': rec[2],\n                                   'started': started, 'host': rec[5]})\n        except ImportError:\n            return False", "response": "Return logged - in users."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nretrieves the lsof information of the given process name.", "response": "def lsof(name):\n    '''\n    Retrieve the lsof information of the given process name.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' ps.lsof apache2\n    '''\n    sanitize_name = six.text_type(name)\n    lsof_infos = __salt__['cmd.run'](\"lsof -c \" + sanitize_name)\n    ret = []\n    ret.extend([sanitize_name, lsof_infos])\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nretrieve the netstat information of the given process name.", "response": "def netstat(name):\n    '''\n    Retrieve the netstat information of the given process name.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' ps.netstat apache2\n    '''\n    sanitize_name = six.text_type(name)\n    netstat_infos = __salt__['cmd.run'](\"netstat -nap\")\n    found_infos = []\n    ret = []\n    for info in netstat_infos.splitlines():\n        if info.find(sanitize_name) != -1:\n            found_infos.append(info)\n    ret.extend([sanitize_name, found_infos])\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretrieves the ss information of the given process name.", "response": "def ss(name):\n    '''\n    Retrieve the ss information of the given process name.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' ps.ss apache2\n\n    .. versionadded:: 2016.11.6\n\n    '''\n    sanitize_name = six.text_type(name)\n    ss_infos = __salt__['cmd.run'](\"ss -neap\")\n    found_infos = []\n    ret = []\n    for info in ss_infos.splitlines():\n        if info.find(sanitize_name) != -1:\n            found_infos.append(info)\n    ret.extend([sanitize_name, found_infos])\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn information corresponding to a ps aux filtered by pattern.", "response": "def psaux(name):\n    '''\n    Retrieve information corresponding to a \"ps aux\" filtered\n    with the given pattern. It could be just a name or a regular\n    expression (using python search from \"re\" module).\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' ps.psaux www-data.+apache2\n    '''\n    sanitize_name = six.text_type(name)\n    pattern = re.compile(sanitize_name)\n    salt_exception_pattern = re.compile(\"salt.+ps.psaux.+\")\n    ps_aux = __salt__['cmd.run'](\"ps aux\")\n    found_infos = []\n    ret = []\n    nb_lines = 0\n    for info in ps_aux.splitlines():\n        found = pattern.search(info)\n        if found is not None:\n            # remove 'salt' command from results\n            if not salt_exception_pattern.search(info):\n                nb_lines += 1\n                found_infos.append(info)\n    pid_count = six.text_type(nb_lines) + \" occurence(s).\"\n    ret = []\n    ret.extend([sanitize_name, found_infos, pid_count])\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef present(profile='pagerduty', subdomain=None, api_key=None, **kwargs):\n    '''\n    Ensure pagerduty service exists.\n    This method accepts as arguments everything defined in\n    https://developer.pagerduty.com/documentation/rest/services/create\n\n    Note that many arguments are mutually exclusive, depending on the \"type\" argument.\n\n    Examples:\n\n    .. code-block:: yaml\n\n        # create a PagerDuty email service at test-email@DOMAIN.pagerduty.com\n        ensure generic email service exists:\n            pagerduty_service.present:\n                - name: my email service\n                - service:\n                    description: \"email service controlled by salt\"\n                    escalation_policy_id: \"my escalation policy\"\n                    type: \"generic_email\"\n                    service_key: \"test-email\"\n\n    .. code-block:: yaml\n\n        # create a pagerduty service using cloudwatch integration\n        ensure my cloudwatch service exists:\n            pagerduty_service.present:\n                - name: my cloudwatch service\n                - service:\n                    escalation_policy_id: \"my escalation policy\"\n                    type: aws_cloudwatch\n                    description: \"my cloudwatch service controlled by salt\"\n\n    '''\n    # TODO: aws_cloudwatch type should be integrated with boto_sns\n    # for convenience, we accept id, name, or email for users\n    # and we accept the id or name for schedules\n    kwargs['service']['name'] = kwargs['name']  # make args mirror PD API structure\n    escalation_policy_id = kwargs['service']['escalation_policy_id']\n    escalation_policy = __salt__['pagerduty_util.get_resource']('escalation_policies',\n                                                                escalation_policy_id,\n                                                                ['name', 'id'],\n                                                                profile=profile,\n                                                                subdomain=subdomain,\n                                                                api_key=api_key)\n    if escalation_policy:\n        kwargs['service']['escalation_policy_id'] = escalation_policy['id']\n    r = __salt__['pagerduty_util.resource_present']('services',\n                                                    ['name', 'id'],\n                                                    _diff,\n                                                    profile,\n                                                    subdomain,\n                                                    api_key,\n                                                    **kwargs)\n    return r", "response": "Ensure PagerDuty service exists and is up - to - date and is up - to - date with PagerDuty s API."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nensuring a pagerduty service does not exist.", "response": "def absent(profile='pagerduty', subdomain=None, api_key=None, **kwargs):\n    '''\n    Ensure a pagerduty service does not exist.\n    Name can be the service name or pagerduty service id.\n    '''\n    r = __salt__['pagerduty_util.resource_absent']('services',\n                                                   ['name', 'id'],\n                                                   profile,\n                                                   subdomain,\n                                                   api_key,\n                                                   **kwargs)\n    return r"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a dictionary or a list of callable functions that are available in the modules and states directories.", "response": "def modules_and_args(modules=True, states=False, names_only=False):\n    '''\n    Walk the Salt install tree and return a dictionary or a list\n    of the functions therein as well as their arguments.\n\n    :param modules: Walk the modules directory if True\n    :param states: Walk the states directory if True\n    :param names_only: Return only a list of the callable functions instead of a dictionary with arguments\n    :return: An OrderedDict with callable function names as keys and lists of arguments as\n             values (if ``names_only``==False) or simply an ordered list of callable\n             function nanes (if ``names_only``==True).\n\n    CLI Example:\n    (example truncated for brevity)\n\n    .. code-block:: bash\n\n        salt myminion baredoc.modules_and_args\n\n        myminion:\n            ----------\n        [...]\n            at.atrm:\n            at.jobcheck:\n            at.mod_watch:\n                - name\n            at.present:\n                - unique_tag\n                - name\n                - timespec\n                - job\n                - tag\n                - user\n            at.watch:\n                - unique_tag\n                - name\n                - timespec\n                - job\n                - tag\n                - user\n        [...]\n    '''\n    dirs = []\n    module_dir = os.path.dirname(os.path.realpath(__file__))\n    state_dir = os.path.join(os.path.dirname(module_dir), 'states')\n\n    if modules:\n        dirs.append(module_dir)\n    if states:\n        dirs.append(state_dir)\n\n    ret = _mods_with_args(dirs)\n    if names_only:\n        return sorted(ret.keys())\n    else:\n        return OrderedDict(sorted(ret.items()))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef output(data, **kwargs):  # pylint: disable=unused-argument\n    '''\n    Mane function\n    '''\n    high_out = __salt__['highstate'](data)\n    return subprocess.check_output(['ponysay', salt.utils.data.decode(high_out)])", "response": "Mane function\n    Mane function\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef apply_config(path, source=None, salt_env='base'):\n    r'''\n    Run an compiled DSC configuration (a folder containing a .mof file). The\n    folder can be cached from the salt master using the ``source`` option.\n\n    Args:\n\n        path (str): Local path to the directory that contains the .mof\n            configuration file to apply. Required.\n\n        source (str): Path to the directory that contains the .mof file on the\n            ``file_roots``. The source directory will be copied to the path\n            directory and then executed. If the path and source directories\n            differ, the source directory will be applied. If source is not\n            passed, the config located at ``path`` will be applied. Optional.\n\n        salt_env (str): The salt environment to use when copying your source.\n            Default is 'base'\n\n    Returns:\n        bool: True if successful, otherwise False\n\n    CLI Example:\n\n    To apply a config that already exists on the the system\n\n    .. code-block:: bash\n\n        salt '*' dsc.apply_config C:\\\\DSC\\\\WebSiteConfiguration\n\n    To cache a configuration from the master and apply it:\n\n    .. code-block:: bash\n\n        salt '*' dsc.apply_config C:\\\\DSC\\\\WebSiteConfiguration salt://dsc/configs/WebSiteConfiguration\n\n    '''\n    # If you're getting an error along the lines of \"The client cannot connect\n    # to the destination specified in the request.\", try the following:\n    # Enable-PSRemoting -SkipNetworkProfileCheck\n    config = path\n    if source:\n        # Make sure the folder names match\n        path_name = os.path.basename(os.path.normpath(path))\n        source_name = os.path.basename(os.path.normpath(source))\n        if path_name.lower() != source_name.lower():\n            # Append the Source name to the Path\n            path = '{0}\\\\{1}'.format(path, source_name)\n            log.debug('DSC: %s appended to the path.', source_name)\n\n        # Destination path minus the basename\n        dest_path = os.path.dirname(os.path.normpath(path))\n        log.info('DSC: Caching %s', source)\n        cached_files = __salt__['cp.get_dir'](source, dest_path, salt_env)\n        if not cached_files:\n            error = 'Failed to copy {0}'.format(source)\n            log.error('DSC: %s', error)\n            raise CommandExecutionError(error)\n        else:\n            config = os.path.dirname(cached_files[0])\n\n    # Make sure the path exists\n    if not os.path.exists(config):\n        error = '{0} not found'.format(config)\n        log.error('DSC: %s', error)\n        raise CommandExecutionError(error)\n\n    # Run the DSC Configuration\n    # Putting quotes around the parameter protects against command injection\n    cmd = 'Start-DscConfiguration -Path \"{0}\" -Wait -Force'.format(config)\n    _pshell(cmd)\n\n    cmd = '$status = Get-DscConfigurationStatus; $status.Status'\n    ret = _pshell(cmd)\n    log.info('DSC: Apply Config: %s', ret)\n\n    return ret == 'Success' or ret == {}", "response": "r This function will apply a configuration file to the system and returns True if successful False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the current DSC Configuration on the machine", "response": "def get_config():\n    '''\n    Get the current DSC Configuration\n\n    Returns:\n        dict: A dictionary representing the DSC Configuration on the machine\n\n    Raises:\n        CommandExecutionError: On failure\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' dsc.get_config\n    '''\n    cmd = 'Get-DscConfiguration | Select-Object * -ExcludeProperty Cim*'\n\n    try:\n        raw_config = _pshell(cmd, ignore_retcode=True)\n    except CommandExecutionError as exc:\n        if 'Current configuration does not exist' in exc.info['stderr']:\n            raise CommandExecutionError('Not Configured')\n        raise\n\n    config = dict()\n    if raw_config:\n        # Get DSC Configuration Name\n        if 'ConfigurationName' in raw_config[0]:\n            config[raw_config[0]['ConfigurationName']] = {}\n        # Add all DSC Configurations by ResourceId\n        for item in raw_config:\n            config[item['ConfigurationName']][item['ResourceId']] = {}\n            for key in item:\n                if key not in ['ConfigurationName', 'ResourceId']:\n                    config[item['ConfigurationName']][item['ResourceId']][key] = item[key]\n\n    return config"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef remove_config(reset=False):\n    '''\n    Remove the current DSC Configuration. Removes current, pending, and previous\n    dsc configurations.\n\n    .. versionadded:: 2017.7.5\n\n    Args:\n        reset (bool):\n            Attempts to reset the DSC configuration by removing the following\n            from ``C:\\\\Windows\\\\System32\\\\Configuration``:\n\n            - File: DSCStatusHistory.mof\n            - File: DSCEngineCache.mof\n            - Dir: ConfigurationStatus\n\n            Default is False\n\n            .. warning::\n                ``remove_config`` may fail to reset the DSC environment if any\n                of the files in the ``ConfigurationStatus`` directory. If you\n                wait a few minutes and run again, it may complete successfully.\n\n    Returns:\n        bool: True if successful\n\n    Raises:\n        CommandExecutionError: On failure\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' dsc.remove_config True\n    '''\n    # Stopping a running config (not likely to occur)\n    cmd = 'Stop-DscConfiguration'\n    log.info('DSC: Stopping Running Configuration')\n    try:\n        _pshell(cmd)\n    except CommandExecutionError as exc:\n        if exc.info['retcode'] != 0:\n            raise CommandExecutionError('Failed to Stop DSC Configuration',\n                                        info=exc.info)\n        log.info('DSC: %s', exc.info['stdout'])\n\n    # Remove configuration files\n    cmd = 'Remove-DscConfigurationDocument -Stage Current, Pending, Previous ' \\\n          '-Force'\n    log.info('DSC: Removing Configuration')\n    try:\n        _pshell(cmd)\n    except CommandExecutionError as exc:\n        if exc.info['retcode'] != 0:\n            raise CommandExecutionError('Failed to remove DSC Configuration',\n                                        info=exc.info)\n        log.info('DSC: %s', exc.info['stdout'])\n\n    if not reset:\n        return True\n\n    def _remove_fs_obj(path):\n        if os.path.exists(path):\n            log.info('DSC: Removing %s', path)\n            if not __salt__['file.remove'](path):\n                error = 'Failed to remove {0}'.format(path)\n                log.error('DSC: %s', error)\n                raise CommandExecutionError(error)\n\n    dsc_config_dir = '{0}\\\\System32\\\\Configuration' \\\n                     ''.format(os.getenv('SystemRoot', 'C:\\\\Windows'))\n\n    # Remove History\n    _remove_fs_obj('{0}\\\\DSCStatusHistory.mof'.format(dsc_config_dir))\n\n    # Remove Engine Cache\n    _remove_fs_obj('{0}\\\\DSCEngineCache.mof'.format(dsc_config_dir))\n\n    # Remove Status Directory\n    _remove_fs_obj('{0}\\\\ConfigurationStatus'.format(dsc_config_dir))\n\n    return True", "response": "Remove the current DSC Configuration. Removes the current pending and previous DSC configuration."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef restore_config():\n    '''\n    Reapplies the previous configuration.\n\n    .. versionadded:: 2017.7.5\n\n    .. note::\n        The current configuration will be come the previous configuration. If\n        run a second time back-to-back it is like toggling between two configs.\n\n    Returns:\n        bool: True if successfully restored\n\n    Raises:\n        CommandExecutionError: On failure\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' dsc.restore_config\n    '''\n    cmd = 'Restore-DscConfiguration'\n    try:\n        _pshell(cmd, ignore_retcode=True)\n    except CommandExecutionError as exc:\n        if 'A previous configuration does not exist' in exc.info['stderr']:\n            raise CommandExecutionError('Previous Configuration Not Found')\n        raise\n    return True", "response": "Restores the previous configuration."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the status of the current DSC Configuration on the machine", "response": "def get_config_status():\n    '''\n    Get the status of the current DSC Configuration\n\n    Returns:\n        dict: A dictionary representing the status of the current DSC\n            Configuration on the machine\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' dsc.get_config_status\n    '''\n    cmd = 'Get-DscConfigurationStatus | ' \\\n          'Select-Object -Property HostName, Status, MetaData, ' \\\n          '@{Name=\"StartDate\";Expression={Get-Date ($_.StartDate) -Format g}}, ' \\\n          'Type, Mode, RebootRequested, NumberofResources'\n    try:\n        return _pshell(cmd, ignore_retcode=True)\n    except CommandExecutionError as exc:\n        if 'No status information available' in exc.info['stderr']:\n            raise CommandExecutionError('Not Configured')\n        raise"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_lcm_config(config_mode=None,\n                   config_mode_freq=None,\n                   refresh_freq=None,\n                   reboot_if_needed=None,\n                   action_after_reboot=None,\n                   refresh_mode=None,\n                   certificate_id=None,\n                   configuration_id=None,\n                   allow_module_overwrite=None,\n                   debug_mode=False,\n                   status_retention_days=None):\n    '''\n    For detailed descriptions of the parameters see:\n    https://msdn.microsoft.com/en-us/PowerShell/DSC/metaConfig\n\n    config_mode (str): How the LCM applies the configuration. Valid values\n        are:\n\n        - ApplyOnly\n        - ApplyAndMonitor\n        - ApplyAndAutoCorrect\n\n    config_mode_freq (int): How often, in minutes, the current configuration\n        is checked and applied. Ignored if config_mode is set to ApplyOnly.\n        Default is 15.\n\n    refresh_mode (str): How the LCM gets configurations. Valid values are:\n\n        - Disabled\n        - Push\n        - Pull\n\n    refresh_freq (int): How often, in minutes, the LCM checks for updated\n        configurations. (pull mode only) Default is 30.\n\n    reboot_if_needed (bool): Reboot the machine if needed after a\n        configuration is applied. Default is False.\n\n    action_after_reboot (str): Action to take after reboot. Valid values\n        are:\n\n        - ContinueConfiguration\n        - StopConfiguration\n\n    certificate_id (guid): A GUID that specifies a certificate used to\n        access the configuration: (pull mode)\n\n    configuration_id (guid): A GUID that identifies the config file to get\n        from a pull server. (pull mode)\n\n    allow_module_overwrite (bool): New configs are allowed to overwrite old\n        ones on the target node.\n\n    debug_mode (str): Sets the debug level. Valid values are:\n\n        - None\n        - ForceModuleImport\n        - All\n\n    status_retention_days (int): Number of days to keep status of the\n        current config.\n\n    .. note::\n        Either ``config_mode_freq`` or ``refresh_freq`` needs to be a\n        multiple of the other. See documentation on MSDN for more details.\n\n    Returns:\n        bool: True if successful, otherwise False\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' dsc.set_lcm_config ApplyOnly\n    '''\n    temp_dir = os.getenv('TEMP', '{0}\\\\temp'.format(os.getenv('WINDIR')))\n    cmd = 'Configuration SaltConfig {'\n    cmd += '    Node localhost {'\n    cmd += '        LocalConfigurationManager {'\n    if config_mode:\n        if config_mode not in ('ApplyOnly', 'ApplyAndMonitor',\n                               'ApplyAndAutoCorrect'):\n            error = 'config_mode must be one of ApplyOnly, ApplyAndMonitor, ' \\\n                    'or ApplyAndAutoCorrect. Passed {0}'.format(config_mode)\n            raise SaltInvocationError(error)\n        cmd += '            ConfigurationMode = \"{0}\";'.format(config_mode)\n    if config_mode_freq:\n        if not isinstance(config_mode_freq, int):\n            error = 'config_mode_freq must be an integer. Passed {0}'.format(\n                config_mode_freq\n            )\n            raise SaltInvocationError(error)\n        cmd += '            ConfigurationModeFrequencyMins = {0};'.format(config_mode_freq)\n    if refresh_mode:\n        if refresh_mode not in ('Disabled', 'Push', 'Pull'):\n            raise SaltInvocationError(\n                'refresh_mode must be one of Disabled, Push, or Pull'\n            )\n        cmd += '            RefreshMode = \"{0}\";'.format(refresh_mode)\n    if refresh_freq:\n        if not isinstance(refresh_freq, int):\n            raise SaltInvocationError('refresh_freq must be an integer')\n        cmd += '            RefreshFrequencyMins = {0};'.format(refresh_freq)\n    if reboot_if_needed is not None:\n        if not isinstance(reboot_if_needed, bool):\n            raise SaltInvocationError('reboot_if_needed must be a boolean value')\n        if reboot_if_needed:\n            reboot_if_needed = '$true'\n        else:\n            reboot_if_needed = '$false'\n        cmd += '            RebootNodeIfNeeded = {0};'.format(reboot_if_needed)\n    if action_after_reboot:\n        if action_after_reboot not in ('ContinueConfiguration',\n                                       'StopConfiguration'):\n            raise SaltInvocationError(\n                'action_after_reboot must be one of '\n                'ContinueConfiguration or StopConfiguration'\n            )\n        cmd += '            ActionAfterReboot = \"{0}\"'.format(action_after_reboot)\n    if certificate_id is not None:\n        if certificate_id == '':\n            certificate_id = None\n        cmd += '            CertificateID = \"{0}\";'.format(certificate_id)\n    if configuration_id is not None:\n        if configuration_id == '':\n            configuration_id = None\n        cmd += '            ConfigurationID = \"{0}\";'.format(configuration_id)\n    if allow_module_overwrite is not None:\n        if not isinstance(allow_module_overwrite, bool):\n            raise SaltInvocationError('allow_module_overwrite must be a boolean value')\n        if allow_module_overwrite:\n            allow_module_overwrite = '$true'\n        else:\n            allow_module_overwrite = '$false'\n        cmd += '            AllowModuleOverwrite = {0};'.format(allow_module_overwrite)\n    if debug_mode is not False:\n        if debug_mode is None:\n            debug_mode = 'None'\n        if debug_mode not in ('None', 'ForceModuleImport', 'All'):\n            raise SaltInvocationError(\n                'debug_mode must be one of None, ForceModuleImport, '\n                'ResourceScriptBreakAll, or All'\n            )\n        cmd += '            DebugMode = \"{0}\";'.format(debug_mode)\n    if status_retention_days:\n        if not isinstance(status_retention_days, int):\n            raise SaltInvocationError('status_retention_days must be an integer')\n        cmd += '            StatusRetentionTimeInDays = {0};'.format(status_retention_days)\n    cmd += '        }}};'\n    cmd += r'SaltConfig -OutputPath \"{0}\\SaltConfig\"'.format(temp_dir)\n\n    # Execute Config to create the .mof\n    _pshell(cmd)\n\n    # Apply the config\n    cmd = r'Set-DscLocalConfigurationManager -Path \"{0}\\SaltConfig\"' \\\n          r''.format(temp_dir)\n    ret = __salt__['cmd.run_all'](cmd, shell='powershell', python_shell=True)\n    __salt__['file.remove'](r'{0}\\SaltConfig'.format(temp_dir))\n    if not ret['retcode']:\n        log.info('DSC: LCM config applied successfully')\n        return True\n    else:\n        log.error('DSC: Failed to apply LCM config. Error %s', ret)\n        return False", "response": "Sets the LCM configuration for the current node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngather lm - sensors data from a given chip", "response": "def sense(chip, fahrenheit=False):\n    '''\n    Gather lm-sensors data from a given chip\n\n    To determine the chip to query, use the 'sensors' command\n    and see the leading line in the block.\n\n    Example:\n\n    /usr/bin/sensors\n\n    coretemp-isa-0000\n    Adapter: ISA adapter\n    Physical id 0:  +56.0\u00b0C  (high = +87.0\u00b0C, crit = +105.0\u00b0C)\n    Core 0:         +52.0\u00b0C  (high = +87.0\u00b0C, crit = +105.0\u00b0C)\n    Core 1:         +50.0\u00b0C  (high = +87.0\u00b0C, crit = +105.0\u00b0C)\n    Core 2:         +56.0\u00b0C  (high = +87.0\u00b0C, crit = +105.0\u00b0C)\n    Core 3:         +53.0\u00b0C  (high = +87.0\u00b0C, crit = +105.0\u00b0C)\n\n    Given the above, the chip is 'coretemp-isa-0000'.\n    '''\n    extra_args = ''\n    if fahrenheit is True:\n        extra_args = '-f'\n    sensors = __salt__['cmd.run']('/usr/bin/sensors {0} {1}'.format(chip, extra_args), python_shell=False).splitlines()\n    ret = {}\n    for sensor in sensors:\n        sensor_list = sensor.split(':')\n        if len(sensor_list) >= 2:\n            ret[sensor_list[0]] = sensor_list[1].lstrip()\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts a duration string into XXhYYmZZs format", "response": "def convert_duration(duration):\n    '''\n    Convert the a duration string into XXhYYmZZs format\n\n    duration\n        Duration to convert\n\n    Returns: duration_string\n        String representation of duration in XXhYYmZZs format\n    '''\n\n    # durations must be specified in days, weeks or hours\n\n    if duration.endswith('h'):\n        hours = int(duration.split('h'))\n\n    elif duration.endswith('d'):\n        days = duration.split('d')\n        hours = int(days[0]) * 24\n\n    elif duration.endswith('w'):\n        weeks = duration.split('w')\n        hours = int(weeks[0]) * 24 * 7\n\n    duration_string = str(hours)+'h0m0s'\n    return duration_string"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef present(name, database, duration=\"7d\",\n            replication=1, default=False,\n            **client_args):\n    '''\n    Ensure that given retention policy is present.\n\n    name\n        Name of the retention policy to create.\n\n    database\n        Database to create retention policy on.\n    '''\n    ret = {'name': name,\n           'changes': {},\n           'result': True,\n           'comment': 'retention policy {0} is already present'.format(name)}\n\n    if not __salt__['influxdb.retention_policy_exists'](name=name,\n                                                        database=database,\n                                                        **client_args):\n        if __opts__['test']:\n            ret['result'] = None\n            ret['comment'] = ' {0} is absent and will be created'\\\n                .format(name)\n            return ret\n        if __salt__['influxdb.create_retention_policy'](\n            database, name,\n            duration, replication, default, **client_args\n        ):\n            ret['comment'] = 'retention policy {0} has been created'\\\n                .format(name)\n            ret['changes'][name] = 'Present'\n            return ret\n        else:\n            ret['comment'] = 'Failed to create retention policy {0}'\\\n                .format(name)\n            ret['result'] = False\n            return ret\n\n    else:\n        current_policy = __salt__['influxdb.get_retention_policy'](database=database, name=name)\n        update_policy = False\n        if current_policy['duration'] != convert_duration(duration):\n            update_policy = True\n            ret['changes']['duration'] = \"Retention changed from {0} to {1}.\".format(current_policy['duration'], duration)\n\n        if current_policy['replicaN'] != replication:\n            update_policy = True\n            ret['changes']['replication'] = \"Replication changed from {0} to {1}.\".format(current_policy['replicaN'], replication)\n\n        if current_policy['default'] != default:\n            update_policy = True\n            ret['changes']['default'] = \"Default changed from {0} to {1}.\".format(current_policy['default'], default)\n\n        if update_policy:\n            if __opts__['test']:\n                ret['result'] = None\n                ret['comment'] = ' {0} is present and set to be changed'\\\n                    .format(name)\n                return ret\n            else:\n                if __salt__['influxdb.alter_retention_policy'](\n                    database, name,\n                    duration, replication, default, **client_args\n                ):\n                    ret['comment'] = 'retention policy {0} has been changed'\\\n                        .format(name)\n                    return ret\n                else:\n                    ret['comment'] = 'Failed to update retention policy {0}'\\\n                        .format(name)\n                    ret['result'] = False\n                    return ret\n\n    return ret", "response": "Ensure that the given retention policy is present on the given database."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a dictionary of only the subset of keys and values specified in keys", "response": "def _dict_subset(keys, master_dict):\n    '''\n    Return a dictionary of only the subset of keys/values specified in keys\n    '''\n    return dict([(k, v) for k, v in six.iteritems(master_dict) if k in keys])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fire_master(data, tag, preload=None, timeout=60):\n    '''\n    Fire an event off up to the master server\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' event.fire_master '{\"data\":\"my event data\"}' 'tag'\n    '''\n    if (__opts__.get('local', None) or __opts__.get('file_client', None) == 'local') and not __opts__.get('use_master_when_local', False):\n        #  We can't send an event if we're in masterless mode\n        log.warning('Local mode detected. Event with tag %s will NOT be sent.', tag)\n        return False\n\n    if preload or __opts__.get('__cli') == 'salt-call':\n        # If preload is specified, we must send a raw event (this is\n        # slower because it has to independently authenticate)\n        if 'master_uri' not in __opts__:\n            __opts__['master_uri'] = 'tcp://{ip}:{port}'.format(\n                    ip=salt.utils.zeromq.ip_bracket(__opts__['interface']),\n                    port=__opts__.get('ret_port', '4506')  # TODO, no fallback\n                    )\n        masters = list()\n        ret = None\n        if 'master_uri_list' in __opts__:\n            for master_uri in __opts__['master_uri_list']:\n                masters.append(master_uri)\n        else:\n            masters.append(__opts__['master_uri'])\n        auth = salt.crypt.SAuth(__opts__)\n        load = {'id': __opts__['id'],\n                'tag': tag,\n                'data': data,\n                'tok': auth.gen_token(b'salt'),\n                'cmd': '_minion_event'}\n\n        if isinstance(preload, dict):\n            load.update(preload)\n\n        for master in masters:\n            channel = salt.transport.client.ReqChannel.factory(__opts__, master_uri=master)\n            try:\n                channel.send(load, timeout=timeout)\n                # channel.send was successful.\n                # Ensure ret is True.\n                ret = True\n            except Exception:\n                # only set a False ret if it hasn't been sent atleast once\n                if ret is None:\n                    ret = False\n            finally:\n                channel.close()\n        return ret\n    else:\n        # Usually, we can send the event via the minion, which is faster\n        # because it is already authenticated\n        try:\n            me = salt.utils.event.MinionEvent(__opts__, listen=False, keep_loop=True)\n            return me.fire_event({'data': data, 'tag': tag, 'events': None, 'pretag': None}, 'fire_master')\n        except Exception:\n            exc_type, exc_value, exc_traceback = sys.exc_info()\n            lines = traceback.format_exception(exc_type, exc_value, exc_traceback)\n            log.debug(lines)\n            return False", "response": "Fire an event off up to the master server"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fire(data, tag, timeout=None):\n    '''\n    Fire an event on the local minion event bus. Data must be formed as a dict.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' event.fire '{\"data\":\"my event data\"}' 'tag'\n    '''\n    if timeout is None:\n        timeout = 60000\n    else:\n        timeout = timeout * 1000\n    try:\n        event = salt.utils.event.get_event(__opts__.get('__role', 'minion'),\n                                           sock_dir=__opts__['sock_dir'],\n                                           transport=__opts__['transport'],\n                                           opts=__opts__,\n                                           keep_loop=True,\n                                           listen=False)\n\n        return event.fire_event(data, tag, timeout=timeout)\n    except Exception:\n        exc_type, exc_value, exc_traceback = sys.exc_info()\n        lines = traceback.format_exception(exc_type, exc_value, exc_traceback)\n        log.debug(lines)\n        return False", "response": "Fire an event on the local minion event bus."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsend an event to the Master", "response": "def send(tag,\n        data=None,\n        preload=None,\n        with_env=False,\n        with_grains=False,\n        with_pillar=False,\n        with_env_opts=False,\n        timeout=60,\n        **kwargs):\n    '''\n    Send an event to the Salt Master\n\n    .. versionadded:: 2014.7.0\n\n    :param tag: A tag to give the event.\n        Use slashes to create a namespace for related events. E.g.,\n        ``myco/build/buildserver1/start``, ``myco/build/buildserver1/success``,\n        ``myco/build/buildserver1/failure``.\n\n    :param data: A dictionary of data to send in the event.\n        This is free-form. Send any data points that are needed for whoever is\n        consuming the event. Arguments on the CLI are interpreted as YAML so\n        complex data structures are possible.\n\n    :param with_env: Include environment variables from the current shell\n        environment in the event data as ``environ``.. This is a short-hand for\n        working with systems that seed the environment with relevant data such\n        as Jenkins.\n    :type with_env: Specify ``True`` to include all environment variables, or\n        specify a list of strings of variable names to include.\n\n    :param with_grains: Include grains from the current minion in the event\n        data as ``grains``.\n    :type with_grains: Specify ``True`` to include all grains, or specify a\n        list of strings of grain names to include.\n\n    :param with_pillar: Include Pillar values from the current minion in the\n        event data as ``pillar``. Remember Pillar data is often sensitive data\n        so be careful. This is useful for passing ephemeral Pillar values\n        through an event. Such as passing the ``pillar={}`` kwarg in\n        :py:func:`state.sls <salt.modules.state.sls>` from the Master, through\n        an event on the Minion, then back to the Master.\n    :type with_pillar: Specify ``True`` to include all Pillar values, or\n        specify a list of strings of Pillar keys to include. It is a\n        best-practice to only specify a relevant subset of Pillar data.\n\n    :param with_env_opts: Include ``saltenv`` and ``pillarenv`` set on minion\n        at the moment when event is send into event data.\n    :type with_env_opts: Specify ``True`` to include ``saltenv`` and\n        ``pillarenv`` values or ``False`` to omit them.\n\n    :param timeout: maximum duration to wait to connect to Salt's\n        IPCMessageServer in seconds. Defaults to 60s\n\n    :param kwargs: Any additional keyword arguments passed to this function\n        will be interpreted as key-value pairs and included in the event data.\n        This provides a convenient alternative to YAML for simple values.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call event.send myco/mytag foo=Foo bar=Bar\n        salt-call event.send 'myco/mytag' '{foo: Foo, bar: Bar}'\n\n    A convenient way to allow Jenkins to execute ``salt-call`` is via sudo. The\n    following rule in sudoers will allow the ``jenkins`` user to run only the\n    following command.\n\n    ``/etc/sudoers`` (allow preserving the environment):\n\n    .. code-block:: text\n\n        jenkins ALL=(ALL) NOPASSWD:SETENV: /usr/bin/salt-call event.send*\n\n    Call Jenkins via sudo (preserve the environment):\n\n    .. code-block:: bash\n\n        sudo -E salt-call event.send myco/jenkins/build/success with_env='[BUILD_ID, BUILD_URL, GIT_BRANCH, GIT_COMMIT]'\n\n    '''\n    data_dict = {}\n\n    if with_env:\n        if isinstance(with_env, list):\n            data_dict['environ'] = _dict_subset(with_env, dict(os.environ))\n        else:\n            data_dict['environ'] = dict(os.environ)\n\n    if with_grains:\n        if isinstance(with_grains, list):\n            data_dict['grains'] = _dict_subset(with_grains, __grains__)\n        else:\n            data_dict['grains'] = __grains__\n\n    if with_pillar:\n        if isinstance(with_pillar, list):\n            data_dict['pillar'] = _dict_subset(with_pillar, __pillar__)\n        else:\n            data_dict['pillar'] = __pillar__\n\n    if with_env_opts:\n        data_dict['saltenv'] = __opts__.get('saltenv', 'base')\n        data_dict['pillarenv'] = __opts__.get('pillarenv')\n\n    if kwargs:\n        data_dict.update(kwargs)\n\n    # Allow values in the ``data`` arg to override any of the above values.\n    if isinstance(data, collections.Mapping):\n        data_dict.update(data)\n\n    if __opts__.get('local') or __opts__.get('file_client') == 'local' or __opts__.get('master_type') == 'disable':\n        return fire(data_dict, tag, timeout=timeout)\n    else:\n        return fire_master(data_dict, tag, preload=preload, timeout=timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef present(version,\n            name,\n            port=None,\n            encoding=None,\n            locale=None,\n            datadir=None,\n            allow_group_access=None,\n            data_checksums=None,\n            wal_segsize=None\n            ):\n    '''\n    Ensure that the named cluster is present with the specified properties.\n    For more information about all of these options see man pg_createcluster(1)\n\n    version\n        Version of the postgresql cluster\n\n    name\n        The name of the cluster\n\n    port\n        Cluster port\n\n    encoding\n        The character encoding scheme to be used in this database\n\n    locale\n        Locale with which to create cluster\n\n    datadir\n        Where the cluster is stored\n\n    allow_group_access\n        Allows users in the same group as the cluster owner to read all cluster files created by initdb\n\n    data_checksums\n        Use checksums on data pages\n\n    wal_segsize\n        Set the WAL segment size, in megabytes\n\n        .. versionadded:: 2015.XX\n    '''\n    msg = 'Cluster {0}/{1} is already present'.format(version, name)\n    ret = {'name': name,\n           'changes': {},\n           'result': True,\n           'comment': msg}\n\n    if __salt__['postgres.cluster_exists'](version, name):\n        # check cluster config is correct\n        infos = __salt__['postgres.cluster_list'](verbose=True)\n        info = infos['{0}/{1}'.format(version, name)]\n        # TODO: check locale en encoding configs also\n        if any((port != info['port'] if port else False,\n                datadir != info['datadir'] if datadir else False,)):\n            ret['comment'] = 'Cluster {0}/{1} has wrong parameters ' \\\n                             'which couldn\\'t be changed on fly.' \\\n                                 .format(version, name)\n            ret['result'] = False\n        return ret\n\n    # The cluster is not present, add it!\n    if __opts__.get('test'):\n        ret['result'] = None\n        msg = 'Cluster {0}/{1} is set to be created'\n        ret['comment'] = msg.format(version, name)\n        return ret\n    cluster = __salt__['postgres.cluster_create'](\n        version=version,\n        name=name,\n        port=port,\n        locale=locale,\n        encoding=encoding,\n        datadir=datadir,\n        allow_group_access=allow_group_access,\n        data_checksums=data_checksums,\n        wal_segsize=wal_segsize)\n    if cluster:\n        msg = 'The cluster {0}/{1} has been created'\n        ret['comment'] = msg.format(version, name)\n        ret['changes']['{0}/{1}'.format(version, name)] = 'Present'\n    else:\n        msg = 'Failed to create cluster {0}/{1}'\n        ret['comment'] = msg.format(version, name)\n        ret['result'] = False\n    return ret", "response": "Ensure that the named cluster is present with the specified properties."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nensure that the named cluster is absent", "response": "def absent(version,\n           name):\n    '''\n    Ensure that the named cluster is absent\n\n    version\n        Version of the postgresql server of the cluster to remove\n\n    name\n        The name of the cluster to remove\n\n        .. versionadded:: 2015.XX\n    '''\n    ret = {'name': name,\n           'changes': {},\n           'result': True,\n           'comment': ''}\n\n    #check if cluster exists and remove it\n    if __salt__['postgres.cluster_exists'](version, name):\n        if __opts__.get('test'):\n            ret['result'] = None\n            msg = 'Cluster {0}/{1} is set to be removed'\n            ret['comment'] = msg.format(version, name)\n            return ret\n        if __salt__['postgres.cluster_remove'](version, name, True):\n            msg = 'Cluster {0}/{1} has been removed'\n            ret['comment'] = msg.format(version, name)\n            ret['changes'][name] = 'Absent'\n            return ret\n\n    # fallback\n    ret['comment'] = 'Cluster {0}/{1} is not present, so it cannot ' \\\n                     'be removed'.format(version, name)\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _send_splunk(event, index_override=None, sourcetype_override=None):\n    '''\n    Send the results to Splunk.\n    Requires the Splunk HTTP Event Collector running on port 8088.\n    This is available on Splunk Enterprise version 6.3 or higher.\n\n    '''\n    # Get Splunk Options\n    opts = _get_options()\n    log.info(str('Options: %s'),  # future lint: disable=blacklisted-function\n             salt.utils.json.dumps(opts))\n    http_event_collector_key = opts['token']\n    http_event_collector_host = opts['indexer']\n    # Set up the collector\n    splunk_event = http_event_collector(http_event_collector_key, http_event_collector_host)\n    # init the payload\n    payload = {}\n\n    # Set up the event metadata\n    if index_override is None:\n        payload.update({\"index\": opts['index']})\n    else:\n        payload.update({\"index\": index_override})\n    if sourcetype_override is None:\n        payload.update({\"sourcetype\": opts['sourcetype']})\n    else:\n        payload.update({\"index\": sourcetype_override})\n\n    # Add the event\n    payload.update({\"event\": event})\n    log.info(str('Payload: %s'),  # future lint: disable=blacklisted-function\n             salt.utils.json.dumps(payload))\n    # Fire it off\n    splunk_event.sendEvent(payload)\n    return True", "response": "Send the event to Splunk."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef output(data, **kwargs):  # pylint: disable=unused-argument\n    '''\n    Print the output data in JSON\n    '''\n    try:\n        dump_opts = {'indent': 4, 'default': repr}\n\n        if 'output_indent' in __opts__:\n\n            indent = __opts__.get('output_indent')\n            sort_keys = False\n\n            if indent == 'pretty':\n                indent = 4\n                sort_keys = True\n\n            elif isinstance(indent, six.integer_types):\n                if indent >= 0:\n                    indent = indent\n                else:\n                    indent = None\n\n            dump_opts['indent'] = indent\n            dump_opts['sort_keys'] = sort_keys\n\n        return dson.dumps(data, **dump_opts)\n\n    except UnicodeDecodeError as exc:\n        log.error('Unable to serialize output to dson')\n        return dson.dumps(\n            {'error': 'Unable to serialize output to DSON',\n             'message': six.text_type(exc)}\n        )\n\n    except TypeError:\n        log.debug('An error occurred while outputting DSON', exc_info=True)\n    # Return valid JSON for unserializable objects\n    return dson.dumps({})", "response": "Print the output data in JSON\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_salt_params():\n    '''\n    Try to get all sort of parameters for Server Density server info.\n\n    NOTE: Missing publicDNS and publicIPs parameters. There might be way of\n    getting them with salt-cloud.\n    '''\n    all_stats = __salt__['status.all_status']()\n    all_grains = __salt__['grains.items']()\n    params = {}\n    try:\n        params['name'] = all_grains['id']\n        params['hostname'] = all_grains['host']\n        if all_grains['kernel'] == 'Darwin':\n            sd_os = {'code': 'mac', 'name': 'Mac'}\n        else:\n            sd_os = {'code': all_grains['kernel'].lower(), 'name': all_grains['kernel']}\n        params['os'] = salt.utils.json.dumps(sd_os)\n        params['cpuCores'] = all_stats['cpuinfo']['cpu cores']\n        params['installedRAM'] = six.text_type(int(all_stats['meminfo']['MemTotal']['value']) / 1024)\n        params['swapSpace'] = six.text_type(int(all_stats['meminfo']['SwapTotal']['value']) / 1024)\n        params['privateIPs'] = salt.utils.json.dumps(all_grains['fqdn_ip4'])\n        params['privateDNS'] = salt.utils.json.dumps(all_grains['fqdn'])\n    except KeyError:\n        pass\n\n    return params", "response": "Get all sort of parameters for Server Density server info."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns an sqlite connection", "response": "def _conn(queue):\n    '''\n    Return an sqlite connection\n    '''\n    queue_dir = __opts__['sqlite_queue_dir']\n    db = os.path.join(queue_dir, '{0}.db'.format(queue))\n    log.debug('Connecting to: %s', db)\n\n    con = sqlite3.connect(db)\n    tables = _list_tables(con)\n    if queue not in tables:\n        _create_table(con, queue)\n    return con"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _list_queues():\n    '''\n    Return a list of sqlite databases in the queue_dir\n    '''\n    queue_dir = __opts__['sqlite_queue_dir']\n    files = os.path.join(queue_dir, '*.db')\n    paths = glob.glob(files)\n    queues = [os.path.splitext(os.path.basename(item))[0] for item in paths]\n\n    return queues", "response": "Return a list of sqlite databases in the queue_dir\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlists contents of a queue", "response": "def list_items(queue):\n    '''\n    List contents of a queue\n    '''\n    itemstuple = _list_items(queue)\n    items = [item[0] for item in itemstuple]\n    return items"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _quote_escape(item):\n    '''\n    Make sure single quotes are escaped properly in sqlite3 fashion.\n    e.g.: ' becomes ''\n    '''\n\n    rex_sqlquote = re.compile(\"'\", re.M)\n\n    return rex_sqlquote.sub(\"''\", item)", "response": "Escape single quotes in sqlite3 fashion."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndeleting an item or items from a queue", "response": "def delete(queue, items):\n    '''\n    Delete an item or items from a queue\n    '''\n    con = _conn(queue)\n    with con:\n        cur = con.cursor()\n        if isinstance(items, six.string_types):\n            items = _quote_escape(items)\n            cmd = \"\"\"DELETE FROM {0} WHERE name = '{1}'\"\"\".format(queue, items)\n            log.debug('SQL Query: %s', cmd)\n            cur.execute(cmd)\n            return True\n        if isinstance(items, list):\n            items = [_quote_escape(el) for el in items]\n            cmd = 'DELETE FROM {0} WHERE name = ?'.format(queue)\n            log.debug('SQL Query: %s', cmd)\n            newitems = []\n            for item in items:\n                newitems.append((item,))\n                # we need a list of one item tuples here\n            cur.executemany(cmd, newitems)\n        if isinstance(items, dict):\n            items = salt.utils.json.dumps(items).replace('\"', \"'\")\n            items = _quote_escape(items)\n            cmd = (\"\"\"DELETE FROM {0} WHERE name = '{1}'\"\"\").format(queue, items)  # future lint: disable=blacklisted-function\n            log.debug('SQL Query: %s', cmd)\n            cur.execute(cmd)\n            return True\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\npopping one or more items from the queue return them.", "response": "def pop(queue, quantity=1, is_runner=False):\n    '''\n    Pop one or more or all items from the queue return them.\n    '''\n    cmd = 'SELECT name FROM {0}'.format(queue)\n    if quantity != 'all':\n        try:\n            quantity = int(quantity)\n        except ValueError as exc:\n            error_txt = ('Quantity must be an integer or \"all\".\\n'\n                         'Error: \"{0}\".'.format(exc))\n            raise SaltInvocationError(error_txt)\n        cmd = ''.join([cmd, ' LIMIT {0}'.format(quantity)])\n    log.debug('SQL Query: %s', cmd)\n    con = _conn(queue)\n    items = []\n    with con:\n        cur = con.cursor()\n        result = cur.execute(cmd).fetchall()\n        if result:\n            items = [item[0] for item in result]\n            itemlist = '\",\"'.join(items)\n            _quote_escape(itemlist)\n            del_cmd = '''DELETE FROM {0} WHERE name IN (\"{1}\")'''.format(\n                queue, itemlist)\n\n            log.debug('SQL Query: %s', del_cmd)\n\n            cur.execute(del_cmd)\n        con.commit()\n    if is_runner:\n        items = [salt.utils.json.loads(item[0].replace(\"'\", '\"')) for item in result]\n    log.info(items)\n    return items"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_all_topics(region=None, key=None, keyid=None, profile=None):\n    '''\n    Returns a list of the all topics..\n\n    CLI example::\n\n        salt myminion boto_sns.get_all_topics\n    '''\n    cache_key = _cache_get_key()\n    try:\n        return __context__[cache_key]\n    except KeyError:\n        pass\n\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n    __context__[cache_key] = {}\n    # TODO: support >100 SNS topics (via NextToken)\n    topics = conn.get_all_topics()\n    for t in topics['ListTopicsResponse']['ListTopicsResult']['Topics']:\n        short_name = t['TopicArn'].split(':')[-1]\n        __context__[cache_key][short_name] = t['TopicArn']\n    return __context__[cache_key]", "response": "Returns a list of the all topics.. boto_sns. get_all_topics"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef exists(name, region=None, key=None, keyid=None, profile=None):\n    '''\n    Check to see if an SNS topic exists.\n\n    CLI example::\n\n        salt myminion boto_sns.exists mytopic region=us-east-1\n    '''\n    topics = get_all_topics(region=region, key=key, keyid=keyid,\n                            profile=profile)\n    if name.startswith('arn:aws:sns:'):\n        return name in list(topics.values())\n    else:\n        return name in list(topics.keys())", "response": "Check to see if an SNS topic exists."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates an SNS topic.", "response": "def create(name, region=None, key=None, keyid=None, profile=None):\n    '''\n    Create an SNS topic.\n\n    CLI example to create a topic::\n\n        salt myminion boto_sns.create mytopic region=us-east-1\n    '''\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n    conn.create_topic(name)\n    log.info('Created SNS topic %s', name)\n    _invalidate_cache()\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef delete(name, region=None, key=None, keyid=None, profile=None):\n    '''\n    Delete an SNS topic.\n\n    CLI example to delete a topic::\n\n        salt myminion boto_sns.delete mytopic region=us-east-1\n    '''\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n    conn.delete_topic(get_arn(name, region, key, keyid, profile))\n    log.info('Deleted SNS topic %s', name)\n    _invalidate_cache()\n    return True", "response": "Delete an SNS topic"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_all_subscriptions_by_topic(name, region=None, key=None, keyid=None, profile=None):\n    '''\n    Get list of all subscriptions to a specific topic.\n\n    CLI example to delete a topic::\n\n        salt myminion boto_sns.get_all_subscriptions_by_topic mytopic region=us-east-1\n    '''\n    cache_key = _subscriptions_cache_key(name)\n    try:\n        return __context__[cache_key]\n    except KeyError:\n        pass\n\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n    ret = conn.get_all_subscriptions_by_topic(get_arn(name, region, key, keyid, profile))\n    __context__[cache_key] = ret['ListSubscriptionsByTopicResponse']['ListSubscriptionsByTopicResult']['Subscriptions']\n    return __context__[cache_key]", "response": "Get all subscriptions to a specific topic."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef subscribe(topic, protocol, endpoint, region=None, key=None, keyid=None, profile=None):\n    '''\n    Subscribe to a Topic.\n\n    CLI example to delete a topic::\n\n        salt myminion boto_sns.subscribe mytopic https https://www.example.com/sns-endpoint region=us-east-1\n    '''\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n    conn.subscribe(get_arn(topic, region, key, keyid, profile), protocol, endpoint)\n    log.info('Subscribe %s %s to %s topic', protocol, endpoint, topic)\n    try:\n        del __context__[_subscriptions_cache_key(topic)]\n    except KeyError:\n        pass\n    return True", "response": "Subscribe to a topic."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef unsubscribe(topic, subscription_arn, region=None, key=None, keyid=None, profile=None):\n    '''\n    Unsubscribe a specific SubscriptionArn of a topic.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_sns.unsubscribe my_topic my_subscription_arn region=us-east-1\n\n    .. versionadded:: 2016.11.0\n    '''\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n\n    if subscription_arn.startswith('arn:aws:sns:') is False:\n        return False\n\n    try:\n        conn.unsubscribe(subscription_arn)\n        log.info('Unsubscribe %s to %s topic', subscription_arn, topic)\n    except Exception as e:\n        log.error('Unsubscribe Error', exc_info=True)\n        return False\n    else:\n        __context__.pop(_subscriptions_cache_key(topic), None)\n        return True", "response": "Unsubscribe a specific SubscriptionArn of a topic."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_arn(name, region=None, key=None, keyid=None, profile=None):\n    '''\n    Returns the full ARN for a given topic name.\n\n    CLI example::\n\n        salt myminion boto_sns.get_arn mytopic\n    '''\n    if name.startswith('arn:aws:sns:'):\n        return name\n\n    account_id = __salt__['boto_iam.get_account_id'](\n        region=region, key=key, keyid=keyid, profile=profile\n    )\n    return 'arn:aws:sns:{0}:{1}:{2}'.format(_get_region(region, profile),\n                                            account_id, name)", "response": "Get the full ARN for a given topic name."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef current_ioloop(io_loop):\n    '''\n    A context manager that will set the current ioloop to io_loop for the context\n    '''\n    orig_loop = tornado.ioloop.IOLoop.current()\n    io_loop.make_current()\n    try:\n        yield\n    finally:\n        orig_loop.make_current()", "response": "A context manager that will set the current ioloop to io_loop for the context\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_top_file_envs():\n    '''\n    Get all environments from the top file\n    '''\n    try:\n        return __context__['saltutil._top_file_envs']\n    except KeyError:\n        try:\n            st_ = salt.state.HighState(__opts__,\n                                       initial_pillar=__pillar__)\n            top = st_.get_top()\n            if top:\n                envs = list(st_.top_matches(top).keys()) or 'base'\n            else:\n                envs = 'base'\n        except SaltRenderError as exc:\n            raise CommandExecutionError(\n                'Unable to render top file(s): {0}'.format(exc)\n            )\n        __context__['saltutil._top_file_envs'] = envs\n        return envs", "response": "Get all environments from the top file"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _sync(form, saltenv=None, extmod_whitelist=None, extmod_blacklist=None):\n    '''\n    Sync the given directory in the given environment\n    '''\n    if saltenv is None:\n        saltenv = _get_top_file_envs()\n    if isinstance(saltenv, six.string_types):\n        saltenv = saltenv.split(',')\n    ret, touched = salt.utils.extmods.sync(__opts__, form, saltenv=saltenv, extmod_whitelist=extmod_whitelist,\n                                           extmod_blacklist=extmod_blacklist)\n    # Dest mod_dir is touched? trigger reload if requested\n    if touched:\n        mod_file = os.path.join(__opts__['cachedir'], 'module_refresh')\n        with salt.utils.files.fopen(mod_file, 'a'):\n            pass\n    if form == 'grains' and \\\n       __opts__.get('grains_cache') and \\\n       os.path.isfile(os.path.join(__opts__['cachedir'], 'grains.cache.p')):\n        try:\n            os.remove(os.path.join(__opts__['cachedir'], 'grains.cache.p'))\n        except OSError:\n            log.error('Could not remove grains cache!')\n    return ret", "response": "Sync the given directory in the given environment"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update(version=None):\n    '''\n    Update the salt minion from the URL defined in opts['update_url']\n    SaltStack, Inc provides the latest builds here:\n    update_url: https://repo.saltstack.com/windows/\n\n    Be aware that as of 2014-8-11 there's a bug in esky such that only the\n    latest version available in the update_url can be downloaded and installed.\n\n    This feature requires the minion to be running a bdist_esky build.\n\n    The version number is optional and will default to the most recent version\n    available at opts['update_url'].\n\n    Returns details about the transaction upon completion.\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' saltutil.update\n        salt '*' saltutil.update 0.10.3\n    '''\n    ret = {}\n    if not HAS_ESKY:\n        ret['_error'] = 'Esky not available as import'\n        return ret\n    if not getattr(sys, 'frozen', False):\n        ret['_error'] = 'Minion is not running an Esky build'\n        return ret\n    if not __salt__['config.option']('update_url'):\n        ret['_error'] = '\"update_url\" not configured on this minion'\n        return ret\n    app = esky.Esky(sys.executable, __opts__['update_url'])\n    oldversion = __grains__['saltversion']\n    if not version:\n        try:\n            version = app.find_update()\n        except URLError as exc:\n            ret['_error'] = 'Could not connect to update_url. Error: {0}'.format(exc)\n            return ret\n    if not version:\n        ret['_error'] = 'No updates available'\n        return ret\n    try:\n        app.fetch_version(version)\n    except EskyVersionError as exc:\n        ret['_error'] = 'Unable to fetch version {0}. Error: {1}'.format(version, exc)\n        return ret\n    try:\n        app.install_version(version)\n    except EskyVersionError as exc:\n        ret['_error'] = 'Unable to install version {0}. Error: {1}'.format(version, exc)\n        return ret\n    try:\n        app.cleanup()\n    except Exception as exc:\n        ret['_error'] = 'Unable to cleanup. Error: {0}'.format(exc)\n    restarted = {}\n    for service in __opts__['update_restart_services']:\n        restarted[service] = __salt__['service.restart'](service)\n    ret['comment'] = 'Updated from {0} to {1}'.format(oldversion, version)\n    ret['restarted'] = restarted\n    return ret", "response": "Update the salt minion from the specified update_url"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sync_beacons(saltenv=None, refresh=True, extmod_whitelist=None, extmod_blacklist=None):\n    '''\n    .. versionadded:: 2015.5.1\n\n    Sync beacons from ``salt://_beacons`` to the minion\n\n    saltenv\n        The fileserver environment from which to sync. To sync from more than\n        one environment, pass a comma-separated list.\n\n        If not passed, then all environments configured in the :ref:`top files\n        <states-top>` will be checked for beacons to sync. If no top files are\n        found, then the ``base`` environment will be synced.\n\n    refresh : True\n        If ``True``, refresh the available beacons on the minion. This refresh\n        will be performed even if no new beacons are synced. Set to ``False``\n        to prevent this refresh.\n\n    extmod_whitelist : None\n        comma-seperated list of modules to sync\n\n    extmod_blacklist : None\n        comma-seperated list of modules to blacklist based on type\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' saltutil.sync_beacons\n        salt '*' saltutil.sync_beacons saltenv=dev\n        salt '*' saltutil.sync_beacons saltenv=base,dev\n    '''\n    ret = _sync('beacons', saltenv, extmod_whitelist, extmod_blacklist)\n    if refresh:\n        refresh_beacons()\n    return ret", "response": "Sync the beacons from the minion to the master."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sync_sdb(saltenv=None, extmod_whitelist=None, extmod_blacklist=None):\n    '''\n    .. versionadded:: 2015.5.8,2015.8.3\n\n    Sync sdb modules from ``salt://_sdb`` to the minion\n\n    saltenv\n        The fileserver environment from which to sync. To sync from more than\n        one environment, pass a comma-separated list.\n\n        If not passed, then all environments configured in the :ref:`top files\n        <states-top>` will be checked for sdb modules to sync. If no top files\n        are found, then the ``base`` environment will be synced.\n\n    refresh : False\n        This argument has no affect and is included for consistency with the\n        other sync functions.\n\n    extmod_whitelist : None\n        comma-seperated list of modules to sync\n\n    extmod_blacklist : None\n        comma-seperated list of modules to blacklist based on type\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' saltutil.sync_sdb\n        salt '*' saltutil.sync_sdb saltenv=dev\n        salt '*' saltutil.sync_sdb saltenv=base,dev\n    '''\n    ret = _sync('sdb', saltenv, extmod_whitelist, extmod_blacklist)\n    return ret", "response": "Syncs the modules from the sdb server to the minion sdb server"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sync_modules(saltenv=None, refresh=True, extmod_whitelist=None, extmod_blacklist=None):\n    '''\n    .. versionadded:: 0.10.0\n\n    Sync execution modules from ``salt://_modules`` to the minion\n\n    saltenv\n        The fileserver environment from which to sync. To sync from more than\n        one environment, pass a comma-separated list.\n\n        If not passed, then all environments configured in the :ref:`top files\n        <states-top>` will be checked for execution modules to sync. If no top\n        files are found, then the ``base`` environment will be synced.\n\n    refresh : True\n        If ``True``, refresh the available execution modules on the minion.\n        This refresh will be performed even if no new execution modules are\n        synced. Set to ``False`` to prevent this refresh.\n\n    .. important::\n\n        If this function is executed using a :py:func:`module.run\n        <salt.states.module.run>` state, the SLS file will not have access to\n        newly synced execution modules unless a ``refresh`` argument is\n        added to the state, like so:\n\n        .. code-block:: yaml\n\n            load_my_custom_module:\n              module.run:\n                - name: saltutil.sync_modules\n                - refresh: True\n\n        See :ref:`here <reloading-modules>` for a more detailed explanation of\n        why this is necessary.\n\n    extmod_whitelist : None\n        comma-seperated list of modules to sync\n\n    extmod_blacklist : None\n        comma-seperated list of modules to blacklist based on type\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' saltutil.sync_modules\n        salt '*' saltutil.sync_modules saltenv=dev\n        salt '*' saltutil.sync_modules saltenv=base,dev\n    '''\n    ret = _sync('modules', saltenv, extmod_whitelist, extmod_blacklist)\n    if refresh:\n        refresh_modules()\n    return ret", "response": "Syncs the execution modules from the minion to the specified environment."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef refresh_grains(**kwargs):\n    '''\n    .. versionadded:: 2016.3.6,2016.11.4,2017.7.0\n\n    Refresh the minion's grains without syncing custom grains modules from\n    ``salt://_grains``.\n\n    .. note::\n        The available execution modules will be reloaded as part of this\n        proceess, as grains can affect which modules are available.\n\n    refresh_pillar : True\n        Set to ``False`` to keep pillar data from being refreshed.\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' saltutil.refresh_grains\n    '''\n    kwargs = salt.utils.args.clean_kwargs(**kwargs)\n    _refresh_pillar = kwargs.pop('refresh_pillar', True)\n    if kwargs:\n        salt.utils.args.invalid_kwargs(kwargs)\n    # Modules and pillar need to be refreshed in case grains changes affected\n    # them, and the module refresh process reloads the grains and assigns the\n    # newly-reloaded grains to each execution module's __grains__ dunder.\n    refresh_modules()\n    if _refresh_pillar:\n        refresh_pillar()\n    return True", "response": "Refresh the grains of the available execution modules and pillar."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sync_grains(saltenv=None, refresh=True, extmod_whitelist=None, extmod_blacklist=None):\n    '''\n    .. versionadded:: 0.10.0\n\n    Sync grains modules from ``salt://_grains`` to the minion\n\n    saltenv\n        The fileserver environment from which to sync. To sync from more than\n        one environment, pass a comma-separated list.\n\n        If not passed, then all environments configured in the :ref:`top files\n        <states-top>` will be checked for grains modules to sync. If no top\n        files are found, then the ``base`` environment will be synced.\n\n    refresh : True\n        If ``True``, refresh the available execution modules and recompile\n        pillar data for the minion. This refresh will be performed even if no\n        new grains modules are synced. Set to ``False`` to prevent this\n        refresh.\n\n    extmod_whitelist : None\n        comma-seperated list of modules to sync\n\n    extmod_blacklist : None\n        comma-seperated list of modules to blacklist based on type\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' saltutil.sync_grains\n        salt '*' saltutil.sync_grains saltenv=dev\n        salt '*' saltutil.sync_grains saltenv=base,dev\n    '''\n    ret = _sync('grains', saltenv, extmod_whitelist, extmod_blacklist)\n    if refresh:\n        refresh_modules()\n        refresh_pillar()\n    return ret", "response": "Sync grains modules from the master to the master"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sync_matchers(saltenv=None, refresh=False, extmod_whitelist=None, extmod_blacklist=None):\n    '''\n    .. versionadded:: 2019.2.0\n\n    Sync engine modules from ``salt://_matchers`` to the minion\n\n    saltenv\n        The fileserver environment from which to sync. To sync from more than\n        one environment, pass a comma-separated list.\n\n        If not passed, then all environments configured in the :ref:`top files\n        <states-top>` will be checked for engines to sync. If no top files are\n        found, then the ``base`` environment will be synced.\n\n    refresh : True\n        If ``True``, refresh the available execution modules on the minion.\n        This refresh will be performed even if no new matcher modules are synced.\n        Set to ``False`` to prevent this refresh.\n\n    extmod_whitelist : None\n        comma-separated list of modules to sync\n\n    extmod_blacklist : None\n        comma-separated list of modules to blacklist based on type\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' saltutil.sync_matchers\n        salt '*' saltutil.sync_matchers saltenv=base,dev\n    '''\n    ret = _sync('matchers', saltenv, extmod_whitelist, extmod_blacklist)\n    if refresh:\n        refresh_modules()\n    return ret", "response": "Sync the available modules from the master to the master"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef list_extmods():\n    '''\n    .. versionadded:: 2017.7.0\n\n    List Salt modules which have been synced externally\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' saltutil.list_extmods\n    '''\n    ret = {}\n    ext_dir = os.path.join(__opts__['cachedir'], 'extmods')\n    mod_types = os.listdir(ext_dir)\n    for mod_type in mod_types:\n        ret[mod_type] = set()\n        for _, _, files in salt.utils.path.os_walk(os.path.join(ext_dir, mod_type)):\n            for fh_ in files:\n                ret[mod_type].add(fh_.split('.')[0])\n        ret[mod_type] = list(ret[mod_type])\n    return ret", "response": "List all external modules which have been synced externally"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sync_pillar(saltenv=None, refresh=True, extmod_whitelist=None, extmod_blacklist=None):\n    '''\n    .. versionadded:: 2015.8.11,2016.3.2\n\n    Sync pillar modules from the ``salt://_pillar`` directory on the Salt\n    fileserver. This function is environment-aware, pass the desired\n    environment to grab the contents of the ``_pillar`` directory from that\n    environment. The default environment, if none is specified,  is ``base``.\n\n    refresh : True\n        Also refresh the execution modules available to the minion, and refresh\n        pillar data.\n\n    extmod_whitelist : None\n        comma-seperated list of modules to sync\n\n    extmod_blacklist : None\n        comma-seperated list of modules to blacklist based on type\n\n    .. note::\n        This function will raise an error if executed on a traditional (i.e.\n        not masterless) minion\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' saltutil.sync_pillar\n        salt '*' saltutil.sync_pillar saltenv=dev\n    '''\n    if __opts__['file_client'] != 'local':\n        raise CommandExecutionError(\n            'Pillar modules can only be synced to masterless minions'\n        )\n    ret = _sync('pillar', saltenv, extmod_whitelist, extmod_blacklist)\n    if refresh:\n        refresh_modules()\n        refresh_pillar()\n    return ret", "response": "Sync the pillar modules from the masterless minion"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sync_all(saltenv=None, refresh=True, extmod_whitelist=None, extmod_blacklist=None):\n    '''\n    .. versionchanged:: 2015.8.11,2016.3.2\n        On masterless minions, pillar modules are now synced, and refreshed\n        when ``refresh`` is set to ``True``.\n\n    Sync down all of the dynamic modules from the file server for a specific\n    environment. This function synchronizes custom modules, states, beacons,\n    grains, returners, output modules, renderers, and utils.\n\n    refresh : True\n        Also refresh the execution modules and recompile pillar data available\n        to the minion. This refresh will be performed even if no new dynamic\n        modules are synced. Set to ``False`` to prevent this refresh.\n\n    .. important::\n\n        If this function is executed using a :py:func:`module.run\n        <salt.states.module.run>` state, the SLS file will not have access to\n        newly synced execution modules unless a ``refresh`` argument is\n        added to the state, like so:\n\n        .. code-block:: yaml\n\n            load_my_custom_module:\n              module.run:\n                - name: saltutil.sync_all\n                - refresh: True\n\n        See :ref:`here <reloading-modules>` for a more detailed explanation of\n        why this is necessary.\n\n    extmod_whitelist : None\n        dictionary of modules to sync based on type\n\n    extmod_blacklist : None\n        dictionary of modules to blacklist based on type\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' saltutil.sync_all\n        salt '*' saltutil.sync_all saltenv=dev\n        salt '*' saltutil.sync_all saltenv=base,dev\n        salt '*' saltutil.sync_all extmod_whitelist={'modules': ['custom_module']}\n    '''\n    log.debug('Syncing all')\n    ret = {}\n    ret['clouds'] = sync_clouds(saltenv, False, extmod_whitelist, extmod_blacklist)\n    ret['beacons'] = sync_beacons(saltenv, False, extmod_whitelist, extmod_blacklist)\n    ret['modules'] = sync_modules(saltenv, False, extmod_whitelist, extmod_blacklist)\n    ret['states'] = sync_states(saltenv, False, extmod_whitelist, extmod_blacklist)\n    ret['sdb'] = sync_sdb(saltenv, extmod_whitelist, extmod_blacklist)\n    ret['grains'] = sync_grains(saltenv, False, extmod_whitelist, extmod_blacklist)\n    ret['renderers'] = sync_renderers(saltenv, False, extmod_whitelist, extmod_blacklist)\n    ret['returners'] = sync_returners(saltenv, False, extmod_whitelist, extmod_blacklist)\n    ret['output'] = sync_output(saltenv, False, extmod_whitelist, extmod_blacklist)\n    ret['utils'] = sync_utils(saltenv, False, extmod_whitelist, extmod_blacklist)\n    ret['log_handlers'] = sync_log_handlers(saltenv, False, extmod_whitelist, extmod_blacklist)\n    ret['proxymodules'] = sync_proxymodules(saltenv, False, extmod_whitelist, extmod_blacklist)\n    ret['engines'] = sync_engines(saltenv, False, extmod_whitelist, extmod_blacklist)\n    ret['thorium'] = sync_thorium(saltenv, False, extmod_whitelist, extmod_blacklist)\n    ret['serializers'] = sync_serializers(saltenv, False, extmod_whitelist, extmod_blacklist)\n    ret['matchers'] = sync_matchers(saltenv, False, extmod_whitelist, extmod_blacklist)\n    ret['executors'] = sync_executors(saltenv, False, extmod_whitelist, extmod_blacklist)\n    if __opts__['file_client'] == 'local':\n        ret['pillar'] = sync_pillar(saltenv, False, extmod_whitelist, extmod_blacklist)\n    if refresh:\n        refresh_modules()\n        refresh_pillar()\n    return ret", "response": "Synchronizes all available modules for a specific environment."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef refresh_pillar(**kwargs):\n    '''\n    Signal the minion to refresh the pillar data.\n\n    .. versionchanged:: Neon\n        The ``async`` argument has been added. The default value is True.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' saltutil.refresh_pillar\n        salt '*' saltutil.refresh_pillar async=False\n    '''\n    asynchronous = bool(kwargs.get('async', True))\n    try:\n        if asynchronous:\n            #  If we're going to block, first setup a listener\n            ret = __salt__['event.fire']({}, 'pillar_refresh')\n        else:\n            eventer = salt.utils.event.get_event(\n                'minion', opts=__opts__, listen=True)\n            ret = __salt__['event.fire']({'notify': True}, 'pillar_refresh')\n            # Wait for the finish event to fire\n            log.trace('refresh_pillar waiting for pillar refresh to complete')\n            # Blocks until we hear this event or until the timeout expires\n            eventer.get_event(\n                tag=salt.defaults.events.MINION_PILLAR_COMPLETE, wait=30)\n    except KeyError:\n        log.error('Event module not available. Pillar refresh failed.')\n        ret = False  # Effectively a no-op, since we can't really return without an event system\n    return ret", "response": "Refresh the pillar data for the minion"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_running(fun):\n    '''\n    If the named function is running return the data associated with it/them.\n    The argument can be a glob\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' saltutil.is_running state.highstate\n    '''\n    run = running()\n    ret = []\n    for data in run:\n        if fnmatch.fnmatch(data.get('fun', ''), fun):\n            ret.append(data)\n    return ret", "response": "Returns the data associated with it if the named function is running"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef clear_cache(days=-1):\n    '''\n    Forcibly removes all caches on a minion.\n\n    .. versionadded:: 2014.7.0\n\n    WARNING: The safest way to clear a minion cache is by first stopping\n    the minion and then deleting the cache files before restarting it.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' saltutil.clear_cache days=7\n    '''\n    threshold = time.time() - days * 24 * 60 * 60\n    for root, dirs, files in salt.utils.files.safe_walk(__opts__['cachedir'], followlinks=False):\n        for name in files:\n            try:\n                file = os.path.join(root, name)\n                mtime = os.path.getmtime(file)\n                if mtime < threshold:\n                    os.remove(file)\n            except OSError as exc:\n                log.error(\n                    'Attempt to clear cache with saltutil.clear_cache '\n                    'FAILED with: %s', exc\n                )\n                return False\n    return True", "response": "Forcibly removes all caches on a minion."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the data for a specific cached job id.", "response": "def find_cached_job(jid):\n    '''\n    Return the data for a specific cached job id. Note this only works if\n    cache_jobs has previously been set to True on the minion.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' saltutil.find_cached_job <job id>\n    '''\n    serial = salt.payload.Serial(__opts__)\n    proc_dir = os.path.join(__opts__['cachedir'], 'minion_jobs')\n    job_dir = os.path.join(proc_dir, six.text_type(jid))\n    if not os.path.isdir(job_dir):\n        if not __opts__.get('cache_jobs'):\n            return ('Local jobs cache directory not found; you may need to'\n                    ' enable cache_jobs on this minion')\n        else:\n            return 'Local jobs cache directory {0} not found'.format(job_dir)\n    path = os.path.join(job_dir, 'return.p')\n    with salt.utils.files.fopen(path, 'rb') as fp_:\n        buf = fp_.read()\n    if buf:\n        try:\n            data = serial.loads(buf)\n        except NameError:\n            # msgpack error in salt-ssh\n            pass\n        else:\n            if isinstance(data, dict):\n                # if not a dict, this was an invalid serialized object\n                return data\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsends a signal to the named salt job", "response": "def signal_job(jid, sig):\n    '''\n    Sends a signal to the named salt job's process\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' saltutil.signal_job <job id> 15\n    '''\n    if HAS_PSUTIL is False:\n        log.warning('saltutil.signal job called, but psutil is not installed. '\n                    'Install psutil to ensure more reliable and accurate PID '\n                    'management.')\n    for data in running():\n        if data['jid'] == jid:\n            try:\n                if HAS_PSUTIL:\n                    for proc in salt.utils.psutil_compat.Process(pid=data['pid']).children(recursive=True):\n                        proc.send_signal(sig)\n                os.kill(int(data['pid']), sig)\n                if HAS_PSUTIL is False and 'child_pids' in data:\n                    for pid in data['child_pids']:\n                        os.kill(int(pid), sig)\n                return 'Signal {0} sent to job {1} at pid {2}'.format(\n                        int(sig),\n                        jid,\n                        data['pid']\n                        )\n            except OSError:\n                path = os.path.join(__opts__['cachedir'], 'proc', six.text_type(jid))\n                if os.path.isfile(path):\n                    os.remove(path)\n                return ('Job {0} was not running and job data has been '\n                        ' cleaned up').format(jid)\n    return ''"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef term_all_jobs():\n    '''\n    Sends a termination signal (SIGTERM 15) to all currently running jobs\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' saltutil.term_all_jobs\n    '''\n    ret = []\n    for data in running():\n        ret.append(signal_job(data['jid'], signal.SIGTERM))\n    return ret", "response": "Sends a termination signal to all currently running jobs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nregenerating the minion keys.", "response": "def regen_keys():\n    '''\n    Used to regenerate the minion keys.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' saltutil.regen_keys\n    '''\n    for fn_ in os.listdir(__opts__['pki_dir']):\n        path = os.path.join(__opts__['pki_dir'], fn_)\n        try:\n            os.remove(path)\n        except os.error:\n            pass\n    # TODO: move this into a channel function? Or auth?\n    # create a channel again, this will force the key regen\n    channel = salt.transport.client.ReqChannel.factory(__opts__)\n    channel.close()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef revoke_auth(preserve_minion_cache=False):\n    '''\n    The minion sends a request to the master to revoke its own key.\n    Note that the minion session will be revoked and the minion may\n    not be able to return the result of this command back to the master.\n\n    If the 'preserve_minion_cache' flag is set to True, the master\n    cache for this minion will not be removed.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' saltutil.revoke_auth\n    '''\n    masters = list()\n    ret = True\n    if 'master_uri_list' in __opts__:\n        for master_uri in __opts__['master_uri_list']:\n            masters.append(master_uri)\n    else:\n        masters.append(__opts__['master_uri'])\n\n    for master in masters:\n        channel = salt.transport.client.ReqChannel.factory(__opts__, master_uri=master)\n        tok = channel.auth.gen_token(b'salt')\n        load = {'cmd': 'revoke_auth',\n                'id': __opts__['id'],\n                'tok': tok,\n                'preserve_minion_cache': preserve_minion_cache}\n        try:\n            channel.send(load)\n        except SaltReqTimeoutError:\n            ret = False\n        finally:\n            channel.close()\n    return ret", "response": "Revoke the master s own key."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cmd(tgt,\n        fun,\n        arg=(),\n        timeout=None,\n        tgt_type='glob',\n        ret='',\n        kwarg=None,\n        ssh=False,\n        **kwargs):\n    '''\n    .. versionchanged:: 2017.7.0\n        The ``expr_form`` argument has been renamed to ``tgt_type``, earlier\n        releases must use ``expr_form``.\n\n    Assuming this minion is a master, execute a salt command\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' saltutil.cmd\n    '''\n    cfgfile = __opts__['conf_file']\n    client = _get_ssh_or_api_client(cfgfile, ssh)\n    fcn_ret = _exec(\n        client, tgt, fun, arg, timeout, tgt_type, ret, kwarg, **kwargs)\n    # if return is empty, we may have not used the right conf,\n    # try with the 'minion relative master configuration counter part\n    # if available\n    master_cfgfile = '{0}master'.format(cfgfile[:-6])  # remove 'minion'\n    if (\n        not fcn_ret\n        and cfgfile.endswith('{0}{1}'.format(os.path.sep, 'minion'))\n        and os.path.exists(master_cfgfile)\n    ):\n        client = _get_ssh_or_api_client(master_cfgfile, ssh)\n        fcn_ret = _exec(\n            client, tgt, fun, arg, timeout, tgt_type, ret, kwarg, **kwargs)\n\n    if 'batch' in kwargs:\n        old_ret, fcn_ret = fcn_ret, {}\n        for key, value in old_ret.items():\n            fcn_ret[key] = {\n                'out': value.get('out', 'highstate') if isinstance(value, dict) else 'highstate',\n                'ret': value,\n            }\n\n    return fcn_ret", "response": "Execute a salt command on a master configuration file"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef cmd_iter(tgt,\n             fun,\n             arg=(),\n             timeout=None,\n             tgt_type='glob',\n             ret='',\n             kwarg=None,\n             ssh=False,\n             **kwargs):\n    '''\n    .. versionchanged:: 2017.7.0\n        The ``expr_form`` argument has been renamed to ``tgt_type``, earlier\n        releases must use ``expr_form``.\n\n    Assuming this minion is a master, execute a salt command\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' saltutil.cmd_iter\n    '''\n    if ssh:\n        client = salt.client.ssh.client.SSHClient(__opts__['conf_file'])\n    else:\n        client = salt.client.get_local_client(__opts__['conf_file'])\n    for ret in client.cmd_iter(\n            tgt,\n            fun,\n            arg,\n            timeout,\n            tgt_type,\n            ret,\n            kwarg,\n            **kwargs):\n        yield ret", "response": "Execute a salt command in a sequence of commands"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef runner(name, arg=None, kwarg=None, full_return=False, saltenv='base', jid=None, asynchronous=False, **kwargs):\n    '''\n    Execute a runner function. This function must be run on the master,\n    either by targeting a minion running on a master or by using\n    salt-call on a master.\n\n    .. versionadded:: 2014.7.0\n\n    name\n        The name of the function to run\n\n    kwargs\n        Any keyword arguments to pass to the runner function\n\n    asynchronous\n        Run the salt command but don't wait for a reply.\n\n        .. versionadded:: neon\n\n    CLI Example:\n\n    In this example, assume that `master_minion` is a minion running\n    on a master.\n\n    .. code-block:: bash\n\n        salt master_minion saltutil.runner jobs.list_jobs\n        salt master_minion saltutil.runner test.arg arg=\"['baz']\" kwarg=\"{'foo': 'bar'}\"\n    '''\n    if arg is None:\n        arg = []\n    if kwarg is None:\n        kwarg = {}\n    jid = kwargs.pop('__orchestration_jid__', jid)\n    saltenv = kwargs.pop('__env__', saltenv)\n    kwargs = salt.utils.args.clean_kwargs(**kwargs)\n    if kwargs:\n        kwarg.update(kwargs)\n\n    if 'master_job_cache' not in __opts__:\n        master_config = os.path.join(os.path.dirname(__opts__['conf_file']),\n                                     'master')\n        master_opts = salt.config.master_config(master_config)\n        rclient = salt.runner.RunnerClient(master_opts)\n    else:\n        rclient = salt.runner.RunnerClient(__opts__)\n\n    if name in rclient.functions:\n        aspec = salt.utils.args.get_function_argspec(rclient.functions[name])\n        if 'saltenv' in aspec.args:\n            kwarg['saltenv'] = saltenv\n\n    if name in ['state.orchestrate', 'state.orch', 'state.sls']:\n        kwarg['orchestration_jid'] = jid\n\n    if jid:\n        salt.utils.event.fire_args(\n            __opts__,\n            jid,\n            {'type': 'runner', 'name': name, 'args': arg, 'kwargs': kwarg},\n            prefix='run'\n        )\n\n    if asynchronous:\n        master_key = salt.utils.master.get_master_key('root', __opts__)\n        low = {'arg': arg, 'kwarg': kwarg, 'fun': name, 'key': master_key}\n        return rclient.cmd_async(low)\n    else:\n        return rclient.cmd(name,\n                           arg=arg,\n                           kwarg=kwarg,\n                           print_event=False,\n                           full_return=full_return)", "response": "Execute a runner function on a master"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef wheel(name, *args, **kwargs):\n    '''\n    Execute a wheel module and function. This function must be run against a\n    minion that is local to the master.\n\n    .. versionadded:: 2014.7.0\n\n    name\n        The name of the function to run\n\n    args\n        Any positional arguments to pass to the wheel function. A common example\n        of this would be the ``match`` arg needed for key functions.\n\n        .. versionadded:: v2015.8.11\n\n    kwargs\n        Any keyword arguments to pass to the wheel function\n\n    asynchronous\n        Run the salt command but don't wait for a reply.\n\n        .. versionadded:: neon\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt my-local-minion saltutil.wheel key.accept jerry\n        salt my-local-minion saltutil.wheel minions.connected\n\n    .. note::\n\n        Since this function must be run against a minion that is running locally\n        on the master in order to get accurate returns, if this function is run\n        against minions that are not local to the master, \"empty\" returns are\n        expected. The remote minion does not have access to wheel functions and\n        their return data.\n\n    '''\n    jid = kwargs.pop('__orchestration_jid__', None)\n    saltenv = kwargs.pop('__env__', 'base')\n\n    if __opts__['__role'] == 'minion':\n        master_config = os.path.join(os.path.dirname(__opts__['conf_file']),\n                                     'master')\n        master_opts = salt.config.client_config(master_config)\n        wheel_client = salt.wheel.WheelClient(master_opts)\n    else:\n        wheel_client = salt.wheel.WheelClient(__opts__)\n\n    # The WheelClient cmd needs args, kwargs, and pub_data separated out from\n    # the \"normal\" kwargs structure, which at this point contains __pub_x keys.\n    pub_data = {}\n    valid_kwargs = {}\n    for key, val in six.iteritems(kwargs):\n        if key.startswith('__'):\n            pub_data[key] = val\n        else:\n            valid_kwargs[key] = val\n\n    try:\n        if name in wheel_client.functions:\n            aspec = salt.utils.args.get_function_argspec(\n                wheel_client.functions[name]\n            )\n            if 'saltenv' in aspec.args:\n                valid_kwargs['saltenv'] = saltenv\n\n        if jid:\n            salt.utils.event.fire_args(\n                __opts__,\n                jid,\n                {'type': 'wheel', 'name': name, 'args': valid_kwargs},\n                prefix='run'\n            )\n\n        if kwargs.pop('asynchronous', False):\n            master_key = salt.utils.master.get_master_key('root', __opts__)\n            low = {'arg': args, 'kwarg': kwargs, 'fun': name, 'key': master_key}\n            ret = wheel_client.cmd_async(low)\n        else:\n            ret = wheel_client.cmd(name,\n                                   arg=args,\n                                   pub_data=pub_data,\n                                   kwarg=valid_kwargs,\n                                   print_event=False,\n                                   full_return=True)\n    except SaltInvocationError as e:\n        raise CommandExecutionError(\n            'This command can only be executed on a minion that is located on '\n            'the master.'\n        )\n\n    return ret", "response": "Execute a wheel module and function."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef mmodule(saltenv, fun, *args, **kwargs):\n    '''\n    Loads minion modules from an environment so that they can be used in pillars\n    for that environment\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' saltutil.mmodule base test.ping\n    '''\n    mminion = _MMinion(saltenv)\n    return mminion.functions[fun](*args, **kwargs)", "response": "Load minion modules from an environment"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _session():\n    '''\n    Create a session to be used when connecting to Zenoss.\n    '''\n\n    config = __salt__['config.option']('zenoss')\n    session = requests.session()\n    session.auth = (config.get('username'), config.get('password'))\n    session.verify = False\n    session.headers.update({'Content-type': 'application/json; charset=utf-8'})\n    return session", "response": "Create a session to be used when connecting to Zenoss"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _router_request(router, method, data=None):\n    '''\n    Make a request to the Zenoss API router\n    '''\n    if router not in ROUTERS:\n        return False\n\n    req_data = salt.utils.json.dumps([dict(\n        action=router,\n        method=method,\n        data=data,\n        type='rpc',\n        tid=1)])\n\n    config = __salt__['config.option']('zenoss')\n    log.debug('Making request to router %s with method %s', router, method)\n    url = '{0}/zport/dmd/{1}_router'.format(config.get('hostname'), ROUTERS[router])\n    response = _session().post(url, data=req_data)\n\n    # The API returns a 200 response code even whe auth is bad.\n    # With bad auth, the login page is displayed. Here I search for\n    # an element on the login form to determine if auth failed.\n    if re.search('name=\"__ac_name\"', response.content):\n        log.error('Request failed. Bad username/password.')\n        raise Exception('Request failed. Bad username/password.')\n\n    return salt.utils.json.loads(response.content).get('result', None)", "response": "Make a request to the Zenoss API router\n   "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfinding a device in Zenoss.", "response": "def find_device(device=None):\n    '''\n    Find a device in Zenoss. If device not found, returns None.\n\n    Parameters:\n        device:         (Optional) Will use the grain 'fqdn' by default\n\n    CLI Example:\n        salt '*' zenoss.find_device\n    '''\n\n    data = [{'uid': '/zport/dmd/Devices', 'params': {}, 'limit': None}]\n    all_devices = _router_request('DeviceRouter', 'getDevices', data=data)\n    for dev in all_devices['devices']:\n        if dev['name'] == device:\n            # We need to save the has for later operations\n            dev['hash'] = all_devices['hash']\n            log.info('Found device %s in Zenoss', device)\n            return dev\n\n    log.info('Unable to find device %s in Zenoss', device)\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_device(device=None, device_class=None, collector='localhost', prod_state=1000):\n    '''\n    A function to connect to a zenoss server and add a new device entry.\n\n    Parameters:\n        device:         (Optional) Will use the grain 'fqdn' by default.\n        device_class:   (Optional) The device class to use. If none, will determine based on kernel grain.\n        collector:      (Optional) The collector to use for this device. Defaults to 'localhost'.\n        prod_state:     (Optional) The prodState to set on the device. If none, defaults to 1000 ( production )\n\n    CLI Example:\n        salt '*' zenoss.add_device\n    '''\n\n    if not device:\n        device = __salt__['grains.get']('fqdn')\n\n    if not device_class:\n        device_class = _determine_device_class()\n    log.info('Adding device %s to zenoss', device)\n    data = dict(deviceName=device, deviceClass=device_class, model=True, collector=collector, productionState=prod_state)\n    response = _router_request('DeviceRouter', 'addDevice', data=[data])\n    return response", "response": "A function to connect to a zenoss server and add a new entry to the list of available devices."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the state of the current production of the current node in Zenoss.", "response": "def set_prod_state(prod_state, device=None):\n    '''\n    A function to set the prod_state in zenoss.\n\n    Parameters:\n        prod_state:     (Required) Integer value of the state\n        device:         (Optional) Will use the grain 'fqdn' by default.\n\n    CLI Example:\n        salt zenoss.set_prod_state 1000 hostname\n    '''\n\n    if not device:\n        device = __salt__['grains.get']('fqdn')\n\n    device_object = find_device(device)\n\n    if not device_object:\n        return \"Unable to find a device in Zenoss for {0}\".format(device)\n\n    log.info('Setting prodState to %d on %s device', prod_state, device)\n    data = dict(uids=[device_object['uid']], prodState=prod_state, hashcheck=device_object['hash'])\n    return _router_request('DeviceRouter', 'setProductionState', [data])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfilter file path against unwanted directories and decides whether file is desired deleted file.", "response": "def _valid_deleted_file(path):\n    '''\n    Filters file path against unwanted directories and decides whether file is marked as deleted.\n\n    Returns:\n        True if file is desired deleted file, else False.\n\n    Args:\n        path: A string - path to file\n    '''\n    ret = False\n    if path.endswith(' (deleted)'):\n        ret = True\n    if re.compile(r\"\\(path inode=[0-9]+\\)$\").search(path):\n        ret = True\n\n    regex = re.compile(\"|\".join(LIST_DIRS))\n    if regex.match(path):\n        ret = False\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _deleted_files():\n    '''\n    Iterates over /proc/PID/maps and /proc/PID/fd links and returns list of desired deleted files.\n\n    Returns:\n        List of deleted files to analyze, False on failure.\n\n    '''\n    deleted_files = []\n\n    for proc in psutil.process_iter():  # pylint: disable=too-many-nested-blocks\n        try:\n            pinfo = proc.as_dict(attrs=['pid', 'name'])\n            try:\n                with salt.utils.files.fopen('/proc/{0}/maps'.format(pinfo['pid'])) as maps:  # pylint: disable=resource-leakage\n                    dirpath = '/proc/' + six.text_type(pinfo['pid']) + '/fd/'\n                    listdir = os.listdir(dirpath)\n                    maplines = maps.readlines()\n            except (OSError, IOError):\n                yield False\n\n            # /proc/PID/maps\n            mapline = re.compile(r'^[\\da-f]+-[\\da-f]+ [r-][w-][x-][sp-] '\n                                 r'[\\da-f]+ [\\da-f]{2}:[\\da-f]{2} (\\d+) *(.+)( \\(deleted\\))?\\n$')\n\n            for line in maplines:\n                line = salt.utils.stringutils.to_unicode(line)\n                matched = mapline.match(line)\n                if not matched:\n                    continue\n                path = matched.group(2)\n                if not path:\n                    continue\n                valid = _valid_deleted_file(path)\n                if not valid:\n                    continue\n                val = (pinfo['name'], pinfo['pid'], path[0:-10])\n                if val not in deleted_files:\n                    deleted_files.append(val)\n                    yield val\n\n            # /proc/PID/fd\n            try:\n                for link in listdir:\n                    path = dirpath + link\n                    readlink = os.readlink(path)\n                    filenames = []\n\n                    if os.path.isfile(readlink):\n                        filenames.append(readlink)\n                    elif os.path.isdir(readlink) and readlink != '/':\n                        for root, dummy_dirs, files in salt.utils.path.os_walk(readlink, followlinks=True):\n                            for name in files:\n                                filenames.append(os.path.join(root, name))\n\n                    for filename in filenames:\n                        valid = _valid_deleted_file(filename)\n                        if not valid:\n                            continue\n                        val = (pinfo['name'], pinfo['pid'], filename)\n                        if val not in deleted_files:\n                            deleted_files.append(val)\n                            yield val\n            except OSError:\n                pass\n\n        except psutil.NoSuchProcess:\n            pass", "response": "Yields the list of deleted files in the system."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nformatting the output of the restartcheck module.", "response": "def _format_output(kernel_restart, packages, verbose, restartable, nonrestartable, restartservicecommands,\n                   restartinitcommands):\n    '''\n    Formats the output of the restartcheck module.\n\n    Returns:\n        String - formatted output.\n\n    Args:\n        kernel_restart: indicates that newer kernel is instaled\n        packages: list of packages that should be restarted\n        verbose: enables extensive output\n        restartable: list of restartable packages\n        nonrestartable: list of non-restartable packages\n        restartservicecommands: list of commands to restart services\n        restartinitcommands: list of commands to restart init.d scripts\n\n    '''\n    if not verbose:\n        packages = restartable + nonrestartable\n        if kernel_restart:\n            packages.append('System restart required.')\n        return packages\n    else:\n        ret = ''\n        if kernel_restart:\n            ret = 'System restart required.\\n\\n'\n\n        if packages:\n            ret += \"Found {0} processes using old versions of upgraded files.\\n\".format(len(packages))\n            ret += \"These are the packages:\\n\"\n\n        if restartable:\n            ret += \"Of these, {0} seem to contain systemd service definitions or init scripts \" \\\n                   \"which can be used to restart them:\\n\".format(len(restartable))\n            for package in restartable:\n                ret += package + ':\\n'\n                for program in packages[package]['processes']:\n                    ret += program + '\\n'\n\n            if restartservicecommands:\n                ret += \"\\n\\nThese are the systemd services:\\n\"\n                ret += '\\n'.join(restartservicecommands)\n\n            if restartinitcommands:\n                ret += \"\\n\\nThese are the initd scripts:\\n\"\n                ret += '\\n'.join(restartinitcommands)\n\n        if nonrestartable:\n            ret += \"\\n\\nThese processes {0} do not seem to have an associated init script \" \\\n                   \"to restart them:\\n\".format(len(nonrestartable))\n            for package in nonrestartable:\n                ret += package + ':\\n'\n                for program in packages[package]['processes']:\n                    ret += program + '\\n'\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list of possible names of last installed kernels for Debian based systems.", "response": "def _kernel_versions_debian():\n    '''\n    Last installed kernel name, for Debian based systems.\n\n    Returns:\n            List with possible names of last installed kernel\n            as they are probably interpreted in output of `uname -a` command.\n    '''\n    kernel_get_selections = __salt__['cmd.run']('dpkg --get-selections linux-image-*')\n    kernels = []\n    kernel_versions = []\n    for line in kernel_get_selections.splitlines():\n        kernels.append(line)\n\n    try:\n        kernel = kernels[-2]\n    except IndexError:\n        kernel = kernels[0]\n\n    kernel = kernel.rstrip('\\t\\tinstall')\n\n    kernel_get_version = __salt__['cmd.run']('apt-cache policy ' + kernel)\n\n    for line in kernel_get_version.splitlines():\n        if line.startswith('  Installed: '):\n            kernel_v = line.strip('  Installed: ')\n            kernel_versions.append(kernel_v)\n            break\n\n    if __grains__['os'] == 'Ubuntu':\n        kernel_v = kernel_versions[0].rsplit('.', 1)\n        kernel_ubuntu_generic = kernel_v[0] + '-generic #' + kernel_v[1]\n        kernel_ubuntu_lowlatency = kernel_v[0] + '-lowlatency #' + kernel_v[1]\n        kernel_versions.extend([kernel_ubuntu_generic, kernel_ubuntu_lowlatency])\n\n    return kernel_versions"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _kernel_versions_redhat():\n    '''\n    Name of the last installed kernel, for Red Hat based systems.\n\n    Returns:\n            List with name of last installed kernel as it is interpreted in output of `uname -a` command.\n    '''\n    kernel_get_last = __salt__['cmd.run']('rpm -q --last kernel')\n    kernels = []\n    kernel_versions = []\n    for line in kernel_get_last.splitlines():\n        if 'kernel-' in line:\n            kernels.append(line)\n\n    kernel = kernels[0].split(' ', 1)[0]\n    kernel = kernel.strip('kernel-')\n    kernel_versions.append(kernel)\n\n    return kernel_versions", "response": "Return kernel versions for Red Hat based systems"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of possible kernel names for the NILRT kernels.", "response": "def _kernel_versions_nilrt():\n    '''\n    Last installed kernel name, for Debian based systems.\n\n    Returns:\n            List with possible names of last installed kernel\n            as they are probably interpreted in output of `uname -a` command.\n    '''\n    kver = None\n\n    def _get_kver_from_bin(kbin):\n        '''\n        Get kernel version from a binary image or None if detection fails\n        '''\n        kvregex = r'[0-9]+\\.[0-9]+\\.[0-9]+-rt\\S+'\n        kernel_strings = __salt__['cmd.run']('strings {0}'.format(kbin))\n        re_result = re.search(kvregex, kernel_strings)\n        return None if re_result is None else re_result.group(0)\n\n    if __grains__.get('lsb_distrib_id') == 'nilrt':\n        if 'arm' in __grains__.get('cpuarch'):\n            # the kernel is inside a uboot created itb (FIT) image alongside the\n            # device tree, ramdisk and a bootscript. There is no package management\n            # or any other kind of versioning info, so we need to extract the itb.\n            itb_path = '/boot/linux_runmode.itb'\n            compressed_kernel = '/var/volatile/tmp/uImage.gz'\n            uncompressed_kernel = '/var/volatile/tmp/uImage'\n            __salt__['cmd.run']('dumpimage -i {0} -T flat_dt -p0 kernel -o {1}'\n                                .format(itb_path, compressed_kernel))\n            __salt__['cmd.run']('gunzip -f {0}'.format(compressed_kernel))\n            kver = _get_kver_from_bin(uncompressed_kernel)\n        else:\n            # the kernel bzImage is copied to rootfs without package management or\n            # other versioning info.\n            kver = _get_kver_from_bin('/boot/runmode/bzImage')\n    else:\n        # kernels in newer NILRT's are installed via package management and\n        # have the version appended to the kernel image filename\n        if 'arm' in __grains__.get('cpuarch'):\n            kver = os.path.basename(os.readlink('/boot/uImage')).strip('uImage-')\n        else:\n            kver = os.path.basename(os.readlink('/boot/bzImage')).strip('bzImage-')\n\n    return [] if kver is None else [kver]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if timeout has expired.", "response": "def _check_timeout(start_time, timeout):\n    '''\n    Name of the last installed kernel, for Red Hat based systems.\n\n    Returns:\n            List with name of last installed kernel as it is interpreted in output of `uname -a` command.\n    '''\n    timeout_milisec = timeout * 60000\n    if timeout_milisec < (int(round(time.time() * 1000)) - start_time):\n        raise salt.exceptions.TimeoutError('Timeout expired.')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _file_changed_nilrt(full_filepath):\n    '''\n    Detect whether a file changed in an NILinuxRT system using md5sum and timestamp\n    files from a state directory.\n\n    Returns:\n             - False if md5sum/timestamp state files don't exist\n             - True/False depending if ``base_filename`` got modified/touched\n    '''\n    rs_state_dir = \"/var/lib/salt/restartcheck_state\"\n    base_filename = os.path.basename(full_filepath)\n    timestamp_file = os.path.join(rs_state_dir, '{0}.timestamp'.format(base_filename))\n    md5sum_file = os.path.join(rs_state_dir, '{0}.md5sum'.format(base_filename))\n\n    if not os.path.exists(timestamp_file) or not os.path.exists(md5sum_file):\n        return True\n\n    prev_timestamp = __salt__['file.read'](timestamp_file).rstrip()\n    # Need timestamp in seconds so floor it using int()\n    cur_timestamp = str(int(os.path.getmtime(full_filepath)))\n\n    if prev_timestamp != cur_timestamp:\n        return True\n\n    return bool(__salt__['cmd.retcode']('md5sum -cs {0}'.format(md5sum_file), output_loglevel=\"quiet\"))", "response": "Detect whether a file has changed in an NILinuxRT system using md5sum and timestamp state files."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _sysapi_changed_nilrt():\n    '''\n    Besides the normal Linux kernel driver interfaces, NILinuxRT-supported hardware features an\n    extensible, plugin-based device enumeration and configuration interface named \"System API\".\n    When an installed package is extending the API it is very hard to know all repercurssions and\n    actions to be taken, so reboot making sure all drivers are reloaded, hardware reinitialized,\n    daemons restarted, etc.\n\n    Returns:\n             - True/False depending if nisysapi .ini files got modified/touched\n             - False if no nisysapi .ini files exist\n    '''\n    nisysapi_path = '/usr/local/natinst/share/nisysapi.ini'\n    if os.path.exists(nisysapi_path) and _file_changed_nilrt(nisysapi_path):\n        return True\n\n    restartcheck_state_dir = '/var/lib/salt/restartcheck_state'\n    nisysapi_conf_d_path = \"/usr/lib/{0}/nisysapi/conf.d/experts/\".format(\n        'arm-linux-gnueabi' if 'arm' in __grains__.get('cpuarch') else 'x86_64-linux-gnu'\n    )\n\n    if os.path.exists(nisysapi_conf_d_path):\n        rs_count_file = '{0}/sysapi.conf.d.count'.format(restartcheck_state_dir)\n        if not os.path.exists(rs_count_file):\n            return True\n\n        with salt.utils.files.fopen(rs_count_file, 'r') as fcount:\n            current_nb_files = len(os.listdir(nisysapi_conf_d_path))\n            rs_stored_nb_files = int(fcount.read())\n            if current_nb_files != rs_stored_nb_files:\n                return True\n\n        for fexpert in os.listdir(nisysapi_conf_d_path):\n            if _file_changed_nilrt('{0}/{1}'.format(nisysapi_conf_d_path, fexpert)):\n                return True\n\n    return False", "response": "Return True if sysapi. ini files have changed and False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nrestarts the named package if it is available.", "response": "def restartcheck(ignorelist=None, blacklist=None, excludepid=None, **kwargs):\n    '''\n    Analyzes files openeded by running processes and seeks for packages which need to be restarted.\n\n    Args:\n        ignorelist: string or list of packages to be ignored\n        blacklist: string or list of file paths to be ignored\n        excludepid: string or list of process IDs to be ignored\n        verbose: boolean, enables extensive output\n        timeout: int, timeout in minute\n\n    Returns:\n        Dict on error: { 'result': False, 'comment': '<reason>' }\n        String with checkrestart output if some package seems to need to be restarted or\n        if no packages need restarting.\n\n    .. versionadded:: 2015.8.3\n\n    CLI Example:\n    .. code-block:: bash\n\n        salt '*' restartcheck.restartcheck\n    '''\n    kwargs = salt.utils.args.clean_kwargs(**kwargs)\n    start_time = int(round(time.time() * 1000))\n    kernel_restart = True\n    verbose = kwargs.pop('verbose', True)\n    timeout = kwargs.pop('timeout', 5)\n    if __grains__.get('os_family') == 'Debian':\n        cmd_pkg_query = 'dpkg-query --listfiles '\n        systemd_folder = '/lib/systemd/system/'\n        systemd = '/bin/systemd'\n        kernel_versions = _kernel_versions_debian()\n    elif __grains__.get('os_family') == 'RedHat':\n        cmd_pkg_query = 'repoquery -l '\n        systemd_folder = '/usr/lib/systemd/system/'\n        systemd = '/usr/bin/systemctl'\n        kernel_versions = _kernel_versions_redhat()\n    elif __grains__.get('os_family') == NILRT_FAMILY_NAME:\n        cmd_pkg_query = 'opkg files '\n        systemd = ''\n        kernel_versions = _kernel_versions_nilrt()\n    else:\n        return {'result': False, 'comment': 'Only available on Debian, Red Hat and NI Linux Real-Time based systems.'}\n\n    # Check kernel versions\n    kernel_current = __salt__['cmd.run']('uname -a')\n    for kernel in kernel_versions:\n        _check_timeout(start_time, timeout)\n        if kernel in kernel_current:\n            if __grains__.get('os_family') == 'NILinuxRT':\n                # Check kernel modules and hardware API's for version changes\n                # If a restartcheck=True event was previously witnessed, propagate it\n                if not _kernel_modules_changed_nilrt(kernel) and \\\n                   not _sysapi_changed_nilrt() and \\\n                   not __salt__['system.get_reboot_required_witnessed']():\n                    kernel_restart = False\n                    break\n            else:\n                kernel_restart = False\n                break\n\n    packages = {}\n    running_services = {}\n    restart_services = []\n\n    if ignorelist:\n        if not isinstance(ignorelist, list):\n            ignorelist = [ignorelist]\n    else:\n        ignorelist = ['screen', 'systemd']\n\n    if blacklist:\n        if not isinstance(blacklist, list):\n            blacklist = [blacklist]\n    else:\n        blacklist = []\n\n    if excludepid:\n        if not isinstance(excludepid, list):\n            excludepid = [excludepid]\n    else:\n        excludepid = []\n\n    for service in __salt__['service.get_running']():\n        _check_timeout(start_time, timeout)\n        service_show = __salt__['service.show'](service)\n        if 'ExecMainPID' in service_show:\n            running_services[service] = int(service_show['ExecMainPID'])\n\n    owners_cache = {}\n    for deleted_file in _deleted_files():\n        if deleted_file is False:\n            return {'result': False, 'comment': 'Could not get list of processes.'\n                                                ' (Do you have root access?)'}\n\n        _check_timeout(start_time, timeout)\n        name, pid, path = deleted_file[0], deleted_file[1], deleted_file[2]\n        if path in blacklist or pid in excludepid:\n            continue\n        try:\n            readlink = os.readlink('/proc/{0}/exe'.format(pid))\n        except OSError:\n            excludepid.append(pid)\n            continue\n        try:\n            packagename = owners_cache[readlink]\n        except KeyError:\n            packagename = __salt__['pkg.owner'](readlink)\n            if not packagename:\n                packagename = name\n            owners_cache[readlink] = packagename\n        for running_service in running_services:\n            _check_timeout(start_time, timeout)\n            if running_service not in restart_services and pid == running_services[running_service]:\n                if packagename and packagename not in ignorelist:\n                    restart_services.append(running_service)\n                    name = running_service\n        if packagename and packagename not in ignorelist:\n            program = '\\t' + six.text_type(pid) + ' ' + readlink + ' (file: ' + six.text_type(path) + ')'\n            if packagename not in packages:\n                packages[packagename] = {'initscripts': [], 'systemdservice': [], 'processes': [program],\n                                         'process_name': name}\n            else:\n                if program not in packages[packagename]['processes']:\n                    packages[packagename]['processes'].append(program)\n\n    if not packages and not kernel_restart:\n        return 'No packages seem to need to be restarted.'\n\n    for package in packages:\n        _check_timeout(start_time, timeout)\n        cmd = cmd_pkg_query + package\n        paths = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)\n\n        while True:\n            _check_timeout(start_time, timeout)\n            line = salt.utils.stringutils.to_unicode(paths.stdout.readline())\n            if not line:\n                break\n            pth = line[:-1]\n            if pth.startswith('/etc/init.d/') and not pth.endswith('.sh'):\n                packages[package]['initscripts'].append(pth[12:])\n\n            if os.path.exists(systemd) and pth.startswith(systemd_folder) and pth.endswith('.service') and \\\n               pth.find('.wants') == -1:\n                is_oneshot = False\n                try:\n                    servicefile = salt.utils.files.fopen(pth)  # pylint: disable=resource-leakage\n                except IOError:\n                    continue\n                sysfold_len = len(systemd_folder)\n\n                for line in servicefile.readlines():\n                    line = salt.utils.stringutils.to_unicode(line)\n                    if line.find('Type=oneshot') > 0:\n                        # scripts that does a single job and then exit\n                        is_oneshot = True\n                    continue\n                servicefile.close()\n\n                if not is_oneshot:\n                    packages[package]['systemdservice'].append(pth[sysfold_len:])\n\n            sys.stdout.flush()\n        paths.stdout.close()\n\n    # Alternatively, find init.d script or service that match the process name\n    for package in packages:\n        _check_timeout(start_time, timeout)\n        if not packages[package]['systemdservice'] and not packages[package]['initscripts']:\n            service = __salt__['service.available'](packages[package]['process_name'])\n\n            if service:\n                if os.path.exists('/etc/init.d/' + packages[package]['process_name']):\n                    packages[package]['initscripts'].append(packages[package]['process_name'])\n                else:\n                    packages[package]['systemdservice'].append(packages[package]['process_name'])\n\n    restartable = []\n    nonrestartable = []\n    restartinitcommands = []\n    restartservicecommands = []\n\n    for package in packages:\n        _check_timeout(start_time, timeout)\n        if packages[package]['initscripts']:\n            restartable.append(package)\n            restartinitcommands.extend(['service ' + s + ' restart' for s in packages[package]['initscripts']])\n        elif packages[package]['systemdservice']:\n            restartable.append(package)\n            restartservicecommands.extend(['systemctl restart ' + s for s in packages[package]['systemdservice']])\n        else:\n            nonrestartable.append(package)\n        if packages[package]['process_name'] in restart_services:\n            restart_services.remove(packages[package]['process_name'])\n\n    for restart_service in restart_services:\n        _check_timeout(start_time, timeout)\n        restartservicecommands.extend(['systemctl restart ' + restart_service])\n\n    ret = _format_output(kernel_restart, packages, verbose, restartable, nonrestartable,\n                         restartservicecommands, restartinitcommands)\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef default_vsan_policy_configured(name, policy):\n    '''\n    Configures the default VSAN policy on a vCenter.\n    The state assumes there is only one default VSAN policy on a vCenter.\n\n    policy\n        Dict representation of a policy\n    '''\n    # TODO Refactor when recurse_differ supports list_differ\n    # It's going to make the whole thing much easier\n    policy_copy = copy.deepcopy(policy)\n    proxy_type = __salt__['vsphere.get_proxy_type']()\n    log.trace('proxy_type = %s', proxy_type)\n    # All allowed proxies have a shim execution module with the same\n    # name which implementes a get_details function\n    # All allowed proxies have a vcenter detail\n    vcenter = __salt__['{0}.get_details'.format(proxy_type)]()['vcenter']\n    log.info('Running %s on vCenter \\'%s\\'', name, vcenter)\n    log.trace('policy = %s', policy)\n    changes_required = False\n    ret = {'name': name,\n           'changes': {},\n           'result': None,\n           'comment': None}\n    comments = []\n    changes = {}\n    changes_required = False\n    si = None\n\n    try:\n        #TODO policy schema validation\n        si = __salt__['vsphere.get_service_instance_via_proxy']()\n        current_policy = __salt__['vsphere.list_default_vsan_policy'](si)\n        log.trace('current_policy = %s', current_policy)\n        # Building all diffs between the current and expected policy\n        # XXX We simplify the comparison by assuming we have at most 1\n        # sub_profile\n        if policy.get('subprofiles'):\n            if len(policy['subprofiles']) > 1:\n                raise ArgumentValueError('Multiple sub_profiles ({0}) are not '\n                                         'supported in the input policy')\n            subprofile = policy['subprofiles'][0]\n            current_subprofile = current_policy['subprofiles'][0]\n            capabilities_differ = list_diff(current_subprofile['capabilities'],\n                                            subprofile.get('capabilities', []),\n                                            key='id')\n            del policy['subprofiles']\n            if subprofile.get('capabilities'):\n                del subprofile['capabilities']\n            del current_subprofile['capabilities']\n            # Get the subprofile diffs without the capability keys\n            subprofile_differ = recursive_diff(current_subprofile,\n                                               dict(subprofile))\n\n        del current_policy['subprofiles']\n        policy_differ = recursive_diff(current_policy, policy)\n        if policy_differ.diffs or capabilities_differ.diffs or \\\n           subprofile_differ.diffs:\n\n            if 'name' in policy_differ.new_values or \\\n               'description' in policy_differ.new_values:\n\n                raise ArgumentValueError(\n                    '\\'name\\' and \\'description\\' of the default VSAN policy '\n                    'cannot be updated')\n            changes_required = True\n            if __opts__['test']:\n                str_changes = []\n                if policy_differ.diffs:\n                    str_changes.extend([change for change in\n                                        policy_differ.changes_str.split('\\n')])\n                if subprofile_differ.diffs or capabilities_differ.diffs:\n                    str_changes.append('subprofiles:')\n                    if subprofile_differ.diffs:\n                        str_changes.extend(\n                            ['  {0}'.format(change) for change in\n                             subprofile_differ.changes_str.split('\\n')])\n                    if capabilities_differ.diffs:\n                        str_changes.append('  capabilities:')\n                        str_changes.extend(\n                            ['  {0}'.format(change) for change in\n                             capabilities_differ.changes_str2.split('\\n')])\n                comments.append(\n                    'State {0} will update the default VSAN policy on '\n                    'vCenter \\'{1}\\':\\n{2}'\n                    ''.format(name, vcenter, '\\n'.join(str_changes)))\n            else:\n                __salt__['vsphere.update_storage_policy'](\n                    policy=current_policy['name'],\n                    policy_dict=policy_copy,\n                    service_instance=si)\n                comments.append('Updated the default VSAN policy in vCenter '\n                                '\\'{0}\\''.format(vcenter))\n            log.info(comments[-1])\n\n            new_values = policy_differ.new_values\n            new_values['subprofiles'] = [subprofile_differ.new_values]\n            new_values['subprofiles'][0]['capabilities'] = \\\n                    capabilities_differ.new_values\n            if not new_values['subprofiles'][0]['capabilities']:\n                del new_values['subprofiles'][0]['capabilities']\n            if not new_values['subprofiles'][0]:\n                del new_values['subprofiles']\n            old_values = policy_differ.old_values\n            old_values['subprofiles'] = [subprofile_differ.old_values]\n            old_values['subprofiles'][0]['capabilities'] = \\\n                    capabilities_differ.old_values\n            if not old_values['subprofiles'][0]['capabilities']:\n                del old_values['subprofiles'][0]['capabilities']\n            if not old_values['subprofiles'][0]:\n                del old_values['subprofiles']\n            changes.update({'default_vsan_policy':\n                            {'new': new_values,\n                             'old': old_values}})\n            log.trace(changes)\n        __salt__['vsphere.disconnect'](si)\n    except CommandExecutionError as exc:\n        log.error('Error: %s', exc)\n        if si:\n            __salt__['vsphere.disconnect'](si)\n        if not __opts__['test']:\n            ret['result'] = False\n        ret.update({'comment': exc.strerror,\n                    'result': False if not __opts__['test'] else None})\n        return ret\n    if not changes_required:\n        # We have no changes\n        ret.update({'comment': ('Default VSAN policy in vCenter '\n                                '\\'{0}\\' is correctly configured. '\n                                'Nothing to be done.'.format(vcenter)),\n                    'result': True})\n    else:\n        ret.update({\n            'comment': '\\n'.join(comments),\n            'changes': changes,\n            'result': None if __opts__['test'] else True,\n        })\n    return ret", "response": "Configure the default VSAN policy on a vCenter."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef default_storage_policy_assigned(name, policy, datastore):\n    '''\n    Assigns a default storage policy to a datastore\n\n    policy\n        Name of storage policy\n\n    datastore\n        Name of datastore\n    '''\n    log.info('Running state %s for policy \\'%s\\', datastore \\'%s\\'.',\n             name, policy, datastore)\n    changes = {}\n    changes_required = False\n    ret = {'name': name,\n           'changes': {},\n           'result': None,\n           'comment': None}\n    si = None\n    try:\n        si = __salt__['vsphere.get_service_instance_via_proxy']()\n        existing_policy = \\\n            __salt__['vsphere.list_default_storage_policy_of_datastore'](\n                datastore=datastore, service_instance=si)\n        if existing_policy['name'] == policy:\n            comment = ('Storage policy \\'{0}\\' is already assigned to '\n                       'datastore \\'{1}\\'. Nothing to be done.'\n                       ''.format(policy, datastore))\n        else:\n            changes_required = True\n            changes = {\n                'default_storage_policy': {'old': existing_policy['name'],\n                                           'new': policy}}\n            if __opts__['test']:\n                comment = ('State {0} will assign storage policy \\'{1}\\' to '\n                           'datastore \\'{2}\\'.').format(name, policy,\n                                                        datastore)\n            else:\n                __salt__['vsphere.assign_default_storage_policy_to_datastore'](\n                    policy=policy, datastore=datastore, service_instance=si)\n                comment = ('Storage policy \\'{0} was assigned to datastore '\n                           '\\'{1}\\'.').format(policy, name)\n        log.info(comment)\n    except CommandExecutionError as exc:\n        log.error('Error: %s', exc)\n        if si:\n            __salt__['vsphere.disconnect'](si)\n        ret.update({'comment': exc.strerror,\n                    'result': False if not __opts__['test'] else None})\n        return ret\n\n    ret['comment'] = comment\n    if changes_required:\n        ret.update({\n            'changes': changes,\n            'result': None if __opts__['test'] else True,\n        })\n    else:\n        ret['result'] = True\n    return ret", "response": "Assign a default storage policy to a datastore"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a cloud client", "response": "def _get_client():\n    '''\n    Return a cloud client\n    '''\n    client = salt.cloud.CloudClient(\n        os.path.join(os.path.dirname(__opts__['conf_file']), 'cloud'),\n        pillars=copy.deepcopy(__pillar__.get('cloud', {}))\n    )\n    return client"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef has_instance(name, provider=None):\n    '''\n    Return true if the instance is found on a provider\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt minionname cloud.has_instance myinstance\n    '''\n    data = get_instance(name, provider)\n    if data is None:\n        return False\n    return True", "response": "Return true if the instance is found on a provider\n    CLI Example : bash\n\n    cloud. has_instance myinstance\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns details on an instance.", "response": "def get_instance(name, provider=None):\n    '''\n    Return details on an instance.\n\n    Similar to the cloud action show_instance\n    but returns only the instance details.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt minionname cloud.get_instance myinstance\n\n    SLS Example:\n\n    .. code-block:: bash\n\n        {{ salt['cloud.get_instance']('myinstance')['mac_address'] }}\n\n    '''\n    data = action(fun='show_instance', names=[name], provider=provider)\n    info = salt.utils.data.simple_types_filter(data)\n    try:\n        # get the first: [alias][driver][vm_name]\n        info = next(six.itervalues(next(six.itervalues(next(six.itervalues(info))))))\n    except AttributeError:\n        return None\n    return info"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef profile_(profile, names, vm_overrides=None, opts=None, **kwargs):\n    '''\n    Spin up an instance using Salt Cloud\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt minionname cloud.profile my-gce-config myinstance\n    '''\n    client = _get_client()\n    if isinstance(opts, dict):\n        client.opts.update(opts)\n    info = client.profile(profile, names, vm_overrides=vm_overrides, **kwargs)\n    return info", "response": "Spin up an instance using Salt Cloud\n    CLI Example"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef map_run(path=None, **kwargs):\n    '''\n    Execute a salt cloud map file\n\n    Cloud Map data can be retrieved from several sources:\n\n    - a local file (provide the path to the file to the 'path' argument)\n    - a JSON-formatted map directly (provide the appropriately formatted to using the 'map_data' argument)\n    - the Salt Pillar (provide the map name of under 'pillar:cloud:maps' to the 'map_pillar' argument)\n\n    .. note::\n        Only one of these sources can be read at a time. The options are listed\n        in their order of precedence.\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt minionname cloud.map_run /path/to/cloud.map\n        salt minionname cloud.map_run path=/path/to/cloud.map\n        salt minionname cloud.map_run map_pillar='<map_pillar>'\n          .. versionchanged:: 2018.3.1\n        salt minionname cloud.map_run map_data='<actual map data>'\n    '''\n    client = _get_client()\n    info = client.map_run(path, **kwargs)\n    return info", "response": "Execute a salt cloud map file"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef action(\n        fun=None,\n        cloudmap=None,\n        names=None,\n        provider=None,\n        instance=None,\n        **kwargs):\n    '''\n    Execute a single action on the given provider/instance\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt minionname cloud.action start instance=myinstance\n        salt minionname cloud.action stop instance=myinstance\n        salt minionname cloud.action show_image provider=my-ec2-config image=ami-1624987f\n    '''\n    client = _get_client()\n    try:\n        info = client.action(fun, cloudmap, names, provider, instance, kwargs)\n    except SaltCloudConfigError as err:\n        log.error(err)\n        return None\n\n    return info", "response": "Execute a single action on the given provider and instance"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating an instance using Salt Cloud CLI Example", "response": "def create(provider, names, opts=None, **kwargs):\n    '''\n    Create an instance using Salt Cloud\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt minionname cloud.create my-ec2-config myinstance image=ami-1624987f size='t1.micro' ssh_username=ec2-user securitygroup=default delvol_on_destroy=True\n    '''\n    client = _get_client()\n    if isinstance(opts, dict):\n        client.opts.update(opts)\n    info = client.create(provider, names, **kwargs)\n    return info"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef volume_list(provider):\n    '''\n    List block storage volumes\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt minionname cloud.volume_list my-nova\n\n    '''\n    client = _get_client()\n    info = client.extra_action(action='volume_list', provider=provider, names='name')\n    return info['name']", "response": "List block storage volumes"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef volume_attach(provider, names, **kwargs):\n    '''\n    Attach volume to a server\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt minionname cloud.volume_attach my-nova myblock server_name=myserver device='/dev/xvdf'\n\n    '''\n    client = _get_client()\n    info = client.extra_action(provider=provider, names=names, action='volume_attach', **kwargs)\n    return info", "response": "Attach a volume to a server\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef network_list(provider):\n    '''\n    List private networks\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt minionname cloud.network_list my-nova\n\n    '''\n    client = _get_client()\n    return client.extra_action(action='network_list', provider=provider, names='names')", "response": "List private networks\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt minionname cloud.network_list my-nova"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef network_create(provider, names, **kwargs):\n    '''\n    Create private network\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt minionname cloud.network_create my-nova names=['salt'] cidr='192.168.100.0/24'\n\n    '''\n    client = _get_client()\n    return client.extra_action(provider=provider, names=names, action='network_create', **kwargs)", "response": "Create private network in a single provider"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef virtual_interface_list(provider, names, **kwargs):\n    '''\n    List virtual interfaces on a server\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt minionname cloud.virtual_interface_list my-nova names=['salt-master']\n\n    '''\n    client = _get_client()\n    return client.extra_action(provider=provider, names=names, action='virtual_interface_list', **kwargs)", "response": "List virtual interfaces on a server"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nattaches private interfaces to a server", "response": "def virtual_interface_create(provider, names, **kwargs):\n    '''\n    Attach private interfaces to a server\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt minionname cloud.virtual_interface_create my-nova names=['salt-master'] net_name='salt'\n\n    '''\n    client = _get_client()\n    return client.extra_action(provider=provider, names=names, action='virtual_interface_create', **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef show(config_file=False):\n    '''\n    Return a list of sysctl parameters for this minion\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' sysctl.show\n    '''\n    roots = (\n        'kern',\n        'vm',\n        'vfs',\n        'net',\n        'hw',\n        'machdep',\n        'user',\n        'ddb',\n        'proc',\n        'emul',\n        'security',\n        'init'\n    )\n    cmd = 'sysctl -ae'\n    ret = {}\n    out = __salt__['cmd.run'](cmd, output_loglevel='trace')\n    comps = ['']\n    for line in out.splitlines():\n        if any([line.startswith('{0}.'.format(root)) for root in roots]):\n            comps = re.split('[=:]', line, 1)\n            ret[comps[0]] = comps[1]\n        elif comps[0]:\n            ret[comps[0]] += '{0}\\n'.format(line)\n        else:\n            continue\n    return ret", "response": "Return a list of sysctl parameters for this minion"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a single sysctl parameter for this minion", "response": "def get(name):\n    '''\n    Return a single sysctl parameter for this minion\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' sysctl.get hw.physmem\n    '''\n    cmd = 'sysctl -n {0}'.format(name)\n    out = __salt__['cmd.run'](cmd, python_shell=False)\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nassigns and persist a simple sysctl parameter for this minion", "response": "def persist(name, value, config='/etc/sysctl.conf'):\n    '''\n    Assign and persist a simple sysctl parameter for this minion\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' sysctl.persist net.inet.icmp.icmplim 50\n    '''\n    nlines = []\n    edited = False\n    value = six.text_type(value)\n\n    # create /etc/sysctl.conf if not present\n    if not os.path.isfile(config):\n        try:\n            with salt.utils.files.fopen(config, 'w+'):\n                pass\n        except (IOError, OSError):\n            msg = 'Could not create {0}'\n            raise CommandExecutionError(msg.format(config))\n\n    with salt.utils.files.fopen(config, 'r') as ifile:\n        for line in ifile:\n            line = salt.utils.stringutils.to_unicode(line)\n            m = re.match(r'{0}(\\??=)'.format(name), line)\n            if not m:\n                nlines.append(line)\n                continue\n            else:\n                key, rest = line.split('=', 1)\n                if rest.startswith('\"'):\n                    _, rest_v, rest = rest.split('\"', 2)\n                elif rest.startswith('\\''):\n                    _, rest_v, rest = rest.split('\\'', 2)\n                else:\n                    rest_v = rest.split()[0]\n                    rest = rest[len(rest_v):]\n                if rest_v == value:\n                    return 'Already set'\n                new_line = '{0}{1}{2}{3}'.format(name, m.group(1), value, rest)\n                nlines.append(new_line)\n                edited = True\n\n    if not edited:\n        newline = '{0}={1}'.format(name, value)\n        nlines.append(\"{0}\\n\".format(newline))\n\n    with salt.utils.files.fopen(config, 'wb') as ofile:\n        ofile.writelines(\n            salt.utils.data.encode(nlines)\n        )\n\n    assign(name, value)\n\n    return 'Updated'"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsends a message to a PushOver channel.", "response": "def post_message(name,\n                 user=None,\n                 device=None,\n                 message=None,\n                 title=None,\n                 priority=None,\n                 expire=None,\n                 retry=None,\n                 sound=None,\n                 api_version=1,\n                 token=None):\n    '''\n    Send a message to a PushOver channel.\n\n    .. code-block:: yaml\n\n        pushover-message:\n          pushover.post_message:\n            - user: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n            - token: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n            - title: Salt Returner\n            - device: phone\n            - priority: -1\n            - expire: 3600\n            - retry: 5\n\n    The following parameters are required:\n\n    name\n        The unique name for this event.\n\n    user\n        The user or group of users to send the message to. Must be ID of user, not name\n        or email address.\n\n    message\n        The message that is to be sent to the PushOver channel.\n\n    The following parameters are optional:\n\n    title\n        The title to use for the message.\n\n    device\n        The device for the user to send the message to.\n\n    priority\n        The priority for the message.\n\n    expire\n        The message should expire after specified amount of seconds.\n\n    retry\n        The message should be resent this many times.\n\n    token\n        The token for PushOver to use for authentication,\n        if not specified in the configuration options of master or minion.\n\n    '''\n    ret = {'name': name,\n           'changes': {},\n           'result': False,\n           'comment': ''}\n\n    if __opts__['test']:\n        ret['comment'] = 'The following message is to be sent to PushOver: {0}'.format(message)\n        ret['result'] = None\n        return ret\n\n    if not user:\n        ret['comment'] = 'PushOver user is missing: {0}'.format(user)\n        return ret\n\n    if not message:\n        ret['comment'] = 'PushOver message is missing: {0}'.format(message)\n        return ret\n\n    result = __salt__['pushover.post_message'](\n        user=user,\n        message=message,\n        title=title,\n        device=device,\n        priority=priority,\n        expire=expire,\n        retry=retry,\n        token=token,\n    )\n\n    if result:\n        ret['result'] = True\n        ret['comment'] = 'Sent message: {0}'.format(name)\n    else:\n        ret['comment'] = 'Failed to send message: {0}'.format(name)\n\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check_ip(addr):\n    '''\n    Check if address is a valid IP. returns True if valid, otherwise False.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt ns1 dig.check_ip 127.0.0.1\n        salt ns1 dig.check_ip 1111:2222:3333:4444:5555:6666:7777:8888\n    '''\n\n    try:\n        addr = addr.rsplit('/', 1)\n    except AttributeError:\n        # Non-string passed\n        return False\n\n    if salt.utils.network.is_ipv4(addr[0]):\n        try:\n            if 1 <= int(addr[1]) <= 32:\n                return True\n        except ValueError:\n            # Non-int subnet notation\n            return False\n        except IndexError:\n            # No subnet notation used (i.e. just an IPv4 address)\n            return True\n\n    if salt.utils.network.is_ipv6(addr[0]):\n        try:\n            if 8 <= int(addr[1]) <= 128:\n                return True\n        except ValueError:\n            # Non-int subnet notation\n            return False\n        except IndexError:\n            # No subnet notation used (i.e. just an IPv4 address)\n            return True\n\n    return False", "response": "Check if the given address is a valid IP. returns True if valid otherwise False."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef AAAA(host, nameserver=None):\n    '''\n    Return the AAAA record for ``host``.\n\n    Always returns a list.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt ns1 dig.AAAA www.google.com\n    '''\n    dig = ['dig', '+short', six.text_type(host), 'AAAA']\n\n    if nameserver is not None:\n        dig.append('@{0}'.format(nameserver))\n\n    cmd = __salt__['cmd.run_all'](dig, python_shell=False)\n    # In this case, 0 is not the same as False\n    if cmd['retcode'] != 0:\n        log.warning(\n            'dig returned exit code \\'%s\\'. Returning empty list as fallback.',\n            cmd['retcode']\n        )\n        return []\n\n    # make sure all entries are IPs\n    return [x for x in cmd['stdout'].split('\\n') if check_ip(x)]", "response": "Return the AAAA record for host."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef NS(domain, resolve=True, nameserver=None):\n    '''\n    Return a list of IPs of the nameservers for ``domain``\n\n    If ``resolve`` is False, don't resolve names.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt ns1 dig.NS google.com\n    '''\n    dig = ['dig', '+short', six.text_type(domain), 'NS']\n\n    if nameserver is not None:\n        dig.append('@{0}'.format(nameserver))\n\n    cmd = __salt__['cmd.run_all'](dig, python_shell=False)\n    # In this case, 0 is not the same as False\n    if cmd['retcode'] != 0:\n        log.warning(\n            'dig returned exit code \\'%s\\'. Returning empty list as fallback.',\n            cmd['retcode']\n        )\n        return []\n\n    if resolve:\n        ret = []\n        for ns_host in cmd['stdout'].split('\\n'):\n            for ip_addr in A(ns_host, nameserver):\n                ret.append(ip_addr)\n        return ret\n\n    return cmd['stdout'].split('\\n')", "response": "Return a list of IPs of the nameservers for the given domain."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the allowed IPv4 ranges in the specified record for the specified domain.", "response": "def SPF(domain, record='SPF', nameserver=None):\n    '''\n    Return the allowed IPv4 ranges in the SPF record for ``domain``.\n\n    If record is ``SPF`` and the SPF record is empty, the TXT record will be\n    searched automatically. If you know the domain uses TXT and not SPF,\n    specifying that will save a lookup.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt ns1 dig.SPF google.com\n    '''\n    spf_re = re.compile(r'(?:\\+|~)?(ip[46]|include):(.+)')\n    cmd = ['dig', '+short', six.text_type(domain), record]\n\n    if nameserver is not None:\n        cmd.append('@{0}'.format(nameserver))\n\n    result = __salt__['cmd.run_all'](cmd, python_shell=False)\n    # In this case, 0 is not the same as False\n    if result['retcode'] != 0:\n        log.warning(\n            'dig returned exit code \\'%s\\'. Returning empty list as fallback.',\n            result['retcode']\n        )\n        return []\n\n    if result['stdout'] == '' and record == 'SPF':\n        # empty string is successful query, but nothing to return. So, try TXT\n        # record.\n        return SPF(domain, 'TXT', nameserver)\n\n    sections = re.sub('\"', '', result['stdout']).split()\n    if not sections or sections[0] != 'v=spf1':\n        return []\n\n    if sections[1].startswith('redirect='):\n        # Run a lookup on the part after 'redirect=' (9 chars)\n        return SPF(sections[1][9:], 'SPF', nameserver)\n    ret = []\n    for section in sections[1:]:\n        try:\n            mechanism, address = spf_re.match(section).groups()\n        except AttributeError:\n            # Regex was not matched\n            continue\n        if mechanism == 'include':\n            ret.extend(SPF(address, 'SPF', nameserver))\n        elif mechanism in ('ip4', 'ip6') and check_ip(address):\n            ret.append(address)\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef MX(domain, resolve=False, nameserver=None):\n    '''\n    Return a list of lists for the MX of ``domain``.\n\n    If the ``resolve`` argument is True, resolve IPs for the servers.\n\n    It's limited to one IP, because although in practice it's very rarely a\n    round robin, it is an acceptable configuration and pulling just one IP lets\n    the data be similar to the non-resolved version. If you think an MX has\n    multiple IPs, don't use the resolver here, resolve them in a separate step.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt ns1 dig.MX google.com\n    '''\n    dig = ['dig', '+short', six.text_type(domain), 'MX']\n\n    if nameserver is not None:\n        dig.append('@{0}'.format(nameserver))\n\n    cmd = __salt__['cmd.run_all'](dig, python_shell=False)\n    # In this case, 0 is not the same as False\n    if cmd['retcode'] != 0:\n        log.warning(\n            'dig returned exit code \\'%s\\'. Returning empty list as fallback.',\n            cmd['retcode']\n        )\n        return []\n\n    stdout = [x.split() for x in cmd['stdout'].split('\\n')]\n\n    if resolve:\n        return [\n            (lambda x: [x[0], A(x[1], nameserver)[0]])(x) for x in stdout\n        ]\n\n    return stdout", "response": "Return a list of lists for the MX of the given domain."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the TXT record for host.", "response": "def TXT(host, nameserver=None):\n    '''\n    Return the TXT record for ``host``.\n\n    Always returns a list.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt ns1 dig.TXT google.com\n    '''\n    dig = ['dig', '+short', six.text_type(host), 'TXT']\n\n    if nameserver is not None:\n        dig.append('@{0}'.format(nameserver))\n\n    cmd = __salt__['cmd.run_all'](dig, python_shell=False)\n\n    if cmd['retcode'] != 0:\n        log.warning(\n            'dig returned exit code \\'%s\\'. Returning empty list as fallback.',\n            cmd['retcode']\n        )\n        return []\n\n    return [i for i in cmd['stdout'].split('\\n')]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of sysctl parameters for this minion", "response": "def show(config_file=False):\n    '''\n    Return a list of sysctl parameters for this minion\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' sysctl.show\n    '''\n    roots = (\n        'compat',\n        'debug',\n        'dev',\n        'hptmv',\n        'hw',\n        'kern',\n        'machdep',\n        'net',\n        'p1003_1b',\n        'security',\n        'user',\n        'vfs',\n        'vm'\n    )\n    cmd = 'sysctl -ae'\n    ret = {}\n    comps = ['']\n\n    if config_file:\n        # If the file doesn't exist, return an empty list\n        if not os.path.exists(config_file):\n            return []\n\n        try:\n            with salt.utils.files.fopen(config_file, 'r') as f:\n                for line in f.readlines():\n                    l = line.strip()\n                    if l != \"\" and not l.startswith(\"#\"):\n                        comps = line.split('=', 1)\n                        ret[comps[0]] = comps[1]\n            return ret\n        except (OSError, IOError):\n            log.error('Could not open sysctl config file')\n            return None\n    else:\n        out = __salt__['cmd.run'](cmd, output_loglevel='trace')\n        for line in out.splitlines():\n            if any([line.startswith('{0}.'.format(root)) for root in roots]):\n                comps = line.split('=', 1)\n                ret[comps[0]] = comps[1]\n            elif comps[0]:\n                ret[comps[0]] += '{0}\\n'.format(line)\n            else:\n                continue\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef assign(name, value):\n    '''\n    Assign a single sysctl parameter for this minion\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' sysctl.assign net.inet.icmp.icmplim 50\n    '''\n    ret = {}\n    cmd = 'sysctl {0}=\"{1}\"'.format(name, value)\n    data = __salt__['cmd.run_all'](cmd, python_shell=False)\n\n    if data['retcode'] != 0:\n        raise CommandExecutionError('sysctl failed: {0}'.format(\n            data['stderr']))\n    new_name, new_value = data['stdout'].split(':', 1)\n    ret[new_name] = new_value.split(' -> ')[-1]\n    return ret", "response": "Assign a single sysctl parameter for this minion"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef persist(name, value, config='/etc/sysctl.conf'):\n    '''\n    Assign and persist a simple sysctl parameter for this minion\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' sysctl.persist net.inet.icmp.icmplim 50\n        salt '*' sysctl.persist coretemp_load NO config=/boot/loader.conf\n    '''\n    nlines = []\n    edited = False\n    value = six.text_type(value)\n\n    with salt.utils.files.fopen(config, 'r') as ifile:\n        for line in ifile:\n            line = salt.utils.stringutils.to_unicode(line).rstrip('\\n')\n            if not line.startswith('{0}='.format(name)):\n                nlines.append(line)\n                continue\n            else:\n                key, rest = line.split('=', 1)\n                if rest.startswith('\"'):\n                    _, rest_v, rest = rest.split('\"', 2)\n                elif rest.startswith('\\''):\n                    _, rest_v, rest = rest.split('\\'', 2)\n                else:\n                    rest_v = rest.split()[0]\n                    rest = rest[len(rest_v):]\n                if rest_v == value:\n                    return 'Already set'\n                new_line = _formatfor(key, value, config, rest)\n                nlines.append(new_line)\n                edited = True\n    if not edited:\n        nlines.append(\"{0}\\n\".format(_formatfor(name, value, config)))\n    with salt.utils.files.fopen(config, 'w+') as ofile:\n        nlines = [salt.utils.stringutils.to_str(_l) + '\\n' for _l in nlines]\n        ofile.writelines(nlines)\n    if config != '/boot/loader.conf':\n        assign(name, value)\n    return 'Updated'", "response": "Assign and persist a simple sysctl parameter for this minion"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _workout_filename(filename):\n    '''\n    Recursively workout the file name from an augeas change\n    '''\n    if os.path.isfile(filename) or filename == '/':\n        if filename == '/':\n            filename = None\n        return filename\n    else:\n        return _workout_filename(os.path.dirname(filename))", "response": "Recursively workout the file name from an augeas change\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks if the filepath of the changes are valid.", "response": "def _check_filepath(changes):\n    '''\n    Ensure all changes are fully qualified and affect only one file.\n    This ensures that the diff output works and a state change is not\n    incorrectly reported.\n    '''\n    filename = None\n    for change_ in changes:\n        try:\n            cmd, arg = change_.split(' ', 1)\n\n            if cmd not in METHOD_MAP:\n                error = 'Command {0} is not supported (yet)'.format(cmd)\n                raise ValueError(error)\n            method = METHOD_MAP[cmd]\n            parts = salt.utils.args.shlex_split(arg)\n            if method in ['set', 'setm', 'move', 'remove']:\n                filename_ = parts[0]\n            else:\n                _, _, filename_ = parts\n            if not filename_.startswith('/files'):\n                error = 'Changes should be prefixed with ' \\\n                        '/files if no context is provided,' \\\n                        ' change: {0}'.format(change_)\n                raise ValueError(error)\n            filename_ = re.sub('^/files|/$', '', filename_)\n            if filename is not None:\n                if filename != filename_:\n                    error = 'Changes should be made to one ' \\\n                            'file at a time, detected changes ' \\\n                            'to {0} and {1}'.format(filename, filename_)\n                    raise ValueError(error)\n            filename = filename_\n        except (ValueError, IndexError) as err:\n            log.error(err)\n            if 'error' not in locals():\n                error = 'Invalid formatted command, ' \\\n                               'see debug log for details: {0}' \\\n                               .format(change_)\n            else:\n                error = six.text_type(err)\n            raise ValueError(error)\n\n    filename = _workout_filename(filename)\n\n    return filename"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef change(name, context=None, changes=None, lens=None,\n        load_path=None, **kwargs):\n    '''\n    .. versionadded:: 2014.7.0\n\n    This state replaces :py:func:`~salt.states.augeas.setvalue`.\n\n    Issue changes to Augeas, optionally for a specific context, with a\n    specific lens.\n\n    name\n        State name\n\n    context\n        A file path, prefixed by ``/files``. Should resolve to an actual file\n        (not an arbitrary augeas path). This is used to avoid duplicating the\n        file name for each item in the changes list (for example, ``set bind 0.0.0.0``\n        in the example below operates on the file specified by ``context``). If\n        ``context`` is not specified, a file path prefixed by ``/files`` should be\n        included with the ``set`` command.\n\n        The file path is examined to determine if the\n        specified changes are already present.\n\n        .. code-block:: yaml\n\n            redis-conf:\n              augeas.change:\n                - context: /files/etc/redis/redis.conf\n                - changes:\n                  - set bind 0.0.0.0\n                  - set maxmemory 1G\n\n    changes\n        List of changes that are issued to Augeas. Available commands are\n        ``set``, ``setm``, ``mv``/``move``, ``ins``/``insert``, and\n        ``rm``/``remove``.\n\n    lens\n        The lens to use, needs to be suffixed with `.lns`, e.g.: `Nginx.lns`.\n        See the `list of stock lenses <http://augeas.net/stock_lenses.html>`_\n        shipped with Augeas.\n\n    .. versionadded:: 2016.3.0\n\n    load_path\n        A list of directories that modules should be searched in. This is in\n        addition to the standard load path and the directories in\n        AUGEAS_LENS_LIB.\n\n\n    Usage examples:\n\n    Set the ``bind`` parameter in ``/etc/redis/redis.conf``:\n\n    .. code-block:: yaml\n\n        redis-conf:\n          augeas.change:\n            - changes:\n              - set /files/etc/redis/redis.conf/bind 0.0.0.0\n\n    .. note::\n\n        Use the ``context`` parameter to specify the file you want to\n        manipulate. This way you don't have to include this in the changes\n        every time:\n\n        .. code-block:: yaml\n\n            redis-conf:\n              augeas.change:\n                - context: /files/etc/redis/redis.conf\n                - changes:\n                  - set bind 0.0.0.0\n                  - set databases 4\n                  - set maxmemory 1G\n\n    Augeas is aware of a lot of common configuration files and their syntax.\n    It knows the difference between for example ini and yaml files, but also\n    files with very specific syntax, like the hosts file. This is done with\n    *lenses*, which provide mappings between the Augeas tree and the file.\n\n    There are many `preconfigured lenses`_ that come with Augeas by default,\n    and they specify the common locations for configuration files. So most\n    of the time Augeas will know how to manipulate a file. In the event that\n    you need to manipulate a file that Augeas doesn't know about, you can\n    specify the lens to use like this:\n\n    .. code-block:: yaml\n\n        redis-conf:\n          augeas.change:\n            - lens: redis.lns\n            - context: /files/etc/redis/redis.conf\n            - changes:\n              - set bind 0.0.0.0\n\n    .. note::\n\n        Even though Augeas knows that ``/etc/redis/redis.conf`` is a Redis\n        configuration file and knows how to parse it, it is recommended to\n        specify the lens anyway. This is because by default, Augeas loads all\n        known lenses and their associated file paths. All these files are\n        parsed when Augeas is loaded, which can take some time. When specifying\n        a lens, Augeas is loaded with only that lens, which speeds things up\n        quite a bit.\n\n    .. _preconfigured lenses: http://augeas.net/stock_lenses.html\n\n    A more complex example, this adds an entry to the services file for Zabbix,\n    and removes an obsolete service:\n\n    .. code-block:: yaml\n\n        zabbix-service:\n          augeas.change:\n            - lens: services.lns\n            - context: /files/etc/services\n            - changes:\n              - ins service-name after service-name[last()]\n              - set service-name[last()] \"zabbix-agent\"\n              - set \"service-name[. = 'zabbix-agent']/port\" 10050\n              - set \"service-name[. = 'zabbix-agent']/protocol\" tcp\n              - set \"service-name[. = 'zabbix-agent']/#comment\" \"Zabbix Agent service\"\n              - rm \"service-name[. = 'im-obsolete']\"\n            - unless: grep \"zabbix-agent\" /etc/services\n\n    .. warning::\n\n        Don't forget the ``unless`` here, otherwise it will fail on next runs\n        because the service is already defined. Additionally you have to quote\n        lines containing ``service-name[. = 'zabbix-agent']`` otherwise\n        :mod:`augeas_cfg <salt.modules.augeas_cfg>` execute will fail because\n        it will receive more parameters than expected.\n\n    .. note::\n\n        Order is important when defining a service with Augeas, in this case\n        it's ``port``, ``protocol`` and ``#comment``. For more info about\n        the lens check `services lens documentation`_.\n\n    .. _services lens documentation:\n\n    http://augeas.net/docs/references/lenses/files/services-aug.html#Services.record\n\n    '''\n    ret = {'name': name, 'result': False, 'comment': '', 'changes': {}}\n\n    if not changes or not isinstance(changes, list):\n        ret['comment'] = '\\'changes\\' must be specified as a list'\n        return ret\n\n    if load_path is not None:\n        if not isinstance(load_path, list):\n            ret['comment'] = '\\'load_path\\' must be specified as a list'\n            return ret\n        else:\n            load_path = ':'.join(load_path)\n\n    filename = None\n    if context is None:\n        try:\n            filename = _check_filepath(changes)\n        except ValueError as err:\n            ret['comment'] = 'Error: {0}'.format(err)\n            return ret\n    else:\n        filename = re.sub('^/files|/$', '', context)\n\n    if __opts__['test']:\n        ret['result'] = True\n        ret['comment'] = 'Executing commands'\n        if context:\n            ret['comment'] += ' in file \"{0}\":\\n'.format(context)\n        ret['comment'] += \"\\n\".join(changes)\n        return ret\n\n    old_file = []\n    if filename is not None and os.path.isfile(filename):\n        with salt.utils.files.fopen(filename, 'r') as file_:\n            old_file = [salt.utils.stringutils.to_unicode(x)\n                        for x in file_.readlines()]\n\n    result = __salt__['augeas.execute'](\n        context=context, lens=lens,\n        commands=changes, load_path=load_path)\n    ret['result'] = result['retval']\n\n    if ret['result'] is False:\n        ret['comment'] = 'Error: {0}'.format(result['error'])\n        return ret\n\n    if filename is not None and os.path.isfile(filename):\n        with salt.utils.files.fopen(filename, 'r') as file_:\n            new_file = [salt.utils.stringutils.to_unicode(x)\n                        for x in file_.readlines()]\n            diff = ''.join(\n                difflib.unified_diff(old_file, new_file, n=0))\n\n        if diff:\n            ret['comment'] = 'Changes have been saved'\n            ret['changes'] = {'diff': diff}\n        else:\n            ret['comment'] = 'No changes made'\n\n    else:\n        ret['comment'] = 'Changes have been saved'\n        ret['changes'] = {'updates': changes}\n\n    return ret", "response": "A state that changes the value of a specific item in Augeas."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _connect(host=None, port=None, db=None, password=None):\n    '''\n    Returns an instance of the redis client\n    '''\n    if not host:\n        host = __salt__['config.option']('redis.host')\n    if not port:\n        port = __salt__['config.option']('redis.port')\n    if not db:\n        db = __salt__['config.option']('redis.db')\n    if not password:\n        password = __salt__['config.option']('redis.password')\n\n    return redis.StrictRedis(host, port, db, password, decode_responses=True)", "response": "Connect to the redis server"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning an instance of the redis client", "response": "def _sconnect(host=None, port=None, password=None):\n    '''\n    Returns an instance of the redis client\n    '''\n    if host is None:\n        host = __salt__['config.option']('redis_sentinel.host', 'localhost')\n    if port is None:\n        port = __salt__['config.option']('redis_sentinel.port', 26379)\n    if password is None:\n        password = __salt__['config.option']('redis_sentinel.password')\n\n    return redis.StrictRedis(host, port, password=password, decode_responses=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef config_get(pattern='*', host=None, port=None, db=None, password=None):\n    '''\n    Get redis server configuration values\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' redis.config_get\n        salt '*' redis.config_get port\n    '''\n    server = _connect(host, port, db, password)\n    return server.config_get(pattern)", "response": "Get redis server configuration values\n    CLI Example :"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef config_set(name, value, host=None, port=None, db=None, password=None):\n    '''\n    Set redis server configuration values\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' redis.config_set masterauth luv_kittens\n    '''\n    server = _connect(host, port, db, password)\n    return server.config_set(name, value)", "response": "Set redis server configuration values\n\n    CLI Example : redis. config_set masterauth luv_kittens\n    masterauth luv_kittens\n    masterauth luv_kittens\n    masterauth luv_kittens\n    masterauth luv_kittens\n    masterauth luv_kittens\n    masterauth luv_kittens\n    masterauth luv_kittens\n    masterauth luv"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the number of keys in the selected database", "response": "def dbsize(host=None, port=None, db=None, password=None):\n    '''\n    Return the number of keys in the selected database\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' redis.dbsize\n    '''\n    server = _connect(host, port, db, password)\n    return server.dbsize()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndelete the keys from redis returns number of keys deleted", "response": "def delete(*keys, **connection_args):\n    '''\n    Deletes the keys from redis, returns number of keys deleted\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' redis.delete foo\n    '''\n    # Get connection args from keywords if set\n    conn_args = {}\n    for arg in ['host', 'port', 'db', 'password']:\n        if arg in connection_args:\n            conn_args[arg] = connection_args[arg]\n\n    server = _connect(**conn_args)\n    return server.delete(*keys)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn true if the key exists in redis", "response": "def exists(key, host=None, port=None, db=None, password=None):\n    '''\n    Return true if the key exists in redis\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' redis.exists foo\n    '''\n    server = _connect(host, port, db, password)\n    return server.exists(key)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef expire(key, seconds, host=None, port=None, db=None, password=None):\n    '''\n    Set a keys time to live in seconds\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' redis.expire foo 300\n    '''\n    server = _connect(host, port, db, password)\n    return server.expire(key, seconds)", "response": "Set a keys time to live in seconds"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset a keys expire at given UNIX time", "response": "def expireat(key, timestamp, host=None, port=None, db=None, password=None):\n    '''\n    Set a keys expire at given UNIX time\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' redis.expireat foo 1400000000\n    '''\n    server = _connect(host, port, db, password)\n    return server.expireat(key, timestamp)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef flushall(host=None, port=None, db=None, password=None):\n    '''\n    Remove all keys from all databases\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' redis.flushall\n    '''\n    server = _connect(host, port, db, password)\n    return server.flushall()", "response": "Remove all keys from all databases\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove all keys from the selected database CLI Example: .. code-block:: bash salt '*' redis.flushdb", "response": "def flushdb(host=None, port=None, db=None, password=None):\n    '''\n    Remove all keys from the selected database\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' redis.flushdb\n    '''\n    server = _connect(host, port, db, password)\n    return server.flushdb()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget redis key value", "response": "def get_key(key, host=None, port=None, db=None, password=None):\n    '''\n    Get redis key value\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' redis.get_key foo\n    '''\n    server = _connect(host, port, db, password)\n    return server.get(key)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning True if a hash field exists in a Redis object.", "response": "def hexists(key, field, host=None, port=None, db=None, password=None):\n    '''\n    Determine if a hash fields exists.\n\n    .. versionadded:: 2017.7.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' redis.hexists foo_hash bar_field\n    '''\n    server = _connect(host, port, db, password)\n    return server.hexists(key, field)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef hgetall(key, host=None, port=None, db=None, password=None):\n    '''\n    Get all fields and values from a redis hash, returns dict\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' redis.hgetall foo_hash\n    '''\n    server = _connect(host, port, db, password)\n    return server.hgetall(key)", "response": "Get all fields and values from a redis hash Returns dict\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the integer value of a hash field by the given number.", "response": "def hincrby(key, field, increment=1, host=None, port=None, db=None, password=None):\n    '''\n    Increment the integer value of a hash field by the given number.\n\n    .. versionadded:: 2017.7.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' redis.hincrby foo_hash bar_field 5\n    '''\n    server = _connect(host, port, db, password)\n    return server.hincrby(key, field, amount=increment)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the number of fields of a hash.", "response": "def hlen(key, host=None, port=None, db=None, password=None):\n    '''\n    Returns number of fields of a hash.\n\n    .. versionadded:: 2017.7.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' redis.hlen foo_hash\n    '''\n    server = _connect(host, port, db, password)\n    return server.hlen(key)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef hmget(key, *fields, **options):\n    '''\n    Returns the values of all the given hash fields.\n\n    .. versionadded:: 2017.7.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' redis.hmget foo_hash bar_field1 bar_field2\n    '''\n    host = options.get('host', None)\n    port = options.get('port', None)\n    database = options.get('db', None)\n    password = options.get('password', None)\n    server = _connect(host, port, database, password)\n    return server.hmget(key, *fields)", "response": "Returns the values of all the given hash fields."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting multiple hash fields to multiple values.", "response": "def hmset(key, **fieldsvals):\n    '''\n    Sets multiple hash fields to multiple values.\n\n    .. versionadded:: 2017.7.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' redis.hmset foo_hash bar_field1=bar_value1 bar_field2=bar_value2\n    '''\n    host = fieldsvals.pop('host', None)\n    port = fieldsvals.pop('port', None)\n    database = fieldsvals.pop('db', None)\n    password = fieldsvals.pop('password', None)\n    server = _connect(host, port, database, password)\n    return server.hmset(key, salt.utils.args.clean_kwargs(**fieldsvals))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef hset(key, field, value, host=None, port=None, db=None, password=None):\n    '''\n    Set the value of a hash field.\n\n    .. versionadded:: 2017.7.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' redis.hset foo_hash bar_field bar_value\n    '''\n    server = _connect(host, port, db, password)\n    return server.hset(key, field, value)", "response": "Set the value of a hash field."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning all the values in a hash.", "response": "def hvals(key, host=None, port=None, db=None, password=None):\n    '''\n    Return all the values in a hash.\n\n    .. versionadded:: 2017.7.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' redis.hvals foo_hash bar_field1 bar_value1\n    '''\n    server = _connect(host, port, db, password)\n    return server.hvals(key)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef hscan(key, cursor=0, match=None, count=None, host=None, port=None, db=None, password=None):\n    '''\n    Incrementally iterate hash fields and associated values.\n\n    .. versionadded:: 2017.7.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' redis.hscan foo_hash match='field_prefix_*' count=1\n    '''\n    server = _connect(host, port, db, password)\n    return server.hscan(key, cursor=cursor, match=match, count=count)", "response": "Incrementally iterate hash fields and associated values."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef info(host=None, port=None, db=None, password=None):\n    '''\n    Get information and statistics about the server\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' redis.info\n    '''\n    server = _connect(host, port, db, password)\n    return server.info()", "response": "Get information about the server\n    CLI Example :"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting redis keys supports glob style patterns", "response": "def keys(pattern='*', host=None, port=None, db=None, password=None):\n    '''\n    Get redis keys, supports glob style patterns\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' redis.keys\n        salt '*' redis.keys test*\n    '''\n    server = _connect(host, port, db, password)\n    return server.keys(pattern)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting redis key type", "response": "def key_type(key, host=None, port=None, db=None, password=None):\n    '''\n    Get redis key type\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' redis.type foo\n    '''\n    server = _connect(host, port, db, password)\n    return server.type(key)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the UNIX time in seconds of the last successful save to disk CLI Example : bash salt '*' redis. lastsave", "response": "def lastsave(host=None, port=None, db=None, password=None):\n    '''\n    Get the UNIX time in seconds of the last successful save to disk\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' redis.lastsave\n    '''\n    # Use of %s to get the timestamp is not supported by Python. The reason it\n    # works is because it's passed to the system strftime which may not support\n    # it. See: https://stackoverflow.com/a/11743262\n    server = _connect(host, port, db, password)\n    if six.PY2:\n        return int((server.lastsave() - datetime(1970, 1, 1)).total_seconds())\n    else:\n        return int(server.lastsave().timestamp())"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nping the server returns False on connection errors", "response": "def ping(host=None, port=None, db=None, password=None):\n    '''\n    Ping the server, returns False on connection errors\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' redis.ping\n    '''\n    server = _connect(host, port, db, password)\n    try:\n        return server.ping()\n    except redis.ConnectionError:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset redis key value", "response": "def set_key(key, value, host=None, port=None, db=None, password=None):\n    '''\n    Set redis key value\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' redis.set_key foo bar\n    '''\n    server = _connect(host, port, db, password)\n    return server.set(key, value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nshuts down the server and save the dataset to disk and then shut down the server CLI Example", "response": "def shutdown(host=None, port=None, db=None, password=None):\n    '''\n    Synchronously save the dataset to disk and then shut down the server\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' redis.shutdown\n    '''\n    server = _connect(host, port, db, password)\n    try:\n        # Return false if unable to ping server\n        server.ping()\n    except redis.ConnectionError:\n        return False\n\n    server.shutdown()\n    try:\n        # This should fail now if the server is shutdown, which we want\n        server.ping()\n    except redis.ConnectionError:\n        return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmake the server a slave of another instance or promote it as master", "response": "def slaveof(master_host=None, master_port=None, host=None, port=None, db=None,\n            password=None):\n    '''\n    Make the server a slave of another instance, or promote it as master\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        # Become slave of redis-n01.example.com:6379\n        salt '*' redis.slaveof redis-n01.example.com 6379\n        salt '*' redis.slaveof redis-n01.example.com\n        # Become master\n        salt '*' redis.slaveof\n    '''\n    if master_host and not master_port:\n        master_port = 6379\n    server = _connect(host, port, db, password)\n    return server.slaveof(master_host, master_port)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef smembers(key, host=None, port=None, db=None, password=None):\n    '''\n    Get members in a Redis set\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' redis.smembers foo_set\n    '''\n    server = _connect(host, port, db, password)\n    return list(server.smembers(key))", "response": "Get members in a Redis set\n    CLI Example : smembers foo_set\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the current UNIX time in seconds", "response": "def time(host=None, port=None, db=None, password=None):\n    '''\n    Return the current server UNIX time in seconds\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' redis.time\n    '''\n    server = _connect(host, port, db, password)\n    return server.time()[0]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef zrange(key, start, stop, host=None, port=None, db=None, password=None):\n    '''\n    Get a range of values from a sorted set in Redis by index\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' redis.zrange foo_sorted 0 10\n    '''\n    server = _connect(host, port, db, password)\n    return server.zrange(key, start, stop)", "response": "Get a range of values from a sorted set in Redis by index\n    CLI Example : zrange foo_sorted 0 10\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget ip for sentinel master", "response": "def sentinel_get_master_ip(master, host=None, port=None, password=None):\n    '''\n    Get ip for sentinel master\n\n    .. versionadded: 2016.3.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' redis.sentinel_get_master_ip 'mymaster'\n    '''\n    server = _sconnect(host, port, password)\n    ret = server.sentinel_get_master_addr_by_name(master)\n    return dict(list(zip(('master_host', 'master_port'), ret)))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget master IP information about slave", "response": "def get_master_ip(host=None, port=None, password=None):\n    '''\n    Get host information about slave\n\n    .. versionadded: 2016.3.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' redis.get_master_ip\n    '''\n    server = _connect(host, port, password)\n    srv_info = server.info()\n    ret = (srv_info.get('master_host', ''), srv_info.get('master_port', ''))\n    return dict(list(zip(('master_host', 'master_port'), ret)))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_conn(key=None,\n              keyid=None,\n              profile=None,\n              region=None,\n              **kwargs):\n    '''\n    Create a boto3 client connection to EFS\n    '''\n    client = None\n    if profile:\n        if isinstance(profile, six.string_types):\n            if profile in __pillar__:\n                profile = __pillar__[profile]\n            elif profile in __opts__:\n                profile = __opts__[profile]\n    elif key or keyid or region:\n        profile = {}\n        if key:\n            profile['key'] = key\n        if keyid:\n            profile['keyid'] = keyid\n        if region:\n            profile['region'] = region\n\n    if isinstance(profile, dict):\n        if 'region' in profile:\n            profile['region_name'] = profile['region']\n            profile.pop('region', None)\n        if 'key' in profile:\n            profile['aws_secret_access_key'] = profile['key']\n            profile.pop('key', None)\n        if 'keyid' in profile:\n            profile['aws_access_key_id'] = profile['keyid']\n            profile.pop('keyid', None)\n\n        client = boto3.client('efs', **profile)\n    else:\n        client = boto3.client('efs')\n\n    return client", "response": "Create a boto3 client connection to EFS\nWorkItem"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_file_system(name,\n                       performance_mode='generalPurpose',\n                       keyid=None,\n                       key=None,\n                       profile=None,\n                       region=None,\n                       creation_token=None,\n                       **kwargs):\n    '''\n    Creates a new, empty file system.\n\n    name\n        (string) - The name for the new file system\n\n    performance_mode\n        (string) - The PerformanceMode of the file system. Can be either\n        generalPurpose or maxIO\n\n    creation_token\n        (string) - A unique name to be used as reference when creating an EFS.\n        This will ensure idempotency. Set to name if not specified otherwise\n\n    returns\n        (dict) - A dict of the data for the elastic file system\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt 'my-minion' boto_efs.create_file_system efs-name generalPurpose\n    '''\n\n    if creation_token is None:\n        creation_token = name\n\n    tags = {\"Key\": \"Name\", \"Value\": name}\n\n    client = _get_conn(key=key, keyid=keyid, profile=profile, region=region)\n\n    response = client.create_file_system(CreationToken=creation_token,\n                                         PerformanceMode=performance_mode)\n\n    if 'FileSystemId' in response:\n        client.create_tags(FileSystemId=response['FileSystemId'], Tags=tags)\n\n    if 'Name' in response:\n        response['Name'] = name\n\n    return response", "response": "Creates a new file system with the specified name."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a mount target for a given file system.", "response": "def create_mount_target(filesystemid,\n                        subnetid,\n                        ipaddress=None,\n                        securitygroups=None,\n                        keyid=None,\n                        key=None,\n                        profile=None,\n                        region=None,\n                        **kwargs):\n    '''\n    Creates a mount target for a file system.\n    You can then mount the file system on EC2 instances via the mount target.\n\n    You can create one mount target in each Availability Zone in your VPC.\n    All EC2 instances in a VPC within a given Availability Zone share a\n    single mount target for a given file system.\n\n    If you have multiple subnets in an Availability Zone,\n    you create a mount target in one of the subnets.\n    EC2 instances do not need to be in the same subnet as the mount target\n    in order to access their file system.\n\n    filesystemid\n        (string) - ID of the file system for which to create the mount target.\n\n    subnetid\n        (string) - ID of the subnet to add the mount target in.\n\n    ipaddress\n        (string) - Valid IPv4 address within the address range\n                    of the specified subnet.\n\n    securitygroups\n        (list[string]) - Up to five VPC security group IDs,\n                            of the form sg-xxxxxxxx.\n                            These must be for the same VPC as subnet specified.\n\n    returns\n        (dict) - A dict of the response data\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt 'my-minion' boto_efs.create_mount_target filesystemid subnetid\n    '''\n\n    client = _get_conn(key=key, keyid=keyid, profile=profile, region=region)\n\n    if ipaddress is None and securitygroups is None:\n        return client.create_mount_target(FileSystemId=filesystemid,\n                                          SubnetId=subnetid)\n\n    if ipaddress is None:\n        return client.create_mount_target(FileSystemId=filesystemid,\n                                          SubnetId=subnetid,\n                                          SecurityGroups=securitygroups)\n    if securitygroups is None:\n        return client.create_mount_target(FileSystemId=filesystemid,\n                                          SubnetId=subnetid,\n                                          IpAddress=ipaddress)\n\n    return client.create_mount_target(FileSystemId=filesystemid,\n                                      SubnetId=subnetid,\n                                      IpAddress=ipaddress,\n                                      SecurityGroups=securitygroups)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_tags(filesystemid,\n                tags,\n                keyid=None,\n                key=None,\n                profile=None,\n                region=None,\n                **kwargs):\n    '''\n    Creates or overwrites tags associated with a file system.\n    Each tag is a key-value pair. If a tag key specified in the request\n    already exists on the file system, this operation overwrites\n    its value with the value provided in the request.\n\n    filesystemid\n        (string) - ID of the file system for whose tags will be modified.\n\n    tags\n        (dict) - The tags to add to the file system\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt 'my-minion' boto_efs.create_tags\n    '''\n\n    client = _get_conn(key=key, keyid=keyid, profile=profile, region=region)\n\n    new_tags = []\n    for k, v in six.iteritems(tags):\n        new_tags.append({'Key': k, 'Value': v})\n\n    client.create_tags(FileSystemId=filesystemid, Tags=new_tags)", "response": "Creates or overwrites tags associated with a file system."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef delete_file_system(filesystemid,\n                       keyid=None,\n                       key=None,\n                       profile=None,\n                       region=None,\n                       **kwargs):\n    '''\n    Deletes a file system, permanently severing access to its contents.\n    Upon return, the file system no longer exists and you can't access\n    any contents of the deleted file system. You can't delete a file system\n    that is in use. That is, if the file system has any mount targets,\n    you must first delete them.\n\n    filesystemid\n        (string) - ID of the file system to delete.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt 'my-minion' boto_efs.delete_file_system filesystemid\n    '''\n\n    client = _get_conn(key=key, keyid=keyid, profile=profile, region=region)\n\n    client.delete_file_system(FileSystemId=filesystemid)", "response": "Deletes a file system."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef delete_mount_target(mounttargetid,\n                       keyid=None,\n                       key=None,\n                       profile=None,\n                       region=None,\n                       **kwargs):\n    '''\n    Deletes the specified mount target.\n\n    This operation forcibly breaks any mounts of the file system via the\n    mount target that is being deleted, which might disrupt instances or\n    applications using those mounts. To avoid applications getting cut off\n    abruptly, you might consider unmounting any mounts of the mount target,\n    if feasible. The operation also deletes the associated network interface.\n    Uncommitted writes may be lost, but breaking a mount target using this\n    operation does not corrupt the file system itself.\n    The file system you created remains.\n    You can mount an EC2 instance in your VPC via another mount target.\n\n    mounttargetid\n        (string) - ID of the mount target to delete\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt 'my-minion' boto_efs.delete_mount_target mounttargetid\n    '''\n\n    client = _get_conn(key=key, keyid=keyid, profile=profile, region=region)\n\n    client.delete_mount_target(MountTargetId=mounttargetid)", "response": "Deletes the specified mount target."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndeletes the specified tags from a file system.", "response": "def delete_tags(filesystemid,\n                tags,\n                keyid=None,\n                key=None,\n                profile=None,\n                region=None,\n                **kwargs):\n    '''\n    Deletes the specified tags from a file system.\n\n    filesystemid\n        (string) - ID of the file system for whose tags will be removed.\n\n    tags\n        (list[string]) - The tag keys to delete to the file system\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt 'my-minion' boto_efs.delete_tags\n    '''\n\n    client = _get_conn(key=key, keyid=keyid, profile=profile, region=region)\n\n    client.delete_tags(FileSystemId=filesystemid, Tags=tags)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_file_systems(filesystemid=None,\n                     keyid=None,\n                     key=None,\n                     profile=None,\n                     region=None,\n                     creation_token=None,\n                     **kwargs):\n    '''\n    Get all EFS properties or a specific instance property\n    if filesystemid is specified\n\n    filesystemid\n        (string) - ID of the file system to retrieve properties\n\n    creation_token\n        (string) - A unique token that identifies an EFS.\n        If fileysystem created via create_file_system this would\n        either be explictitly passed in or set to name.\n        You can limit your search with this.\n\n    returns\n        (list[dict]) - list of all elastic file system properties\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt 'my-minion' boto_efs.get_file_systems efs-id\n    '''\n\n    result = None\n    client = _get_conn(key=key, keyid=keyid, profile=profile, region=region)\n\n    if filesystemid and creation_token:\n        response = client.describe_file_systems(FileSystemId=filesystemid,\n                                                CreationToken=creation_token)\n        result = response[\"FileSystems\"]\n    elif filesystemid:\n        response = client.describe_file_systems(FileSystemId=filesystemid)\n        result = response[\"FileSystems\"]\n    elif creation_token:\n        response = client.describe_file_systems(CreationToken=creation_token)\n        result = response[\"FileSystems\"]\n    else:\n        response = client.describe_file_systems()\n\n        result = response[\"FileSystems\"]\n\n        while \"NextMarker\" in response:\n            response = client.describe_file_systems(\n                        Marker=response[\"NextMarker\"])\n            result.extend(response[\"FileSystems\"])\n\n    return result", "response": "Get all properties of a specific file system or a specific instance property"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting all the EFS mount point properties for a specific filesystemid or a specific mounttargetid.", "response": "def get_mount_targets(filesystemid=None,\n                      mounttargetid=None,\n                      keyid=None,\n                      key=None,\n                      profile=None,\n                      region=None,\n                      **kwargs):\n    '''\n    Get all the EFS mount point properties for a specific filesystemid or\n    the properties for a specific mounttargetid. One or the other must be\n    specified\n\n    filesystemid\n        (string) - ID of the file system whose mount targets to list\n                   Must be specified if mounttargetid is not\n\n    mounttargetid\n        (string) - ID of the mount target to have its properties returned\n                   Must be specified if filesystemid is not\n\n    returns\n        (list[dict]) - list of all mount point properties\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt 'my-minion' boto_efs.get_mount_targets\n    '''\n\n    result = None\n    client = _get_conn(key=key, keyid=keyid, profile=profile, region=region)\n\n    if filesystemid:\n        response = client.describe_mount_targets(FileSystemId=filesystemid)\n        result = response[\"MountTargets\"]\n        while \"NextMarker\" in response:\n            response = client.describe_mount_targets(FileSystemId=filesystemid,\n                                                 Marker=response[\"NextMarker\"])\n            result.extend(response[\"MountTargets\"])\n    elif mounttargetid:\n        response = client.describe_mount_targets(MountTargetId=mounttargetid)\n        result = response[\"MountTargets\"]\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_tags(filesystemid,\n             keyid=None,\n             key=None,\n             profile=None,\n             region=None,\n             **kwargs):\n    '''\n    Return the tags associated with an EFS instance.\n\n    filesystemid\n        (string) - ID of the file system whose tags to list\n\n    returns\n        (list) - list of tags as key/value pairs\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt 'my-minion' boto_efs.get_tags efs-id\n    '''\n    client = _get_conn(key=key, keyid=keyid, profile=profile, region=region)\n    response = client.describe_tags(FileSystemId=filesystemid)\n    result = response[\"Tags\"]\n\n    while \"NextMarker\" in response:\n        response = client.describe_tags(FileSystemId=filesystemid,\n                                        Marker=response[\"NextMarker\"])\n        result.extend(response[\"Tags\"])\n\n    return result", "response": "Get the tags associated with an EFS instance"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the security groups in effect for a mount target.", "response": "def set_security_groups(mounttargetid,\n                        securitygroup,\n                        keyid=None,\n                        key=None,\n                        profile=None,\n                        region=None,\n                        **kwargs):\n    '''\n    Modifies the set of security groups in effect for a mount target\n\n    mounttargetid\n        (string) - ID of the mount target whose security groups will be modified\n\n    securitygroups\n        (list[string]) - list of no more than 5 VPC security group IDs.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt 'my-minion' boto_efs.set_security_groups my-mount-target-id my-sec-group\n    '''\n\n    client = _get_conn(key=key, keyid=keyid, profile=profile, region=region)\n    client.modify_mount_target_security_groups(MountTargetId=mounttargetid,\n                                               SecurityGroups=securitygroup)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nensures an update is installed on the minion", "response": "def installed(name, source):\n    '''\n    Ensure an update is installed on the minion\n\n    Args:\n\n        name(str):\n            Name of the Windows KB (\"KB123456\")\n\n        source (str):\n            Source of .msu file corresponding to the KB\n\n    Example:\n\n    .. code-block:: yaml\n\n        KB123456:\n          wusa.installed:\n            - source: salt://kb123456.msu\n    '''\n    ret = {'name': name,\n           'changes': {},\n           'result': False,\n           'comment': ''}\n\n    # Input validation\n    if not name:\n        raise SaltInvocationError('Must specify a KB \"name\"')\n    if not source:\n        raise SaltInvocationError('Must specify a \"source\" file to install')\n\n    # Is the KB already installed\n    if __salt__['wusa.is_installed'](name):\n        ret['result'] = True\n        ret['comment'] = '{0} already installed'.format(name)\n        return ret\n\n    # Check for test=True\n    if __opts__['test'] is True:\n        ret['result'] = None\n        ret['comment'] = '{0} would be installed'.format(name)\n        ret['result'] = None\n        return ret\n\n    # Cache the file\n    cached_source_path = __salt__['cp.cache_file'](path=source, saltenv=__env__)\n    if not cached_source_path:\n        msg = 'Unable to cache {0} from saltenv \"{1}\"'.format(\n            salt.utils.url.redact_http_basic_auth(source), __env__)\n        ret['comment'] = msg\n        return ret\n\n    # Install the KB\n    __salt__['wusa.install'](cached_source_path)\n\n    # Verify successful install\n    if __salt__['wusa.is_installed'](name):\n        ret['comment'] = '{0} was installed'.format(name)\n        ret['changes'] = {'old': False, 'new': True}\n        ret['result'] = True\n    else:\n        ret['comment'] = '{0} failed to install'.format(name)\n\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef uninstalled(name):\n    '''\n    Ensure an update is uninstalled from the minion\n\n    Args:\n\n        name(str):\n            Name of the Windows KB (\"KB123456\")\n\n    Example:\n\n    .. code-block:: yaml\n\n        KB123456:\n          wusa.uninstalled\n    '''\n    ret = {'name': name,\n           'changes': {},\n           'result': False,\n           'comment': ''}\n\n    # Is the KB already uninstalled\n    if not __salt__['wusa.is_installed'](name):\n        ret['result'] = True\n        ret['comment'] = '{0} already uninstalled'.format(name)\n        return ret\n\n    # Check for test=True\n    if __opts__['test'] is True:\n        ret['result'] = None\n        ret['comment'] = '{0} would be uninstalled'.format(name)\n        ret['result'] = None\n        return ret\n\n    # Uninstall the KB\n    __salt__['wusa.uninstall'](name)\n\n    # Verify successful uninstall\n    if not __salt__['wusa.is_installed'](name):\n        ret['comment'] = '{0} was uninstalled'.format(name)\n        ret['changes'] = {'old': True, 'new': False}\n        ret['result'] = True\n    else:\n        ret['comment'] = '{0} failed to uninstall'.format(name)\n\n    return ret", "response": "Ensure an update is uninstalled from the Windows KB"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef send(kwargs, opts):\n    '''\n    Send an email with the data\n    '''\n    opt_keys = (\n        'smtp.to',\n        'smtp.from',\n        'smtp.host',\n        'smtp.port',\n        'smtp.tls',\n        'smtp.username',\n        'smtp.password',\n        'smtp.subject',\n        'smtp.gpgowner',\n        'smtp.content',\n    )\n\n    config = {}\n    for key in opt_keys:\n        config[key] = opts.get(key, '')\n\n    config.update(kwargs)\n\n    if not config['smtp.port']:\n        config['smtp.port'] = 25\n\n    log.debug('SMTP port has been set to %s', config['smtp.port'])\n    log.debug(\"smtp_return: Subject is '%s'\", config['smtp.subject'])\n\n    if HAS_GNUPG and config['smtp.gpgowner']:\n        gpg = gnupg.GPG(\n            gnupghome=os.path.expanduser(\n                '~{0}/.gnupg'.format(config['smtp.gpgowner'])\n            ),\n            options=['--trust-model always']\n        )\n        encrypted_data = gpg.encrypt(config['smtp.content'], config['smtp.to'])\n        if encrypted_data.ok:\n            log.debug('smtp_return: Encryption successful')\n            config['smtp.content'] = six.text_type(encrypted_data)\n        else:\n            log.error(\n                'SMTP: Encryption failed, only an error message will be sent'\n            )\n            config['smtp.content'] = (\n                'Encryption failed, the return data was not sent.'\n                '\\r\\n\\r\\n{0}\\r\\n{1}'\n            ).format(encrypted_data.status, encrypted_data.stderr)\n\n    message = (\n        'From: {0}\\r\\n'\n        'To: {1}\\r\\n'\n        'Date: {2}\\r\\n'\n        'Subject: {3}\\r\\n'\n        '\\r\\n'\n        '{4}'\n    ).format(\n        config['smtp.from'],\n        config['smtp.to'],\n        formatdate(localtime=True),\n        config['smtp.subject'],\n        config['smtp.content'],\n    )\n\n    log.debug('smtp_return: Connecting to the server...')\n    server = smtplib.SMTP(config['smtp.host'], int(config['smtp.port']))\n\n    if config['smtp.tls'] is True:\n        server.starttls()\n        log.debug('smtp_return: TLS enabled')\n\n    if config['smtp.username'] and config['smtp.password']:\n        server.login(config['smtp.username'], config['smtp.password'])\n        log.debug('smtp_return: Authenticated')\n\n    server.sendmail(config['smtp.from'], config['smtp.to'], message)\n    log.debug('smtp_return: Message sent.')\n    server.quit()", "response": "Send an email with the data"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting a list of IP addresses from a CIDR.", "response": "def list_cidr_ips(cidr):\n    '''\n    Get a list of IP addresses from a CIDR.\n\n    CLI example::\n\n        salt myminion netaddress.list_cidr_ips 192.168.0.0/20\n    '''\n    ips = netaddr.IPNetwork(cidr)\n    return [six.text_type(ip) for ip in list(ips)]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef list_cidr_ips_ipv6(cidr):\n    '''\n    Get a list of IPv6 addresses from a CIDR.\n\n    CLI example::\n\n        salt myminion netaddress.list_cidr_ips_ipv6 192.168.0.0/20\n    '''\n    ips = netaddr.IPNetwork(cidr)\n    return [six.text_type(ip.ipv6()) for ip in list(ips)]", "response": "Get a list of IPv6 addresses from a CIDR."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the netmask associated with a CIDR address.", "response": "def cidr_netmask(cidr):\n    '''\n    Get the netmask address associated with a CIDR address.\n\n    CLI example::\n\n        salt myminion netaddress.cidr_netmask 192.168.0.0/20\n    '''\n    ips = netaddr.IPNetwork(cidr)\n    return six.text_type(ips.netmask)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the broadcast address associated with a CIDR address.", "response": "def cidr_broadcast(cidr):\n    '''\n    Get the broadcast address associated with a CIDR address.\n\n    CLI example::\n\n        salt myminion netaddress.cidr_netmask 192.168.0.0/20\n    '''\n    ips = netaddr.IPNetwork(cidr)\n    return six.text_type(ips.broadcast)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrestart OpenStack service immediately or only if it s running longer than minimum_running_time specified value", "response": "def restart_service(service_name, minimum_running_time=None):\n    '''\n    Restart OpenStack service immediately, or only if it's running longer than\n    specified value\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' openstack_mng.restart_service neutron\n        salt '*' openstack_mng.restart_service neutron minimum_running_time=600\n    '''\n\n    if minimum_running_time:\n        ret_code = False\n        # get system services list for interesting openstack service\n        services = __salt__['cmd.run'](['/usr/bin/openstack-service', 'list', service_name]).split('\\n')\n        for service in services:\n            service_info = __salt__['service.show'](service)\n            with salt.utils.files.fopen('/proc/uptime') as rfh:\n                boot_time = float(\n                    salt.utils.stringutils.to_unicode(\n                        rfh.read()\n                    ).split(' ')[0]\n                )\n\n            expr_time = int(service_info.get('ExecMainStartTimestampMonotonic', 0)) / 1000000 < boot_time - minimum_running_time\n            expr_active = service_info.get('ActiveState') == \"active\"\n\n            if expr_time or not expr_active:\n                # restart specific system service\n                ret = __salt__['service.restart'](service)\n                if ret:\n                    ret_code = True\n\n        return ret_code\n    else:\n        # just restart\n        os_cmd = ['/usr/bin/openstack-service', 'restart', service_name]\n        return __salt__['cmd.retcode'](os_cmd) == 0"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_options(ret=None):\n    '''\n    Returns options used for the MySQL connection.\n    '''\n    defaults = {'host': 'salt',\n                'user': 'salt',\n                'pass': 'salt',\n                'db': 'salt',\n                'port': 3306,\n                'ssl_ca': None,\n                'ssl_cert': None,\n                'ssl_key': None}\n\n    attrs = {'host': 'host',\n             'user': 'user',\n             'pass': 'pass',\n             'db': 'db',\n             'port': 'port',\n             'ssl_ca': 'ssl_ca',\n             'ssl_cert': 'ssl_cert',\n             'ssl_key': 'ssl_key'}\n\n    _options = salt.returners.get_returner_options(__virtualname__,\n                                                   ret,\n                                                   attrs,\n                                                   __salt__=__salt__,\n                                                   __opts__=__opts__,\n                                                   defaults=defaults)\n    # post processing\n    for k, v in six.iteritems(_options):\n        if isinstance(v, six.string_types) and v.lower() == 'none':\n            # Ensure 'None' is rendered as None\n            _options[k] = None\n        if k == 'port':\n            # Ensure port is an int\n            _options[k] = int(v)\n\n    return _options", "response": "Returns the options used for the MySQL connection."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_serv(ret=None, commit=False):\n    '''\n    Return a mysql cursor\n    '''\n    _options = _get_options(ret)\n\n    connect = True\n    if __context__ and 'mysql_returner_conn' in __context__:\n        try:\n            log.debug('Trying to reuse MySQL connection pool')\n            conn = __context__['mysql_returner_conn']\n            conn.ping()\n            connect = False\n        except OperationalError as exc:\n            log.debug('OperationalError on ping: %s', exc)\n\n    if connect:\n        log.debug('Generating new MySQL connection pool')\n        try:\n            # An empty ssl_options dictionary passed to MySQLdb.connect will\n            # effectively connect w/o SSL.\n            ssl_options = {}\n            if _options.get('ssl_ca'):\n                ssl_options['ca'] = _options.get('ssl_ca')\n            if _options.get('ssl_cert'):\n                ssl_options['cert'] = _options.get('ssl_cert')\n            if _options.get('ssl_key'):\n                ssl_options['key'] = _options.get('ssl_key')\n            conn = MySQLdb.connect(host=_options.get('host'),\n                                   user=_options.get('user'),\n                                   passwd=_options.get('pass'),\n                                   db=_options.get('db'),\n                                   port=_options.get('port'),\n                                   ssl=ssl_options)\n\n            try:\n                __context__['mysql_returner_conn'] = conn\n            except TypeError:\n                pass\n        except OperationalError as exc:\n            raise salt.exceptions.SaltMasterError('MySQL returner could not connect to database: {exc}'.format(exc=exc))\n\n    cursor = conn.cursor()\n\n    try:\n        yield cursor\n    except MySQLdb.DatabaseError as err:\n        error = err.args\n        sys.stderr.write(six.text_type(error))\n        cursor.execute(\"ROLLBACK\")\n        raise err\n    else:\n        if commit:\n            cursor.execute(\"COMMIT\")\n        else:\n            cursor.execute(\"ROLLBACK\")", "response": "Return a MySQL cursor for the current returner instance"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning data to a MySQL server", "response": "def returner(ret):\n    '''\n    Return data to a mysql server\n    '''\n    # if a minion is returning a standalone job, get a jobid\n    if ret['jid'] == 'req':\n        ret['jid'] = prep_jid(nocache=ret.get('nocache', False))\n        save_load(ret['jid'], ret)\n\n    try:\n        with _get_serv(ret, commit=True) as cur:\n            sql = '''INSERT INTO `salt_returns`\n                     (`fun`, `jid`, `return`, `id`, `success`, `full_ret`)\n                     VALUES (%s, %s, %s, %s, %s, %s)'''\n\n            cur.execute(sql, (ret['fun'], ret['jid'],\n                              salt.utils.json.dumps(ret['return']),\n                              ret['id'],\n                              ret.get('success', False),\n                              salt.utils.json.dumps(ret)))\n    except salt.exceptions.SaltMasterError as exc:\n        log.critical(exc)\n        log.critical('Could not store return with MySQL returner. MySQL server unavailable.')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef event_return(events):\n    '''\n    Return event to mysql server\n\n    Requires that configuration be enabled via 'event_return'\n    option in master config.\n    '''\n    with _get_serv(events, commit=True) as cur:\n        for event in events:\n            tag = event.get('tag', '')\n            data = event.get('data', '')\n            sql = '''INSERT INTO `salt_events` (`tag`, `data`, `master_id`)\n                     VALUES (%s, %s, %s)'''\n            cur.execute(sql, (tag, salt.utils.json.dumps(data), __opts__['id']))", "response": "Return event to mysql server"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsave the load to the specified jid id", "response": "def save_load(jid, load, minions=None):\n    '''\n    Save the load to the specified jid id\n    '''\n    with _get_serv(commit=True) as cur:\n\n        sql = '''INSERT INTO `jids` (`jid`, `load`) VALUES (%s, %s)'''\n\n        try:\n            cur.execute(sql, (jid, salt.utils.json.dumps(load)))\n        except MySQLdb.IntegrityError:\n            # https://github.com/saltstack/salt/issues/22171\n            # Without this try/except we get tons of duplicate entry errors\n            # which result in job returns not being stored properly\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_load(jid):\n    '''\n    Return the load data that marks a specified jid\n    '''\n    with _get_serv(ret=None, commit=True) as cur:\n\n        sql = '''SELECT `load` FROM `jids` WHERE `jid` = %s;'''\n        cur.execute(sql, (jid,))\n        data = cur.fetchone()\n        if data:\n            return salt.utils.json.loads(data[0])\n        return {}", "response": "Return the load data that marks a specified jid\n    Return the load data that marks a specified jid\n    Return the load data that marks a specified jid\n    Return the load data that marks a specified jid\n    Return the empty dict if no load data is found"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a dict of the last function called for all minions", "response": "def get_fun(fun):\n    '''\n    Return a dict of the last function called for all minions\n    '''\n    with _get_serv(ret=None, commit=True) as cur:\n\n        sql = '''SELECT s.id,s.jid, s.full_ret\n                FROM `salt_returns` s\n                JOIN ( SELECT MAX(`jid`) as jid\n                    from `salt_returns` GROUP BY fun, id) max\n                ON s.jid = max.jid\n                WHERE s.fun = %s\n                '''\n\n        cur.execute(sql, (fun,))\n        data = cur.fetchall()\n\n        ret = {}\n        if data:\n            for minion, _, full_ret in data:\n                ret[minion] = salt.utils.json.loads(full_ret)\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_jids_filter(count, filter_find_job=True):\n    '''\n    Return a list of all job ids\n    :param int count: show not more than the count of most recent jobs\n    :param bool filter_find_jobs: filter out 'saltutil.find_job' jobs\n    '''\n    with _get_serv(ret=None, commit=True) as cur:\n\n        sql = '''SELECT * FROM (\n                     SELECT DISTINCT `jid` ,`load` FROM `jids`\n                     {0}\n                     ORDER BY `jid` DESC limit {1}\n                     ) `tmp`\n                 ORDER BY `jid`;'''\n        where = '''WHERE `load` NOT LIKE '%\"fun\": \"saltutil.find_job\"%' '''\n\n        cur.execute(sql.format(where if filter_find_job else '', count))\n        data = cur.fetchall()\n        ret = []\n        for jid in data:\n            ret.append(salt.utils.jid.format_jid_instance_ext(\n                jid[0],\n                salt.utils.json.loads(jid[1])))\n        return ret", "response": "Return a list of all job ids in the order of load"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_minions():\n    '''\n    Return a list of minions\n    '''\n    with _get_serv(ret=None, commit=True) as cur:\n\n        sql = '''SELECT DISTINCT id\n                FROM `salt_returns`'''\n\n        cur.execute(sql)\n        data = cur.fetchall()\n        ret = []\n        for minion in data:\n            ret.append(minion[0])\n        return ret", "response": "Return a list of minions"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _purge_jobs(timestamp):\n    '''\n    Purge records from the returner tables.\n    :param job_age_in_seconds:  Purge jobs older than this\n    :return:\n    '''\n    with _get_serv() as cur:\n        try:\n            sql = 'delete from `jids` where jid in (select distinct jid from salt_returns where alter_time < %s)'\n            cur.execute(sql, (timestamp,))\n            cur.execute('COMMIT')\n        except MySQLdb.Error as e:\n            log.error('mysql returner archiver was unable to delete contents of table \\'jids\\'')\n            log.error(six.text_type(e))\n            raise salt.exceptions.SaltRunnerError(six.text_type(e))\n\n        try:\n            sql = 'delete from `salt_returns` where alter_time < %s'\n            cur.execute(sql, (timestamp,))\n            cur.execute('COMMIT')\n        except MySQLdb.Error as e:\n            log.error('mysql returner archiver was unable to delete contents of table \\'salt_returns\\'')\n            log.error(six.text_type(e))\n            raise salt.exceptions.SaltRunnerError(six.text_type(e))\n\n        try:\n            sql = 'delete from `salt_events` where alter_time < %s'\n            cur.execute(sql, (timestamp,))\n            cur.execute('COMMIT')\n        except MySQLdb.Error as e:\n            log.error('mysql returner archiver was unable to delete contents of table \\'salt_events\\'')\n            log.error(six.text_type(e))\n            raise salt.exceptions.SaltRunnerError(six.text_type(e))\n\n    return True", "response": "Purge jobs older than the given timestamp."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncopying rows to a set of backup tables then purge rows.", "response": "def _archive_jobs(timestamp):\n    '''\n    Copy rows to a set of backup tables, then purge rows.\n    :param timestamp: Archive rows older than this timestamp\n    :return:\n    '''\n    source_tables = ['jids',\n                     'salt_returns',\n                     'salt_events']\n\n    with _get_serv() as cur:\n        target_tables = {}\n        for table_name in source_tables:\n            try:\n                tmp_table_name = table_name + '_archive'\n                sql = 'create table if not exists {0} like {1}'.format(tmp_table_name, table_name)\n                cur.execute(sql)\n                cur.execute('COMMIT')\n                target_tables[table_name] = tmp_table_name\n            except MySQLdb.Error as e:\n                log.error('mysql returner archiver was unable to create the archive tables.')\n                log.error(six.text_type(e))\n                raise salt.exceptions.SaltRunnerError(six.text_type(e))\n\n        try:\n            sql = 'insert into `{0}` select * from `{1}` where jid in (select distinct jid from salt_returns where alter_time < %s)'.format(target_tables['jids'], 'jids')\n            cur.execute(sql, (timestamp,))\n            cur.execute('COMMIT')\n        except MySQLdb.Error as e:\n            log.error('mysql returner archiver was unable to copy contents of table \\'jids\\'')\n            log.error(six.text_type(e))\n            raise salt.exceptions.SaltRunnerError(six.text_type(e))\n        except Exception as e:\n            log.error(e)\n            raise\n\n        try:\n            sql = 'insert into `{0}` select * from `{1}` where alter_time < %s'.format(target_tables['salt_returns'], 'salt_returns')\n            cur.execute(sql, (timestamp,))\n            cur.execute('COMMIT')\n        except MySQLdb.Error as e:\n            log.error('mysql returner archiver was unable to copy contents of table \\'salt_returns\\'')\n            log.error(six.text_type(e))\n            raise salt.exceptions.SaltRunnerError(six.text_type(e))\n\n        try:\n            sql = 'insert into `{0}` select * from `{1}` where alter_time < %s'.format(target_tables['salt_events'], 'salt_events')\n            cur.execute(sql, (timestamp,))\n            cur.execute('COMMIT')\n        except MySQLdb.Error as e:\n            log.error('mysql returner archiver was unable to copy contents of table \\'salt_events\\'')\n            log.error(six.text_type(e))\n            raise salt.exceptions.SaltRunnerError(six.text_type(e))\n\n    return _purge_jobs(timestamp)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef clean_old_jobs():\n    '''\n    Called in the master's event loop every loop_interval.  Archives and/or\n    deletes the events and job details from the database.\n    :return:\n    '''\n    if __opts__.get('keep_jobs', False) and int(__opts__.get('keep_jobs', 0)) > 0:\n        try:\n            with _get_serv() as cur:\n                sql = 'select date_sub(now(), interval {0} hour) as stamp;'.format(__opts__['keep_jobs'])\n                cur.execute(sql)\n                rows = cur.fetchall()\n                stamp = rows[0][0]\n\n            if __opts__.get('archive_jobs', False):\n                _archive_jobs(stamp)\n            else:\n                _purge_jobs(stamp)\n        except MySQLdb.Error as e:\n            log.error('Mysql returner was unable to get timestamp for purge/archive of jobs')\n            log.error(six.text_type(e))\n            raise salt.exceptions.SaltRunnerError(six.text_type(e))", "response": "Called in the master s event loop every loop_interval."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _verify_options(options):\n    '''\n    Verify options and log warnings\n\n    Returns True if all options can be verified,\n    otherwise False\n    '''\n\n    # sanity check all vals used for bitwise operations later\n    bitwise_args = [('level', options['level']),\n                    ('facility', options['facility'])\n                    ]\n    bitwise_args.extend([('option', x) for x in options['options']])\n\n    for opt_name, opt in bitwise_args:\n        if not hasattr(syslog, opt):\n            log.error('syslog has no attribute %s', opt)\n            return False\n        if not isinstance(getattr(syslog, opt), int):\n            log.error('%s is not a valid syslog %s', opt, opt_name)\n            return False\n\n    # Sanity check tag\n    if 'tag' in options:\n        if not isinstance(options['tag'], six.string_types):\n            log.error('tag must be a string')\n            return False\n        if len(options['tag']) > 32:\n            log.error('tag size is limited to 32 characters')\n            return False\n\n    return True", "response": "Verify options and log warnings\n    Returns True if all options can be verified otherwise False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef returner(ret):\n    '''\n    Return data to the local syslog\n    '''\n\n    _options = _get_options(ret)\n\n    if not _verify_options(_options):\n        return\n\n    # Get values from syslog module\n    level = getattr(syslog, _options['level'])\n    facility = getattr(syslog, _options['facility'])\n\n    # parse for syslog options\n    logoption = 0\n    for opt in _options['options']:\n        logoption = logoption | getattr(syslog, opt)\n\n    # Open syslog correctly based on options and tag\n    if 'tag' in _options:\n        syslog.openlog(ident=salt.utils.stringutils.to_str(_options['tag']), logoption=logoption)\n    else:\n        syslog.openlog(logoption=logoption)\n\n    # Send log of given level and facility\n    syslog.syslog(facility | level, salt.utils.json.dumps(ret))\n\n    # Close up to reset syslog to defaults\n    syslog.closelog()", "response": "Return data to the local syslog\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprints a usage message incorporating msg to stderr and exit.", "response": "def error(self, msg):\n        '''\n        error(msg : string)\n\n        Print a usage message incorporating 'msg' to stderr and exit.\n        This keeps option parsing exit status uniform for all parsing errors.\n        '''\n        self.print_usage(sys.stderr)\n        self.exit(salt.defaults.exitcodes.EX_USAGE, '{0}: error: {1}\\n'.format(self.get_prog_name(), msg))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check_running(self):\n        '''\n        Check if a pid file exists and if it is associated with\n        a running process.\n        '''\n\n        if self.check_pidfile():\n            pid = self.get_pidfile()\n            if not salt.utils.platform.is_windows():\n                if self.check_pidfile() and self.is_daemonized(pid) and os.getppid() != pid:\n                    return True\n            else:\n                # We have no os.getppid() on Windows. Use salt.utils.win_functions.get_parent_pid\n                if self.check_pidfile() and self.is_daemonized(pid) and salt.utils.win_functions.get_parent_pid() != pid:\n                    return True\n        return False", "response": "Check if a pid file exists and if it is associated with\n        a running process."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind existing configuration files on the system.", "response": "def find_existing_configs(self, default):\n        '''\n        Find configuration files on the system.\n        :return:\n        '''\n        configs = []\n        for cfg in [default, self._config_filename_, 'minion', 'proxy', 'cloud', 'spm']:\n            if not cfg:\n                continue\n            config_path = self.get_config_file_path(cfg)\n            if os.path.exists(config_path):\n                configs.append(cfg)\n\n        if default and default not in configs:\n            raise SystemExit('Unknown configuration unit: {}'.format(default))\n\n        return configs"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nopening suitable config file.", "response": "def setup_config(self, cfg=None):\n        '''\n        Open suitable config file.\n        :return:\n        '''\n        _opts, _args = optparse.OptionParser.parse_args(self)\n        configs = self.find_existing_configs(_opts.support_unit)\n        if configs and cfg not in configs:\n            cfg = configs[0]\n\n        return config.master_config(self.get_config_file_path(cfg))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmanage coalescing settings of network device", "response": "def coalesce(name, **kwargs):\n    '''\n    Manage coalescing settings of network device\n\n    name\n        Interface name to apply coalescing settings\n\n    .. code-block:: yaml\n\n        eth0:\n          ethtool.coalesce:\n            - name: eth0\n            - adaptive_rx: on\n            - adaptive_tx: on\n            - rx_usecs: 24\n            - rx_frame: 0\n            - rx_usecs_irq: 0\n            - rx_frames_irq: 0\n            - tx_usecs: 48\n            - tx_frames: 0\n            - tx_usecs_irq: 0\n            - tx_frames_irq: 0\n            - stats_block_usecs: 0\n            - pkt_rate_low: 0\n            - rx_usecs_low: 0\n            - rx_frames_low: 0\n            - tx_usecs_low: 0\n            - tx_frames_low: 0\n            - pkt_rate_high: 0\n            - rx_usecs_high: 0\n            - rx_frames_high: 0\n            - tx_usecs_high: 0\n            - tx_frames_high: 0\n            - sample_interval: 0\n\n    '''\n    ret = {\n        'name': name,\n        'changes': {},\n        'result': True,\n        'comment': 'Network device {0} coalescing settings are up to date.'.format(name),\n    }\n    apply_coalescing = False\n    if 'test' not in kwargs:\n        kwargs['test'] = __opts__.get('test', False)\n\n    # Build coalescing settings\n    try:\n        old = __salt__['ethtool.show_coalesce'](name)\n        if not isinstance(old, dict):\n            ret['result'] = False\n            ret['comment'] = 'Device {0} coalescing settings are not supported'.format(name)\n            return ret\n\n        new = {}\n        diff = []\n\n        # Retreive changes to made\n        for key, value in kwargs.items():\n            if key in old and value != old[key]:\n                new.update({key: value})\n                diff.append('{0}: {1}'.format(key, value))\n\n        # Dry run\n        if kwargs['test']:\n            if not new:\n                return ret\n            if new:\n                ret['result'] = None\n                ret['comment'] = 'Device {0} coalescing settings are set to be ' \\\n                                 'updated:\\n{1}'.format(name, '\\n'.join(diff))\n                return ret\n\n        # Prepare return output\n        if new:\n            apply_coalescing = True\n            ret['comment'] = 'Device {0} coalescing settings updated.'.format(name)\n            ret['changes']['ethtool_coalesce'] = '\\n'.join(diff)\n\n    except AttributeError as error:\n        ret['result'] = False\n        ret['comment'] = six.text_type(error)\n        return ret\n\n    # Apply coalescing settings\n    if apply_coalescing:\n        try:\n            __salt__['ethtool.set_coalesce'](name, **new)\n        except AttributeError as error:\n            ret['result'] = False\n            ret['comment'] = six.text_type(error)\n            return ret\n\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef datacenter_configured(name):\n    '''\n    Makes sure a datacenter exists.\n\n    If the state is run by an ``esxdatacenter`` minion, the name of the\n    datacenter is retrieved from the proxy details, otherwise the datacenter\n    has the same name as the state.\n\n    Supported proxies: esxdatacenter\n\n    name:\n        Datacenter name. Ignored if the proxytype is ``esxdatacenter``.\n    '''\n    proxy_type = __salt__['vsphere.get_proxy_type']()\n    if proxy_type == 'esxdatacenter':\n        dc_name = __salt__['esxdatacenter.get_details']()['datacenter']\n    else:\n        dc_name = name\n    log.info('Running datacenter_configured for datacenter \\'%s\\'', dc_name)\n    ret = {'name': name,\n           'changes': {},\n           'result': None,\n           'comment': 'Default'}\n    comments = []\n    si = None\n    try:\n        si = __salt__['vsphere.get_service_instance_via_proxy']()\n        dcs = __salt__['vsphere.list_datacenters_via_proxy'](\n            datacenter_names=[dc_name], service_instance=si)\n        if not dcs:\n            if __opts__['test']:\n                comments.append('State will create '\n                                'datacenter \\'{0}\\'.'.format(dc_name))\n            else:\n                log.debug('Creating datacenter \\'%s\\'', dc_name)\n                __salt__['vsphere.create_datacenter'](dc_name, si)\n                comments.append('Created datacenter \\'{0}\\'.'.format(dc_name))\n            log.info(comments[-1])\n            ret['changes'].update({'new': {'name': dc_name}})\n        else:\n            comments.append('Datacenter \\'{0}\\' already exists. Nothing to be '\n                            'done.'.format(dc_name))\n            log.info(comments[-1])\n        __salt__['vsphere.disconnect'](si)\n        ret['comment'] = '\\n'.join(comments)\n        ret['result'] = None if __opts__['test'] and ret['changes'] else True\n        return ret\n    except salt.exceptions.CommandExecutionError as exc:\n        log.error('Error: %s', exc)\n        if si:\n            __salt__['vsphere.disconnect'](si)\n        ret.update({\n            'result': False if not __opts__['test'] else None,\n            'comment': six.text_type(exc)})\n        return ret", "response": "Ensures that a datacenter exists and is configured."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating all the possible paths within an OpenConfig - like object.", "response": "def _container_path(model,\n                    key=None,\n                    container=None,\n                    delim=DEFAULT_TARGET_DELIM):\n    '''\n    Generate all the possible paths within an OpenConfig-like object.\n    This function returns a generator.\n    '''\n    if not key:\n        key = ''\n    if not container:\n        container = 'config'\n    for model_key, model_value in six.iteritems(model):\n        if key:\n            key_depth = '{prev_key}{delim}{cur_key}'.format(prev_key=key,\n                                                            delim=delim,\n                                                            cur_key=model_key)\n        else:\n            key_depth = model_key\n        if model_key == container:\n            yield key_depth\n        else:\n            for value in _container_path(model_value,\n                                         key=key_depth,\n                                         container=container,\n                                         delim=delim):\n                yield value"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef traverse(data, key, default=None, delimiter=DEFAULT_TARGET_DELIM):\n    '''\n    Traverse a dict or list using a colon-delimited (or otherwise delimited,\n    using the ``delimiter`` param) target string. The target ``foo:bar:0`` will\n    return ``data['foo']['bar'][0]`` if this value exists, and will otherwise\n    return the dict in the default argument.\n    Function will automatically determine the target type.\n    The target ``foo:bar:0`` will return data['foo']['bar'][0] if data like\n    ``{'foo':{'bar':['baz']}}`` , if data like ``{'foo':{'bar':{'0':'baz'}}}``\n    then ``return data['foo']['bar']['0']``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' napalm_formula.traverse \"{'foo': {'bar': {'baz': True}}}\" foo:baz:bar\n    '''\n    return _traverse_dict_and_list(data, key, default=default, delimiter=delimiter)", "response": "Traverse a dict or list using colon - delimited or otherwise delimited target string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating dict with upd recursively into dest.", "response": "def dictupdate(dest, upd, recursive_update=True, merge_lists=False):\n    '''\n    Recursive version of the default dict.update\n\n    Merges upd recursively into dest\n\n    If recursive_update=False, will use the classic dict.update, or fall back\n    on a manual merge (helpful for non-dict types like ``FunctionWrapper``).\n\n    If ``merge_lists=True``, will aggregate list object types instead of replace.\n    The list in ``upd`` is added to the list in ``dest``, so the resulting list\n    is ``dest[key] + upd[key]``. This behaviour is only activated when\n    ``recursive_update=True``. By default ``merge_lists=False``.\n    '''\n    return salt.utils.dictupdate.update(dest, upd, recursive_update=recursive_update, merge_lists=merge_lists)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\napplies the defaults to a Python dictionary having the structure as described by the OpenConfig standards.", "response": "def defaults(model,\n             defaults_,\n             delim='//',\n             flipped_merge=False):\n    '''\n    Apply the defaults to a Python dictionary having the structure as described\n    in the OpenConfig standards.\n\n    model\n        The OpenConfig model to apply the defaults to.\n\n    defaults\n        The dictionary of defaults. This argument must equally be structured\n        with respect to the OpenConfig standards.\n\n        For ease of use, the keys of these support glob matching, therefore\n        we don't have to provide the defaults for each entity but only for\n        the entity type. See an example below.\n\n    delim: ``//``\n        The key delimiter to use. Generally, ``//`` should cover all the possible\n        cases, and you don't need to override this value.\n\n    flipped_merge: ``False``\n        Whether should merge the model into the defaults, or the defaults\n        into the model. Default: ``False`` (merge the model into the defaults,\n        i.e., any defaults would be overridden by the values from the ``model``).\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' napalm_formula.defaults \"{'interfaces': {'interface': {'Ethernet1': {'config': {'name': 'Ethernet1'}}}}}\" \"{'interfaces': {'interface': {'*': {'config': {'enabled': True}}}}}\"\n\n    As one can notice in the example above, the ``*`` corresponds to the\n    interface name, therefore, the defaults will be applied on all the\n    interfaces.\n    '''\n    merged = {}\n    log.debug('Applying the defaults:')\n    log.debug(defaults_)\n    log.debug('openconfig like dictionary:')\n    log.debug(model)\n    for model_path in _container_path(model, delim=delim):\n        for default_path in _container_path(defaults_, delim=delim):\n            log.debug('Comparing %s to %s', model_path, default_path)\n            if not fnmatch.fnmatch(model_path, default_path) or\\\n               not len(model_path.split(delim)) == len(default_path.split(delim)):\n                continue\n            log.debug('%s matches %s', model_path, default_path)\n            # If there's a match, it will build the dictionary from the top\n            devault_val = _traverse_dict_and_list(defaults_,\n                                                 default_path,\n                                                 delimiter=delim)\n            merged = setval(model_path, devault_val, dict_=merged, delim=delim)\n    log.debug('Complete default dictionary')\n    log.debug(merged)\n    log.debug('Merging with the model')\n    log.debug(model)\n    if flipped_merge:\n        return salt.utils.dictupdate.update(model, merged)\n    return salt.utils.dictupdate.update(merged, model)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrendering a field in a Jinja template.", "response": "def render_field(dictionary,\n                 field,\n                 prepend=None,\n                 append=None,\n                 quotes=False,\n                 **opts):\n    '''\n    Render a field found under the ``field`` level of the hierarchy in the\n    ``dictionary`` object.\n    This is useful to render a field in a Jinja template without worrying that\n    the hierarchy might not exist. For example if we do the following in Jinja:\n    ``{{ interfaces.interface.Ethernet5.config.description }}`` for the\n    following object:\n    ``{'interfaces': {'interface': {'Ethernet1': {'config': {'enabled': True}}}}}``\n    it would error, as the ``Ethernet5`` key does not exist.\n    With this helper, we can skip this and avoid existence checks. This must be\n    however used with care.\n\n    dictionary\n        The dictionary to traverse.\n\n    field\n        The key name or part to traverse in the ``dictionary``.\n\n    prepend: ``None``\n        The text to prepend in front of the text. Usually, we need to have the\n        name of the field too when generating the configuration.\n\n    append: ``None``\n        Text to append at the end.\n\n    quotes: ``False``\n        Whether should wrap the text around quotes.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' napalm_formula.render_field \"{'enabled': True}\" enabled\n        # This would return the value of the ``enabled`` leaf key\n        salt '*' napalm_formula.render_field \"{'enabled': True}\" description\n        # This would not error\n\n    Jinja usage example:\n\n    .. code-block:: jinja\n\n        {%- set config = {'enabled': True, 'description': 'Interface description'} %}\n        {{ salt.napalm_formula.render_field(config, 'description', quotes=True) }}\n\n    The example above would be rendered on Arista / Cisco as:\n\n    .. code-block:: text\n\n        description \"Interface description\"\n\n    While on Junos (the semicolon is important to be added, otherwise the\n    configuration won't be accepted by Junos):\n\n    .. code-block:: text\n\n        description \"Interface description\";\n    '''\n    value = traverse(dictionary, field)\n    if value is None:\n        return ''\n    if prepend is None:\n        prepend = field.replace('_', '-')\n    if append is None:\n        if __grains__['os'] in ('junos',):\n            append = ';'\n        else:\n            append = ''\n    if quotes:\n        value = '\"{value}\"'.format(value=value)\n    return '{prepend} {value}{append}'.format(prepend=prepend,\n                                              value=value,\n                                              append=append)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef installed(name,\n              features=None,\n              recurse=False,\n              restart=False,\n              source=None,\n              exclude=None):\n    '''\n    Install the windows feature. To install a single feature, use the ``name``\n    parameter. To install multiple features, use the ``features`` parameter.\n\n    .. note::\n        Some features require reboot after un/installation. If so, until the\n        server is restarted other features can not be installed!\n\n    Args:\n\n        name (str):\n            Short name of the feature (the right column in\n            win_servermanager.list_available). This can be a single feature or a\n            string of features in a comma delimited list (no spaces)\n\n            .. note::\n                A list is not allowed in the name parameter of any state. Use\n                the ``features`` parameter if you want to pass the features as a\n                list\n\n        features (Optional[list]):\n            A list of features to install. If this is passed it will be used\n            instead of the ``name`` parameter.\n\n            .. versionadded:: 2018.3.0\n\n        recurse (Optional[bool]):\n            Install all sub-features as well. If the feature is installed but\n            one of its sub-features are not installed set this will install\n            additional sub-features\n\n        source (Optional[str]):\n            Path to the source files if missing from the target system. None\n            means that the system will use windows update services to find the\n            required files. Default is None\n\n        restart (Optional[bool]):\n            Restarts the computer when installation is complete, if required by\n            the role/feature installed. Default is False\n\n        exclude (Optional[str]):\n            The name of the feature to exclude when installing the named\n            feature. This can be a single feature, a string of features in a\n            comma-delimited list (no spaces), or a list of features.\n\n            .. warning::\n                As there is no exclude option for the ``Add-WindowsFeature``\n                or ``Install-WindowsFeature`` PowerShell commands the features\n                named in ``exclude`` will be installed with other sub-features\n                and will then be removed. **If the feature named in ``exclude``\n                is not a sub-feature of one of the installed items it will still\n                be removed.**\n\n    Example:\n\n        Do not use the role or feature names mentioned in the PKGMGR\n        documentation. To get a list of available roles and features run the\n        following command:\n\n        .. code-block:: bash\n\n            salt <minion_name> win_servermanager.list_available\n\n        Use the name in the right column of the results.\n\n    .. code-block:: yaml\n\n        # Installs the IIS Web Server Role (Web-Server)\n        IIS-WebServerRole:\n          win_servermanager.installed:\n            - recurse: True\n            - name: Web-Server\n\n        # Install multiple features, exclude the Web-Service\n        install_multiple_features:\n          win_servermanager.installed:\n            - recurse: True\n            - features:\n              - RemoteAccess\n              - XPS-Viewer\n              - SNMP-Service\n            - exclude:\n              - Web-Server\n    '''\n    ret = {'name': name,\n           'result': True,\n           'changes': {},\n           'comment': ''}\n\n    # Check if features is not passed, use name. Split commas\n    if features is None:\n        features = name.split(',')\n\n    # Make sure features is a list, split commas\n    if not isinstance(features, list):\n        features = features.split(',')\n\n    # Determine if the feature is installed\n    old = __salt__['win_servermanager.list_installed']()\n\n    cur_feat = []\n    for feature in features:\n\n        if feature not in old:\n            ret['changes'][feature] = \\\n                'Will be installed recurse={0}'.format(recurse)\n        elif recurse:\n            ret['changes'][feature] = \\\n                'Already installed but might install sub-features'\n        else:\n            cur_feat.append(feature)\n\n    if cur_feat:\n        cur_feat.insert(0, 'The following features are already installed:')\n        ret['comment'] = '\\n- '.join(cur_feat)\n\n    if not ret['changes']:\n        return ret\n\n    if __opts__['test']:\n        ret['result'] = None\n        return ret\n\n    # Install the features\n    status = __salt__['win_servermanager.install'](\n        features, recurse=recurse, restart=restart, source=source,\n        exclude=exclude)\n\n    ret['result'] = status['Success']\n\n    # Show items failed to install\n    fail_feat = []\n    new_feat = []\n    rem_feat = []\n    for feature in status['Features']:\n        # Features that failed to install or be removed\n        if not status['Features'][feature].get('Success', True):\n            fail_feat.append('- {0}'.format(feature))\n        # Features that installed\n        elif '(exclude)' not in status['Features'][feature]['Message']:\n            new_feat.append('- {0}'.format(feature))\n        # Show items that were removed because they were part of `exclude`\n        elif '(exclude)' in status['Features'][feature]['Message']:\n            rem_feat.append('- {0}'.format(feature))\n\n    if fail_feat:\n        fail_feat.insert(0, 'Failed to install the following:')\n    if new_feat:\n        new_feat.insert(0, 'Installed the following:')\n    if rem_feat:\n        rem_feat.insert(0, 'Removed the following (exclude):')\n\n    ret['comment'] = '\\n'.join(fail_feat + new_feat + rem_feat)\n\n    # Get the changes\n    new = __salt__['win_servermanager.list_installed']()\n    ret['changes'] = salt.utils.data.compare_dicts(old, new)\n\n    return ret", "response": "Installs the named windows feature."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nremoving the windows feature from the PKGMGR.", "response": "def removed(name, features=None, remove_payload=False, restart=False):\n    '''\n    Remove the windows feature To remove a single feature, use the ``name``\n    parameter. To remove multiple features, use the ``features`` parameter.\n\n    Args:\n        name (str):\n            Short name of the feature (the right column in\n            win_servermanager.list_available). This can be a single feature or a\n            string of features in a comma-delimited list (no spaces)\n\n            .. note::\n                A list is not allowed in the name parameter of any state. Use\n                the ``features`` parameter if you want to pass the features as a\n                list\n\n        features (Optional[list]):\n            A list of features to remove. If this is passed it will be used\n            instead of the ``name`` parameter.\n\n            .. versionadded:: 2018.3.0\n\n        remove_payload (Optional[bool]):\n            True will cause the feature to be removed from the side-by-side\n            store. To install the feature in the future you will need to\n            specify the ``source``\n\n        restart (Optional[bool]):\n            Restarts the computer when uninstall is complete if required by the\n            role/feature uninstall. Default is False\n\n    .. note::\n        Some features require a reboot after uninstall. If so the feature will\n        not be completely uninstalled until the server is restarted.\n\n    Example:\n\n        Do not use the role or feature names mentioned in the PKGMGR\n        documentation. To get a list of available roles and features run the\n        following command:\n\n        .. code-block:: bash\n\n            salt <minion_name> win_servermanager.list_available\n\n        Use the name in the right column of the results.\n\n    .. code-block:: yaml\n\n        # Uninstall the IIS Web Server Rol (Web-Server)\n        IIS-WebserverRole:\n          win_servermanager.removed:\n            - name: Web-Server\n\n        # Uninstall multiple features, reboot if required\n        uninstall_multiple_features:\n          win_servermanager.removed:\n            - features:\n              - RemoteAccess\n              - XPX-Viewer\n              - SNMP-Service\n            - restart: True\n    '''\n    ret = {'name': name,\n           'result': True,\n           'changes': {},\n           'comment': ''}\n\n    # Check if features is not passed, use name. Split commas\n    if features is None:\n        features = name.split(',')\n\n    # Make sure features is a list, split commas\n    if not isinstance(features, list):\n        features = features.split(',')\n\n    # Determine if the feature is installed\n    old = __salt__['win_servermanager.list_installed']()\n\n    rem_feat = []\n    for feature in features:\n\n        if feature in old:\n            ret['changes'][feature] = 'Will be removed'\n        else:\n            rem_feat.append(feature)\n\n    if rem_feat:\n        rem_feat.insert(0, 'The following features are not installed:')\n        ret['comment'] = '\\n- '.join(rem_feat)\n\n    if not ret['changes']:\n        return ret\n\n    if __opts__['test']:\n        ret['result'] = None\n        return ret\n\n    # Remove the features\n    status = __salt__['win_servermanager.remove'](\n        features, remove_payload=remove_payload, restart=restart)\n\n    ret['result'] = status['Success']\n\n    # Some items failed to uninstall\n    fail_feat = []\n    rem_feat = []\n    for feature in status['Features']:\n        # Use get because sometimes 'Success' isn't defined such as when the\n        # feature is already uninstalled\n        if not status['Features'][feature].get('Success', True):\n            # Show items that failed to uninstall\n            fail_feat.append('- {0}'.format(feature))\n        else:\n            # Show items that uninstalled\n            rem_feat.append('- {0}'.format(feature))\n\n    if fail_feat:\n        fail_feat.insert(0, 'Failed to remove the following:')\n    if rem_feat:\n        rem_feat.insert(0, 'Removed the following:')\n\n    ret['comment'] = '\\n'.join(fail_feat + rem_feat)\n\n    # Get the changes\n    new = __salt__['win_servermanager.list_installed']()\n    ret['changes'] = salt.utils.data.compare_dicts(old, new)\n\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _check_cors_origin(origin, allowed_origins):\n    '''\n    Check if an origin match cors allowed origins\n    '''\n    if isinstance(allowed_origins, list):\n        if origin in allowed_origins:\n            return origin\n    elif allowed_origins == '*':\n        return allowed_origins\n    elif allowed_origins == origin:\n        # Cors origin is either * or specific origin\n        return allowed_origins", "response": "Check if an origin matches cors allowed origins\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_event(self,\n                  request,\n                  tag='',\n                  matcher=prefix_matcher.__func__,\n                  callback=None,\n                  timeout=None\n                  ):\n        '''\n        Get an event (asynchronous of course) return a future that will get it later\n        '''\n        # if the request finished, no reason to allow event fetching, since we\n        # can't send back to the client\n        if request._finished:\n            future = Future()\n            future.set_exception(TimeoutException())\n            return future\n\n        future = Future()\n        if callback is not None:\n            def handle_future(future):\n                tornado.ioloop.IOLoop.current().add_callback(callback, future)\n            future.add_done_callback(handle_future)\n        # add this tag and future to the callbacks\n        self.tag_map[(tag, matcher)].append(future)\n        self.request_map[request].append((tag, matcher, future))\n\n        if timeout:\n            timeout_future = tornado.ioloop.IOLoop.current().call_later(timeout, self._timeout_future, tag, matcher, future)\n            self.timeout_map[future] = timeout_future\n\n        return future", "response": "Get an event from the queue."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _handle_event_socket_recv(self, raw):\n        '''\n        Callback for events on the event sub socket\n        '''\n        mtag, data = self.event.unpack(raw, self.event.serial)\n\n        # see if we have any futures that need this info:\n        for (tag, matcher), futures in six.iteritems(self.tag_map):\n            try:\n                is_matched = matcher(mtag, tag)\n            except Exception:\n                log.error('Failed to run a matcher.', exc_info=True)\n                is_matched = False\n\n            if not is_matched:\n                continue\n\n            for future in futures:\n                if future.done():\n                    continue\n                future.set_result({'data': data, 'tag': mtag})\n                self.tag_map[(tag, matcher)].remove(future)\n                if future in self.timeout_map:\n                    tornado.ioloop.IOLoop.current().remove_timeout(self.timeout_map[future])\n                    del self.timeout_map[future]", "response": "Callback for events on the event sub socket."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nverifying that the client is in fact one we have.", "response": "def _verify_client(self, low):\n        '''\n        Verify that the client is in fact one we have\n        '''\n        if 'client' not in low or low.get('client') not in self.saltclients:\n            self.set_status(400)\n            self.write(\"400 Invalid Client: Client not found in salt clients\")\n            self.finish()\n            return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef initialize(self):\n        '''\n        Initialize the handler before requests are called\n        '''\n        if not hasattr(self.application, 'event_listener'):\n            log.debug('init a listener')\n            self.application.event_listener = EventListener(\n                self.application.mod_opts,\n                self.application.opts,\n            )\n\n        if not hasattr(self, 'saltclients'):\n            local_client = salt.client.get_local_client(mopts=self.application.opts)\n            self.saltclients = {\n                'local': local_client.run_job_async,\n                # not the actual client we'll use.. but its what we'll use to get args\n                'local_async': local_client.run_job_async,\n                'runner': salt.runner.RunnerClient(opts=self.application.opts).cmd_async,\n                'runner_async': None,  # empty, since we use the same client as `runner`\n                }\n\n        if not hasattr(self, 'ckminions'):\n            self.ckminions = salt.utils.minions.CkMinions(self.application.opts)", "response": "Initialize the handler before requests are called\n           "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _verify_auth(self):\n        '''\n        Boolean whether the request is auth'd\n        '''\n\n        return self.token and bool(self.application.auth.get_tok(self.token))", "response": "Check if the request is auth'd and if so verify that the token is valid."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nruns before get or posts etc.", "response": "def prepare(self):\n        '''\n        Run before get/posts etc. Pre-flight checks:\n            - verify that we can speak back to them (compatible accept header)\n        '''\n        # Find an acceptable content-type\n        accept_header = self.request.headers.get('Accept', '*/*')\n        # Ignore any parameter, including q (quality) one\n        parsed_accept_header = [cgi.parse_header(h)[0] for h in accept_header.split(',')]\n\n        def find_acceptable_content_type(parsed_accept_header):\n            for media_range in parsed_accept_header:\n                for content_type, dumper in self.ct_out_map:\n                    if fnmatch.fnmatch(content_type, media_range):\n                        return content_type, dumper\n            return None, None\n\n        content_type, dumper = find_acceptable_content_type(parsed_accept_header)\n\n        # better return message?\n        if not content_type:\n            self.send_error(406)\n\n        self.content_type = content_type\n        self.dumper = dumper\n\n        # do the common parts\n        self.start = time.time()\n        self.connected = True\n\n        self.lowstate = self._get_lowstate()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nserialize the data into a string.", "response": "def serialize(self, data):\n        '''\n        Serlialize the output based on the Accept header\n        '''\n        self.set_header('Content-Type', self.content_type)\n\n        return self.dumper(data)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _form_loader(self, _):\n        '''\n        function to get the data from the urlencoded forms\n        ignore the data passed in and just get the args from wherever they are\n        '''\n        data = {}\n        for key in self.request.arguments:\n            val = self.get_arguments(key)\n            if len(val) == 1:\n                data[key] = val[0]\n            else:\n                data[key] = val\n        return data", "response": "function to get the data from the urlencoded forms\n        ignore the data passed in and just get the args from wherever they are"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nformat the incoming data into a lowstate object", "response": "def _get_lowstate(self):\n        '''\n        Format the incoming data into a lowstate object\n        '''\n        if not self.request.body:\n            return\n        data = self.deserialize(self.request.body)\n        self.request_payload = copy(data)\n\n        if data and 'arg' in data and not isinstance(data['arg'], list):\n            data['arg'] = [data['arg']]\n\n        if not isinstance(data, list):\n            lowstate = [data]\n        else:\n            lowstate = data\n\n        return lowstate"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_default_headers(self):\n        '''\n        Set default CORS headers\n        '''\n        mod_opts = self.application.mod_opts\n\n        if mod_opts.get('cors_origin'):\n            origin = self.request.headers.get('Origin')\n\n            allowed_origin = _check_cors_origin(origin, mod_opts['cors_origin'])\n\n            if allowed_origin:\n                self.set_header(\"Access-Control-Allow-Origin\", allowed_origin)", "response": "Set default CORS headers."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef options(self, *args, **kwargs):\n        '''\n        Return CORS headers for preflight requests\n        '''\n        # Allow X-Auth-Token in requests\n        request_headers = self.request.headers.get('Access-Control-Request-Headers')\n        allowed_headers = request_headers.split(',')\n\n        # Filter allowed header here if needed.\n\n        # Allow request headers\n        self.set_header('Access-Control-Allow-Headers', ','.join(allowed_headers))\n\n        # Allow X-Auth-Token in responses\n        self.set_header('Access-Control-Expose-Headers', 'X-Auth-Token')\n\n        # Allow all methods\n        self.set_header('Access-Control-Allow-Methods', 'OPTIONS, GET, POST')\n\n        self.set_status(204)\n        self.finish()", "response": "Return CORS headers for preflight requests\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get(self):\n        '''\n        All logins are done over post, this is a parked endpoint\n\n        .. http:get:: /login\n\n            :status 401: |401|\n            :status 406: |406|\n\n        **Example request:**\n\n        .. code-block:: bash\n\n            curl -i localhost:8000/login\n\n        .. code-block:: text\n\n            GET /login HTTP/1.1\n            Host: localhost:8000\n            Accept: application/json\n\n        **Example response:**\n\n        .. code-block:: text\n\n            HTTP/1.1 401 Unauthorized\n            Content-Type: application/json\n            Content-Length: 58\n\n            {\"status\": \"401 Unauthorized\", \"return\": \"Please log in\"}\n        '''\n        self.set_status(401)\n        self.set_header('WWW-Authenticate', 'Session')\n\n        ret = {'status': '401 Unauthorized',\n               'return': 'Please log in'}\n\n        self.write(self.serialize(ret))", "response": "This method is used to get the logins for a specific user."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef post(self):\n        '''\n        :ref:`Authenticate <rest_tornado-auth>` against Salt's eauth system\n\n        .. http:post:: /login\n\n            :reqheader X-Auth-Token: |req_token|\n            :reqheader Accept: |req_accept|\n            :reqheader Content-Type: |req_ct|\n\n            :form eauth: the eauth backend configured for the user\n            :form username: username\n            :form password: password\n\n            :status 200: |200|\n            :status 400: |400|\n            :status 401: |401|\n            :status 406: |406|\n            :status 500: |500|\n\n        **Example request:**\n\n        .. code-block:: bash\n\n            curl -si localhost:8000/login \\\\\n                    -H \"Accept: application/json\" \\\\\n                    -d username='saltuser' \\\\\n                    -d password='saltpass' \\\\\n                    -d eauth='pam'\n\n        .. code-block:: text\n\n            POST / HTTP/1.1\n            Host: localhost:8000\n            Content-Length: 42\n            Content-Type: application/x-www-form-urlencoded\n            Accept: application/json\n\n            username=saltuser&password=saltpass&eauth=pam\n\n        **Example response:**\n\n        .. code-block:: text\n\n            HTTP/1.1 200 OK\n            Content-Type: application/json\n            Content-Length: 206\n            X-Auth-Token: 6d1b722e\n            Set-Cookie: session_id=6d1b722e; expires=Sat, 17 Nov 2012 03:23:52 GMT; Path=/\n\n            {\"return\": {\n                \"token\": \"6d1b722e\",\n                \"start\": 1363805943.776223,\n                \"expire\": 1363849143.776224,\n                \"user\": \"saltuser\",\n                \"eauth\": \"pam\",\n                \"perms\": [\n                    \"grains.*\",\n                    \"status.*\",\n                    \"sys.*\",\n                    \"test.*\"\n                ]\n            }}\n        '''\n        try:\n            if not isinstance(self.request_payload, dict):\n                self.send_error(400)\n                return\n\n            creds = {'username': self.request_payload['username'],\n                     'password': self.request_payload['password'],\n                     'eauth': self.request_payload['eauth'],\n                     }\n        # if any of the args are missing, its a bad request\n        except KeyError:\n            self.send_error(400)\n            return\n\n        token = self.application.auth.mk_token(creds)\n        if 'token' not in token:\n            # TODO: nicer error message\n            # 'Could not authenticate using provided credentials')\n            self.send_error(401)\n            # return since we don't want to execute any more\n            return\n\n        # Grab eauth config for the current backend for the current user\n        try:\n            eauth = self.application.opts['external_auth'][token['eauth']]\n            # Get sum of '*' perms, user-specific perms, and group-specific perms\n            _perms = eauth.get(token['name'], [])\n            _perms.extend(eauth.get('*', []))\n\n            if 'groups' in token and token['groups']:\n                user_groups = set(token['groups'])\n                eauth_groups = set([i.rstrip('%') for i in eauth.keys() if i.endswith('%')])\n\n                for group in user_groups & eauth_groups:\n                    _perms.extend(eauth['{0}%'.format(group)])\n\n            # dedup. perm can be a complex dict, so we cant use set\n            perms = []\n            for perm in _perms:\n                if perm not in perms:\n                    perms.append(perm)\n\n        # If we can't find the creds, then they aren't authorized\n        except KeyError:\n            self.send_error(401)\n            return\n\n        except (AttributeError, IndexError):\n            log.debug(\n                \"Configuration for external_auth malformed for eauth '%s', \"\n                \"and user '%s'.\", token.get('eauth'), token.get('name'),\n                exc_info=True\n            )\n            # TODO better error -- 'Configuration for external_auth could not be read.'\n            self.send_error(500)\n            return\n\n        ret = {'return': [{\n            'token': token['token'],\n            'expire': token['expire'],\n            'start': token['start'],\n            'user': token['name'],\n            'eauth': token['eauth'],\n            'perms': perms,\n            }]}\n\n        self.write(self.serialize(ret))", "response": "Authenticate the user with the Salt s eauth system and return the current user s session_id."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get(self):\n        '''\n        An endpoint to determine salt-api capabilities\n\n        .. http:get:: /\n\n            :reqheader Accept: |req_accept|\n\n            :status 200: |200|\n            :status 401: |401|\n            :status 406: |406|\n\n        **Example request:**\n\n        .. code-block:: bash\n\n            curl -i localhost:8000\n\n        .. code-block:: text\n\n            GET / HTTP/1.1\n            Host: localhost:8000\n            Accept: application/json\n\n        **Example response:**\n\n        .. code-block:: text\n\n            HTTP/1.1 200 OK\n            Content-Type: application/json\n            Content-Legnth: 83\n\n            {\"clients\": [\"local\", \"local_async\", \"runner\", \"runner_async\"], \"return\": \"Welcome\"}\n        '''\n        ret = {\"clients\": list(self.saltclients.keys()),\n               \"return\": \"Welcome\"}\n        self.write(self.serialize(ret))", "response": "This method is used to get the salt - api capabilities of a specific key."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef disbatch(self):\n        '''\n        Disbatch all lowstates to the appropriate clients\n        '''\n        ret = []\n\n        # check clients before going, we want to throw 400 if one is bad\n        for low in self.lowstate:\n            if not self._verify_client(low):\n                return\n\n            # Make sure we have 'token' or 'username'/'password' in each low chunk.\n            # Salt will verify the credentials are correct.\n            if self.token is not None and 'token' not in low:\n                low['token'] = self.token\n\n            if not (('token' in low)\n                    or ('username' in low and 'password' in low and 'eauth' in low)):\n                ret.append('Failed to authenticate')\n                break\n\n            # disbatch to the correct handler\n            try:\n                chunk_ret = yield getattr(self, '_disbatch_{0}'.format(low['client']))(low)\n                ret.append(chunk_ret)\n            except (AuthenticationError, AuthorizationError, EauthAuthenticationError):\n                ret.append('Failed to authenticate')\n                break\n            except Exception as ex:\n                ret.append('Unexpected exception while handling request: {0}'.format(ex))\n                log.error('Unexpected exception while handling request:', exc_info=True)\n\n        if not self._finished:\n            self.write(self.serialize({'return': ret}))\n            self.finish()", "response": "Disbatch all lowstates to the appropriate clients\n           "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndispatch local client commands", "response": "def _disbatch_local(self, chunk):\n        '''\n        Dispatch local client commands\n        '''\n        # Generate jid and find all minions before triggering a job to subscribe all returns from minions\n        full_return = chunk.pop('full_return', False)\n        chunk['jid'] = salt.utils.jid.gen_jid(self.application.opts) if not chunk.get('jid', None) else chunk['jid']\n        minions = set(self.ckminions.check_minions(chunk['tgt'], chunk.get('tgt_type', 'glob')))\n\n        def subscribe_minion(minion):\n            salt_evt = self.application.event_listener.get_event(\n                     self,\n                     tag='salt/job/{}/ret/{}'.format(chunk['jid'], minion),\n                     matcher=EventListener.exact_matcher)\n            syndic_evt = self.application.event_listener.get_event(\n                    self,\n                    tag='syndic/job/{}/ret/{}'.format(chunk['jid'], minion),\n                    matcher=EventListener.exact_matcher)\n            return salt_evt, syndic_evt\n\n        # start listening for the event before we fire the job to avoid races\n        events = []\n        for minion in minions:\n            salt_evt, syndic_evt = subscribe_minion(minion)\n            events.append(salt_evt)\n            events.append(syndic_evt)\n\n        f_call = self._format_call_run_job_async(chunk)\n        # fire a job off\n        pub_data = yield self.saltclients['local'](*f_call.get('args', ()), **f_call.get('kwargs', {}))\n\n        # if the job didn't publish, lets not wait around for nothing\n        # TODO: set header??\n        if 'jid' not in pub_data:\n            for future in events:\n                try:\n                    future.set_result(None)\n                except Exception:\n                    pass\n            raise tornado.gen.Return('No minions matched the target. No command was sent, no jid was assigned.')\n\n        # get_event for missing minion\n        for minion in list(set(pub_data['minions']) - set(minions)):\n            salt_evt, syndic_evt = subscribe_minion(minion)\n            events.append(salt_evt)\n            events.append(syndic_evt)\n\n        # Map of minion_id -> returned for all minions we think we need to wait on\n        minions = {m: False for m in pub_data['minions']}\n\n        # minimum time required for return to complete. By default no waiting, if\n        # we are a syndic then we must wait syndic_wait at a minimum\n        min_wait_time = Future()\n        min_wait_time.set_result(True)\n\n        # wait syndic a while to avoid missing published events\n        if self.application.opts['order_masters']:\n            min_wait_time = tornado.gen.sleep(self.application.opts['syndic_wait'])\n\n        # To ensure job_not_running and all_return are terminated by each other, communicate using a future\n        is_timed_out = tornado.gen.sleep(self.application.opts['gather_job_timeout'])\n        is_finished = Future()\n\n        # ping until the job is not running, while doing so, if we see new minions returning\n        # that they are running the job, add them to the list\n        tornado.ioloop.IOLoop.current().spawn_callback(self.job_not_running, pub_data['jid'],\n                                                      chunk['tgt'],\n                                                      f_call['kwargs']['tgt_type'],\n                                                      minions,\n                                                      is_finished)\n\n        def more_todo():\n            '''\n            Check if there are any more minions we are waiting on returns from\n            '''\n            return any(x is False for x in six.itervalues(minions))\n\n        # here we want to follow the behavior of LocalClient.get_iter_returns\n        # namely we want to wait at least syndic_wait (assuming we are a syndic)\n        # and that there are no more jobs running on minions. We are allowed to exit\n        # early if gather_job_timeout has been exceeded\n        chunk_ret = {}\n        while True:\n            to_wait = events+[is_finished, is_timed_out]\n            if not min_wait_time.done():\n                to_wait += [min_wait_time]\n\n            def cancel_inflight_futures():\n                for event in to_wait:\n                    if not event.done() and event is not is_timed_out:\n                        event.set_result(None)\n            f = yield Any(to_wait)\n            try:\n                # When finished entire routine, cleanup other futures and return result\n                if f is is_finished or f is is_timed_out:\n                    cancel_inflight_futures()\n                    raise tornado.gen.Return(chunk_ret)\n                elif f is min_wait_time:\n                    if not more_todo():\n                        cancel_inflight_futures()\n                        raise tornado.gen.Return(chunk_ret)\n                    continue\n\n                f_result = f.result()\n                if f in events:\n                    events.remove(f)\n                # if this is a start, then we need to add it to the pile\n                if f_result['tag'].endswith('/new'):\n                    for minion_id in f_result['data']['minions']:\n                        if minion_id not in minions:\n                            minions[minion_id] = False\n                else:\n                    chunk_ret[f_result['data']['id']] = f_result if full_return else f_result['data']['return']\n                    # clear finished event future\n                    minions[f_result['data']['id']] = True\n                    # if there are no more minions to wait for, then we are done\n                    if not more_todo() and min_wait_time.done():\n                        cancel_inflight_futures()\n                        raise tornado.gen.Return(chunk_ret)\n\n            except TimeoutException:\n                break"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a future which will complete once the job is no longer running on tgt", "response": "def job_not_running(self, jid, tgt, tgt_type, minions, is_finished):\n        '''\n        Return a future which will complete once jid (passed in) is no longer\n        running on tgt\n        '''\n        ping_pub_data = yield self.saltclients['local'](tgt,\n                                                        'saltutil.find_job',\n                                                        [jid],\n                                                        tgt_type=tgt_type)\n        ping_tag = tagify([ping_pub_data['jid'], 'ret'], 'job')\n\n        minion_running = False\n        while True:\n            try:\n                event = self.application.event_listener.get_event(self,\n                                                                  tag=ping_tag,\n                                                                  timeout=self.application.opts['gather_job_timeout'])\n                event = yield event\n            except TimeoutException:\n                if not event.done():\n                    event.set_result(None)\n\n                if not minion_running or is_finished.done():\n                    raise tornado.gen.Return(True)\n                else:\n                    ping_pub_data = yield self.saltclients['local'](tgt,\n                                                                    'saltutil.find_job',\n                                                                    [jid],\n                                                                    tgt_type=tgt_type)\n                    ping_tag = tagify([ping_pub_data['jid'], 'ret'], 'job')\n                    minion_running = False\n                    continue\n\n            # Minions can return, we want to see if the job is running...\n            if event['data'].get('return', {}) == {}:\n                continue\n            if event['data']['id'] not in minions:\n                minions[event['data']['id']] = False\n            minion_running = True"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _disbatch_runner(self, chunk):\n        '''\n        Disbatch runner client commands\n        '''\n        full_return = chunk.pop('full_return', False)\n        pub_data = self.saltclients['runner'](chunk)\n        tag = pub_data['tag'] + '/ret'\n        try:\n            event = yield self.application.event_listener.get_event(self, tag=tag)\n\n            # only return the return data\n            ret = event if full_return else event['data']['return']\n            raise tornado.gen.Return(ret)\n        except TimeoutException:\n            raise tornado.gen.Return('Timeout waiting for runner to execute')", "response": "Disbatch runner client commands\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _disbatch_runner_async(self, chunk):\n        '''\n        Disbatch runner client_async commands\n        '''\n        pub_data = self.saltclients['runner'](chunk)\n        raise tornado.gen.Return(pub_data)", "response": "Disbatch runner client_async commands\n       "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nexecuting a command and immediately return the job id", "response": "def post(self):\n        '''\n        Start an execution command and immediately return the job id\n\n        .. http:post:: /minions\n\n            :reqheader X-Auth-Token: |req_token|\n            :reqheader Accept: |req_accept|\n            :reqheader Content-Type: |req_ct|\n\n            :resheader Content-Type: |res_ct|\n\n            :status 200: |200|\n            :status 401: |401|\n            :status 406: |406|\n\n            :term:`lowstate` data describing Salt commands must be sent in the\n            request body. The ``client`` option will be set to\n            :py:meth:`~salt.client.LocalClient.local_async`.\n\n        **Example request:**\n\n        .. code-block:: bash\n\n            curl -sSi localhost:8000/minions \\\\\n                -H \"Accept: application/x-yaml\" \\\\\n                -d tgt='*' \\\\\n                -d fun='status.diskusage'\n\n        .. code-block:: text\n\n            POST /minions HTTP/1.1\n            Host: localhost:8000\n            Accept: application/x-yaml\n            Content-Length: 26\n            Content-Type: application/x-www-form-urlencoded\n\n            tgt=*&fun=status.diskusage\n\n        **Example response:**\n\n        .. code-block:: text\n\n            HTTP/1.1 202 Accepted\n            Content-Length: 86\n            Content-Type: application/x-yaml\n\n            return:\n            - jid: '20130603122505459265'\n              minions: [ms-4, ms-3, ms-2, ms-1, ms-0]\n        '''\n        # if you aren't authenticated, redirect to login\n        if not self._verify_auth():\n            self.redirect('/login')\n            return\n\n        # verify that all lowstates are the correct client type\n        for low in self.lowstate:\n            # if you didn't specify, its fine\n            if 'client' not in low:\n                low['client'] = 'local_async'\n                continue\n            # if you specified something else, we don't do that\n            if low.get('client') != 'local_async':\n                self.set_status(400)\n                self.write('We don\\'t serve your kind here')\n                self.finish()\n                return\n\n        self.disbatch()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get(self, jid=None):  # pylint: disable=W0221\n        '''\n        A convenience URL for getting lists of previously run jobs or getting\n        the return from a single job\n\n        .. http:get:: /jobs/(jid)\n\n            List jobs or show a single job from the job cache.\n\n            :status 200: |200|\n            :status 401: |401|\n            :status 406: |406|\n\n        **Example request:**\n\n        .. code-block:: bash\n\n            curl -i localhost:8000/jobs\n\n        .. code-block:: text\n\n            GET /jobs HTTP/1.1\n            Host: localhost:8000\n            Accept: application/x-yaml\n\n        **Example response:**\n\n        .. code-block:: text\n\n            HTTP/1.1 200 OK\n            Content-Length: 165\n            Content-Type: application/x-yaml\n\n            return:\n            - '20121130104633606931':\n                Arguments:\n                - '3'\n                Function: test.fib\n                Start Time: 2012, Nov 30 10:46:33.606931\n                Target: jerry\n                Target-type: glob\n\n        **Example request:**\n\n        .. code-block:: bash\n\n            curl -i localhost:8000/jobs/20121130104633606931\n\n        .. code-block:: text\n\n            GET /jobs/20121130104633606931 HTTP/1.1\n            Host: localhost:8000\n            Accept: application/x-yaml\n\n        **Example response:**\n\n        .. code-block:: text\n\n            HTTP/1.1 200 OK\n            Content-Length: 73\n            Content-Type: application/x-yaml\n\n            info:\n            - Arguments:\n                - '3'\n                Function: test.fib\n                Minions:\n                - jerry\n                Start Time: 2012, Nov 30 10:46:33.606931\n                Target: '*'\n                Target-type: glob\n                User: saltdev\n                jid: '20121130104633606931'\n            return:\n            - jerry:\n                - - 0\n                - 1\n                - 1\n                - 2\n                - 6.9141387939453125e-06\n        '''\n        # if you aren't authenticated, redirect to login\n        if not self._verify_auth():\n            self.redirect('/login')\n            return\n\n        if jid:\n            self.lowstate = [{\n                'fun': 'jobs.list_job',\n                'jid': jid,\n                'client': 'runner',\n            }]\n        else:\n            self.lowstate = [{\n                'fun': 'jobs.list_jobs',\n                'client': 'runner',\n            }]\n\n        self.disbatch()", "response": "A convenience method for getting a single job from a job cache or show a single job from a job cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get(self):\n        r'''\n        An HTTP stream of the Salt master event bus\n\n        This stream is formatted per the Server Sent Events (SSE) spec. Each\n        event is formatted as JSON.\n\n        .. http:get:: /events\n\n            :status 200: |200|\n            :status 401: |401|\n            :status 406: |406|\n\n        **Example request:**\n\n        .. code-block:: bash\n\n            curl -NsS localhost:8000/events\n\n        .. code-block:: text\n\n            GET /events HTTP/1.1\n            Host: localhost:8000\n\n        **Example response:**\n\n        .. code-block:: text\n\n            HTTP/1.1 200 OK\n            Connection: keep-alive\n            Cache-Control: no-cache\n            Content-Type: text/event-stream;charset=utf-8\n\n            retry: 400\n            data: {'tag': '', 'data': {'minions': ['ms-4', 'ms-3', 'ms-2', 'ms-1', 'ms-0']}}\n\n            data: {'tag': '20130802115730568475', 'data': {'jid': '20130802115730568475', 'return': True, 'retcode': 0, 'success': True, 'cmd': '_return', 'fun': 'test.ping', 'id': 'ms-1'}}\n\n        The event stream can be easily consumed via JavaScript:\n\n        .. code-block:: javascript\n\n            # Note, you must be authenticated!\n            var source = new EventSource('/events');\n            source.onopen = function() { console.debug('opening') };\n            source.onerror = function(e) { console.debug('error!', e) };\n            source.onmessage = function(e) { console.debug(e.data) };\n\n        Or using CORS:\n\n        .. code-block:: javascript\n\n            var source = new EventSource('/events', {withCredentials: true});\n\n        Some browser clients lack CORS support for the ``EventSource()`` API. Such\n        clients may instead pass the :mailheader:`X-Auth-Token` value as an URL\n        parameter:\n\n        .. code-block:: bash\n\n            curl -NsS localhost:8000/events/6d1b722e\n\n        It is also possible to consume the stream via the shell.\n\n        Records are separated by blank lines; the ``data:`` and ``tag:``\n        prefixes will need to be removed manually before attempting to\n        unserialize the JSON.\n\n        curl's ``-N`` flag turns off input buffering which is required to\n        process the stream incrementally.\n\n        Here is a basic example of printing each event as it comes in:\n\n        .. code-block:: bash\n\n            curl -NsS localhost:8000/events |\\\n                    while IFS= read -r line ; do\n                        echo $line\n                    done\n\n        Here is an example of using awk to filter events based on tag:\n\n        .. code-block:: bash\n\n            curl -NsS localhost:8000/events |\\\n                    awk '\n                        BEGIN { RS=\"\"; FS=\"\\\\n\" }\n                        $1 ~ /^tag: salt\\/job\\/[0-9]+\\/new$/ { print $0 }\n                    '\n            tag: salt/job/20140112010149808995/new\n            data: {\"tag\": \"salt/job/20140112010149808995/new\", \"data\": {\"tgt_type\": \"glob\", \"jid\": \"20140112010149808995\", \"tgt\": \"jerry\", \"_stamp\": \"2014-01-12_01:01:49.809617\", \"user\": \"shouse\", \"arg\": [], \"fun\": \"test.ping\", \"minions\": [\"jerry\"]}}\n            tag: 20140112010149808995\n            data: {\"tag\": \"20140112010149808995\", \"data\": {\"fun_args\": [], \"jid\": \"20140112010149808995\", \"return\": true, \"retcode\": 0, \"success\": true, \"cmd\": \"_return\", \"_stamp\": \"2014-01-12_01:01:49.819316\", \"fun\": \"test.ping\", \"id\": \"jerry\"}}\n        '''\n        # if you aren't authenticated, redirect to login\n        if not self._verify_auth():\n            self.redirect('/login')\n            return\n        # set the streaming headers\n        self.set_header('Content-Type', 'text/event-stream')\n        self.set_header('Cache-Control', 'no-cache')\n        self.set_header('Connection', 'keep-alive')\n\n        self.write('retry: {0}\\n'.format(400))\n        self.flush()\n\n        while True:\n            try:\n                event = yield self.application.event_listener.get_event(self)\n                self.write('tag: {0}\\n'.format(event.get('tag', '')))\n                self.write(str('data: {0}\\n\\n').format(_json_dumps(event)))  # future lint: disable=blacklisted-function\n                self.flush()\n            except TimeoutException:\n                break", "response": "r Returns a string containing the current state of the master event bus."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsending a POST request to the URL of the Knockout build log.", "response": "def post(self, tag_suffix=None):  # pylint: disable=W0221\n        '''\n        Fire an event in Salt with a custom event tag and data\n\n        .. http:post:: /hook\n\n            :status 200: |200|\n            :status 401: |401|\n            :status 406: |406|\n            :status 413: request body is too large\n\n        **Example request:**\n\n        .. code-block:: bash\n\n            curl -sS localhost:8000/hook -d foo='Foo!' -d bar='Bar!'\n\n        .. code-block:: text\n\n            POST /hook HTTP/1.1\n            Host: localhost:8000\n            Content-Length: 16\n            Content-Type: application/x-www-form-urlencoded\n\n            foo=Foo&bar=Bar!\n\n        **Example response**:\n\n        .. code-block:: text\n\n            HTTP/1.1 200 OK\n            Content-Length: 14\n            Content-Type: application/json\n\n            {\"success\": true}\n\n        As a practical example, an internal continuous-integration build\n        server could send an HTTP POST request to the URL\n        ``http://localhost:8000/hook/mycompany/build/success`` which contains\n        the result of a build and the SHA of the version that was built as\n        JSON. That would then produce the following event in Salt that could be\n        used to kick off a deployment via Salt's Reactor:\n\n        .. code-block:: text\n\n            Event fired at Fri Feb 14 17:40:11 2014\n            *************************\n            Tag: salt/netapi/hook/mycompany/build/success\n            Data:\n            {'_stamp': '2014-02-14_17:40:11.440996',\n                'headers': {\n                    'X-My-Secret-Key': 'F0fAgoQjIT@W',\n                    'Content-Length': '37',\n                    'Content-Type': 'application/json',\n                    'Host': 'localhost:8000',\n                    'Remote-Addr': '127.0.0.1'},\n                'post': {'revision': 'aa22a3c4b2e7', 'result': True}}\n\n        Salt's Reactor could listen for the event:\n\n        .. code-block:: yaml\n\n            reactor:\n              - 'salt/netapi/hook/mycompany/build/*':\n                - /srv/reactor/react_ci_builds.sls\n\n        And finally deploy the new build:\n\n        .. code-block:: jinja\n\n            {% set secret_key = data.get('headers', {}).get('X-My-Secret-Key') %}\n            {% set build = data.get('post', {}) %}\n\n            {% if secret_key == 'F0fAgoQjIT@W' and build.result == True %}\n            deploy_my_app:\n              cmd.state.sls:\n                - tgt: 'application*'\n                - arg:\n                  - myapp.deploy\n                - kwarg:\n                    pillar:\n                      revision: {{ revision }}\n            {% endif %}\n        '''\n        disable_auth = self.application.mod_opts.get('webhook_disable_auth')\n        if not disable_auth and not self._verify_auth():\n            self.redirect('/login')\n            return\n\n        # if you have the tag, prefix\n        tag = 'salt/netapi/hook'\n        if tag_suffix:\n            tag += tag_suffix\n\n        # TODO: consolidate??\n        self.event = salt.utils.event.get_event(\n            'master',\n            self.application.opts['sock_dir'],\n            self.application.opts['transport'],\n            opts=self.application.opts,\n            listen=False)\n\n        arguments = {}\n        for argname in self.request.query_arguments:\n            value = self.get_arguments(argname)\n            if len(value) == 1:\n                value = value[0]\n            arguments[argname] = value\n        ret = self.event.fire_event({\n            'post': self.request_payload,\n            'get': arguments,\n            # In Tornado >= v4.0.3, the headers come\n            # back as an HTTPHeaders instance, which\n            # is a dictionary. We must cast this as\n            # a dictionary in order for msgpack to\n            # serialize it.\n            'headers': dict(self.request.headers),\n        }, tag)\n\n        self.write(self.serialize({'success': ret}))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef beacon(config):\n    '''\n    Called several times each second\n    https://docs.saltstack.com/en/latest/topics/beacons/#the-beacon-function\n\n    .. code-block:: yaml\n\n        beacons:\n          proxy_example:\n            - endpoint: beacon\n    '''\n    # Important!!!\n    # Although this toy example makes an HTTP call\n    # to get beacon information\n    # please be advised that doing CPU or IO intensive\n    # operations in this method will cause the beacon loop\n    # to block.\n    _config = {}\n    list(map(_config.update, config))\n\n    beacon_url = '{0}{1}'.format(__opts__['proxy']['url'],\n                                 _config['endpoint'])\n    ret = salt.utils.http.query(beacon_url,\n                                decode_type='json',\n                                decode=True)\n    return [ret['dict']]", "response": "Get the beacon information for the second\n countryCode"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nensures user account is absent", "response": "def absent(name):\n    '''\n    Ensure user account is absent\n\n    name : string\n        username\n    '''\n    ret = {'name': name,\n           'changes': {},\n           'result': True,\n           'comment': ''}\n\n    # remove if needed\n    if name in __salt__['pdbedit.list'](False):\n        res = __salt__['pdbedit.delete'](name)\n        if res[name] in ['deleted']:  # check if we need to update changes\n            ret['changes'].update(res)\n        elif res[name] not in ['absent']:  # oops something went wrong\n            ret['result'] = False\n    else:\n        ret['comment'] = 'account {login} is absent'.format(login=name)\n\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmanage user account and user account", "response": "def managed(name, **kwargs):\n    '''\n    Manage user account\n\n    login : string\n        login name\n    password : string\n        password\n    password_hashed : boolean\n        set if password is a nt hash instead of plain text\n    domain : string\n        users domain\n    profile : string\n        profile path\n    script : string\n        logon script\n    drive : string\n        home drive\n    homedir : string\n        home directory\n    fullname : string\n        full name\n    account_desc : string\n        account description\n    machine_sid : string\n        specify the machines new primary group SID or rid\n    user_sid : string\n        specify the users new primary group SID or rid\n    account_control : string\n        specify user account control properties\n\n        .. note::\n            Only the following can be set:\n            - N: No password required\n            - D: Account disabled\n            - H: Home directory required\n            - L: Automatic Locking\n            - X: Password does not expire\n    reset_login_hours : boolean\n        reset the users allowed logon hours\n    reset_bad_password_count : boolean\n        reset the stored bad login counter\n    '''\n    ret = {'name': name,\n           'changes': {},\n           'result': True,\n           'comment': ''}\n\n    # save state\n    saved = __salt__['pdbedit.list'](hashes=True)\n    saved = saved[name] if name in saved else {}\n\n    # call pdbedit.modify\n    kwargs['login'] = name\n    res = __salt__['pdbedit.modify'](**kwargs)\n\n    # calculate changes\n    if res[name] in ['created']:\n        ret['changes'] = res\n    elif res[name] in ['updated']:\n        ret['changes'][name] = salt.utils.data.compare_dicts(\n            saved,\n            __salt__['pdbedit.list'](hashes=True)[name],\n        )\n    elif res[name] not in ['unchanged']:\n        ret['result'] = False\n        ret['comment'] = res[name]\n\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _proxy_conf_file(proxyfile, test):\n    '''\n    Check if proxy conf exists and update\n    '''\n    changes_old = []\n    changes_new = []\n    success = True\n    if not os.path.exists(proxyfile):\n        try:\n            if not test:\n                changes_new.append(_write_proxy_conf(proxyfile))\n                msg = 'Salt Proxy: Wrote proxy conf {0}'.format(proxyfile)\n            else:\n                msg = 'Salt Proxy: Update required to proxy conf {0}' \\\n                    .format(proxyfile)\n        except (OSError, IOError) as err:\n            success = False\n            msg = 'Salt Proxy: Error writing proxy file {0}'.format(err)\n            log.error(msg)\n            changes_new.append(msg)\n        changes_new.append(msg)\n        log.debug(msg)\n    else:\n        msg = 'Salt Proxy: {0} already exists, skipping'.format(proxyfile)\n        changes_old.append(msg)\n        log.debug(msg)\n    return success, changes_new, changes_old", "response": "Check if proxy conf exists and update it"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _is_proxy_running(proxyname):\n    '''\n    Check if proxy for this name is running\n    '''\n    cmd = ('ps ax | grep \"salt-proxy --proxyid={0}\" | grep -v grep'\n           .format(salt.ext.six.moves.shlex_quote(proxyname)))\n    cmdout = __salt__['cmd.run_all'](\n        cmd,\n        timeout=5,\n        python_shell=True)\n    if not cmdout['stdout']:\n        return False\n    else:\n        return True", "response": "Check if proxy for this name is running"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _proxy_process(proxyname, test):\n    '''\n    Check and execute proxy process\n    '''\n    changes_old = []\n    changes_new = []\n    if not _is_proxy_running(proxyname):\n        if not test:\n            __salt__['cmd.run_all'](\n                'salt-proxy --proxyid={0} -l info -d'.format(salt.ext.six.moves.shlex_quote(proxyname)),\n                timeout=5)\n            changes_new.append('Salt Proxy: Started proxy process for {0}'\n                               .format(proxyname))\n        else:\n            changes_new.append('Salt Proxy: process {0} will be started'\n                               .format(proxyname))\n    else:\n        changes_old.append('Salt Proxy: already running for {0}'\n                           .format(proxyname))\n    return True, changes_new, changes_old", "response": "Check and execute proxy process\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconfigure the salt - proxy process", "response": "def configure_proxy(proxyname, start=True):\n    '''\n    Create the salt proxy file and start the proxy process\n    if required\n\n    Parameters:\n        proxyname:\n            Name to be used for this proxy (should match entries in pillar)\n        start:\n            Boolean indicating if the process should be started\n            default = True\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt deviceminion salt_proxy.configure_proxy p8000\n    '''\n    changes_new = []\n    changes_old = []\n    status_file = True\n    test = __opts__['test']\n\n    # write the proxy file if necessary\n    proxyfile = '/etc/salt/proxy'\n    status_file, msg_new, msg_old = _proxy_conf_file(proxyfile, test)\n    changes_new.extend(msg_new)\n    changes_old.extend(msg_old)\n    status_proc = False\n\n    # start the proxy process\n    if start:\n        status_proc, msg_new, msg_old = _proxy_process(proxyname, test)\n        changes_old.extend(msg_old)\n        changes_new.extend(msg_new)\n    else:\n        changes_old.append('Start is False, not starting salt-proxy process')\n        log.debug('Process not started')\n\n    return {\n        'result': status_file and status_proc,\n        'changes': {\n            'old': '\\n'.join(changes_old),\n            'new': '\\n'.join(changes_new),\n        },\n    }"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef publish_minions(self):\n        '''\n        Publishes minions as a list of dicts.\n        '''\n        log.debug('in publish minions')\n        minions = {}\n\n        log.debug('starting loop')\n        for minion, minion_info in six.iteritems(self.minions):\n            log.debug(minion)\n            # log.debug(minion_info)\n            curr_minion = {}\n            curr_minion.update(minion_info)\n            curr_minion.update({'id': minion})\n            minions[minion] = curr_minion\n        log.debug('ended loop')\n        ret = {'minions': minions}\n        self.handler.write_message(\n            salt.utils.json.dumps(ret) + str('\\n\\n'))", "response": "Publishes the minions as a list of dicts."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef publish(self, key, data):\n        '''\n        Publishes the data to the event stream.\n        '''\n        publish_data = {key: data}\n        pub = salt.utils.json.dumps(publish_data) + str('\\n\\n')  # future lint: disable=blacklisted-function\n        self.handler.write_message(pub)", "response": "Publishes the data to the event stream."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nassociating grains data with a minion and publish minion update", "response": "def process_minion_update(self, event_data):\n        '''\n        Associate grains data with a minion and publish minion update\n        '''\n        tag = event_data['tag']\n        event_info = event_data['data']\n\n        mid = tag.split('/')[-1]\n\n        if not self.minions.get(mid, None):\n            self.minions[mid] = {}\n\n        minion = self.minions[mid]\n\n        minion.update({'grains': event_info['return']})\n        log.debug(\"In process minion grains update with minions=%s\", self.minions)\n        self.publish_minions()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef process_ret_job_event(self, event_data):\n        '''\n        Process a /ret event returned by Salt for a particular minion.\n        These events contain the returned results from a particular execution.\n        '''\n        tag = event_data['tag']\n        event_info = event_data['data']\n\n        _, _, jid, _, mid = tag.split('/')\n        job = self.jobs.setdefault(jid, {})\n\n        minion = job.setdefault('minions', {}).setdefault(mid, {})\n        minion.update({'return': event_info['return']})\n        minion.update({'retcode': event_info['retcode']})\n        minion.update({'success': event_info['success']})\n\n        job_complete = all([minion['success'] for mid, minion\n                            in six.iteritems(job['minions'])])\n\n        if job_complete:\n            job['state'] = 'complete'\n\n        self.publish('jobs', self.jobs)", "response": "Process a ret event returned by Salt for a particular minion."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprocesses a new job event.", "response": "def process_new_job_event(self, event_data):\n        '''\n        Creates a new job with properties from the event data\n        like jid, function, args, timestamp.\n\n        Also sets the initial state to started.\n\n        Minions that are participating in this job are also noted.\n\n        '''\n        job = None\n        tag = event_data['tag']\n        event_info = event_data['data']\n        minions = {}\n        for mid in event_info['minions']:\n            minions[mid] = {'success': False}\n\n        job = {\n            'jid': event_info['jid'],\n            'start_time': event_info['_stamp'],\n            'minions': minions,  # is a dictionary keyed by mids\n            'fun': event_info['fun'],\n            'tgt': event_info['tgt'],\n            'tgt_type': event_info['tgt_type'],\n            'state': 'running',\n        }\n        self.jobs[event_info['jid']] = job\n        self.publish('jobs', self.jobs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef process_key_event(self, event_data):\n        '''\n        Tag: salt/key\n        Data:\n        {'_stamp': '2014-05-20T22:45:04.345583',\n         'act': 'delete',\n         'id': 'compute.home',\n         'result': True}\n        '''\n\n        tag = event_data['tag']\n        event_info = event_data['data']\n\n        if event_info['act'] == 'delete':\n            self.minions.pop(event_info['id'], None)\n        elif event_info['act'] == 'accept':\n            self.minions.setdefault(event_info['id'], {})\n\n        self.publish_minions()", "response": "Process a key event from the master"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprocesses presence events from the client", "response": "def process_presence_events(self, salt_data, token, opts):\n        '''\n        Check if any minions have connected or dropped.\n        Send a message to the client if they have.\n        '''\n        log.debug('In presence')\n        changed = False\n\n        # check if any connections were dropped\n        if set(salt_data['data'].get('lost', [])):\n            dropped_minions = set(salt_data['data'].get('lost', []))\n        else:\n            dropped_minions = set(self.minions) - set(salt_data['data'].get('present', []))\n\n        for minion in dropped_minions:\n            changed = True\n            log.debug('Popping %s', minion)\n            self.minions.pop(minion, None)\n\n        # check if any new connections were made\n        if set(salt_data['data'].get('new', [])):\n            log.debug('got new minions')\n            new_minions = set(salt_data['data'].get('new', []))\n            changed = True\n        elif set(salt_data['data'].get('present', [])) - set(self.minions):\n            log.debug('detected new minions')\n            new_minions = set(salt_data['data'].get('present', [])) - set(self.minions)\n            changed = True\n        else:\n            new_minions = []\n\n        tgt = ','.join(new_minions)\n        for mid in new_minions:\n            log.debug('Adding minion')\n            self.minions[mid] = {}\n\n        if tgt:\n            changed = True\n            client = salt.netapi.NetapiClient(opts)\n            client.run(\n                {\n                    'fun': 'grains.items',\n                    'tgt': tgt,\n                    'expr_type': 'list',\n                    'mode': 'client',\n                    'client': 'local',\n                    'asynchronous': 'local_async',\n                    'token': token,\n                })\n\n        if changed:\n            self.publish_minions()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprocess events and publish data", "response": "def process(self, salt_data, token, opts):\n        '''\n        Process events and publish data\n        '''\n        log.debug('In process %s', threading.current_thread())\n        log.debug(salt_data['tag'])\n        log.debug(salt_data)\n\n        parts = salt_data['tag'].split('/')\n        if len(parts) < 2:\n            return\n\n        # TBD: Simplify these conditional expressions\n        if parts[1] == 'job':\n            log.debug('In job part 1')\n            if parts[3] == 'new':\n                log.debug('In new job')\n                self.process_new_job_event(salt_data)\n                # if salt_data['data']['fun'] == 'grains.items':\n                #     self.minions = {}\n            elif parts[3] == 'ret':\n                log.debug('In ret')\n                self.process_ret_job_event(salt_data)\n                if salt_data['data']['fun'] == 'grains.items':\n                    self.process_minion_update(salt_data)\n        elif parts[1] == 'key':\n            log.debug('In key')\n            self.process_key_event(salt_data)\n        elif parts[1] == 'presence':\n            self.process_presence_events(salt_data, token, opts)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the data from the minion", "response": "def get(tgt, fun, tgt_type='glob', roster='flat'):\n    '''\n    Get data from the mine based on the target, function and tgt_type\n\n    This will actually run the function on all targeted minions (like\n    publish.publish), as salt-ssh clients can't update the mine themselves.\n\n    We will look for mine_functions in the roster, pillar, and master config,\n    in that order, looking for a match for the defined function\n\n    Targets can be matched based on any standard matching system that can be\n    matched on the defined roster (in salt-ssh) via these keywords::\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-ssh '*' mine.get '*' network.interfaces\n        salt-ssh '*' mine.get 'myminion' network.interfaces roster=flat\n        salt-ssh '*' mine.get '192.168.5.0' network.ipaddrs roster=scan\n    '''\n    # Set up opts for the SSH object\n    opts = copy.deepcopy(__context__['master_opts'])\n    minopts = copy.deepcopy(__opts__)\n    opts.update(minopts)\n    if roster:\n        opts['roster'] = roster\n    opts['argv'] = [fun]\n    opts['selected_target_option'] = tgt_type\n    opts['tgt'] = tgt\n    opts['arg'] = []\n\n    # Create the SSH object to handle the actual call\n    ssh = salt.client.ssh.SSH(opts)\n\n    # Run salt-ssh to get the minion returns\n    rets = {}\n    for ret in ssh.run_iter(mine=True):\n        rets.update(ret)\n\n    cret = {}\n    for host in rets:\n        if 'return' in rets[host]:\n            cret[host] = rets[host]['return']\n        else:\n            cret[host] = rets[host]\n    return cret"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef wait_for(func, timeout=10, step=1, default=None, func_args=(), func_kwargs=None):\n    '''\n    Call `func` at regular intervals and Waits until the given function returns\n    a truthy value within the given timeout and returns that value.\n\n    @param func:\n    @type func: function\n    @param timeout:\n    @type timeout: int | float\n    @param step: Interval at which we should check for the value\n    @type step: int | float\n    @param default: Value that should be returned should `func` not return a truthy value\n    @type default:\n    @param func_args: *args for `func`\n    @type func_args: list | tuple\n    @param func_kwargs: **kwargs for `func`\n    @type func_kwargs: dict\n    @return: `default` or result of `func`\n    '''\n    if func_kwargs is None:\n        func_kwargs = dict()\n    max_time = time.time() + timeout\n    # Time moves forward so we might not reenter the loop if we step too long\n    step = min(step or 1, timeout) * BLUR_FACTOR\n\n    ret = default\n    while time.time() <= max_time:\n        call_ret = func(*func_args, **func_kwargs)\n        if call_ret:\n            ret = call_ret\n            break\n        else:\n            time.sleep(step)\n\n            # Don't allow cases of over-stepping the timeout\n            step = min(step, max_time - time.time()) * BLUR_FACTOR\n    if time.time() > max_time:\n        log.warning(\"Exceeded waiting time (%s seconds) to exectute %s\", timeout, func)\n    return ret", "response": "Call a function at regular intervals and Waits until the given function returns a truthy value within the given timeout and returns that value."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a list of service that are enabled on boot CLI Example : = > list of services that are enabled on boot salt '*' service. get_enabled", "response": "def get_enabled():\n    '''\n    Return a list of service that are enabled on boot\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.get_enabled\n    '''\n    prefix = '/etc/rc[S{0}].d/S'.format(_get_runlevel())\n    ret = set()\n    lines = glob.glob('{0}*'.format(prefix))\n    for line in lines:\n        ret.add(re.split(prefix + r'\\d+', line)[1])\n    return sorted(ret)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_all():\n    '''\n    Return all available boot services\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.get_all\n    '''\n    ret = set()\n    lines = glob.glob('/etc/init.d/*')\n    for line in lines:\n        service = line.split('/etc/init.d/')[1]\n        # Remove README.  If it's an enabled service, it will be added back in.\n        if service != 'README':\n            ret.add(service)\n    return sorted(ret | set(get_enabled()))", "response": "Return all available boot services"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nenables the named service to start at boot Taxonomy CLI Example : bash AttributeNames enable", "response": "def enable(name, **kwargs):\n    '''\n    Enable the named service to start at boot\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.enable <service name>\n    '''\n    osmajor = _osrel()[0]\n    if osmajor < '6':\n        cmd = 'update-rc.d -f {0} defaults 99'.format(_cmd_quote(name))\n    else:\n        cmd = 'update-rc.d {0} enable'.format(_cmd_quote(name))\n    try:\n        if int(osmajor) >= 6:\n            cmd = 'insserv {0} && '.format(_cmd_quote(name)) + cmd\n    except ValueError:\n        osrel = _osrel()\n        if osrel == 'testing/unstable' or osrel == 'unstable' or osrel.endswith(\"/sid\"):\n            cmd = 'insserv {0} && '.format(_cmd_quote(name)) + cmd\n    return not __salt__['cmd.retcode'](cmd, python_shell=True)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndisabling the named service to start at boot CLI Example : bash disable - f boot remove - f boot remove - f boot remove - f boot remove - f boot remove - f boot remove - f boot remove - f boot remove - f boot remove - f boot remove - f boot remove - f boot remove - f boot remove - f boot", "response": "def disable(name, **kwargs):\n    '''\n    Disable the named service to start at boot\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.disable <service name>\n    '''\n    osmajor = _osrel()[0]\n    if osmajor < '6':\n        cmd = 'update-rc.d -f {0} remove'.format(name)\n    else:\n        cmd = 'update-rc.d {0} disable'.format(name)\n    return not __salt__['cmd.retcode'](cmd)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_kwargs(profile=None, **connection_args):\n    '''\n    get connection args\n    '''\n    if profile:\n        prefix = profile + \":keystone.\"\n    else:\n        prefix = \"keystone.\"\n\n    def get(key, default=None):\n        '''\n        look in connection_args first, then default to config file\n        '''\n        return connection_args.get('connection_' + key,\n                                   __salt__['config.get'](prefix + key, default))\n\n    user = get('user', 'admin')\n    password = get('password', 'ADMIN')\n    tenant = get('tenant', 'admin')\n    tenant_id = get('tenant_id')\n    auth_url = get('auth_url', 'http://127.0.0.1:35357/v2.0/')\n    insecure = get('insecure', False)\n    token = get('token')\n    endpoint = get('endpoint', 'http://127.0.0.1:35357/v2.0')\n    user_domain_name = get('user_domain_name', 'Default')\n    project_domain_name = get('project_domain_name', 'Default')\n    if token:\n        kwargs = {'token': token,\n                  'endpoint': endpoint}\n    else:\n        kwargs = {'username': user,\n                  'password': password,\n                  'tenant_name': tenant,\n                  'tenant_id': tenant_id,\n                  'auth_url': auth_url,\n                  'user_domain_name': user_domain_name,\n                  'project_domain_name': project_domain_name}\n        # 'insecure' keyword not supported by all v2.0 keystone clients\n        #   this ensures it's only passed in when defined\n        if insecure:\n            kwargs['insecure'] = True\n    return kwargs", "response": "get kwargs for keystone client"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef api_version(profile=None, **connection_args):\n    '''\n    Returns the API version derived from endpoint's response.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' keystone.api_version\n    '''\n    kwargs = _get_kwargs(profile=profile, **connection_args)\n    auth_url = kwargs.get('auth_url', kwargs.get('endpoint', None))\n    try:\n        return salt.utils.http.query(auth_url, decode=True, decode_type='json',\n                                     verify_ssl=False)['dict']['version']['id']\n    except KeyError:\n        return None", "response": "Returns the API version derived from endpoint s response."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef auth(profile=None, **connection_args):\n    '''\n    Set up keystone credentials. Only intended to be used within Keystone-enabled modules.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' keystone.auth\n    '''\n    __utils__['versions.warn_until'](\n        'Neon',\n        (\n            'The keystone module has been deprecated and will be removed in {version}.  '\n            'Please update to using the keystoneng module'\n        ),\n    )\n    kwargs = _get_kwargs(profile=profile, **connection_args)\n\n    disc = discover.Discover(auth_url=kwargs['auth_url'])\n    v2_auth_url = disc.url_for('v2.0')\n    v3_auth_url = disc.url_for('v3.0')\n    if v3_auth_url:\n        global _OS_IDENTITY_API_VERSION\n        global _TENANTS\n        _OS_IDENTITY_API_VERSION = 3\n        _TENANTS = 'projects'\n        kwargs['auth_url'] = v3_auth_url\n    else:\n        kwargs['auth_url'] = v2_auth_url\n        kwargs.pop('user_domain_name')\n        kwargs.pop('project_domain_name')\n    auth = generic.Password(**kwargs)\n    sess = session.Session(auth=auth)\n    ks_cl = disc.create_client(session=sess)\n    return ks_cl", "response": "Set up keystone credentials. Only intended to be used within Keystone - enabled modules."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates EC2 - compatible credentials for user per tenant", "response": "def ec2_credentials_create(user_id=None, name=None,\n                           tenant_id=None, tenant=None,\n                           profile=None, **connection_args):\n    '''\n    Create EC2-compatible credentials for user per tenant\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' keystone.ec2_credentials_create name=admin tenant=admin\n\n        salt '*' keystone.ec2_credentials_create \\\n        user_id=c965f79c4f864eaaa9c3b41904e67082 \\\n        tenant_id=722787eb540849158668370dc627ec5f\n    '''\n    kstone = auth(profile, **connection_args)\n\n    if name:\n        user_id = user_get(name=name, profile=profile,\n                           **connection_args)[name]['id']\n    if not user_id:\n        return {'Error': 'Could not resolve User ID'}\n\n    if tenant:\n        tenant_id = tenant_get(name=tenant, profile=profile,\n                               **connection_args)[tenant]['id']\n    if not tenant_id:\n        return {'Error': 'Could not resolve Tenant ID'}\n\n    newec2 = kstone.ec2.create(user_id, tenant_id)\n    return {'access': newec2.access,\n            'secret': newec2.secret,\n            'tenant_id': newec2.tenant_id,\n            'user_id': newec2.user_id}"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndeleting EC2 - compatible credentials", "response": "def ec2_credentials_delete(user_id=None, name=None, access_key=None,\n                           profile=None, **connection_args):\n    '''\n    Delete EC2-compatible credentials\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' keystone.ec2_credentials_delete \\\n        860f8c2c38ca4fab989f9bc56a061a64 access_key=5f66d2f24f604b8bb9cd28886106f442\n\n        salt '*' keystone.ec2_credentials_delete name=admin \\\n        access_key=5f66d2f24f604b8bb9cd28886106f442\n    '''\n    kstone = auth(profile, **connection_args)\n\n    if name:\n        user_id = user_get(name=name, profile=None, **connection_args)[name]['id']\n    if not user_id:\n        return {'Error': 'Could not resolve User ID'}\n    kstone.ec2.delete(user_id, access_key)\n    return 'ec2 key \"{0}\" deleted under user id \"{1}\"'.format(access_key,\n                                                              user_id)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting ec2 credentials for a user", "response": "def ec2_credentials_get(user_id=None, name=None, access=None,\n                        profile=None, **connection_args):\n    '''\n    Return ec2_credentials for a user (keystone ec2-credentials-get)\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' keystone.ec2_credentials_get c965f79c4f864eaaa9c3b41904e67082 access=722787eb540849158668370\n        salt '*' keystone.ec2_credentials_get user_id=c965f79c4f864eaaa9c3b41904e67082 access=722787eb540849158668370\n        salt '*' keystone.ec2_credentials_get name=nova access=722787eb540849158668370dc627ec5f\n    '''\n    kstone = auth(profile, **connection_args)\n    ret = {}\n    if name:\n        for user in kstone.users.list():\n            if user.name == name:\n                user_id = user.id\n                break\n    if not user_id:\n        return {'Error': 'Unable to resolve user id'}\n    if not access:\n        return {'Error': 'Access key is required'}\n    ec2_credentials = kstone.ec2.get(user_id=user_id, access=access,\n                                     profile=profile, **connection_args)\n    ret[ec2_credentials.user_id] = {'user_id': ec2_credentials.user_id,\n                                    'tenant': ec2_credentials.tenant_id,\n                                    'access': ec2_credentials.access,\n                                    'secret': ec2_credentials.secret}\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list of ec2 credentials for a specific user", "response": "def ec2_credentials_list(user_id=None, name=None, profile=None,\n                         **connection_args):\n    '''\n    Return a list of ec2_credentials for a specific user (keystone ec2-credentials-list)\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' keystone.ec2_credentials_list 298ce377245c4ec9b70e1c639c89e654\n        salt '*' keystone.ec2_credentials_list user_id=298ce377245c4ec9b70e1c639c89e654\n        salt '*' keystone.ec2_credentials_list name=jack\n    '''\n    kstone = auth(profile, **connection_args)\n    ret = {}\n    if name:\n        for user in kstone.users.list():\n            if user.name == name:\n                user_id = user.id\n                break\n    if not user_id:\n        return {'Error': 'Unable to resolve user id'}\n    for ec2_credential in kstone.ec2.list(user_id):\n        ret[ec2_credential.user_id] = {'user_id': ec2_credential.user_id,\n                                       'tenant_id': ec2_credential.tenant_id,\n                                       'access': ec2_credential.access,\n                                       'secret': ec2_credential.secret}\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef endpoint_get(service, region=None, profile=None, interface=None, **connection_args):\n    '''\n    Return a specific endpoint (keystone endpoint-get)\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt 'v2' keystone.endpoint_get nova [region=RegionOne]\n\n        salt 'v3' keystone.endpoint_get nova interface=admin [region=RegionOne]\n    '''\n    auth(profile, **connection_args)\n    services = service_list(profile, **connection_args)\n    if service not in services:\n        return {'Error': 'Could not find the specified service'}\n    service_id = services[service]['id']\n    endpoints = endpoint_list(profile, **connection_args)\n\n    e = [_f for _f in [e\n          if e['service_id'] == service_id and\n            (e['region'] == region if region else True) and\n            (e['interface'] == interface if interface else True)\n            else None for e in endpoints.values()] if _f]\n    if len(e) > 1:\n        return {'Error': 'Multiple endpoints found ({0}) for the {1} service. Please specify region.'.format(e, service)}\n    if len(e) == 1:\n        return e[0]\n    return {'Error': 'Could not find endpoint for the specified service'}", "response": "Return a specific endpoint for a specific service"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef endpoint_list(profile=None, **connection_args):\n    '''\n    Return a list of available endpoints (keystone endpoints-list)\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' keystone.endpoint_list\n    '''\n    kstone = auth(profile, **connection_args)\n    ret = {}\n\n    for endpoint in kstone.endpoints.list():\n        ret[endpoint.id] = dict((value, getattr(endpoint, value)) for value in dir(endpoint)\n                                if not value.startswith('_') and\n                                isinstance(getattr(endpoint, value), (six.string_types, dict, bool)))\n    return ret", "response": "Return a list of available endpoints"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef endpoint_create(service, publicurl=None, internalurl=None, adminurl=None,\n                    region=None, profile=None, url=None, interface=None, **connection_args):\n    '''\n    Create an endpoint for an Openstack service\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt 'v2' keystone.endpoint_create nova 'http://public/url' 'http://internal/url' 'http://adminurl/url' region\n\n        salt 'v3' keystone.endpoint_create nova url='http://public/url' interface='public' region='RegionOne'\n    '''\n    kstone = auth(profile, **connection_args)\n    keystone_service = service_get(name=service, profile=profile,\n                                   **connection_args)\n    if not keystone_service or 'Error' in keystone_service:\n        return {'Error': 'Could not find the specified service'}\n\n    if _OS_IDENTITY_API_VERSION > 2:\n        kstone.endpoints.create(service=keystone_service[service]['id'],\n                                region_id=region,\n                                url=url,\n                                interface=interface)\n    else:\n        kstone.endpoints.create(region=region,\n                                service_id=keystone_service[service]['id'],\n                                publicurl=publicurl,\n                                adminurl=adminurl,\n                                internalurl=internalurl)\n    return endpoint_get(service, region, profile, interface, **connection_args)", "response": "Create an endpoint for an Openstack service"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef endpoint_delete(service, region=None, profile=None, interface=None, **connection_args):\n    '''\n    Delete endpoints of an Openstack service\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt 'v2' keystone.endpoint_delete nova [region=RegionOne]\n\n        salt 'v3' keystone.endpoint_delete nova interface=admin [region=RegionOne]\n    '''\n    kstone = auth(profile, **connection_args)\n    endpoint = endpoint_get(service, region, profile, interface, **connection_args)\n    if not endpoint or 'Error' in endpoint:\n        return {'Error': 'Could not find any endpoints for the service'}\n    kstone.endpoints.delete(endpoint['id'])\n    endpoint = endpoint_get(service, region, profile, interface, **connection_args)\n    if not endpoint or 'Error' in endpoint:\n        return True", "response": "Delete endpoints of an Openstack service"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a named role.", "response": "def role_create(name, profile=None, **connection_args):\n    '''\n    Create a named role.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' keystone.role_create admin\n    '''\n\n    kstone = auth(profile, **connection_args)\n    if 'Error' not in role_get(name=name, profile=profile, **connection_args):\n        return {'Error': 'Role \"{0}\" already exists'.format(name)}\n    kstone.roles.create(name)\n    return role_get(name=name, profile=profile, **connection_args)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef role_delete(role_id=None, name=None, profile=None,\n                **connection_args):\n    '''\n    Delete a role (keystone role-delete)\n\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' keystone.role_delete c965f79c4f864eaaa9c3b41904e67082\n        salt '*' keystone.role_delete role_id=c965f79c4f864eaaa9c3b41904e67082\n        salt '*' keystone.role_delete name=admin\n    '''\n    kstone = auth(profile, **connection_args)\n\n    if name:\n        for role in kstone.roles.list():\n            if role.name == name:\n                role_id = role.id\n                break\n    if not role_id:\n        return {'Error': 'Unable to resolve role id'}\n\n    role = kstone.roles.get(role_id)\n    kstone.roles.delete(role)\n\n    ret = 'Role ID {0} deleted'.format(role_id)\n    if name:\n        ret += ' ({0})'.format(name)\n    return ret", "response": "Delete a role from a keystone account"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef role_get(role_id=None, name=None, profile=None, **connection_args):\n    '''\n    Return a specific roles (keystone role-get)\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' keystone.role_get c965f79c4f864eaaa9c3b41904e67082\n        salt '*' keystone.role_get role_id=c965f79c4f864eaaa9c3b41904e67082\n        salt '*' keystone.role_get name=nova\n    '''\n    kstone = auth(profile, **connection_args)\n    ret = {}\n    if name:\n        for role in kstone.roles.list():\n            if role.name == name:\n                role_id = role.id\n                break\n    if not role_id:\n        return {'Error': 'Unable to resolve role id'}\n    role = kstone.roles.get(role_id)\n\n    ret[role.name] = {'id': role.id,\n                      'name': role.name}\n    return ret", "response": "Return a specific roles"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of available roles", "response": "def role_list(profile=None, **connection_args):\n    '''\n    Return a list of available roles (keystone role-list)\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' keystone.role_list\n    '''\n    kstone = auth(profile, **connection_args)\n    ret = {}\n    for role in kstone.roles.list():\n        ret[role.name] = dict((value, getattr(role, value)) for value in dir(role)\n                              if not value.startswith('_') and\n                              isinstance(getattr(role, value), (six.string_types, dict, bool)))\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef service_create(name, service_type, description=None, profile=None,\n                   **connection_args):\n    '''\n    Add service to Keystone service catalog\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' keystone.service_create nova compute \\\n'OpenStack Compute Service'\n    '''\n    kstone = auth(profile, **connection_args)\n    service = kstone.services.create(name, service_type, description=description)\n    return service_get(service.id, profile=profile, **connection_args)", "response": "Add a service to Keystone service catalog\nTaxonomy"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndelete a service from Keystone service catalog", "response": "def service_delete(service_id=None, name=None, profile=None, **connection_args):\n    '''\n    Delete a service from Keystone service catalog\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' keystone.service_delete c965f79c4f864eaaa9c3b41904e67082\n        salt '*' keystone.service_delete name=nova\n    '''\n    kstone = auth(profile, **connection_args)\n    if name:\n        service_id = service_get(name=name, profile=profile,\n                                 **connection_args)[name]['id']\n    kstone.services.delete(service_id)\n    return 'Keystone service ID \"{0}\" deleted'.format(service_id)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a specific services", "response": "def service_get(service_id=None, name=None, profile=None, **connection_args):\n    '''\n    Return a specific services (keystone service-get)\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' keystone.service_get c965f79c4f864eaaa9c3b41904e67082\n        salt '*' keystone.service_get service_id=c965f79c4f864eaaa9c3b41904e67082\n        salt '*' keystone.service_get name=nova\n    '''\n    kstone = auth(profile, **connection_args)\n    ret = {}\n    if name:\n        for service in kstone.services.list():\n            if service.name == name:\n                service_id = service.id\n                break\n    if not service_id:\n        return {'Error': 'Unable to resolve service id'}\n    service = kstone.services.get(service_id)\n    ret[service.name] = dict((value, getattr(service, value)) for value in dir(service)\n                             if not value.startswith('_') and\n                             isinstance(getattr(service, value), (six.string_types, dict, bool)))\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef service_list(profile=None, **connection_args):\n    '''\n    Return a list of available services (keystone services-list)\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' keystone.service_list\n    '''\n    kstone = auth(profile, **connection_args)\n    ret = {}\n    for service in kstone.services.list():\n        ret[service.name] = dict((value, getattr(service, value)) for value in dir(service)\n                                 if not value.startswith('_') and\n                                 isinstance(getattr(service, value), (six.string_types, dict, bool)))\n    return ret", "response": "Return a list of available services"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef tenant_create(name, description=None, enabled=True, profile=None,\n                  **connection_args):\n    '''\n    Create a keystone tenant\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' keystone.tenant_create nova description='nova tenant'\n        salt '*' keystone.tenant_create test enabled=False\n    '''\n    kstone = auth(profile, **connection_args)\n    new = getattr(kstone, _TENANTS, None).create(name, description, enabled)\n    return tenant_get(new.id, profile=profile, **connection_args)", "response": "Create a keystone tenant"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndeleting a tenant (keystone tenant-delete) CLI Examples: .. code-block:: bash salt '*' keystone.tenant_delete c965f79c4f864eaaa9c3b41904e67082 salt '*' keystone.tenant_delete tenant_id=c965f79c4f864eaaa9c3b41904e67082 salt '*' keystone.tenant_delete name=demo", "response": "def tenant_delete(tenant_id=None, name=None, profile=None, **connection_args):\n    '''\n    Delete a tenant (keystone tenant-delete)\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' keystone.tenant_delete c965f79c4f864eaaa9c3b41904e67082\n        salt '*' keystone.tenant_delete tenant_id=c965f79c4f864eaaa9c3b41904e67082\n        salt '*' keystone.tenant_delete name=demo\n    '''\n    kstone = auth(profile, **connection_args)\n    if name:\n        for tenant in getattr(kstone, _TENANTS, None).list():\n            if tenant.name == name:\n                tenant_id = tenant.id\n                break\n    if not tenant_id:\n        return {'Error': 'Unable to resolve tenant id'}\n    getattr(kstone, _TENANTS, None).delete(tenant_id)\n    ret = 'Tenant ID {0} deleted'.format(tenant_id)\n    if name:\n\n        ret += ' ({0})'.format(name)\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef project_delete(project_id=None, name=None, profile=None, **connection_args):\n    '''\n    Delete a project (keystone project-delete).\n    Overrides keystone tenant-delete form api V2. For keystone api V3 only.\n\n    .. versionadded:: 2016.11.0\n\n    project_id\n        The project id.\n\n    name\n        The project name.\n\n    profile\n        Configuration profile - if configuration for multiple openstack accounts required.\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' keystone.project_delete c965f79c4f864eaaa9c3b41904e67082\n        salt '*' keystone.project_delete project_id=c965f79c4f864eaaa9c3b41904e67082\n        salt '*' keystone.project_delete name=demo\n    '''\n    auth(profile, **connection_args)\n\n    if _OS_IDENTITY_API_VERSION > 2:\n        return tenant_delete(tenant_id=project_id, name=name, profile=None, **connection_args)\n    else:\n        return False", "response": "A wrapper for tenant_delete that uses keystone project - delete to delete a project."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting a specific tenant", "response": "def tenant_get(tenant_id=None, name=None, profile=None,\n               **connection_args):\n    '''\n    Return a specific tenants (keystone tenant-get)\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' keystone.tenant_get c965f79c4f864eaaa9c3b41904e67082\n        salt '*' keystone.tenant_get tenant_id=c965f79c4f864eaaa9c3b41904e67082\n        salt '*' keystone.tenant_get name=nova\n    '''\n    kstone = auth(profile, **connection_args)\n    ret = {}\n\n    if name:\n        for tenant in getattr(kstone, _TENANTS, None).list():\n            if tenant.name == name:\n                tenant_id = tenant.id\n                break\n    if not tenant_id:\n        return {'Error': 'Unable to resolve tenant id'}\n    tenant = getattr(kstone, _TENANTS, None).get(tenant_id)\n    ret[tenant.name] = dict((value, getattr(tenant, value)) for value in dir(tenant)\n                            if not value.startswith('_') and\n                            isinstance(getattr(tenant, value), (six.string_types, dict, bool)))\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef project_get(project_id=None, name=None, profile=None, **connection_args):\n    '''\n    Return a specific projects (keystone project-get)\n    Overrides keystone tenant-get form api V2.\n    For keystone api V3 only.\n\n    .. versionadded:: 2016.11.0\n\n    project_id\n        The project id.\n\n    name\n        The project name.\n\n    profile\n        Configuration profile - if configuration for multiple openstack accounts required.\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' keystone.project_get c965f79c4f864eaaa9c3b41904e67082\n        salt '*' keystone.project_get project_id=c965f79c4f864eaaa9c3b41904e67082\n        salt '*' keystone.project_get name=nova\n    '''\n    auth(profile, **connection_args)\n\n    if _OS_IDENTITY_API_VERSION > 2:\n        return tenant_get(tenant_id=project_id, name=name, profile=None, **connection_args)\n    else:\n        return False", "response": "Get a specific project"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef tenant_list(profile=None, **connection_args):\n    '''\n    Return a list of available tenants (keystone tenants-list)\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' keystone.tenant_list\n    '''\n    kstone = auth(profile, **connection_args)\n    ret = {}\n\n    for tenant in getattr(kstone, _TENANTS, None).list():\n        ret[tenant.name] = dict((value, getattr(tenant, value)) for value in dir(tenant)\n                                if not value.startswith('_') and\n                                isinstance(getattr(tenant, value), (six.string_types, dict, bool)))\n    return ret", "response": "Return a list of available tenants CLI Example : keystone tenants - list"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of available projects", "response": "def project_list(profile=None, **connection_args):\n    '''\n    Return a list of available projects (keystone projects-list).\n    Overrides keystone tenants-list form api V2.\n    For keystone api V3 only.\n\n    .. versionadded:: 2016.11.0\n\n    profile\n        Configuration profile - if configuration for multiple openstack accounts required.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' keystone.project_list\n    '''\n    auth(profile, **connection_args)\n\n    if _OS_IDENTITY_API_VERSION > 2:\n        return tenant_list(profile, **connection_args)\n    else:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef tenant_update(tenant_id=None, name=None, description=None,\n                  enabled=None, profile=None, **connection_args):\n    '''\n    Update a tenant's information (keystone tenant-update)\n    The following fields may be updated: name, description, enabled.\n    Can only update name if targeting by ID\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' keystone.tenant_update name=admin enabled=True\n        salt '*' keystone.tenant_update c965f79c4f864eaaa9c3b41904e67082 name=admin email=admin@domain.com\n    '''\n    kstone = auth(profile, **connection_args)\n\n    if not tenant_id:\n        for tenant in getattr(kstone, _TENANTS, None).list():\n            if tenant.name == name:\n                tenant_id = tenant.id\n                break\n    if not tenant_id:\n        return {'Error': 'Unable to resolve tenant id'}\n\n    tenant = getattr(kstone, _TENANTS, None).get(tenant_id)\n    if not name:\n        name = tenant.name\n    if not description:\n        description = tenant.description\n    if enabled is None:\n        enabled = tenant.enabled\n    updated = getattr(kstone, _TENANTS, None).update(tenant_id, name=name, description=description, enabled=enabled)\n\n    return dict((value, getattr(updated, value)) for value in dir(updated)\n                if not value.startswith('_') and\n                isinstance(getattr(updated, value), (six.string_types, dict, bool)))", "response": "Update a tenant s information"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate a tenant s information.", "response": "def project_update(project_id=None, name=None, description=None,\n                   enabled=None, profile=None, **connection_args):\n    '''\n    Update a tenant's information (keystone project-update)\n    The following fields may be updated: name, description, enabled.\n    Can only update name if targeting by ID\n\n    Overrides keystone tenant_update form api V2.\n    For keystone api V3 only.\n\n    .. versionadded:: 2016.11.0\n\n    project_id\n        The project id.\n\n    name\n        The project name, which must be unique within the owning domain.\n\n    description\n        The project description.\n\n    enabled\n        Enables or disables the project.\n\n    profile\n        Configuration profile - if configuration for multiple openstack accounts required.\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' keystone.project_update name=admin enabled=True\n        salt '*' keystone.project_update c965f79c4f864eaaa9c3b41904e67082 name=admin email=admin@domain.com\n    '''\n    auth(profile, **connection_args)\n\n    if _OS_IDENTITY_API_VERSION > 2:\n        return tenant_update(tenant_id=project_id, name=name, description=description,\n                             enabled=enabled, profile=profile, **connection_args)\n    else:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef token_get(profile=None, **connection_args):\n    '''\n    Return the configured tokens (keystone token-get)\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' keystone.token_get c965f79c4f864eaaa9c3b41904e67082\n    '''\n    kstone = auth(profile, **connection_args)\n    token = kstone.service_catalog.get_token()\n    return {'id': token['id'],\n            'expires': token['expires'],\n            'user_id': token['user_id'],\n            'tenant_id': token['tenant_id']}", "response": "Return the configured tokens ( keystone token - get"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef user_list(profile=None, **connection_args):\n    '''\n    Return a list of available users (keystone user-list)\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' keystone.user_list\n    '''\n    kstone = auth(profile, **connection_args)\n    ret = {}\n    for user in kstone.users.list():\n        ret[user.name] = dict((value, getattr(user, value, None)) for value in dir(user)\n                              if not value.startswith('_') and\n                              isinstance(getattr(user, value, None), (six.string_types, dict, bool)))\n        tenant_id = getattr(user, 'tenantId', None)\n        if tenant_id:\n            ret[user.name]['tenant_id'] = tenant_id\n    return ret", "response": "Return a list of available users"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets a specific user", "response": "def user_get(user_id=None, name=None, profile=None, **connection_args):\n    '''\n    Return a specific users (keystone user-get)\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' keystone.user_get c965f79c4f864eaaa9c3b41904e67082\n        salt '*' keystone.user_get user_id=c965f79c4f864eaaa9c3b41904e67082\n        salt '*' keystone.user_get name=nova\n    '''\n    kstone = auth(profile, **connection_args)\n    ret = {}\n    if name:\n        for user in kstone.users.list():\n            if user.name == name:\n                user_id = user.id\n                break\n    if not user_id:\n        return {'Error': 'Unable to resolve user id'}\n    try:\n        user = kstone.users.get(user_id)\n    except keystoneclient.exceptions.NotFound:\n        msg = 'Could not find user \\'{0}\\''.format(user_id)\n        log.error(msg)\n        return {'Error': msg}\n\n    ret[user.name] = dict((value, getattr(user, value, None)) for value in dir(user)\n                          if not value.startswith('_') and\n                          isinstance(getattr(user, value, None), (six.string_types, dict, bool)))\n\n    tenant_id = getattr(user, 'tenantId', None)\n    if tenant_id:\n        ret[user.name]['tenant_id'] = tenant_id\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef user_create(name, password, email, tenant_id=None,\n                enabled=True, profile=None, project_id=None, description=None, **connection_args):\n    '''\n    Create a user (keystone user-create)\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' keystone.user_create name=jack password=zero email=jack@halloweentown.org \\\n        tenant_id=a28a7b5a999a455f84b1f5210264375e enabled=True\n    '''\n    kstone = auth(profile, **connection_args)\n\n    if _OS_IDENTITY_API_VERSION > 2:\n        if tenant_id and not project_id:\n            project_id = tenant_id\n        item = kstone.users.create(name=name,\n                                   password=password,\n                                   email=email,\n                                   project_id=project_id,\n                                   enabled=enabled,\n                                   description=description)\n    else:\n        item = kstone.users.create(name=name,\n                                   password=password,\n                                   email=email,\n                                   tenant_id=tenant_id,\n                                   enabled=enabled)\n    return user_get(item.id, profile=profile, **connection_args)", "response": "Create a user in a keystone user store"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef user_delete(user_id=None, name=None, profile=None, **connection_args):\n    '''\n    Delete a user (keystone user-delete)\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' keystone.user_delete c965f79c4f864eaaa9c3b41904e67082\n        salt '*' keystone.user_delete user_id=c965f79c4f864eaaa9c3b41904e67082\n        salt '*' keystone.user_delete name=nova\n    '''\n    kstone = auth(profile, **connection_args)\n    if name:\n        for user in kstone.users.list():\n            if user.name == name:\n                user_id = user.id\n                break\n    if not user_id:\n        return {'Error': 'Unable to resolve user id'}\n    kstone.users.delete(user_id)\n    ret = 'User ID {0} deleted'.format(user_id)\n    if name:\n\n        ret += ' ({0})'.format(name)\n    return ret", "response": "Delete a user in a keystone user store"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef user_update(user_id=None, name=None, email=None, enabled=None,\n                tenant=None, profile=None, project=None, description=None, **connection_args):\n    '''\n    Update a user's information (keystone user-update)\n    The following fields may be updated: name, email, enabled, tenant.\n    Because the name is one of the fields, a valid user id is required.\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' keystone.user_update user_id=c965f79c4f864eaaa9c3b41904e67082 name=newname\n        salt '*' keystone.user_update c965f79c4f864eaaa9c3b41904e67082 name=newname email=newemail@domain.com\n    '''\n    kstone = auth(profile, **connection_args)\n    if not user_id:\n        for user in kstone.users.list():\n            if user.name == name:\n                user_id = user.id\n                break\n        if not user_id:\n            return {'Error': 'Unable to resolve user id'}\n    user = kstone.users.get(user_id)\n    # Keep previous settings if not updating them\n    if not name:\n        name = user.name\n    if not email:\n        email = user.email\n    if enabled is None:\n        enabled = user.enabled\n\n    if _OS_IDENTITY_API_VERSION > 2:\n        if description is None:\n            description = getattr(user, 'description', None)\n        else:\n            description = six.text_type(description)\n\n        project_id = None\n        if project:\n            for proj in kstone.projects.list():\n                if proj.name == project:\n                    project_id = proj.id\n                    break\n        if not project_id:\n            project_id = getattr(user, 'project_id', None)\n\n        kstone.users.update(user=user_id, name=name, email=email, enabled=enabled, description=description,\n                            project_id=project_id)\n    else:\n        kstone.users.update(user=user_id, name=name, email=email, enabled=enabled)\n\n        tenant_id = None\n        if tenant:\n            for tnt in kstone.tenants.list():\n                if tnt.name == tenant:\n                    tenant_id = tnt.id\n                    break\n            if tenant_id:\n                kstone.users.update_tenant(user_id, tenant_id)\n\n    ret = 'Info updated for user ID {0}'.format(user_id)\n    return ret", "response": "Update a user s information"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef user_verify_password(user_id=None, name=None, password=None,\n                         profile=None, **connection_args):\n    '''\n    Verify a user's password\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' keystone.user_verify_password name=test password=foobar\n        salt '*' keystone.user_verify_password user_id=c965f79c4f864eaaa9c3b41904e67082 password=foobar\n    '''\n    kstone = auth(profile, **connection_args)\n    if 'connection_endpoint' in connection_args:\n        auth_url = connection_args.get('connection_endpoint')\n    else:\n        auth_url_opt = 'keystone.auth_url'\n        if __salt__['config.option']('keystone.token'):\n            auth_url_opt = 'keystone.endpoint'\n\n        if _OS_IDENTITY_API_VERSION > 2:\n            auth_url = __salt__['config.option'](auth_url_opt,\n                                                 'http://127.0.0.1:35357/v3')\n        else:\n            auth_url = __salt__['config.option'](auth_url_opt,\n                                                 'http://127.0.0.1:35357/v2.0')\n\n    if user_id:\n        for user in kstone.users.list():\n            if user.id == user_id:\n                name = user.name\n                break\n    if not name:\n        return {'Error': 'Unable to resolve user name'}\n    kwargs = {'username': name,\n              'password': password,\n              'auth_url': auth_url}\n    try:\n        if _OS_IDENTITY_API_VERSION > 2:\n            client3.Client(**kwargs)\n        else:\n            client.Client(**kwargs)\n    except (keystoneclient.exceptions.Unauthorized,\n            keystoneclient.exceptions.AuthorizationFailure):\n        return False\n    return True", "response": "Verify a user s password"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef user_password_update(user_id=None, name=None, password=None,\n                         profile=None, **connection_args):\n    '''\n    Update a user's password (keystone user-password-update)\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' keystone.user_password_update c965f79c4f864eaaa9c3b41904e67082 password=12345\n        salt '*' keystone.user_password_update user_id=c965f79c4f864eaaa9c3b41904e67082 password=12345\n        salt '*' keystone.user_password_update name=nova password=12345\n    '''\n    kstone = auth(profile, **connection_args)\n    if name:\n        for user in kstone.users.list():\n            if user.name == name:\n                user_id = user.id\n                break\n    if not user_id:\n        return {'Error': 'Unable to resolve user id'}\n\n    if _OS_IDENTITY_API_VERSION > 2:\n        kstone.users.update(user=user_id, password=password)\n    else:\n        kstone.users.update_password(user=user_id, password=password)\n    ret = 'Password updated for user ID {0}'.format(user_id)\n    if name:\n        ret += ' ({0})'.format(name)\n    return ret", "response": "Update a user s password"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds user to tenant and role", "response": "def user_role_add(user_id=None, user=None, tenant_id=None,\n                  tenant=None, role_id=None, role=None, profile=None,\n                  project_id=None, project_name=None, **connection_args):\n    '''\n    Add role for user in tenant (keystone user-role-add)\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' keystone.user_role_add \\\nuser_id=298ce377245c4ec9b70e1c639c89e654 \\\ntenant_id=7167a092ece84bae8cead4bf9d15bb3b \\\nrole_id=ce377245c4ec9b70e1c639c89e8cead4\n        salt '*' keystone.user_role_add user=admin tenant=admin role=admin\n    '''\n    kstone = auth(profile, **connection_args)\n\n    if project_id and not tenant_id:\n        tenant_id = project_id\n    elif project_name and not tenant:\n        tenant = project_name\n\n    if user:\n        user_id = user_get(name=user, profile=profile,\n                           **connection_args)[user].get('id')\n    else:\n        user = next(six.iterkeys(user_get(user_id, profile=profile,\n                                          **connection_args)))['name']\n    if not user_id:\n        return {'Error': 'Unable to resolve user id'}\n\n    if tenant:\n        tenant_id = tenant_get(name=tenant, profile=profile,\n                               **connection_args)[tenant].get('id')\n    else:\n        tenant = next(six.iterkeys(tenant_get(tenant_id, profile=profile,\n                                              **connection_args)))['name']\n    if not tenant_id:\n        return {'Error': 'Unable to resolve tenant/project id'}\n\n    if role:\n        role_id = role_get(name=role, profile=profile,\n                           **connection_args)[role]['id']\n    else:\n        role = next(six.iterkeys(role_get(role_id, profile=profile,\n                                          **connection_args)))['name']\n    if not role_id:\n        return {'Error': 'Unable to resolve role id'}\n\n    if _OS_IDENTITY_API_VERSION > 2:\n        kstone.roles.grant(role_id, user=user_id, project=tenant_id)\n    else:\n        kstone.roles.add_user_role(user_id, role_id, tenant_id)\n    ret_msg = '\"{0}\" role added for user \"{1}\" for \"{2}\" tenant/project'\n    return ret_msg.format(role, user, tenant)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of available user roles", "response": "def user_role_list(user_id=None, tenant_id=None, user_name=None,\n                   tenant_name=None, profile=None, project_id=None, project_name=None, **connection_args):\n    '''\n    Return a list of available user_roles (keystone user-roles-list)\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' keystone.user_role_list \\\nuser_id=298ce377245c4ec9b70e1c639c89e654 \\\ntenant_id=7167a092ece84bae8cead4bf9d15bb3b\n        salt '*' keystone.user_role_list user_name=admin tenant_name=admin\n    '''\n    kstone = auth(profile, **connection_args)\n    ret = {}\n\n    if project_id and not tenant_id:\n        tenant_id = project_id\n    elif project_name and not tenant_name:\n        tenant_name = project_name\n\n    if user_name:\n        for user in kstone.users.list():\n            if user.name == user_name:\n                user_id = user.id\n                break\n    if tenant_name:\n        for tenant in getattr(kstone, _TENANTS, None).list():\n            if tenant.name == tenant_name:\n                tenant_id = tenant.id\n                break\n    if not user_id or not tenant_id:\n        return {'Error': 'Unable to resolve user or tenant/project id'}\n\n    if _OS_IDENTITY_API_VERSION > 2:\n        for role in kstone.roles.list(user=user_id, project=tenant_id):\n            ret[role.name] = dict((value, getattr(role, value)) for value in dir(role)\n                                  if not value.startswith('_') and\n                                  isinstance(getattr(role, value), (six.string_types, dict, bool)))\n    else:\n        for role in kstone.roles.roles_for_user(user=user_id, tenant=tenant_id):\n            ret[role.name] = {'id': role.id,\n                              'name': role.name,\n                              'user_id': user_id,\n                              'tenant_id': tenant_id}\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of available items", "response": "def _item_list(profile=None, **connection_args):\n    '''\n    Template for writing list functions\n    Return a list of available items (keystone items-list)\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' keystone.item_list\n    '''\n    kstone = auth(profile, **connection_args)\n    ret = []\n    for item in kstone.items.list():\n        ret.append(item.__dict__)\n        # ret[item.name] = {\n        #         'id': item.id,\n        #         'name': item.name,\n        #         }\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the path to the. ssh config file - usually <home >. ssh", "response": "def _get_ssh_config_file(opts):\n    '''\n    :return: Path to the .ssh/config file - usually <home>/.ssh/config\n    '''\n    ssh_config_file = opts.get('ssh_config_file')\n    if not os.path.isfile(ssh_config_file):\n        raise IOError('Cannot find SSH config file')\n    if not os.access(ssh_config_file, os.R_OK):\n        raise IOError('Cannot access SSH config file: {}'.format(ssh_config_file))\n    return ssh_config_file"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse the SSH config file to create roster targets.", "response": "def parse_ssh_config(lines):\n    '''\n    Parses lines from the SSH config to create roster targets.\n\n    :param lines: Individual lines from the ssh config file\n    :return: Dictionary of targets in similar style to the flat roster\n    '''\n    # transform the list of individual lines into a list of sublists where each\n    # sublist represents a single Host definition\n    hosts = []\n    for line in lines:\n        line = salt.utils.stringutils.to_unicode(line)\n        if not line or line.startswith('#'):\n            continue\n        elif line.startswith('Host '):\n            hosts.append([])\n        hosts[-1].append(line)\n\n    # construct a dictionary of Host names to mapped roster properties\n    targets = collections.OrderedDict()\n    for host_data in hosts:\n        target = collections.OrderedDict()\n        hostnames = host_data[0].split()[1:]\n        for line in host_data[1:]:\n            for field in _ROSTER_FIELDS:\n                match = re.match(field.pattern, line)\n                if match:\n                    target[field.target_field] = match.group(1)\n        for hostname in hostnames:\n            targets[hostname] = target\n\n    # apply matching for glob hosts\n    wildcard_targets = []\n    non_wildcard_targets = []\n    for target in targets.keys():\n        if '*' in target or '?' in target:\n            wildcard_targets.append(target)\n        else:\n            non_wildcard_targets.append(target)\n    for pattern in wildcard_targets:\n        for candidate in non_wildcard_targets:\n            if fnmatch.fnmatch(candidate, pattern):\n                targets[candidate].update(targets[pattern])\n        del targets[pattern]\n\n    # finally, update the 'host' to refer to its declaration in the SSH config\n    # so that its connection parameters can be utilized\n    for target in targets:\n        targets[target]['host'] = target\n    return targets"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef targets(tgt, tgt_type='glob', **kwargs):\n    '''\n    Return the targets from the flat yaml file, checks opts for location but\n    defaults to /etc/salt/roster\n    '''\n    ssh_config_file = _get_ssh_config_file(__opts__)\n    with salt.utils.files.fopen(ssh_config_file, 'r') as fp:\n        all_minions = parse_ssh_config([line.rstrip() for line in fp])\n    rmatcher = RosterMatcher(all_minions, tgt, tgt_type)\n    matched = rmatcher.targets()\n    return matched", "response": "Return the targets from the flat yaml file"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ret_glob_minions(self):\n        '''\n        Return minions that match via glob\n        '''\n        minions = {}\n        for minion in self.raw:\n            if fnmatch.fnmatch(minion, self.tgt):\n                data = self.get_data(minion)\n                if data:\n                    minions[minion] = data\n        return minions", "response": "Return minions that match via glob\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_data(self, minion):\n        '''\n        Return the configured ip\n        '''\n        if isinstance(self.raw[minion], six.string_types):\n            return {'host': self.raw[minion]}\n        if isinstance(self.raw[minion], dict):\n            return self.raw[minion]\n        return False", "response": "Return the configured ip\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\npack this exception into a serializable dictionary that is safe for msgpack transport via msgpack. msgpack.", "response": "def pack(self):\n        '''\n        Pack this exception into a serializable dictionary that is safe for\n        transport via msgpack\n        '''\n        if six.PY3:\n            return {'message': six.text_type(self), 'args': self.args}\n        return dict(message=self.__unicode__(), args=self.args)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef reactivate(domain_name):\n    '''\n    Try to reactivate the expired domain name\n\n    Returns the following information:\n\n    - Whether or not the domain was reactivated successfully\n    - The amount charged for reactivation\n    - The order ID\n    - The transaction ID\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt 'my-minion' namecheap_domains.reactivate my-domain-name\n    '''\n    opts = salt.utils.namecheap.get_opts('namecheap.domains.reactivate')\n    opts['DomainName'] = domain_name\n\n    response_xml = salt.utils.namecheap.post_request(opts)\n\n    if response_xml is None:\n        return {}\n\n    domainreactivateresult = response_xml.getElementsByTagName('DomainReactivateResult')[0]\n    return salt.utils.namecheap.xml_to_dict(domainreactivateresult)", "response": "Try to reactivate the expired domain name\n    Returns the following information"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntry to renew the specified expiring domain name for a specified number of years and promotion code", "response": "def renew(domain_name, years, promotion_code=None):\n    '''\n    Try to renew the specified expiring domain name for a specified number of years\n\n    domain_name\n        The domain name to be renewed\n\n    years\n        Number of years to renew\n\n    Returns the following information:\n\n    - Whether or not the domain was renewed successfully\n    - The domain ID\n    - The order ID\n    - The transaction ID\n    - The amount charged for renewal\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt 'my-minion' namecheap_domains.renew my-domain-name 5\n    '''\n\n    opts = salt.utils.namecheap.get_opts('namecheap.domains.renew')\n    opts['DomainName'] = domain_name\n    opts['Years'] = years\n    if promotion_code is not None:\n        opts['PromotionCode'] = promotion_code\n\n    response_xml = salt.utils.namecheap.post_request(opts)\n\n    if response_xml is None:\n        return {}\n\n    domainrenewresult = response_xml.getElementsByTagName(\"DomainRenewResult\")[0]\n    return salt.utils.namecheap.xml_to_dict(domainrenewresult)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create(domain_name, years, **kwargs):\n    '''\n    Try to register the specified domain name\n\n    domain_name\n        The domain name to be registered\n\n    years\n        Number of years to register\n\n    Returns the following information:\n\n    - Whether or not the domain was renewed successfully\n    - Whether or not WhoisGuard is enabled\n    - Whether or not registration is instant\n    - The amount charged for registration\n    - The domain ID\n    - The order ID\n    - The transaction ID\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt 'my-minion' namecheap_domains.create my-domain-name 2\n    '''\n    idn_codes = ('afr', 'alb', 'ara', 'arg', 'arm', 'asm', 'ast', 'ave', 'awa', 'aze', 'bak', 'bal', 'ban', 'baq',\n                 'bas', 'bel', 'ben', 'bho', 'bos', 'bul', 'bur', 'car', 'cat', 'che', 'chi', 'chv', 'cop', 'cos',\n                 'cze', 'dan', 'div', 'doi', 'dut', 'eng', 'est', 'fao', 'fij', 'fin', 'fre', 'fry', 'geo', 'ger',\n                 'gla', 'gle', 'gon', 'gre', 'guj', 'heb', 'hin', 'hun', 'inc', 'ind', 'inh', 'isl', 'ita', 'jav',\n                 'jpn', 'kas', 'kaz', 'khm', 'kir', 'kor', 'kur', 'lao', 'lav', 'lit', 'ltz', 'mal', 'mkd', 'mlt',\n                 'mol', 'mon', 'mri', 'msa', 'nep', 'nor', 'ori', 'oss', 'pan', 'per', 'pol', 'por', 'pus', 'raj',\n                 'rum', 'rus', 'san', 'scr', 'sin', 'slo', 'slv', 'smo', 'snd', 'som', 'spa', 'srd', 'srp', 'swa',\n                 'swe', 'syr', 'tam', 'tel', 'tgk', 'tha', 'tib', 'tur', 'ukr', 'urd', 'uzb', 'vie', 'wel', 'yid')\n\n    require_opts = ['AdminAddress1', 'AdminCity', 'AdminCountry', 'AdminEmailAddress', 'AdminFirstName',\n                    'AdminLastName', 'AdminPhone', 'AdminPostalCode', 'AdminStateProvince', 'AuxBillingAddress1',\n                    'AuxBillingCity', 'AuxBillingCountry', 'AuxBillingEmailAddress', 'AuxBillingFirstName',\n                    'AuxBillingLastName', 'AuxBillingPhone', 'AuxBillingPostalCode', 'AuxBillingStateProvince',\n                    'RegistrantAddress1', 'RegistrantCity', 'RegistrantCountry', 'RegistrantEmailAddress',\n                    'RegistrantFirstName', 'RegistrantLastName', 'RegistrantPhone', 'RegistrantPostalCode',\n                    'RegistrantStateProvince', 'TechAddress1', 'TechCity', 'TechCountry', 'TechEmailAddress',\n                    'TechFirstName', 'TechLastName', 'TechPhone', 'TechPostalCode', 'TechStateProvince', 'Years']\n    opts = salt.utils.namecheap.get_opts('namecheap.domains.create')\n    opts['DomainName'] = domain_name\n    opts['Years'] = six.text_type(years)\n\n    def add_to_opts(opts_dict, kwargs, value, suffix, prefices):\n        for prefix in prefices:\n            nextkey = prefix + suffix\n            if nextkey not in kwargs:\n                opts_dict[nextkey] = value\n\n    for key, value in six.iteritems(kwargs):\n        if key.startswith('Registrant'):\n            add_to_opts(opts, kwargs, value, key[10:], ['Tech', 'Admin', 'AuxBilling', 'Billing'])\n\n        if key.startswith('Tech'):\n            add_to_opts(opts, kwargs, value, key[4:], ['Registrant', 'Admin', 'AuxBilling', 'Billing'])\n\n        if key.startswith('Admin'):\n            add_to_opts(opts, kwargs, value, key[5:], ['Registrant', 'Tech', 'AuxBilling', 'Billing'])\n\n        if key.startswith('AuxBilling'):\n            add_to_opts(opts, kwargs, value, key[10:], ['Registrant', 'Tech', 'Admin', 'Billing'])\n\n        if key.startswith('Billing'):\n            add_to_opts(opts, kwargs, value, key[7:], ['Registrant', 'Tech', 'Admin', 'AuxBilling'])\n\n        if key == 'IdnCode' and key not in idn_codes:\n            log.error('Invalid IdnCode')\n            raise Exception('Invalid IdnCode')\n\n        opts[key] = value\n\n    for requiredkey in require_opts:\n        if requiredkey not in opts:\n            log.error('Missing required parameter \\'%s\\'', requiredkey)\n            raise Exception('Missing required parameter \\'{0}\\''.format(requiredkey))\n\n    response_xml = salt.utils.namecheap.post_request(opts)\n\n    if response_xml is None:\n        return {}\n\n    domainresult = response_xml.getElementsByTagName(\"DomainCreateResult\")[0]\n    return salt.utils.namecheap.atts_to_dict(domainresult)", "response": "Create a new domain name and year."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks availability of domains domains_to_check array of strings", "response": "def check(*domains_to_check):\n    '''\n    Checks the availability of domains\n\n    domains_to_check\n        array of strings  List of domains to check\n\n    Returns a dictionary mapping the each domain name to a boolean denoting\n    whether or not it is available.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt 'my-minion' namecheap_domains.check domain-to-check\n    '''\n    opts = salt.utils.namecheap.get_opts('namecheap.domains.check')\n    opts['DomainList'] = ','.join(domains_to_check)\n\n    response_xml = salt.utils.namecheap.get_request(opts)\n\n    if response_xml is None:\n        return {}\n\n    domains_checked = {}\n    for result in response_xml.getElementsByTagName(\"DomainCheckResult\"):\n        available = result.getAttribute(\"Available\")\n        domains_checked[result.getAttribute(\"Domain\").lower()] = salt.utils.namecheap.string_to_value(available)\n\n    return domains_checked"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_info(domain_name):\n    '''\n    Returns information about the requested domain\n\n    returns a dictionary of information about the domain_name\n\n    domain_name\n        string  Domain name to get information about\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt 'my-minion' namecheap_domains.get_info my-domain-name\n    '''\n    opts = salt.utils.namecheap.get_opts('namecheap.domains.getinfo')\n    opts['DomainName'] = domain_name\n\n    response_xml = salt.utils.namecheap.get_request(opts)\n\n    if response_xml is None:\n        return []\n\n    domaingetinforesult = response_xml.getElementsByTagName(\"DomainGetInfoResult\")[0]\n\n    return salt.utils.namecheap.xml_to_dict(domaingetinforesult)", "response": "Returns information about the requested domain_name"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_tld_list():\n    '''\n    Returns a list of TLDs as objects\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt 'my-minion' namecheap_domains.get_tld_list\n    '''\n\n    response_xml = salt.utils.namecheap.get_request(salt.utils.namecheap.get_opts('namecheap.domains.gettldlist'))\n\n    if response_xml is None:\n        return []\n\n    tldresult = response_xml.getElementsByTagName(\"Tlds\")[0]\n    tlds = []\n\n    for e in tldresult.getElementsByTagName(\"Tld\"):\n        tld = salt.utils.namecheap.atts_to_dict(e)\n        tld['data'] = e.firstChild.data\n        categories = []\n        subcategories = e.getElementsByTagName(\"Categories\")[0]\n        for c in subcategories.getElementsByTagName(\"TldCategory\"):\n            categories.append(salt.utils.namecheap.atts_to_dict(c))\n        tld['categories'] = categories\n        tlds.append(tld)\n\n    return tlds", "response": "Returns a list of TLDs as objects\n    CLI Example : namecheap_domains. get_tld_list\n   "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_list(list_type=None,\n             search_term=None,\n             page=None,\n             page_size=None,\n             sort_by=None):\n    '''\n    Returns a list of domains for the particular user as a list of objects\n    offset by ``page`` length of ``page_size``\n\n    list_type : ALL\n        One of ``ALL``, ``EXPIRING``, ``EXPIRED``\n\n    search_term\n        Keyword to look for on the domain list\n\n    page : 1\n        Number of result page to return\n\n    page_size : 20\n        Number of domains to be listed per page (minimum: ``10``, maximum:\n        ``100``)\n\n    sort_by\n        One of ``NAME``, ``NAME_DESC``, ``EXPIREDATE``, ``EXPIREDATE_DESC``,\n        ``CREATEDATE``, or ``CREATEDATE_DESC``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt 'my-minion' namecheap_domains.get_list\n    '''\n    opts = salt.utils.namecheap.get_opts('namecheap.domains.getList')\n\n    if list_type is not None:\n        if list_type not in ['ALL', 'EXPIRING', 'EXPIRED']:\n            log.error('Invalid option for list_type')\n            raise Exception('Invalid option for list_type')\n        opts['ListType'] = list_type\n\n    if search_term is not None:\n        if len(search_term) > 70:\n            log.warning('search_term trimmed to first 70 characters')\n            search_term = search_term[0:70]\n        opts['SearchTerm'] = search_term\n\n    if page is not None:\n        opts['Page'] = page\n\n    if page_size is not None:\n        if page_size > 100 or page_size < 10:\n            log.error('Invalid option for page')\n            raise Exception('Invalid option for page')\n        opts['PageSize'] = page_size\n\n    if sort_by is not None:\n        if sort_by not in ['NAME', 'NAME_DESC', 'EXPIREDATE', 'EXPIREDATE_DESC', 'CREATEDATE', 'CREATEDATE_DESC']:\n            log.error('Invalid option for sort_by')\n            raise Exception('Invalid option for sort_by')\n        opts['SortBy'] = sort_by\n\n    response_xml = salt.utils.namecheap.get_request(opts)\n\n    if response_xml is None:\n        return []\n\n    domainresult = response_xml.getElementsByTagName(\"DomainGetListResult\")[0]\n\n    domains = []\n    for d in domainresult.getElementsByTagName(\"Domain\"):\n        domains.append(salt.utils.namecheap.atts_to_dict(d))\n\n    return domains", "response": "Returns a list of domains for the particular user"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the Pillar data from reclass for the given minion_id.", "response": "def ext_pillar(minion_id, pillar, **kwargs):\n    '''\n    Obtain the Pillar data from **reclass** for the given ``minion_id``.\n    '''\n\n    # If reclass is installed, __virtual__ put it onto the search path, so we\n    # don't need to protect against ImportError:\n    # pylint: disable=3rd-party-module-not-gated\n    from reclass.adapters.salt import ext_pillar as reclass_ext_pillar\n    from reclass.errors import ReclassException\n    # pylint: enable=3rd-party-module-not-gated\n\n    try:\n        # the source path we used above isn't something reclass needs to care\n        # about, so filter it:\n        filter_out_source_path_option(kwargs)\n\n        # if no inventory_base_uri was specified, initialize it to the first\n        # file_roots of class 'base' (if that exists):\n        set_inventory_base_uri_default(__opts__, kwargs)\n\n        # I purposely do not pass any of __opts__ or __salt__ or __grains__\n        # to reclass, as I consider those to be Salt-internal and reclass\n        # should not make any assumptions about it.\n        return reclass_ext_pillar(minion_id, pillar, **kwargs)\n\n    except TypeError as e:\n        if 'unexpected keyword argument' in six.text_type(e):\n            arg = six.text_type(e).split()[-1]\n            raise SaltInvocationError('ext_pillar.reclass: unexpected option: '\n                                      + arg)\n        else:\n            raise\n\n    except KeyError as e:\n        if 'id' in six.text_type(e):\n            raise SaltInvocationError('ext_pillar.reclass: __opts__ does not '\n                                      'define minion ID')\n        else:\n            raise\n\n    except ReclassException as e:\n        raise SaltInvocationError('ext_pillar.reclass: {0}'.format(e))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _set_status(m,\n                comment=INVALID_RESPONSE,\n                status=False,\n                out=None):\n    '''\n    Assign status data to a dict.\n    '''\n    m['out'] = out\n    m['status'] = status\n    m['logs'] = LOG.messages[:]\n    m['logs_by_level'] = LOG.by_level.copy()\n    outlog, outlog_by_level = '', ''\n    m['comment'] = comment\n    if out and isinstance(out, six.string_types):\n        outlog += HR\n        outlog += 'OUTPUT:\\n'\n        outlog += '{0}\\n'.format(_encode_string(out))\n        outlog += HR\n    if m['logs']:\n        outlog += HR\n        outlog += 'Log summary:\\n'\n        outlog += HR\n        outlog_by_level += HR\n        outlog_by_level += 'Log summary by level:\\n'\n        outlog_by_level += HR\n        for level, msg in m['logs']:\n            outlog += '\\n{0}: {1}\\n'.format(level.upper(),\n                                            _encode_string(msg))\n        for logger in 'error', 'warn', 'info', 'debug':\n            logs = m['logs_by_level'].get(logger, [])\n            if logs:\n                outlog_by_level += '\\n{0}:\\n'.format(logger.upper())\n                for idx, log in enumerate(logs[:]):\n                    logs[idx] = _encode_string(log)\n                outlog_by_level += '\\n'.join(logs)\n                outlog_by_level += '\\n'\n        outlog += HR\n    m['outlog'] = outlog\n    m['outlog_by_level'] = outlog_by_level\n    return _encode_status(m)", "response": "Set status data to a dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _Popen(command,\n           output=False,\n           directory='.',\n           runas=None,\n           env=(),\n           exitcode=0,\n           use_vt=False,\n           loglevel=None):\n    '''\n    Run a command.\n\n    output\n        return output if true\n\n    directory\n        directory to execute in\n\n    runas\n        user used to run buildout as\n\n    env\n        environment variables to set when running\n\n    exitcode\n        fails if cmd does not return this exit code\n        (set to None to disable check)\n\n    use_vt\n        Use the new salt VT to stream output [experimental]\n\n    '''\n    ret = None\n    directory = os.path.abspath(directory)\n    if isinstance(command, list):\n        command = ' '.join(command)\n    LOG.debug('Running {0}'.format(command))  # pylint: disable=str-format-in-logging\n    if not loglevel:\n        loglevel = 'debug'\n    ret = __salt__['cmd.run_all'](\n        command, cwd=directory, output_loglevel=loglevel,\n        runas=runas, env=env, use_vt=use_vt, python_shell=False)\n    out = ret['stdout'] + '\\n\\n' + ret['stderr']\n    if (exitcode is not None) and (ret['retcode'] != exitcode):\n        raise _BuildoutError(out)\n    ret['output'] = out\n    if output:\n        ret = out\n    return ret", "response": "Execute a command and return the output of the command."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfinds all buildout configs in a subdirectory.", "response": "def _find_cfgs(path, cfgs=None):\n    '''\n    Find all buildout configs in a subdirectory.\n    only buildout.cfg and etc/buildout.cfg are valid in::\n\n    path\n        directory where to start to search\n\n    cfg\n        a optional list to append to\n\n            .\n            \u251c\u2500\u2500 buildout.cfg\n            \u251c\u2500\u2500 etc\n            \u2502\u00a0\u00a0 \u2514\u2500\u2500 buildout.cfg\n            \u251c\u2500\u2500 foo\n            \u2502\u00a0\u00a0 \u2514\u2500\u2500 buildout.cfg\n            \u2514\u2500\u2500 var\n                \u2514\u2500\u2500 buildout.cfg\n    '''\n    ignored = ['var', 'parts']\n    dirs = []\n    if not cfgs:\n        cfgs = []\n    for i in os.listdir(path):\n        fi = os.path.join(path, i)\n        if fi.endswith('.cfg') and os.path.isfile(fi):\n            cfgs.append(fi)\n        if os.path.isdir(fi) and (i not in ignored):\n            dirs.append(fi)\n    for fpath in dirs:\n        for p, ids, ifs in salt.utils.path.os_walk(fpath):\n            for i in ifs:\n                if i.endswith('.cfg'):\n                    cfgs.append(os.path.join(p, i))\n    return cfgs"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_bootstrap_content(directory='.'):\n    '''\n    Get the current bootstrap.py script content\n    '''\n    try:\n        with salt.utils.files.fopen(os.path.join(\n                                os.path.abspath(directory),\n                                'bootstrap.py')) as fic:\n            oldcontent = salt.utils.stringutils.to_unicode(\n                fic.read()\n            )\n    except (OSError, IOError):\n        oldcontent = ''\n    return oldcontent", "response": "Get the current bootstrap. py script content"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_buildout_ver(directory='.'):\n    '''Check for buildout versions.\n\n    In any cases, check for a version pinning\n    Also check for buildout.dumppickedversions which is buildout1 specific\n    Also check for the version targeted by the local bootstrap file\n    Take as default buildout2\n\n    directory\n        directory to execute in\n    '''\n    directory = os.path.abspath(directory)\n    buildoutver = 2\n    try:\n        files = _find_cfgs(directory)\n        for f in files:\n            with salt.utils.files.fopen(f) as fic:\n                buildout1re = re.compile(r'^zc\\.buildout\\s*=\\s*1', RE_F)\n                dfic = salt.utils.stringutils.to_unicode(fic.read())\n                if (\n                        ('buildout.dumppick' in dfic)\n                        or\n                        (buildout1re.search(dfic))\n                ):\n                    buildoutver = 1\n        bcontent = _get_bootstrap_content(directory)\n        if (\n            '--download-base' in bcontent\n            or '--setup-source' in bcontent\n            or '--distribute' in bcontent\n        ):\n            buildoutver = 1\n    except (OSError, IOError):\n        pass\n    return buildoutver", "response": "Check for buildout versions."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_bootstrap_url(directory):\n    '''\n    Get the most appropriate download URL for the bootstrap script.\n\n    directory\n        directory to execute in\n\n    '''\n    v = _get_buildout_ver(directory)\n    return _URL_VERSIONS.get(v, _URL_VERSIONS[DEFAULT_VER])", "response": "Get the most appropriate download URL for the bootstrap script."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nupgrades the bootstrap. py with the last released one.", "response": "def upgrade_bootstrap(directory='.',\n                      onlyif=None,\n                      unless=None,\n                      runas=None,\n                      env=(),\n                      offline=False,\n                      buildout_ver=None):\n    '''\n    Upgrade current bootstrap.py with the last released one.\n\n    Indeed, when we first run a buildout, a common source of problem\n    is to have a locally stale bootstrap, we just try to grab a new copy\n\n    directory\n        directory to execute in\n\n    offline\n        are we executing buildout in offline mode\n\n    buildout_ver\n        forcing to use a specific buildout version (1 | 2)\n\n    onlyif\n        Only execute cmd if statement on the host return 0\n\n    unless\n        Do not execute cmd if statement on the host return 0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' buildout.upgrade_bootstrap /srv/mybuildout\n    '''\n    if buildout_ver:\n        booturl = _URL_VERSIONS[buildout_ver]\n    else:\n        buildout_ver = _get_buildout_ver(directory)\n        booturl = _get_bootstrap_url(directory)\n    LOG.debug('Using {0}'.format(booturl))  # pylint: disable=str-format-in-logging\n    # try to download an up-to-date bootstrap\n    # set defaulttimeout\n    # and add possible content\n    directory = os.path.abspath(directory)\n    b_py = os.path.join(directory, 'bootstrap.py')\n    comment = ''\n    try:\n        oldcontent = _get_bootstrap_content(directory)\n        dbuild = _dot_buildout(directory)\n        data = oldcontent\n        updated = False\n        dled = False\n        if not offline:\n            try:\n                if not os.path.isdir(dbuild):\n                    os.makedirs(dbuild)\n                # only try to download once per buildout checkout\n                with salt.utils.files.fopen(os.path.join(\n                        dbuild,\n                        '{0}.updated_bootstrap'.format(buildout_ver))):\n                    pass\n            except (OSError, IOError):\n                LOG.info('Bootstrap updated from repository')\n                data = _urlopen(booturl).read()\n                updated = True\n                dled = True\n        if 'socket.setdefaulttimeout' not in data:\n            updated = True\n            ldata = data.splitlines()\n            ldata.insert(1, 'import socket;socket.setdefaulttimeout(2)')\n            data = '\\n'.join(ldata)\n        if updated:\n            comment = 'Bootstrap updated'\n            with salt.utils.files.fopen(b_py, 'w') as fic:\n                fic.write(salt.utils.stringutils.to_str(data))\n        if dled:\n            with salt.utils.files.fopen(os.path.join(dbuild,\n                                               '{0}.updated_bootstrap'.format(\n                                                   buildout_ver)), 'w') as afic:\n                afic.write('foo')\n    except (OSError, IOError):\n        if oldcontent:\n            with salt.utils.files.fopen(b_py, 'w') as fic:\n                fic.write(salt.utils.stringutils.to_str(oldcontent))\n\n    return {'comment': comment}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrun the buildout bootstrap dance.", "response": "def bootstrap(directory='.',\n              config='buildout.cfg',\n              python=sys.executable,\n              onlyif=None,\n              unless=None,\n              runas=None,\n              env=(),\n              distribute=None,\n              buildout_ver=None,\n              test_release=False,\n              offline=False,\n              new_st=None,\n              use_vt=False,\n              loglevel=None):\n    '''\n    Run the buildout bootstrap dance (python bootstrap.py).\n\n    directory\n        directory to execute in\n\n    config\n        alternative buildout configuration file to use\n\n    runas\n        User used to run buildout as\n\n    env\n        environment variables to set when running\n\n    buildout_ver\n        force a specific buildout version (1 | 2)\n\n    test_release\n        buildout accept test release\n\n    offline\n        are we executing buildout in offline mode\n\n    distribute\n        Forcing use of distribute\n\n    new_st\n        Forcing use of setuptools >= 0.7\n\n    python\n        path to a python executable to use in place of default (salt one)\n\n    onlyif\n        Only execute cmd if statement on the host return 0\n\n    unless\n        Do not execute cmd if statement on the host return 0\n\n    use_vt\n        Use the new salt VT to stream output [experimental]\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' buildout.bootstrap /srv/mybuildout\n    '''\n    directory = os.path.abspath(directory)\n    dbuild = _dot_buildout(directory)\n    bootstrap_args = ''\n    has_distribute = _has_old_distribute(python=python, runas=runas, env=env)\n    has_new_st = _has_setuptools7(python=python, runas=runas, env=env)\n    if (\n        has_distribute and has_new_st\n        and not distribute and new_st\n    ):\n        new_st = True\n        distribute = False\n    if (\n        has_distribute and has_new_st\n        and not distribute and new_st\n    ):\n        new_st = True\n        distribute = False\n    if (\n        has_distribute and has_new_st\n        and distribute and not new_st\n    ):\n        new_st = True\n        distribute = False\n    if (\n        has_distribute and has_new_st\n        and not distribute and not new_st\n    ):\n        new_st = True\n        distribute = False\n\n    if (\n        not has_distribute and has_new_st\n        and not distribute and new_st\n    ):\n        new_st = True\n        distribute = False\n    if (\n        not has_distribute and has_new_st\n        and not distribute and new_st\n    ):\n        new_st = True\n        distribute = False\n    if (\n        not has_distribute and has_new_st\n        and distribute and not new_st\n    ):\n        new_st = True\n        distribute = False\n    if (\n        not has_distribute and has_new_st\n        and not distribute and not new_st\n    ):\n        new_st = True\n        distribute = False\n\n    if (\n        has_distribute and not has_new_st\n        and not distribute and new_st\n    ):\n        new_st = True\n        distribute = False\n    if (\n        has_distribute and not has_new_st\n        and not distribute and new_st\n    ):\n        new_st = True\n        distribute = False\n    if (\n        has_distribute and not has_new_st\n        and distribute and not new_st\n    ):\n        new_st = False\n        distribute = True\n    if (\n        has_distribute and not has_new_st\n        and not distribute and not new_st\n    ):\n        new_st = False\n        distribute = True\n\n    if (\n        not has_distribute and not has_new_st\n        and not distribute and new_st\n    ):\n        new_st = True\n        distribute = False\n    if (\n        not has_distribute and not has_new_st\n        and not distribute and new_st\n    ):\n        new_st = True\n        distribute = False\n    if (\n        not has_distribute and not has_new_st\n        and distribute and not new_st\n    ):\n        new_st = False\n        distribute = True\n    if (\n        not has_distribute and not has_new_st\n        and not distribute and not new_st\n    ):\n        new_st = True\n        distribute = False\n\n    if new_st and distribute:\n        distribute = False\n    if new_st:\n        distribute = False\n        LOG.warning('Forcing to use setuptools as we have setuptools >= 0.7')\n    if distribute:\n        new_st = False\n        if buildout_ver == 1:\n            LOG.warning('Using distribute !')\n            bootstrap_args += ' --distribute'\n    if not os.path.isdir(dbuild):\n        os.makedirs(dbuild)\n    upgrade_bootstrap(directory,\n                      offline=offline,\n                      buildout_ver=buildout_ver)\n    # be sure which buildout bootstrap we have\n    b_py = os.path.join(directory, 'bootstrap.py')\n    with salt.utils.files.fopen(b_py) as fic:\n        content = salt.utils.stringutils.to_unicode(fic.read())\n    if (\n        (test_release is not False)\n        and ' --accept-buildout-test-releases' in content\n    ):\n        bootstrap_args += ' --accept-buildout-test-releases'\n    if config and '\"-c\"' in content:\n        bootstrap_args += ' -c {0}'.format(config)\n    # be sure that the bootstrap belongs to the running user\n    try:\n        if runas:\n            uid = __salt__['user.info'](runas)['uid']\n            gid = __salt__['user.info'](runas)['gid']\n            os.chown('bootstrap.py', uid, gid)\n    except (IOError, OSError) as exc:\n        # don't block here, try to execute it if can pass\n        _logger.error('BUILDOUT bootstrap permissions error: %s',\n                      exc, exc_info=_logger.isEnabledFor(logging.DEBUG))\n    cmd = '{0} bootstrap.py {1}'.format(python, bootstrap_args)\n    ret = _Popen(cmd, directory=directory, runas=runas, loglevel=loglevel,\n                 env=env, use_vt=use_vt)\n    output = ret['output']\n    return {'comment': cmd, 'out': output}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrunning buildout in a directory.", "response": "def run_buildout(directory='.',\n                 config='buildout.cfg',\n                 parts=None,\n                 onlyif=None,\n                 unless=None,\n                 offline=False,\n                 newest=True,\n                 runas=None,\n                 env=(),\n                 verbose=False,\n                 debug=False,\n                 use_vt=False,\n                 loglevel=None):\n    '''\n    Run a buildout in a directory.\n\n    directory\n        directory to execute in\n\n    config\n        alternative buildout configuration file to use\n\n    offline\n        are we executing buildout in offline mode\n\n    runas\n        user used to run buildout as\n\n    env\n        environment variables to set when running\n\n    onlyif\n        Only execute cmd if statement on the host return 0\n\n    unless\n        Do not execute cmd if statement on the host return 0\n\n    newest\n        run buildout in newest mode\n\n    force\n        run buildout unconditionally\n\n    verbose\n        run buildout in verbose mode (-vvvvv)\n\n    use_vt\n        Use the new salt VT to stream output [experimental]\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' buildout.run_buildout /srv/mybuildout\n    '''\n    directory = os.path.abspath(directory)\n    bcmd = os.path.join(directory, 'bin', 'buildout')\n    installed_cfg = os.path.join(directory, '.installed.cfg')\n    argv = []\n    if verbose:\n        LOG.debug('Buildout is running in verbose mode!')\n        argv.append('-vvvvvvv')\n    if not newest and os.path.exists(installed_cfg):\n        LOG.debug('Buildout is running in non newest mode!')\n        argv.append('-N')\n    if newest:\n        LOG.debug('Buildout is running in newest mode!')\n        argv.append('-n')\n    if offline:\n        LOG.debug('Buildout is running in offline mode!')\n        argv.append('-o')\n    if debug:\n        LOG.debug('Buildout is running in debug mode!')\n        argv.append('-D')\n    cmds, outputs = [], []\n    if parts:\n        for part in parts:\n            LOG.info('Installing single part: {0}'.format(part))  # pylint: disable=str-format-in-logging\n            cmd = '{0} -c {1} {2} install {3}'.format(\n                bcmd, config, ' '.join(argv), part)\n            cmds.append(cmd)\n            outputs.append(\n                _Popen(\n                    cmd, directory=directory,\n                    runas=runas,\n                    env=env,\n                    output=True,\n                    loglevel=loglevel,\n                    use_vt=use_vt)\n            )\n    else:\n        LOG.info('Installing all buildout parts')\n        cmd = '{0} -c {1} {2}'.format(\n            bcmd, config, ' '.join(argv))\n        cmds.append(cmd)\n        outputs.append(\n            _Popen(\n                cmd, directory=directory, runas=runas, loglevel=loglevel,\n                env=env, output=True, use_vt=use_vt)\n        )\n\n    return {'comment': '\\n'.join(cmds),\n            'out': '\\n'.join(outputs)}"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nbuilding out a single node in a directory.", "response": "def buildout(directory='.',\n             config='buildout.cfg',\n             parts=None,\n             runas=None,\n             env=(),\n             buildout_ver=None,\n             test_release=False,\n             distribute=None,\n             new_st=None,\n             offline=False,\n             newest=False,\n             python=sys.executable,\n             debug=False,\n             verbose=False,\n             onlyif=None,\n             unless=None,\n             use_vt=False,\n             loglevel=None):\n    '''\n    Run buildout in a directory.\n\n    directory\n        directory to execute in\n\n    config\n        buildout config to use\n\n    parts\n        specific buildout parts to run\n\n    runas\n        user used to run buildout as\n\n    env\n        environment variables to set when running\n\n    buildout_ver\n        force a specific buildout version (1 | 2)\n\n    test_release\n        buildout accept test release\n\n    new_st\n        Forcing use of setuptools >= 0.7\n\n    distribute\n        use distribute over setuptools if possible\n\n    offline\n        does buildout run offline\n\n    python\n        python to use\n\n    debug\n        run buildout with -D debug flag\n\n    onlyif\n        Only execute cmd if statement on the host return 0\n\n    unless\n        Do not execute cmd if statement on the host return 0\n    newest\n        run buildout in newest mode\n\n    verbose\n        run buildout in verbose mode (-vvvvv)\n\n    use_vt\n        Use the new salt VT to stream output [experimental]\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' buildout.buildout /srv/mybuildout\n    '''\n    LOG.info('Running buildout in {0} ({1})'.format(directory, config))  # pylint: disable=str-format-in-logging\n    boot_ret = bootstrap(directory,\n                         config=config,\n                         buildout_ver=buildout_ver,\n                         test_release=test_release,\n                         offline=offline,\n                         new_st=new_st,\n                         env=env,\n                         runas=runas,\n                         distribute=distribute,\n                         python=python,\n                         use_vt=use_vt,\n                         loglevel=loglevel)\n    buildout_ret = run_buildout(directory=directory,\n                                config=config,\n                                parts=parts,\n                                offline=offline,\n                                newest=newest,\n                                runas=runas,\n                                env=env,\n                                verbose=verbose,\n                                debug=debug,\n                                use_vt=use_vt,\n                                loglevel=loglevel)\n    # signal the decorator or our return\n    return _merge_statuses([boot_ret, buildout_ret])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nexecuting queries against SQLite3 merge and return as a dict", "response": "def ext_pillar(minion_id,\n               pillar,\n               *args,\n               **kwargs):\n    '''\n    Execute queries against SQLite3, merge and return as a dict\n    '''\n    return SQLite3ExtPillar().fetch(minion_id, pillar, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading pillar data from HTTP request.", "response": "def ext_pillar(minion_id,\n               pillar,  # pylint: disable=W0613\n               url,\n               with_grains=False):\n    '''\n    Read pillar data from HTTP response.\n\n    :param str url: Url to request.\n    :param bool with_grains: Whether to substitute strings in the url with their grain values.\n\n    :return: A dictionary of the pillar data to add.\n    :rtype: dict\n    '''\n\n    url = url.replace('%s', _quote(minion_id))\n\n    grain_pattern = r'<(?P<grain_name>.*?)>'\n\n    if with_grains:\n        # Get the value of the grain and substitute each grain\n        # name for the url-encoded version of its grain value.\n        for match in re.finditer(grain_pattern, url):\n            grain_name = match.group('grain_name')\n            grain_value = __salt__['grains.get'](grain_name, None)\n\n            if not grain_value:\n                log.error(\"Unable to get minion '%s' grain: %s\", minion_id, grain_name)\n                return {}\n\n            grain_value = _quote(six.text_type(grain_value))\n            url = re.sub('<{0}>'.format(grain_name), grain_value, url)\n\n    log.debug('Getting url: %s', url)\n    data = __salt__['http.query'](url=url, decode=True, decode_type='json')\n\n    if 'dict' in data:\n        return data['dict']\n\n    log.error(\"Error on minion '%s' http query: %s\\nMore Info:\\n\", minion_id, url)\n\n    for key in data:\n        log.error('%s: %s', key, data[key])\n\n    return {}"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nverify user password. username Username on which to perform password check password Password to check encrypted Whether or not the password is encrypted Default: False .. code-block: bash salt '*' nxos.cmd check_password username=admin password=admin salt '*' nxos.cmd check_password username=admin \\\\ password='$5$2fWwO2vK$s7.Hr3YltMNHuhywQQ3nfOd.gAPHgs3SOBYYdGT3E.A' \\\\ encrypted=True", "response": "def check_password(username, password, encrypted=False, **kwargs):\n    '''\n    Verify user password.\n\n    username\n        Username on which to perform password check\n\n    password\n        Password to check\n\n    encrypted\n        Whether or not the password is encrypted\n        Default: False\n\n    .. code-block: bash\n\n        salt '*' nxos.cmd check_password username=admin password=admin\n        salt '*' nxos.cmd check_password username=admin \\\\\n            password='$5$2fWwO2vK$s7.Hr3YltMNHuhywQQ3nfOd.gAPHgs3SOBYYdGT3E.A' \\\\\n            encrypted=True\n    '''\n    hash_algorithms = {'1': 'md5',\n                       '2a': 'blowfish',\n                       '5': 'sha256',\n                       '6': 'sha512', }\n    password_line = get_user(username, **kwargs)\n    if not password_line:\n        return None\n    if '!' in password_line:\n        return False\n    cur_hash = re.search(r'(\\$[0-6](?:\\$[^$ ]+)+)', password_line).group(0)\n    if encrypted is False:\n        hash_type, cur_salt, hashed_pass = re.search(r'^\\$([0-6])\\$([^$]+)\\$(.*)$', cur_hash).groups()\n        new_hash = gen_hash(crypt_salt=cur_salt, password=password, algorithm=hash_algorithms[hash_type])\n    else:\n        new_hash = password\n    if new_hash == cur_hash:\n        return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cmd(command, *args, **kwargs):\n    '''\n    NOTE: This function is preserved for backwards compatibilty.  This allows\n    commands to be executed using either of the following syntactic forms.\n\n    salt '*' nxos.cmd <function>\n\n    or\n\n    salt '*' nxos.<function>\n\n    command\n        function from `salt.modules.nxos` to run\n\n    args\n        positional args to pass to `command` function\n\n    kwargs\n        key word arguments to pass to `command` function\n\n    .. code-block:: bash\n\n        salt '*' nxos.cmd sendline 'show ver'\n        salt '*' nxos.cmd show_run\n        salt '*' nxos.cmd check_password username=admin password='$5$lkjsdfoi$blahblahblah' encrypted=True\n    '''\n    for k in list(kwargs):\n        if k.startswith('__pub_'):\n            kwargs.pop(k)\n    local_command = '.'.join(['nxos', command])\n    log.info('local command: %s', local_command)\n    if local_command not in __salt__:\n        return False\n    return __salt__[local_command](*args, **kwargs)", "response": "Execute a command on the neccesary container."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets roles assigned to a username.", "response": "def get_roles(username, **kwargs):\n    '''\n    Get roles assigned to a username.\n\n    .. code-block: bash\n\n        salt '*' nxos.cmd get_roles username=admin\n    '''\n    user = get_user(username)\n    if not user:\n        return []\n    command = 'show user-account {0}'.format(username)\n    info = ''\n    info = show(command, **kwargs)\n    if isinstance(info, list):\n        info = info[0]\n    roles = re.search(r'^\\s*roles:(.*)$', info, re.MULTILINE)\n    if roles:\n        roles = roles.group(1).strip().split(' ')\n    else:\n        roles = []\n    return roles"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets username line from switch.", "response": "def get_user(username, **kwargs):\n    '''\n    Get username line from switch.\n\n    .. code-block: bash\n\n        salt '*' nxos.cmd get_user username=admin\n    '''\n    command = 'show run | include \"^username {0} password 5 \"'.format(username)\n    info = ''\n    info = show(command, **kwargs)\n    if isinstance(info, list):\n        info = info[0]\n    return info"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef grains(**kwargs):\n    '''\n    Get grains for minion.\n\n    .. code-block: bash\n\n        salt '*' nxos.cmd grains\n    '''\n    if not DEVICE_DETAILS['grains_cache']:\n        ret = salt.utils.nxos.system_info(show_ver(**kwargs))\n        log.debug(ret)\n        DEVICE_DETAILS['grains_cache'].update(ret['nxos'])\n    return DEVICE_DETAILS['grains_cache']", "response": "Return the grains for the minion."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sendline(command, method='cli_show_ascii', **kwargs):\n    '''\n    Send arbitray commands to the NX-OS device.\n\n    command\n        The command to be sent.\n\n    method:\n        ``cli_show_ascii``: Return raw test or unstructured output.\n        ``cli_show``: Return structured output.\n        ``cli_conf``: Send configuration commands to the device.\n        Defaults to ``cli_show_ascii``.\n        NOTE: method is ignored for SSH proxy minion.  All data is returned\n        unstructured.\n\n    .. code-block: bash\n\n        salt '*' nxos.cmd sendline 'show run | include \"^username admin password\"'\n    '''\n    smethods = ['cli_show_ascii', 'cli_show', 'cli_conf']\n    if method not in smethods:\n        msg = \"\"\"\n        INPUT ERROR: Second argument 'method' must be one of {0}\n        Value passed: {1}\n        Hint: White space separated commands should be wrapped by double quotes\n        \"\"\".format(smethods, method)\n        return msg\n\n    if salt.utils.platform.is_proxy():\n        return __proxy__['nxos.sendline'](command, method, **kwargs)\n    else:\n        return _nxapi_request(command, method, **kwargs)", "response": "Send arbitray commands to NX - OS device."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nexecuting one or more show commands.", "response": "def show(commands, raw_text=True, **kwargs):\n    '''\n    Execute one or more show (non-configuration) commands.\n\n    commands\n        The commands to be executed.\n\n    raw_text: ``True``\n        Whether to return raw text or structured data.\n        NOTE: raw_text option is ignored for SSH proxy minion.  Data is\n        returned unstructured.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call --local nxos.show 'show version'\n        salt '*' nxos.show 'show bgp sessions ; show processes' raw_text=False\n        salt 'regular-minion' nxos.show 'show interfaces' host=sw01.example.com username=test password=test\n    '''\n    if not isinstance(raw_text, bool):\n        msg = \"\"\"\n        INPUT ERROR: Second argument 'raw_text' must be either True or False\n        Value passed: {0}\n        Hint: White space separated show commands should be wrapped by double quotes\n        \"\"\".format(raw_text)\n        return msg\n\n    if raw_text:\n        method = 'cli_show_ascii'\n    else:\n        method = 'cli_show'\n\n    response_list = sendline(commands, method, **kwargs)\n    if isinstance(response_list, list):\n        ret = [response for response in response_list if response]\n        if not ret:\n            ret = ['']\n        return ret\n    else:\n        return response_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconfigures the Nexus switch with the specified commands. This method is used to send configuration commands to the switch. It will take either a string or a list and prepend the necessary commands to put the session into config mode. .. warning:: All the commands will be applied directly to the running-config. config_file The source file with the configuration commands to be sent to the device. The file can also be a template that can be rendered using the template engine of choice. This can be specified using the absolute path to the file, or using one of the following URL schemes: - ``salt://``, to fetch the file from the Salt fileserver. - ``http://`` or ``https://`` - ``ftp://`` - ``s3://`` - ``swift://`` commands The commands to send to the switch in config mode. If the commands argument is a string it will be cast to a list. The list of commands will also be prepended with the necessary commands to put the session in config mode. .. note:: This argument is ignored when ``config_file`` is specified. template_engine: ``jinja`` The template engine to use when rendering the source file. Default: ``jinja``. To simply fetch the file without attempting to render, set this argument to ``None``. context Variables to add to the template context. defaults Default values of the context_dict. no_save_config If True, don't save configuration commands to startup configuration. If False, save configuration to startup configuration. Default: False CLI Example: .. code-block:: bash salt '*' nxos.config commands=\"['spanning-tree mode mstp']\" salt '*' nxos.config config_file=salt://config.txt salt '*' nxos.config config_file=https://bit.ly/2LGLcDy context=\"{'servers': ['1.2.3.4']}\"", "response": "def config(commands=None,\n           config_file=None,\n           template_engine='jinja',\n           context=None,\n           defaults=None,\n           saltenv='base',\n           **kwargs):\n    '''\n    Configures the Nexus switch with the specified commands.\n\n    This method is used to send configuration commands to the switch.  It\n    will take either a string or a list and prepend the necessary commands\n    to put the session into config mode.\n\n    .. warning::\n\n        All the commands will be applied directly to the running-config.\n\n    config_file\n        The source file with the configuration commands to be sent to the\n        device.\n\n        The file can also be a template that can be rendered using the template\n        engine of choice.\n\n        This can be specified using the absolute path to the file, or using one\n        of the following URL schemes:\n\n        - ``salt://``, to fetch the file from the Salt fileserver.\n        - ``http://`` or ``https://``\n        - ``ftp://``\n        - ``s3://``\n        - ``swift://``\n\n    commands\n        The commands to send to the switch in config mode.  If the commands\n        argument is a string it will be cast to a list.\n        The list of commands will also be prepended with the necessary commands\n        to put the session in config mode.\n\n        .. note::\n\n            This argument is ignored when ``config_file`` is specified.\n\n    template_engine: ``jinja``\n        The template engine to use when rendering the source file. Default:\n        ``jinja``. To simply fetch the file without attempting to render, set\n        this argument to ``None``.\n\n    context\n        Variables to add to the template context.\n\n    defaults\n        Default values of the context_dict.\n\n    no_save_config\n        If True, don't save configuration commands to startup configuration.\n        If False, save configuration to startup configuration.\n        Default: False\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' nxos.config commands=\"['spanning-tree mode mstp']\"\n        salt '*' nxos.config config_file=salt://config.txt\n        salt '*' nxos.config config_file=https://bit.ly/2LGLcDy context=\"{'servers': ['1.2.3.4']}\"\n    '''\n    initial_config = show('show running-config', **kwargs)\n    if isinstance(initial_config, list):\n        initial_config = initial_config[0]\n    if config_file:\n        file_str = __salt__['cp.get_file_str'](config_file, saltenv=saltenv)\n        if file_str is False:\n            raise CommandExecutionError('Source file {} not found'.format(config_file))\n    elif commands:\n        if isinstance(commands, (six.string_types, six.text_type)):\n            commands = [commands]\n        file_str = '\\n'.join(commands)\n        # unify all the commands in a single file, to render them in a go\n    if template_engine:\n        file_str = __salt__['file.apply_template_on_contents'](file_str,\n                                                               template_engine,\n                                                               context,\n                                                               defaults,\n                                                               saltenv)\n    # whatever the source of the commands would be, split them line by line\n    commands = [line for line in file_str.splitlines() if line.strip()]\n    config_result = _parse_config_result(_configure_device(commands, **kwargs))\n    current_config = show('show running-config', **kwargs)\n    if isinstance(current_config, list):\n        current_config = current_config[0]\n    diff = difflib.unified_diff(initial_config.splitlines(1)[4:], current_config.splitlines(1)[4:])\n    clean_diff = ''.join([x.replace('\\r', '') for x in diff])\n    head = 'COMMAND_LIST: '\n    cc = config_result[0]\n    cr = config_result[1]\n    return head + cc + '\\n' + cr + '\\n' + clean_diff"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndeletes one or more config lines to the switch running config.", "response": "def delete_config(lines, **kwargs):\n    '''\n    Delete one or more config lines to the switch running config.\n\n    lines\n        Configuration lines to remove.\n\n    no_save_config\n        If True, don't save configuration commands to startup configuration.\n        If False, save configuration to startup configuration.\n        Default: False\n\n    .. code-block:: bash\n\n        salt '*' nxos.cmd delete_config 'snmp-server community TESTSTRINGHERE group network-operator'\n\n    .. note::\n        For more than one config deleted per command, lines should be a list.\n    '''\n    if not isinstance(lines, list):\n        lines = [lines]\n    for i, _ in enumerate(lines):\n        lines[i] = 'no ' + lines[i]\n    result = None\n    try:\n        result = config(lines, **kwargs)\n    except CommandExecutionError as e:\n        # Some commands will generate error code 400 if they do not exist\n        # and we try to remove them.  These can be ignored.\n        if ast.literal_eval(e.message)['code'] != '400':\n            raise\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_password(username,\n                 password,\n                 encrypted=False,\n                 role=None,\n                 crypt_salt=None,\n                 algorithm='sha256',\n                 **kwargs):\n    '''\n    Set users password on switch.\n\n    username\n        Username to configure\n\n    password\n        Password to configure for username\n\n    encrypted\n        Whether or not to encrypt the password\n        Default: False\n\n    role\n        Configure role for the username\n        Default: None\n\n    crypt_salt\n        Configure crypt_salt setting\n        Default: None\n\n    alogrithm\n        Encryption algorithm\n        Default: sha256\n\n    no_save_config\n        If True, don't save configuration commands to startup configuration.\n        If False, save configuration to startup configuration.\n        Default: False\n\n    .. code-block:: bash\n\n        salt '*' nxos.cmd set_password admin TestPass\n        salt '*' nxos.cmd set_password admin \\\\\n            password='$5$2fWwO2vK$s7.Hr3YltMNHuhywQQ3nfOd.gAPHgs3SOBYYdGT3E.A' \\\\\n            encrypted=True\n    '''\n    password_line = get_user(username, **kwargs)\n    if encrypted is False:\n        if crypt_salt is None:\n            # NXOS does not like non alphanumeric characters.  Using the random module from pycrypto\n            # can lead to having non alphanumeric characters in the salt for the hashed password.\n            crypt_salt = secure_password(8, use_random=False)\n        hashed_pass = gen_hash(crypt_salt=crypt_salt, password=password, algorithm=algorithm)\n    else:\n        hashed_pass = password\n    password_line = 'username {0} password 5 {1}'.format(username, hashed_pass)\n    if role is not None:\n        password_line += ' role {0}'.format(role)\n    return config(password_line, **kwargs)", "response": "Configure the users password on switch."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_role(username, role, **kwargs):\n    '''\n    Assign role to username.\n\n    username\n        Username for role configuration\n\n    role\n        Configure role for username\n\n    no_save_config\n        If True, don't save configuration commands to startup configuration.\n        If False, save configuration to startup configuration.\n        Default: False\n\n    .. code-block:: bash\n\n        salt '*' nxos.cmd set_role username=daniel role=vdc-admin.\n    '''\n    role_line = 'username {0} role {1}'.format(username, role)\n    return config(role_line, **kwargs)", "response": "Assign role to username."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving a role from a username.", "response": "def unset_role(username, role, **kwargs):\n    '''\n    Remove role from username.\n\n    username\n        Username for role removal\n\n    role\n        Role to remove\n\n    no_save_config\n        If True, don't save configuration commands to startup configuration.\n        If False, save configuration to startup configuration.\n        Default: False\n\n    .. code-block:: bash\n\n        salt '*' nxos.cmd unset_role username=daniel role=vdc-admin\n    '''\n    role_line = 'no username {0} role {1}'.format(username, role)\n    return config(role_line, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the data compressed at gzip level compression.", "response": "def compress(data, compresslevel=9):\n    '''\n    Returns the data compressed at gzip level compression.\n    '''\n    buf = BytesIO()\n    with open_fileobj(buf, 'wb', compresslevel) as ogz:\n        if six.PY3 and not isinstance(data, bytes):\n            data = data.encode(__salt_system_encoding__)\n        ogz.write(data)\n    compressed = buf.getvalue()\n    return compressed"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrelocating an absolute path to a new root directory.", "response": "def _root(path, root):\n    '''\n    Relocate an absolute path to a new root directory.\n    '''\n    if root:\n        return os.path.join(root, os.path.relpath(path, os.path.sep))\n    else:\n        return path"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nbuild a canonical unit name treating unit names without one of the valid suffixes as a service.", "response": "def _canonical_unit_name(name):\n    '''\n    Build a canonical unit name treating unit names without one\n    of the valid suffixes as a service.\n    '''\n    if not isinstance(name, six.string_types):\n        name = six.text_type(name)\n    if any(name.endswith(suffix) for suffix in VALID_UNIT_TYPES):\n        return name\n    return '%s.service' % name"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _check_available(name):\n    '''\n    Returns boolean telling whether or not the named service is available\n    '''\n    _status = _systemctl_status(name)\n    sd_version = salt.utils.systemd.version(__context__)\n    if sd_version is not None and sd_version >= 231:\n        # systemd 231 changed the output of \"systemctl status\" for unknown\n        # services, and also made it return an exit status of 4. If we are on\n        # a new enough version, check the retcode, otherwise fall back to\n        # parsing the \"systemctl status\" output.\n        # See: https://github.com/systemd/systemd/pull/3385\n        # Also: https://github.com/systemd/systemd/commit/3dced37\n        return 0 <= _status['retcode'] < 4\n\n    out = _status['stdout'].lower()\n    if 'could not be found' in out:\n        # Catch cases where the systemd version is < 231 but the return code\n        # and output changes have been backported (e.g. RHEL 7.3).\n        return False\n\n    for line in salt.utils.itertools.split(out, '\\n'):\n        match = re.match(r'\\s+loaded:\\s+(\\S+)', line)\n        if match:\n            ret = match.group(1) != 'not-found'\n            break\n    else:\n        raise CommandExecutionError(\n            'Failed to get information on unit \\'%s\\'' % name\n        )\n    return ret", "response": "Returns whether or not the named service is available on the system."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks for modified or updated unit files and run a daemon - reload if any are found.", "response": "def _check_for_unit_changes(name):\n    '''\n    Check for modified/updated unit files, and run a daemon-reload if any are\n    found.\n    '''\n    contextkey = 'systemd._check_for_unit_changes.{0}'.format(name)\n    if contextkey not in __context__:\n        if _untracked_custom_unit_found(name) or _unit_file_changed(name):\n            systemctl_reload()\n        # Set context key to avoid repeating this check\n        __context__[contextkey] = True"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _check_unmask(name, unmask, unmask_runtime, root=None):\n    '''\n    Common code for conditionally removing masks before making changes to a\n    service's state.\n    '''\n    if unmask:\n        unmask_(name, runtime=False, root=root)\n    if unmask_runtime:\n        unmask_(name, runtime=True, root=root)", "response": "Internal helper to remove masks from a\n   ."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove all the context from the system.", "response": "def _clear_context():\n    '''\n    Remove context\n    '''\n    # Using list() here because modifying a dictionary during iteration will\n    # raise a RuntimeError.\n    for key in list(__context__):\n        try:\n            if key.startswith('systemd._systemctl_status.'):\n                __context__.pop(key)\n        except AttributeError:\n            continue"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _default_runlevel():\n    '''\n    Try to figure out the default runlevel.  It is kept in\n    /etc/init/rc-sysinit.conf, but can be overridden with entries\n    in /etc/inittab, or via the kernel command-line at boot\n    '''\n    # Try to get the \"main\" default.  If this fails, throw up our\n    # hands and just guess \"2\", because things are horribly broken\n    try:\n        with salt.utils.files.fopen('/etc/init/rc-sysinit.conf') as fp_:\n            for line in fp_:\n                line = salt.utils.stringutils.to_unicode(line)\n                if line.startswith('env DEFAULT_RUNLEVEL'):\n                    runlevel = line.split('=')[-1].strip()\n    except Exception:\n        return '2'\n\n    # Look for an optional \"legacy\" override in /etc/inittab\n    try:\n        with salt.utils.files.fopen('/etc/inittab') as fp_:\n            for line in fp_:\n                line = salt.utils.stringutils.to_unicode(line)\n                if not line.startswith('#') and 'initdefault' in line:\n                    runlevel = line.split(':')[1]\n    except Exception:\n        pass\n\n    # The default runlevel can also be set via the kernel command-line.\n    # Kinky.\n    try:\n        valid_strings = set(\n            ('0', '1', '2', '3', '4', '5', '6', 's', 'S', '-s', 'single'))\n        with salt.utils.files.fopen('/proc/cmdline') as fp_:\n            for line in fp_:\n                line = salt.utils.stringutils.to_unicode(line)\n                for arg in line.strip().split():\n                    if arg in valid_strings:\n                        runlevel = arg\n                        break\n    except Exception:\n        pass\n\n    return runlevel", "response": "Try to figure out the default runlevel for the current system."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget all the systemd services", "response": "def _get_systemd_services(root):\n    '''\n    Use os.listdir() to get all the unit files\n    '''\n    ret = set()\n    for path in SYSTEM_CONFIG_PATHS + (LOCAL_CONFIG_PATH,):\n        # Make sure user has access to the path, and if the path is a\n        # link it's likely that another entry in SYSTEM_CONFIG_PATHS\n        # or LOCAL_CONFIG_PATH points to it, so we can ignore it.\n        path = _root(path, root)\n        if os.access(path, os.R_OK) and not os.path.islink(path):\n            for fullname in os.listdir(path):\n                try:\n                    unit_name, unit_type = fullname.rsplit('.', 1)\n                except ValueError:\n                    continue\n                if unit_type in VALID_UNIT_TYPES:\n                    ret.add(unit_name if unit_type == 'service' else fullname)\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting a list of sysvinit scripts for a given root.", "response": "def _get_sysv_services(root, systemd_services=None):\n    '''\n    Use os.listdir() and os.access() to get all the initscripts\n    '''\n    initscript_path = _root(INITSCRIPT_PATH, root)\n    try:\n        sysv_services = os.listdir(initscript_path)\n    except OSError as exc:\n        if exc.errno == errno.ENOENT:\n            pass\n        elif exc.errno == errno.EACCES:\n            log.error(\n                'Unable to check sysvinit scripts, permission denied to %s',\n                initscript_path\n            )\n        else:\n            log.error(\n                'Error %d encountered trying to check sysvinit scripts: %s',\n                exc.errno,\n                exc.strerror\n            )\n        return []\n\n    if systemd_services is None:\n        systemd_services = _get_systemd_services(root)\n\n    ret = []\n    for sysv_service in sysv_services:\n        if os.access(os.path.join(initscript_path, sysv_service), os.X_OK):\n            if sysv_service in systemd_services:\n                log.debug(\n                    'sysvinit script \\'%s\\' found, but systemd unit '\n                    '\\'%s.service\\' already exists',\n                    sysv_service, sysv_service\n                )\n                continue\n            ret.append(sysv_service)\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_service_exec():\n    '''\n    Returns the path to the sysv service manager (either update-rc.d or\n    chkconfig)\n    '''\n    contextkey = 'systemd._get_service_exec'\n    if contextkey not in __context__:\n        executables = ('update-rc.d', 'chkconfig')\n        for executable in executables:\n            service_exec = salt.utils.path.which(executable)\n            if service_exec is not None:\n                break\n        else:\n            raise CommandExecutionError(\n                'Unable to find sysv service manager (tried {0})'.format(\n                    ', '.join(executables)\n                )\n            )\n        __context__[contextkey] = service_exec\n    return __context__[contextkey]", "response": "Returns the path to the sysv service manager"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the current runlevel", "response": "def _runlevel():\n    '''\n    Return the current runlevel\n    '''\n    contextkey = 'systemd._runlevel'\n    if contextkey in __context__:\n        return __context__[contextkey]\n    out = __salt__['cmd.run']('runlevel', python_shell=False, ignore_retcode=True)\n    try:\n        ret = out.split()[1]\n    except IndexError:\n        # The runlevel is unknown, return the default\n        ret = _default_runlevel()\n    __context__[contextkey] = ret\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _strip_scope(msg):\n    '''\n    Strip unnecessary message about running the command with --scope from\n    stderr so that we can raise an exception with the remaining stderr text.\n    '''\n    ret = []\n    for line in msg.splitlines():\n        if not line.endswith('.scope'):\n            ret.append(line)\n    return '\\n'.join(ret).strip()", "response": "Strip unnecessary message about running the command with. scope from\n    stderr so that we can raise an exception with the remaining stderr text."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _systemctl_cmd(action, name=None, systemd_scope=False, no_block=False,\n                   root=None):\n    '''\n    Build a systemctl command line. Treat unit names without one\n    of the valid suffixes as a service.\n    '''\n    ret = []\n    if systemd_scope \\\n            and salt.utils.systemd.has_scope(__context__) \\\n            and __salt__['config.get']('systemd.scope', True):\n        ret.extend(['systemd-run', '--scope'])\n    ret.append('systemctl')\n    if no_block:\n        ret.append('--no-block')\n    if root:\n        ret.extend(['--root', root])\n    if isinstance(action, six.string_types):\n        action = shlex.split(action)\n    ret.extend(action)\n    if name is not None:\n        ret.append(_canonical_unit_name(name))\n    if 'status' in ret:\n        ret.extend(['-n', '0'])\n    return ret", "response": "Build a systemctl command line."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _systemctl_status(name):\n    '''\n    Helper function which leverages __context__ to keep from running 'systemctl\n    status' more than once.\n    '''\n    contextkey = 'systemd._systemctl_status.%s' % name\n    if contextkey in __context__:\n        return __context__[contextkey]\n    __context__[contextkey] = __salt__['cmd.run_all'](\n        _systemctl_cmd('status', name),\n        python_shell=False,\n        redirect_stderr=True,\n        ignore_retcode=True\n    )\n    return __context__[contextkey]", "response": "Helper function which leverages systemctl\n    status more than once."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _sysv_enabled(name, root):\n    '''\n    A System-V style service is assumed disabled if the \"startup\" symlink\n    (starts with \"S\") to its script is found in /etc/init.d in the current\n    runlevel.\n    '''\n    # Find exact match (disambiguate matches like \"S01anacron\" for cron)\n    rc = _root('/etc/rc{}.d/S*{}'.format(_runlevel(), name), root)\n    for match in glob.glob(rc):\n        if re.match(r'S\\d{,2}%s' % name, os.path.basename(match)):\n            return True\n    return False", "response": "Check if a System - V service is enabled."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _untracked_custom_unit_found(name, root=None):\n    '''\n    If the passed service name is not available, but a unit file exist in\n    /etc/systemd/system, return True. Otherwise, return False.\n    '''\n    system = _root('/etc/systemd/system', root)\n    unit_path = os.path.join(system, _canonical_unit_name(name))\n    return os.access(unit_path, os.R_OK) and not _check_available(name)", "response": "Return True if a custom unit file is not available in the system."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreload systemctl daemon - reload", "response": "def systemctl_reload():\n    '''\n    .. versionadded:: 0.15.0\n\n    Reloads systemctl, an action needed whenever unit files are updated.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.systemctl_reload\n    '''\n    out = __salt__['cmd.run_all'](\n        _systemctl_cmd('--system daemon-reload'),\n        python_shell=False,\n        redirect_stderr=True)\n    if out['retcode'] != 0:\n        raise CommandExecutionError(\n            'Problem performing systemctl daemon-reload: %s' % out['stdout']\n        )\n    _clear_context()\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of all running systemd units", "response": "def get_running():\n    '''\n    Return a list of all running services, so far as systemd is concerned\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.get_running\n    '''\n    ret = set()\n    # Get running systemd units\n    out = __salt__['cmd.run'](\n        _systemctl_cmd('--full --no-legend --no-pager'),\n        python_shell=False,\n        ignore_retcode=True)\n    for line in salt.utils.itertools.split(out, '\\n'):\n        try:\n            comps = line.strip().split()\n            fullname = comps[0]\n            if len(comps) > 3:\n                active_state = comps[3]\n        except ValueError as exc:\n            log.error(exc)\n            continue\n        else:\n            if active_state != 'running':\n                continue\n        try:\n            unit_name, unit_type = fullname.rsplit('.', 1)\n        except ValueError:\n            continue\n        if unit_type in VALID_UNIT_TYPES:\n            ret.add(unit_name if unit_type == 'service' else fullname)\n\n    return sorted(ret)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_static(root=None):\n    '''\n    .. versionadded:: 2015.8.5\n\n    Return a list of all static services\n\n    root\n        Enable/disable/mask unit files in the specified root directory\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.get_static\n    '''\n    ret = set()\n    # Get static systemd units. Can't use --state=static here because it's\n    # not present until systemd 216.\n    out = __salt__['cmd.run'](\n        _systemctl_cmd('--full --no-legend --no-pager list-unit-files',\n                       root=root),\n        python_shell=False,\n        ignore_retcode=True)\n    for line in salt.utils.itertools.split(out, '\\n'):\n        try:\n            fullname, unit_state = line.strip().split(None, 1)\n        except ValueError:\n            continue\n        else:\n            if unit_state != 'static':\n                continue\n        try:\n            unit_name, unit_type = fullname.rsplit('.', 1)\n        except ValueError:\n            continue\n        if unit_type in VALID_UNIT_TYPES:\n            ret.add(unit_name if unit_type == 'service' else fullname)\n\n    # sysvinit services cannot be static\n    return sorted(ret)", "response": "Return a list of all static services in the specified root directory"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_all(root=None):\n    '''\n    Return a list of all available services\n\n    root\n        Enable/disable/mask unit files in the specified root directory\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.get_all\n    '''\n    ret = _get_systemd_services(root)\n    ret.update(set(_get_sysv_services(root, systemd_services=ret)))\n    return sorted(ret)", "response": "Return a list of all available systemd services"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nunmasks the specified service with systemd - run.", "response": "def unmask_(name, runtime=False, root=None):\n    '''\n    .. versionadded:: 2015.5.0\n    .. versionchanged:: 2015.8.12,2016.3.3,2016.11.0\n        On minions running systemd>=205, `systemd-run(1)`_ is now used to\n        isolate commands run by this function from the ``salt-minion`` daemon's\n        control group. This is done to avoid a race condition in cases where\n        the ``salt-minion`` service is restarted while a service is being\n        modified. If desired, usage of `systemd-run(1)`_ can be suppressed by\n        setting a :mod:`config option <salt.modules.config.get>` called\n        ``systemd.scope``, with a value of ``False`` (no quotes).\n\n    .. _`systemd-run(1)`: https://www.freedesktop.org/software/systemd/man/systemd-run.html\n\n    Unmask the specified service with systemd\n\n    runtime : False\n        Set to ``True`` to unmask this service only until the next reboot\n\n        .. versionadded:: 2017.7.0\n            In previous versions, this function would remove whichever mask was\n            identified by running ``systemctl is-enabled`` on the service.\n            However, since it is possible to both have both indefinite and\n            runtime masks on a service simultaneously, this function now\n            removes a runtime mask only when this argument is set to ``True``,\n            and otherwise removes an indefinite mask.\n\n    root\n        Enable/disable/mask unit files in the specified root directory\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.unmask foo\n        salt '*' service.unmask foo runtime=True\n    '''\n    _check_for_unit_changes(name)\n    if not masked(name, runtime, root=root):\n        log.debug('Service \\'%s\\' is not %smasked',\n                  name, 'runtime-' if runtime else '')\n        return True\n\n    cmd = 'unmask --runtime' if runtime else 'unmask'\n    out = __salt__['cmd.run_all'](\n        _systemctl_cmd(cmd, name, systemd_scope=True, root=root),\n        python_shell=False,\n        redirect_stderr=True)\n\n    if out['retcode'] != 0:\n        raise CommandExecutionError('Failed to unmask service \\'%s\\'' % name)\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmasking the specified service with systemd - mask", "response": "def mask(name, runtime=False, root=None):\n    '''\n    .. versionadded:: 2015.5.0\n    .. versionchanged:: 2015.8.12,2016.3.3,2016.11.0\n        On minions running systemd>=205, `systemd-run(1)`_ is now used to\n        isolate commands run by this function from the ``salt-minion`` daemon's\n        control group. This is done to avoid a race condition in cases where\n        the ``salt-minion`` service is restarted while a service is being\n        modified. If desired, usage of `systemd-run(1)`_ can be suppressed by\n        setting a :mod:`config option <salt.modules.config.get>` called\n        ``systemd.scope``, with a value of ``False`` (no quotes).\n\n    .. _`systemd-run(1)`: https://www.freedesktop.org/software/systemd/man/systemd-run.html\n\n    Mask the specified service with systemd\n\n    runtime : False\n        Set to ``True`` to mask this service only until the next reboot\n\n        .. versionadded:: 2015.8.5\n\n    root\n        Enable/disable/mask unit files in the specified root directory\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.mask foo\n        salt '*' service.mask foo runtime=True\n    '''\n    _check_for_unit_changes(name)\n\n    cmd = 'mask --runtime' if runtime else 'mask'\n    out = __salt__['cmd.run_all'](\n        _systemctl_cmd(cmd, name, systemd_scope=True, root=root),\n        python_shell=False,\n        redirect_stderr=True)\n\n    if out['retcode'] != 0:\n        raise CommandExecutionError(\n            'Failed to mask service \\'%s\\'' % name,\n            info=out['stdout']\n        )\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a copy of the systemd unit file that is masked on the named service.", "response": "def masked(name, runtime=False, root=None):\n    '''\n    .. versionadded:: 2015.8.0\n    .. versionchanged:: 2015.8.5\n        The return data for this function has changed. If the service is\n        masked, the return value will now be the output of the ``systemctl\n        is-enabled`` command (so that a persistent mask can be distinguished\n        from a runtime mask). If the service is not masked, then ``False`` will\n        be returned.\n    .. versionchanged:: 2017.7.0\n        This function now returns a boolean telling the user whether a mask\n        specified by the new ``runtime`` argument is set. If ``runtime`` is\n        ``False``, this function will return ``True`` if an indefinite mask is\n        set for the named service (otherwise ``False`` will be returned). If\n        ``runtime`` is ``False``, this function will return ``True`` if a\n        runtime mask is set, otherwise ``False``.\n\n    Check whether or not a service is masked\n\n    runtime : False\n        Set to ``True`` to check for a runtime mask\n\n        .. versionadded:: 2017.7.0\n            In previous versions, this function would simply return the output\n            of ``systemctl is-enabled`` when the service was found to be\n            masked. However, since it is possible to both have both indefinite\n            and runtime masks on a service simultaneously, this function now\n            only checks for runtime masks if this argument is set to ``True``.\n            Otherwise, it will check for an indefinite mask.\n\n    root\n        Enable/disable/mask unit files in the specified root directory\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' service.masked foo\n        salt '*' service.masked foo runtime=True\n    '''\n    _check_for_unit_changes(name)\n    root_dir = _root('/run' if runtime else '/etc', root)\n    link_path = os.path.join(root_dir,\n                             'systemd',\n                             'system',\n                             _canonical_unit_name(name))\n    try:\n        return os.readlink(link_path) == '/dev/null'\n    except OSError as exc:\n        if exc.errno == errno.ENOENT:\n            log.trace(\n                'Path %s does not exist. This is normal if service \\'%s\\' is '\n                'not masked or does not exist.', link_path, name\n            )\n        elif exc.errno == errno.EINVAL:\n            log.error(\n                'Failed to check mask status for service %s. Path %s is a '\n                'file, not a symlink. This could be caused by changes in '\n                'systemd and is probably a bug in Salt. Please report this '\n                'to the developers.', name, link_path\n            )\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nstops a service with the specified name.", "response": "def stop(name, no_block=False):\n    '''\n    .. versionchanged:: 2015.8.12,2016.3.3,2016.11.0\n        On minions running systemd>=205, `systemd-run(1)`_ is now used to\n        isolate commands run by this function from the ``salt-minion`` daemon's\n        control group. This is done to avoid a race condition in cases where\n        the ``salt-minion`` service is restarted while a service is being\n        modified. If desired, usage of `systemd-run(1)`_ can be suppressed by\n        setting a :mod:`config option <salt.modules.config.get>` called\n        ``systemd.scope``, with a value of ``False`` (no quotes).\n\n    .. _`systemd-run(1)`: https://www.freedesktop.org/software/systemd/man/systemd-run.html\n\n    Stop the specified service with systemd\n\n    no_block : False\n        Set to ``True`` to start the service using ``--no-block``.\n\n        .. versionadded:: 2017.7.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.stop <service name>\n    '''\n    _check_for_unit_changes(name)\n    # Using cmd.run_all instead of cmd.retcode here to make unit tests easier\n    return __salt__['cmd.run_all'](\n        _systemctl_cmd('stop', name, systemd_scope=True, no_block=no_block),\n        python_shell=False)['retcode'] == 0"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef restart(name, no_block=False, unmask=False, unmask_runtime=False):\n    '''\n    .. versionchanged:: 2015.8.12,2016.3.3,2016.11.0\n        On minions running systemd>=205, `systemd-run(1)`_ is now used to\n        isolate commands run by this function from the ``salt-minion`` daemon's\n        control group. This is done to avoid a race condition in cases where\n        the ``salt-minion`` service is restarted while a service is being\n        modified. If desired, usage of `systemd-run(1)`_ can be suppressed by\n        setting a :mod:`config option <salt.modules.config.get>` called\n        ``systemd.scope``, with a value of ``False`` (no quotes).\n\n    .. _`systemd-run(1)`: https://www.freedesktop.org/software/systemd/man/systemd-run.html\n\n    Restart the specified service with systemd\n\n    no_block : False\n        Set to ``True`` to start the service using ``--no-block``.\n\n        .. versionadded:: 2017.7.0\n\n    unmask : False\n        Set to ``True`` to remove an indefinite mask before attempting to\n        restart the service.\n\n        .. versionadded:: 2017.7.0\n            In previous releases, Salt would simply unmask a service before\n            restarting. This behavior is no longer the default.\n\n    unmask_runtime : False\n        Set to ``True`` to remove a runtime mask before attempting to restart\n        the service.\n\n        .. versionadded:: 2017.7.0\n            In previous releases, Salt would simply unmask a service before\n            restarting. This behavior is no longer the default.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.restart <service name>\n    '''\n    _check_for_unit_changes(name)\n    _check_unmask(name, unmask, unmask_runtime)\n    ret = __salt__['cmd.run_all'](\n        _systemctl_cmd('restart', name, systemd_scope=True, no_block=no_block),\n        python_shell=False)\n\n    if ret['retcode'] != 0:\n        # Instead of returning a bool, raise an exception so that we can\n        # include the error message in the return data. This helps give more\n        # information to the user in instances where the service is masked.\n        raise CommandExecutionError(_strip_scope(ret['stderr']))\n    return True", "response": "Restart the specified service with the specified name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the status of a service via systemd.", "response": "def status(name, sig=None):  # pylint: disable=unused-argument\n    '''\n    Return the status for a service via systemd.\n    If the name contains globbing, a dict mapping service name to True/False\n    values is returned.\n\n    .. versionchanged:: 2018.3.0\n        The service name can now be a glob (e.g. ``salt*``)\n\n    Args:\n        name (str): The name of the service to check\n        sig (str): Not implemented\n\n    Returns:\n        bool: True if running, False otherwise\n        dict: Maps service name to True if running, False otherwise\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.status <service name> [service signature]\n    '''\n    contains_globbing = bool(re.search(r'\\*|\\?|\\[.+\\]', name))\n    if contains_globbing:\n        services = fnmatch.filter(get_all(), name)\n    else:\n        services = [name]\n    results = {}\n    for service in services:\n        _check_for_unit_changes(service)\n        results[service] = __salt__['cmd.retcode'](\n            _systemctl_cmd('is-active', service),\n            python_shell=False,\n            ignore_retcode=True) == 0\n    if contains_globbing:\n        return results\n    return results[name]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef enable(name, no_block=False, unmask=False, unmask_runtime=False,\n           root=None, **kwargs):  # pylint: disable=unused-argument\n    '''\n    .. versionchanged:: 2015.8.12,2016.3.3,2016.11.0\n        On minions running systemd>=205, `systemd-run(1)`_ is now used to\n        isolate commands run by this function from the ``salt-minion`` daemon's\n        control group. This is done to avoid a race condition in cases where\n        the ``salt-minion`` service is restarted while a service is being\n        modified. If desired, usage of `systemd-run(1)`_ can be suppressed by\n        setting a :mod:`config option <salt.modules.config.get>` called\n        ``systemd.scope``, with a value of ``False`` (no quotes).\n\n    .. _`systemd-run(1)`: https://www.freedesktop.org/software/systemd/man/systemd-run.html\n\n    Enable the named service to start when the system boots\n\n    no_block : False\n        Set to ``True`` to start the service using ``--no-block``.\n\n        .. versionadded:: 2017.7.0\n\n    unmask : False\n        Set to ``True`` to remove an indefinite mask before attempting to\n        enable the service.\n\n        .. versionadded:: 2017.7.0\n            In previous releases, Salt would simply unmask a service before\n            enabling. This behavior is no longer the default.\n\n    unmask_runtime : False\n        Set to ``True`` to remove a runtime mask before attempting to enable\n        the service.\n\n        .. versionadded:: 2017.7.0\n            In previous releases, Salt would simply unmask a service before\n            enabling. This behavior is no longer the default.\n\n    root\n        Enable/disable/mask unit files in the specified root directory\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.enable <service name>\n    '''\n    _check_for_unit_changes(name)\n    _check_unmask(name, unmask, unmask_runtime, root)\n    if name in _get_sysv_services(root):\n        cmd = []\n        if salt.utils.systemd.has_scope(__context__) \\\n                and __salt__['config.get']('systemd.scope', True):\n            cmd.extend(['systemd-run', '--scope'])\n        service_exec = _get_service_exec()\n        if service_exec.endswith('/update-rc.d'):\n            cmd.extend([service_exec, '-f', name, 'defaults', '99'])\n        elif service_exec.endswith('/chkconfig'):\n            cmd.extend([service_exec, name, 'on'])\n        return __salt__['cmd.retcode'](cmd,\n                                       python_shell=False,\n                                       ignore_retcode=True) == 0\n    ret = __salt__['cmd.run_all'](\n        _systemctl_cmd('enable', name, systemd_scope=True, no_block=no_block,\n                       root=root),\n        python_shell=False,\n        ignore_retcode=True)\n\n    if ret['retcode'] != 0:\n        # Instead of returning a bool, raise an exception so that we can\n        # include the error message in the return data. This helps give more\n        # information to the user in instances where the service is masked.\n        raise CommandExecutionError(_strip_scope(ret['stderr']))\n    return True", "response": "Enable or disable a named service in the specified root directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef disable(name, no_block=False, root=None, **kwargs):  # pylint: disable=unused-argument\n    '''\n    .. versionchanged:: 2015.8.12,2016.3.3,2016.11.0\n        On minions running systemd>=205, `systemd-run(1)`_ is now used to\n        isolate commands run by this function from the ``salt-minion`` daemon's\n        control group. This is done to avoid a race condition in cases where\n        the ``salt-minion`` service is restarted while a service is being\n        modified. If desired, usage of `systemd-run(1)`_ can be suppressed by\n        setting a :mod:`config option <salt.modules.config.get>` called\n        ``systemd.scope``, with a value of ``False`` (no quotes).\n\n    .. _`systemd-run(1)`: https://www.freedesktop.org/software/systemd/man/systemd-run.html\n\n    Disable the named service to not start when the system boots\n\n    no_block : False\n        Set to ``True`` to start the service using ``--no-block``.\n\n        .. versionadded:: 2017.7.0\n\n    root\n        Enable/disable/mask unit files in the specified root directory\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.disable <service name>\n    '''\n    _check_for_unit_changes(name)\n    if name in _get_sysv_services(root):\n        cmd = []\n        if salt.utils.systemd.has_scope(__context__) \\\n                and __salt__['config.get']('systemd.scope', True):\n            cmd.extend(['systemd-run', '--scope'])\n        service_exec = _get_service_exec()\n        if service_exec.endswith('/update-rc.d'):\n            cmd.extend([service_exec, '-f', name, 'remove'])\n        elif service_exec.endswith('/chkconfig'):\n            cmd.extend([service_exec, name, 'off'])\n        return __salt__['cmd.retcode'](cmd,\n                                       python_shell=False,\n                                       ignore_retcode=True) == 0\n    # Using cmd.run_all instead of cmd.retcode here to make unit tests easier\n    return __salt__['cmd.run_all'](\n        _systemctl_cmd('disable', name, systemd_scope=True, no_block=no_block,\n                       root=root),\n        python_shell=False,\n        ignore_retcode=True)['retcode'] == 0", "response": "Disable a named service in the specified root directory."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef enabled(name, root=None, **kwargs):  # pylint: disable=unused-argument\n    '''\n    Return if the named service is enabled to start on boot\n\n    root\n        Enable/disable/mask unit files in the specified root directory\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.enabled <service name>\n    '''\n    # Try 'systemctl is-enabled' first, then look for a symlink created by\n    # systemctl (older systemd releases did not support using is-enabled to\n    # check templated services), and lastly check for a sysvinit service.\n    if __salt__['cmd.retcode'](_systemctl_cmd('is-enabled', name, root=root),\n                               python_shell=False,\n                               ignore_retcode=True) == 0:\n        return True\n    elif '@' in name:\n        # On older systemd releases, templated services could not be checked\n        # with ``systemctl is-enabled``. As a fallback, look for the symlinks\n        # created by systemctl when enabling templated services.\n        local_config_path = _root(LOCAL_CONFIG_PATH, '/')\n        cmd = ['find', local_config_path, '-name', name,\n               '-type', 'l', '-print', '-quit']\n        # If the find command returns any matches, there will be output and the\n        # string will be non-empty.\n        if bool(__salt__['cmd.run'](cmd, python_shell=False)):\n            return True\n    elif name in _get_sysv_services(root):\n        return _sysv_enabled(name, root)\n\n    return False", "response": "Return if the named service is enabled to start on booton systemctl is - enabled or sysvinit is - enabled."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef show(name, root=None):\n    '''\n    .. versionadded:: 2014.7.0\n\n    Show properties of one or more units/jobs or the manager\n\n    root\n        Enable/disable/mask unit files in the specified root directory\n\n    CLI Example:\n\n        salt '*' service.show <service name>\n    '''\n    ret = {}\n    out = __salt__['cmd.run'](_systemctl_cmd('show', name, root=root),\n                              python_shell=False)\n    for line in salt.utils.itertools.split(out, '\\n'):\n        comps = line.split('=')\n        name = comps[0]\n        value = '='.join(comps[1:])\n        if value.startswith('{'):\n            value = value.replace('{', '').replace('}', '')\n            ret[name] = {}\n            for item in value.split(' ; '):\n                comps = item.split('=')\n                ret[name][comps[0].strip()] = comps[1].strip()\n        elif name in ('Before', 'After', 'Wants'):\n            ret[name] = value.split()\n        else:\n            ret[name] = value\n\n    return ret", "response": "Show properties of one or more units or jobs in a manager"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of all execs in the specified root directory", "response": "def execs(root=None):\n    '''\n    .. versionadded:: 2014.7.0\n\n    Return a list of all files specified as ``ExecStart`` for all services.\n\n    root\n        Enable/disable/mask unit files in the specified root directory\n\n    CLI Example:\n\n        salt '*' service.execs\n    '''\n    ret = {}\n    for service in get_all(root=root):\n        data = show(service, root=root)\n        if 'ExecStart' not in data:\n            continue\n        ret[service] = data['ExecStart']['path']\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmanages libvirt keys on the hypervisor", "response": "def keys(name, basepath='/etc/pki', **kwargs):\n    '''\n    Manage libvirt keys.\n\n    name\n        The name variable used to track the execution\n\n    basepath\n        Defaults to ``/etc/pki``, this is the root location used for libvirt\n        keys on the hypervisor\n\n    The following parameters are optional:\n\n        country\n            The country that the certificate should use.  Defaults to US.\n\n        .. versionadded:: 2018.3.0\n\n        state\n            The state that the certificate should use.  Defaults to Utah.\n\n        .. versionadded:: 2018.3.0\n\n        locality\n            The locality that the certificate should use.\n            Defaults to Salt Lake City.\n\n        .. versionadded:: 2018.3.0\n\n        organization\n            The organization that the certificate should use.\n            Defaults to Salted.\n\n        .. versionadded:: 2018.3.0\n\n        expiration_days\n            The number of days that the certificate should be valid for.\n            Defaults to 365 days (1 year)\n\n        .. versionadded:: 2018.3.0\n\n    '''\n    ret = {'name': name, 'changes': {}, 'result': True, 'comment': ''}\n\n    # Grab all kwargs to make them available as pillar values\n    # rename them to something hopefully unique to avoid\n    # overriding anything existing\n    pillar_kwargs = {}\n    for key, value in six.iteritems(kwargs):\n        pillar_kwargs['ext_pillar_virt.{0}'.format(key)] = value\n\n    pillar = __salt__['pillar.ext']({'libvirt': '_'}, pillar_kwargs)\n    paths = {\n        'serverkey': os.path.join(basepath, 'libvirt',\n                                  'private', 'serverkey.pem'),\n        'servercert': os.path.join(basepath, 'libvirt',\n                                   'servercert.pem'),\n        'clientkey': os.path.join(basepath, 'libvirt',\n                                  'private', 'clientkey.pem'),\n        'clientcert': os.path.join(basepath, 'libvirt',\n                                   'clientcert.pem'),\n        'cacert': os.path.join(basepath, 'CA', 'cacert.pem')\n    }\n\n    for key in paths:\n        p_key = 'libvirt.{0}.pem'.format(key)\n        if p_key not in pillar:\n            continue\n        if not os.path.exists(os.path.dirname(paths[key])):\n            os.makedirs(os.path.dirname(paths[key]))\n        if os.path.isfile(paths[key]):\n            with salt.utils.files.fopen(paths[key], 'r') as fp_:\n                if salt.utils.stringutils.to_unicode(fp_.read()) != pillar[p_key]:\n                    ret['changes'][key] = 'update'\n        else:\n            ret['changes'][key] = 'new'\n\n    if not ret['changes']:\n        ret['comment'] = 'All keys are correct'\n    elif __opts__['test']:\n        ret['result'] = None\n        ret['comment'] = 'Libvirt keys are set to be updated'\n        ret['changes'] = {}\n    else:\n        for key in ret['changes']:\n            with salt.utils.files.fopen(paths[key], 'w+') as fp_:\n                fp_.write(\n                    salt.utils.stringutils.to_str(\n                        pillar['libvirt.{0}.pem'.format(key)]\n                    )\n                )\n\n        ret['comment'] = 'Updated libvirt certs and keys'\n\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _virt_call(domain, function, section, comment,\n               connection=None, username=None, password=None, **kwargs):\n    '''\n    Helper to call the virt functions. Wildcards supported.\n\n    :param domain:\n    :param function:\n    :param section:\n    :param comment:\n    :return:\n    '''\n    ret = {'name': domain, 'changes': {}, 'result': True, 'comment': ''}\n    targeted_domains = fnmatch.filter(__salt__['virt.list_domains'](), domain)\n    changed_domains = list()\n    ignored_domains = list()\n    for targeted_domain in targeted_domains:\n        try:\n            response = __salt__['virt.{0}'.format(function)](targeted_domain,\n                                                             connection=connection,\n                                                             username=username,\n                                                             password=password,\n                                                             **kwargs)\n            if isinstance(response, dict):\n                response = response['name']\n            changed_domains.append({'domain': targeted_domain, function: response})\n        except libvirt.libvirtError as err:\n            ignored_domains.append({'domain': targeted_domain, 'issue': six.text_type(err)})\n    if not changed_domains:\n        ret['result'] = False\n        ret['comment'] = 'No changes had happened'\n        if ignored_domains:\n            ret['changes'] = {'ignored': ignored_domains}\n    else:\n        ret['changes'] = {section: changed_domains}\n        ret['comment'] = comment\n\n    return ret", "response": "Helper to call the virt functions. Wildcards supported."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nstop a VM by shutting it down nicely.", "response": "def stopped(name, connection=None, username=None, password=None):\n    '''\n    Stops a VM by shutting it down nicely.\n\n    .. versionadded:: 2016.3.0\n\n    :param connection: libvirt connection URI, overriding defaults\n\n        .. versionadded:: 2019.2.0\n    :param username: username to connect with, overriding defaults\n\n        .. versionadded:: 2019.2.0\n    :param password: password to connect with, overriding defaults\n\n        .. versionadded:: 2019.2.0\n\n    .. code-block:: yaml\n\n        domain_name:\n          virt.stopped\n    '''\n\n    return _virt_call(name, 'shutdown', 'stopped', \"Machine has been shut down\",\n                      connection=connection, username=username, password=password)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef powered_off(name, connection=None, username=None, password=None):\n    '''\n    Stops a VM by power off.\n\n    .. versionadded:: 2016.3.0\n\n    :param connection: libvirt connection URI, overriding defaults\n\n        .. versionadded:: 2019.2.0\n    :param username: username to connect with, overriding defaults\n\n        .. versionadded:: 2019.2.0\n    :param password: password to connect with, overriding defaults\n\n        .. versionadded:: 2019.2.0\n\n    .. code-block:: yaml\n\n        domain_name:\n          virt.stopped\n    '''\n\n    return _virt_call(name, 'stop', 'unpowered', 'Machine has been powered off',\n                      connection=connection, username=username, password=password)", "response": "Stops a VM by power off."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef running(name,\n            cpu=None,\n            mem=None,\n            image=None,\n            vm_type=None,\n            disk_profile=None,\n            disks=None,\n            nic_profile=None,\n            interfaces=None,\n            graphics=None,\n            loader=None,\n            seed=True,\n            install=True,\n            pub_key=None,\n            priv_key=None,\n            update=False,\n            connection=None,\n            username=None,\n            password=None,\n            os_type=None,\n            arch=None):\n    '''\n    Starts an existing guest, or defines and starts a new VM with specified arguments.\n\n    .. versionadded:: 2016.3.0\n\n    :param name: name of the virtual machine to run\n    :param cpu: number of CPUs for the virtual machine to create\n    :param mem: amount of memory in MiB for the new virtual machine\n    :param image: disk image to use for the first disk of the new VM\n\n        .. deprecated:: 2019.2.0\n    :param vm_type: force virtual machine type for the new VM. The default value is taken from\n        the host capabilities. This could be useful for example to use ``'qemu'`` type instead\n        of the ``'kvm'`` one.\n\n        .. versionadded:: 2019.2.0\n    :param disk_profile:\n        Name of the disk profile to use for the new virtual machine\n\n        .. versionadded:: 2019.2.0\n    :param disks:\n        List of disk to create for the new virtual machine.\n        See the **Disk Definitions** section of the :py:func:`virt.init\n        <salt.modules.virt.init>` function for more details on the items on\n        this list.\n\n        .. versionadded:: 2019.2.0\n    :param nic_profile:\n        Name of the network interfaces profile to use for the new virtual machine\n\n        .. versionadded:: 2019.2.0\n    :param interfaces:\n        List of network interfaces to create for the new virtual machine.\n        See the **Network Interface Definitions** section of the\n        :py:func:`virt.init <salt.modules.virt.init>` function for more details\n        on the items on this list.\n\n        .. versionadded:: 2019.2.0\n    :param graphics:\n        Graphics device to create for the new virtual machine.\n        See the **Graphics Definition** section of the :py:func:`virt.init\n        <salt.modules.virt.init>` function for more details on this dictionary.\n\n        .. versionadded:: 2019.2.0\n    :param loader:\n        Firmware loader for the new virtual machine.\n        See the **Loader Definition** section of the :py:func:`virt.init\n        <salt.modules.virt.init>` function for more details on this dictionary.\n\n        .. versionadded:: 2019.2.0\n    :param saltenv:\n        Fileserver environment (Default: ``'base'``).\n        See :mod:`cp module for more details <salt.modules.cp>`\n\n        .. versionadded:: 2019.2.0\n    :param seed: ``True`` to seed the disk image. Only used when the ``image`` parameter is provided.\n                 (Default: ``True``)\n\n        .. versionadded:: 2019.2.0\n    :param install: install salt minion if absent (Default: ``True``)\n\n        .. versionadded:: 2019.2.0\n    :param pub_key: public key to seed with (Default: ``None``)\n\n        .. versionadded:: 2019.2.0\n    :param priv_key: public key to seed with (Default: ``None``)\n\n        .. versionadded:: 2019.2.0\n    :param seed_cmd: Salt command to execute to seed the image. (Default: ``'seed.apply'``)\n\n        .. versionadded:: 2019.2.0\n    :param update: set to ``True`` to update a defined module. (Default: ``False``)\n\n        .. versionadded:: 2019.2.0\n    :param connection: libvirt connection URI, overriding defaults\n\n        .. versionadded:: 2019.2.0\n    :param username: username to connect with, overriding defaults\n\n        .. versionadded:: 2019.2.0\n    :param password: password to connect with, overriding defaults\n\n        .. versionadded:: 2019.2.0\n    :param os_type:\n        type of virtualization as found in the ``//os/type`` element of the libvirt definition.\n        The default value is taken from the host capabilities, with a preference for ``hvm``.\n        Only used when creating a new virtual machine.\n\n        .. versionadded:: Neon\n    :param arch:\n        architecture of the virtual machine. The default value is taken from the host capabilities,\n        but ``x86_64`` is prefed over ``i686``. Only used when creating a new virtual machine.\n\n        .. versionadded:: Neon\n\n    .. rubric:: Example States\n\n    Make sure an already-defined virtual machine called ``domain_name`` is running:\n\n    .. code-block:: yaml\n\n        domain_name:\n          virt.running\n\n    Do the same, but define the virtual machine if needed:\n\n    .. code-block:: yaml\n\n        domain_name:\n          virt.running:\n            - cpu: 2\n            - mem: 2048\n            - disk_profile: prod\n            - disks:\n              - name: system\n                size: 8192\n                overlay_image: True\n                pool: default\n                image: /path/to/image.qcow2\n              - name: data\n                size: 16834\n            - nic_profile: prod\n            - interfaces:\n              - name: eth0\n                mac: 01:23:45:67:89:AB\n              - name: eth1\n                type: network\n                source: admin\n            - graphics:\n              - type: spice\n                listen:\n                  - type: address\n                    address: 192.168.0.125\n\n    '''\n\n    ret = {'name': name,\n           'changes': {},\n           'result': True,\n           'comment': '{0} is running'.format(name)\n           }\n\n    try:\n        try:\n            __salt__['virt.vm_state'](name)\n            if __salt__['virt.vm_state'](name) != 'running':\n                action_msg = 'started'\n                if update:\n                    status = __salt__['virt.update'](name,\n                                                     cpu=cpu,\n                                                     mem=mem,\n                                                     disk_profile=disk_profile,\n                                                     disks=disks,\n                                                     nic_profile=nic_profile,\n                                                     interfaces=interfaces,\n                                                     graphics=graphics,\n                                                     live=False,\n                                                     connection=connection,\n                                                     username=username,\n                                                     password=password)\n                    if status['definition']:\n                        action_msg = 'updated and started'\n                __salt__['virt.start'](name)\n                ret['changes'][name] = 'Domain {0}'.format(action_msg)\n                ret['comment'] = 'Domain {0} {1}'.format(name, action_msg)\n            else:\n                if update:\n                    status = __salt__['virt.update'](name,\n                                                     cpu=cpu,\n                                                     mem=mem,\n                                                     disk_profile=disk_profile,\n                                                     disks=disks,\n                                                     nic_profile=nic_profile,\n                                                     interfaces=interfaces,\n                                                     graphics=graphics,\n                                                     connection=connection,\n                                                     username=username,\n                                                     password=password)\n                    ret['changes'][name] = status\n                    if status.get('errors', None):\n                        ret['comment'] = 'Domain {0} updated, but some live update(s) failed'.format(name)\n                    elif not status['definition']:\n                        ret['comment'] = 'Domain {0} exists and is running'.format(name)\n                    else:\n                        ret['comment'] = 'Domain {0} updated, restart to fully apply the changes'.format(name)\n                else:\n                    ret['comment'] = 'Domain {0} exists and is running'.format(name)\n        except CommandExecutionError:\n            if image:\n                salt.utils.versions.warn_until(\n                    'Sodium',\n                    '\\'image\\' parameter has been deprecated. Rather use the \\'disks\\' parameter '\n                    'to override or define the image. \\'image\\' will be removed in {version}.'\n                )\n            __salt__['virt.init'](name,\n                                  cpu=cpu,\n                                  mem=mem,\n                                  os_type=os_type,\n                                  arch=arch,\n                                  image=image,\n                                  hypervisor=vm_type,\n                                  disk=disk_profile,\n                                  disks=disks,\n                                  nic=nic_profile,\n                                  interfaces=interfaces,\n                                  graphics=graphics,\n                                  loader=loader,\n                                  seed=seed,\n                                  install=install,\n                                  pub_key=pub_key,\n                                  priv_key=priv_key,\n                                  connection=connection,\n                                  username=username,\n                                  password=password)\n            ret['changes'][name] = 'Domain defined and started'\n            ret['comment'] = 'Domain {0} defined and started'.format(name)\n    except libvirt.libvirtError as err:\n        # Something bad happened when starting / updating the VM, report it\n        ret['comment'] = six.text_type(err)\n        ret['result'] = False\n\n    return ret", "response": "Start a new virtual machine or defines and starts a new virtual machine."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef snapshot(name, suffix=None, connection=None, username=None, password=None):\n    '''\n    Takes a snapshot of a particular VM or by a UNIX-style wildcard.\n\n    .. versionadded:: 2016.3.0\n\n    :param connection: libvirt connection URI, overriding defaults\n\n        .. versionadded:: 2019.2.0\n    :param username: username to connect with, overriding defaults\n\n        .. versionadded:: 2019.2.0\n    :param password: password to connect with, overriding defaults\n\n        .. versionadded:: 2019.2.0\n\n    .. code-block:: yaml\n\n        domain_name:\n          virt.snapshot:\n            - suffix: periodic\n\n        domain*:\n          virt.snapshot:\n            - suffix: periodic\n    '''\n\n    return _virt_call(name, 'snapshot', 'saved', 'Snapshot has been taken', suffix=suffix,\n                      connection=connection, username=username, password=password)", "response": "Takes a snapshot of a particular VM or by a UNIX - style wildcard."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef rebooted(name, connection=None, username=None, password=None):\n    '''\n    Reboots VMs\n\n    .. versionadded:: 2016.3.0\n\n    :param name:\n\n    :param connection: libvirt connection URI, overriding defaults\n\n        .. versionadded:: 2019.2.0\n    :param username: username to connect with, overriding defaults\n\n        .. versionadded:: 2019.2.0\n    :param password: password to connect with, overriding defaults\n\n        .. versionadded:: 2019.2.0\n    '''\n\n    return _virt_call(name, 'reboot', 'rebooted', \"Machine has been rebooted\",\n                      connection=connection, username=username, password=password)", "response": "Check if a virtual machine has been rebooted."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a new virtual machine with the given name reverted to the particular snapshot.", "response": "def reverted(name, snapshot=None, cleanup=False):  # pylint: disable=redefined-outer-name\n    '''\n    .. deprecated:: 2016.3.0\n\n    Reverts to the particular snapshot.\n\n    .. versionadded:: 2016.3.0\n\n    .. code-block:: yaml\n\n        domain_name:\n          virt.reverted:\n            - cleanup: True\n\n        domain_name_1:\n          virt.reverted:\n            - snapshot: snapshot_name\n            - cleanup: False\n    '''\n    ret = {'name': name, 'changes': {}, 'result': False, 'comment': ''}\n\n    try:\n        domains = fnmatch.filter(__salt__['virt.list_domains'](), name)\n        if not domains:\n            ret['comment'] = 'No domains found for criteria \"{0}\"'.format(name)\n        else:\n            ignored_domains = list()\n            if len(domains) > 1:\n                ret['changes'] = {'reverted': list()}\n            for domain in domains:\n                result = {}\n                try:\n                    result = __salt__['virt.revert_snapshot'](domain, snapshot=snapshot, cleanup=cleanup)\n                    result = {'domain': domain, 'current': result['reverted'], 'deleted': result['deleted']}\n                except CommandExecutionError as err:\n                    if len(domains) > 1:\n                        ignored_domains.append({'domain': domain, 'issue': six.text_type(err)})\n                if len(domains) > 1:\n                    if result:\n                        ret['changes']['reverted'].append(result)\n                else:\n                    ret['changes'] = result\n                    break\n\n            ret['result'] = len(domains) != len(ignored_domains)\n            if ret['result']:\n                ret['comment'] = 'Domain{0} has been reverted'.format(len(domains) > 1 and \"s\" or \"\")\n            if ignored_domains:\n                ret['changes']['ignored'] = ignored_domains\n            if not ret['changes']['reverted']:\n                ret['changes'].pop('reverted')\n    except libvirt.libvirtError as err:\n        ret['comment'] = six.text_type(err)\n    except CommandExecutionError as err:\n        ret['comment'] = six.text_type(err)\n\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmakes sure that the network is running.", "response": "def network_running(name,\n                    bridge,\n                    forward,\n                    vport=None,\n                    tag=None,\n                    autostart=True,\n                    connection=None,\n                    username=None,\n                    password=None):\n    '''\n    Defines and starts a new network with specified arguments.\n\n    :param connection: libvirt connection URI, overriding defaults\n\n        .. versionadded:: 2019.2.0\n    :param username: username to connect with, overriding defaults\n\n        .. versionadded:: 2019.2.0\n    :param password: password to connect with, overriding defaults\n\n        .. versionadded:: 2019.2.0\n\n    .. code-block:: yaml\n\n        domain_name:\n          virt.network_define\n\n    .. code-block:: yaml\n\n        network_name:\n          virt.network_define:\n            - bridge: main\n            - forward: bridge\n            - vport: openvswitch\n            - tag: 180\n            - autostart: True\n\n    '''\n    ret = {'name': name,\n           'changes': {},\n           'result': True,\n           'comment': ''\n           }\n\n    try:\n        info = __salt__['virt.network_info'](name, connection=connection, username=username, password=password)\n        if info:\n            if info['active']:\n                ret['comment'] = 'Network {0} exists and is running'.format(name)\n            else:\n                __salt__['virt.network_start'](name, connection=connection, username=username, password=password)\n                ret['changes'][name] = 'Network started'\n                ret['comment'] = 'Network {0} started'.format(name)\n        else:\n            __salt__['virt.network_define'](name,\n                                            bridge,\n                                            forward,\n                                            vport,\n                                            tag=tag,\n                                            autostart=autostart,\n                                            start=True,\n                                            connection=connection,\n                                            username=username,\n                                            password=password)\n            ret['changes'][name] = 'Network defined and started'\n            ret['comment'] = 'Network {0} defined and started'.format(name)\n    except libvirt.libvirtError as err:\n        ret['result'] = False\n        ret['comment'] = err.get_error_message()\n\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef pool_running(name,\n                 ptype=None,\n                 target=None,\n                 permissions=None,\n                 source=None,\n                 transient=False,\n                 autostart=True,\n                 connection=None,\n                 username=None,\n                 password=None):\n    '''\n    Defines and starts a new pool with specified arguments.\n\n    .. versionadded:: 2019.2.0\n\n    :param ptype: libvirt pool type\n    :param target: full path to the target device or folder. (Default: ``None``)\n    :param permissions: target permissions. See the **Permissions definition**\n        section of the :py:func:`virt.pool_define\n        <salt.module.virt.pool_define>` documentation for more details on this\n        structure.\n    :param source:\n        dictionary containing keys matching the ``source_*`` parameters in function\n        :py:func:`virt.pool_define <salt.modules.virt.pool_define>`.\n    :param transient:\n        when set to ``True``, the pool will be automatically undefined after\n        being stopped. (Default: ``False``)\n    :param autostart:\n        Whether to start the pool when booting the host. (Default: ``True``)\n    :param start:\n        When ``True``, define and start the pool, otherwise the pool will be\n        left stopped.\n    :param connection: libvirt connection URI, overriding defaults\n    :param username: username to connect with, overriding defaults\n    :param password: password to connect with, overriding defaults\n\n    .. code-block:: yaml\n\n        pool_name:\n          virt.pool_define\n\n    .. code-block:: yaml\n\n        pool_name:\n          virt.pool_define:\n            - ptype: netfs\n            - target: /mnt/cifs\n            - permissions:\n                - mode: 0770\n                - owner: 1000\n                - group: 100\n            - source:\n                - dir: samba_share\n                - hosts:\n                   one.example.com\n                   two.example.com\n                - format: cifs\n            - autostart: True\n\n    '''\n    ret = {'name': name,\n           'changes': {},\n           'result': True,\n           'comment': ''\n           }\n\n    try:\n        info = __salt__['virt.pool_info'](name, connection=connection, username=username, password=password)\n        if info:\n            if info['state'] == 'running':\n                ret['comment'] = 'Pool {0} exists and is running'.format(name)\n            else:\n                __salt__['virt.pool_start'](name, connection=connection, username=username, password=password)\n                ret['changes'][name] = 'Pool started'\n                ret['comment'] = 'Pool {0} started'.format(name)\n        else:\n            __salt__['virt.pool_define'](name,\n                                         ptype=ptype,\n                                         target=target,\n                                         permissions=permissions,\n                                         source_devices=(source or {}).get('devices', None),\n                                         source_dir=(source or {}).get('dir', None),\n                                         source_adapter=(source or {}).get('adapter', None),\n                                         source_hosts=(source or {}).get('hosts', None),\n                                         source_auth=(source or {}).get('auth', None),\n                                         source_name=(source or {}).get('name', None),\n                                         source_format=(source or {}).get('format', None),\n                                         transient=transient,\n                                         start=True,\n                                         connection=connection,\n                                         username=username,\n                                         password=password)\n            if autostart:\n                __salt__['virt.pool_set_autostart'](name,\n                                                    state='on' if autostart else 'off',\n                                                    connection=connection,\n                                                    username=username,\n                                                    password=password)\n\n            __salt__['virt.pool_build'](name,\n                                        connection=connection,\n                                        username=username,\n                                        password=password)\n\n            __salt__['virt.pool_start'](name,\n                                        connection=connection,\n                                        username=username,\n                                        password=password)\n            ret['changes'][name] = 'Pool defined and started'\n            ret['comment'] = 'Pool {0} defined and started'.format(name)\n    except libvirt.libvirtError as err:\n        ret['comment'] = err.get_error_message()\n        ret['result'] = False\n\n    return ret", "response": "Defines and starts a new libvirt pool with specified arguments."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef status(name, sig=None):\n    '''\n    Return ``True`` if service is running\n\n    name\n        the service's name\n\n    sig\n        signature to identify with ps\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' runit.status <service name>\n    '''\n    if sig:\n        # usual way to do by others (debian_service, netbsdservice).\n        # XXX probably does not work here (check 'runsv sshd' instead of 'sshd' ?)\n        return bool(__salt__['status.pid'](sig))\n\n    svc_path = _service_path(name)\n    if not os.path.exists(svc_path):\n        # service does not exist\n        return False\n\n    # sv return code is not relevant to get a service status.\n    # Check its output instead.\n    cmd = 'sv status {0}'.format(svc_path)\n    try:\n        out = __salt__['cmd.run_stdout'](cmd)\n        return out.startswith('run: ')\n    except Exception:\n        # sv (as a command) returned an error\n        return False", "response": "Return True if service is running and if service is running"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _is_svc(svc_path):\n    '''\n    Return ``True`` if directory <svc_path> is really a service:\n    file <svc_path>/run exists and is executable\n\n    svc_path\n        the (absolute) directory to check for compatibility\n    '''\n    run_file = os.path.join(svc_path, 'run')\n    if (os.path.exists(svc_path)\n         and os.path.exists(run_file)\n         and os.access(run_file, os.X_OK)):\n        return True\n    return False", "response": "Return True if directory <svc_path > is really a service"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef status_autostart(name):\n    '''\n    Return ``True`` if service <name> is autostarted by sv\n    (file $service_folder/down does not exist)\n    NB: return ``False`` if the service is not enabled.\n\n    name\n        the service's name\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' runit.status_autostart <service name>\n    '''\n    return not os.path.exists(os.path.join(_service_path(name), 'down'))", "response": "Return True if service <name > is autostarted by sv\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning list of broken path in SERVICE_DIR that match name", "response": "def get_svc_broken_path(name='*'):\n    '''\n    Return list of broken path(s) in SERVICE_DIR that match ``name``\n\n    A path is broken if it is a broken symlink or can not be a runit service\n\n    name\n        a glob for service name. default is '*'\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' runit.get_svc_broken_path <service name>\n    '''\n    if not SERVICE_DIR:\n        raise CommandExecutionError('Could not find service directory.')\n\n    ret = set()\n\n    for el in glob.glob(os.path.join(SERVICE_DIR, name)):\n        if not _is_svc(el):\n            ret.add(el)\n    return sorted(ret)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_svc_avail_path(path):\n    '''\n    Add a path that may contain available services.\n    Return ``True`` if added (or already present), ``False`` on error.\n\n    path\n        directory to add to AVAIL_SVR_DIRS\n    '''\n    if os.path.exists(path):\n        if path not in AVAIL_SVR_DIRS:\n            AVAIL_SVR_DIRS.append(path)\n        return True\n    return False", "response": "Adds a path that may contain available services. Returns True if added False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_svc_path(name='*', status=None):\n    '''\n    Return a list of paths to services with ``name`` that have the specified ``status``\n\n    name\n        a glob for service name. default is '*'\n\n    status\n        None       : all services (no filter, default choice)\n        'DISABLED' : available service(s) that is not enabled\n        'ENABLED'  : enabled service (whether started on boot or not)\n    '''\n\n    # This is the core routine to work with services, called by many\n    # other functions of this module.\n    #\n    # The name of a service is the \"apparent\" folder's name that contains its\n    # \"run\" script. If its \"folder\" is a symlink, the service is an \"alias\" of\n    # the targeted service.\n\n    if not SERVICE_DIR:\n        raise CommandExecutionError('Could not find service directory.')\n\n    # path list of enabled services as /AVAIL_SVR_DIRS/$service,\n    # taking care of any service aliases (do not use os.path.realpath()).\n    ena = set()\n    for el in glob.glob(os.path.join(SERVICE_DIR, name)):\n        if _is_svc(el):\n            ena.add(os.readlink(el))\n            log.trace('found enabled service path: %s', el)\n\n    if status == 'ENABLED':\n        return sorted(ena)\n\n    # path list of available services as /AVAIL_SVR_DIRS/$service\n    ava = set()\n    for d in AVAIL_SVR_DIRS:\n        for el in glob.glob(os.path.join(d, name)):\n            if _is_svc(el):\n                ava.add(el)\n                log.trace('found available service path: %s', el)\n\n    if status == 'DISABLED':\n        # service available but not enabled\n        ret = ava.difference(ena)\n    else:\n        # default: return available services\n        ret = ava.union(ena)\n\n    return sorted(ret)", "response": "Return a list of paths to services with the specified status"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_svc_list(name='*', status=None):\n    '''\n    Return list of services that have the specified service ``status``\n\n    name\n        a glob for service name. default is '*'\n\n    status\n        None       : all services (no filter, default choice)\n        'DISABLED' : available service that is not enabled\n        'ENABLED'  : enabled service (whether started on boot or not)\n    '''\n    return sorted([os.path.basename(el) for el in _get_svc_path(name, status)])", "response": "Return a list of services that have the specified status"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_svc_alias():\n    '''\n    Returns the list of service's name that are aliased and their alias path(s)\n    '''\n\n    ret = {}\n    for d in AVAIL_SVR_DIRS:\n        for el in glob.glob(os.path.join(d, '*')):\n            if not os.path.islink(el):\n                continue\n            psvc = os.readlink(el)\n            if not os.path.isabs(psvc):\n                psvc = os.path.join(d, psvc)\n            nsvc = os.path.basename(psvc)\n            if nsvc not in ret:\n                ret[nsvc] = []\n            ret[nsvc].append(el)\n    return ret", "response": "Returns the list of service s name that are aliased and their alias path ( s )"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nshows properties of one or more units or jobs or the manager name the service s name CLI Example : salt '*' service. show <service name >", "response": "def show(name):\n    '''\n    Show properties of one or more units/jobs or the manager\n\n    name\n        the service's name\n\n    CLI Example:\n\n        salt '*' service.show <service name>\n    '''\n    ret = {}\n    ret['enabled'] = False\n    ret['disabled'] = True\n    ret['running'] = False\n    ret['service_path'] = None\n    ret['autostart'] = False\n    ret['command_path'] = None\n\n    ret['available'] = available(name)\n    if not ret['available']:\n        return ret\n\n    ret['enabled'] = enabled(name)\n    ret['disabled'] = not ret['enabled']\n    ret['running'] = status(name)\n    ret['autostart'] = status_autostart(name)\n    ret['service_path'] = _get_svc_path(name)[0]\n    if ret['service_path']:\n        ret['command_path'] = os.path.join(ret['service_path'], 'run')\n\n    # XXX provide info about alias ?\n\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nenabling a service at boot.", "response": "def enable(name, start=False, **kwargs):\n    '''\n    Start service ``name`` at boot.\n    Returns ``True`` if operation is successful\n\n    name\n        the service's name\n\n    start : False\n        If ``True``, start the service once enabled.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.enable <name> [start=True]\n    '''\n\n    # non-existent service\n    if not available(name):\n        return False\n\n    # if service is aliased, refuse to enable it\n    alias = get_svc_alias()\n    if name in alias:\n        log.error('This service is aliased, enable its alias instead')\n        return False\n\n    # down_file: file that disables sv autostart\n    svc_realpath = _get_svc_path(name)[0]\n    down_file = os.path.join(svc_realpath, 'down')\n\n    # if service already enabled, remove down_file to\n    # let service starts on boot (as requested)\n    if enabled(name):\n        if os.path.exists(down_file):\n            try:\n                os.unlink(down_file)\n            except OSError:\n                log.error('Unable to remove file %s', down_file)\n                return False\n        return True\n\n    # let's enable the service\n\n    if not start:\n        # create a temp 'down' file BEFORE enabling service.\n        # will prevent sv from starting this service automatically.\n        log.trace('need a temporary file %s', down_file)\n        if not os.path.exists(down_file):\n            try:\n                salt.utils.files.fopen(down_file, \"w\").close()  # pylint: disable=resource-leakage\n            except IOError:\n                log.error('Unable to create file %s', down_file)\n                return False\n\n    # enable the service\n    try:\n        os.symlink(svc_realpath, _service_path(name))\n\n    except IOError:\n        # (attempt to) remove temp down_file anyway\n        log.error('Unable to create symlink %s', down_file)\n        if not start:\n            os.unlink(down_file)\n        return False\n\n    # ensure sv is aware of this new service before continuing.\n    # if not, down_file might be removed too quickly,\n    # before 'sv' have time to take care about it.\n    # Documentation indicates that a change is handled within 5 seconds.\n    cmd = 'sv status {0}'.format(_service_path(name))\n    retcode_sv = 1\n    count_sv = 0\n    while retcode_sv != 0 and count_sv < 10:\n        time.sleep(0.5)\n        count_sv += 1\n        call = __salt__['cmd.run_all'](cmd)\n        retcode_sv = call['retcode']\n\n    # remove the temp down_file in any case.\n    if (not start) and os.path.exists(down_file):\n        try:\n            os.unlink(down_file)\n        except OSError:\n            log.error('Unable to remove temp file %s', down_file)\n            retcode_sv = 1\n\n    # if an error happened, revert our changes\n    if retcode_sv != 0:\n        os.unlink(os.path.join([_service_path(name), name]))\n        return False\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndisable a service by name", "response": "def disable(name, stop=False, **kwargs):\n    '''\n    Don't start service ``name`` at boot\n    Returns ``True`` if operation is successful\n\n    name\n        the service's name\n\n    stop\n        if True, also stops the service\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.disable <name> [stop=True]\n    '''\n\n    # non-existent as registrered service\n    if not enabled(name):\n        return False\n\n    # down_file: file that prevent sv autostart\n    svc_realpath = _get_svc_path(name)[0]\n    down_file = os.path.join(svc_realpath, 'down')\n\n    if stop:\n        stop(name)\n\n    if not os.path.exists(down_file):\n        try:\n            salt.utils.files.fopen(down_file, \"w\").close()  # pylint: disable=resource-leakage\n        except IOError:\n            log.error('Unable to create file %s', down_file)\n            return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nremoving the service named name from system.", "response": "def remove(name):\n    '''\n    Remove the service <name> from system.\n    Returns ``True`` if operation is successful.\n    The service will be also stopped.\n\n    name\n        the service's name\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.remove <name>\n    '''\n\n    if not enabled(name):\n        return False\n\n    svc_path = _service_path(name)\n    if not os.path.islink(svc_path):\n        log.error('%s is not a symlink: not removed', svc_path)\n        return False\n\n    if not stop(name):\n        log.error('Failed to stop service %s', name)\n        return False\n    try:\n        os.remove(svc_path)\n    except IOError:\n        log.error('Unable to remove symlink %s', svc_path)\n        return False\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ext_pillar(minion_id,\n               pillar,  # pylint: disable=W0613\n               bucket,\n               key=None,\n               keyid=None,\n               verify_ssl=True,\n               location=None,\n               multiple_env=False,\n               environment='base',\n               prefix='',\n               service_url=None,\n               kms_keyid=None,\n               s3_cache_expire=30,  # cache for 30 seconds\n               s3_sync_on_update=True,  # sync cache on update rather than jit\n               path_style=False,\n               https_enable=True):\n\n    '''\n    Execute a command and read the output as YAML\n    '''\n\n    s3_creds = S3Credentials(key, keyid, bucket, service_url, verify_ssl,\n                             kms_keyid, location, path_style, https_enable)\n\n    # normpath is needed to remove appended '/' if root is empty string.\n    pillar_dir = os.path.normpath(os.path.join(_get_cache_dir(), environment,\n                                               bucket))\n    if prefix:\n        pillar_dir = os.path.normpath(os.path.join(pillar_dir, prefix))\n\n    if __opts__['pillar_roots'].get(environment, []) == [pillar_dir]:\n        return {}\n\n    metadata = _init(s3_creds, bucket, multiple_env, environment, prefix, s3_cache_expire)\n\n    if s3_sync_on_update:\n        # sync the buckets to the local cache\n        log.info('Syncing local pillar cache from S3...')\n        for saltenv, env_meta in six.iteritems(metadata):\n            for bucket, files in six.iteritems(_find_files(env_meta)):\n                for file_path in files:\n                    cached_file_path = _get_cached_file_name(bucket, saltenv,\n                                                             file_path)\n                    log.info('%s - %s : %s', bucket, saltenv, file_path)\n                    # load the file from S3 if not in the cache or too old\n                    _get_file_from_s3(s3_creds, metadata, saltenv, bucket,\n                                      file_path, cached_file_path)\n\n        log.info('Sync local pillar cache from S3 completed.')\n\n    opts = deepcopy(__opts__)\n    opts['pillar_roots'][environment] = [os.path.join(pillar_dir, environment)] if multiple_env else [pillar_dir]\n\n    # Avoid recursively re-adding this same pillar\n    opts['ext_pillar'] = [x for x in opts['ext_pillar'] if 's3' not in x]\n\n    pil = Pillar(opts, __grains__, minion_id, environment)\n\n    compiled_pillar = pil.compile_pillar(ext=False)\n\n    return compiled_pillar", "response": "Execute a command and read the output as YAML and return the resulting pillar."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _init(creds, bucket, multiple_env, environment, prefix, s3_cache_expire):\n    '''\n    Connect to S3 and download the metadata for each file in all buckets\n    specified and cache the data to disk.\n    '''\n\n    cache_file = _get_buckets_cache_filename(bucket, prefix)\n    exp = time.time() - s3_cache_expire\n\n    # check if cache_file exists and its mtime\n    if os.path.isfile(cache_file):\n        cache_file_mtime = os.path.getmtime(cache_file)\n    else:\n        # file does not exists then set mtime to 0 (aka epoch)\n        cache_file_mtime = 0\n\n    expired = (cache_file_mtime <= exp)\n\n    log.debug(\n        'S3 bucket cache file %s is %sexpired, mtime_diff=%ss, expiration=%ss',\n        cache_file,\n        '' if expired else 'not ',\n        cache_file_mtime - exp,\n        s3_cache_expire\n    )\n\n    if expired:\n        pillars = _refresh_buckets_cache_file(creds, cache_file, multiple_env,\n                                           environment, prefix)\n    else:\n        pillars = _read_buckets_cache_file(cache_file)\n\n    log.debug('S3 bucket retrieved pillars %s', pillars)\n    return pillars", "response": "Initialize the pillars for the specified bucket."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets pillar cache directory. Initialize it if it does not exist.", "response": "def _get_cache_dir():\n    '''\n    Get pillar cache directory. Initialize it if it does not exist.\n    '''\n\n    cache_dir = os.path.join(__opts__['cachedir'], 'pillar_s3fs')\n\n    if not os.path.isdir(cache_dir):\n        log.debug('Initializing S3 Pillar Cache')\n        os.makedirs(cache_dir)\n\n    return cache_dir"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the filename of the cache for the given bucket and prefix.", "response": "def _get_buckets_cache_filename(bucket, prefix):\n    '''\n    Return the filename of the cache for bucket contents.\n    Create the path if it does not exist.\n    '''\n\n    cache_dir = _get_cache_dir()\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n\n    return os.path.join(cache_dir, '{0}-{1}-files.cache'.format(bucket, prefix))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _refresh_buckets_cache_file(creds, cache_file, multiple_env, environment, prefix):\n    '''\n    Retrieve the content of all buckets and cache the metadata to the buckets\n    cache file\n    '''\n\n    # helper s3 query function\n    def __get_s3_meta():\n        return __utils__['s3.query'](\n            key=creds.key,\n            keyid=creds.keyid,\n            kms_keyid=creds.kms_keyid,\n            bucket=creds.bucket,\n            service_url=creds.service_url,\n            verify_ssl=creds.verify_ssl,\n            location=creds.location,\n            return_bin=False,\n            params={'prefix': prefix},\n            path_style=creds.path_style,\n            https_enable=creds.https_enable)\n\n    # grab only the files/dirs in the bucket\n    def __get_pillar_files_from_s3_meta(s3_meta):\n        return [k for k in s3_meta if 'Key' in k]\n\n    # pull out the environment dirs (e.g. the root dirs)\n    def __get_pillar_environments(files):\n        environments = [(os.path.dirname(k['Key']).split('/', 1))[0] for k in files]\n        return set(environments)\n\n    log.debug('Refreshing S3 buckets pillar cache file')\n\n    metadata = {}\n    bucket = creds.bucket\n\n    if not multiple_env:\n        # Single environment per bucket\n        log.debug('Single environment per bucket mode')\n\n        bucket_files = {}\n        s3_meta = __get_s3_meta()\n\n        # s3 query returned something\n        if s3_meta:\n            bucket_files[bucket] = __get_pillar_files_from_s3_meta(s3_meta)\n\n            metadata[environment] = bucket_files\n\n    else:\n        # Multiple environments per buckets\n        log.debug('Multiple environment per bucket mode')\n        s3_meta = __get_s3_meta()\n\n        # s3 query returned data\n        if s3_meta:\n            files = __get_pillar_files_from_s3_meta(s3_meta)\n            environments = __get_pillar_environments(files)\n\n            # pull out the files for the environment\n            for saltenv in environments:\n                # grab only files/dirs that match this saltenv.\n                env_files = [k for k in files if k['Key'].startswith(saltenv)]\n\n                if saltenv not in metadata:\n                    metadata[saltenv] = {}\n\n                if bucket not in metadata[saltenv]:\n                    metadata[saltenv][bucket] = []\n\n                metadata[saltenv][bucket] += env_files\n\n    # write the metadata to disk\n    if os.path.isfile(cache_file):\n        os.remove(cache_file)\n\n    log.debug('Writing S3 buckets pillar cache file')\n\n    with salt.utils.files.fopen(cache_file, 'w') as fp_:\n        pickle.dump(metadata, fp_)\n\n    return metadata", "response": "Retrieve the content of all buckets and cache the metadata to the buckets\n    cache file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread the contents of the buckets cache file", "response": "def _read_buckets_cache_file(cache_file):\n    '''\n    Return the contents of the buckets cache file\n    '''\n\n    log.debug('Reading buckets cache file')\n\n    with salt.utils.files.fopen(cache_file, 'rb') as fp_:\n        data = pickle.load(fp_)\n\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _find_files(metadata):\n    '''\n    Looks for all the files in the S3 bucket cache metadata\n    '''\n\n    ret = {}\n\n    for bucket, data in six.iteritems(metadata):\n        if bucket not in ret:\n            ret[bucket] = []\n\n        # grab the paths from the metadata\n        filePaths = [k['Key'] for k in data]\n        # filter out the dirs\n        ret[bucket] += [k for k in filePaths if not k.endswith('/')]\n\n    return ret", "response": "Find all the files in the S3 bucket cache metadata"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the file from S3", "response": "def _get_file_from_s3(creds, metadata, saltenv, bucket, path,\n                      cached_file_path):\n    '''\n    Checks the local cache for the file, if it's old or missing go grab the\n    file from S3 and update the cache\n    '''\n\n    # check the local cache...\n    if os.path.isfile(cached_file_path):\n        file_meta = _find_file_meta(metadata, bucket, saltenv, path)\n        file_md5 = \"\".join(list(filter(str.isalnum, file_meta['ETag']))) \\\n            if file_meta else None\n\n        cached_md5 = salt.utils.hashutils.get_hash(cached_file_path, 'md5')\n\n        # hashes match we have a cache hit\n        log.debug('Cached file: path=%s, md5=%s, etag=%s',\n                  cached_file_path, cached_md5, file_md5)\n        if cached_md5 == file_md5:\n            return\n\n    # ... or get the file from S3\n    __utils__['s3.query'](\n        key=creds.key,\n        keyid=creds.keyid,\n        kms_keyid=creds.kms_keyid,\n        bucket=bucket,\n        service_url=creds.service_url,\n        path=_quote(path),\n        local_file=cached_file_path,\n        verify_ssl=creds.verify_ssl,\n        location=creds.location,\n        path_style=creds.path_style,\n        https_enable=creds.https_enable\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _arg2opt(arg):\n    '''\n    Turn a pass argument into the correct option\n    '''\n    res = [o for o, a in option_toggles.items() if a == arg]\n    res += [o for o, a in option_flags.items() if a == arg]\n    return res[0] if res else None", "response": "Turn a pass argument into the correct option\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _parse_conf(conf_file=default_conf):\n    '''\n    Parse a logadm configuration file.\n    '''\n    ret = {}\n    with salt.utils.files.fopen(conf_file, 'r') as ifile:\n        for line in ifile:\n            line = salt.utils.stringutils.to_unicode(line).strip()\n            if not line:\n                continue\n            if line.startswith('#'):\n                continue\n            splitline = line.split(' ', 1)\n            ret[splitline[0]] = splitline[1]\n    return ret", "response": "Parse a logadm configuration file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _parse_options(entry, options, include_unset=True):\n    '''\n    Parse a logadm options string\n    '''\n    log_cfg = {}\n    options = shlex.split(options)\n    if not options:\n        return None\n\n    ## identifier is entry or log?\n    if entry.startswith('/'):\n        log_cfg['log_file'] = entry\n    else:\n        log_cfg['entryname'] = entry\n\n    ## parse options\n    # NOTE: we loop over the options because values may exist multiple times\n    index = 0\n    while index < len(options):\n        # log file\n        if index in [0, (len(options)-1)] and options[index].startswith('/'):\n            log_cfg['log_file'] = options[index]\n\n        # check if toggle option\n        elif options[index] in option_toggles:\n            log_cfg[option_toggles[options[index]]] = True\n\n        # check if flag option\n        elif options[index] in option_flags and (index+1) <= len(options):\n            log_cfg[option_flags[options[index]]] = int(options[index+1]) if options[index+1].isdigit() else options[index+1]\n            index += 1\n\n        # unknown options\n        else:\n            if 'additional_options' not in log_cfg:\n                log_cfg['additional_options'] = []\n            if ' ' in options[index]:\n                log_cfg['dditional_options'] = \"'{}'\".format(options[index])\n            else:\n                log_cfg['additional_options'].append(options[index])\n\n        index += 1\n\n    ## turn additional_options into string\n    if 'additional_options' in log_cfg:\n        log_cfg['additional_options'] = \" \".join(log_cfg['additional_options'])\n\n    ## ensure we have a log_file\n    # NOTE: logadm assumes logname is a file if no log_file is given\n    if 'log_file' not in log_cfg and 'entryname' in log_cfg:\n        log_cfg['log_file'] = log_cfg['entryname']\n        del log_cfg['entryname']\n\n    ## include unset\n    if include_unset:\n        # toggle optioons\n        for name in option_toggles.values():\n            if name not in log_cfg:\n                log_cfg[name] = False\n\n        # flag options\n        for name in option_flags.values():\n            if name not in log_cfg:\n                log_cfg[name] = None\n\n    return log_cfg", "response": "Parse a logadm options string and return a dictionary of logadm options."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef show_conf(conf_file=default_conf, name=None):\n    '''\n    Show configuration\n\n    conf_file : string\n        path to logadm.conf, defaults to /etc/logadm.conf\n    name : string\n        optional show only a single entry\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' logadm.show_conf\n        salt '*' logadm.show_conf name=/var/log/syslog\n    '''\n    cfg = _parse_conf(conf_file)\n\n    # filter\n    if name and name in cfg:\n        return {name: cfg[name]}\n    elif name:\n        return {name: 'not found in {}'.format(conf_file)}\n    else:\n        return cfg", "response": "Show configuration of logadm. conf"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nshows parsed configuration file and all options in logadm. conf", "response": "def list_conf(conf_file=default_conf, log_file=None, include_unset=False):\n    '''\n    Show parsed configuration\n\n    .. versionadded:: 2018.3.0\n\n    conf_file : string\n        path to logadm.conf, defaults to /etc/logadm.conf\n    log_file : string\n        optional show only one log file\n    include_unset : boolean\n        include unset flags in output\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' logadm.list_conf\n        salt '*' logadm.list_conf log=/var/log/syslog\n        salt '*' logadm.list_conf include_unset=False\n    '''\n    cfg = _parse_conf(conf_file)\n    cfg_parsed = {}\n\n    ## parse all options\n    for entry in cfg:\n        log_cfg = _parse_options(entry, cfg[entry], include_unset)\n        cfg_parsed[log_cfg['log_file'] if 'log_file' in log_cfg else log_cfg['entryname']] = log_cfg\n\n    ## filter\n    if log_file and log_file in cfg_parsed:\n        return {log_file: cfg_parsed[log_file]}\n    elif log_file:\n        return {log_file: 'not found in {}'.format(conf_file)}\n    else:\n        return cfg_parsed"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nshows which arguments map to which flags and options are used.", "response": "def show_args():\n    '''\n    Show which arguments map to which flags and options.\n\n    .. versionadded:: 2018.3.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' logadm.show_args\n    '''\n    mapping = {'flags': {}, 'options': {}}\n    for flag, arg in option_toggles.items():\n        mapping['flags'][flag] = arg\n    for option, arg in option_flags.items():\n        mapping['options'][option] = arg\n\n    return mapping"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a new log file with the given name and pattern.", "response": "def rotate(name, pattern=None, conf_file=default_conf, **kwargs):\n    '''\n    Set up pattern for logging.\n\n    name : string\n        alias for entryname\n    pattern : string\n        alias for log_file\n    conf_file : string\n        optional path to alternative configuration file\n    kwargs : boolean|string|int\n        optional additional flags and parameters\n\n    .. note::\n        ``name`` and ``pattern`` were kept for backwards compatibility reasons.\n\n        ``name`` is an alias for the ``entryname`` argument, ``pattern`` is an alias\n        for ``log_file``. These aliases will only be used if the ``entryname`` and\n        ``log_file`` arguments are not passed.\n\n        For a full list of arguments see ```logadm.show_args```.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' logadm.rotate myapplog pattern='/var/log/myapp/*.log' count=7\n        salt '*' logadm.rotate myapplog log_file='/var/log/myapp/*.log' count=4 owner=myappd mode='0700'\n\n    '''\n    ## cleanup kwargs\n    kwargs = salt.utils.args.clean_kwargs(**kwargs)\n\n    ## inject name into kwargs\n    if 'entryname' not in kwargs and name and not name.startswith('/'):\n        kwargs['entryname'] = name\n\n    ## inject pattern into kwargs\n    if 'log_file' not in kwargs:\n        if pattern and pattern.startswith('/'):\n            kwargs['log_file'] = pattern\n        # NOTE: for backwards compatibility check if name is a path\n        elif name and name.startswith('/'):\n            kwargs['log_file'] = name\n\n    ## build command\n    log.debug(\"logadm.rotate - kwargs: %s\", kwargs)\n    command = \"logadm -f {}\".format(conf_file)\n    for arg, val in kwargs.items():\n        if arg in option_toggles.values() and val:\n            command = \"{} {}\".format(\n                command,\n                _arg2opt(arg),\n            )\n        elif arg in option_flags.values():\n            command = \"{} {} {}\".format(\n                command,\n                _arg2opt(arg),\n                _quote_args(six.text_type(val))\n            )\n        elif arg != 'log_file':\n            log.warning(\"Unknown argument %s, don't know how to map this!\", arg)\n    if 'log_file' in kwargs:\n        # NOTE: except from ```man logadm```\n        #   If no log file name is provided on a logadm command line, the entry\n        #   name is assumed to be the same as the log file name. For example,\n        #   the following two lines achieve the same thing, keeping two copies\n        #   of rotated log files:\n        #\n        #     % logadm -C2 -w mylog /my/really/long/log/file/name\n        #     % logadm -C2 -w /my/really/long/log/file/name\n        if 'entryname' not in kwargs:\n            command = \"{} -w {}\".format(command, _quote_args(kwargs['log_file']))\n        else:\n            command = \"{} {}\".format(command, _quote_args(kwargs['log_file']))\n\n    log.debug(\"logadm.rotate - command: %s\", command)\n    result = __salt__['cmd.run_all'](command, python_shell=False)\n    if result['retcode'] != 0:\n        return dict(Error='Failed in adding log', Output=result['stderr'])\n\n    return dict(Result='Success')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef remove(name, conf_file=default_conf):\n    '''\n    Remove log pattern from logadm\n\n    CLI Example:\n\n    .. code-block:: bash\n\n      salt '*' logadm.remove myapplog\n    '''\n    command = \"logadm -f {0} -r {1}\".format(conf_file, name)\n    result = __salt__['cmd.run_all'](command, python_shell=False)\n    if result['retcode'] != 0:\n        return dict(\n            Error='Failure in removing log. Possibly already removed?',\n            Output=result['stderr']\n        )\n    return dict(Result='Success')", "response": "Remove log pattern from logadm"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of string encodings to try", "response": "def get_encodings():\n    '''\n    return a list of string encodings to try\n    '''\n    encodings = [__salt_system_encoding__]\n\n    try:\n        sys_enc = sys.getdefaultencoding()\n    except ValueError:  # system encoding is nonstandard or malformed\n        sys_enc = None\n    if sys_enc and sys_enc not in encodings:\n        encodings.append(sys_enc)\n\n    for enc in ['utf-8', 'latin-1']:\n        if enc not in encodings:\n            encodings.append(enc)\n\n    return encodings"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef split_locale(loc):\n    '''\n    Split a locale specifier.  The general format is\n\n    language[_territory][.codeset][@modifier] [charmap]\n\n    For example:\n\n    ca_ES.UTF-8@valencia UTF-8\n    '''\n    def split(st, char):\n        '''\n        Split a string `st` once by `char`; always return a two-element list\n        even if the second element is empty.\n        '''\n        split_st = st.split(char, 1)\n        if len(split_st) == 1:\n            split_st.append('')\n        return split_st\n\n    comps = {}\n    work_st, comps['charmap'] = split(loc, ' ')\n    work_st, comps['modifier'] = split(work_st, '@')\n    work_st, comps['codeset'] = split(work_st, '.')\n    comps['language'], comps['territory'] = split(work_st, '_')\n    return comps", "response": "Split a locale specifier into a list of locale - specific components."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef join_locale(comps):\n    '''\n    Join a locale specifier split in the format returned by split_locale.\n    '''\n    loc = comps['language']\n    if comps.get('territory'):\n        loc += '_' + comps['territory']\n    if comps.get('codeset'):\n        loc += '.' + comps['codeset']\n    if comps.get('modifier'):\n        loc += '@' + comps['modifier']\n    if comps.get('charmap'):\n        loc += ' ' + comps['charmap']\n    return loc", "response": "Join a locale specifier split in the format returned by split_locale.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef normalize_locale(loc):\n    '''\n    Format a locale specifier according to the format returned by `locale -a`.\n    '''\n    comps = split_locale(loc)\n    comps['territory'] = comps['territory'].upper()\n    comps['codeset'] = comps['codeset'].lower().replace('-', '')\n    comps['charmap'] = ''\n    return join_locale(comps)", "response": "Normalizes a locale specifier according to the format returned by locale - a."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nremoves dots from the given data structure", "response": "def _remove_dots(src):\n    '''\n    Remove dots from the given data structure\n    '''\n    output = {}\n    for key, val in six.iteritems(src):\n        if isinstance(val, dict):\n            val = _remove_dots(val)\n        output[key.replace('.', '-')] = val\n    return output"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a mongodb connection object", "response": "def _get_conn(ret):\n    '''\n    Return a mongodb connection object\n    '''\n    _options = _get_options(ret)\n\n    host = _options.get('host')\n    port = _options.get('port')\n    db_ = _options.get('db')\n    user = _options.get('user')\n    password = _options.get('password')\n    indexes = _options.get('indexes', False)\n\n    # at some point we should remove support for\n    # pymongo versions < 2.3 until then there are\n    # a bunch of these sections that need to be supported\n\n    if PYMONGO_VERSION > _LooseVersion('2.3'):\n        conn = pymongo.MongoClient(host, port)\n    else:\n        conn = pymongo.Connection(host, port)\n    mdb = conn[db_]\n\n    if user and password:\n        mdb.authenticate(user, password)\n\n    if indexes:\n        if PYMONGO_VERSION > _LooseVersion('2.3'):\n            mdb.saltReturns.create_index('minion')\n            mdb.saltReturns.create_index('jid')\n\n            mdb.jobs.create_index('jid')\n        else:\n            mdb.saltReturns.ensure_index('minion')\n            mdb.saltReturns.ensure_index('jid')\n\n            mdb.jobs.ensure_index('jid')\n\n    return conn, mdb"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_jid(jid):\n    '''\n    Return the return information associated with a jid\n    '''\n    conn, mdb = _get_conn(ret=None)\n    ret = {}\n    rdata = mdb.saltReturns.find({'jid': jid}, {'_id': 0})\n    if rdata:\n        for data in rdata:\n            minion = data['minion']\n            # return data in the format {<minion>: { <unformatted full return data>}}\n            ret[minion] = data['full_ret']\n    return ret", "response": "Return the return information associated with a jid\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the most recent jobs that have executed the named function", "response": "def get_fun(fun):\n    '''\n    Return the most recent jobs that have executed the named function\n    '''\n    conn, mdb = _get_conn(ret=None)\n    ret = {}\n    rdata = mdb.saltReturns.find_one({'fun': fun}, {'_id': 0})\n    if rdata:\n        ret = rdata\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _cert_file(name, cert_type):\n    '''\n    Return expected path of a Let's Encrypt live cert\n    '''\n    return os.path.join(LE_LIVE, name, '{0}.pem'.format(cert_type))", "response": "Return expected path of a Let s Encrypt live cert\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _expires(name):\n    '''\n    Return the expiry date of a cert\n\n    :return datetime object of expiry date\n    '''\n    cert_file = _cert_file(name, 'cert')\n    # Use the salt module if available\n    if 'tls.cert_info' in __salt__:\n        expiry = __salt__['tls.cert_info'](cert_file)['not_after']\n    # Cobble it together using the openssl binary\n    else:\n        openssl_cmd = 'openssl x509 -in {0} -noout -enddate'.format(cert_file)\n        # No %e format on my Linux'es here\n        strptime_sux_cmd = 'date --date=\"$({0} | cut -d= -f2)\" +%s'.format(openssl_cmd)\n        expiry = float(__salt__['cmd.shell'](strptime_sux_cmd, output_loglevel='quiet'))\n        # expiry = datetime.datetime.strptime(expiry.split('=', 1)[-1], '%b %e %H:%M:%S %Y %Z')\n\n    return datetime.datetime.fromtimestamp(expiry)", "response": "Return the expiry date of a certificate"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _renew_by(name, window=None):\n    '''\n    Date before a certificate should be renewed\n\n    :param name: Common Name of the certificate (DNS name of certificate)\n    :param window: days before expiry date to renew\n    :return datetime object of first renewal date\n    '''\n    expiry = _expires(name)\n    if window is not None:\n        expiry = expiry - datetime.timedelta(days=window)\n\n    return expiry", "response": "Returns the date before a certificate should be renewed."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning information about a certificate", "response": "def info(name):\n    '''\n    Return information about a certificate\n\n    .. note::\n        Will output tls.cert_info if that's available, or OpenSSL text if not\n\n    :param name: CommonName of cert\n\n    CLI example:\n\n    .. code-block:: bash\n\n        salt 'gitlab.example.com' acme.info dev.example.com\n    '''\n    cert_file = _cert_file(name, 'cert')\n    # Use the salt module if available\n    if 'tls.cert_info' in __salt__:\n        cert_info = __salt__['tls.cert_info'](cert_file)\n        # Strip out the extensions object contents;\n        # these trip over our poor state output\n        # and they serve no real purpose here anyway\n        cert_info['extensions'] = cert_info['extensions'].keys()\n        return cert_info\n    # Cobble it together using the openssl binary\n    openssl_cmd = 'openssl x509 -in {0} -noout -text'.format(cert_file)\n    return __salt__['cmd.run'](openssl_cmd, output_loglevel='quiet')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck if a certificate needs renewal", "response": "def needs_renewal(name, window=None):\n    '''\n    Check if a certificate needs renewal\n\n    :param name: CommonName of cert\n    :param window: Window in days to renew earlier or True/force to just return True\n\n    Code example:\n\n    .. code-block:: python\n\n        if __salt__['acme.needs_renewal']('dev.example.com'):\n            __salt__['acme.cert']('dev.example.com', **kwargs)\n        else:\n            log.info('Your certificate is still good')\n    '''\n    if window is not None and window in ('force', 'Force', True):\n        return True\n\n    return _renew_by(name, window) <= datetime.datetime.today()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _analyse_overview_field(content):\n    '''\n    Split the field in drbd-overview\n    '''\n    if \"(\" in content:\n        # Output like \"Connected(2*)\" or \"UpToDate(2*)\"\n        return content.split(\"(\")[0], content.split(\"(\")[0]\n    elif \"/\" in content:\n        # Output like \"Primar/Second\" or \"UpToDa/UpToDa\"\n        return content.split(\"/\")[0], content.split(\"/\")[1]\n\n    return content, \"\"", "response": "Analyse the field in drbd - overview"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _count_spaces_startswith(line):\n    '''\n    Count the number of spaces before the first character\n    '''\n    if line.split('#')[0].strip() == \"\":\n        return None\n\n    spaces = 0\n    for i in line:\n        if i.isspace():\n            spaces += 1\n        else:\n            return spaces", "response": "Count the number of spaces before the first character in the line."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _analyse_status_type(line):\n    '''\n    Figure out the sections in drbdadm status\n    '''\n    spaces = _count_spaces_startswith(line)\n\n    if spaces is None:\n        return ''\n\n    switch = {\n        0: 'RESOURCE',\n        2: {' disk:': 'LOCALDISK', ' role:': 'PEERNODE', ' connection:': 'PEERNODE'},\n        4: {' peer-disk:': 'PEERDISK'}\n    }\n\n    ret = switch.get(spaces, 'UNKNOWN')\n\n    # isinstance(ret, str) only works when run directly, calling need unicode(six)\n    if isinstance(ret, six.text_type):\n        return ret\n\n    for x in ret:\n        if x in line:\n            return ret[x]\n\n    return 'UNKNOWN'", "response": "Analyse the status type of the current resource"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nanalyse the line of local resource of drbdadm status", "response": "def _add_res(line):\n    '''\n    Analyse the line of local resource of ``drbdadm status``\n    '''\n    global resource\n    fields = line.strip().split()\n\n    if resource:\n        ret.append(resource)\n        resource = {}\n\n    resource[\"resource name\"] = fields[0]\n    resource[\"local role\"] = fields[1].split(\":\")[1]\n    resource[\"local volumes\"] = []\n    resource[\"peer nodes\"] = []"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nanalyses the line of volumes of drbdadm status", "response": "def _add_volume(line):\n    '''\n    Analyse the line of volumes of ``drbdadm status``\n    '''\n    section = _analyse_status_type(line)\n    fields = line.strip().split()\n\n    volume = {}\n    for field in fields:\n        volume[field.split(':')[0]] = field.split(':')[1]\n\n    if section == 'LOCALDISK':\n        resource['local volumes'].append(volume)\n    else:\n        # 'PEERDISK'\n        lastpnodevolumes.append(volume)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _add_peernode(line):\n    '''\n    Analyse the line of peer nodes of ``drbdadm status``\n    '''\n    global lastpnodevolumes\n\n    fields = line.strip().split()\n\n    peernode = {}\n    peernode[\"peernode name\"] = fields[0]\n    #Could be role or connection:\n    peernode[fields[1].split(\":\")[0]] = fields[1].split(\":\")[1]\n    peernode[\"peer volumes\"] = []\n    resource[\"peer nodes\"].append(peernode)\n    lastpnodevolumes = peernode[\"peer volumes\"]", "response": "Analyse the line of peer nodes of drbdadm status"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _line_parser(line):\n    '''\n    Call action for different lines\n    '''\n    section = _analyse_status_type(line)\n    fields = line.strip().split()\n\n    switch = {\n        '': _empty,\n        'RESOURCE': _add_res,\n        'PEERNODE': _add_peernode,\n        'LOCALDISK': _add_volume,\n        'PEERDISK': _add_volume,\n    }\n\n    func = switch.get(section, _unknown_parser)\n\n    func(line)", "response": "Call action for different lines\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef overview():\n    '''\n    Show status of the DRBD devices, support two nodes only.\n    drbd-overview is removed since drbd-utils-9.6.0,\n    use status instead.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' drbd.overview\n    '''\n    cmd = 'drbd-overview'\n    for line in __salt__['cmd.run'](cmd).splitlines():\n        ret = {}\n        fields = line.strip().split()\n        minnum = fields[0].split(':')[0]\n        device = fields[0].split(':')[1]\n        connstate, _ = _analyse_overview_field(fields[1])\n        localrole, partnerrole = _analyse_overview_field(fields[2])\n        localdiskstate, partnerdiskstate = _analyse_overview_field(fields[3])\n        if localdiskstate.startswith(\"UpTo\"):\n            if partnerdiskstate.startswith(\"UpTo\"):\n                if len(fields) >= 5:\n                    mountpoint = fields[4]\n                    fs_mounted = fields[5]\n                    totalsize = fields[6]\n                    usedsize = fields[7]\n                    remainsize = fields[8]\n                    perc = fields[9]\n                    ret = {\n                        'minor number': minnum,\n                        'device': device,\n                        'connection state': connstate,\n                        'local role': localrole,\n                        'partner role': partnerrole,\n                        'local disk state': localdiskstate,\n                        'partner disk state': partnerdiskstate,\n                        'mountpoint': mountpoint,\n                        'fs': fs_mounted,\n                        'total size': totalsize,\n                        'used': usedsize,\n                        'remains': remainsize,\n                        'percent': perc,\n                    }\n                else:\n                    ret = {\n                        'minor number': minnum,\n                        'device': device,\n                        'connection state': connstate,\n                        'local role': localrole,\n                        'partner role': partnerrole,\n                        'local disk state': localdiskstate,\n                        'partner disk state': partnerdiskstate,\n                    }\n            else:\n                syncbar = fields[4]\n                synced = fields[6]\n                syncedbytes = fields[7]\n                sync = synced+syncedbytes\n                ret = {\n                    'minor number': minnum,\n                    'device': device,\n                    'connection state': connstate,\n                    'local role': localrole,\n                    'partner role': partnerrole,\n                    'local disk state': localdiskstate,\n                    'partner disk state': partnerdiskstate,\n                    'synchronisation: ': syncbar,\n                    'synched': sync,\n                }\n    return ret", "response": "Show status of the DRBD devices support two nodes only."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nshow status of the current node and volume of the current node.", "response": "def status(name='all'):\n    '''\n    Using drbdadm to show status of the DRBD devices,\n    available in the latest drbd9.\n    Support multiple nodes, multiple volumes.\n\n    :type name: str\n    :param name:\n        Resource name.\n\n    :return: drbd status of resource.\n    :rtype: list(dict(res))\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' drbd.status\n        salt '*' drbd.status name=<resource name>\n    '''\n\n    # Initialize for multiple times test cases\n    global ret\n    global resource\n    ret = []\n    resource = {}\n\n    cmd = ['drbdadm', 'status']\n    cmd.append(name)\n\n    #One possible output: (number of resource/node/vol are flexible)\n    #resource role:Secondary\n    #  volume:0 disk:Inconsistent\n    #  volume:1 disk:Inconsistent\n    #  drbd-node1 role:Primary\n    #    volume:0 replication:SyncTarget peer-disk:UpToDate done:10.17\n    #    volume:1 replication:SyncTarget peer-disk:UpToDate done:74.08\n    #  drbd-node2 role:Secondary\n    #    volume:0 peer-disk:Inconsistent resync-suspended:peer\n    #    volume:1 peer-disk:Inconsistent resync-suspended:peer\n    for line in __salt__['cmd.run'](cmd).splitlines():\n        _line_parser(line)\n\n    if resource:\n        ret.append(resource)\n\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _parse_return_code_powershell(string):\n    '''\n    return from the input string the return code of the powershell command\n    '''\n\n    regex = re.search(r'ReturnValue\\s*: (\\d*)', string)\n    if not regex:\n        return (False, 'Could not parse PowerShell return code.')\n    else:\n        return int(regex.group(1))", "response": "Parses the return code of the powershell command"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nexecutes the command in a Win32_TerminalServiceSetting WMI Object and return the STDOUT of the command", "response": "def _psrdp(cmd):\n    '''\n    Create a Win32_TerminalServiceSetting WMI Object as $RDP and execute the\n    command cmd returns the STDOUT of the command\n    '''\n    rdp = ('$RDP = Get-WmiObject -Class Win32_TerminalServiceSetting '\n           '-Namespace root\\\\CIMV2\\\\TerminalServices -Computer . '\n           '-Authentication 6 -ErrorAction Stop')\n    return __salt__['cmd.run']('{0} ; {1}'.format(rdp, cmd),\n                               shell='powershell', python_shell=True)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nlist all the sessions in the current server.", "response": "def list_sessions(logged_in_users_only=False):\n    '''\n    List information about the sessions.\n\n    .. versionadded:: 2016.11.0\n\n    :param logged_in_users_only: If True, only return sessions with users logged in.\n    :return: A list containing dictionaries of session information.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' rdp.list_sessions\n    '''\n    ret = list()\n    server = win32ts.WTS_CURRENT_SERVER_HANDLE\n    protocols = {win32ts.WTS_PROTOCOL_TYPE_CONSOLE: 'console',\n                 win32ts.WTS_PROTOCOL_TYPE_ICA: 'citrix',\n                 win32ts.WTS_PROTOCOL_TYPE_RDP: 'rdp'}\n    statuses = {win32ts.WTSActive: 'active', win32ts.WTSConnected: 'connected',\n                win32ts.WTSConnectQuery: 'connect_query', win32ts.WTSShadow: 'shadow',\n                win32ts.WTSDisconnected: 'disconnected', win32ts.WTSIdle: 'idle',\n                win32ts.WTSListen: 'listen', win32ts.WTSReset: 'reset',\n                win32ts.WTSDown: 'down', win32ts.WTSInit: 'init'}\n\n    for session in win32ts.WTSEnumerateSessions(server):\n        user = win32ts.WTSQuerySessionInformation(server, session['SessionId'],\n                                                  win32ts.WTSUserName) or None\n        protocol_id = win32ts.WTSQuerySessionInformation(server, session['SessionId'],\n                                                         win32ts.WTSClientProtocolType)\n        status_id = win32ts.WTSQuerySessionInformation(server, session['SessionId'],\n                                                       win32ts.WTSConnectState)\n        protocol = protocols.get(protocol_id, 'unknown')\n        connection_status = statuses.get(status_id, 'unknown')\n        station = session['WinStationName'] or 'Disconnected'\n        connection_info = {'connection_status': connection_status, 'protocol': protocol,\n                           'session_id': session['SessionId'], 'station': station,\n                           'user': user}\n        if logged_in_users_only:\n            if user:\n                ret.append(connection_info)\n        else:\n            ret.append(connection_info)\n\n    if not ret:\n        _LOG.warning('No sessions found.')\n    return sorted(ret, key=lambda k: k['session_id'])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting information about a session.", "response": "def get_session(session_id):\n    '''\n    Get information about a session.\n\n    .. versionadded:: 2016.11.0\n\n    :param session_id: The numeric Id of the session.\n    :return: A dictionary of session information.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' rdp.get_session session_id\n\n        salt '*' rdp.get_session 99\n    '''\n    ret = dict()\n    sessions = list_sessions()\n    session = [item for item in sessions if item['session_id'] == session_id]\n\n    if session:\n        ret = session[0]\n\n    if not ret:\n        _LOG.warning('No session found for id: %s', session_id)\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndisconnects a session. .. versionadded:: 2016.11.0 :param session_id: The numeric Id of the session. :return: A boolean representing whether the disconnect succeeded. CLI Example: .. code-block:: bash salt '*' rdp.disconnect_session session_id salt '*' rdp.disconnect_session 99", "response": "def disconnect_session(session_id):\n    '''\n    Disconnect a session.\n\n    .. versionadded:: 2016.11.0\n\n    :param session_id: The numeric Id of the session.\n    :return: A boolean representing whether the disconnect succeeded.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' rdp.disconnect_session session_id\n\n        salt '*' rdp.disconnect_session 99\n    '''\n    try:\n        win32ts.WTSDisconnectSession(win32ts.WTS_CURRENT_SERVER_HANDLE, session_id, True)\n    except PyWinError as error:\n        _LOG.error('Error calling WTSDisconnectSession: %s', error)\n        return False\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninitiating the logoff of a session.", "response": "def logoff_session(session_id):\n    '''\n    Initiate the logoff of a session.\n\n    .. versionadded:: 2016.11.0\n\n    :param session_id: The numeric Id of the session.\n    :return: A boolean representing whether the logoff succeeded.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' rdp.logoff_session session_id\n\n        salt '*' rdp.logoff_session 99\n    '''\n    try:\n        win32ts.WTSLogoffSession(win32ts.WTS_CURRENT_SERVER_HANDLE, session_id, True)\n    except PyWinError as error:\n        _LOG.error('Error calling WTSLogoffSession: %s', error)\n        return False\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef envs(backend=None, sources=False):\n    '''\n    Return the available fileserver environments. If no backend is provided,\n    then the environments for all configured backends will be returned.\n\n    backend\n        Narrow fileserver backends to a subset of the enabled ones.\n\n        .. versionchanged:: 2015.5.0\n            If all passed backends start with a minus sign (``-``), then these\n            backends will be excluded from the enabled backends. However, if\n            there is a mix of backends with and without a minus sign (ex:\n            ``backend=-roots,git``) then the ones starting with a minus\n            sign will be disregarded.\n\n            Additionally, fileserver backends can now be passed as a\n            comma-separated list. In earlier versions, they needed to be passed\n            as a python list (ex: ``backend=\"['roots', 'git']\"``)\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run fileserver.envs\n        salt-run fileserver.envs backend=roots,git\n        salt-run fileserver.envs git\n    '''\n    fileserver = salt.fileserver.Fileserver(__opts__)\n    return sorted(fileserver.envs(back=backend, sources=sources))", "response": "Return the available fileserver environments."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nclears the file list caches for the specified environment and environment.", "response": "def clear_file_list_cache(saltenv=None, backend=None):\n    '''\n    .. versionadded:: 2016.11.0\n\n    The Salt fileserver caches the files/directories/symlinks for each\n    fileserver backend and environment as they are requested. This is done to\n    help the fileserver scale better. Without this caching, when\n    hundreds/thousands of minions simultaneously ask the master what files are\n    available, this would cause the master's CPU load to spike as it obtains\n    the same information separately for each minion.\n\n    saltenv\n        By default, this runner will clear the file list caches for all\n        environments. This argument allows for a list of environments to be\n        passed, to clear more selectively. This list can be passed either as a\n        comma-separated string, or a Python list.\n\n    backend\n        Similar to the ``saltenv`` parameter, this argument will restrict the\n        cache clearing to specific fileserver backends (the default behavior is\n        to clear from all enabled fileserver backends). This list can be passed\n        either as a comma-separated string, or a Python list.\n\n    .. note:\n        The maximum age for the cached file lists (i.e. the age at which the\n        cache will be disregarded and rebuilt) is defined by the\n        :conf_master:`fileserver_list_cache_time` configuration parameter.\n\n    Since the ability to clear these caches is often required by users writing\n    custom runners which add/remove files, this runner can easily be called\n    from within a custom runner using any of the following examples:\n\n    .. code-block:: python\n\n        # Clear all file list caches\n        __salt__['fileserver.clear_file_list_cache']()\n        # Clear just the 'base' saltenv file list caches\n        __salt__['fileserver.clear_file_list_cache'](saltenv='base')\n        # Clear just the 'base' saltenv file list caches from just the 'roots'\n        # fileserver backend\n        __salt__['fileserver.clear_file_list_cache'](saltenv='base', backend='roots')\n        # Clear all file list caches from the 'roots' fileserver backend\n        __salt__['fileserver.clear_file_list_cache'](backend='roots')\n\n    .. note::\n        In runners, the ``__salt__`` dictionary will likely be renamed to\n        ``__runner__`` in a future Salt release to distinguish runner functions\n        from remote execution functions. See `this GitHub issue`_ for\n        discussion/updates on this.\n\n    .. _`this GitHub issue`: https://github.com/saltstack/salt/issues/34958\n\n    If using Salt's Python API (not a runner), the following examples are\n    equivalent to the ones above:\n\n    .. code-block:: python\n\n        import salt.config\n        import salt.runner\n\n        opts = salt.config.master_config('/etc/salt/master')\n        opts['fun'] = 'fileserver.clear_file_list_cache'\n\n        # Clear all file list_caches\n        opts['arg'] = []  # No arguments\n        runner = salt.runner.Runner(opts)\n        cleared = runner.run()\n\n        # Clear just the 'base' saltenv file list caches\n        opts['arg'] = ['base', None]\n        runner = salt.runner.Runner(opts)\n        cleared = runner.run()\n\n        # Clear just the 'base' saltenv file list caches from just the 'roots'\n        # fileserver backend\n        opts['arg'] = ['base', 'roots']\n        runner = salt.runner.Runner(opts)\n        cleared = runner.run()\n\n        # Clear all file list caches from the 'roots' fileserver backend\n        opts['arg'] = [None, 'roots']\n        runner = salt.runner.Runner(opts)\n        cleared = runner.run()\n\n\n    This function will return a dictionary showing a list of environments which\n    were cleared for each backend. An empty return dictionary means that no\n    changes were made.\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        # Clear all file list caches\n        salt-run fileserver.clear_file_list_cache\n        # Clear just the 'base' saltenv file list caches\n        salt-run fileserver.clear_file_list_cache saltenv=base\n        # Clear just the 'base' saltenv file list caches from just the 'roots'\n        # fileserver backend\n        salt-run fileserver.clear_file_list_cache saltenv=base backend=roots\n        # Clear all file list caches from the 'roots' fileserver backend\n        salt-run fileserver.clear_file_list_cache backend=roots\n    '''\n    fileserver = salt.fileserver.Fileserver(__opts__)\n    load = {'saltenv': saltenv, 'fsbackend': backend}\n    return fileserver.clear_file_list_cache(load=load)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a list of files from the salt - master", "response": "def file_list(saltenv='base', backend=None):\n    '''\n    Return a list of files from the salt fileserver\n\n    saltenv : base\n        The salt fileserver environment to be listed\n\n    backend\n        Narrow fileserver backends to a subset of the enabled ones. If all\n        passed backends start with a minus sign (``-``), then these backends\n        will be excluded from the enabled backends. However, if there is a mix\n        of backends with and without a minus sign (ex:\n        ``backend=-roots,git``) then the ones starting with a minus sign will\n        be disregarded.\n\n        .. versionadded:: 2015.5.0\n\n    .. note:\n        Keep in mind that executing this function spawns a new process,\n        separate from the master. This means that if the fileserver\n        configuration has been changed in some way since the master has been\n        restarted (e.g. if :conf_master:`fileserver_backend`,\n        :conf_master:`gitfs_remotes`, :conf_master:`hgfs_remotes`, etc. have\n        been updated), then the results of this runner will not accurately\n        reflect what files are available to minions.\n\n        When in doubt, use :py:func:`cp.list_master\n        <salt.modules.cp.list_master>` to see what files the minion can see,\n        and always remember to restart the salt-master daemon when updating\n        the fileserver configuration.\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt-run fileserver.file_list\n        salt-run fileserver.file_list saltenv=prod\n        salt-run fileserver.file_list saltenv=dev backend=git\n        salt-run fileserver.file_list base hg,roots\n        salt-run fileserver.file_list -git\n    '''\n    fileserver = salt.fileserver.Fileserver(__opts__)\n    load = {'saltenv': saltenv, 'fsbackend': backend}\n    return fileserver.file_list(load=load)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef symlink_list(saltenv='base', backend=None):\n    '''\n    Return a list of symlinked files and dirs\n\n    saltenv : base\n        The salt fileserver environment to be listed\n\n    backend\n        Narrow fileserver backends to a subset of the enabled ones. If all\n        passed backends start with a minus sign (``-``), then these backends\n        will be excluded from the enabled backends. However, if there is a mix\n        of backends with and without a minus sign (ex:\n        ``backend=-roots,git``) then the ones starting with a minus sign will\n        be disregarded.\n\n        .. versionadded:: 2015.5.0\n\n    .. note:\n        Keep in mind that executing this function spawns a new process,\n        separate from the master. This means that if the fileserver\n        configuration has been changed in some way since the master has been\n        restarted (e.g. if :conf_master:`fileserver_backend`,\n        :conf_master:`gitfs_remotes`, :conf_master:`hgfs_remotes`, etc. have\n        been updated), then the results of this runner will not accurately\n        reflect what symlinks are available to minions.\n\n        When in doubt, use :py:func:`cp.list_master_symlinks\n        <salt.modules.cp.list_master_symlinks>` to see what symlinks the minion\n        can see, and always remember to restart the salt-master daemon when\n        updating the fileserver configuration.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run fileserver.symlink_list\n        salt-run fileserver.symlink_list saltenv=prod\n        salt-run fileserver.symlink_list saltenv=dev backend=git\n        salt-run fileserver.symlink_list base hg,roots\n        salt-run fileserver.symlink_list -git\n    '''\n    fileserver = salt.fileserver.Fileserver(__opts__)\n    load = {'saltenv': saltenv, 'fsbackend': backend}\n    return fileserver.symlink_list(load=load)", "response": "Return a list of symlinked files and directories for the specified master environment"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dir_list(saltenv='base', backend=None):\n    '''\n    Return a list of directories in the given environment\n\n    saltenv : base\n        The salt fileserver environment to be listed\n\n    backend\n        Narrow fileserver backends to a subset of the enabled ones. If all\n        passed backends start with a minus sign (``-``), then these backends\n        will be excluded from the enabled backends. However, if there is a mix\n        of backends with and without a minus sign (ex:\n        ``backend=-roots,git``) then the ones starting with a minus sign will\n        be disregarded.\n\n        .. versionadded:: 2015.5.0\n\n    .. note:\n        Keep in mind that executing this function spawns a new process,\n        separate from the master. This means that if the fileserver\n        configuration has been changed in some way since the master has been\n        restarted (e.g. if :conf_master:`fileserver_backend`,\n        :conf_master:`gitfs_remotes`, :conf_master:`hgfs_remotes`, etc. have\n        been updated), then the results of this runner will not accurately\n        reflect what dirs are available to minions.\n\n        When in doubt, use :py:func:`cp.list_master_dirs\n        <salt.modules.cp.list_master_dirs>` to see what dirs the minion can see,\n        and always remember to restart the salt-master daemon when updating\n        the fileserver configuration.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run fileserver.dir_list\n        salt-run fileserver.dir_list saltenv=prod\n        salt-run fileserver.dir_list saltenv=dev backend=git\n        salt-run fileserver.dir_list base hg,roots\n        salt-run fileserver.dir_list -git\n    '''\n    fileserver = salt.fileserver.Fileserver(__opts__)\n    load = {'saltenv': saltenv, 'fsbackend': backend}\n    return fileserver.dir_list(load=load)", "response": "Return a list of directories in the given environment"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef empty_dir_list(saltenv='base', backend=None):\n    '''\n    .. versionadded:: 2015.5.0\n\n    Return a list of empty directories in the given environment\n\n    saltenv : base\n        The salt fileserver environment to be listed\n\n    backend\n        Narrow fileserver backends to a subset of the enabled ones. If all\n        passed backends start with a minus sign (``-``), then these backends\n        will be excluded from the enabled backends. However, if there is a mix\n        of backends with and without a minus sign (ex:\n        ``backend=-roots,git``) then the ones starting with a minus sign will\n        be disregarded.\n\n        .. note::\n\n            Some backends (such as :mod:`git <salt.fileserver.gitfs>` and\n            :mod:`hg <salt.fileserver.hgfs>`) do not support empty directories.\n            So, passing ``backend=git`` or ``backend=hg`` will result in an\n            empty list being returned.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run fileserver.empty_dir_list\n        salt-run fileserver.empty_dir_list saltenv=prod\n        salt-run fileserver.empty_dir_list backend=roots\n    '''\n    fileserver = salt.fileserver.Fileserver(__opts__)\n    load = {'saltenv': saltenv, 'fsbackend': backend}\n    return fileserver.file_list_emptydirs(load=load)", "response": "Return a list of empty directories in the given environment"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update(backend=None):\n    '''\n    Update the fileserver cache. If no backend is provided, then the cache for\n    all configured backends will be updated.\n\n    backend\n        Narrow fileserver backends to a subset of the enabled ones.\n\n        .. versionchanged:: 2015.5.0\n            If all passed backends start with a minus sign (``-``), then these\n            backends will be excluded from the enabled backends. However, if\n            there is a mix of backends with and without a minus sign (ex:\n            ``backend=-roots,git``) then the ones starting with a minus\n            sign will be disregarded.\n\n            Additionally, fileserver backends can now be passed as a\n            comma-separated list. In earlier versions, they needed to be passed\n            as a python list (ex: ``backend=\"['roots', 'git']\"``)\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run fileserver.update\n        salt-run fileserver.update backend=roots,git\n    '''\n    fileserver = salt.fileserver.Fileserver(__opts__)\n    fileserver.update(back=backend)\n    return True", "response": "Update the fileserver cache for the current node."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nclear the fileserver cache for the specified backend.", "response": "def clear_cache(backend=None):\n    '''\n    .. versionadded:: 2015.5.0\n\n    Clear the fileserver cache from VCS fileserver backends (:mod:`git\n    <salt.fileserver.gitfs>`, :mod:`hg <salt.fileserver.hgfs>`, :mod:`svn\n    <salt.fileserver.svnfs>`). Executing this runner with no arguments will\n    clear the cache for all enabled VCS fileserver backends, but this\n    can be narrowed using the ``backend`` argument.\n\n    backend\n        Only clear the update lock for the specified backend(s). If all passed\n        backends start with a minus sign (``-``), then these backends will be\n        excluded from the enabled backends. However, if there is a mix of\n        backends with and without a minus sign (ex: ``backend=-roots,git``)\n        then the ones starting with a minus sign will be disregarded.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run fileserver.clear_cache\n        salt-run fileserver.clear_cache backend=git,hg\n        salt-run fileserver.clear_cache hg\n        salt-run fileserver.clear_cache -roots\n    '''\n    fileserver = salt.fileserver.Fileserver(__opts__)\n    cleared, errors = fileserver.clear_cache(back=backend)\n    ret = {}\n    if cleared:\n        ret['cleared'] = cleared\n    if errors:\n        ret['errors'] = errors\n    if not ret:\n        return 'No cache was cleared'\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nclears the update lock for the specified backend and remote", "response": "def clear_lock(backend=None, remote=None):\n    '''\n    .. versionadded:: 2015.5.0\n\n    Clear the fileserver update lock from VCS fileserver backends (:mod:`git\n    <salt.fileserver.gitfs>`, :mod:`hg <salt.fileserver.hgfs>`, :mod:`svn\n    <salt.fileserver.svnfs>`). This should only need to be done if a fileserver\n    update was interrupted and a remote is not updating (generating a warning\n    in the Master's log file). Executing this runner with no arguments will\n    remove all update locks from all enabled VCS fileserver backends, but this\n    can be narrowed by using the following arguments:\n\n    backend\n        Only clear the update lock for the specified backend(s).\n\n    remote\n        If specified, then any remotes which contain the passed string will\n        have their lock cleared. For example, a ``remote`` value of **github**\n        will remove the lock from all github.com remotes.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run fileserver.clear_lock\n        salt-run fileserver.clear_lock backend=git,hg\n        salt-run fileserver.clear_lock backend=git remote=github\n        salt-run fileserver.clear_lock remote=bitbucket\n    '''\n    fileserver = salt.fileserver.Fileserver(__opts__)\n    cleared, errors = fileserver.clear_lock(back=backend, remote=remote)\n    ret = {}\n    if cleared:\n        ret['cleared'] = cleared\n    if errors:\n        ret['errors'] = errors\n    if not ret:\n        return 'No locks were removed'\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef lock(backend=None, remote=None):\n    '''\n    .. versionadded:: 2015.5.0\n\n    Set a fileserver update lock for VCS fileserver backends (:mod:`git\n    <salt.fileserver.gitfs>`, :mod:`hg <salt.fileserver.hgfs>`, :mod:`svn\n    <salt.fileserver.svnfs>`).\n\n    .. note::\n\n        This will only operate on enabled backends (those configured in\n        :conf_master:`fileserver_backend`).\n\n    backend\n        Only set the update lock for the specified backend(s).\n\n    remote\n        If not None, then any remotes which contain the passed string will have\n        their lock cleared. For example, a ``remote`` value of ``*github.com*``\n        will remove the lock from all github.com remotes.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run fileserver.lock\n        salt-run fileserver.lock backend=git,hg\n        salt-run fileserver.lock backend=git remote='*github.com*'\n        salt-run fileserver.lock remote=bitbucket\n    '''\n    fileserver = salt.fileserver.Fileserver(__opts__)\n    locked, errors = fileserver.lock(back=backend, remote=remote)\n    ret = {}\n    if locked:\n        ret['locked'] = locked\n    if errors:\n        ret['errors'] = errors\n    if not ret:\n        return 'No locks were set'\n    return ret", "response": "Return a new set of fileserver update locks for the specified backend and remote."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrun the logic for saltkey", "response": "def run(self):\n        '''\n        Run the logic for saltkey\n        '''\n        self._update_opts()\n        cmd = self.opts['fun']\n\n        veri = None\n        ret = None\n        try:\n            if cmd in ('accept', 'reject', 'delete'):\n                ret = self._run_cmd('name_match')\n                if not isinstance(ret, dict):\n                    salt.output.display_output(ret, 'key', opts=self.opts)\n                    return ret\n                ret = self._filter_ret(cmd, ret)\n                if not ret:\n                    self._print_no_match(cmd, self.opts['match'])\n                    return\n                print('The following keys are going to be {0}ed:'.format(cmd.rstrip('e')))\n                salt.output.display_output(ret, 'key', opts=self.opts)\n\n                if not self.opts.get('yes', False):\n                    try:\n                        if cmd.startswith('delete'):\n                            veri = input('Proceed? [N/y] ')\n                            if not veri:\n                                veri = 'n'\n                        else:\n                            veri = input('Proceed? [n/Y] ')\n                            if not veri:\n                                veri = 'y'\n                    except KeyboardInterrupt:\n                        raise SystemExit(\"\\nExiting on CTRL-c\")\n                # accept/reject/delete the same keys we're printed to the user\n                self.opts['match_dict'] = ret\n                self.opts.pop('match', None)\n                list_ret = ret\n\n            if veri is None or veri.lower().startswith('y'):\n                ret = self._run_cmd(cmd)\n                if cmd in ('accept', 'reject', 'delete'):\n                    if cmd == 'delete':\n                        ret = list_ret\n                    for minions in ret.values():\n                        for minion in minions:\n                            print('Key for minion {0} {1}ed.'.format(minion,\n                                                                     cmd.rstrip('e')))\n                elif isinstance(ret, dict):\n                    salt.output.display_output(ret, 'key', opts=self.opts)\n                else:\n                    salt.output.display_output({'return': ret}, 'key', opts=self.opts)\n        except salt.exceptions.SaltException as exc:\n            ret = '{0}'.format(exc)\n            if not self.opts.get('quiet', False):\n                salt.output.display_output(ret, 'nested', self.opts)\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef gen_keys(self, keydir=None, keyname=None, keysize=None, user=None):\n        '''\n        Generate minion RSA public keypair\n        '''\n        keydir, keyname, keysize, user = self._get_key_attrs(keydir, keyname,\n                                                             keysize, user)\n        salt.crypt.gen_keys(keydir, keyname, keysize, user, self.passphrase)\n        return salt.utils.crypt.pem_finger(os.path.join(keydir, keyname + '.pub'))", "response": "Generate minion RSA public keypair"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef gen_signature(self, privkey, pubkey, sig_path):\n        '''\n        Generate master public-key-signature\n        '''\n        return salt.crypt.gen_signature(privkey,\n                                        pubkey,\n                                        sig_path,\n                                        self.passphrase)", "response": "Generate master public - key - signature"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate master public - key - signature for a given public - key and private - key.", "response": "def gen_keys_signature(self, priv, pub, signature_path, auto_create=False, keysize=None):\n        '''\n        Generate master public-key-signature\n        '''\n        # check given pub-key\n        if pub:\n            if not os.path.isfile(pub):\n                return 'Public-key {0} does not exist'.format(pub)\n        # default to master.pub\n        else:\n            mpub = self.opts['pki_dir'] + '/' + 'master.pub'\n            if os.path.isfile(mpub):\n                pub = mpub\n\n        # check given priv-key\n        if priv:\n            if not os.path.isfile(priv):\n                return 'Private-key {0} does not exist'.format(priv)\n        # default to master_sign.pem\n        else:\n            mpriv = self.opts['pki_dir'] + '/' + 'master_sign.pem'\n            if os.path.isfile(mpriv):\n                priv = mpriv\n\n        if not priv:\n            if auto_create:\n                log.debug(\n                    'Generating new signing key-pair .%s.* in %s',\n                    self.opts['master_sign_key_name'], self.opts['pki_dir']\n                )\n                salt.crypt.gen_keys(self.opts['pki_dir'],\n                                    self.opts['master_sign_key_name'],\n                                    keysize or self.opts['keysize'],\n                                    self.opts.get('user'),\n                                    self.passphrase)\n\n                priv = self.opts['pki_dir'] + '/' + self.opts['master_sign_key_name'] + '.pem'\n            else:\n                return 'No usable private-key found'\n\n        if not pub:\n            return 'No usable public-key found'\n\n        log.debug('Using public-key %s', pub)\n        log.debug('Using private-key %s', priv)\n\n        if signature_path:\n            if not os.path.isdir(signature_path):\n                log.debug('target directory %s does not exist', signature_path)\n        else:\n            signature_path = self.opts['pki_dir']\n\n        sign_path = signature_path + '/' + self.opts['master_pubkey_signature']\n\n        skey = get_key(self.opts)\n        return skey.gen_signature(priv, pub, sign_path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks the minion cache to make sure that all data for the minion is cleared", "response": "def check_minion_cache(self, preserve_minions=None):\n        '''\n        Check the minion cache to make sure that old minion data is cleared\n\n        Optionally, pass in a list of minions which should have their caches\n        preserved. To preserve all caches, set __opts__['preserve_minion_cache']\n        '''\n        if preserve_minions is None:\n            preserve_minions = []\n        keys = self.list_keys()\n        minions = []\n        for key, val in six.iteritems(keys):\n            minions.extend(val)\n        if not self.opts.get('preserve_minion_cache', False):\n            m_cache = os.path.join(self.opts['cachedir'], self.ACC)\n            if os.path.isdir(m_cache):\n                for minion in os.listdir(m_cache):\n                    if minion not in minions and minion not in preserve_minions:\n                        try:\n                            shutil.rmtree(os.path.join(m_cache, minion))\n                        except (OSError, IOError) as ex:\n                            log.warning('Key: Delete cache for %s got OSError/IOError: %s \\n',\n                                        minion,\n                                        ex)\n                            continue\n            cache = salt.cache.factory(self.opts)\n            clist = cache.list(self.ACC)\n            if clist:\n                for minion in clist:\n                    if minion not in minions and minion not in preserve_minions:\n                        cache.flush('{0}/{1}'.format(self.ACC, minion))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if the master is running.", "response": "def check_master(self):\n        '''\n        Log if the master is not running\n\n        :rtype: bool\n        :return: Whether or not the master is running\n        '''\n        if not os.path.exists(\n                os.path.join(\n                    self.opts['sock_dir'],\n                    'publish_pull.ipc'\n                    )\n                ):\n            return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\naccepting a glob which to match the of a key and return the key s location", "response": "def name_match(self, match, full=False):\n        '''\n        Accept a glob which to match the of a key and return the key's location\n        '''\n        if full:\n            matches = self.all_keys()\n        else:\n            matches = self.list_keys()\n        ret = {}\n        if ',' in match and isinstance(match, six.string_types):\n            match = match.split(',')\n        for status, keys in six.iteritems(matches):\n            for key in salt.utils.data.sorted_ignorecase(keys):\n                if isinstance(match, list):\n                    for match_item in match:\n                        if fnmatch.fnmatch(key, match_item):\n                            if status not in ret:\n                                ret[status] = []\n                            ret[status].append(key)\n                else:\n                    if fnmatch.fnmatch(key, match):\n                        if status not in ret:\n                            ret[status] = []\n                        ret[status].append(key)\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\naccepts a dictionary of keys and return the current state of the theCOOKIES specified keys", "response": "def dict_match(self, match_dict):\n        '''\n        Accept a dictionary of keys and return the current state of the\n        specified keys\n        '''\n        ret = {}\n        cur_keys = self.list_keys()\n        for status, keys in six.iteritems(match_dict):\n            for key in salt.utils.data.sorted_ignorecase(keys):\n                for keydir in (self.ACC, self.PEND, self.REJ, self.DEN):\n                    if keydir and fnmatch.filter(cur_keys.get(keydir, []), key):\n                        ret.setdefault(keydir, []).append(key)\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef local_keys(self):\n        '''\n        Return a dict of local keys\n        '''\n        ret = {'local': []}\n        for fn_ in salt.utils.data.sorted_ignorecase(os.listdir(self.opts['pki_dir'])):\n            if fn_.endswith('.pub') or fn_.endswith('.pem'):\n                path = os.path.join(self.opts['pki_dir'], fn_)\n                if os.path.isfile(path):\n                    ret['local'].append(fn_)\n        return ret", "response": "Return a dict of local keys"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a dict of managed keys and what the key status are", "response": "def list_keys(self):\n        '''\n        Return a dict of managed keys and what the key status are\n        '''\n        key_dirs = self._check_minions_directories()\n\n        ret = {}\n\n        for dir_ in key_dirs:\n            if dir_ is None:\n                continue\n            ret[os.path.basename(dir_)] = []\n            try:\n                for fn_ in salt.utils.data.sorted_ignorecase(os.listdir(dir_)):\n                    if not fn_.startswith('.'):\n                        if os.path.isfile(os.path.join(dir_, fn_)):\n                            ret[os.path.basename(dir_)].append(\n                                salt.utils.stringutils.to_unicode(fn_)\n                            )\n            except (OSError, IOError):\n                # key dir kind is not created yet, just skip\n                continue\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef all_keys(self):\n        '''\n        Merge managed keys with local keys\n        '''\n        keys = self.list_keys()\n        keys.update(self.local_keys())\n        return keys", "response": "Returns a list of all managed keys with local keys"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a dict of managed keys under a named status", "response": "def list_status(self, match):\n        '''\n        Return a dict of managed keys under a named status\n        '''\n        acc, pre, rej, den = self._check_minions_directories()\n        ret = {}\n        if match.startswith('acc'):\n            ret[os.path.basename(acc)] = []\n            for fn_ in salt.utils.data.sorted_ignorecase(os.listdir(acc)):\n                if not fn_.startswith('.'):\n                    if os.path.isfile(os.path.join(acc, fn_)):\n                        ret[os.path.basename(acc)].append(fn_)\n        elif match.startswith('pre') or match.startswith('un'):\n            ret[os.path.basename(pre)] = []\n            for fn_ in salt.utils.data.sorted_ignorecase(os.listdir(pre)):\n                if not fn_.startswith('.'):\n                    if os.path.isfile(os.path.join(pre, fn_)):\n                        ret[os.path.basename(pre)].append(fn_)\n        elif match.startswith('rej'):\n            ret[os.path.basename(rej)] = []\n            for fn_ in salt.utils.data.sorted_ignorecase(os.listdir(rej)):\n                if not fn_.startswith('.'):\n                    if os.path.isfile(os.path.join(rej, fn_)):\n                        ret[os.path.basename(rej)].append(fn_)\n        elif match.startswith('den') and den is not None:\n            ret[os.path.basename(den)] = []\n            for fn_ in salt.utils.data.sorted_ignorecase(os.listdir(den)):\n                if not fn_.startswith('.'):\n                    if os.path.isfile(os.path.join(den, fn_)):\n                        ret[os.path.basename(den)].append(fn_)\n        elif match.startswith('all'):\n            return self.all_keys()\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the specified public key or keys based on a glob", "response": "def key_str(self, match):\n        '''\n        Return the specified public key or keys based on a glob\n        '''\n        ret = {}\n        for status, keys in six.iteritems(self.name_match(match)):\n            ret[status] = {}\n            for key in salt.utils.data.sorted_ignorecase(keys):\n                path = os.path.join(self.opts['pki_dir'], status, key)\n                with salt.utils.files.fopen(path, 'r') as fp_:\n                    ret[status][key] = \\\n                        salt.utils.stringutils.to_unicode(fp_.read())\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\naccepting public keys. If \"match\" is passed, it is evaluated as a glob. Pre-gathered matches can also be passed via \"match_dict\".", "response": "def accept(self, match=None, match_dict=None, include_rejected=False, include_denied=False):\n        '''\n        Accept public keys. If \"match\" is passed, it is evaluated as a glob.\n        Pre-gathered matches can also be passed via \"match_dict\".\n        '''\n        if match is not None:\n            matches = self.name_match(match)\n        elif match_dict is not None and isinstance(match_dict, dict):\n            matches = match_dict\n        else:\n            matches = {}\n        keydirs = [self.PEND]\n        if include_rejected:\n            keydirs.append(self.REJ)\n        if include_denied:\n            keydirs.append(self.DEN)\n        for keydir in keydirs:\n            for key in matches.get(keydir, []):\n                try:\n                    shutil.move(\n                            os.path.join(\n                                self.opts['pki_dir'],\n                                keydir,\n                                key),\n                            os.path.join(\n                                self.opts['pki_dir'],\n                                self.ACC,\n                                key)\n                            )\n                    eload = {'result': True,\n                             'act': 'accept',\n                             'id': key}\n                    self.event.fire_event(eload,\n                                          salt.utils.event.tagify(prefix='key'))\n                except (IOError, OSError):\n                    pass\n        return (\n            self.name_match(match) if match is not None\n            else self.dict_match(matches)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\naccepting all keys in pre", "response": "def accept_all(self):\n        '''\n        Accept all keys in pre\n        '''\n        keys = self.list_keys()\n        for key in keys[self.PEND]:\n            try:\n                shutil.move(\n                        os.path.join(\n                            self.opts['pki_dir'],\n                            self.PEND,\n                            key),\n                        os.path.join(\n                            self.opts['pki_dir'],\n                            self.ACC,\n                            key)\n                        )\n                eload = {'result': True,\n                         'act': 'accept',\n                         'id': key}\n                self.event.fire_event(eload,\n                                      salt.utils.event.tagify(prefix='key'))\n            except (IOError, OSError):\n                pass\n        return self.list_keys()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef delete_key(self,\n                    match=None,\n                    match_dict=None,\n                    preserve_minions=None,\n                    revoke_auth=False):\n        '''\n        Delete public keys. If \"match\" is passed, it is evaluated as a glob.\n        Pre-gathered matches can also be passed via \"match_dict\".\n\n        To preserve the master caches of minions who are matched, set preserve_minions\n        '''\n        if match is not None:\n            matches = self.name_match(match)\n        elif match_dict is not None and isinstance(match_dict, dict):\n            matches = match_dict\n        else:\n            matches = {}\n        for status, keys in six.iteritems(matches):\n            for key in keys:\n                try:\n                    if revoke_auth:\n                        if self.opts.get('rotate_aes_key') is False:\n                            print('Immediate auth revocation specified but AES key rotation not allowed. '\n                                     'Minion will not be disconnected until the master AES key is rotated.')\n                        else:\n                            try:\n                                client = salt.client.get_local_client(mopts=self.opts)\n                                client.cmd_async(key, 'saltutil.revoke_auth')\n                            except salt.exceptions.SaltClientError:\n                                print('Cannot contact Salt master. '\n                                      'Connection for {0} will remain up until '\n                                      'master AES key is rotated or auth is revoked '\n                                      'with \\'saltutil.revoke_auth\\'.'.format(key))\n                    os.remove(os.path.join(self.opts['pki_dir'], status, key))\n                    eload = {'result': True,\n                             'act': 'delete',\n                             'id': key}\n                    self.event.fire_event(eload,\n                                          salt.utils.event.tagify(prefix='key'))\n                except (OSError, IOError):\n                    pass\n        if self.opts.get('preserve_minions') is True:\n            self.check_minion_cache(preserve_minions=matches.get('minions', []))\n        else:\n            self.check_minion_cache()\n        if self.opts.get('rotate_aes_key'):\n            salt.crypt.dropfile(self.opts['cachedir'], self.opts['user'])\n        return (\n            self.name_match(match) if match is not None\n            else self.dict_match(matches)\n        )", "response": "Delete public keys. If \"match\" is passed, it is evaluated as a glob.\n        Pre-gathered matches can also be passed via \"match_dict\".\n\n        To preserve the master caches of minions who are matched, set preserve_minions"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndeleting all keys in the minion", "response": "def delete_all(self):\n        '''\n        Delete all keys\n        '''\n        for status, keys in six.iteritems(self.list_keys()):\n            for key in keys:\n                try:\n                    os.remove(os.path.join(self.opts['pki_dir'], status, key))\n                    eload = {'result': True,\n                             'act': 'delete',\n                             'id': key}\n                    self.event.fire_event(eload,\n                                          salt.utils.event.tagify(prefix='key'))\n                except (OSError, IOError):\n                    pass\n        self.check_minion_cache()\n        if self.opts.get('rotate_aes_key'):\n            salt.crypt.dropfile(self.opts['cachedir'], self.opts['user'])\n        return self.list_keys()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef reject_all(self):\n        '''\n        Reject all keys in pre\n        '''\n        keys = self.list_keys()\n        for key in keys[self.PEND]:\n            try:\n                shutil.move(\n                        os.path.join(\n                            self.opts['pki_dir'],\n                            self.PEND,\n                            key),\n                        os.path.join(\n                            self.opts['pki_dir'],\n                            self.REJ,\n                            key)\n                        )\n                eload = {'result': True,\n                         'act': 'reject',\n                         'id': key}\n                self.event.fire_event(eload,\n                                      salt.utils.event.tagify(prefix='key'))\n            except (IOError, OSError):\n                pass\n        self.check_minion_cache()\n        if self.opts.get('rotate_aes_key'):\n            salt.crypt.dropfile(self.opts['cachedir'], self.opts['user'])\n        return self.list_keys()", "response": "Reject all keys in pre\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the fingerprint for a specified key", "response": "def finger(self, match, hash_type=None):\n        '''\n        Return the fingerprint for a specified key\n        '''\n        if hash_type is None:\n            hash_type = __opts__['hash_type']\n\n        matches = self.name_match(match, True)\n        ret = {}\n        for status, keys in six.iteritems(matches):\n            ret[status] = {}\n            for key in keys:\n                if status == 'local':\n                    path = os.path.join(self.opts['pki_dir'], key)\n                else:\n                    path = os.path.join(self.opts['pki_dir'], status, key)\n                ret[status][key] = salt.utils.crypt.pem_finger(path, sum_type=hash_type)\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning fingerprints for all keys", "response": "def finger_all(self, hash_type=None):\n        '''\n        Return fingerprints for all keys\n        '''\n        if hash_type is None:\n            hash_type = __opts__['hash_type']\n\n        ret = {}\n        for status, keys in six.iteritems(self.all_keys()):\n            ret[status] = {}\n            for key in keys:\n                if status == 'local':\n                    path = os.path.join(self.opts['pki_dir'], key)\n                else:\n                    path = os.path.join(self.opts['pki_dir'], status, key)\n                ret[status][key] = salt.utils.crypt.pem_finger(path, sum_type=hash_type)\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the path to the catalina home directory.", "response": "def __catalina_home():\n    '''\n    Tomcat paths differ depending on packaging\n    '''\n    locations = ['/usr/share/tomcat*', '/opt/tomcat']\n    for location in locations:\n        folders = glob.glob(location)\n        if folders:\n            for catalina_home in folders:\n                if os.path.isdir(catalina_home + \"/bin\"):\n                    return catalina_home\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the username and password from opts grains or pillar", "response": "def _get_credentials():\n    '''\n    Get the username and password from opts, grains, or pillar\n    '''\n    ret = {\n        'user': False,\n        'passwd': False\n    }\n\n    # Loop through opts, grains, and pillar\n    # Return the first acceptable configuration found\n    for item in ret:\n        for struct in [__opts__, __grains__, __pillar__]:\n            # Look for the config key\n            # Support old-style config format and new\n            for config_key in __valid_configs[item]:\n                value = salt.utils.data.traverse_dict_and_list(\n                    struct,\n                    config_key,\n                    None)\n                if value:\n                    ret[item] = value\n                    break\n    return ret['user'], ret['passwd']"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a authentication handler.", "response": "def _auth(uri):\n    '''\n    returns a authentication handler.\n    Get user & password from grains, if are not set default to\n    modules.config.option\n\n    If user & pass are missing return False\n    '''\n\n    user, password = _get_credentials()\n    if user is False or password is False:\n        return False\n\n    basic = _HTTPBasicAuthHandler()\n    basic.add_password(realm='Tomcat Manager Application', uri=uri,\n                       user=user, passwd=password)\n    digest = _HTTPDigestAuthHandler()\n    digest.add_password(realm='Tomcat Manager Application', uri=uri,\n                        user=user, passwd=password)\n    return _build_opener(basic, digest)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef extract_war_version(war):\n    '''\n    Extract the version from the war file name. There does not seem to be a\n    standard for encoding the version into the `war file name`_\n\n    .. _`war file name`: https://tomcat.apache.org/tomcat-6.0-doc/deployer-howto.html\n\n    Examples:\n\n    .. code-block:: bash\n\n        /path/salt-2015.8.6.war -> 2015.8.6\n        /path/V6R2013xD5.war -> None\n    '''\n    basename = os.path.basename(war)\n    war_package = os.path.splitext(basename)[0]  # remove '.war'\n    version = re.findall(\"-([\\\\d.-]+)$\", war_package)  # try semver\n    return version[0] if version and len(version) == 1 else None", "response": "Extract the version from the war file name."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _wget(cmd, opts=None, url='http://localhost:8080/manager', timeout=180):\n    '''\n    A private function used to issue the command to tomcat via the manager\n    webapp\n\n    cmd\n        the command to execute\n\n    url\n        The URL of the server manager webapp (example:\n        http://localhost:8080/manager)\n\n    opts\n        a dict of arguments\n\n    timeout\n        timeout for HTTP request\n\n    Return value is a dict in the from of::\n\n        {\n            res: [True|False]\n            msg: list of lines we got back from the manager\n        }\n    '''\n\n    ret = {\n        'res': True,\n        'msg': []\n    }\n\n    # prepare authentication\n    auth = _auth(url)\n    if auth is False:\n        ret['res'] = False\n        ret['msg'] = 'missing username and password settings (grain/pillar)'\n        return ret\n\n    # prepare URL\n    if url[-1] != '/':\n        url += '/'\n    url6 = url\n    url += 'text/{0}'.format(cmd)\n    url6 += '{0}'.format(cmd)\n    if opts:\n        url += '?{0}'.format(_urlencode(opts))\n        url6 += '?{0}'.format(_urlencode(opts))\n\n    # Make the HTTP request\n    _install_opener(auth)\n\n    try:\n        # Trying tomcat >= 7 url\n        ret['msg'] = _urlopen(url, timeout=timeout).read().splitlines()\n    except Exception:\n        try:\n            # Trying tomcat6 url\n            ret['msg'] = _urlopen(url6, timeout=timeout).read().splitlines()\n        except Exception:\n            ret['msg'] = 'Failed to create HTTP request'\n\n    if not ret['msg'][0].startswith('OK'):\n        ret['res'] = False\n\n    return ret", "response": "Internal function to issue a command to tomcat via the manager webapp"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nlist all the deployed webapps", "response": "def ls(url='http://localhost:8080/manager', timeout=180):\n    '''\n    list all the deployed webapps\n\n    url : http://localhost:8080/manager\n        the URL of the server manager webapp\n    timeout : 180\n        timeout for HTTP request\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' tomcat.ls\n        salt '*' tomcat.ls http://localhost:8080/manager\n    '''\n\n    ret = {}\n    data = _wget('list', '', url, timeout=timeout)\n    if data['res'] is False:\n        return {}\n    data['msg'].pop(0)\n    for line in data['msg']:\n        tmp = line.split(':')\n        ret[tmp[0]] = {\n            'mode': tmp[1],\n            'sessions': tmp[2],\n            'fullname': tmp[3],\n            'version': '',\n        }\n        sliced = tmp[3].split('##')\n        if len(sliced) > 1:\n            ret[tmp[0]]['version'] = sliced[1]\n\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the status of the webapp", "response": "def status_webapp(app, url='http://localhost:8080/manager', timeout=180):\n    '''\n    return the status of the webapp (stopped | running | missing)\n\n    app\n        the webapp context path\n    url : http://localhost:8080/manager\n        the URL of the server manager webapp\n    timeout : 180\n        timeout for HTTP request\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' tomcat.status_webapp /jenkins\n        salt '*' tomcat.status_webapp /jenkins http://localhost:8080/manager\n    '''\n\n    webapps = ls(url, timeout=timeout)\n    for i in webapps:\n        if i == app:\n            return webapps[i]['mode']\n\n    return 'missing'"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget serverinfo for webapp", "response": "def serverinfo(url='http://localhost:8080/manager', timeout=180):\n    '''\n    return details about the server\n\n    url : http://localhost:8080/manager\n        the URL of the server manager webapp\n    timeout : 180\n        timeout for HTTP request\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' tomcat.serverinfo\n        salt '*' tomcat.serverinfo http://localhost:8080/manager\n    '''\n\n    data = _wget('serverinfo', {}, url, timeout=timeout)\n    if data['res'] is False:\n        return {'error': data['msg']}\n\n    ret = {}\n    data['msg'].pop(0)\n    for line in data['msg']:\n        tmp = line.split(':')\n        ret[tmp[0].strip()] = tmp[1].strip()\n\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef deploy_war(war,\n               context,\n               force='no',\n               url='http://localhost:8080/manager',\n               saltenv='base',\n               timeout=180,\n               temp_war_location=None,\n               version=True):\n    '''\n    Deploy a WAR file\n\n    war\n        absolute path to WAR file (should be accessible by the user running\n        tomcat) or a path supported by the salt.modules.cp.get_file function\n    context\n        the context path to deploy\n    force : False\n        set True to deploy the webapp even one is deployed in the context\n    url : http://localhost:8080/manager\n        the URL of the server manager webapp\n    saltenv : base\n        the environment for WAR file in used by salt.modules.cp.get_url\n        function\n    timeout : 180\n        timeout for HTTP request\n    temp_war_location : None\n        use another location to temporarily copy to war file\n        by default the system's temp directory is used\n    version : ''\n        Specify the war version.  If this argument is provided, it overrides\n        the version encoded in the war file name, if one is present.\n\n        Examples:\n\n        .. code-block:: bash\n\n            salt '*' tomcat.deploy_war salt://salt-2015.8.6.war version=2015.08.r6\n\n        .. versionadded:: 2015.8.6\n\n    CLI Examples:\n\n    cp module\n\n    .. code-block:: bash\n\n        salt '*' tomcat.deploy_war salt://application.war /api\n        salt '*' tomcat.deploy_war salt://application.war /api no\n        salt '*' tomcat.deploy_war salt://application.war /api yes http://localhost:8080/manager\n\n    minion local file system\n\n    .. code-block:: bash\n\n        salt '*' tomcat.deploy_war /tmp/application.war /api\n        salt '*' tomcat.deploy_war /tmp/application.war /api no\n        salt '*' tomcat.deploy_war /tmp/application.war /api yes http://localhost:8080/manager\n    '''\n    # Decide the location to copy the war for the deployment\n    tfile = 'salt.{0}'.format(os.path.basename(war))\n    if temp_war_location is not None:\n        if not os.path.isdir(temp_war_location):\n            return 'Error - \"{0}\" is not a directory'.format(temp_war_location)\n        tfile = os.path.join(temp_war_location, tfile)\n    else:\n        tfile = os.path.join(tempfile.gettempdir(), tfile)\n\n    # Copy file name if needed\n    cache = False\n    if not os.path.isfile(war):\n        cache = True\n        cached = __salt__['cp.get_url'](war, tfile, saltenv)\n        if not cached:\n            return 'FAIL - could not cache the WAR file'\n        try:\n            __salt__['file.set_mode'](cached, '0644')\n        except KeyError:\n            pass\n    else:\n        tfile = war\n\n    # Prepare options\n    opts = {\n        'war': 'file:{0}'.format(tfile),\n        'path': context,\n    }\n\n    # If parallel versions are desired or not disabled\n    if version:\n        # Set it to defined version or attempt extract\n        version = extract_war_version(war) if version is True else version\n\n        if isinstance(version, _string_types):\n            # Only pass version to Tomcat if not undefined\n            opts['version'] = version\n\n    if force == 'yes':\n        opts['update'] = 'true'\n\n    # Deploy\n    deployed = _wget('deploy', opts, url, timeout=timeout)\n    res = '\\n'.join(deployed['msg'])\n\n    # Cleanup\n    if cache:\n        __salt__['file.remove'](tfile)\n\n    return res", "response": "Deploy a WAR file to the server manager"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef passwd(passwd,\n           user='',\n           alg='sha1',\n           realm=None):\n    '''\n    This function replaces the $CATALINA_HOME/bin/digest.sh script\n    convert a clear-text password to the $CATALINA_BASE/conf/tomcat-users.xml\n    format\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' tomcat.passwd secret\n        salt '*' tomcat.passwd secret tomcat sha1\n        salt '*' tomcat.passwd secret tomcat sha1 'Protected Realm'\n    '''\n    # Shouldn't it be SHA265 instead of SHA1?\n    digest = hasattr(hashlib, alg) and getattr(hashlib, alg) or None\n    if digest:\n        if realm:\n            digest.update('{0}:{1}:{2}'.format(user, realm, passwd, ))\n        else:\n            digest.update(passwd)\n\n    return digest and digest.hexdigest() or False", "response": "This function replaces the $CATALINA_HOME / conf / tomcat - users. xml script\n    convert a clear - text password to the $CATALINA_BASE / conf / tomcat - users. xml script\n    format\n           "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef version():\n    '''\n    Return server version from catalina.sh version\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' tomcat.version\n    '''\n    cmd = __catalina_home() + '/bin/catalina.sh version'\n    out = __salt__['cmd.run'](cmd).splitlines()\n    for line in out:\n        if not line:\n            continue\n        if 'Server version' in line:\n            comps = line.split(': ')\n            return comps[1]", "response": "Return server version from catalina. sh version CLI Example :"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns all server information from catalina. sh version CLI Example : catalina. sh version salt '*' tomcat. fullversion", "response": "def fullversion():\n    '''\n    Return all server information from catalina.sh version\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' tomcat.fullversion\n    '''\n    cmd = __catalina_home() + '/bin/catalina.sh version'\n    ret = {}\n    out = __salt__['cmd.run'](cmd).splitlines()\n    for line in out:\n        if not line:\n            continue\n        if ': ' in line:\n            comps = line.split(': ')\n            ret[comps[0]] = comps[1].lstrip()\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsignaling a catalina container", "response": "def signal(signal=None):\n    '''\n    Signals catalina to start, stop, securestart, forcestop.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' tomcat.signal start\n    '''\n    valid_signals = {'forcestop': 'stop -force',\n                     'securestart': 'start -security',\n                     'start': 'start',\n                     'stop': 'stop'}\n\n    if signal not in valid_signals:\n        return\n\n    cmd = '{0}/bin/catalina.sh {1}'.format(\n        __catalina_home(), valid_signals[signal]\n    )\n    __salt__['cmd.run'](cmd)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_options(ret=None):\n    '''\n    Get the appoptics options from salt.\n    '''\n    attrs = {\n        'api_token': 'api_token',\n        'api_url': 'api_url',\n        'tags': 'tags',\n        'sls_states': 'sls_states'\n    }\n\n    _options = salt.returners.get_returner_options(\n        __virtualname__,\n        ret,\n        attrs,\n        __salt__=__salt__,\n        __opts__=__opts__)\n\n    _options['api_url'] = _options.get('api_url', 'api.appoptics.com')\n    _options['sls_states'] = _options.get('sls_states', [])\n    _options['tags'] = _options.get('tags', {\n        'host_hostname_alias': __salt__['grains.get']('id')\n    })\n\n    log.debug('Retrieved appoptics options: %s', _options)\n    return _options", "response": "Get the appoptics options from salt.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn an appoptics connection object.", "response": "def _get_appoptics(options):\n    '''\n    Return an appoptics connection object.\n    '''\n    conn = appoptics_metrics.connect(\n        options.get('api_token'),\n        sanitizer=appoptics_metrics.sanitize_metric_name,\n        hostname=options.get('api_url'))\n    log.info(\"Connected to appoptics.\")\n    return conn"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef returner(ret):\n    '''\n    Parse the return data and return metrics to AppOptics.\n\n    For each state that's provided in the configuration, return tagged metrics for\n    the result of that state if it's present.\n    '''\n\n    options = _get_options(ret)\n    states_to_report = ['state.highstate']\n    if options.get('sls_states'):\n        states_to_report.append('state.sls')\n    if ret['fun'] in states_to_report:\n        tags = options.get('tags', {}).copy()\n        tags['state_type'] = ret['fun']\n        log.info(\"Tags for this run are %s\", tags)\n        matched_states = set(ret['fun_args']).intersection(\n            set(options.get('sls_states', [])))\n        # What can I do if a run has multiple states that match?\n        # In the mean time, find one matching state name and use it.\n        if matched_states:\n            tags['state_name'] = sorted(matched_states)[0]\n            log.debug('Found returned data from %s.', tags['state_name'])\n        _state_metrics(ret, options, tags)", "response": "Parse the return data and return metrics to AppOptics."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef present(host, service_root, trigger_desc, service_name=None, **kwargs):\n    '''\n    .. versionadded:: Fluorine\n\n    Ensure service exists under service root.\n\n    :param host: Technical name of the host\n    :param service_root: Path of service (path is split by /)\n    :param service_name: Name of service\n    :param trigger_desc: Description of trigger in zabbix\n    :param _connection_user: Optional - zabbix user (can also be set in opts or pillar, see module's docstring)\n    :param _connection_password: Optional - zabbix password (can also be set in opts or pillar, see module's docstring)\n    :param _connection_url: Optional - url of zabbix frontend (can also be set in opts, pillar, see module's docstring)\n\n    .. note::\n        If services on path does not exists they are created.\n\n    .. code-block:: yaml\n        create_service_icmp:\n            zabbix_service.present:\n                - host: server-1\n                - service_root: Server-group/server icmp\n                - service_name: server-1-icmp\n                - trigger_desc: is unavailable by ICMP\n    '''\n    if not service_name:\n        service_name = host\n\n    changes_service_added = {host: {'old': 'Service {0} does not exist under {1}.'.format(service_name, service_root),\n                                    'new': 'Service {0} added under {1}.'.format(service_name, service_root),\n                                    }\n                             }\n\n    connection_args = {}\n    if '_connection_user' in kwargs:\n        connection_args['_connection_user'] = kwargs['_connection_user']\n    if '_connection_password' in kwargs:\n        connection_args['_connection_password'] = kwargs['_connection_password']\n    if '_connection_url' in kwargs:\n        connection_args['_connection_url'] = kwargs['_connection_url']\n\n    ret = {'name': host, 'changes': {}, 'result': False, 'comment': ''}\n\n    host_exists = __salt__['zabbix.host_exists'](host, **connection_args)\n\n    if not host_exists:\n        ret['comment'] = 'Host {0} does not exists.'.format(host)\n        return ret\n\n    host = __salt__['zabbix.host_get'](name=host, **connection_args)[0]\n    hostid = host['hostid']\n\n    trigger = __salt__['zabbix.triggerid_get'](hostid=hostid, trigger_desc=trigger_desc, **kwargs)\n\n    if not trigger:\n        ret['comment'] = 'Trigger with description: \"{0}\" does not exists for host {1}.'.format(\n            trigger_desc, host['name'])\n        return ret\n\n    trigger_id = trigger['result']['triggerid']\n\n    root_services = service_root.split('/')\n    root_id = None\n\n    if __opts__['test']:\n        for root_s in root_services:\n            service = __salt__['zabbix.service_get'](service_rootid=root_id, service_name=root_s, **kwargs)\n            if not service:\n                ret['result'] = None\n                ret['comment'] = \"Service {0} will be added\".format(service_name)\n                ret['changes'] = changes_service_added\n                return ret\n\n            root_id = service[0]['serviceid']\n\n        service = __salt__['zabbix.service_get'](service_rootid=root_id, service_name=service_name, **kwargs)\n        if service:\n            ret['result'] = True\n            ret['comment'] = \"Service {0} already exists\".format(service_name)\n        else:\n            ret['result'] = None\n            ret['comment'] = \"Service {0} will be added\".format(service_name)\n            ret['changes'] = changes_service_added\n        return ret\n\n    root_id = None\n    # ensure that root services exists\n    for root_s in root_services:\n        service = __salt__['zabbix.service_get'](service_rootid=root_id, service_name=root_s, **kwargs)\n        if not service:\n            service = __salt__['zabbix.service_add'](service_rootid=root_id, service_name=root_s, **kwargs)\n            root_id = service['serviceids'][0]\n        else:\n            root_id = service[0]['serviceid']\n\n    service = __salt__['zabbix.service_get'](service_rootid=root_id, service_name=service_name, **kwargs)\n    if not service:\n        service = __salt__['zabbix.service_add'](\n            service_rootid=root_id, service_name=service_name, triggerid=trigger_id, **kwargs)\n        if service:\n            ret['comment'] = \"Service {0} added {1} {0} {2}\".format(service_name, root_id, trigger_id)\n            ret['changes'] = changes_service_added\n            ret['result'] = True\n        else:\n            ret['comment'] = \"Service {0} could not be added\".format(service_name)\n            ret['result'] = False\n\n    else:\n        ret['comment'] = \"Service {0} already exists\".format(service_name)\n        ret['result'] = True\n\n    return ret", "response": "Ensure a service exists under a given service root."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef absent(host, service_root, service_name=None, **kwargs):\n    '''\n    .. versionadded:: Fluorine\n    Ensure service does not exists under service root.\n\n    :param host: Technical name of the host\n    :param service_root: Path of service (path is split /)\n    :param service_name: Name of service\n    :param _connection_user: Optional - zabbix user (can also be set in opts or pillar, see module's docstring)\n    :param _connection_password: Optional - zabbix password (can also be set in opts or pillar, see module's docstring)\n    :param _connection_url: Optional - url of zabbix frontend (can also be set in opts, pillar, see module's docstring)\n\n\n    .. code-block:: yaml\n        delete_service_icmp:\n            zabbix_service.absent:\n                - host: server-1\n                - service_root: server-group/server icmp\n                - service_name: server-1-icmp\n    '''\n    if not service_name:\n        service_name = host\n\n    changes_service_deleted = {host: {'old': 'Service {0} exist under {1}.'.format(service_name, service_root),\n                                      'new': 'Service {0} deleted under {1}.'.format(service_name, service_root),\n                                      }\n                               }\n\n    connection_args = {}\n    if '_connection_user' in kwargs:\n        connection_args['_connection_user'] = kwargs['_connection_user']\n    if '_connection_password' in kwargs:\n        connection_args['_connection_password'] = kwargs['_connection_password']\n    if '_connection_url' in kwargs:\n        connection_args['_connection_url'] = kwargs['_connection_url']\n\n    ret = {'name': host, 'changes': {}, 'result': False, 'comment': ''}\n\n    host_exists = __salt__['zabbix.host_exists'](host, **connection_args)\n\n    if not host_exists:\n        ret['comment'] = 'Host {0} does not exists.'.format(host)\n        return ret\n\n    root_services = service_root.split('/')\n    root_id = None\n\n    if __opts__['test']:\n        for root_s in root_services:\n            service = __salt__['zabbix.service_get'](service_rootid=root_id, service_name=root_s, **kwargs)\n            if not service:\n                ret['result'] = None\n                ret['comment'] = \"Service {0} will be deleted\".format(service_name)\n                ret['changes'] = changes_service_deleted\n                return ret\n\n            root_id = service[0]['serviceid']\n\n        service = __salt__['zabbix.service_get'](service_rootid=root_id, service_name=service_name, **kwargs)\n        if not service:\n            ret['result'] = True\n            ret['comment'] = \"Service {0} does not exists\".format(service_name)\n        else:\n            ret['result'] = None\n            ret['comment'] = \"Service {0} will be deleted\".format(service_name)\n            ret['changes'] = changes_service_deleted\n        return ret\n\n    root_id = None\n    # ensure that root services exists\n    for root_s in root_services:\n        service = __salt__['zabbix.service_get'](service_rootid=root_id, service_name=root_s, **kwargs)\n        if not service:\n            ret['result'] = True\n            ret['comment'] = \"Service {0} does not exists\".format(service_name)\n            return ret\n        else:\n            root_id = service[0]['serviceid']\n\n    service = __salt__['zabbix.service_get'](service_rootid=root_id, service_name=service_name, **kwargs)\n    if not service:\n        ret['result'] = True\n        ret['comment'] = \"Service {0} does not exists\".format(service_name)\n        return ret\n    else:\n        service = __salt__['zabbix.service_delete'](service_id=service[0]['serviceid'], **connection_args)\n        if service:\n            ret['comment'] = \"Service {0} deleted\".format(service_name)\n            ret['changes'] = changes_service_deleted\n            ret['result'] = True\n        else:\n            ret['comment'] = \"Service {0} could not be deleted\".format(service_name)\n            ret['result'] = False\n\n    return ret", "response": "Ensure that a service is absent under a service root."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd all files in the tree.", "response": "def filetree(collector, *paths):\n    '''\n    Add all files in the tree. If the \"path\" is a file,\n    only that file will be added.\n\n    :param path: File or directory\n    :return:\n    '''\n    _paths = []\n    # Unglob\n    for path in paths:\n        _paths += glob.glob(path)\n    for path in set(_paths):\n        if not path:\n            out.error('Path not defined', ident=2)\n        elif not os.path.exists(path):\n            out.warning('Path {} does not exists'.format(path))\n        else:\n            # The filehandler needs to be explicitly passed here, so PyLint needs to accept that.\n            # pylint: disable=W8470\n            if os.path.isfile(path):\n                filename = os.path.basename(path)\n                try:\n                    file_ref = salt.utils.files.fopen(path)  # pylint: disable=W\n                    out.put('Add {}'.format(filename), indent=2)\n                    collector.add(filename)\n                    collector.link(title=path, path=file_ref)\n                except Exception as err:\n                    out.error(err, ident=4)\n            # pylint: enable=W8470\n            else:\n                try:\n                    for fname in os.listdir(path):\n                        fname = os.path.join(path, fname)\n                        filetree(collector, [fname])\n                except Exception as err:\n                    out.error(err, ident=4)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_powercfg_minute_values(scheme, guid, subguid, safe_name):\n    '''\n    Returns the AC/DC values in an dict for a guid and subguid for a the given\n    scheme\n    '''\n    if scheme is None:\n        scheme = _get_current_scheme()\n\n    if __grains__['osrelease'] == '7':\n        cmd = 'powercfg /q {0} {1}'.format(scheme, guid)\n    else:\n        cmd = 'powercfg /q {0} {1} {2}'.format(scheme, guid, subguid)\n    out = __salt__['cmd.run'](cmd, python_shell=False)\n\n    split = out.split('\\r\\n\\r\\n')\n    if len(split) > 1:\n        for s in split:\n            if safe_name in s or subguid in s:\n                out = s\n                break\n    else:\n        out = split[0]\n\n    raw_settings = re.findall(r'Power Setting Index: ([0-9a-fx]+)', out)\n    return {'ac': int(raw_settings[0], 0) / 60,\n            'dc': int(raw_settings[1], 0) / 60}", "response": "Returns the AC and DC values in an array for a guid and subguid for a given scheme"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the value of a given power setting in the given scheme", "response": "def _set_powercfg_value(scheme, sub_group, setting_guid, power, value):\n    '''\n    Sets the AC/DC values of a setting with the given power for the given scheme\n    '''\n    if scheme is None:\n        scheme = _get_current_scheme()\n\n    cmd = 'powercfg /set{0}valueindex {1} {2} {3} {4}' \\\n          ''.format(power, scheme, sub_group, setting_guid, value * 60)\n    return __salt__['cmd.retcode'](cmd, python_shell=False) == 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the monitor timeout in minutes for the given power scheme and returns True if successful False otherwise.", "response": "def set_monitor_timeout(timeout, power='ac', scheme=None):\n    '''\n    Set the monitor timeout in minutes for the given power scheme\n\n    Args:\n        timeout (int):\n            The amount of time in minutes before the monitor will timeout\n\n        power (str):\n            Set the value for AC or DC power. Default is ``ac``. Valid options\n            are:\n\n                - ``ac`` (AC Power)\n                - ``dc`` (Battery)\n\n        scheme (str):\n            The scheme to use, leave as ``None`` to use the current. Default is\n            ``None``. This can be the GUID or the Alias for the Scheme. Known\n            Aliases are:\n\n                - ``SCHEME_BALANCED`` - Balanced\n                - ``SCHEME_MAX`` - Power saver\n                - ``SCHEME_MIN`` - High performance\n\n    Returns:\n        bool: ``True`` if successful, otherwise ``False``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        # Sets the monitor timeout to 30 minutes\n        salt '*' powercfg.set_monitor_timeout 30\n    '''\n    return _set_powercfg_value(\n        scheme=scheme,\n        sub_group='SUB_VIDEO',\n        setting_guid='VIDEOIDLE',\n        power=power,\n        value=timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_disk_timeout(timeout, power='ac', scheme=None):\n    '''\n    Set the disk timeout in minutes for the given power scheme\n\n    Args:\n        timeout (int):\n            The amount of time in minutes before the disk will timeout\n\n        power (str):\n            Set the value for AC or DC power. Default is ``ac``. Valid options\n            are:\n\n                - ``ac`` (AC Power)\n                - ``dc`` (Battery)\n\n        scheme (str):\n            The scheme to use, leave as ``None`` to use the current. Default is\n            ``None``. This can be the GUID or the Alias for the Scheme. Known\n            Aliases are:\n\n                - ``SCHEME_BALANCED`` - Balanced\n                - ``SCHEME_MAX`` - Power saver\n                - ``SCHEME_MIN`` - High performance\n\n    Returns:\n        bool: ``True`` if successful, otherwise ``False``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        # Sets the disk timeout to 30 minutes on battery\n        salt '*' powercfg.set_disk_timeout 30 power=dc\n    '''\n    return _set_powercfg_value(\n        scheme=scheme,\n        sub_group='SUB_DISK',\n        setting_guid='DISKIDLE',\n        power=power,\n        value=timeout)", "response": "Sets the disk timeout in minutes for the given power scheme\n            Returns True if successful False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_standby_timeout(timeout, power='ac', scheme=None):\n    '''\n    Set the standby timeout in minutes for the given power scheme\n\n    Args:\n        timeout (int):\n            The amount of time in minutes before the computer sleeps\n\n        power (str):\n            Set the value for AC or DC power. Default is ``ac``. Valid options\n            are:\n\n                - ``ac`` (AC Power)\n                - ``dc`` (Battery)\n\n        scheme (str):\n            The scheme to use, leave as ``None`` to use the current. Default is\n            ``None``. This can be the GUID or the Alias for the Scheme. Known\n            Aliases are:\n\n                - ``SCHEME_BALANCED`` - Balanced\n                - ``SCHEME_MAX`` - Power saver\n                - ``SCHEME_MIN`` - High performance\n\n    Returns:\n        bool: ``True`` if successful, otherwise ``False``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        # Sets the system standby timeout to 30 minutes on Battery\n        salt '*' powercfg.set_standby_timeout 30 power=dc\n    '''\n    return _set_powercfg_value(\n        scheme=scheme,\n        sub_group='SUB_SLEEP',\n        setting_guid='STANDBYIDLE',\n        power=power,\n        value=timeout)", "response": "Sets the standby timeout in minutes for the given power scheme."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_hibernate_timeout(timeout, power='ac', scheme=None):\n    '''\n    Set the hibernate timeout in minutes for the given power scheme\n\n    Args:\n        timeout (int):\n            The amount of time in minutes before the computer hibernates\n\n        power (str):\n            Set the value for AC or DC power. Default is ``ac``. Valid options\n            are:\n\n                - ``ac`` (AC Power)\n                - ``dc`` (Battery)\n\n        scheme (str):\n            The scheme to use, leave as ``None`` to use the current. Default is\n            ``None``. This can be the GUID or the Alias for the Scheme. Known\n            Aliases are:\n\n                - ``SCHEME_BALANCED`` - Balanced\n                - ``SCHEME_MAX`` - Power saver\n                - ``SCHEME_MIN`` - High performance\n\n    Returns:\n        bool: ``True`` if successful, otherwise ``False``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        # Sets the hibernate timeout to 30 minutes on Battery\n        salt '*' powercfg.set_hibernate_timeout 30 power=dc\n    '''\n    return _set_powercfg_value(\n        scheme=scheme,\n        sub_group='SUB_SLEEP',\n        setting_guid='HIBERNATEIDLE',\n        power=power,\n        value=timeout)", "response": "Sets the hibernate timeout in minutes for the given power scheme and returns a boolean value."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nensure that a pagerduty schedule exists.", "response": "def present(profile='pagerduty', subdomain=None, api_key=None, **kwargs):\n    '''\n    Ensure that a pagerduty schedule exists.\n    This method accepts as args everything defined in\n    https://developer.pagerduty.com/documentation/rest/schedules/create.\n    This means that most arguments are in a dict called \"schedule.\"\n\n    User id's can be pagerduty id, or name, or email address.\n    '''\n    # for convenience, we accept id, name, or email as the user id.\n    kwargs['schedule']['name'] = kwargs['name']  # match PD API structure\n    for schedule_layer in kwargs['schedule']['schedule_layers']:\n        for user in schedule_layer['users']:\n            u = __salt__['pagerduty_util.get_resource']('users',\n                                                        user['user']['id'],\n                                                        ['email', 'name', 'id'],\n                                                        profile=profile,\n                                                        subdomain=subdomain,\n                                                        api_key=api_key)\n            if u is None:\n                raise Exception('unknown user: {0}'.format(user))\n            user['user']['id'] = u['id']\n    r = __salt__['pagerduty_util.resource_present']('schedules',\n                                                    ['name', 'id'],\n                                                    _diff,\n                                                    profile,\n                                                    subdomain,\n                                                    api_key,\n                                                    **kwargs)\n    return r"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _diff(state_data, resource_object):\n    '''helper method to compare salt state info with the PagerDuty API json structure,\n    and determine if we need to update.\n\n    returns the dict to pass to the PD API to perform the update, or empty dict if no update.\n    '''\n\n    state_data['id'] = resource_object['schedule']['id']\n    objects_differ = None\n\n    # first check all the easy top-level properties: everything except the schedule_layers.\n    for k, v in state_data['schedule'].items():\n        if k == 'schedule_layers':\n            continue\n        if v != resource_object['schedule'][k]:\n            objects_differ = '{0} {1} {2}'.format(k, v, resource_object['schedule'][k])\n            break\n\n    # check schedule_layers\n    if not objects_differ:\n        for layer in state_data['schedule']['schedule_layers']:\n            # find matching layer name\n            resource_layer = None\n            for resource_layer in resource_object['schedule']['schedule_layers']:\n                found = False\n                if layer['name'] == resource_layer['name']:\n                    found = True\n                    break\n            if not found:\n                objects_differ = 'layer {0} missing'.format(layer['name'])\n                break\n            # set the id, so that we will update this layer instead of creating a new one\n            layer['id'] = resource_layer['id']\n            # compare contents of layer and resource_layer\n            for k, v in layer.items():\n                if k == 'users':\n                    continue\n                if k == 'start':\n                    continue\n                if v != resource_layer[k]:\n                    objects_differ = 'layer {0} key {1} {2} != {3}'.format(layer['name'], k, v, resource_layer[k])\n                    break\n            if objects_differ:\n                break\n            # compare layer['users']\n            if len(layer['users']) != len(resource_layer['users']):\n                objects_differ = 'num users in layer {0} {1} != {2}'.format(layer['name'], len(layer['users']), len(resource_layer['users']))\n                break\n\n            for user1 in layer['users']:\n                found = False\n                user2 = None\n                for user2 in resource_layer['users']:\n                    # deal with PD API bug: when you submit member_order=N, you get back member_order=N+1\n                    if user1['member_order'] == user2['member_order'] - 1:\n                        found = True\n                        break\n                if not found:\n                    objects_differ = 'layer {0} no one with member_order {1}'.format(layer['name'], user1['member_order'])\n                    break\n                if user1['user']['id'] != user2['user']['id']:\n                    objects_differ = 'layer {0} user at member_order {1} {2} != {3}'.format(layer['name'],\n                                                                                            user1['member_order'],\n                                                                                            user1['user']['id'],\n                                                                                            user2['user']['id'])\n                    break\n    if objects_differ:\n        return state_data\n    else:\n        return {}", "response": "helper method to compare salt state info with PagerDuty API json structure and resource object and return the dict to pass to the PagerDuty API to perform the update."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the identifier of the current object.", "response": "def _cron_id(cron):\n    '''SAFETYBELT, Only set if we really have an identifier'''\n    cid = None\n    if cron['identifier']:\n        cid = cron['identifier']\n    else:\n        cid = SALT_CRON_NO_IDENTIFIER\n    if cid:\n        return _ensure_string(cid)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _cron_matched(cron, cmd, identifier=None):\n    '''Check if:\n      - we find a cron with same cmd, old state behavior\n      - but also be smart enough to remove states changed crons where we do\n        not removed priorly by a cron.absent by matching on the provided\n        identifier.\n        We assure retrocompatibility by only checking on identifier if\n        and only if an identifier was set on the serialized crontab\n    '''\n    ret, id_matched = False, None\n    cid = _cron_id(cron)\n    if cid:\n        if not identifier:\n            identifier = SALT_CRON_NO_IDENTIFIER\n        eidentifier = _ensure_string(identifier)\n        # old style second round\n        # after saving crontab, we must check that if\n        # we have not the same command, but the default id\n        # to not set that as a match\n        if (\n            cron.get('cmd', None) != cmd\n            and cid == SALT_CRON_NO_IDENTIFIER\n            and eidentifier == SALT_CRON_NO_IDENTIFIER\n        ):\n            id_matched = False\n        else:\n            # on saving, be sure not to overwrite a cron\n            # with specific identifier but also track\n            # crons where command is the same\n            # but with the default if that we gonna overwrite\n            if (\n                cron.get('cmd', None) == cmd\n                and cid == SALT_CRON_NO_IDENTIFIER\n                and identifier\n            ):\n                cid = eidentifier\n            id_matched = eidentifier == cid\n    if (\n        ((id_matched is None) and cmd == cron.get('cmd', None))\n        or id_matched\n    ):\n        ret = True\n    return ret", "response": "Check if a cron entry is matched by matching on the provided command and identifier."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _render_tab(lst):\n    '''\n    Takes a tab list structure and renders it to a list for applying it to\n    a file\n    '''\n    ret = []\n    for pre in lst['pre']:\n        ret.append('{0}\\n'.format(pre))\n    if ret:\n        if ret[-1] != TAG:\n            ret.append(TAG)\n    else:\n        ret.append(TAG)\n    for env in lst['env']:\n        if (env['value'] is None) or (env['value'] == \"\"):\n            ret.append('{0}=\"\"\\n'.format(env['name']))\n        else:\n            ret.append('{0}={1}\\n'.format(env['name'], env['value']))\n    for cron in lst['crons']:\n        if cron['comment'] is not None or cron['identifier'] is not None:\n            comment = '#'\n            if cron['comment']:\n                comment += ' {0}'.format(\n                    cron['comment'].replace('\\n', '\\n# '))\n            if cron['identifier']:\n                comment += ' {0}:{1}'.format(SALT_CRON_IDENTIFIER,\n                                             cron['identifier'])\n\n            comment += '\\n'\n            ret.append(comment)\n        ret.append('{0}{1} {2} {3} {4} {5} {6}\\n'.format(\n                            cron['commented'] is True and '#DISABLED#' or '',\n                            cron['minute'],\n                            cron['hour'],\n                            cron['daymonth'],\n                            cron['month'],\n                            cron['dayweek'],\n                            cron['cmd']\n                            )\n                   )\n    for cron in lst['special']:\n        if cron['comment'] is not None or cron['identifier'] is not None:\n            comment = '#'\n            if cron['comment']:\n                comment += ' {0}'.format(\n                    cron['comment'].rstrip().replace('\\n', '\\n# '))\n            if cron['identifier']:\n                comment += ' {0}:{1}'.format(SALT_CRON_IDENTIFIER,\n                                             cron['identifier'])\n\n            comment += '\\n'\n            ret.append(comment)\n        ret.append('{0}{1} {2}\\n'.format(\n                            cron['commented'] is True and '#DISABLED#' or '',\n                            cron['spec'],\n                            cron['cmd']\n                            )\n                  )\n    return ret", "response": "Takes a tab list structure and renders it to a list for applying it to\n    a file\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_cron_cmdstr(path, user=None):\n    '''\n    Returns a format string, to be used to build a crontab command.\n    '''\n    if user:\n        cmd = 'crontab -u {0}'.format(user)\n    else:\n        cmd = 'crontab'\n    return '{0} {1}'.format(cmd, path)", "response": "Returns a format string to be used to build a crontab command."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write_cron_file(user, path):\n    '''\n    Writes the contents of a file to a user's crontab\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' cron.write_cron_file root /tmp/new_cron\n\n    .. versionchanged:: 2015.8.9\n\n    .. note::\n\n        Some OS' do not support specifying user via the `crontab` command i.e. (Solaris, AIX)\n    '''\n    if _check_instance_uid_match(user) or __grains__.get('os_family') in ('Solaris', 'AIX'):\n        return __salt__['cmd.retcode'](_get_cron_cmdstr(path),\n                                       runas=user,\n                                       python_shell=False) == 0\n    else:\n        return __salt__['cmd.retcode'](_get_cron_cmdstr(path, user),\n                                       python_shell=False) == 0", "response": "Writes the contents of a file to a user s crontab\n    CLI Example"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _write_cron_lines(user, lines):\n    '''\n    Takes a list of lines to be committed to a user's crontab and writes it\n    '''\n    lines = [salt.utils.stringutils.to_str(_l) for _l in lines]\n    path = salt.utils.files.mkstemp()\n    if _check_instance_uid_match(user) or __grains__.get('os_family') in ('Solaris', 'AIX'):\n        # In some cases crontab command should be executed as user rather than root\n        with salt.utils.files.fpopen(path, 'w+', uid=__salt__['file.user_to_uid'](user), mode=0o600) as fp_:\n            fp_.writelines(lines)\n        ret = __salt__['cmd.run_all'](_get_cron_cmdstr(path),\n                                      runas=user,\n                                      python_shell=False)\n    else:\n        with salt.utils.files.fpopen(path, 'w+', mode=0o600) as fp_:\n            fp_.writelines(lines)\n        ret = __salt__['cmd.run_all'](_get_cron_cmdstr(path, user),\n                                      python_shell=False)\n    os.remove(path)\n    return ret", "response": "Writes a list of lines to a user s crontab and returns the path to the new crontab file"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _date_time_match(cron, **kwargs):\n    '''\n    Returns true if the minute, hour, etc. params match their counterparts from\n    the dict returned from list_tab().\n    '''\n    return all([kwargs.get(x) is None or cron[x] == six.text_type(kwargs[x])\n                or (six.text_type(kwargs[x]).lower() == 'random' and cron[x] != '*')\n                for x in ('minute', 'hour', 'daymonth', 'month', 'dayweek')])", "response": "Returns true if the minute hour etc. params match their counterparts from\n    the dict returned from list_tab().\n    Returns false otherwise"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef raw_cron(user):\n    '''\n    Return the contents of the user's crontab\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' cron.raw_cron root\n    '''\n    if _check_instance_uid_match(user) or __grains__.get('os_family') in ('Solaris', 'AIX'):\n        cmd = 'crontab -l'\n        # Preserve line endings\n        lines = salt.utils.data.decode(\n            __salt__['cmd.run_stdout'](cmd,\n                                       runas=user,\n                                       ignore_retcode=True,\n                                       rstrip=False,\n                                       python_shell=False)\n        ).splitlines(True)\n    else:\n        cmd = 'crontab -u {0} -l'.format(user)\n        # Preserve line endings\n        lines = salt.utils.data.decode(\n            __salt__['cmd.run_stdout'](cmd,\n                                       ignore_retcode=True,\n                                       rstrip=False,\n                                       python_shell=False)\n        ).splitlines(True)\n\n    if lines and lines[0].startswith('# DO NOT EDIT THIS FILE - edit the master and reinstall.'):\n        del lines[0:3]\n    return ''.join(lines)", "response": "Return the contents of the user s crontab\nTaxonomy CLI Example : bash\nAttributeNames salt '*' cron. raw_cron root\nAttributeNames"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef list_tab(user):\n    '''\n    Return the contents of the specified user's crontab\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' cron.list_tab root\n    '''\n    data = raw_cron(user)\n    ret = {'pre': [],\n           'crons': [],\n           'special': [],\n           'env': []}\n    flag = False\n    comment = None\n    identifier = None\n    for line in data.splitlines():\n        if line == '# Lines below here are managed by Salt, do not edit':\n            flag = True\n            continue\n        if flag:\n            commented_cron_job = False\n            if line.startswith('#DISABLED#'):\n                # It's a commented cron job\n                line = line[10:]\n                commented_cron_job = True\n            if line.startswith('@'):\n                # Its a \"special\" line\n                dat = {}\n                comps = line.split()\n                if len(comps) < 2:\n                    # Invalid line\n                    continue\n                dat['spec'] = comps[0]\n                dat['cmd'] = ' '.join(comps[1:])\n                dat['identifier'] = identifier\n                dat['comment'] = comment\n                dat['commented'] = False\n                if commented_cron_job:\n                    dat['commented'] = True\n                ret['special'].append(dat)\n                identifier = None\n                comment = None\n                commented_cron_job = False\n            elif line.startswith('#'):\n                # It's a comment! Catch it!\n                comment_line = line.lstrip('# ')\n\n                # load the identifier if any\n                if SALT_CRON_IDENTIFIER in comment_line:\n                    parts = comment_line.split(SALT_CRON_IDENTIFIER)\n                    comment_line = parts[0].rstrip()\n                    # skip leading :\n                    if len(parts[1]) > 1:\n                        identifier = parts[1][1:]\n\n                if comment is None:\n                    comment = comment_line\n                else:\n                    comment += '\\n' + comment_line\n            elif line.find('=') > 0 and (' ' not in line or line.index('=') < line.index(' ')):\n                # Appears to be a ENV setup line\n                comps = line.split('=', 1)\n                dat = {}\n                dat['name'] = comps[0]\n                dat['value'] = comps[1]\n                ret['env'].append(dat)\n            elif len(line.split(' ')) > 5:\n                # Appears to be a standard cron line\n                comps = line.split(' ')\n                dat = {'minute': comps[0],\n                       'hour': comps[1],\n                       'daymonth': comps[2],\n                       'month': comps[3],\n                       'dayweek': comps[4],\n                       'identifier': identifier,\n                       'cmd': ' '.join(comps[5:]),\n                       'comment': comment,\n                       'commented': False}\n                if commented_cron_job:\n                    dat['commented'] = True\n                ret['crons'].append(dat)\n                identifier = None\n                comment = None\n                commented_cron_job = False\n        else:\n            ret['pre'].append(line)\n    return ret", "response": "Return the contents of the specified user s crontab\n    CLI Example : bash\n    salt '*' cron. list_tab root\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_entry(user, identifier=None, cmd=None):\n    '''\n    Return the specified entry from user's crontab.\n    identifier will be used if specified, otherwise will lookup cmd\n    Either identifier or cmd should be specified.\n\n    user:\n        User's crontab to query\n\n    identifier:\n        Search for line with identifier\n\n    cmd:\n        Search for cron line with cmd\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' cron.identifier_exists root identifier=task1\n    '''\n    cron_entries = list_tab(user).get('crons', False)\n    for cron_entry in cron_entries:\n        if identifier and cron_entry.get('identifier') == identifier:\n            return cron_entry\n        elif cmd and cron_entry.get('cmd') == cmd:\n            return cron_entry\n    return False", "response": "Get the specified entry from user s crontab."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset a special command in the crontab.", "response": "def set_special(user,\n                special,\n                cmd,\n                commented=False,\n                comment=None,\n                identifier=None):\n    '''\n    Set up a special command in the crontab.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' cron.set_special root @hourly 'echo foobar'\n    '''\n    lst = list_tab(user)\n    for cron in lst['special']:\n        cid = _cron_id(cron)\n        if _cron_matched(cron, cmd, identifier):\n            test_setted_id = (\n                cron['identifier'] is None\n                and SALT_CRON_NO_IDENTIFIER\n                or cron['identifier'])\n            tests = [(cron['comment'], comment),\n                     (cron['commented'], commented),\n                     (identifier, test_setted_id),\n                     (cron['spec'], special)]\n            if cid or identifier:\n                tests.append((cron['cmd'], cmd))\n            if any([_needs_change(x, y) for x, y in tests]):\n                rm_special(user, cmd, identifier=cid)\n\n                # Use old values when setting the new job if there was no\n                # change needed for a given parameter\n                if not _needs_change(cron['spec'], special):\n                    special = cron['spec']\n                if not _needs_change(cron['commented'], commented):\n                    commented = cron['commented']\n                if not _needs_change(cron['comment'], comment):\n                    comment = cron['comment']\n                if not _needs_change(cron['cmd'], cmd):\n                    cmd = cron['cmd']\n                    if (\n                        cid == SALT_CRON_NO_IDENTIFIER\n                    ):\n                        if identifier:\n                            cid = identifier\n                        if (\n                            cid == SALT_CRON_NO_IDENTIFIER\n                            and cron['identifier'] is None\n                        ):\n                            cid = None\n                        cron['identifier'] = cid\n                if not cid or (\n                    cid and not _needs_change(cid, identifier)\n                ):\n                    identifier = cid\n                jret = set_special(user, special, cmd, commented=commented,\n                                   comment=comment, identifier=identifier)\n                if jret == 'new':\n                    return 'updated'\n                else:\n                    return jret\n            return 'present'\n    cron = {'spec': special,\n            'cmd': cmd,\n            'identifier': identifier,\n            'comment': comment,\n            'commented': commented}\n    lst['special'].append(cron)\n\n    comdat = _write_cron_lines(user, _render_tab(lst))\n    if comdat['retcode']:\n        # Failed to commit, return the error\n        return comdat['stderr']\n    return 'new'"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a dict of date and time values to be used in a cron entry", "response": "def _get_cron_date_time(**kwargs):\n    '''\n    Returns a dict of date/time values to be used in a cron entry\n    '''\n    # Define ranges (except daymonth, as it depends on the month)\n    range_max = {\n        'minute': list(list(range(60))),\n        'hour': list(list(range(24))),\n        'month': list(list(range(1, 13))),\n        'dayweek': list(list(range(7)))\n    }\n\n    ret = {}\n    for param in ('minute', 'hour', 'month', 'dayweek'):\n        value = six.text_type(kwargs.get(param, '1')).lower()\n        if value == 'random':\n            ret[param] = six.text_type(random.sample(range_max[param], 1)[0])\n        elif len(value.split(':')) == 2:\n            cron_range = sorted(value.split(':'))\n            start, end = int(cron_range[0]), int(cron_range[1])\n            ret[param] = six.text_type(random.randint(start, end))\n        else:\n            ret[param] = value\n\n    if ret['month'] in '1 3 5 7 8 10 12'.split():\n        daymonth_max = 31\n    elif ret['month'] in '4 6 9 11'.split():\n        daymonth_max = 30\n    else:\n        # This catches both '2' and '*'\n        daymonth_max = 28\n\n    daymonth = six.text_type(kwargs.get('daymonth', '1')).lower()\n    if daymonth == 'random':\n        ret['daymonth'] = \\\n            six.text_type(random.sample(list(list(range(1, (daymonth_max + 1)))), 1)[0])\n    else:\n        ret['daymonth'] = daymonth\n\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting a cron job up for a user.", "response": "def set_job(user,\n            minute,\n            hour,\n            daymonth,\n            month,\n            dayweek,\n            cmd,\n            commented=False,\n            comment=None,\n            identifier=None):\n    '''\n    Sets a cron job up for a specified user.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' cron.set_job root '*' '*' '*' '*' 1 /usr/local/weekly\n    '''\n    # Scrub the types\n    minute = six.text_type(minute).lower()\n    hour = six.text_type(hour).lower()\n    daymonth = six.text_type(daymonth).lower()\n    month = six.text_type(month).lower()\n    dayweek = six.text_type(dayweek).lower()\n    lst = list_tab(user)\n    for cron in lst['crons']:\n        cid = _cron_id(cron)\n        if _cron_matched(cron, cmd, identifier):\n            test_setted_id = (\n                cron['identifier'] is None\n                and SALT_CRON_NO_IDENTIFIER\n                or cron['identifier'])\n            tests = [(cron['comment'], comment),\n                     (cron['commented'], commented),\n                     (identifier, test_setted_id),\n                     (cron['minute'], minute),\n                     (cron['hour'], hour),\n                     (cron['daymonth'], daymonth),\n                     (cron['month'], month),\n                     (cron['dayweek'], dayweek)]\n            if cid or identifier:\n                tests.append((cron['cmd'], cmd))\n            if any([_needs_change(x, y) for x, y in tests]):\n                rm_job(user, cmd, identifier=cid)\n\n                # Use old values when setting the new job if there was no\n                # change needed for a given parameter\n                if not _needs_change(cron['minute'], minute):\n                    minute = cron['minute']\n                if not _needs_change(cron['hour'], hour):\n                    hour = cron['hour']\n                if not _needs_change(cron['daymonth'], daymonth):\n                    daymonth = cron['daymonth']\n                if not _needs_change(cron['month'], month):\n                    month = cron['month']\n                if not _needs_change(cron['dayweek'], dayweek):\n                    dayweek = cron['dayweek']\n                if not _needs_change(cron['commented'], commented):\n                    commented = cron['commented']\n                if not _needs_change(cron['comment'], comment):\n                    comment = cron['comment']\n                if not _needs_change(cron['cmd'], cmd):\n                    cmd = cron['cmd']\n                    if (\n                        cid == SALT_CRON_NO_IDENTIFIER\n                    ):\n                        if identifier:\n                            cid = identifier\n                        if (\n                            cid == SALT_CRON_NO_IDENTIFIER\n                            and cron['identifier'] is None\n                        ):\n                            cid = None\n                        cron['identifier'] = cid\n                if not cid or (\n                    cid and not _needs_change(cid, identifier)\n                ):\n                    identifier = cid\n                jret = set_job(user, minute, hour, daymonth,\n                               month, dayweek, cmd, commented=commented,\n                               comment=comment, identifier=identifier)\n                if jret == 'new':\n                    return 'updated'\n                else:\n                    return jret\n            return 'present'\n    cron = {'cmd': cmd,\n            'identifier': identifier,\n            'comment': comment,\n            'commented': commented}\n    cron.update(_get_cron_date_time(minute=minute, hour=hour,\n                                    daymonth=daymonth, month=month,\n                                    dayweek=dayweek))\n    lst['crons'].append(cron)\n\n    comdat = _write_cron_lines(user, _render_tab(lst))\n    if comdat['retcode']:\n        # Failed to commit, return the error\n        return comdat['stderr']\n    return 'new'"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving a special cron job for a specified user.", "response": "def rm_special(user, cmd, special=None, identifier=None):\n    '''\n    Remove a special cron job for a specified user.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' cron.rm_special root /usr/bin/foo\n    '''\n    lst = list_tab(user)\n    ret = 'absent'\n    rm_ = None\n    for ind in range(len(lst['special'])):\n        if rm_ is not None:\n            break\n        if _cron_matched(lst['special'][ind], cmd, identifier=identifier):\n            if special is None:\n                # No special param was specified\n                rm_ = ind\n            else:\n                if lst['special'][ind]['spec'] == special:\n                    rm_ = ind\n    if rm_ is not None:\n        lst['special'].pop(rm_)\n        ret = 'removed'\n    comdat = _write_cron_lines(user, _render_tab(lst))\n    if comdat['retcode']:\n        # Failed to commit, return the error\n        return comdat['stderr']\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nremoves a cron job for a specified user.", "response": "def rm_job(user,\n           cmd,\n           minute=None,\n           hour=None,\n           daymonth=None,\n           month=None,\n           dayweek=None,\n           identifier=None):\n    '''\n    Remove a cron job for a specified user. If any of the day/time params are\n    specified, the job will only be removed if the specified params match.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' cron.rm_job root /usr/local/weekly\n        salt '*' cron.rm_job root /usr/bin/foo dayweek=1\n    '''\n    lst = list_tab(user)\n    ret = 'absent'\n    rm_ = None\n    for ind in range(len(lst['crons'])):\n        if rm_ is not None:\n            break\n        if _cron_matched(lst['crons'][ind], cmd, identifier=identifier):\n            if not any([x is not None\n                        for x in (minute, hour, daymonth, month, dayweek)]):\n                # No date/time params were specified\n                rm_ = ind\n            else:\n                if _date_time_match(lst['crons'][ind],\n                                    minute=minute,\n                                    hour=hour,\n                                    daymonth=daymonth,\n                                    month=month,\n                                    dayweek=dayweek):\n                    rm_ = ind\n    if rm_ is not None:\n        lst['crons'].pop(rm_)\n        ret = 'removed'\n        comdat = _write_cron_lines(user, _render_tab(lst))\n        if comdat['retcode']:\n            # Failed to commit, return the error\n            return comdat['stderr']\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_env(user, name, value=None):\n    '''\n    Set up an environment variable in the crontab.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' cron.set_env root MAILTO user@example.com\n    '''\n    lst = list_tab(user)\n    for env in lst['env']:\n        if name == env['name']:\n            if value != env['value']:\n                rm_env(user, name)\n                jret = set_env(user, name, value)\n                if jret == 'new':\n                    return 'updated'\n                else:\n                    return jret\n            return 'present'\n    env = {'name': name, 'value': value}\n    lst['env'].append(env)\n    comdat = _write_cron_lines(user, _render_tab(lst))\n    if comdat['retcode']:\n        # Failed to commit, return the error\n        return comdat['stderr']\n    return 'new'", "response": "Set an environment variable in the crontab."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef rm_env(user, name):\n    '''\n    Remove cron environment variable for a specified user.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' cron.rm_env root MAILTO\n    '''\n    lst = list_tab(user)\n    ret = 'absent'\n    rm_ = None\n    for ind in range(len(lst['env'])):\n        if name == lst['env'][ind]['name']:\n            rm_ = ind\n    if rm_ is not None:\n        lst['env'].pop(rm_)\n        ret = 'removed'\n    comdat = _write_cron_lines(user, _render_tab(lst))\n    if comdat['retcode']:\n        # Failed to commit, return the error\n        return comdat['stderr']\n    return ret", "response": "Remove a specific environment variable for a specified user."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef exists(Bucket,\n           region=None, key=None, keyid=None, profile=None):\n    '''\n    Given a bucket name, check to see if the given bucket exists.\n\n    Returns True if the given bucket exists and returns False if the given\n    bucket does not exist.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_s3_bucket.exists mybucket\n\n    '''\n\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        buckets = conn.head_bucket(Bucket=Bucket)\n        return {'exists': True}\n    except ClientError as e:\n        if e.response.get('Error', {}).get('Code') == '404':\n            return {'exists': False}\n        err = __utils__['boto3.get_error'](e)\n        return {'error': err}", "response": "Given a bucket name check to see if the given bucket exists."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngiving a valid config create an S3 Bucket. Returns a dictionary with the name of the bucket and location of the new bucket.", "response": "def create(Bucket,\n           ACL=None, LocationConstraint=None,\n           GrantFullControl=None,\n           GrantRead=None,\n           GrantReadACP=None,\n           GrantWrite=None,\n           GrantWriteACP=None,\n           region=None, key=None, keyid=None, profile=None):\n    '''\n    Given a valid config, create an S3 Bucket.\n\n    Returns {created: true} if the bucket was created and returns\n    {created: False} if the bucket was not created.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_s3_bucket.create my_bucket \\\\\n                         GrantFullControl='emailaddress=example@example.com' \\\\\n                         GrantRead='uri=\"http://acs.amazonaws.com/groups/global/AllUsers\"' \\\\\n                         GrantReadACP='emailaddress=\"exampl@example.com\",id=\"2345678909876432\"' \\\\\n                         LocationConstraint=us-west-1\n\n    '''\n\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        kwargs = {}\n        for arg in ('ACL', 'GrantFullControl',\n                    'GrantRead', 'GrantReadACP',\n                    'GrantWrite', 'GrantWriteACP'):\n            if locals()[arg] is not None:\n                kwargs[arg] = str(locals()[arg])  # future lint: disable=blacklisted-function\n        if LocationConstraint:\n            kwargs['CreateBucketConfiguration'] = {'LocationConstraint': LocationConstraint}\n        location = conn.create_bucket(Bucket=Bucket,\n                                  **kwargs)\n        conn.get_waiter(\"bucket_exists\").wait(Bucket=Bucket)\n        if location:\n            log.info('The newly created bucket name is located at %s', location['Location'])\n\n            return {'created': True, 'name': Bucket, 'Location': location['Location']}\n        else:\n            log.warning('Bucket was not created')\n            return {'created': False}\n    except ClientError as e:\n        return {'created': False, 'error': __utils__['boto3.get_error'](e)}"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef delete(Bucket, MFA=None, RequestPayer=None, Force=False,\n           region=None, key=None, keyid=None, profile=None):\n    '''\n    Given a bucket name, delete it, optionally emptying it first.\n\n    Returns {deleted: true} if the bucket was deleted and returns\n    {deleted: false} if the bucket was not deleted.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_s3_bucket.delete mybucket\n\n    '''\n\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        if Force:\n            empty(Bucket, MFA=MFA, RequestPayer=RequestPayer, region=region,\n                  key=key, keyid=keyid, profile=profile)\n        conn.delete_bucket(Bucket=Bucket)\n        return {'deleted': True}\n    except ClientError as e:\n        return {'deleted': False, 'error': __utils__['boto3.get_error'](e)}", "response": "Given a bucket name delete it optionally emptying it first."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndeletes objects in a given S3 bucket.", "response": "def delete_objects(Bucket, Delete, MFA=None, RequestPayer=None,\n                   region=None, key=None, keyid=None, profile=None):\n    '''\n    Delete objects in a given S3 bucket.\n\n    Returns {deleted: true} if all objects were deleted\n    and {deleted: false, failed: [key, ...]} otherwise\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_s3_bucket.delete_objects mybucket '{Objects: [Key: myobject]}'\n\n    '''\n\n    if isinstance(Delete, six.string_types):\n        Delete = salt.utils.json.loads(Delete)\n    if not isinstance(Delete, dict):\n        raise SaltInvocationError(\"Malformed Delete request.\")\n    if 'Objects' not in Delete:\n        raise SaltInvocationError(\"Malformed Delete request.\")\n\n    failed = []\n    objs = Delete['Objects']\n    for i in range(0, len(objs), 1000):\n        chunk = objs[i:i+1000]\n        subset = {'Objects': chunk, 'Quiet': True}\n        try:\n            args = {'Bucket': Bucket}\n            args.update({'MFA': MFA}) if MFA else None\n            args.update({'RequestPayer': RequestPayer}) if RequestPayer else None\n            args.update({'Delete': subset})\n            conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n            ret = conn.delete_objects(**args)\n            failed += ret.get('Errors', [])\n        except ClientError as e:\n            return {'deleted': False, 'error': __utils__['boto3.get_error'](e)}\n\n    if failed:\n        return {'deleted': False, 'failed': failed}\n    else:\n        return {'deleted': True}"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef describe(Bucket,\n             region=None, key=None, keyid=None, profile=None):\n    '''\n    Given a bucket name describe its properties.\n\n    Returns a dictionary of interesting properties.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_s3_bucket.describe mybucket\n\n    '''\n\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        result = {}\n        conn_dict = {'ACL': conn.get_bucket_acl,\n                     'CORS': conn.get_bucket_cors,\n                     'LifecycleConfiguration': conn.get_bucket_lifecycle_configuration,\n                     'Location': conn.get_bucket_location,\n                     'Logging': conn.get_bucket_logging,\n                     'NotificationConfiguration': conn.get_bucket_notification_configuration,\n                     'Policy': conn.get_bucket_policy,\n                     'Replication': conn.get_bucket_replication,\n                     'RequestPayment': conn.get_bucket_request_payment,\n                     'Versioning': conn.get_bucket_versioning,\n                     'Website': conn.get_bucket_website}\n\n        for key, query in six.iteritems(conn_dict):\n            try:\n                data = query(Bucket=Bucket)\n            except ClientError as e:\n                if e.response.get('Error', {}).get('Code') in (\n                            'NoSuchLifecycleConfiguration',\n                            'NoSuchCORSConfiguration',\n                            'NoSuchBucketPolicy',\n                            'NoSuchWebsiteConfiguration',\n                            'ReplicationConfigurationNotFoundError',\n                            'NoSuchTagSet',\n                            ):\n                    continue\n                raise\n            if 'ResponseMetadata' in data:\n                del data['ResponseMetadata']\n            result[key] = data\n\n        tags = {}\n        try:\n            data = conn.get_bucket_tagging(Bucket=Bucket)\n            for tagdef in data.get('TagSet'):\n                tags[tagdef.get('Key')] = tagdef.get('Value')\n        except ClientError as e:\n            if not e.response.get('Error', {}).get('Code') == 'NoSuchTagSet':\n                raise\n        if tags:\n            result['Tagging'] = tags\n        return {'bucket': result}\n    except ClientError as e:\n        err = __utils__['boto3.get_error'](e)\n        if e.response.get('Error', {}).get('Code') == 'NoSuchBucket':\n            return {'bucket': None}\n        return {'error': __utils__['boto3.get_error'](e)}", "response": "Given a bucket name describe its properties."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef empty(Bucket, MFA=None, RequestPayer=None, region=None, key=None,\n          keyid=None, profile=None):\n    '''\n    Delete all objects in a given S3 bucket.\n\n    Returns {deleted: true} if all objects were deleted\n    and {deleted: false, failed: [key, ...]} otherwise\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_s3_bucket.empty mybucket\n\n    '''\n\n    stuff = list_object_versions(Bucket, region=region, key=key, keyid=keyid,\n                                 profile=profile)\n    Delete = {}\n    Delete['Objects'] = [{'Key': v['Key'], 'VersionId': v['VersionId']} for v in stuff.get('Versions', [])]\n    Delete['Objects'] += [{'Key': v['Key'], 'VersionId': v['VersionId']} for v in stuff.get('DeleteMarkers', [])]\n    if Delete['Objects']:\n        ret = delete_objects(Bucket, Delete, MFA=MFA, RequestPayer=RequestPayer,\n                             region=region, key=key, keyid=keyid, profile=profile)\n        failed = ret.get('failed', [])\n        if failed:\n            return {'deleted': False, 'failed': ret[failed]}\n    return {'deleted': True}", "response": "Delete all objects in a given S3 bucket."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef list(region=None, key=None, keyid=None, profile=None):\n    '''\n    List all buckets owned by the authenticated sender of the request.\n\n    Returns list of buckets\n\n    CLI Example:\n\n    .. code-block:: yaml\n\n        Owner: {...}\n        Buckets:\n          - {...}\n          - {...}\n\n    '''\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        buckets = conn.list_buckets()\n        if not bool(buckets.get('Buckets')):\n            log.warning('No buckets found')\n        if 'ResponseMetadata' in buckets:\n            del buckets['ResponseMetadata']\n        return buckets\n    except ClientError as e:\n        return {'error': __utils__['boto3.get_error'](e)}", "response": "List all buckets owned by the authenticated sender of the request."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlists objects in a given S3 bucket.", "response": "def list_object_versions(Bucket, Delimiter=None, EncodingType=None, Prefix=None,\n                 region=None, key=None, keyid=None, profile=None):\n    '''\n    List objects in a given S3 bucket.\n\n    Returns a list of objects.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_s3_bucket.list_object_versions mybucket\n\n    '''\n\n    try:\n        Versions = []\n        DeleteMarkers = []\n        args = {'Bucket': Bucket}\n        args.update({'Delimiter': Delimiter}) if Delimiter else None\n        args.update({'EncodingType': EncodingType}) if Delimiter else None\n        args.update({'Prefix': Prefix}) if Prefix else None\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        IsTruncated = True\n        while IsTruncated:\n            ret = conn.list_object_versions(**args)\n            IsTruncated = ret.get('IsTruncated', False)\n            if IsTruncated in ('True', 'true', True):\n                args['KeyMarker'] = ret['NextKeyMarker']\n                args['VersionIdMarker'] = ret['NextVersionIdMarker']\n            Versions += ret.get('Versions', [])\n            DeleteMarkers += ret.get('DeleteMarkers', [])\n        return {'Versions': Versions, 'DeleteMarkers': DeleteMarkers}\n    except ClientError as e:\n        return {'error': __utils__['boto3.get_error'](e)}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nlist objects in a given S3 bucket.", "response": "def list_objects(Bucket, Delimiter=None, EncodingType=None, Prefix=None,\n                 FetchOwner=False, StartAfter=None, region=None, key=None,\n                 keyid=None, profile=None):\n    '''\n    List objects in a given S3 bucket.\n\n    Returns a list of objects.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_s3_bucket.list_objects mybucket\n\n    '''\n\n    try:\n        Contents = []\n        args = {'Bucket': Bucket, 'FetchOwner': FetchOwner}\n        args.update({'Delimiter': Delimiter}) if Delimiter else None\n        args.update({'EncodingType': EncodingType}) if Delimiter else None\n        args.update({'Prefix': Prefix}) if Prefix else None\n        args.update({'StartAfter': StartAfter}) if StartAfter else None\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        IsTruncated = True\n        while IsTruncated:\n            ret = conn.list_objects_v2(**args)\n            IsTruncated = ret.get('IsTruncated', False)\n            if IsTruncated in ('True', 'true', True):\n                args['ContinuationToken'] = ret['NextContinuationToken']\n            Contents += ret.get('Contents', [])\n        return {'Contents': Contents}\n    except ClientError as e:\n        return {'error': __utils__['boto3.get_error'](e)}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngive a valid config update the ACL for a bucket.", "response": "def put_acl(Bucket,\n           ACL=None,\n           AccessControlPolicy=None,\n           GrantFullControl=None,\n           GrantRead=None,\n           GrantReadACP=None,\n           GrantWrite=None,\n           GrantWriteACP=None,\n           region=None, key=None, keyid=None, profile=None):\n    '''\n    Given a valid config, update the ACL for a bucket.\n\n    Returns {updated: true} if the ACL was updated and returns\n    {updated: False} if the ACL was not updated.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_s3_bucket.put_acl my_bucket 'public' \\\\\n                         GrantFullControl='emailaddress=example@example.com' \\\\\n                         GrantRead='uri=\"http://acs.amazonaws.com/groups/global/AllUsers\"' \\\\\n                         GrantReadACP='emailaddress=\"exampl@example.com\",id=\"2345678909876432\"'\n\n    '''\n\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        kwargs = {}\n        if AccessControlPolicy is not None:\n            if isinstance(AccessControlPolicy, six.string_types):\n                AccessControlPolicy = salt.utils.json.loads(AccessControlPolicy)\n            kwargs['AccessControlPolicy'] = AccessControlPolicy\n        for arg in ('ACL',\n                    'GrantFullControl',\n                    'GrantRead', 'GrantReadACP',\n                    'GrantWrite', 'GrantWriteACP'):\n            if locals()[arg] is not None:\n                kwargs[arg] = str(locals()[arg])  # future lint: disable=blacklisted-function\n        conn.put_bucket_acl(Bucket=Bucket, **kwargs)\n        return {'updated': True, 'name': Bucket}\n    except ClientError as e:\n        return {'updated': False, 'error': __utils__['boto3.get_error'](e)}"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef put_cors(Bucket,\n           CORSRules,\n           region=None, key=None, keyid=None, profile=None):\n    '''\n    Given a valid config, update the CORS rules for a bucket.\n\n    Returns {updated: true} if CORS was updated and returns\n    {updated: False} if CORS was not updated.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_s3_bucket.put_cors my_bucket '[{\\\\\n              \"AllowedHeaders\":[],\\\\\n              \"AllowedMethods\":[\"GET\"],\\\\\n              \"AllowedOrigins\":[\"*\"],\\\\\n              \"ExposeHeaders\":[],\\\\\n              \"MaxAgeSeconds\":123,\\\\\n        }]'\n\n    '''\n\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        if CORSRules is not None and isinstance(CORSRules, six.string_types):\n            CORSRules = salt.utils.json.loads(CORSRules)\n        conn.put_bucket_cors(Bucket=Bucket, CORSConfiguration={'CORSRules': CORSRules})\n        return {'updated': True, 'name': Bucket}\n    except ClientError as e:\n        return {'updated': False, 'error': __utils__['boto3.get_error'](e)}", "response": "Given a valid config update the CORS rules for a bucket."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngive a valid config update the Lifecycle rules for a bucket.", "response": "def put_lifecycle_configuration(Bucket,\n           Rules,\n           region=None, key=None, keyid=None, profile=None):\n    '''\n    Given a valid config, update the Lifecycle rules for a bucket.\n\n    Returns {updated: true} if Lifecycle was updated and returns\n    {updated: False} if Lifecycle was not updated.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_s3_bucket.put_lifecycle_configuration my_bucket '[{\\\\\n              \"Expiration\": {...},\\\\\n              \"ID\": \"idstring\",\\\\\n              \"Prefix\": \"prefixstring\",\\\\\n              \"Status\": \"enabled\",\\\\\n              \"Transitions\": [{...},],\\\\\n              \"NoncurrentVersionTransitions\": [{...},],\\\\\n              \"NoncurrentVersionExpiration\": {...},\\\\\n        }]'\n\n    '''\n\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        if Rules is not None and isinstance(Rules, six.string_types):\n            Rules = salt.utils.json.loads(Rules)\n        conn.put_bucket_lifecycle_configuration(Bucket=Bucket, LifecycleConfiguration={'Rules': Rules})\n        return {'updated': True, 'name': Bucket}\n    except ClientError as e:\n        return {'updated': False, 'error': __utils__['boto3.get_error'](e)}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngiving a valid config update the logging parameters for a bucket.", "response": "def put_logging(Bucket,\n           TargetBucket=None, TargetPrefix=None, TargetGrants=None,\n           region=None, key=None, keyid=None, profile=None):\n    '''\n    Given a valid config, update the logging parameters for a bucket.\n\n    Returns {updated: true} if parameters were updated and returns\n    {updated: False} if parameters were not updated.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_s3_bucket.put_logging my_bucket log_bucket '[{...}]' prefix\n\n    '''\n\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        logstate = {}\n        targets = {'TargetBucket': TargetBucket,\n                   'TargetGrants': TargetGrants,\n                   'TargetPrefix': TargetPrefix}\n        for key, val in six.iteritems(targets):\n            if val is not None:\n                logstate[key] = val\n        if logstate:\n            logstatus = {'LoggingEnabled': logstate}\n        else:\n            logstatus = {}\n        if TargetGrants is not None and isinstance(TargetGrants, six.string_types):\n            TargetGrants = salt.utils.json.loads(TargetGrants)\n        conn.put_bucket_logging(Bucket=Bucket, BucketLoggingStatus=logstatus)\n        return {'updated': True, 'name': Bucket}\n    except ClientError as e:\n        return {'updated': False, 'error': __utils__['boto3.get_error'](e)}"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef put_notification_configuration(Bucket,\n           TopicConfigurations=None, QueueConfigurations=None,\n           LambdaFunctionConfigurations=None,\n           region=None, key=None, keyid=None, profile=None):\n    '''\n    Given a valid config, update the notification parameters for a bucket.\n\n    Returns {updated: true} if parameters were updated and returns\n    {updated: False} if parameters were not updated.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_s3_bucket.put_notification_configuration my_bucket\n                [{...}] \\\\\n                [{...}] \\\\\n                [{...}]\n\n    '''\n\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        if TopicConfigurations is None:\n            TopicConfigurations = []\n        elif isinstance(TopicConfigurations, six.string_types):\n            TopicConfigurations = salt.utils.json.loads(TopicConfigurations)\n        if QueueConfigurations is None:\n            QueueConfigurations = []\n        elif isinstance(QueueConfigurations, six.string_types):\n            QueueConfigurations = salt.utils.json.loads(QueueConfigurations)\n        if LambdaFunctionConfigurations is None:\n            LambdaFunctionConfigurations = []\n        elif isinstance(LambdaFunctionConfigurations, six.string_types):\n            LambdaFunctionConfigurations = salt.utils.json.loads(LambdaFunctionConfigurations)\n        # TODO allow the user to use simple names & substitute ARNs for those names\n        conn.put_bucket_notification_configuration(Bucket=Bucket, NotificationConfiguration={\n                'TopicConfigurations': TopicConfigurations,\n                'QueueConfigurations': QueueConfigurations,\n                'LambdaFunctionConfigurations': LambdaFunctionConfigurations,\n        })\n        return {'updated': True, 'name': Bucket}\n    except ClientError as e:\n        return {'updated': False, 'error': __utils__['boto3.get_error'](e)}", "response": "Given a valid config update the notification parameters for a bucket."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef put_policy(Bucket, Policy,\n           region=None, key=None, keyid=None, profile=None):\n    '''\n    Given a valid config, update the policy for a bucket.\n\n    Returns {updated: true} if policy was updated and returns\n    {updated: False} if policy was not updated.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_s3_bucket.put_policy my_bucket {...}\n\n    '''\n\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        if Policy is None:\n            Policy = '{}'\n        elif not isinstance(Policy, six.string_types):\n            Policy = salt.utils.json.dumps(Policy)\n        conn.put_bucket_policy(Bucket=Bucket, Policy=Policy)\n        return {'updated': True, 'name': Bucket}\n    except ClientError as e:\n        return {'updated': False, 'error': __utils__['boto3.get_error'](e)}", "response": "Given a valid config update the policy for a bucket."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef put_replication(Bucket, Role, Rules,\n           region=None, key=None, keyid=None, profile=None):\n    '''\n    Given a valid config, update the replication configuration for a bucket.\n\n    Returns {updated: true} if replication configuration was updated and returns\n    {updated: False} if replication configuration was not updated.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_s3_bucket.put_replication my_bucket my_role [...]\n\n    '''\n\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        Role = _get_role_arn(name=Role,\n                             region=region, key=key, keyid=keyid, profile=profile)\n        if Rules is None:\n            Rules = []\n        elif isinstance(Rules, six.string_types):\n            Rules = salt.utils.json.loads(Rules)\n        conn.put_bucket_replication(Bucket=Bucket, ReplicationConfiguration={\n                'Role': Role,\n                'Rules': Rules\n        })\n        return {'updated': True, 'name': Bucket}\n    except ClientError as e:\n        return {'updated': False, 'error': __utils__['boto3.get_error'](e)}", "response": "Given a valid config update the replication configuration for a bucket."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngives a valid config update the request payment configuration for a bucket.", "response": "def put_request_payment(Bucket, Payer,\n           region=None, key=None, keyid=None, profile=None):\n    '''\n    Given a valid config, update the request payment configuration for a bucket.\n\n    Returns {updated: true} if request payment configuration was updated and returns\n    {updated: False} if request payment configuration was not updated.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_s3_bucket.put_request_payment my_bucket Requester\n\n    '''\n\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        conn.put_bucket_request_payment(Bucket=Bucket, RequestPaymentConfiguration={\n                'Payer': Payer,\n        })\n        return {'updated': True, 'name': Bucket}\n    except ClientError as e:\n        return {'updated': False, 'error': __utils__['boto3.get_error'](e)}"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngiving a valid config update the tags for a bucket.", "response": "def put_tagging(Bucket,\n           region=None, key=None, keyid=None, profile=None, **kwargs):\n    '''\n    Given a valid config, update the tags for a bucket.\n\n    Returns {updated: true} if tags were updated and returns\n    {updated: False} if tags were not updated.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_s3_bucket.put_tagging my_bucket my_role [...]\n\n    '''\n\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        tagslist = []\n        for k, v in six.iteritems(kwargs):\n            if six.text_type(k).startswith('__'):\n                continue\n            tagslist.append({'Key': six.text_type(k), 'Value': six.text_type(v)})\n        conn.put_bucket_tagging(Bucket=Bucket, Tagging={\n                'TagSet': tagslist,\n        })\n        return {'updated': True, 'name': Bucket}\n    except ClientError as e:\n        return {'updated': False, 'error': __utils__['boto3.get_error'](e)}"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef put_versioning(Bucket, Status, MFADelete=None, MFA=None,\n           region=None, key=None, keyid=None, profile=None):\n    '''\n    Given a valid config, update the versioning configuration for a bucket.\n\n    Returns {updated: true} if versioning configuration was updated and returns\n    {updated: False} if versioning configuration was not updated.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_s3_bucket.put_versioning my_bucket Enabled\n\n    '''\n\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        VersioningConfiguration = {'Status': Status}\n        if MFADelete is not None:\n            VersioningConfiguration['MFADelete'] = MFADelete\n        kwargs = {}\n        if MFA is not None:\n            kwargs['MFA'] = MFA\n        conn.put_bucket_versioning(Bucket=Bucket,\n                VersioningConfiguration=VersioningConfiguration,\n                **kwargs)\n        return {'updated': True, 'name': Bucket}\n    except ClientError as e:\n        return {'updated': False, 'error': __utils__['boto3.get_error'](e)}", "response": "Given a valid config update the versioning configuration for a bucket."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef put_website(Bucket, ErrorDocument=None, IndexDocument=None,\n           RedirectAllRequestsTo=None, RoutingRules=None,\n           region=None, key=None, keyid=None, profile=None):\n    '''\n    Given a valid config, update the website configuration for a bucket.\n\n    Returns {updated: true} if website configuration was updated and returns\n    {updated: False} if website configuration was not updated.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_s3_bucket.put_website my_bucket IndexDocument='{\"Suffix\":\"index.html\"}'\n\n    '''\n\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        WebsiteConfiguration = {}\n        for key in ('ErrorDocument', 'IndexDocument',\n                    'RedirectAllRequestsTo', 'RoutingRules'):\n            val = locals()[key]\n            if val is not None:\n                if isinstance(val, six.string_types):\n                    WebsiteConfiguration[key] = salt.utils.json.loads(val)\n                else:\n                    WebsiteConfiguration[key] = val\n        conn.put_bucket_website(Bucket=Bucket,\n                WebsiteConfiguration=WebsiteConfiguration)\n        return {'updated': True, 'name': Bucket}\n    except ClientError as e:\n        return {'updated': False, 'error': __utils__['boto3.get_error'](e)}", "response": "Given a valid config update the website configuration for a bucket."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef delete_cors(Bucket,\n           region=None, key=None, keyid=None, profile=None):\n    '''\n    Delete the CORS configuration for the given bucket\n\n    Returns {deleted: true} if CORS was deleted and returns\n    {deleted: False} if CORS was not deleted.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_s3_bucket.delete_cors my_bucket\n\n    '''\n\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        conn.delete_bucket_cors(Bucket=Bucket)\n        return {'deleted': True, 'name': Bucket}\n    except ClientError as e:\n        return {'deleted': False, 'error': __utils__['boto3.get_error'](e)}", "response": "Delete the CORS configuration for the given bucket"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndeleting the lifecycle configuration for the given bucket", "response": "def delete_lifecycle_configuration(Bucket,\n           region=None, key=None, keyid=None, profile=None):\n    '''\n    Delete the lifecycle configuration for the given bucket\n\n    Returns {deleted: true} if Lifecycle was deleted and returns\n    {deleted: False} if Lifecycle was not deleted.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_s3_bucket.delete_lifecycle_configuration my_bucket\n\n    '''\n\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        conn.delete_bucket_lifecycle(Bucket=Bucket)\n        return {'deleted': True, 'name': Bucket}\n    except ClientError as e:\n        return {'deleted': False, 'error': __utils__['boto3.get_error'](e)}"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef delete_replication(Bucket,\n           region=None, key=None, keyid=None, profile=None):\n    '''\n    Delete the replication config from the given bucket\n\n    Returns {deleted: true} if replication configuration was deleted and returns\n    {deleted: False} if replication configuration was not deleted.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_s3_bucket.delete_replication my_bucket\n\n    '''\n\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        conn.delete_bucket_replication(Bucket=Bucket)\n        return {'deleted': True, 'name': Bucket}\n    except ClientError as e:\n        return {'deleted': False, 'error': __utils__['boto3.get_error'](e)}", "response": "Delete the replication config from the given bucket"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef delete_tagging(Bucket,\n           region=None, key=None, keyid=None, profile=None):\n    '''\n    Delete the tags from the given bucket\n\n    Returns {deleted: true} if tags were deleted and returns\n    {deleted: False} if tags were not deleted.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_s3_bucket.delete_tagging my_bucket\n\n    '''\n\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        conn.delete_bucket_tagging(Bucket=Bucket)\n        return {'deleted': True, 'name': Bucket}\n    except ClientError as e:\n        return {'deleted': False, 'error': __utils__['boto3.get_error'](e)}", "response": "Delete the tags from the given bucket\n    Returns True if the tags were deleted False otherwise"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef delete_website(Bucket,\n           region=None, key=None, keyid=None, profile=None):\n    '''\n    Remove the website configuration from the given bucket\n\n    Returns {deleted: true} if website configuration was deleted and returns\n    {deleted: False} if website configuration was not deleted.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_s3_bucket.delete_website my_bucket\n\n    '''\n\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        conn.delete_bucket_website(Bucket=Bucket)\n        return {'deleted': True, 'name': Bucket}\n    except ClientError as e:\n        return {'deleted': False, 'error': __utils__['boto3.get_error'](e)}", "response": "Delete the website configuration from the given bucket"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a boolean indicating whether the user is blacklisted by the current user.", "response": "def user_is_blacklisted(self, user):\n        '''\n        Takes a username as a string and returns a boolean. True indicates that\n        the provided user has been blacklisted\n        '''\n        return not salt.utils.stringutils.check_whitelist_blacklist(user, blacklist=self.blacklist.get('users', []))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef present(profile='pagerduty', subdomain=None, api_key=None, **kwargs):\n    '''\n    Ensure that a pagerduty escalation policy exists.  Will create or update as needed.\n\n    This method accepts as args everything defined in\n    https://developer.pagerduty.com/documentation/rest/escalation_policies/create.\n    In addition, user and schedule id's will be translated from name (or email address)\n    into PagerDuty unique ids.  For example:\n\n    .. code-block:: yaml\n\n    pagerduty_escalation_policy.present:\n        - name: bruce test escalation policy\n        - escalation_rules:\n            - targets:\n                - type: schedule\n                  id: 'bruce test schedule level1'\n                - type: user\n                  id: 'Bruce Sherrod'\n\n    In this example, 'Bruce Sherrod' will be looked up and replaced with the\n    PagerDuty id (usually a 7 digit all-caps string, e.g. PX6GQL7)\n\n    '''\n    # for convenience, we accept id, name, or email for users\n    # and we accept the id or name for schedules\n    for escalation_rule in kwargs['escalation_rules']:\n        for target in escalation_rule['targets']:\n            target_id = None\n            if target['type'] == 'user':\n                user = __salt__['pagerduty_util.get_resource']('users',\n                                                               target['id'],\n                                                               ['email', 'name', 'id'],\n                                                               profile=profile,\n                                                               subdomain=subdomain,\n                                                               api_key=api_key)\n                if user:\n                    target_id = user['id']\n            elif target['type'] == 'schedule':\n                schedule = __salt__['pagerduty_util.get_resource']('schedules',\n                                                                   target['id'],\n                                                                   ['name', 'id'],\n                                                                   profile=profile,\n                                                                   subdomain=subdomain,\n                                                                   api_key=api_key)\n                if schedule:\n                    target_id = schedule['schedule']['id']\n            if target_id is None:\n                raise Exception('unidentified target: {0}'.format(target))\n            target['id'] = target_id\n\n    r = __salt__['pagerduty_util.resource_present']('escalation_policies',\n                                                    ['name', 'id'],\n                                                    _diff,\n                                                    profile,\n                                                    subdomain,\n                                                    api_key,\n                                                    **kwargs)\n    return r", "response": "Ensure that a pagerduty escalation policy exists."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert escalation_rules dict to a string for comparison", "response": "def _escalation_rules_to_string(escalation_rules):\n    'convert escalation_rules dict to a string for comparison'\n    result = ''\n    for rule in escalation_rules:\n        result += 'escalation_delay_in_minutes: {0} '.format(rule['escalation_delay_in_minutes'])\n        for target in rule['targets']:\n            result += '{0}:{1} '.format(target['type'], target['id'])\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _check_perms_changes(name, newperms, runas=None, existing=None):\n    '''\n    Check whether Rabbitmq user's permissions need to be changed.\n    '''\n    if not newperms:\n        return False\n\n    if existing is None:\n        try:\n            existing = __salt__['rabbitmq.list_user_permissions'](name, runas=runas)\n        except CommandExecutionError as err:\n            log.error('Error: %s', err)\n            return False\n\n    perm_need_change = False\n    for vhost_perms in newperms:\n        for vhost, perms in six.iteritems(vhost_perms):\n            if vhost in existing:\n                existing_vhost = existing[vhost]\n                if perms != existing_vhost:\n                    # This checks for setting permissions to nothing in the state,\n                    # when previous state runs have already set permissions to\n                    # nothing. We don't want to report a change in this case.\n                    if existing_vhost == '' and perms == ['', '', '']:\n                        continue\n                    perm_need_change = True\n            else:\n                perm_need_change = True\n\n    return perm_need_change", "response": "Check whether Rabbitmq user s permissions need to be changed."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of Rabbitmq user s tags", "response": "def _get_current_tags(name, runas=None):\n    '''\n    Whether Rabbitmq user's tags need to be changed\n    '''\n    try:\n        return list(__salt__['rabbitmq.list_users'](runas=runas)[name])\n    except CommandExecutionError as err:\n        log.error('Error: %s', err)\n        return []"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nensuring a RabbitMQ user exists and is up to date.", "response": "def present(name,\n            password=None,\n            force=False,\n            tags=None,\n            perms=(),\n            runas=None):\n    '''\n    Ensure the RabbitMQ user exists.\n\n    name\n        User name\n    password\n        User's password, if one needs to be set\n    force\n        If user exists, forcibly change the password\n    tags\n        Optional list of tags for the user\n    perms\n        A list of dicts with vhost keys and 3-tuple values\n    runas\n        Name of the user to run the command\n    '''\n    ret = {'name': name, 'result': False, 'comment': '', 'changes': {}}\n\n    try:\n        user = __salt__['rabbitmq.user_exists'](name, runas=runas)\n    except CommandExecutionError as err:\n        ret['comment'] = 'Error: {0}'.format(err)\n        return ret\n\n    passwd_reqs_update = False\n    if user and password is not None:\n        try:\n            if not __salt__['rabbitmq.check_password'](name,\n                password, runas=runas):\n                passwd_reqs_update = True\n                log.debug('RabbitMQ user %s password update required', name)\n        except CommandExecutionError as err:\n            ret['comment'] = 'Error: {0}'.format(err)\n            return ret\n\n    if user and not any((force, perms, tags, passwd_reqs_update)):\n        log.debug(('RabbitMQ user \\'%s\\' exists, password is up to'\n                   ' date and force is not set.'), name)\n        ret['comment'] = 'User \\'{0}\\' is already present.'.format(name)\n        ret['result'] = True\n        return ret\n\n    if not user:\n        ret['changes'].update({'user':\n                              {'old': '',\n                               'new': name}})\n        if __opts__['test']:\n            ret['result'] = None\n            ret['comment'] = 'User \\'{0}\\' is set to be created.'.format(name)\n            return ret\n\n        log.debug(\n            'RabbitMQ user \\'%s\\' doesn\\'t exist - Creating.', name)\n        try:\n            __salt__['rabbitmq.add_user'](name, password, runas=runas)\n        except CommandExecutionError as err:\n            ret['comment'] = 'Error: {0}'.format(err)\n            return ret\n    else:\n        log.debug('RabbitMQ user \\'%s\\' exists', name)\n        if force or passwd_reqs_update:\n            if password is not None:\n                if not __opts__['test']:\n                    try:\n                        __salt__['rabbitmq.change_password'](name, password, runas=runas)\n                    except CommandExecutionError as err:\n                        ret['comment'] = 'Error: {0}'.format(err)\n                        return ret\n                ret['changes'].update({'password':\n                                      {'old': '',\n                                       'new': 'Set password.'}})\n            else:\n                if not __opts__['test']:\n                    log.debug('Password for %s is not set - Clearing password.', name)\n                    try:\n                        __salt__['rabbitmq.clear_password'](name, runas=runas)\n                    except CommandExecutionError as err:\n                        ret['comment'] = 'Error: {0}'.format(err)\n                        return ret\n                ret['changes'].update({'password':\n                                      {'old': 'Removed password.',\n                                       'new': ''}})\n\n    if tags is not None:\n        current_tags = _get_current_tags(name, runas=runas)\n        if isinstance(tags, six.string_types):\n            tags = tags.split()\n        # Diff the tags sets. Symmetric difference operator ^ will give us\n        # any element in one set, but not both\n        if set(tags) ^ set(current_tags):\n            if not __opts__['test']:\n                try:\n                    __salt__['rabbitmq.set_user_tags'](name, tags, runas=runas)\n                except CommandExecutionError as err:\n                    ret['comment'] = 'Error: {0}'.format(err)\n                    return ret\n            ret['changes'].update({'tags':\n                                  {'old': current_tags,\n                                   'new': tags}})\n    try:\n        existing_perms = __salt__['rabbitmq.list_user_permissions'](name, runas=runas)\n    except CommandExecutionError as err:\n        ret['comment'] = 'Error: {0}'.format(err)\n        return ret\n\n    if _check_perms_changes(name, perms, runas=runas, existing=existing_perms):\n        for vhost_perm in perms:\n            for vhost, perm in six.iteritems(vhost_perm):\n                if not __opts__['test']:\n                    try:\n                        __salt__['rabbitmq.set_permissions'](\n                            vhost, name, perm[0], perm[1], perm[2], runas=runas\n                        )\n                    except CommandExecutionError as err:\n                        ret['comment'] = 'Error: {0}'.format(err)\n                        return ret\n                new_perms = {vhost: perm}\n                if existing_perms != new_perms:\n                    if ret['changes'].get('perms') is None:\n                        ret['changes'].update({'perms':\n                                              {'old': {},\n                                               'new': {}}})\n                    ret['changes']['perms']['old'].update(existing_perms)\n                    ret['changes']['perms']['new'].update(new_perms)\n\n    ret['result'] = True\n    if ret['changes'] == {}:\n        ret['comment'] = '\\'{0}\\' is already in the desired state.'.format(name)\n        return ret\n\n    if __opts__['test']:\n        ret['result'] = None\n        ret['comment'] = 'Configuration for \\'{0}\\' will change.'.format(name)\n        return ret\n\n    ret['comment'] = '\\'{0}\\' was configured.'.format(name)\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef absent(name,\n           runas=None):\n    '''\n    Ensure the named user is absent\n\n    name\n        The name of the user to remove\n    runas\n        User to run the command\n    '''\n    ret = {'name': name, 'result': False, 'comment': '', 'changes': {}}\n\n    try:\n        user_exists = __salt__['rabbitmq.user_exists'](name, runas=runas)\n    except CommandExecutionError as err:\n        ret['comment'] = 'Error: {0}'.format(err)\n        return ret\n\n    if user_exists:\n        if not __opts__['test']:\n            try:\n                __salt__['rabbitmq.delete_user'](name, runas=runas)\n            except CommandExecutionError as err:\n                ret['comment'] = 'Error: {0}'.format(err)\n                return ret\n        ret['changes'].update({'name':\n                              {'old': name,\n                               'new': ''}})\n    else:\n        ret['result'] = True\n        ret['comment'] = 'The user \\'{0}\\' is not present.'.format(name)\n        return ret\n\n    if __opts__['test'] and ret['changes']:\n        ret['result'] = None\n        ret['comment'] = 'The user \\'{0}\\' will be removed.'.format(name)\n        return ret\n\n    ret['result'] = True\n    ret['comment'] = 'The user \\'{0}\\' was removed.'.format(name)\n    return ret", "response": "Ensure the named user is absent"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef pause(jid, state_id=None, duration=None):\n    '''\n    Set up a state id pause, this instructs a running state to pause at a given\n    state id. This needs to pass in the jid of the running state and can\n    optionally pass in a duration in seconds.\n    '''\n    minion = salt.minion.MasterMinion(__opts__)\n    minion.functions['state.pause'](jid, state_id, duration)", "response": "Pause a state id at a given jid"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef resume(jid, state_id=None):\n    '''\n    Remove a pause from a jid, allowing it to continue\n    '''\n    minion = salt.minion.MasterMinion(__opts__)\n    minion.functions['state.resume'](jid, state_id)", "response": "Resume a paused state"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting up a state run to die before executing the given state id, this instructs a running state to safely exit at a given state id. This needs to pass in the jid of the running state. If a state_id is not passed then the jid referenced will be safely exited at the beginning of the next state run.", "response": "def soft_kill(jid, state_id=None):\n    '''\n    Set up a state run to die before executing the given state id,\n    this instructs a running state to safely exit at a given\n    state id. This needs to pass in the jid of the running state.\n    If a state_id is not passed then the jid referenced will be safely exited\n    at the beginning of the next state run.\n    '''\n    minion = salt.minion.MasterMinion(__opts__)\n    minion.functions['state.soft_kill'](jid, state_id)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef orchestrate(mods,\n                saltenv='base',\n                test=None,\n                exclude=None,\n                pillar=None,\n                pillarenv=None,\n                pillar_enc=None,\n                orchestration_jid=None):\n    '''\n    .. versionadded:: 0.17.0\n\n    Execute a state run from the master, used as a powerful orchestration\n    system.\n\n    .. seealso:: More Orchestrate documentation\n\n        * :ref:`Full Orchestrate Tutorial <orchestrate-runner>`\n        * :py:mod:`Docs for the master-side state module <salt.states.saltmod>`\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt-run state.orchestrate webserver\n        salt-run state.orchestrate webserver saltenv=dev test=True\n        salt-run state.orchestrate webserver saltenv=dev pillarenv=aws\n\n    .. versionchanged:: 2014.1.1\n\n        Runner renamed from ``state.sls`` to ``state.orchestrate``\n\n    .. versionchanged:: 2014.7.0\n\n        Runner uses the pillar variable\n\n    .. versionchanged:: develop\n\n        Runner uses the pillar_enc variable that allows renderers to render the pillar.\n        This is usable when supplying the contents of a file as pillar, and the file contains\n        gpg-encrypted entries.\n\n    .. seealso:: GPG renderer documentation\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n       salt-run state.orchestrate webserver pillar_enc=gpg pillar=\"$(cat somefile.json)\"\n\n    '''\n    if pillar is not None and not isinstance(pillar, dict):\n        raise SaltInvocationError(\n            'Pillar data must be formatted as a dictionary'\n        )\n    __opts__['file_client'] = 'local'\n    minion = salt.minion.MasterMinion(__opts__)\n\n    if pillarenv is None and 'pillarenv' in __opts__:\n        pillarenv = __opts__['pillarenv']\n    if saltenv is None and 'saltenv' in __opts__:\n        saltenv = __opts__['saltenv']\n    if orchestration_jid is None:\n        orchestration_jid = salt.utils.jid.gen_jid(__opts__)\n\n    running = minion.functions['state.sls'](\n            mods,\n            test,\n            exclude,\n            pillar=pillar,\n            saltenv=saltenv,\n            pillarenv=pillarenv,\n            pillar_enc=pillar_enc,\n            __pub_jid=orchestration_jid,\n            orchestration_jid=orchestration_jid)\n    ret = {'data': {minion.opts['id']: running}, 'outputter': 'highstate'}\n    res = __utils__['state.check_result'](ret['data'])\n    if res:\n        ret['retcode'] = 0\n    else:\n        ret['retcode'] = 1\n    return ret", "response": "Execute a state run from the master and return the state module id"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nexecute a single state orchestration routine", "response": "def orchestrate_single(fun, name, test=None, queue=False, pillar=None, **kwargs):\n    '''\n    Execute a single state orchestration routine\n\n    .. versionadded:: 2015.5.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run state.orchestrate_single fun=salt.wheel name=key.list_all\n    '''\n    if pillar is not None and not isinstance(pillar, dict):\n        raise SaltInvocationError(\n            'Pillar data must be formatted as a dictionary'\n        )\n    __opts__['file_client'] = 'local'\n    minion = salt.minion.MasterMinion(__opts__)\n    running = minion.functions['state.single'](\n            fun,\n            name,\n            test=None,\n            queue=False,\n            pillar=pillar,\n            **kwargs)\n    ret = {minion.opts['id']: running}\n    __jid_event__.fire_event({'data': ret, 'outputter': 'highstate'}, 'progress')\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef orchestrate_high(data, test=None, queue=False, pillar=None, **kwargs):\n    '''\n    Execute a single state orchestration routine\n\n    .. versionadded:: 2015.5.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run state.orchestrate_high '{\n            stage_one:\n                {salt.state: [{tgt: \"db*\"}, {sls: postgres_setup}]},\n            stage_two:\n                {salt.state: [{tgt: \"web*\"}, {sls: apache_setup}, {\n                    require: [{salt: stage_one}],\n                }]},\n            }'\n    '''\n    if pillar is not None and not isinstance(pillar, dict):\n        raise SaltInvocationError(\n            'Pillar data must be formatted as a dictionary'\n        )\n    __opts__['file_client'] = 'local'\n    minion = salt.minion.MasterMinion(__opts__)\n    running = minion.functions['state.high'](\n            data,\n            test=None,\n            queue=False,\n            pillar=pillar,\n            **kwargs)\n    ret = {minion.opts['id']: running}\n    __jid_event__.fire_event({'data': ret, 'outputter': 'highstate'}, 'progress')\n    return ret", "response": "Execute a single state orchestration routine"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndisplay the state data from a specific sls or list of sls files", "response": "def orchestrate_show_sls(mods,\n                         saltenv='base',\n                         test=None,\n                         queue=False,\n                         pillar=None,\n                         pillarenv=None,\n                         pillar_enc=None):\n    '''\n    Display the state data from a specific sls, or list of sls files, after\n    being render using the master minion.\n\n    Note, the master minion adds a \"_master\" suffix to it's minion id.\n\n    .. seealso:: The state.show_sls module function\n\n    CLI Example:\n    .. code-block:: bash\n\n        salt-run state.orch_show_sls my-orch-formula.my-orch-state 'pillar={ nodegroup: ng1 }'\n    '''\n    if pillar is not None and not isinstance(pillar, dict):\n        raise SaltInvocationError(\n            'Pillar data must be formatted as a dictionary')\n\n    __opts__['file_client'] = 'local'\n    minion = salt.minion.MasterMinion(__opts__)\n    running = minion.functions['state.show_sls'](\n        mods,\n        test,\n        queue,\n        pillar=pillar,\n        pillarenv=pillarenv,\n        pillar_enc=pillar_enc,\n        saltenv=saltenv)\n\n    ret = {minion.opts['id']: running}\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef event(tagmatch='*',\n          count=-1,\n          quiet=False,\n          sock_dir=None,\n          pretty=False,\n          node='master'):\n    r'''\n    Watch Salt's event bus and block until the given tag is matched\n\n    .. versionadded:: 2014.7.0\n    .. versionchanged:: 2019.2.0\n        ``tagmatch`` can now be either a glob or regular expression.\n\n    This is useful for utilizing Salt's event bus from shell scripts or for\n    taking simple actions directly from the CLI.\n\n    Enable debug logging to see ignored events.\n\n    :param tagmatch: the event is written to stdout for each tag that matches\n        this glob or regular expression.\n    :param count: this number is decremented for each event that matches the\n        ``tagmatch`` parameter; pass ``-1`` to listen forever.\n    :param quiet: do not print to stdout; just block\n    :param sock_dir: path to the Salt master's event socket file.\n    :param pretty: Output the JSON all on a single line if ``False`` (useful\n        for shell tools); pretty-print the JSON output if ``True``.\n    :param node: Watch the minion-side or master-side event bus.\n        .. versionadded:: 2016.3.0\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        # Reboot a minion and run highstate when it comes back online\n        salt 'jerry' system.reboot && \\\\\n            salt-run state.event 'salt/minion/jerry/start' count=1 quiet=True && \\\\\n            salt 'jerry' state.highstate\n\n        # Reboot multiple minions and run highstate when all are back online\n        salt -L 'kevin,stewart,dave' system.reboot && \\\\\n            salt-run state.event 'salt/minion/*/start' count=3 quiet=True && \\\\\n            salt -L 'kevin,stewart,dave' state.highstate\n\n        # Watch the event bus forever in a shell while-loop.\n        salt-run state.event | while read -r tag data; do\n            echo $tag\n            echo $data | jq --color-output .\n        done\n\n    .. seealso::\n\n        See :blob:`tests/eventlisten.sh` for an example of usage within a shell\n        script.\n    '''\n    statemod = salt.loader.raw_mod(__opts__, 'state', None)\n\n    return statemod['state.event'](\n               tagmatch=tagmatch,\n               count=count,\n               quiet=quiet,\n               sock_dir=sock_dir,\n               pretty=pretty,\n               node=node)", "response": "r Watch the event bus and block until the given tag is matched."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsend a message to an SMTP recipient.", "response": "def send_msg(recipient,\n             message,\n             subject='Message from Salt',\n             sender=None,\n             server=None,\n             use_ssl='True',\n             username=None,\n             password=None,\n             profile=None,\n             attachments=None):\n    '''\n    Send a message to an SMTP recipient. Designed for use in states.\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' smtp.send_msg 'admin@example.com' 'This is a salt module test' profile='my-smtp-account'\n        salt '*' smtp.send_msg 'admin@example.com' 'This is a salt module test' username='myuser' password='verybadpass' sender='admin@example.com' server='smtp.domain.com'\n        salt '*' smtp.send_msg 'admin@example.com' 'This is a salt module test' username='myuser' password='verybadpass' sender='admin@example.com' server='smtp.domain.com' attachments=\"['/var/log/messages']\"\n    '''\n    if profile:\n        creds = __salt__['config.option'](profile)\n        server = creds.get('smtp.server')\n        use_ssl = creds.get('smtp.tls')\n        sender = creds.get('smtp.sender')\n        username = creds.get('smtp.username')\n        password = creds.get('smtp.password')\n\n    if attachments:\n        msg = email.mime.multipart.MIMEMultipart()\n        msg.attach(email.mime.text.MIMEText(message))\n    else:\n        msg = email.mime.text.MIMEText(message)\n    msg['Subject'] = subject\n    msg['From'] = sender\n    msg['To'] = recipient\n    recipients = [r.strip() for r in recipient.split(',')]\n\n    try:\n        if use_ssl in ['True', 'true']:\n            smtpconn = smtplib.SMTP_SSL(server)\n        else:\n            smtpconn = smtplib.SMTP(server)\n\n    except socket.gaierror as _error:\n        log.debug(\"Exception: %s\", _error)\n        return False\n\n    if use_ssl not in ('True', 'true'):\n        smtpconn.ehlo()\n        if smtpconn.has_extn('STARTTLS'):\n            try:\n                smtpconn.starttls()\n            except smtplib.SMTPHeloError:\n                log.debug(\"The server didn\u2019t reply properly \\\n                        to the HELO greeting.\")\n                return False\n            except smtplib.SMTPException:\n                log.debug(\"The server does not support the STARTTLS extension.\")\n                return False\n            except RuntimeError:\n                log.debug(\"SSL/TLS support is not available \\\n                        to your Python interpreter.\")\n                return False\n            smtpconn.ehlo()\n\n    if username and password:\n        try:\n            smtpconn.login(username, password)\n        except smtplib.SMTPAuthenticationError as _error:\n            log.debug(\"SMTP Authentication Failure\")\n            return False\n\n    if attachments:\n        for f in attachments:\n            name = os.path.basename(f)\n            with salt.utils.files.fopen(f, 'rb') as fin:\n                att = email.mime.application.MIMEApplication(fin.read(), Name=name)\n            att['Content-Disposition'] = 'attachment; filename=\"{0}\"'.format(name)\n            msg.attach(att)\n\n    try:\n        smtpconn.sendmail(sender, recipients, msg.as_string())\n    except smtplib.SMTPRecipientsRefused:\n        log.debug(\"All recipients were refused.\")\n        return False\n    except smtplib.SMTPHeloError:\n        log.debug(\"The server didn\u2019t reply properly to the HELO greeting.\")\n        return False\n    except smtplib.SMTPSenderRefused:\n        log.debug(\"The server didn\u2019t accept the %s.\", sender)\n        return False\n    except smtplib.SMTPDataError:\n        log.debug(\"The server replied with an unexpected error code.\")\n        return False\n\n    smtpconn.quit()\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a distinguished name to a specific policy distinguished name list.", "response": "def add_distinguished_name(list_name, item_name):\n    '''\n    Adds a distinguished name to a distinguished name list.\n\n    list_name(str): The name of the specific policy distinguished name list to append to.\n\n    item_name(str): The distinguished name to append.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' bluecoat_sslv.add_distinguished_name MyDistinguishedList cn=foo.bar.com\n\n    '''\n    payload = {\"jsonrpc\": \"2.0\",\n               \"id\": \"ID0\",\n               \"method\": \"add_policy_distinguished_names\",\n               \"params\": [list_name, {\"item_name\": item_name}]}\n\n    response = __proxy__['bluecoat_sslv.call'](payload, True)\n\n    return _validate_change_result(response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds a list of policy distinguished names to the current node.", "response": "def add_distinguished_name_list(list_name):\n    '''\n    Add a list of policy distinguished names.\n\n    list_name(str): The name of the specific policy distinguished name list to add.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' bluecoat_sslv.add_distinguished_name_list MyDistinguishedList\n\n    '''\n    payload = {\"jsonrpc\": \"2.0\",\n               \"id\": \"ID0\",\n               \"method\": \"add_policy_distinguished_names_list\",\n               \"params\": [{\"list_name\": list_name}]}\n\n    response = __proxy__['bluecoat_sslv.call'](payload, True)\n\n    return _validate_change_result(response)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds a domain name to a specific policy domain name list.", "response": "def add_domain_name(list_name, item_name):\n    '''\n    Adds a domain name to a domain name list.\n\n    list_name(str): The name of the specific policy domain name list to append to.\n\n    item_name(str): The domain name to append.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' bluecoat_sslv.add_domain_name MyDomainName foo.bar.com\n\n    '''\n    payload = {\"jsonrpc\": \"2.0\",\n               \"id\": \"ID0\",\n               \"method\": \"add_policy_domain_names\",\n               \"params\": [list_name, {\"item_name\": item_name}]}\n\n    response = __proxy__['bluecoat_sslv.call'](payload, True)\n\n    return _validate_change_result(response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a list of policy domain names.", "response": "def add_domain_name_list(list_name):\n    '''\n    Add a list of policy domain names.\n\n    list_name(str): The name of the specific policy domain name list to add.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' bluecoat_sslv.add_domain_name_list MyDomainNameList\n\n    '''\n    payload = {\"jsonrpc\": \"2.0\",\n               \"id\": \"ID0\",\n               \"method\": \"add_policy_domain_names_list\",\n               \"params\": [{\"list_name\": list_name}]}\n\n    response = __proxy__['bluecoat_sslv.call'](payload, True)\n\n    return _validate_change_result(response)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd an IP address to an IP address list.", "response": "def add_ip_address(list_name, item_name):\n    '''\n    Add an IP address to an IP address list.\n\n    list_name(str): The name of the specific policy IP address list to append to.\n\n    item_name(str): The IP address to append to the list.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' bluecoat_sslv.add_ip_address MyIPAddressList 10.0.0.0/24\n\n    '''\n    payload = {\"jsonrpc\": \"2.0\",\n               \"id\": \"ID0\",\n               \"method\": \"add_policy_ip_addresses\",\n               \"params\": [list_name, {\"item_name\": item_name}]}\n\n    response = __proxy__['bluecoat_sslv.call'](payload, True)\n\n    return _validate_change_result(response)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding a specific IP address list to the current list.", "response": "def add_ip_address_list(list_name):\n    '''\n    Retrieves a list of all IP address lists.\n\n    list_name(str): The name of the specific IP address list to add.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' bluecoat_sslv.add_ip_address_list MyIPAddressList\n\n    '''\n    payload = {\"jsonrpc\": \"2.0\",\n               \"id\": \"ID0\",\n               \"method\": \"add_policy_ip_addresses_list\",\n               \"params\": [{\"list_name\": list_name}]}\n\n    response = __proxy__['bluecoat_sslv.call'](payload, True)\n\n    return _validate_change_result(response)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_distinguished_name_list(list_name):\n    '''\n    Retrieves a specific policy distinguished name list.\n\n    list_name(str): The name of the specific policy distinguished name list to retrieve.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' bluecoat_sslv.get_distinguished_name_list MyDistinguishedList\n\n    '''\n    payload = {\"jsonrpc\": \"2.0\",\n               \"id\": \"ID0\",\n               \"method\": \"get_policy_distinguished_names\",\n               \"params\": [list_name, 0, 256]}\n\n    response = __proxy__['bluecoat_sslv.call'](payload, False)\n\n    return _convert_to_list(response, 'item_name')", "response": "Retrieves a specific policy distinguished name list."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nretrieve a specific policy domain name list.", "response": "def get_domain_list(list_name):\n    '''\n    Retrieves a specific policy domain name list.\n\n    list_name(str): The name of the specific policy domain name list to retrieve.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' bluecoat_sslv.get_domain_list MyDomainNameList\n\n    '''\n    payload = {\"jsonrpc\": \"2.0\",\n               \"id\": \"ID0\",\n               \"method\": \"get_policy_domain_names\",\n               \"params\": [list_name, 0, 256]}\n\n    response = __proxy__['bluecoat_sslv.call'](payload, False)\n\n    return _convert_to_list(response, 'item_name')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_ip_address_list(list_name):\n    '''\n    Retrieves a specific IP address list.\n\n    list_name(str): The name of the specific policy IP address list to retrieve.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' bluecoat_sslv.get_ip_address_list MyIPAddressList\n\n    '''\n    payload = {\"jsonrpc\": \"2.0\",\n               \"id\": \"ID0\",\n               \"method\": \"get_policy_ip_addresses\",\n               \"params\": [list_name, 0, 256]}\n\n    response = __proxy__['bluecoat_sslv.call'](payload, False)\n\n    return _convert_to_list(response, 'item_name')", "response": "Retrieves a specific IP address list."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef runas(cmdLine, username, password=None, cwd=None):\n    '''\n    Run a command as another user. If the process is running as an admin or\n    system account this method does not require a password. Other non\n    privileged accounts need to provide a password for the user to runas.\n    Commands are run in with the highest level privileges possible for the\n    account provided.\n    '''\n\n    # Elevate the token from the current process\n    access = (\n        win32security.TOKEN_QUERY |\n        win32security.TOKEN_ADJUST_PRIVILEGES\n    )\n    th = win32security.OpenProcessToken(win32api.GetCurrentProcess(), access)\n    salt.platform.win.elevate_token(th)\n\n    # Try to impersonate the SYSTEM user. This process needs to be running as a\n    # user who as been granted the SeImpersonatePrivilege, Administrator\n    # accounts have this permission by default.\n    try:\n        impersonation_token = salt.platform.win.impersonate_sid(\n            salt.platform.win.SYSTEM_SID,\n            session_id=0,\n            privs=['SeTcbPrivilege'],\n        )\n    except WindowsError:  # pylint: disable=undefined-variable\n        log.debug(\"Unable to impersonate SYSTEM user\")\n        impersonation_token = None\n\n    # Impersonation of the SYSTEM user failed. Fallback to an un-privileged\n    # runas.\n    if not impersonation_token:\n        log.debug(\"No impersonation token, using unprivileged runas\")\n        return runas_unpriv(cmdLine, username, password, cwd)\n\n    username, domain = split_username(username)\n    # Validate the domain and sid exist for the username\n    try:\n        _, domain, _ = win32security.LookupAccountName(domain, username)\n    except pywintypes.error as exc:\n        message = win32api.FormatMessage(exc.winerror).rstrip('\\n')\n        raise CommandExecutionError(message)\n\n    if domain == 'NT AUTHORITY':\n        # Logon as a system level account, SYSTEM, LOCAL SERVICE, or NETWORK\n        # SERVICE.\n        logonType = win32con.LOGON32_LOGON_SERVICE\n        user_token = win32security.LogonUser(\n            username,\n            domain,\n            '',\n            win32con.LOGON32_LOGON_SERVICE,\n            win32con.LOGON32_PROVIDER_DEFAULT,\n        )\n    elif password:\n        # Login with a password.\n        user_token = win32security.LogonUser(\n            username,\n            domain,\n            password,\n            win32con.LOGON32_LOGON_INTERACTIVE,\n            win32con.LOGON32_PROVIDER_DEFAULT,\n        )\n    else:\n        # Login without a password. This always returns an elevated token.\n        user_token = salt.platform.win.logon_msv1_s4u(username).Token\n\n    # Get a linked user token to elevate if needed\n    elevation_type = win32security.GetTokenInformation(\n        user_token, win32security.TokenElevationType\n    )\n    if elevation_type > 1:\n        user_token = win32security.GetTokenInformation(\n            user_token,\n            win32security.TokenLinkedToken\n        )\n\n    # Elevate the user token\n    salt.platform.win.elevate_token(user_token)\n\n    # Make sure the user's token has access to a windows station and desktop\n    salt.platform.win.grant_winsta_and_desktop(user_token)\n\n    # Create pipes for standard in, out and error streams\n    security_attributes = win32security.SECURITY_ATTRIBUTES()\n    security_attributes.bInheritHandle = 1\n\n    stdin_read, stdin_write = win32pipe.CreatePipe(security_attributes, 0)\n    stdin_read = salt.platform.win.make_inheritable(stdin_read)\n\n    stdout_read, stdout_write = win32pipe.CreatePipe(security_attributes, 0)\n    stdout_write = salt.platform.win.make_inheritable(stdout_write)\n\n    stderr_read, stderr_write = win32pipe.CreatePipe(security_attributes, 0)\n    stderr_write = salt.platform.win.make_inheritable(stderr_write)\n\n    # Run the process without showing a window.\n    creationflags = (\n        win32process.CREATE_NO_WINDOW |\n        win32process.CREATE_NEW_CONSOLE |\n        win32process.CREATE_SUSPENDED\n    )\n\n    startup_info = salt.platform.win.STARTUPINFO(\n        dwFlags=win32con.STARTF_USESTDHANDLES,\n        hStdInput=stdin_read.handle,\n        hStdOutput=stdout_write.handle,\n        hStdError=stderr_write.handle,\n    )\n\n    # Create the environment for the user\n    env = win32profile.CreateEnvironmentBlock(user_token, False)\n\n    # Start the process in a suspended state.\n    process_info = salt.platform.win.CreateProcessWithTokenW(\n        int(user_token),\n        logonflags=1,\n        applicationname=None,\n        commandline=cmdLine,\n        currentdirectory=cwd,\n        creationflags=creationflags,\n        startupinfo=startup_info,\n        environment=env,\n    )\n\n    hProcess = process_info.hProcess\n    hThread = process_info.hThread\n    dwProcessId = process_info.dwProcessId\n    dwThreadId = process_info.dwThreadId\n\n    salt.platform.win.kernel32.CloseHandle(stdin_write.handle)\n    salt.platform.win.kernel32.CloseHandle(stdout_write.handle)\n    salt.platform.win.kernel32.CloseHandle(stderr_write.handle)\n\n    ret = {'pid': dwProcessId}\n    # Resume the process\n    psutil.Process(dwProcessId).resume()\n\n    # Wait for the process to exit and get it's return code.\n    if win32event.WaitForSingleObject(hProcess, win32event.INFINITE) == win32con.WAIT_OBJECT_0:\n        exitcode = win32process.GetExitCodeProcess(hProcess)\n        ret['retcode'] = exitcode\n\n    # Read standard out\n    fd_out = msvcrt.open_osfhandle(stdout_read.handle, os.O_RDONLY | os.O_TEXT)\n    with os.fdopen(fd_out, 'r') as f_out:\n        stdout = f_out.read()\n        ret['stdout'] = stdout\n\n    # Read standard error\n    fd_err = msvcrt.open_osfhandle(stderr_read.handle, os.O_RDONLY | os.O_TEXT)\n    with os.fdopen(fd_err, 'r') as f_err:\n        stderr = f_err.read()\n        ret['stderr'] = stderr\n\n    salt.platform.win.kernel32.CloseHandle(hProcess)\n    win32api.CloseHandle(user_token)\n    if impersonation_token:\n        win32security.RevertToSelf()\n    win32api.CloseHandle(impersonation_token)\n\n    return ret", "response": "Run a command as another user."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nruns a command on the specified user and returns a dictionary of the process info.", "response": "def runas_unpriv(cmd, username, password, cwd=None):\n    '''\n    Runas that works for non-priviledged users\n    '''\n    # Create a pipe to set as stdout in the child. The write handle needs to be\n    # inheritable.\n    c2pread, c2pwrite = salt.platform.win.CreatePipe(\n        inherit_read=False, inherit_write=True,\n    )\n    errread, errwrite = salt.platform.win.CreatePipe(\n        inherit_read=False, inherit_write=True,\n    )\n\n    # Create inheritable copy of the stdin\n    stdin = salt.platform.win.kernel32.GetStdHandle(\n            salt.platform.win.STD_INPUT_HANDLE,\n    )\n    dupin = salt.platform.win.DuplicateHandle(srchandle=stdin, inherit=True)\n\n    # Get startup info structure\n    startup_info = salt.platform.win.STARTUPINFO(\n        dwFlags=win32con.STARTF_USESTDHANDLES,\n        hStdInput=dupin,\n        hStdOutput=c2pwrite,\n        hStdError=errwrite,\n    )\n\n    username, domain = split_username(username)\n\n    # Run command and return process info structure\n    process_info = salt.platform.win.CreateProcessWithLogonW(\n            username=username,\n            domain=domain,\n            password=password,\n            logonflags=salt.platform.win.LOGON_WITH_PROFILE,\n            commandline=cmd,\n            startupinfo=startup_info,\n            currentdirectory=cwd)\n\n    salt.platform.win.kernel32.CloseHandle(dupin)\n    salt.platform.win.kernel32.CloseHandle(c2pwrite)\n    salt.platform.win.kernel32.CloseHandle(errwrite)\n    salt.platform.win.kernel32.CloseHandle(process_info.hThread)\n\n    # Initialize ret and set first element\n    ret = {'pid': process_info.dwProcessId}\n\n    # Get Standard Out\n    fd_out = msvcrt.open_osfhandle(c2pread, os.O_RDONLY | os.O_TEXT)\n    with os.fdopen(fd_out, 'r') as f_out:\n        ret['stdout'] = f_out.read()\n\n    # Get Standard Error\n    fd_err = msvcrt.open_osfhandle(errread, os.O_RDONLY | os.O_TEXT)\n    with os.fdopen(fd_err, 'r') as f_err:\n        ret['stderr'] = f_err.read()\n\n    # Get Return Code\n    if salt.platform.win.kernel32.WaitForSingleObject(process_info.hProcess, win32event.INFINITE) == \\\n            win32con.WAIT_OBJECT_0:\n        exitcode = salt.platform.win.wintypes.DWORD()\n        salt.platform.win.kernel32.GetExitCodeProcess(process_info.hProcess,\n                                    ctypes.byref(exitcode))\n        ret['retcode'] = exitcode.value\n\n    # Close handle to process\n    salt.platform.win.kernel32.CloseHandle(process_info.hProcess)\n\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef match(tgt, functions=None, opts=None):\n    '''\n    Match based on the local data store on the minion\n    '''\n    if not opts:\n        opts = __opts__\n    if functions is None:\n        utils = salt.loader.utils(opts)\n        functions = salt.loader.minion_mods(opts, utils=utils)\n    comps = tgt.split(':')\n    if len(comps) < 2:\n        return False\n    val = functions['data.getval'](comps[0])\n    if val is None:\n        # The value is not defined\n        return False\n    if isinstance(val, list):\n        # We are matching a single component to a single list member\n        for member in val:\n            if fnmatch.fnmatch(six.text_type(member).lower(), comps[1].lower()):\n                return True\n        return False\n    if isinstance(val, dict):\n        if comps[1] in val:\n            return True\n        return False\n    return bool(fnmatch.fnmatch(\n        val,\n        comps[1],\n    ))", "response": "Match based on the local data store on the minion\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _find_image(name):\n    '''\n    Tries to find image with given name, returns\n        - image, 'Found image <name>'\n        - None, 'No such image found'\n        - False, 'Found more than one image with given name'\n    '''\n    try:\n        images = __salt__['glance.image_list'](name=name)\n    except kstone_Unauthorized:\n        return False, 'keystoneclient: Unauthorized'\n    except glance_Unauthorized:\n        return False, 'glanceclient: Unauthorized'\n    log.debug('Got images: %s', images)\n\n    if type(images) is dict and len(images) == 1 and 'images' in images:\n        images = images['images']\n\n    images_list = images.values() if type(images) is dict else images\n\n    if not images_list:\n        return None, 'No image with name \"{0}\"'.format(name)\n    elif len(images_list) == 1:\n        return images_list[0], 'Found image {0}'.format(name)\n    elif len(images_list) > 1:\n        return False, 'Found more than one image with given name'\n    else:\n        raise NotImplementedError", "response": "Tries to find image with given name returns\n        - image None - no such image found returns\n        - glanceclient - Unauthorized"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nensures that the given image is present with the given properties.", "response": "def image_present(name, visibility='public', protected=None,\n        checksum=None, location=None, disk_format='raw', wait_for=None,\n        timeout=30):\n    '''\n    Checks if given image is present with properties\n    set as specified.\n\n    An image should got through the stages 'queued', 'saving'\n    before becoming 'active'. The attribute 'checksum' can\n    only be checked once the image is active.\n    If you don't specify 'wait_for' but 'checksum' the function\n    will wait for the image to become active before comparing\n    checksums. If you don't specify checksum either the function\n    will return when the image reached 'saving'.\n    The default timeout for both is 30 seconds.\n\n    Supported properties:\n      - visibility ('public' or 'private')\n      - protected (bool)\n      - checksum (string, md5sum)\n      - location (URL, to copy from)\n      - disk_format ('raw' (default), 'vhd', 'vhdx', 'vmdk', 'vdi', 'iso',\n        'qcow2', 'aki', 'ari' or 'ami')\n    '''\n    ret = {'name': name,\n            'changes': {},\n            'result': True,\n            'comment': '',\n            }\n    acceptable = ['queued', 'saving', 'active']\n    if wait_for is None and checksum is None:\n        wait_for = 'saving'\n    elif wait_for is None and checksum is not None:\n        wait_for = 'active'\n\n    # Just pop states until we reach the\n    # first acceptable one:\n    while len(acceptable) > 1:\n        if acceptable[0] == wait_for:\n            break\n        else:\n            acceptable.pop(0)\n\n    image, msg = _find_image(name)\n    if image is False:\n        if __opts__['test']:\n            ret['result'] = None\n        else:\n            ret['result'] = False\n        ret['comment'] = msg\n        return ret\n    log.debug(msg)\n    # No image yet and we know where to get one\n    if image is None and location is not None:\n        if __opts__['test']:\n            ret['result'] = None\n            ret['comment'] = 'glance.image_present would ' \\\n                'create an image from {0}'.format(location)\n            return ret\n        image = __salt__['glance.image_create'](name=name,\n            protected=protected, visibility=visibility,\n            location=location, disk_format=disk_format)\n        log.debug('Created new image:\\n%s', image)\n        ret['changes'] = {\n            name:\n                {\n                    'new':\n                        {\n                        'id': image['id']\n                        },\n                    'old': None\n                }\n            }\n        timer = timeout\n        # Kinda busy-loopy but I don't think the Glance\n        # API has events we can listen for\n        while timer > 0:\n            if 'status' in image and \\\n                    image['status'] in acceptable:\n                log.debug('Image %s has reached status %s',\n                          image['name'], image['status'])\n                break\n            else:\n                timer -= 5\n                time.sleep(5)\n                image, msg = _find_image(name)\n                if not image:\n                    ret['result'] = False\n                    ret['comment'] += 'Created image {0} '.format(\n                        name) + ' vanished:\\n' + msg\n                    return ret\n        if timer <= 0 and image['status'] not in acceptable:\n            ret['result'] = False\n            ret['comment'] += 'Image didn\\'t reach an acceptable '+\\\n                    'state ({0}) before timeout:\\n'.format(acceptable)+\\\n                    '\\tLast status was \"{0}\".\\n'.format(image['status'])\n\n    # There's no image but where would I get one??\n    elif location is None:\n        if __opts__['test']:\n            ret['result'] = None\n            ret['comment'] = 'No location to copy image from specified,\\n' +\\\n                         'glance.image_present would not create one'\n        else:\n            ret['result'] = False\n            ret['comment'] = 'No location to copy image from specified,\\n' +\\\n                         'not creating a new image.'\n        return ret\n\n    # If we've created a new image also return its last status:\n    if name in ret['changes']:\n        ret['changes'][name]['new']['status'] = image['status']\n\n    if visibility:\n        if image['visibility'] != visibility:\n            old_value = image['visibility']\n            if not __opts__['test']:\n                image = __salt__['glance.image_update'](\n                    id=image['id'], visibility=visibility)\n            # Check if image_update() worked:\n            if image['visibility'] != visibility:\n                if not __opts__['test']:\n                    ret['result'] = False\n                elif __opts__['test']:\n                    ret['result'] = None\n                ret['comment'] += '\"visibility\" is {0}, '\\\n                    'should be {1}.\\n'.format(image['visibility'],\n                        visibility)\n            else:\n                if 'new' in ret['changes']:\n                    ret['changes']['new']['visibility'] = visibility\n                else:\n                    ret['changes']['new'] = {'visibility': visibility}\n                if 'old' in ret['changes']:\n                    ret['changes']['old']['visibility'] = old_value\n                else:\n                    ret['changes']['old'] = {'visibility': old_value}\n        else:\n            ret['comment'] += '\"visibility\" is correct ({0}).\\n'.format(\n                visibility)\n    if protected is not None:\n        if not isinstance(protected, bool) or image['protected'] ^ protected:\n            if not __opts__['test']:\n                ret['result'] = False\n            else:\n                ret['result'] = None\n            ret['comment'] += '\"protected\" is {0}, should be {1}.\\n'.format(\n                image['protected'], protected)\n        else:\n            ret['comment'] += '\"protected\" is correct ({0}).\\n'.format(\n                protected)\n    if 'status' in image and checksum:\n        if image['status'] == 'active':\n            if 'checksum' not in image:\n                # Refresh our info about the image\n                image = __salt__['glance.image_show'](image['id'])\n            if 'checksum' not in image:\n                if not __opts__['test']:\n                    ret['result'] = False\n                else:\n                    ret['result'] = None\n                ret['comment'] += 'No checksum available for this image:\\n' +\\\n                        '\\tImage has status \"{0}\".'.format(image['status'])\n            elif image['checksum'] != checksum:\n                if not __opts__['test']:\n                    ret['result'] = False\n                else:\n                    ret['result'] = None\n                ret['comment'] += '\"checksum\" is {0}, should be {1}.\\n'.format(\n                    image['checksum'], checksum)\n            else:\n                ret['comment'] += '\"checksum\" is correct ({0}).\\n'.format(\n                    checksum)\n        elif image['status'] in ['saving', 'queued']:\n            ret['comment'] += 'Checksum won\\'t be verified as image ' +\\\n                'hasn\\'t reached\\n\\t \"status=active\" yet.\\n'\n    log.debug('glance.image_present will return: %s', ret)\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning data to one of potentially many clustered cassandra nodes", "response": "def returner(ret):\n    '''\n    Return data to one of potentially many clustered cassandra nodes\n    '''\n    query = '''INSERT INTO {keyspace}.salt_returns (\n                 jid, minion_id, fun, alter_time, full_ret, return, success\n               ) VALUES (?, ?, ?, ?, ?, ?, ?)'''.format(keyspace=_get_keyspace())\n\n    statement_arguments = ['{0}'.format(ret['jid']),\n                           '{0}'.format(ret['id']),\n                           '{0}'.format(ret['fun']),\n                           int(time.time() * 1000),\n                           salt.utils.json.dumps(ret).replace(\"'\", \"''\"),\n                           salt.utils.json.dumps(ret['return']).replace(\"'\", \"''\"),\n                           ret.get('success', False)]\n\n    # cassandra_cql.cql_query may raise a CommandExecutionError\n    try:\n        __salt__['cassandra_cql.cql_query_with_prepare'](query,\n                                                         'returner_return',\n                                                         tuple(statement_arguments),\n                                                         asynchronous=True)\n    except CommandExecutionError:\n        log.critical('Could not insert into salt_returns with Cassandra returner.')\n        raise\n    except Exception as e:\n        log.critical('Unexpected error while inserting into salt_returns: %s', e)\n        raise\n\n    # Store the last function called by the minion\n    # The data in salt.minions will be used by get_fun and get_minions\n    query = '''INSERT INTO {keyspace}.minions (\n                 minion_id, last_fun\n               ) VALUES (?, ?)'''.format(keyspace=_get_keyspace())\n\n    statement_arguments = ['{0}'.format(ret['id']), '{0}'.format(ret['fun'])]\n\n    # cassandra_cql.cql_query may raise a CommandExecutionError\n    try:\n        __salt__['cassandra_cql.cql_query_with_prepare'](query,\n                                                         'returner_minion',\n                                                         tuple(statement_arguments),\n                                                         asynchronous=True)\n    except CommandExecutionError:\n        log.critical('Could not store minion ID with Cassandra returner.')\n        raise\n    except Exception as e:\n        log.critical(\n            'Unexpected error while inserting minion ID into the minions '\n            'table: %s', e\n        )\n        raise"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef event_return(events):\n    '''\n    Return event to one of potentially many clustered cassandra nodes\n\n    Requires that configuration be enabled via 'event_return'\n    option in master config.\n\n    Cassandra does not support an auto-increment feature due to the\n    highly inefficient nature of creating a monotonically increasing\n    number across all nodes in a distributed database. Each event\n    will be assigned a uuid by the connecting client.\n    '''\n    for event in events:\n        tag = event.get('tag', '')\n        data = event.get('data', '')\n        query = '''INSERT INTO {keyspace}.salt_events (\n                     id, alter_time, data, master_id, tag\n                   ) VALUES (\n                     ?, ?, ?, ?, ?)\n                 '''.format(keyspace=_get_keyspace())\n        statement_arguments = [six.text_type(uuid.uuid1()),\n                               int(time.time() * 1000),\n                               salt.utils.json.dumps(data).replace(\"'\", \"''\"),\n                               __opts__['id'],\n                               tag]\n\n        # cassandra_cql.cql_query may raise a CommandExecutionError\n        try:\n            __salt__['cassandra_cql.cql_query_with_prepare'](query, 'salt_events',\n                                                             statement_arguments,\n                                                             asynchronous=True)\n        except CommandExecutionError:\n            log.critical('Could not store events with Cassandra returner.')\n            raise\n        except Exception as e:\n            log.critical(\n                'Unexpected error while inserting into salt_events: %s', e)\n            raise", "response": "Return events to one of potentially many clustered cassandra nodes"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsave the load to the specified jid id", "response": "def save_load(jid, load, minions=None):\n    '''\n    Save the load to the specified jid id\n    '''\n    # Load is being stored as a text datatype. Single quotes are used in the\n    # VALUES list. Therefore, all single quotes contained in the results from\n    # salt.utils.json.dumps(load) must be escaped Cassandra style.\n    query = '''INSERT INTO {keyspace}.jids (\n                 jid, load\n               ) VALUES (?, ?)'''.format(keyspace=_get_keyspace())\n\n    statement_arguments = [\n        jid,\n        salt.utils.json.dumps(load).replace(\"'\", \"''\")\n    ]\n\n    # cassandra_cql.cql_query may raise a CommandExecutionError\n    try:\n        __salt__['cassandra_cql.cql_query_with_prepare'](query, 'save_load',\n                                                         statement_arguments,\n                                                         asynchronous=True)\n    except CommandExecutionError:\n        log.critical('Could not save load in jids table.')\n        raise\n    except Exception as e:\n        log.critical('Unexpected error while inserting into jids: %s', e)\n        raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the information returned when the specified job id was executed", "response": "def get_jid(jid):\n    '''\n    Return the information returned when the specified job id was executed\n    '''\n    query = '''SELECT minion_id, full_ret FROM {keyspace}.salt_returns\n               WHERE jid = ?;'''.format(keyspace=_get_keyspace())\n\n    ret = {}\n\n    # cassandra_cql.cql_query may raise a CommandExecutionError\n    try:\n        data = __salt__['cassandra_cql.cql_query_with_prepare'](query, 'get_jid', [jid])\n        if data:\n            for row in data:\n                minion = row.get('minion_id')\n                full_ret = row.get('full_ret')\n                if minion and full_ret:\n                    ret[minion] = salt.utils.json.loads(full_ret)\n    except CommandExecutionError:\n        log.critical('Could not select job specific information.')\n        raise\n    except Exception as e:\n        log.critical(\n            'Unexpected error while getting job specific information: %s', e)\n        raise\n\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a dict of the last function called for all minions", "response": "def get_fun(fun):\n    '''\n    Return a dict of the last function called for all minions\n    '''\n    query = '''SELECT minion_id, last_fun FROM {keyspace}.minions\n               WHERE last_fun = ?;'''.format(keyspace=_get_keyspace())\n\n    ret = {}\n\n    # cassandra_cql.cql_query may raise a CommandExecutionError\n    try:\n        data = __salt__['cassandra_cql.cql_query'](query, 'get_fun', [fun])\n        if data:\n            for row in data:\n                minion = row.get('minion_id')\n                last_fun = row.get('last_fun')\n                if minion and last_fun:\n                    ret[minion] = last_fun\n    except CommandExecutionError:\n        log.critical('Could not get the list of minions.')\n        raise\n    except Exception as e:\n        log.critical(\n            'Unexpected error while getting list of minions: %s', e)\n        raise\n\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_jids():\n    '''\n    Return a list of all job ids\n    '''\n    query = '''SELECT jid, load FROM {keyspace}.jids;'''.format(keyspace=_get_keyspace())\n\n    ret = {}\n\n    # cassandra_cql.cql_query may raise a CommandExecutionError\n    try:\n        data = __salt__['cassandra_cql.cql_query'](query)\n        if data:\n            for row in data:\n                jid = row.get('jid')\n                load = row.get('load')\n                if jid and load:\n                    ret[jid] = salt.utils.jid.format_jid_instance(\n                        jid,\n                        salt.utils.json.loads(load))\n    except CommandExecutionError:\n        log.critical('Could not get a list of all job ids.')\n        raise\n    except Exception as e:\n        log.critical(\n            'Unexpected error while getting list of all job ids: %s', e)\n        raise\n\n    return ret", "response": "Return a list of all job ids"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_minions():\n    '''\n    Return a list of minions\n    '''\n    query = '''SELECT DISTINCT minion_id\n               FROM {keyspace}.minions;'''.format(keyspace=_get_keyspace())\n\n    ret = []\n\n    # cassandra_cql.cql_query may raise a CommandExecutionError\n    try:\n        data = __salt__['cassandra_cql.cql_query'](query)\n        if data:\n            for row in data:\n                minion = row.get('minion_id')\n                if minion:\n                    ret.append(minion)\n    except CommandExecutionError:\n        log.critical('Could not get the list of minions.')\n        raise\n    except Exception as e:\n        log.critical(\n            'Unexpected error while getting list of minions: %s', e)\n        raise\n\n    return ret", "response": "Return a list of minions in the keyspace"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprint the output data in JSON", "response": "def output(data, **kwargs):  # pylint: disable=unused-argument\n    '''\n    Print the output data in JSON\n    '''\n    try:\n        if 'output_indent' not in __opts__:\n            return salt.utils.json.dumps(data, default=repr, indent=4)\n\n        indent = __opts__.get('output_indent')\n        sort_keys = False\n\n        if indent is None:\n            indent = None\n\n        elif indent == 'pretty':\n            indent = 4\n            sort_keys = True\n\n        elif isinstance(indent, int):\n            if indent >= 0:\n                indent = indent\n            else:\n                indent = None\n\n        return salt.utils.json.dumps(data, default=repr, indent=indent, sort_keys=sort_keys)\n\n    except UnicodeDecodeError as exc:\n        log.error('Unable to serialize output to json')\n        return salt.utils.json.dumps(\n            {'error': 'Unable to serialize output to json',\n             'message': six.text_type(exc)}\n        )\n\n    except TypeError:\n        log.debug('An error occurred while outputting JSON', exc_info=True)\n    # Return valid JSON for unserializable objects\n    return salt.utils.json.dumps({})"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nensure that the named database is present with the specified options", "response": "def present(name, owner=None, grants=None, **kwargs):\n    '''\n    Ensure that the named database is present with the specified options\n\n    name\n        The name of the database to manage\n    owner\n        Adds owner using AUTHORIZATION option\n    Grants\n        Can only be a list of strings\n    '''\n    ret = {'name': name,\n           'changes': {},\n           'result': True,\n           'comment': ''}\n\n    if __salt__['mssql.role_exists'](name, **kwargs):\n        ret['comment'] = 'Role {0} is already present (Not going to try to set its grants)'.format(name)\n        return ret\n    if __opts__['test']:\n        ret['result'] = None\n        ret['comment'] = 'Role {0} is set to be added'.format(name)\n        return ret\n\n    role_created = __salt__['mssql.role_create'](name, owner=owner, grants=grants, **kwargs)\n    if role_created is not True:  # Non-empty strings are also evaluated to True, so we cannot use if not role_created:\n        ret['result'] = False\n        ret['comment'] += 'Role {0} failed to be created: {1}'.format(name, role_created)\n        return ret\n    ret['comment'] += 'Role {0} has been added'.format(name)\n    ret['changes'][name] = 'Present'\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_change_request_state(change_id, state='approved'):\n    '''\n    Set the approval state of a change request/record\n\n    :param change_id: The ID of the change request, e.g. CHG123545\n    :type  change_id: ``str``\n\n    :param state: The target state, e.g. approved\n    :type  state: ``str``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion servicenow.set_change_request_state CHG000123 declined\n        salt myminion servicenow.set_change_request_state CHG000123 approved\n    '''\n    client = _get_client()\n    client.table = 'change_request'\n    # Get the change record first\n    record = client.get({'number': change_id})\n    if not record:\n        log.error('Failed to fetch change record, maybe it does not exist?')\n        return False\n    # Use the sys_id as the unique system record\n    sys_id = record[0]['sys_id']\n    response = client.update({'approval': state}, sys_id)\n    return response", "response": "Set the approval state of a change request"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef delete_record(table, sys_id):\n    '''\n    Delete an existing record\n\n    :param table: The table name, e.g. sys_user\n    :type  table: ``str``\n\n    :param sys_id: The unique ID of the record\n    :type  sys_id: ``str``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion servicenow.delete_record sys_computer 2134566\n    '''\n    client = _get_client()\n    client.table = table\n    response = client.delete(sys_id)\n    return response", "response": "Delete an existing record in a table"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef non_structured_query(table, query=None, **kwargs):\n    '''\n    Run a non-structed (not a dict) query on a servicenow table.\n    See http://wiki.servicenow.com/index.php?title=Encoded_Query_Strings#gsc.tab=0\n    for help on constructing a non-structured query string.\n\n    :param table: The table name, e.g. sys_user\n    :type  table: ``str``\n\n    :param query: The query to run (or use keyword arguments to filter data)\n    :type  query: ``str``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion servicenow.non_structured_query sys_computer 'role=web'\n        salt myminion servicenow.non_structured_query sys_computer role=web type=computer\n    '''\n    client = _get_client()\n    client.table = table\n    # underlying lib doesn't use six or past.basestring,\n    # does isinstance(x, str)\n    # http://bit.ly/1VkMmpE\n    if query is None:\n        # try and assemble a query by keyword\n        query_parts = []\n        for key, value in kwargs.items():\n            query_parts.append('{0}={1}'.format(key, value))\n        query = '^'.join(query_parts)\n    query = six.text_type(query)\n    response = client.get(query)\n    return response", "response": "Run a non - structured query on a servicenow table."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update_record_field(table, sys_id, field, value):\n    '''\n    Update the value of a record's field in a servicenow table\n\n    :param table: The table name, e.g. sys_user\n    :type  table: ``str``\n\n    :param sys_id: The unique ID of the record\n    :type  sys_id: ``str``\n\n    :param field: The new value\n    :type  field: ``str``\n\n    :param value: The new value\n    :type  value: ``str``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion servicenow.update_record_field sys_user 2348234 first_name jimmy\n    '''\n    client = _get_client()\n    client.table = table\n    response = client.update({field: value}, sys_id)\n    return response", "response": "Update the value of a record s field in a servicenow table"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _parse_pkg_string(pkg):\n    '''\n    Parse pkg string and return a tuple of package name, separator, and\n    package version.\n\n    Cabal support install package with following format:\n\n    * foo-1.0\n    * foo < 1.2\n    * foo > 1.3\n\n    For the sake of simplicity only the first form is supported,\n    support for other forms can be added later.\n    '''\n    pkg_name, separator, pkg_ver = pkg.partition('-')\n    return (pkg_name.strip(), separator, pkg_ver.strip())", "response": "Parse pkg string and return a tuple of package name separator and and\n    package version."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef installed(name,\n              pkgs=None,\n              user=None,\n              install_global=False,\n              env=None):\n    '''\n    Verify that the given package is installed and is at the correct version\n    (if specified).\n\n    .. code-block:: yaml\n\n        ShellCheck-0.3.5:\n          cabal:\n            - installed:\n\n    name\n        The package to install\n    user\n        The user to run cabal install with\n    install_global\n        Install package globally instead of locally\n    env\n        A list of environment variables to be set prior to execution. The\n        format is the same as the :py:func:`cmd.run <salt.states.cmd.run>`.\n        state function.\n    '''\n    ret = {'name': name, 'result': None, 'comment': '', 'changes': {}}\n\n    try:\n        call = __salt__['cabal.update'](user=user, env=env)\n    except (CommandNotFoundError, CommandExecutionError) as err:\n        ret['result'] = False\n        ret['comment'] = 'Could not run cabal update {0}'.format(err)\n        return ret\n\n    if pkgs is not None:\n        pkg_list = pkgs\n    else:\n        pkg_list = [name]\n\n    try:\n        installed_pkgs = __salt__['cabal.list'](\n            user=user, installed=True, env=env)\n    except (CommandNotFoundError, CommandExecutionError) as err:\n        ret['result'] = False\n        ret['comment'] = 'Error looking up \\'{0}\\': {1}'.format(name, err)\n        return ret\n\n    pkgs_satisfied = []\n    pkgs_to_install = []\n\n    for pkg in pkg_list:\n        pkg_name, _, pkg_ver = _parse_pkg_string(pkg)\n\n        if pkg_name not in installed_pkgs:\n            pkgs_to_install.append(pkg)\n        else:\n            if pkg_ver:  # version is specified\n                if installed_pkgs[pkg_name] != pkg_ver:\n                    pkgs_to_install.append(pkg)\n                else:\n                    pkgs_satisfied.append(pkg)\n            else:\n                pkgs_satisfied.append(pkg)\n\n    if __opts__['test']:\n        ret['result'] = None\n\n        comment_msg = []\n\n        if pkgs_to_install:\n            comment_msg.append(\n                'Packages(s) \\'{0}\\' are set to be installed'.format(\n                    ', '.join(pkgs_to_install)))\n\n        if pkgs_satisfied:\n            comment_msg.append(\n                'Packages(s) \\'{0}\\' satisfied by {1}'.format(\n                    ', '.join(pkg_list), ', '.join(pkgs_satisfied)))\n\n        ret['comment'] = '. '.join(comment_msg)\n        return ret\n\n    if not pkgs_to_install:\n        ret['result'] = True\n        ret['comment'] = ('Packages(s) \\'{0}\\' satisfied by {1}'.format(\n            ', '.join(pkg_list), ', '.join(pkgs_satisfied)))\n\n        return ret\n\n    try:\n        call = __salt__['cabal.install'](pkgs=pkg_list,\n                                         user=user,\n                                         install_global=install_global,\n                                         env=env)\n    except (CommandNotFoundError, CommandExecutionError) as err:\n        ret['result'] = False\n        ret['comment'] = 'Error installing \\'{0}\\': {1}'.format(\n            ', '.join(pkg_list), err)\n        return ret\n\n    if call and isinstance(call, dict):\n        ret['result'] = True\n        ret['changes'] = {'old': [], 'new': pkgs_to_install}\n        ret['comment'] = 'Packages(s) \\'{0}\\' successfully installed'.format(\n            ', '.join(pkgs_to_install))\n    else:\n        ret['result'] = False\n        ret['comment'] = 'Could not install packages(s) \\'{0}\\''.format(\n            ', '.join(pkg_list))\n\n    return ret", "response": "Verify that the given package is installed at the correct version."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nverify that given package is not installed.", "response": "def removed(name,\n            user=None,\n            env=None):\n    '''\n    Verify that given package is not installed.\n    '''\n\n    ret = {'name': name, 'result': None, 'comment': '', 'changes': {}}\n\n    try:\n        installed_pkgs = __salt__['cabal.list'](\n            user=user, installed=True, env=env)\n    except (CommandNotFoundError, CommandExecutionError) as err:\n        ret['result'] = False\n        ret['comment'] = 'Error looking up \\'{0}\\': {1}'.format(name, err)\n\n    if name not in installed_pkgs:\n        ret['result'] = True\n        ret['comment'] = 'Package \\'{0}\\' is not installed'.format(name)\n        return ret\n\n    if __opts__['test']:\n        ret['result'] = None\n        ret['comment'] = 'Package \\'{0}\\' is set to be removed'.format(name)\n        return ret\n\n    if __salt__['cabal.uninstall'](pkg=name, user=user, env=env):\n        ret['result'] = True\n        ret['changes'][name] = 'Removed'\n        ret['comment'] = 'Package \\'{0}\\' was successfully removed'.format(name)\n    else:\n        ret['result'] = False\n        ret['comment'] = 'Error removing package \\'{0}\\''.format(name)\n\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nscans for the configured services and fire events at each beacon interval.", "response": "def beacon(config):\n    '''\n    Scan for the configured services and fire events\n\n    Example Config\n\n    .. code-block:: yaml\n\n        beacons:\n          service:\n            - services:\n                salt-master: {}\n                mysql: {}\n\n    The config above sets up beacons to check for\n    the salt-master and mysql services.\n\n    The config also supports two other parameters for each service:\n\n    `onchangeonly`: when `onchangeonly` is True the beacon will fire\n    events only when the service status changes.  Otherwise, it will fire an\n    event at each beacon interval.  The default is False.\n\n    `delay`: when `delay` is greater than 0 the beacon will fire events only\n    after the service status changes, and the delay (in seconds) has passed.\n    Applicable only when `onchangeonly` is True.  The default is 0.\n\n    `emitatstartup`: when `emitatstartup` is False the beacon will not fire\n    event when the minion is reload. Applicable only when `onchangeonly` is True.\n    The default is True.\n\n    `uncleanshutdown`: If `uncleanshutdown` is present it should point to the\n    location of a pid file for the service.  Most services will not clean up\n    this pid file if they are shutdown uncleanly (e.g. via `kill -9`) or if they\n    are terminated through a crash such as a segmentation fault.  If the file is\n    present, then the beacon will add `uncleanshutdown: True` to the event.  If\n    not present, the field will be False.  The field is only added when the\n    service is NOT running. Omitting the configuration variable altogether will\n    turn this feature off.\n\n    Please note that some init systems can remove the pid file if the service\n    registers as crashed. One such example is nginx on CentOS 7, where the\n    service unit removes the pid file when the service shuts down (IE: the pid\n    file is observed as removed when kill -9 is sent to the nginx master\n    process). The 'uncleanshutdown' option might not be of much use there,\n    unless the unit file is modified.\n\n    Here is an example that will fire an event 30 seconds after the state of nginx\n    changes and report an uncleanshutdown.  This example is for Arch, which\n    places nginx's pid file in `/run`.\n\n    .. code-block:: yaml\n\n        beacons:\n          service:\n            - services:\n                nginx:\n                  onchangeonly: True\n                  delay: 30\n                  uncleanshutdown: /run/nginx.pid\n    '''\n    ret = []\n    _config = {}\n    list(map(_config.update, config))\n\n    for service in _config.get('services', {}):\n        ret_dict = {}\n\n        service_config = _config['services'][service]\n\n        ret_dict[service] = {'running': __salt__['service.status'](service)}\n        ret_dict['service_name'] = service\n        ret_dict['tag'] = service\n        currtime = time.time()\n\n        # If no options is given to the service, we fall back to the defaults\n        # assign a False value to oncleanshutdown and onchangeonly. Those\n        # key:values are then added to the service dictionary.\n        if not service_config:\n            service_config = {}\n        if 'oncleanshutdown' not in service_config:\n            service_config['oncleanshutdown'] = False\n        if 'emitatstartup' not in service_config:\n            service_config['emitatstartup'] = True\n        if 'onchangeonly' not in service_config:\n            service_config['onchangeonly'] = False\n        if 'delay' not in service_config:\n            service_config['delay'] = 0\n\n        # We only want to report the nature of the shutdown\n        # if the current running status is False\n        # as well as if the config for the beacon asks for it\n        if 'uncleanshutdown' in service_config and not ret_dict[service]['running']:\n            filename = service_config['uncleanshutdown']\n            ret_dict[service]['uncleanshutdown'] = True if os.path.exists(filename) else False\n        if 'onchangeonly' in service_config and service_config['onchangeonly'] is True:\n            if service not in LAST_STATUS:\n                LAST_STATUS[service] = ret_dict[service]\n                if service_config['delay'] > 0:\n                    LAST_STATUS[service]['time'] = currtime\n                elif not service_config['emitatstartup']:\n                    continue\n                else:\n                    ret.append(ret_dict)\n\n            if LAST_STATUS[service]['running'] != ret_dict[service]['running']:\n                LAST_STATUS[service] = ret_dict[service]\n                if service_config['delay'] > 0:\n                    LAST_STATUS[service]['time'] = currtime\n                else:\n                    ret.append(ret_dict)\n\n            if 'time' in LAST_STATUS[service]:\n                elapsedtime = int(round(currtime - LAST_STATUS[service]['time']))\n                if elapsedtime > service_config['delay']:\n                    del LAST_STATUS[service]['time']\n                    ret.append(ret_dict)\n        else:\n            ret.append(ret_dict)\n\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_credentials(server=None,\n                     username=None,\n                     password=None):\n    '''\n    Returns the credentials merged with the config data (opts + pillar).\n    '''\n    jira_cfg = __salt__['config.merge']('jira', default={})\n    if not server:\n        server = jira_cfg.get('server')\n    if not username:\n        username = jira_cfg.get('username')\n    if not password:\n        password = jira_cfg.get('password')\n    return server, username, password", "response": "Returns the server username and password for the JIRA server"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a JIRA issue using the named settings.", "response": "def create_issue(project,\n                 summary,\n                 description,\n                 template_engine='jinja',\n                 context=None,\n                 defaults=None,\n                 saltenv='base',\n                 issuetype='Bug',\n                 priority='Normal',\n                 labels=None,\n                 assignee=None,\n                 server=None,\n                 username=None,\n                 password=None,\n                 **kwargs):\n    '''\n    Create a JIRA issue using the named settings. Return the JIRA ticket ID.\n\n    project\n        The name of the project to attach the JIRA ticket to.\n\n    summary\n        The summary (title) of the JIRA ticket. When the ``template_engine``\n        argument is set to a proper value of an existing Salt template engine\n        (e.g., ``jinja``, ``mako``, etc.) it will render the ``summary`` before\n        creating the ticket.\n\n    description\n        The full body description of the JIRA ticket. When the ``template_engine``\n        argument is set to a proper value of an existing Salt template engine\n        (e.g., ``jinja``, ``mako``, etc.) it will render the ``description`` before\n        creating the ticket.\n\n    template_engine: ``jinja``\n        The name of the template engine to be used to render the values of the\n        ``summary`` and ``description`` arguments. Default: ``jinja``.\n\n    context: ``None``\n        The context to pass when rendering the ``summary`` and ``description``.\n        This argument is ignored when ``template_engine`` is set as ``None``\n\n    defaults: ``None``\n        Default values to pass to the Salt rendering pipeline for the\n        ``summary`` and ``description`` arguments.\n        This argument is ignored when ``template_engine`` is set as ``None``.\n\n    saltenv: ``base``\n        The Salt environment name (for the rendering system).\n\n    issuetype: ``Bug``\n        The type of the JIRA ticket. Default: ``Bug``.\n\n    priority: ``Normal``\n        The priority of the JIRA ticket. Default: ``Normal``.\n\n    labels: ``None``\n        A list of labels to add to the ticket.\n\n    assignee: ``None``\n        The name of the person to assign the ticket to.\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' jira.create_issue NET 'Ticket title' 'Ticket description'\n        salt '*' jira.create_issue NET 'Issue on {{ opts.id }}' 'Error detected on {{ opts.id }}' template_engine=jinja\n    '''\n    if template_engine:\n        summary = __salt__['file.apply_template_on_contents'](summary,\n                                                              template=template_engine,\n                                                              context=context,\n                                                              defaults=defaults,\n                                                              saltenv=saltenv)\n        description = __salt__['file.apply_template_on_contents'](description,\n                                                                  template=template_engine,\n                                                                  context=context,\n                                                                  defaults=defaults,\n                                                                  saltenv=saltenv)\n    jira_ = _get_jira(server=server,\n                      username=username,\n                      password=password)\n    if not labels:\n        labels = []\n    data = {\n        'project': {\n            'key': project\n        },\n        'summary': summary,\n        'description': description,\n        'issuetype': {\n            'name': issuetype\n        },\n        'priority': {\n            'name': priority\n        },\n        'labels': labels\n    }\n    data.update(clean_kwargs(**kwargs))\n    issue = jira_.create_issue(data)\n    issue_key = str(issue)\n    if assignee:\n        assign_issue(issue_key, assignee)\n    return issue_key"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nassigns the issue to an existing user. Return True when the issue has been properly assigned to an existing user. Return False when the issue has been successfully assigned to an existing user.", "response": "def assign_issue(issue_key,\n                 assignee,\n                 server=None,\n                 username=None,\n                 password=None):\n    '''\n    Assign the issue to an existing user. Return ``True`` when the issue has\n    been properly assigned.\n\n    issue_key\n        The JIRA ID of the ticket to manipulate.\n\n    assignee\n        The name of the user to assign the ticket to.\n\n    CLI Example:\n\n        salt '*' jira.assign_issue NET-123 example_user\n    '''\n    jira_ = _get_jira(server=server,\n                      username=username,\n                      password=password)\n    assigned = jira_.assign_issue(issue_key, assignee)\n    return assigned"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_comment(issue_key,\n                comment,\n                visibility=None,\n                is_internal=False,\n                server=None,\n                username=None,\n                password=None):\n    '''\n    Add a comment to an existing ticket. Return ``True`` when it successfully\n    added the comment.\n\n    issue_key\n        The issue ID to add the comment to.\n\n    comment\n        The body of the comment to be added.\n\n    visibility: ``None``\n        A dictionary having two keys:\n\n        - ``type``: is ``role`` (or ``group`` if the JIRA server has configured\n          comment visibility for groups).\n        - ``value``: the name of the role (or group) to which viewing of this\n          comment will be restricted.\n\n    is_internal: ``False``\n        Whether a comment has to be marked as ``Internal`` in Jira Service Desk.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' jira.add_comment NE-123 'This is a comment'\n    '''\n    jira_ = _get_jira(server=server,\n                      username=username,\n                      password=password)\n    comm = jira_.add_comment(issue_key,\n                             comment,\n                             visibility=visibility,\n                             is_internal=is_internal)\n    return True", "response": "Adds a comment to an existing ticket."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if the issue is closed.", "response": "def issue_closed(issue_key,\n                 server=None,\n                 username=None,\n                 password=None):\n    '''\n    Check if the issue is closed.\n\n    issue_key\n        The JIRA iD of the ticket to close.\n\n    Returns:\n\n    - ``True``: the ticket exists and it is closed.\n    - ``False``: the ticket exists and it has not been closed.\n    - ``None``: the ticket does not exist.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' jira.issue_closed NE-123\n    '''\n    if not issue_key:\n        return None\n    jira_ = _get_jira(server=server,\n                      username=username,\n                      password=password)\n    try:\n        ticket = jira_.issue(issue_key)\n    except jira.exceptions.JIRAError:\n        # Ticket not found\n        return None\n    return ticket.fields().status.name == 'Closed'"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove complex types from Linode - python dictionary.", "response": "def _remove_complex_types(dictionary):\n    '''\n    Linode-python is now returning some complex types that\n    are not serializable by msgpack.  Kill those.\n    '''\n    for k, v in six.iteritems(dictionary):\n        if isinstance(v, dict):\n            dictionary[k] = _remove_complex_types(v)\n        elif hasattr(v, 'to_eng_string'):\n            dictionary[k] = v.to_eng_string()\n\n    return dictionary"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run(**kwargs):\n    '''\n    Run a single module function or a range of module functions in a batch.\n    Supersedes ``module.run`` function, which requires ``m_`` prefix to\n    function-specific parameters.\n\n    :param returner:\n        Specify a common returner for the whole batch to send the return data\n\n    :param kwargs:\n        Pass any arguments needed to execute the function(s)\n\n    .. code-block:: yaml\n\n      some_id_of_state:\n        module.run:\n          - network.ip_addrs:\n            - interface: eth0\n          - cloud.create:\n            - names:\n              - test-isbm-1\n              - test-isbm-2\n            - ssh_username: sles\n            - image: sles12sp2\n            - securitygroup: default\n            - size: 'c3.large'\n            - location: ap-northeast-1\n            - delvol_on_destroy: True\n\n\n    :return:\n    '''\n\n    if 'name' in kwargs:\n        kwargs.pop('name')\n    ret = {\n        'name': list(kwargs),\n        'changes': {},\n        'comment': '',\n        'result': None,\n    }\n\n    functions = [func for func in kwargs if '.' in func]\n    missing = []\n    tests = []\n    for func in functions:\n        func = func.split(':')[0]\n        if func not in __salt__:\n            missing.append(func)\n        elif __opts__['test']:\n            tests.append(func)\n\n    if tests or missing:\n        ret['comment'] = ' '.join([\n            missing and \"Unavailable function{plr}: \"\n                        \"{func}.\".format(plr=(len(missing) > 1 or ''),\n                                         func=(', '.join(missing) or '')) or '',\n            tests and \"Function{plr} {func} to be \"\n                      \"executed.\".format(plr=(len(tests) > 1 or ''),\n                                         func=(', '.join(tests)) or '') or '',\n        ]).strip()\n        ret['result'] = not (missing or not tests)\n\n    if ret['result'] is None:\n        ret['result'] = True\n\n        failures = []\n        success = []\n        for func in functions:\n            _func = func.split(':')[0]\n            try:\n                func_ret = _call_function(_func, returner=kwargs.get('returner'),\n                                          func_args=kwargs.get(func))\n                if not _get_result(func_ret, ret['changes'].get('ret', {})):\n                    if isinstance(func_ret, dict):\n                        failures.append(\"'{0}' failed: {1}\".format(\n                            func, func_ret.get('comment', '(error message N/A)')))\n                else:\n                    success.append('{0}: {1}'.format(\n                        func, func_ret.get('comment', 'Success') if isinstance(func_ret, dict) else func_ret))\n                    ret['changes'][func] = func_ret\n            except (SaltInvocationError, TypeError) as ex:\n                failures.append(\"'{0}' failed: {1}\".format(func, ex))\n        ret['comment'] = ', '.join(failures + success)\n        ret['result'] = not bool(failures)\n\n    return ret", "response": "Execute a single module function or a range of module functions in a batch."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncall a function from the specified module.", "response": "def _call_function(name, returner=None, **kwargs):\n    '''\n    Calls a function from the specified module.\n\n    :param name:\n    :param kwargs:\n    :return:\n    '''\n    argspec = salt.utils.args.get_function_argspec(__salt__[name])\n\n    # func_kw is initialized to a dictionary of keyword arguments the function to be run accepts\n    func_kw = dict(zip(argspec.args[-len(argspec.defaults or []):],  # pylint: disable=incompatible-py3-code\n                   argspec.defaults or []))\n\n    # func_args is initialized to a list of positional arguments that the function to be run accepts\n    func_args = argspec.args[:len(argspec.args or []) - len(argspec.defaults or [])]\n    arg_type, kw_to_arg_type, na_type, kw_type = [], {}, {}, False\n    for funcset in reversed(kwargs.get('func_args') or []):\n        if not isinstance(funcset, dict):\n            # We are just receiving a list of args to the function to be run, so just append\n            # those to the arg list that we will pass to the func.\n            arg_type.append(funcset)\n        else:\n            for kwarg_key in six.iterkeys(funcset):\n                # We are going to pass in a keyword argument. The trick here is to make certain\n                # that if we find that in the *args* list that we pass it there and not as a kwarg\n                if kwarg_key in func_args:\n                    kw_to_arg_type[kwarg_key] = funcset[kwarg_key]\n                    continue\n                else:\n                    # Otherwise, we're good and just go ahead and pass the keyword/value pair into\n                    # the kwargs list to be run.\n                    func_kw.update(funcset)\n    arg_type.reverse()\n    for arg in func_args:\n        if arg in kw_to_arg_type:\n            arg_type.append(kw_to_arg_type[arg])\n    _exp_prm = len(argspec.args or []) - len(argspec.defaults or [])\n    _passed_prm = len(arg_type)\n    missing = []\n    if na_type and _exp_prm > _passed_prm:\n        for arg in argspec.args:\n            if arg not in func_kw:\n                missing.append(arg)\n    if missing:\n        raise SaltInvocationError('Missing arguments: {0}'.format(', '.join(missing)))\n    elif _exp_prm > _passed_prm:\n        raise SaltInvocationError('Function expects {0} parameters, got only {1}'.format(\n            _exp_prm, _passed_prm))\n\n    mret = __salt__[name](*arg_type, **func_kw)\n    if returner is not None:\n        returners = salt.loader.returners(__opts__, __salt__)\n        if returner in returners:\n            returners[returner]({'id': __opts__['id'], 'ret': mret,\n                                 'fun': name, 'jid': salt.utils.jid.gen_jid(__opts__)})\n\n    return mret"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _run(name, **kwargs):\n    '''\n    .. deprecated:: 2017.7.0\n       Function name stays the same, behaviour will change.\n\n    Run a single module function\n\n    ``name``\n        The module function to execute\n\n    ``returner``\n        Specify the returner to send the return of the module execution to\n\n    ``kwargs``\n        Pass any arguments needed to execute the function\n    '''\n    ret = {'name': name,\n           'changes': {},\n           'comment': '',\n           'result': None}\n    if name not in __salt__:\n        ret['comment'] = 'Module function {0} is not available'.format(name)\n        ret['result'] = False\n        return ret\n\n    if __opts__['test']:\n        ret['comment'] = 'Module function {0} is set to execute'.format(name)\n        return ret\n\n    aspec = salt.utils.args.get_function_argspec(__salt__[name])\n    args = []\n    defaults = {}\n\n    arglen = 0\n    deflen = 0\n    if isinstance(aspec.args, list):\n        arglen = len(aspec.args)\n    if isinstance(aspec.defaults, tuple):\n        deflen = len(aspec.defaults)\n    # Match up the defaults with the respective args\n    for ind in range(arglen - 1, -1, -1):\n        minus = arglen - ind\n        if deflen - minus > -1:\n            defaults[aspec.args[ind]] = aspec.defaults[-minus]\n    # overwrite passed default kwargs\n    for arg in defaults:\n        if arg == 'name':\n            if 'm_name' in kwargs:\n                defaults[arg] = kwargs.pop('m_name')\n        elif arg == 'fun':\n            if 'm_fun' in kwargs:\n                defaults[arg] = kwargs.pop('m_fun')\n        elif arg == 'state':\n            if 'm_state' in kwargs:\n                defaults[arg] = kwargs.pop('m_state')\n        elif arg == 'saltenv':\n            if 'm_saltenv' in kwargs:\n                defaults[arg] = kwargs.pop('m_saltenv')\n        if arg in kwargs:\n            defaults[arg] = kwargs.pop(arg)\n    missing = set()\n    for arg in aspec.args:\n        if arg == 'name':\n            rarg = 'm_name'\n        elif arg == 'fun':\n            rarg = 'm_fun'\n        elif arg == 'names':\n            rarg = 'm_names'\n        elif arg == 'state':\n            rarg = 'm_state'\n        elif arg == 'saltenv':\n            rarg = 'm_saltenv'\n        else:\n            rarg = arg\n        if rarg not in kwargs and arg not in defaults:\n            missing.add(rarg)\n            continue\n        if arg in defaults:\n            args.append(defaults[arg])\n        else:\n            args.append(kwargs.pop(rarg))\n    if missing:\n        comment = 'The following arguments are missing:'\n        for arg in missing:\n            comment += ' {0}'.format(arg)\n        ret['comment'] = comment\n        ret['result'] = False\n        return ret\n\n    if aspec.varargs:\n        if aspec.varargs == 'name':\n            rarg = 'm_name'\n        elif aspec.varargs == 'fun':\n            rarg = 'm_fun'\n        elif aspec.varargs == 'names':\n            rarg = 'm_names'\n        elif aspec.varargs == 'state':\n            rarg = 'm_state'\n        elif aspec.varargs == 'saltenv':\n            rarg = 'm_saltenv'\n        else:\n            rarg = aspec.varargs\n\n        if rarg in kwargs:\n            varargs = kwargs.pop(rarg)\n\n            if not isinstance(varargs, list):\n                msg = \"'{0}' must be a list.\"\n                ret['comment'] = msg.format(aspec.varargs)\n                ret['result'] = False\n                return ret\n\n            args.extend(varargs)\n\n    nkwargs = {}\n    if aspec.keywords and aspec.keywords in kwargs:\n        nkwargs = kwargs.pop(aspec.keywords)\n        if not isinstance(nkwargs, dict):\n            msg = \"'{0}' must be a dict.\"\n            ret['comment'] = msg.format(aspec.keywords)\n            ret['result'] = False\n            return ret\n\n    try:\n        if aspec.keywords:\n            mret = __salt__[name](*args, **nkwargs)\n        else:\n            mret = __salt__[name](*args)\n    except Exception as e:\n        ret['comment'] = 'Module function {0} threw an exception. Exception: {1}'.format(name, e)\n        ret['result'] = False\n        return ret\n    else:\n        if mret is not None or mret is not {}:\n            ret['changes']['ret'] = mret\n\n    if 'returner' in kwargs:\n        ret_ret = {\n                'id': __opts__['id'],\n                'ret': mret,\n                'fun': name,\n                'jid': salt.utils.jid.gen_jid(__opts__)}\n        returners = salt.loader.returners(__opts__, __salt__)\n        if kwargs['returner'] in returners:\n            returners[kwargs['returner']](ret_ret)\n    ret['comment'] = 'Module function {0} executed'.format(name)\n    ret['result'] = _get_result(mret, ret['changes'])\n\n    return ret", "response": "Execute a single module function"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef match(tgt, opts=None):\n    '''\n    Determines if this host is on the list\n    '''\n\n    if not opts:\n        opts = __opts__\n    try:\n        if ',' + opts['id'] + ',' in tgt \\\n                or tgt.startswith(opts['id'] + ',') \\\n                or tgt.endswith(',' + opts['id']):\n            return True\n        # tgt is a string, which we know because the if statement above did not\n        # cause one of the exceptions being caught. Therefore, look for an\n        # exact match. (e.g. salt -L foo test.ping)\n        return opts['id'] == tgt\n    except (AttributeError, TypeError):\n        # tgt is not a string, maybe it's a sequence type?\n        try:\n            return opts['id'] in tgt\n        except Exception:\n            # tgt was likely some invalid type\n            return False\n\n    # We should never get here based on the return statements in the logic\n    # above. If we do, it is because something above changed, and should be\n    # considered as a bug. Log a warning to help us catch this.\n    log.warning(\n        'List matcher unexpectedly did not return, for target %s, '\n        'this is probably a bug.', tgt\n    )\n    return False", "response": "Determines if this host is on the list\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nlist all InfluxDB databases in a single node", "response": "def db_list(user=None, password=None, host=None, port=None):\n    '''\n    List all InfluxDB databases\n\n    user\n        The user to connect as\n\n    password\n        The password of the user\n\n    host\n        The host to connect to\n\n    port\n        The port to connect to\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' influxdb08.db_list\n        salt '*' influxdb08.db_list <user> <password> <host> <port>\n\n    '''\n    client = _client(user=user, password=password, host=host, port=port)\n    return client.get_list_database()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if a database exists in Influxdb", "response": "def db_exists(name, user=None, password=None, host=None, port=None):\n    '''\n    Checks if a database exists in Influxdb\n\n    name\n        Database name to create\n\n    user\n        The user to connect as\n\n    password\n        The password of the user\n\n    host\n        The host to connect to\n\n    port\n        The port to connect to\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' influxdb08.db_exists <name>\n        salt '*' influxdb08.db_exists <name> <user> <password> <host> <port>\n    '''\n    dbs = db_list(user, password, host, port)\n    if not isinstance(dbs, list):\n        return False\n    return name in [db['name'] for db in dbs]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a database in the nclang", "response": "def db_create(name, user=None, password=None, host=None, port=None):\n    '''\n    Create a database\n\n    name\n        Database name to create\n\n    user\n        The user to connect as\n\n    password\n        The password of the user\n\n    host\n        The host to connect to\n\n    port\n        The port to connect to\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' influxdb08.db_create <name>\n        salt '*' influxdb08.db_create <name> <user> <password> <host> <port>\n    '''\n    if db_exists(name, user, password, host, port):\n        log.info('DB \\'%s\\' already exists', name)\n        return False\n    client = _client(user=user, password=password, host=host, port=port)\n    client.create_database(name)\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nremove a database from the sequence of databases", "response": "def db_remove(name, user=None, password=None, host=None, port=None):\n    '''\n    Remove a database\n\n    name\n        Database name to remove\n\n    user\n        The user to connect as\n\n    password\n        The password of the user\n\n    host\n        The host to connect to\n\n    port\n        The port to connect to\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' influxdb08.db_remove <name>\n        salt '*' influxdb08.db_remove <name> <user> <password> <host> <port>\n    '''\n    if not db_exists(name, user, password, host, port):\n        log.info('DB \\'%s\\' does not exist', name)\n        return False\n    client = _client(user=user, password=password, host=host, port=port)\n    return client.delete_database(name)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nlist cluster admins or database users.", "response": "def user_list(database=None, user=None, password=None, host=None, port=None):\n    '''\n    List cluster admins or database users.\n\n    If a database is specified: it will return database users list.\n    If a database is not specified: it will return cluster admins list.\n\n    database\n        The database to list the users from\n\n    user\n        The user to connect as\n\n    password\n        The password of the user\n\n    host\n        The host to connect to\n\n    port\n        The port to connect to\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' influxdb08.user_list\n        salt '*' influxdb08.user_list <database>\n        salt '*' influxdb08.user_list <database> <user> <password> <host> <port>\n    '''\n    client = _client(user=user, password=password, host=host, port=port)\n\n    if not database:\n        return client.get_list_cluster_admins()\n\n    client.switch_database(database)\n    return client.get_list_users()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if a user exists in a database or admin.", "response": "def user_exists(name, database=None, user=None, password=None, host=None, port=None):\n    '''\n    Checks if a cluster admin or database user exists.\n\n    If a database is specified: it will check for database user existence.\n    If a database is not specified: it will check for cluster admin existence.\n\n    name\n        User name\n\n    database\n        The database to check for the user to exist\n\n    user\n        The user to connect as\n\n    password\n        The password of the user\n\n    host\n        The host to connect to\n\n    port\n        The port to connect to\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' influxdb08.user_exists <name>\n        salt '*' influxdb08.user_exists <name> <database>\n        salt '*' influxdb08.user_exists <name> <database> <user> <password> <host> <port>\n    '''\n    users = user_list(database, user, password, host, port)\n    if not isinstance(users, list):\n        return False\n\n    for user in users:\n        # the dict key could be different depending on influxdb version\n        username = user.get('user', user.get('name'))\n        if username:\n            if username == name:\n                return True\n        else:\n            log.warning('Could not find username in user: %s', user)\n\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a new user in a cluster admin or a database user.", "response": "def user_create(name,\n                passwd,\n                database=None,\n                user=None,\n                password=None,\n                host=None,\n                port=None):\n    '''\n    Create a cluster admin or a database user.\n\n    If a database is specified: it will create database user.\n    If a database is not specified: it will create a cluster admin.\n\n    name\n        User name for the new user to create\n\n    passwd\n        Password for the new user to create\n\n    database\n        The database to create the user in\n\n    user\n        The user to connect as\n\n    password\n        The password of the user\n\n    host\n        The host to connect to\n\n    port\n        The port to connect to\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' influxdb08.user_create <name> <passwd>\n        salt '*' influxdb08.user_create <name> <passwd> <database>\n        salt '*' influxdb08.user_create <name> <passwd> <database> <user> <password> <host> <port>\n    '''\n    if user_exists(name, database, user, password, host, port):\n        if database:\n            log.info('User \\'%s\\' already exists for DB \\'%s\\'', name, database)\n        else:\n            log.info('Cluster admin \\'%s\\' already exists', name)\n        return False\n\n    client = _client(user=user, password=password, host=host, port=port)\n\n    if not database:\n        return client.add_cluster_admin(name, passwd)\n\n    client.switch_database(database)\n    return client.add_database_user(name, passwd)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef user_chpass(name,\n                passwd,\n                database=None,\n                user=None,\n                password=None,\n                host=None,\n                port=None):\n    '''\n    Change password for a cluster admin or a database user.\n\n    If a database is specified: it will update database user password.\n    If a database is not specified: it will update cluster admin password.\n\n    name\n        User name for whom to change the password\n\n    passwd\n        New password\n\n    database\n        The database on which to operate\n\n    user\n        The user to connect as\n\n    password\n        The password of the user\n\n    host\n        The host to connect to\n\n    port\n        The port to connect to\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' influxdb08.user_chpass <name> <passwd>\n        salt '*' influxdb08.user_chpass <name> <passwd> <database>\n        salt '*' influxdb08.user_chpass <name> <passwd> <database> <user> <password> <host> <port>\n    '''\n    if not user_exists(name, database, user, password, host, port):\n        if database:\n            log.info('User \\'%s\\' does not exist for DB \\'%s\\'', name, database)\n        else:\n            log.info('Cluster admin \\'%s\\' does not exist', name)\n        return False\n\n    client = _client(user=user, password=password, host=host, port=port)\n\n    if not database:\n        return client.update_cluster_admin_password(name, passwd)\n\n    client.switch_database(database)\n    return client.update_database_user_password(name, passwd)", "response": "Change password for a user in a database or cluster admin."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef user_remove(name,\n                database=None,\n                user=None,\n                password=None,\n                host=None,\n                port=None):\n    '''\n    Remove a cluster admin or a database user.\n\n    If a database is specified: it will remove the database user.\n    If a database is not specified: it will remove the cluster admin.\n\n    name\n        User name to remove\n\n    database\n        The database to remove the user from\n\n    user\n        User name for the new user to delete\n\n    user\n        The user to connect as\n\n    password\n        The password of the user\n\n    host\n        The host to connect to\n\n    port\n        The port to connect to\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' influxdb08.user_remove <name>\n        salt '*' influxdb08.user_remove <name> <database>\n        salt '*' influxdb08.user_remove <name> <database> <user> <password> <host> <port>\n    '''\n    if not user_exists(name, database, user, password, host, port):\n        if database:\n            log.info('User \\'%s\\' does not exist for DB \\'%s\\'', name, database)\n        else:\n            log.info('Cluster admin \\'%s\\' does not exist', name)\n        return False\n\n    client = _client(user=user, password=password, host=host, port=port)\n\n    if not database:\n        return client.delete_cluster_admin(name)\n\n    client.switch_database(database)\n    return client.delete_database_user(name)", "response": "This function removes a user from a cluster admin or a database user."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets an existing retention policy.", "response": "def retention_policy_get(database,\n                         name,\n                         user=None,\n                         password=None,\n                         host=None,\n                         port=None):\n    '''\n    Get an existing retention policy.\n\n    database\n        The database to operate on.\n\n    name\n        Name of the policy to modify.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' influxdb08.retention_policy_get metrics default\n    '''\n    client = _client(user=user, password=password, host=host, port=port)\n\n    for policy in client.get_list_retention_policies(database):\n        if policy['name'] == name:\n            return policy\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks if a retention policy exists.", "response": "def retention_policy_exists(database,\n                            name,\n                            user=None,\n                            password=None,\n                            host=None,\n                            port=None):\n    '''\n    Check if a retention policy exists.\n\n    database\n        The database to operate on.\n\n    name\n        Name of the policy to modify.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' influxdb08.retention_policy_exists metrics default\n    '''\n    policy = retention_policy_get(database, name, user, password, host, port)\n    return policy is not None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef retention_policy_add(database,\n                         name,\n                         duration,\n                         replication,\n                         default=False,\n                         user=None,\n                         password=None,\n                         host=None,\n                         port=None):\n    '''\n    Add a retention policy.\n\n    database\n        The database to operate on.\n\n    name\n        Name of the policy to modify.\n\n    duration\n        How long InfluxDB keeps the data.\n\n    replication\n        How many copies of the data are stored in the cluster.\n\n    default\n        Whether this policy should be the default or not. Default is False.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' influxdb.retention_policy_add metrics default 1d 1\n    '''\n    client = _client(user=user, password=password, host=host, port=port)\n    client.create_retention_policy(name, duration, replication, database, default)\n    return True", "response": "Adds a retention policy to an InfluxDB database."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmodifies an existing retention policy.", "response": "def retention_policy_alter(database,\n                           name,\n                           duration,\n                           replication,\n                           default=False,\n                           user=None,\n                           password=None,\n                           host=None,\n                           port=None):\n    '''\n    Modify an existing retention policy.\n\n    database\n        The database to operate on.\n\n    name\n        Name of the policy to modify.\n\n    duration\n        How long InfluxDB keeps the data.\n\n    replication\n        How many copies of the data are stored in the cluster.\n\n    default\n        Whether this policy should be the default or not. Default is False.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' influxdb08.retention_policy_modify metrics default 1d 1\n    '''\n    client = _client(user=user, password=password, host=host, port=port)\n    client.alter_retention_policy(name, database, duration, replication, default)\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef query(database,\n          query,\n          time_precision='s',\n          chunked=False,\n          user=None,\n          password=None,\n          host=None,\n          port=None):\n    '''\n    Querying data\n\n    database\n        The database to query\n\n    query\n        Query to be executed\n\n    time_precision\n        Time precision to use ('s', 'm', or 'u')\n\n    chunked\n        Whether is chunked or not\n\n    user\n        The user to connect as\n\n    password\n        The password of the user\n\n    host\n        The host to connect to\n\n    port\n        The port to connect to\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' influxdb08.query <database> <query>\n        salt '*' influxdb08.query <database> <query> <time_precision> <chunked> <user> <password> <host> <port>\n    '''\n    client = _client(user=user, password=password, host=host, port=port)\n    client.switch_database(database)\n    return client.query(query, time_precision=time_precision, chunked=chunked)", "response": "Query the database with the specified query"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sync_with_handlers(self, handlers=()):\n        '''\n        Sync the stored log records to the provided log handlers.\n        '''\n        if not handlers:\n            return\n\n        while self.__messages:\n            record = self.__messages.pop(0)\n            for handler in handlers:\n                if handler.level > record.levelno:\n                    # If the handler's level is higher than the log record one,\n                    # it should not handle the log record\n                    continue\n                handler.handle(record)", "response": "Sync the stored log records to the provided log handlers."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef handleError(self, record):\n        '''\n        Override the default error handling mechanism for py3\n        Deal with syslog os errors when the log file does not exist\n        '''\n        handled = False\n        if sys.stderr and sys.version_info >= (3, 5, 4):\n            t, v, tb = sys.exc_info()\n            if t.__name__ in 'FileNotFoundError':\n                sys.stderr.write('[WARNING ] The log_file does not exist. Logging not setup correctly or syslog service not started.\\n')\n                handled = True\n\n        if not handled:\n            super(SysLogHandler, self).handleError(record)", "response": "Override the default error handling mechanism for py3\n            Deal with syslog os errors when the log file does not exist"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef handleError(self, record):\n        '''\n        Override the default error handling mechanism\n\n        Deal with log file rotation errors due to log file in use\n        more softly.\n        '''\n        handled = False\n\n        # Can't use \"salt.utils.platform.is_windows()\" in this file\n        if (sys.platform.startswith('win') and\n                logging.raiseExceptions and\n                sys.stderr):  # see Python issue 13807\n            exc_type, exc, exc_traceback = sys.exc_info()\n            try:\n                # PermissionError is used since Python 3.3.\n                # OSError is used for previous versions of Python.\n                if exc_type.__name__ in ('PermissionError', 'OSError') and exc.winerror == 32:\n                    if self.level <= logging.WARNING:\n                        sys.stderr.write('[WARNING ] Unable to rotate the log file \"{0}\" '\n                                         'because it is in use\\n'.format(self.baseFilename)\n                        )\n                    handled = True\n            finally:\n                # 'del' recommended. See documentation of\n                # 'sys.exc_info()' for details.\n                del exc_type, exc, exc_traceback\n\n        if not handled:\n            super(RotatingFileHandler, self).handleError(record)", "response": "Override the default error handling mechanism\nxid Deal with log file rotation errors due to log file in use\nxid more softly."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef beacon(config):\n    '''\n    Return status for requested information\n    '''\n    log.debug(config)\n    ctime = datetime.datetime.utcnow().isoformat()\n\n    if not config:\n        config = [{\n            'loadavg': ['all'],\n            'cpustats': ['all'],\n            'meminfo': ['all'],\n            'vmstats': ['all'],\n            'time': ['all'],\n        }]\n\n    if not isinstance(config, list):\n        # To support the old dictionary config format\n        config = [config]\n\n    ret = {}\n    for entry in config:\n        for func in entry:\n            ret[func] = {}\n            try:\n                data = __salt__['status.{0}'.format(func)]()\n            except salt.exceptions.CommandExecutionError as exc:\n                log.debug('Status beacon attempted to process function %s '\n                          'but encountered error: %s', func, exc)\n                continue\n            if not isinstance(entry[func], list):\n                func_items = [entry[func]]\n            else:\n                func_items = entry[func]\n            for item in func_items:\n                if item == 'all':\n                    ret[func] = data\n                else:\n                    try:\n                        try:\n                            ret[func][item] = data[item]\n                        except TypeError:\n                            ret[func][item] = data[int(item)]\n                    except KeyError as exc:\n                        ret[func] = 'Status beacon is incorrectly configured: {0}'.format(exc)\n\n    return [{\n        'tag': ctime,\n        'data': ret,\n    }]", "response": "Return status for requested information"}
