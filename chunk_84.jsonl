{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef send_document(self, document, caption=\"\", **options):\n        return self.bot.api_call(\n            \"sendDocument\",\n            chat_id=str(self.id),\n            document=document,\n            caption=caption,\n            **options\n        )", "response": "Send a general file."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsends an OPUS - encoded. ogg audio file.", "response": "def send_voice(self, voice, **options):\n        \"\"\"\n        Send an OPUS-encoded .ogg audio file.\n\n        :param voice: Object containing the audio data\n        :param options: Additional sendVoice options (see\n            https://core.telegram.org/bots/api#sendvoice)\n\n        :Example:\n\n        >>> with open(\"voice.ogg\", \"rb\") as f:\n        >>>     await chat.send_voice(f)\n        \"\"\"\n        return self.bot.api_call(\n            \"sendVoice\", chat_id=str(self.id), voice=voice, **options\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef send_location(self, latitude, longitude, **options):\n        return self.bot.api_call(\n            \"sendLocation\",\n            chat_id=self.id,\n            latitude=latitude,\n            longitude=longitude,\n            **options\n        )", "response": "Send a point on the map."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef send_venue(self, latitude, longitude, title, address, **options):\n        return self.bot.api_call(\n            \"sendVenue\",\n            chat_id=self.id,\n            latitude=latitude,\n            longitude=longitude,\n            title=title,\n            address=address,\n            **options\n        )", "response": "Send information about a venue.\n\n        :param float latitude: Latitude of the location\n        :param float longitude: Longitude of the location\n        :param str title: Name of the venue\n        :param str address: Address of the venue\n        :param options: Additional sendVenue options (see\n            https://core.telegram.org/bots/api#sendvenue)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef send_contact(self, phone_number, first_name, **options):\n        return self.bot.api_call(\n            \"sendContact\",\n            chat_id=self.id,\n            phone_number=phone_number,\n            first_name=first_name,\n            **options\n        )", "response": "Send phone contacts.\n\n        :param str phone_number: Contact's phone number\n        :param str first_name: Contact's first name\n        :param options: Additional sendContact options (see\n            https://core.telegram.org/bots/api#sendcontact)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsend a chat action to tell the user that something is happening on the bot s side.", "response": "def send_chat_action(self, action):\n        \"\"\"\n        Send a chat action, to tell the user that something is happening on the\n        bot's side.\n\n        Available actions:\n\n        *  `typing` for text messages\n        *  `upload_photo` for photos\n        *  `record_video` and `upload_video` for videos\n        *  `record_audio` and `upload_audio` for audio files\n        *  `upload_document` for general files\n        *  `find_location` for location data\n\n        :param str action: Type of action to broadcast\n        \"\"\"\n        return self.bot.api_call(\"sendChatAction\", chat_id=self.id, action=action)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef send_media_group(\n        self,\n        media: str,\n        disable_notification: bool = False,\n        reply_to_message_id: int = None,\n        **options\n    ):\n        \"\"\"\n        Send a group of photos or videos as an album\n\n        :param media: A JSON-serialized array describing photos and videos\n        to be sent, must include 2\u201310 items\n        :param disable_notification: Sends the messages silently. Users will\n        receive a notification with no sound.\n        :param reply_to_message_id: If the messages are a reply, ID of the original message\n        :param options: Additional sendMediaGroup options (see\n        https://core.telegram.org/bots/api#sendmediagroup)\n\n        :Example:\n        >>> from json import dumps\n        >>> photos_urls = [\n        >>>     \"https://telegram.org/img/t_logo.png\",\n        >>>     \"https://telegram.org/img/SiteAndroid.jpg?1\",\n        >>>     \"https://telegram.org/img/SiteiOs.jpg?1\",\n        >>>     \"https://telegram.org/img/SiteWP.jpg?2\"\n        >>> ]\n        >>> tg_album = []\n        >>> count = len(photos_urls)\n        >>> for i, p in enumerate(photos_urls):\n        >>> {\n        >>>     'type': 'photo',\n        >>>     'media': p,\n        >>>     'caption': f'{i} of {count}'\n        >>> }\n        >>> await chat.send_media_group(dumps(tg_album))\n        \"\"\"\n\n        return self.bot.api_call(\n            \"sendMediaGroup\",\n            chat_id=str(self.id),\n            media=media,\n            disable_notification=disable_notification,\n            reply_to_message_id=reply_to_message_id,\n            **options\n        )", "response": "Send a media group to the Telegram bot."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nforwarding a message from another chat to this chat. :param int from_chat_id: ID of the chat to forward the message from :param int message_id: ID of the message to forward", "response": "def forward_message(self, from_chat_id, message_id):\n        \"\"\"\n        Forward a message from another chat to this chat.\n\n        :param int from_chat_id: ID of the chat to forward the message from\n        :param int message_id: ID of the message to forward\n        \"\"\"\n        return self.bot.api_call(\n            \"forwardMessage\",\n            chat_id=self.id,\n            from_chat_id=from_chat_id,\n            message_id=message_id,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nuses this method to kick a user from a group or supergroup.", "response": "def kick_chat_member(self, user_id):\n        \"\"\"\n        Use this method to kick a user from a group or a supergroup.\n        The bot must be an administrator in the group for this to work.\n\n        :param int user_id: Unique identifier of the target user\n        \"\"\"\n        return self.bot.api_call(\"kickChatMember\", chat_id=self.id, user_id=user_id)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nusing this method to unban a previously kicked user in a supergroup. The bot must be an administrator in the group for this to work. :param int user_id: Unique identifier of the target user", "response": "def unban_chat_member(self, user_id):\n        \"\"\"\n        Use this method to unban a previously kicked user in a supergroup.\n        The bot must be an administrator in the group for this to work.\n\n        :param int user_id: Unique identifier of the target user\n        \"\"\"\n        return self.bot.api_call(\"unbanChatMember\", chat_id=self.id, user_id=user_id)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef delete_message(self, message_id):\n        return self.bot.api_call(\n            \"deleteMessage\", chat_id=self.id, message_id=message_id\n        )", "response": "This method deletes a message from this chat."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a Chat object from a message.", "response": "def from_message(bot, message):\n        \"\"\"\n        Create a ``Chat`` object from a message.\n\n        :param Bot bot: ``Bot`` object the message and chat belong to\n        :param dict message: Message to base the object on\n        :return: A chat object based on the message\n        \"\"\"\n        chat = message[\"chat\"]\n        return Chat(bot, chat[\"id\"], chat[\"type\"], message)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run(self, timeout=POD_RUN_WAIT_TIMEOUT_SECONDS):\n\n        if not isinstance(timeout, int):\n            raise SyntaxError(\"K8sCronJob.run() timeout: [ {} ] is invalid.\")\n\n        if len(self.active):\n            raise CronJobAlreadyRunningException(\n                \"K8sCronJob.run() failed: CronJob: [ {} ] \"\n                \"has [ {} ] active Jobs currently.\".format(self.name, len(self.active)))\n\n        self.suspend = True\n        self.update()\n\n        pod = self.pod\n        if timeout:\n            self.POD_RUN_WAIT_TIMEOUT_SECONDS = timeout\n\n        try:\n            pod.create()\n            start_time = time.time()\n            while pod.phase not in ['Succeeded', 'Failed']:\n                pod.get()\n                time.sleep(2)\n                self._check_timeout(start_time)\n\n        except Exception as err:\n            raise CronJobRunException(\"K8sCronJob.run() failed: {}\".format(err))\n\n        finally:\n            pod.delete()\n            self.suspend = False\n            self.update()", "response": "Forces a K8sCronJob to run immediately.\n\n        - Fail if the K8sCronJob is currently running on-schedule.\n        - Suspend the K8sCronJob.\n        - Spawn a K8sPod.\n        - Unsuspend a K8sCronJob.\n\n        :param timeout: The timeout, in seconds, after which to kill the K8sPod.\n\n        :return: None."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef scale(config=None, name=None, replicas=None):\n\n        rc = K8sReplicationController(config=config, name=name).get()\n        rc.desired_replicas = replicas\n        rc.update()\n        rc._wait_for_desired_replicas()\n        return rc", "response": "Scales the number of pods in the specified K8sReplicationController to the desired replica count."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef rolling_update(config=None, name=None, image=None, container_name=None, rc_new=None):\n\n        if name is None:\n            raise SyntaxError(\n                'K8sReplicationController: name: [ {0} ] cannot be None.'.format(name))\n        if image is None and rc_new is None:\n            raise SyntaxError(\n                \"K8sReplicationController: please specify either 'image' or 'rc_new'\")\n        if container_name is not None and image is not None and rc_new is not None:\n            raise SyntaxError(\n                'K8sReplicationController: rc_new is mutually exclusive with an (container_name, image) pair.')\n\n        return K8sReplicationController._rolling_update_init(\n            config=config,\n            name=name,\n            image=image,\n            container_name=container_name,\n            rc_new=rc_new)", "response": "Performs a simple rolling update of a ReplicationController."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nrestarts the current ReplicationController to the current revision.", "response": "def restart(self):\n        \"\"\"\n        Restart will force a rolling update of the current ReplicationController to the current revision.\n        This essentially spawns a fresh copy of the RC and its pods. Useful when something is misbehaving.\n        \"\"\"\n\n        rc_new = copy.deepcopy(self)\n        return K8sReplicationController.rolling_update(\n            config=self.config,\n            name=self.name,\n            rc_new=rc_new)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef drain(self, ignore_daemonsets=False, delete_local_storage=False, force=False):\n\n        # inventory of K8sPods found on this node.\n        daemonset_pods = []\n        pods = self._pod_inventory()\n\n        # cordon the node.\n        self.unschedulable = True\n        self.update()\n\n        # loop through all pods and delete them.\n        for pod in pods:\n            if self._is_daemonset(pod):\n                if not ignore_daemonsets:\n                    raise DrainNodeException(\"K8sNode: pod: [ {} ] is managed by a DaemonSet.\".format(pod.name))\n                else:\n                    daemonset_pods.append(pod)\n                    continue\n            if self._has_local_storage(pod) and not delete_local_storage:\n                raise DrainNodeException(\"K8sNode: pod: [ {} ] has local storage that will be lost.\".format(pod.name))\n            if self._is_orphan(pod) and not force:\n                raise DrainNodeException(\"K8sNode: pod: [ {} ] is unmanaged and will be lost.\".format(pod.name))\n\n            pod.delete()\n\n        self._wait_for_pod_deletion(daemonset_pods)\n\n        return self", "response": "Deletes all K8sPods from this node and removes all daemons and deletes any unmanaged K8sPods from this node."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _pod_inventory(self):\n\n        pods = []\n        namespaces = K8sNamespace(config=self.config, name=\"yo\").list()\n        for ns in namespaces:\n            cfg = self.config\n            cfg.namespace = ns.name\n            p = K8sPod(config=cfg, name=\"yo\").list()\n            filtered = filter(lambda x: x.node_name == self.name, p)\n            pods += filtered\n        return pods", "response": "Returns the list of all K8sPods found on this K8sNode.\n        \n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndetermine if a K8sPod is part of a K8sDaemonSet.", "response": "def _is_daemonset(self, pod=None):\n        \"\"\"\n        Determines if a K8sPod is part of a K8sDaemonSet.\n        \n        :param pod: The K8sPod we're interested in.\n\n        :return: a boolean.\n        \"\"\"\n\n        if 'kubernetes.io/created-by' in pod.annotations:\n            parent = json.loads(pod.annotations['kubernetes.io/created-by'])\n            if parent['reference']['kind'] == 'DaemonSet':\n                return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndetermining if a K8sPod has any local storage susceptible to be lost.", "response": "def _has_local_storage(self, pod=None):\n        \"\"\"\n        Determines if a K8sPod has any local storage susceptible to be lost.\n\n        :param pod: The K8sPod we're interested in.\n\n        :return: a boolean.\n        \"\"\"\n\n        for vol in pod.volumes:\n            if vol.emptyDir is not None:\n                return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwaiting until all K8sPods are deleted.", "response": "def _wait_for_pod_deletion(self, daemonset_pods=None):\n        \"\"\"\n        Wait until this K8sNode has evicted all its K8sPods.\n        \n        :param daemonset_pods: A list of K8sPods on this K8sNode that are managed by a K8sDaemonSet.\n        \n        :return: None\n        \"\"\"\n\n        pods = self._pod_inventory()\n        start_time = time.time()\n        while len(pods) > 0:\n            if len(pods) == len(daemonset_pods):\n                break\n            pods = self._pod_inventory()\n            self._check_timeout(start_time)\n            time.sleep(1)\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_by_name(config=None, name=None, name_label='name'):\n\n        if name is None:\n            raise SyntaxError(\n                'Deployment: name: [ {0} ] cannot be None.'.format(name))\n        if not isinstance(name, str):\n            raise SyntaxError(\n                'Deployment: name: [ {0} ] must be a string.'.format(name))\n\n        if config is not None and not isinstance(config, K8sConfig):\n            raise SyntaxError(\n                'Deployment: config: [ {0} ] must be a K8sConfig'.format(config))\n\n        deps = K8sDeployment(config=config, name=name).list(labels={\n            name_label: name\n        })\n\n        return deps", "response": "Fetches a K8sDeployment object by name."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nperforms a rollback of the Deployment. If the 'revision' parameter is omitted, we fetch the Deployment's system-generated annotation containing the current revision, and revert to the version immediately preceding the current version. :param revision: The revision to rollback to. :param annotations: Annotations we'd like to update. :return: self", "response": "def rollback(self, revision=None, annotations=None):\n        \"\"\"\n        Performs a rollback of the Deployment.\n\n        If the 'revision' parameter is omitted, we fetch the Deployment's system-generated\n        annotation containing the current revision, and revert to the version immediately\n        preceding the current version.\n\n        :param revision: The revision to rollback to.\n        :param annotations: Annotations we'd like to update.\n        :return: self\n        \"\"\"\n\n        rollback = DeploymentRollback()\n        rollback.name = self.name\n\n        rollback_config = RollbackConfig()\n\n        # to the specified revision\n        if revision is not None:\n            rollback_config.revision = revision\n        # to the revision immediately preceding the current revision\n        else:\n            current_revision = int(self.get_annotation(self.REVISION_ANNOTATION))\n            rev = max(current_revision - 1, 0)\n            rollback_config.revision = rev\n\n        rollback.rollback_to = rollback_config\n\n        if annotations is not None:\n            rollback.updated_annotations = annotations\n\n        url = '{base}/{name}/rollback'.format(base=self.base_url, name=self.name)\n        state = self.request(\n            method='POST',\n            url=url,\n            data=rollback.serialize())\n\n        if not state.get('success'):\n            status = state.get('status', '')\n            reason = state.get('data', dict()).get('message', None)\n            message = 'K8sDeployment: ROLLBACK failed : HTTP {0} : {1}'.format(status, reason)\n            raise BadRequestException(message)\n\n        time.sleep(0.2)\n        self._wait_for_desired_replicas()\n        self.get()\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndeleting the most recent K8s ReplicaSets for this entry in the system.", "response": "def purge_replica_sets(self, keep=3):\n        \"\"\"\n        Builds a list of ReplicaSets, sorted from newest to oldest.\n        Slices the array, keeping the most X most recent ReplicaSets.\n        \n        :param keep: The number of ReplicaSets to keep.\n        :return: None\n        \"\"\"\n\n        rsets = K8sReplicaSet(\n            config=self.config,\n            name=\"yo\"\n        ).list(pattern=self.name, reverse=True)\n\n        to_purge = rsets[keep:]\n        for rset in to_purge:\n            rset.delete(cascade=True)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrolling a die. :return: None", "response": "def roll_dice():\n    \"\"\"\n    Roll a die.\n\n    :return: None\n    \"\"\"\n    sums = 0  # will return the sum of the roll calls.\n    while True:\n        roll = random.randint(1, 6)\n        sums += roll\n        if(input(\"Enter y or n to continue: \").upper()) == 'N':\n            print(sums)  # prints the sum of the roll calls\n            break"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the raw divider file in a string array.", "response": "def get_raw_file():\n    \"\"\"\n    Get the raw divider file in a string array.\n\n    :return: the array\n    :rtype: str\n    \"\"\"\n    with open(\"{0}/dividers.txt\".format(\n        os.path.abspath(\n            os.path.dirname(__file__)\n        )\n    ), mode=\"r\") as file_handler:\n        lines = file_handler.readlines()\n        lines[35] = str(\n            random.randint(0, 999999999999)\n        )\n        return lines"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreduce a repeating divider to the smallest repeating unit possible.", "response": "def reduce_to_unit(divider):\n    \"\"\"\n    Reduce a repeating divider to the smallest repeating unit possible.\n\n    Note: this function is used by make-div\n    :param divider: the divider\n    :return: smallest repeating unit possible\n    :rtype: str\n\n    :Example:\n    'XxXxXxX' -> 'Xx'\n    \"\"\"\n    for unit_size in range(1, len(divider) // 2 + 1):\n        length = len(divider)\n        unit = divider[:unit_size]\n\n        # Ignores mismatches in final characters:\n        divider_item = divider[:unit_size * (length // unit_size)]\n        if unit * (length // unit_size) == divider_item:\n            return unit\n    return divider"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef splitter(div, *args):\n    retstr = \"\"\n    if type(div) is int:\n        div = theArray()[div]\n    if len(args) == 1:\n        return args[0]\n    for s in args:\n        retstr += s\n        retstr += \"\\n\"\n        retstr += div\n        retstr += \"\\n\"\n    return retstr", "response": "Split text with dividers easily."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef area4info():\n    # Info variables:\n    name = \"area4\"\n    author = \"https://github.com/RDIL\"\n    author_email = rdillib.get_email()\n    description = \"Dividers in Python, the easy way!\"\n    return \"{0}: {1}\\n{2}: {3}\\n{4}: {5}\\n{6}: {7}\".format(\n        \"Name:\", name,\n        \"Author:\", author,\n        \"Author Email:\", author_email,\n        \"Description:\", description\n    )", "response": "Get some info about the area4 package."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates and returns a custom divider.", "response": "def make_div(unit, length=24,\n             start='', end='',\n             literal_unit=False):\n    \"\"\"\n    Generate and return a custom divider.\n\n    :param unit: str containing a repeating unit\n    :param length: The maximum length (won't be exceeded) (default: 24)\n    :param start: optional starting string\n    :param end: optional ending string\n    :param literal_unit: if True will not try to break\n    unit down into smaller repeating subunits\n    :return: a custom created divider\n    :rtype: str\n\n    :Example:\n    custom_div = make_div(unit='=-', length=40, start='<', end='=>')\n    note:: The generated string will be terminated\n    at the specified length regardless\n    of whether all the input strings have been fully replicated.\n    A unit > 1 length may\n    not be able to be replicated to extend to the full length.\n    In this situation, the\n    string will be shorter than the specified length.\n    Example: unit of 10 characters and a specified length of\n    25 will contain 2 units for\n    a total length of 20 characters.\n    \"\"\"\n    # Reduce the size if possible to extend closer to full length:\n    if not literal_unit:\n        unit = utils.reduce_to_unit(unit)\n    repeats = (length - len(start + end)) // len(unit)\n    return (start + unit * repeats + end)[0:length]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef localpath(*args):\n    plist = [ROOT] + list(args)\n    return os.path.abspath(pjoin(*plist))", "response": "construct an absolute path from a list relative to the root pycapnp directory"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndownloading an archive to a specific location", "response": "def fetch_archive(savedir, url, fname, force=False):\n    \"\"\"download an archive to a specific location\"\"\"\n    dest = pjoin(savedir, fname)\n    if os.path.exists(dest) and not force:\n        info(\"already have %s\" % fname)\n        return dest\n    info(\"fetching %s into %s\" % (url, savedir))\n    if not os.path.exists(savedir):\n        os.makedirs(savedir)\n    req = urlopen(url)\n    with open(dest, 'wb') as f:\n        f.write(req.read())\n    return dest"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fetch_libcapnp(savedir, url=None):\n    is_preconfigured = False\n    if url is None:\n        url = libcapnp_url\n        is_preconfigured = True\n    dest = pjoin(savedir, 'capnproto-c++')\n    if os.path.exists(dest):\n        info(\"already have %s\" % dest)\n        return\n    fname = fetch_archive(savedir, url, libcapnp)\n    tf = tarfile.open(fname)\n    with_version = pjoin(savedir, tf.firstmember.path)\n    tf.extractall(savedir)\n    tf.close()\n    # remove version suffix:\n    if is_preconfigured:\n        shutil.move(with_version, dest)\n    else:\n        cpp_dir = os.path.join(with_version, 'c++')\n        conf = Popen(['autoreconf', '-i'], cwd=cpp_dir)\n        returncode = conf.wait()\n        if returncode != 0:\n          raise RuntimeError('Autoreconf failed. Make sure autotools are installed on your system.')\n        shutil.move(cpp_dir, dest)", "response": "download and extract libcapnp"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nstaging platform. hpp into libcapnp sources", "response": "def stage_platform_hpp(capnproot):\n    \"\"\"stage platform.hpp into libcapnp sources\n\n    Tries ./configure first (except on Windows),\n    then falls back on included platform.hpp previously generated.\n    \"\"\"\n\n    platform_hpp = pjoin(capnproot, 'src', 'platform.hpp')\n    if os.path.exists(platform_hpp):\n        info(\"already have platform.hpp\")\n        return\n    if os.name == 'nt':\n        # stage msvc platform header\n        platform_dir = pjoin(capnproot, 'builds', 'msvc')\n    else:\n        info(\"attempting ./configure to generate platform.hpp\")\n\n        p = Popen('./configure', cwd=capnproot, shell=True,\n            stdout=PIPE, stderr=PIPE,\n        )\n        o,e = p.communicate()\n        if p.returncode:\n            warn(\"failed to configure libcapnp:\\n%s\" % e)\n            if sys.platform == 'darwin':\n                platform_dir = pjoin(HERE, 'include_darwin')\n            elif sys.platform.startswith('freebsd'):\n                platform_dir = pjoin(HERE, 'include_freebsd')\n            elif sys.platform.startswith('linux-armv'):\n                platform_dir = pjoin(HERE, 'include_linux-armv')\n            else:\n                platform_dir = pjoin(HERE, 'include_linux')\n        else:\n            return\n\n    info(\"staging platform.hpp from: %s\" % platform_dir)\n    shutil.copy(pjoin(platform_dir, 'platform.hpp'), platform_hpp)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _find_library(lib, path):\n    for d in path[::-1]:\n        real_lib = os.path.join(d, lib)\n        if os.path.exists(real_lib):\n            return real_lib", "response": "Find a library in the path"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef patch_lib_paths(fname, library_dirs):\n    if sys.platform != 'darwin':\n        return\n    \n    libs = _get_libs(fname)\n    for lib in libs:\n        if not lib.startswith(('@', '/')):\n            real_lib = _find_library(lib, library_dirs)\n            if real_lib:\n                _install_name_change(fname, lib, real_lib)", "response": "Load any weakly - defined libraries from their real location."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmakes a request that just evaluates the literal value 123. What's interesting here is that evaluate() returns a \"Value\", which is another interface and therefore points back to an object living on the server. We then have to call read() on that object to read it. However, even though we are making two RPC's, this block executes in *one* network round trip because of promise pipelining: we do not wait for the first call to complete before we send the second call to the server.", "response": "def main(host):\n    client = capnp.TwoPartyClient(host)\n\n    # Pass \"calculator\" to ez_restore (there's also a `restore` function that\n    # takes a struct or AnyPointer as an argument), and then cast the returned\n    # capability to it's proper type. This casting is due to capabilities not\n    # having a reference to their schema\n    calculator = client.bootstrap().cast_as(calculator_capnp.Calculator)\n\n    '''Make a request that just evaluates the literal value 123.\n\n    What's interesting here is that evaluate() returns a \"Value\", which is\n    another interface and therefore points back to an object living on the\n    server.  We then have to call read() on that object to read it.\n    However, even though we are making two RPC's, this block executes in\n    *one* network round trip because of promise pipelining:  we do not wait\n    for the first call to complete before we send the second call to the\n    server.'''\n\n    print('Evaluating a literal... ', end=\"\")\n\n    # Make the request. Note we are using the shorter function form (instead\n    # of evaluate_request), and we are passing a dictionary that represents a\n    # struct and its member to evaluate\n    eval_promise = calculator.evaluate({\"literal\": 123})\n\n    # This is equivalent to:\n    '''\n    request = calculator.evaluate_request()\n    request.expression.literal = 123\n\n    # Send it, which returns a promise for the result (without blocking).\n    eval_promise = request.send()\n    '''\n\n    # Using the promise, create a pipelined request to call read() on the\n    # returned object. Note that here we are using the shortened method call\n    # syntax read(), which is mostly just sugar for read_request().send()\n    read_promise = eval_promise.value.read()\n\n    # Now that we've sent all the requests, wait for the response.  Until this\n    # point, we haven't waited at all!\n    response = read_promise.wait()\n    assert response.value == 123\n\n    print(\"PASS\")\n\n    '''Make a request to evaluate 123 + 45 - 67.\n\n    The Calculator interface requires that we first call getOperator() to\n    get the addition and subtraction functions, then call evaluate() to use\n    them.  But, once again, we can get both functions, call evaluate(), and\n    then read() the result -- four RPCs -- in the time of *one* network\n    round trip, because of promise pipelining.'''\n\n    print(\"Using add and subtract... \", end='')\n\n    # Get the \"add\" function from the server.\n    add = calculator.getOperator(op='add').func\n    # Get the \"subtract\" function from the server.\n    subtract = calculator.getOperator(op='subtract').func\n\n    # Build the request to evaluate 123 + 45 - 67. Note the form is 'evaluate'\n    # + '_request', where 'evaluate' is the name of the method we want to call\n    request = calculator.evaluate_request()\n    subtract_call = request.expression.init('call')\n    subtract_call.function = subtract\n    subtract_params = subtract_call.init('params', 2)\n    subtract_params[1].literal = 67.0\n\n    add_call = subtract_params[0].init('call')\n    add_call.function = add\n    add_params = add_call.init('params', 2)\n    add_params[0].literal = 123\n    add_params[1].literal = 45\n\n    # Send the evaluate() request, read() the result, and wait for read() to finish.\n    eval_promise = request.send()\n    read_promise = eval_promise.value.read()\n\n    response = read_promise.wait()\n    assert response.value == 101\n\n    print(\"PASS\")\n\n    '''\n    Note: a one liner version of building the previous request (I highly\n    recommend not doing it this way for such a complicated structure, but I\n    just wanted to demonstrate it is possible to set all of the fields with a\n    dictionary):\n\n    eval_promise = calculator.evaluate(\n{'call': {'function': subtract,\n          'params': [{'call': {'function': add,\n                               'params': [{'literal': 123},\n                                          {'literal': 45}]}},\n                     {'literal': 67.0}]}})\n    '''\n\n    '''Make a request to evaluate 4 * 6, then use the result in two more\n    requests that add 3 and 5.\n\n    Since evaluate() returns its result wrapped in a `Value`, we can pass\n    that `Value` back to the server in subsequent requests before the first\n    `evaluate()` has actually returned.  Thus, this example again does only\n    one network round trip.'''\n\n    print(\"Pipelining eval() calls... \", end=\"\")\n\n    # Get the \"add\" function from the server.\n    add = calculator.getOperator(op='add').func\n    # Get the \"multiply\" function from the server.\n    multiply = calculator.getOperator(op='multiply').func\n\n    # Build the request to evaluate 4 * 6\n    request = calculator.evaluate_request()\n\n    multiply_call = request.expression.init(\"call\")\n    multiply_call.function = multiply\n    multiply_params = multiply_call.init(\"params\", 2)\n    multiply_params[0].literal = 4\n    multiply_params[1].literal = 6\n\n    multiply_result = request.send().value\n\n    # Use the result in two calls that add 3 and add 5.\n\n    add_3_request = calculator.evaluate_request()\n    add_3_call = add_3_request.expression.init(\"call\")\n    add_3_call.function = add\n    add_3_params = add_3_call.init(\"params\", 2)\n    add_3_params[0].previousResult = multiply_result\n    add_3_params[1].literal = 3\n    add_3_promise = add_3_request.send().value.read()\n\n    add_5_request = calculator.evaluate_request()\n    add_5_call = add_5_request.expression.init(\"call\")\n    add_5_call.function = add\n    add_5_params = add_5_call.init(\"params\", 2)\n    add_5_params[0].previousResult = multiply_result\n    add_5_params[1].literal = 5\n    add_5_promise = add_5_request.send().value.read()\n\n    # Now wait for the results.\n    assert add_3_promise.wait().value == 27\n    assert add_5_promise.wait().value == 29\n\n    print(\"PASS\")\n\n    '''Our calculator interface supports defining functions.  Here we use it\n    to define two functions and then make calls to them as follows:\n\n      f(x, y) = x * 100 + y\n      g(x) = f(x, x + 1) * 2;\n      f(12, 34)\n      g(21)\n\n    Once again, the whole thing takes only one network round trip.'''\n\n    print(\"Defining functions... \", end=\"\")\n\n    # Get the \"add\" function from the server.\n    add = calculator.getOperator(op='add').func\n    # Get the \"multiply\" function from the server.\n    multiply = calculator.getOperator(op='multiply').func\n\n    # Define f.\n    request = calculator.defFunction_request()\n    request.paramCount = 2\n\n    # Build the function body.\n    add_call = request.body.init(\"call\")\n    add_call.function = add\n    add_params = add_call.init(\"params\", 2)\n    add_params[1].parameter = 1  # y\n\n    multiply_call = add_params[0].init(\"call\")\n    multiply_call.function = multiply\n    multiply_params = multiply_call.init(\"params\", 2)\n    multiply_params[0].parameter = 0  # x\n    multiply_params[1].literal = 100\n\n    f = request.send().func\n\n    # Define g.\n    request = calculator.defFunction_request()\n    request.paramCount = 1\n\n    # Build the function body.\n    multiply_call = request.body.init(\"call\")\n    multiply_call.function = multiply\n    multiply_params = multiply_call.init(\"params\", 2)\n    multiply_params[1].literal = 2\n\n    f_call = multiply_params[0].init(\"call\")\n    f_call.function = f\n    f_params = f_call.init(\"params\", 2)\n    f_params[0].parameter = 0\n\n    add_call = f_params[1].init(\"call\")\n    add_call.function = add\n    add_params = add_call.init(\"params\", 2)\n    add_params[0].parameter = 0\n    add_params[1].literal = 1\n\n    g = request.send().func\n\n    # OK, we've defined all our functions.  Now create our eval requests.\n\n    # f(12, 34)\n    f_eval_request = calculator.evaluate_request()\n    f_call = f_eval_request.expression.init(\"call\")\n    f_call.function = f\n    f_params = f_call.init(\"params\", 2)\n    f_params[0].literal = 12\n    f_params[1].literal = 34\n    f_eval_promise = f_eval_request.send().value.read()\n\n    # g(21)\n    g_eval_request = calculator.evaluate_request()\n    g_call = g_eval_request.expression.init(\"call\")\n    g_call.function = g\n    g_call.init('params', 1)[0].literal = 21\n    g_eval_promise = g_eval_request.send().value.read()\n\n    # Wait for the results.\n    assert f_eval_promise.wait().value == 1234\n    assert g_eval_promise.wait().value == 4244\n\n    print(\"PASS\")\n\n    '''Make a request that will call back to a function defined locally.\n\n    Specifically, we will compute 2^(4 + 5).  However, exponent is not\n    defined by the Calculator server.  So, we'll implement the Function\n    interface locally and pass it to the server for it to use when\n    evaluating the expression.\n\n    This example requires two network round trips to complete, because the\n    server calls back to the client once before finishing.  In this\n    particular case, this could potentially be optimized by using a tail\n    call on the server side -- see CallContext::tailCall().  However, to\n    keep the example simpler, we haven't implemented this optimization in\n    the sample server.'''\n\n    print(\"Using a callback... \", end=\"\")\n\n    # Get the \"add\" function from the server.\n    add = calculator.getOperator(op='add').func\n\n    # Build the eval request for 2^(4+5).\n    request = calculator.evaluate_request()\n\n    pow_call = request.expression.init(\"call\")\n    pow_call.function = PowerFunction()\n    pow_params = pow_call.init(\"params\", 2)\n    pow_params[0].literal = 2\n\n    add_call = pow_params[1].init(\"call\")\n    add_call.function = add\n    add_params = add_call.init(\"params\", 2)\n    add_params[0].literal = 4\n    add_params[1].literal = 5\n\n    # Send the request and wait.\n    response = request.send().value.read().wait()\n    assert response.value == 512\n\n    print(\"PASS\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the exit status stdout stderr of a command", "response": "def get_output_error(cmd):\n    \"\"\"Return the exit status, stdout, stderr of a command\"\"\"\n    if not isinstance(cmd, list):\n        cmd = [cmd]\n    logging.debug(\"Running: %s\", ' '.join(map(quote, cmd)))\n    try:\n        result = Popen(cmd, stdout=PIPE, stderr=PIPE)\n    except IOError as e:\n        return -1, u(''), u('Failed to run %r: %r' % (cmd, e))\n    so, se = result.communicate()\n    # unicode:\n    so = so.decode('utf8', 'replace')\n    se = se.decode('utf8', 'replace')\n\n    return result.returncode, so, se"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads config dict from JSON file", "response": "def load_config(name, base='conf'):\n    \"\"\"Load config dict from JSON\"\"\"\n    fname = pjoin(base, name + '.json')\n    if not os.path.exists(fname):\n        return {}\n    try:\n        with open(fname) as f:\n            cfg = json.load(f)\n    except Exception as e:\n        warn(\"Couldn't load %s: %s\" % (fname, e))\n        cfg = {}\n    return cfg"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsave config dict to JSON", "response": "def save_config(name, data, base='conf'):\n    \"\"\"Save config dict to JSON\"\"\"\n    if not os.path.exists(base):\n        os.mkdir(base)\n    fname = pjoin(base, name+'.json')\n    with open(fname, 'w') as f:\n        json.dump(data, f, indent=2)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlooking for options in environment vars", "response": "def get_eargs():\n    \"\"\" Look for options in environment vars \"\"\"\n\n    settings = {}\n\n    zmq = os.environ.get(\"ZMQ_PREFIX\", None)\n    if zmq is not None:\n        debug(\"Found environ var ZMQ_PREFIX=%s\" % zmq)\n        settings['zmq_prefix'] = zmq\n\n    return settings"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nturn a ConfigParser into a nested dict because ConfigParser objects are dumb.", "response": "def cfg2dict(cfg):\n    \"\"\"turn a ConfigParser into a nested dict\n    \n    because ConfigParser objects are dumb.\n    \"\"\"\n    d = {}\n    for section in cfg.sections():\n        d[section] = dict(cfg.items(section))\n    return d"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_cfg_args():\n\n    if not os.path.exists('setup.cfg'):\n        return {}\n    cfg = ConfigParser()\n    cfg.read('setup.cfg')\n    cfg = cfg2dict(cfg)\n\n    g = cfg.setdefault('global', {})\n    # boolean keys:\n    for key in ['libzmq_extension',\n                'bundle_libzmq_dylib',\n                'no_libzmq_extension',\n                'have_sys_un_h',\n                'skip_check_zmq',\n                ]:\n        if key in g:\n            g[key] = eval(g[key])\n\n    # globals go to top level\n    cfg.update(cfg.pop('global'))\n    return cfg", "response": "Look for options in setup. cfg and return them as a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef config_from_prefix(prefix):\n    settings = {}\n    if prefix.lower() in ('default', 'auto', ''):\n        settings['zmq_prefix'] = ''\n        settings['libzmq_extension'] = False\n        settings['no_libzmq_extension'] = False\n    elif prefix.lower() in ('bundled', 'extension'):\n        settings['zmq_prefix'] = ''\n        settings['libzmq_extension'] = True\n        settings['no_libzmq_extension'] = False\n    else:\n        settings['zmq_prefix'] = prefix\n        settings['libzmq_extension'] = False\n        settings['no_libzmq_extension'] = True\n    return settings", "response": "Get config from zmq prefix"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmerge two containers into is updated d has priority", "response": "def merge(into, d):\n    \"\"\"merge two containers\n    \n    into is updated, d has priority\n    \"\"\"\n    if isinstance(into, dict):\n        for key in d.keys():\n            if key not in into:\n                into[key] = d[key]\n            else:\n                into[key] = merge(into[key], d[key])\n        return into\n    elif isinstance(into, list):\n        return into + d\n    else:\n        return d"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndiscover custom settings for ZMQ path", "response": "def discover_settings(conf_base=None):\n    \"\"\" Discover custom settings for ZMQ path\"\"\"\n    settings = {\n        'zmq_prefix': '',\n        'libzmq_extension': False,\n        'no_libzmq_extension': False,\n        'skip_check_zmq': False,\n        'build_ext': {},\n        'bdist_egg': {},\n    }\n    if sys.platform.startswith('win'):\n        settings['have_sys_un_h'] = False\n    \n    if conf_base:\n        # lowest priority\n        merge(settings, load_config('config', conf_base))\n    merge(settings, get_cfg_args())\n    merge(settings, get_eargs())\n    \n    return settings"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef evaluate_impl(expression, params=None):\n    '''Implementation of CalculatorImpl::evaluate(), also shared by\n    FunctionImpl::call().  In the latter case, `params` are the parameter\n    values passed to the function; in the former case, `params` is just an\n    empty list.'''\n\n    which = expression.which()\n\n    if which == 'literal':\n        return capnp.Promise(expression.literal)\n    elif which == 'previousResult':\n        return read_value(expression.previousResult)\n    elif which == 'parameter':\n        assert expression.parameter < len(params)\n        return capnp.Promise(params[expression.parameter])\n    elif which == 'call':\n        call = expression.call\n        func = call.function\n\n        # Evaluate each parameter.\n        paramPromises = [evaluate_impl(param, params) for param in call.params]\n\n        joinedParams = capnp.join_promises(paramPromises)\n        # When the parameters are complete, call the function.\n        ret = (joinedParams\n               .then(lambda vals: func.call(vals))\n               .then(lambda result: result.value))\n\n        return ret\n    else:\n        raise ValueError(\"Unknown expression type: \" + which)", "response": "Implementation of CalculatorImpl. evaluate also shared by\n    FunctionImpl. call."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nnotes that we're returning a Promise object here, and bypassing the helper functionality that normally sets the results struct from the returned object. Instead, we set _context.results directly inside of another promise", "response": "def call(self, params, _context, **kwargs):\n        '''Note that we're returning a Promise object here, and bypassing the\n        helper functionality that normally sets the results struct from the\n        returned object. Instead, we set _context.results directly inside of\n        another promise'''\n\n        assert len(params) == self.paramCount\n        # using setattr because '=' is not allowed inside of lambdas\n        return evaluate_impl(self.body, params).then(lambda value: setattr(_context.results, 'value', value))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompile link & execute a test program in empty directory basedir.", "response": "def detect_version(basedir, compiler=None, **compiler_attrs):\n    \"\"\"Compile, link & execute a test program, in empty directory `basedir`.\n\n    The C compiler will be updated with any keywords given via setattr.\n\n    Parameters\n    ----------\n\n    basedir : path\n        The location where the test program will be compiled and run\n    compiler : str\n        The distutils compiler key (e.g. 'unix', 'msvc', or 'mingw32')\n    **compiler_attrs : dict\n        Any extra compiler attributes, which will be set via ``setattr(cc)``.\n\n    Returns\n    -------\n\n    A dict of properties for zmq compilation, with the following two keys:\n\n    vers : tuple\n        The ZMQ version as a tuple of ints, e.g. (2,2,0)\n    settings : dict\n        The compiler options used to compile the test function, e.g. `include_dirs`,\n        `library_dirs`, `libs`, etc.\n    \"\"\"\n    if compiler is None:\n        compiler = get_default_compiler()\n    cfile = pjoin(basedir, 'vers.cpp')\n    shutil.copy(pjoin(os.path.dirname(__file__), 'vers.cpp'), cfile)\n\n    # check if we need to link against Realtime Extensions library\n    if sys.platform.startswith('linux'):\n        cc = ccompiler.new_compiler(compiler=compiler)\n        cc.output_dir = basedir\n        if not cc.has_function('timer_create'):\n            if 'libraries' not in compiler_attrs:\n                compiler_attrs['libraries'] = []\n            compiler_attrs['libraries'].append('rt')\n\n    cc = get_compiler(compiler=compiler, **compiler_attrs)\n    efile = test_compilation(cfile, compiler=cc)\n    patch_lib_paths(efile, cc.library_dirs)\n\n    rc, so, se = get_output_error([efile])\n    if rc:\n        msg = \"Error running version detection script:\\n%s\\n%s\" % (so,se)\n        logging.error(msg)\n        raise IOError(msg)\n\n    handlers = {'vers':  lambda val: tuple(int(v) for v in val.split('.'))}\n\n    props = {}\n    for line in (x for x in so.split('\\n') if x):\n        key, val = line.split(':')\n        props[key] = handlers[key](val)\n\n    return props"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate enum block for constant_enums. pxi", "response": "def cython_enums():\n    \"\"\"generate `enum: ZMQ_CONST` block for constant_enums.pxi\"\"\"\n    lines = []\n    for name in all_names:\n        if no_prefix(name):\n            lines.append('enum: ZMQ_{0} \"{0}\"'.format(name))\n        else:\n            lines.append('enum: ZMQ_{0}'.format(name))\n            \n    return dict(ZMQ_ENUMS='\\n    '.join(lines))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ifndefs():\n    lines = ['#define _PYZMQ_UNDEFINED (-9999)']\n    for name in all_names:\n        if not no_prefix(name):\n            name = 'ZMQ_%s' % name\n        lines.append(ifndef_t.format(name))\n    return dict(ZMQ_IFNDEFS='\\n'.join(lines))", "response": "generate a dict of ZMQ_CONST block for zmq_constants. h"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef constants_pyx():\n    all_lines = []\n    assign_lines = []\n    for name in all_names:\n        if name == \"NULL\":\n            # avoid conflict with NULL in Cython\n            assign_lines.append(\"globals()['NULL'] = ZMQ_NULL\")\n        else:\n            assign_lines.append('{0} = ZMQ_{0}'.format(name))\n        all_lines.append('  \"{0}\",'.format(name))\n    return dict(ASSIGNMENTS='\\n'.join(assign_lines), ALL='\\n'.join(all_lines))", "response": "generate CONST and __all__ for constants. pxi"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef generate_file(fname, ns_func, dest_dir=\".\"):\n    with open(pjoin(root, 'buildutils', 'templates', '%s' % fname), 'r') as f:\n        tpl = f.read()\n    out = tpl.format(**ns_func())\n    dest = pjoin(dest_dir, fname)\n    info(\"generating %s from template\" % dest)\n    with open(dest, 'w') as f:\n        f.write(out)", "response": "generate a constants file from its template"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef render_constants():\n    generate_file(\"constant_enums.pxi\", cython_enums, pjoin(root, 'zmq', 'backend', 'cython'))\n    generate_file(\"constants.pxi\", constants_pyx, pjoin(root, 'zmq', 'backend', 'cython'))\n    generate_file(\"zmq_constants.h\", ifndefs, pjoin(root, 'zmq', 'utils'))", "response": "render generated constant files from templates"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a new SubRipFile object with only the subtitles that match the given time.", "response": "def slice(self, starts_before=None, starts_after=None, ends_before=None,\n              ends_after=None):\n        \"\"\"\n        slice([starts_before][, starts_after][, ends_before][, ends_after]) \\\n-> SubRipFile clone\n\n        All arguments are optional, and should be coercible to SubRipTime\n        object.\n\n        It reduce the set of subtitles to those that match match given time\n        constraints.\n\n        The returned set is a clone, but still contains references to original\n        subtitles. So if you shift this returned set, subs contained in the\n        original SubRipFile instance will be altered too.\n\n        Example:\n            >>> subs.slice(ends_after={'seconds': 20}).shift(seconds=2)\n        \"\"\"\n        clone = copy(self)\n\n        if starts_before:\n            clone.data = (i for i in clone.data if i.start < starts_before)\n        if starts_after:\n            clone.data = (i for i in clone.data if i.start > starts_after)\n        if ends_before:\n            clone.data = (i for i in clone.data if i.end < ends_before)\n        if ends_after:\n            clone.data = (i for i in clone.data if i.end > ends_after)\n\n        clone.data = list(clone.data)\n        return clone"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a new copy of this file with only the subtiles visible at the given timestamp.", "response": "def at(self, timestamp=None, **kwargs):\n        \"\"\"\n        at(timestamp) -> SubRipFile clone\n\n        timestamp argument should be coercible to SubRipFile object.\n\n        A specialization of slice. Return all subtiles visible at the\n        timestamp mark.\n\n        Example:\n            >>> subs.at((0, 0, 20, 0)).shift(seconds=2)\n            >>> subs.at(seconds=20).shift(seconds=2)\n        \"\"\"\n        time = timestamp or kwargs\n        return self.slice(starts_before=time, ends_after=time)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nshifting start and end attributes of each item of file by the given arguments.", "response": "def shift(self, *args, **kwargs):\n        \"\"\"shift(hours, minutes, seconds, milliseconds, ratio)\n\n        Shift `start` and `end` attributes of each items of file either by\n        applying a ratio or by adding an offset.\n\n        `ratio` should be either an int or a float.\n        Example to convert subtitles from 23.9 fps to 25 fps:\n        >>> subs.shift(ratio=25/23.9)\n\n        All \"time\" arguments are optional and have a default value of 0.\n        Example to delay all subs from 2 seconds and half\n        >>> subs.shift(seconds=2, milliseconds=500)\n        \"\"\"\n        for item in self:\n            item.shift(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsorting subs and reset their index attribute.", "response": "def clean_indexes(self):\n        \"\"\"\n        clean_indexes()\n\n        Sort subs and reset their index attribute. Should be called after\n        destructive operations like split or such.\n        \"\"\"\n        self.sort()\n        for index, item in enumerate(self):\n            item.index = index + 1"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nopening a file and return a new instance of the class.", "response": "def open(cls, path='', encoding=None, error_handling=ERROR_PASS):\n        \"\"\"\n        open([path, [encoding]])\n\n        If you do not provide any encoding, it can be detected if the file\n        contain a bit order mark, unless it is set to utf-8 as default.\n        \"\"\"\n        source_file, encoding = cls._open_unicode_file(path, claimed_encoding=encoding)\n        new_file = cls(path=path, encoding=encoding)\n        new_file.read(source_file, error_handling=error_handling)\n        source_file.close()\n        return new_file"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef from_string(cls, source, **kwargs):\n        error_handling = kwargs.pop('error_handling', None)\n        new_file = cls(**kwargs)\n        new_file.read(source.splitlines(True), error_handling=error_handling)\n        return new_file", "response": "Create a new SubRipFile from a string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef read(self, source_file, error_handling=ERROR_PASS):\n        self.eol = self._guess_eol(source_file)\n        self.extend(self.stream(source_file, error_handling=error_handling))\n        return self", "response": "Reads the log entry from a source file and appends it to the current instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef save(self, path=None, encoding=None, eol=None):\n        path = path or self.path\n        encoding = encoding or self.encoding\n\n        save_file = codecs.open(path, 'w+', encoding=encoding)\n        self.write_into(save_file, eol=eol)\n        save_file.close()", "response": "Save the current object to a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write_into(self, output_file, eol=None):\n        output_eol = eol or self.eol\n\n        for item in self:\n            string_repr = str(item)\n            if output_eol != '\\n':\n                string_repr = string_repr.replace('\\n', output_eol)\n            output_file.write(string_repr)\n            # Only add trailing eol if it's not already present.\n            # It was kept in the SubRipItem's text before but it really\n            # belongs here. Existing applications might give us subtitles\n            # which already contain a trailing eol though.\n            if not string_repr.endswith(2 * output_eol):\n                output_file.write(output_eol)", "response": "Serialize current state into output_file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncoercing many types to SubRipTime instance.", "response": "def coerce(cls, other):\n        \"\"\"\n        Coerce many types to SubRipTime instance.\n        Supported types:\n          - str/unicode\n          - int/long\n          - datetime.time\n          - any iterable\n          - dict\n        \"\"\"\n        if isinstance(other, SubRipTime):\n            return other\n        if isinstance(other, basestring):\n            return cls.from_string(other)\n        if isinstance(other, int):\n            return cls.from_ordinal(other)\n        if isinstance(other, time):\n            return cls.from_time(other)\n        try:\n            return cls(**other)\n        except TypeError:\n            return cls(*other)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef shift(self, *args, **kwargs):\n        if 'ratio' in kwargs:\n            self *= kwargs.pop('ratio')\n        self += self.__class__(*args, **kwargs)", "response": "Shift the current time to the next time."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef from_string(cls, source):\n        items = cls.RE_TIME_SEP.split(source)\n        if len(items) != 4:\n            raise InvalidTimeString\n        return cls(*(cls.parse_int(i) for i in items))", "response": "Returns a new SubRipTime instance from a string representation of the ISO8601 formatted time."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a SubRipTime object corresponding to the given datetime. time object.", "response": "def from_time(cls, source):\n        \"\"\"\n        datetime.time -> SubRipTime corresponding to time object\n        \"\"\"\n        return cls(hours=source.hour, minutes=source.minute,\n            seconds=source.second, milliseconds=source.microsecond // 1000)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_time(self):\n        return time(self.hours, self.minutes, self.seconds,\n                    self.milliseconds * 1000)", "response": "Convert this SubRipTime instance into a pure datetime. time object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nshifts the given values from start and end attributes.", "response": "def shift(self, *args, **kwargs):\n        \"\"\"\n        shift(hours, minutes, seconds, milliseconds, ratio)\n\n        Add given values to start and end attributes.\n        All arguments are optional and have a default value of 0.\n        \"\"\"\n        self.start.shift(*args, **kwargs)\n        self.end.shift(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef export_pages(root_page, export_unpublished=False):\n    pages = Page.objects.descendant_of(root_page, inclusive=True).order_by('path').specific()\n    if not export_unpublished:\n        pages = pages.filter(live=True)\n\n    page_data = []\n    exported_paths = set()\n    for (i, page) in enumerate(pages):\n        parent_path = page.path[:-(Page.steplen)]\n        # skip over pages whose parents haven't already been exported\n        # (which means that export_unpublished is false and the parent was unpublished)\n        if i == 0 or (parent_path in exported_paths):\n            page_data.append({\n                'content': json.loads(page.to_json()),\n                'model': page.content_type.model,\n                'app_label': page.content_type.app_label,\n            })\n            exported_paths.add(page.path)\n    return {\n        'pages': page_data\n    }", "response": "Creates a JSON defintion of part of a site s page tree starting from root_page and descending into its descendants."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nimporting a part of a source site's page tree via a direct API request from this Wagtail Admin to the source site The source site's base url and the source page id of the point in the tree to import defined what to import and the destination parent page defines where to import it to.", "response": "def import_from_api(request):\n    \"\"\"\n    Import a part of a source site's page tree via a direct API request from\n    this Wagtail Admin to the source site\n\n    The source site's base url and the source page id of the point in the\n    tree to import defined what to import and the destination parent page\n    defines where to import it to.\n    \"\"\"\n    if request.method == 'POST':\n        form = ImportFromAPIForm(request.POST)\n        if form.is_valid():\n            # remove trailing slash from base url\n            base_url = re.sub(r'\\/$', '', form.cleaned_data['source_site_base_url'])\n            import_url = (\n                base_url + reverse('wagtailimportexport:export', args=[form.cleaned_data['source_page_id']])\n            )\n            r = requests.get(import_url)\n            import_data = r.json()\n            parent_page = form.cleaned_data['parent_page']\n\n            try:\n                page_count = import_pages(import_data, parent_page)\n            except LookupError as e:\n                messages.error(request, _(\n                    \"Import failed: %(reason)s\") % {'reason': e}\n                )\n            else:\n                messages.success(request, ungettext(\n                    \"%(count)s page imported.\",\n                    \"%(count)s pages imported.\",\n                    page_count) % {'count': page_count}\n                )\n            return redirect('wagtailadmin_explore', parent_page.pk)\n    else:\n        form = ImportFromAPIForm()\n\n    return render(request, 'wagtailimportexport/import_from_api.html', {\n        'form': form,\n    })"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nimports a part of a source site's page tree via an import of a JSON file exported to a user's filesystem from the source site's Wagtail Admin The source site's base url and the source page id of the point in the tree to import defined what to import and the destination parent page defines where to import it to.", "response": "def import_from_file(request):\n    \"\"\"\n    Import a part of a source site's page tree via an import of a JSON file\n    exported to a user's filesystem from the source site's Wagtail Admin\n\n    The source site's base url and the source page id of the point in the\n    tree to import defined what to import and the destination parent page\n    defines where to import it to.\n    \"\"\"\n    if request.method == 'POST':\n        form = ImportFromFileForm(request.POST, request.FILES)\n        if form.is_valid():\n            import_data = json.loads(form.cleaned_data['file'].read().decode('utf-8-sig'))\n            parent_page = form.cleaned_data['parent_page']\n\n            try:\n                page_count = import_pages(import_data, parent_page)\n            except LookupError as e:\n                messages.error(request, _(\n                    \"Import failed: %(reason)s\") % {'reason': e}\n                )\n            else:\n                messages.success(request, ungettext(\n                    \"%(count)s page imported.\",\n                    \"%(count)s pages imported.\",\n                    page_count) % {'count': page_count}\n                )\n            return redirect('wagtailadmin_explore', parent_page.pk)\n    else:\n        form = ImportFromFileForm()\n\n    return render(request, 'wagtailimportexport/import_from_file.html', {\n        'form': form,\n    })"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef export_to_file(request):\n    if request.method == 'POST':\n        form = ExportForm(request.POST)\n        if form.is_valid():\n            payload = export_pages(form.cleaned_data['root_page'], export_unpublished=True)\n            response = JsonResponse(payload)\n            response['Content-Disposition'] = 'attachment; filename=\"export.json\"'\n            return response\n    else:\n        form = ExportForm()\n\n    return render(request, 'wagtailimportexport/export_to_file.html', {\n        'form': form,\n    })", "response": "Exports a part of this source site s page tree to a JSON fileon this user s filesystem for subsequent import in a destination\n site s Wagtail Admin"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef export(request, page_id, export_unpublished=False):\n    try:\n        if export_unpublished:\n            root_page = Page.objects.get(id=page_id)\n        else:\n            root_page = Page.objects.get(id=page_id, live=True)\n    except Page.DoesNotExist:\n        return JsonResponse({'error': _('page not found')})\n\n    payload = export_pages(root_page, export_unpublished=export_unpublished)\n\n    return JsonResponse(payload)", "response": "Export a part of the page tree at the given page_id."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef import_pages(import_data, parent_page):\n    pages_by_original_path = {}\n    pages_by_original_id = {}\n\n    # First create the base Page records; these contain no foreign keys, so this allows us to\n    # build a complete mapping from old IDs to new IDs before we go on to importing the\n    # specific page models, which may require us to rewrite page IDs within foreign keys / rich\n    # text / streamfields.\n    page_content_type = ContentType.objects.get_for_model(Page)\n    for (i, page_record) in enumerate(import_data['pages']):\n        # build a base Page instance from the exported content (so that we pick up its title and other\n        # core attributes)\n        page = Page.from_serializable_data(page_record['content'])\n        original_path = page.path\n        original_id = page.id\n\n        # clear id and treebeard-related fields so that they get reassigned when we save via add_child\n        page.id = None\n        page.path = None\n        page.depth = None\n        page.numchild = 0\n        page.url_path = None\n        page.content_type = page_content_type\n        if i == 0:\n            parent_page.add_child(instance=page)\n        else:\n            # Child pages are created in the same sibling path order as the\n            # source tree because the export is ordered by path\n            parent_path = original_path[:-(Page.steplen)]\n            pages_by_original_path[parent_path].add_child(instance=page)\n\n        pages_by_original_path[original_path] = page\n        pages_by_original_id[original_id] = page\n\n    for (i, page_record) in enumerate(import_data['pages']):\n        # Get the page model of the source page by app_label and model name\n        # The content type ID of the source page is not in general the same\n        # between the source and destination sites but the page model needs\n        # to exist on both.\n        # Raises LookupError exception if there is no matching model\n        model = apps.get_model(page_record['app_label'], page_record['model'])\n\n        specific_page = model.from_serializable_data(page_record['content'], check_fks=False, strict_fks=False)\n        base_page = pages_by_original_id[specific_page.id]\n        specific_page.page_ptr = base_page\n        specific_page.__dict__.update(base_page.__dict__)\n        specific_page.content_type = ContentType.objects.get_for_model(model)\n        update_page_references(specific_page, pages_by_original_id)\n        specific_page.save()\n\n    return len(import_data['pages'])", "response": "Create the base page records for each page in the source site s page tree and add them to the parent page tree."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef remove_all_filters(self):\n\n        # attitude: None = no attitude, True = positive, False = negative\n        self.attitude_filter = self.source_filter = None\n        self.question_filter = self.link_filter = False", "response": "Removes all filters from the current object"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_keyword(self, word, or_operator=False):\n\n        if isinstance(word, str if py3k else basestring) and len(word) >= 2:\n            self.searchterms.append(word if \" \" not in word else '\"%s\"' % word)\n        elif isinstance(word, (tuple,list)):\n            word = [ (i if \" \" not in i else '\"%s\"' % i)  for i in word ]\n            self.searchterms += [\" OR \".join(word)] if or_operator else word\n        else:\n            raise TwitterSearchException(1000)", "response": "Adds a given string or list to the current keyword list."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_keywords(self, words, or_operator=False):\n\n        if not isinstance(words, (tuple,list)):\n            raise TwitterSearchException(1001)\n        words = [ (i if \" \" not in i else '\"%s\"' % i)  for i in words ]\n        self.searchterms = [\" OR \".join(words)] if or_operator else words", "response": "Sets a given list as the new keyword list\n       "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_search_url(self, url):\n\n        self.__init__()\n\n        if url[0] == '?':\n            url = url[1:]\n\n        args = parse_qs(url)\n\n        # urldecode keywords\n        for arg in args['q']:\n            self.searchterms += [ unquote(i) for i in arg.split(\" \") ]\n        del args['q']\n\n        for key, value in args.items():\n            self.arguments.update({key: unquote(value[0])})\n\n        # look for advanced operators: attitudes\n        for attitude in self._attitudes:\n            try:\n                i = self.searchterms.index(attitude)\n                del self.searchterms[i]\n                self.attitude_filter = (i == 1)\n            except ValueError:\n                pass\n\n        # look for advanced operators: question\n        try:\n            del self.searchterms[ self.searchterms.index(self._question) ]\n            self.question_filter = True\n        except ValueError:\n            pass\n\n        # look for advanced operators: link-filter\n        try:\n            del self.searchterms[ self.searchterms.index(self._link) ]\n            self.link_filter = True\n        except ValueError:\n            pass\n\n\n\n        # look for advanced operators: source-filter\n        i = None\n        for element in self.searchterms:\n            if element.startswith(self._source):\n                i = element\n                break\n        if i:\n            del self.searchterms[ self.searchterms.index(i) ]\n            self.source_filter = i[ len(self._source): ]", "response": "Reads given query string and stores key - value tuples in self. searchterms."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a url - encoded query string from stored key - values tuples.", "response": "def create_search_url(self):\n        \"\"\" Generates (urlencoded) query string from stored key-values tuples\n\n        :returns: A string containing all arguments in a url-encoded format\n        \"\"\"\n\n        if len(self.searchterms) == 0:\n            raise TwitterSearchException(1015)\n\n        url = '?q='\n        url += '+'.join([quote_plus(i) for i in self.searchterms])\n\n        if self.attitude_filter is not None:\n            url += '+%s' % quote_plus(self._attitudes[0 if self.attitude_filter else 1])\n\n        if self.source_filter:\n            url += '+%s' % quote_plus(self._source + self.source_filter)\n\n        if self.link_filter:\n            url += '+%s' % quote_plus(self._link)\n\n        if self.question_filter:\n            url += '+%s' % quote_plus(self._question)\n\n        for key, value in self.arguments.items():\n            url += '&%s=%s' % (quote_plus(key), (quote_plus(value)\n                                                 if key != 'geocode'\n                                                 else value))\n\n        self.url = url\n        return self.url"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_language(self, lang):\n\n        if lang in self.iso_6391:\n            self.arguments.update({'lang': '%s' % lang})\n        else:\n            raise TwitterSearchException(1002)", "response": "Sets the lang parameter used to only fetch tweets within the specified language"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_locale(self, lang):\n\n        if lang in self.iso_6391:\n            self.arguments.update({'locale': '%s' % lang})\n        else:\n            raise TwitterSearchException(1002)", "response": "Sets the locale parameter of the current request"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the result_type parameter to specify what type of search results you would prefer to receive.", "response": "def set_result_type(self, result_type):\n        \"\"\" Sets 'result_type' parameter to specify what type of search \\\n        results you would prefer to receive. The current default is \u201cmixed.\u201d \\\n        Valid values include: \\\n            - mixed: Include both popular and real time results \\\n            - recent: return only the most recent results \\\n            - popular: return only the most popular results \\\n\n        :param result_type: A string containing one of \\\n        the three valid result types\n\n        :raises: TwitterSearchException\n        \"\"\"\n\n        result_type = result_type.lower()\n        if result_type in ['mixed', 'recent', 'popular']:\n            self.arguments.update({'result_type': '%s' % result_type})\n        else:\n            raise TwitterSearchException(1003)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the geocode parameters to return only tweets by users.", "response": "def set_geocode(self, latitude, longitude, radius, imperial_metric=True):\n        \"\"\" Sets geolocation parameters to return only tweets by users \\\n        located within a given radius of the given latitude/longitude. \\\n        The location is preferentially taking from the Geotagging API, \\\n        but will fall back to their Twitter profile.\n\n        :param latitude: A integer or long describing the latitude\n        :param longitude: A integer or long describing the longitude\n        :param radius: A integer or long describing the radius\n        :param imperial_metric: Whether the radius is given in metric \\\n        (kilometers) or imperial (miles) system. \\\n        Default is ``True`` which relates to usage of the \\\n        imperial kilometer metric\n        :raises: TwitterSearchException\n\n        \"\"\"\n\n        if not isinstance(radius, int if py3k else (int, long)) or radius <= 0:\n            raise TwitterSearchException(1004)\n\n        if isinstance(latitude, float) and isinstance(longitude, float):\n            if isinstance(imperial_metric, bool):\n                self.arguments.update({'geocode': '%s,%s,%s%s' % (latitude,\n                                                                  longitude,\n                                                                  radius,\n                                                                  'km'\n                                                                  if imperial_metric\n                                                                  else 'mi')})\n            else:\n                raise TwitterSearchException(1005)\n        else:\n            raise TwitterSearchException(1004)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_callback(self, func):\n\n        if isinstance(func, str if py3k else basestring) and func:\n            self.arguments.update({'callback': '%s' % func})\n        else:\n            raise TwitterSearchException(1006)", "response": "Sets the callback parameter of the response \\\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_until(self, date):\n\n        if isinstance(date, datetime.date) and date <= datetime.date.today():\n            self.arguments.update({'until': '%s' % date.strftime('%Y-%m-%d')})\n        else:\n            raise TwitterSearchException(1007)", "response": "Sets the until parameter used to return only tweets generated before the given date."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_proxy(self, proxy):\n\n        if isinstance(proxy, str if py3k else basestring):\n            self.__proxy = proxy\n        else:\n            raise TwitterSearchException(1009)", "response": "Sets a HTTPS proxy to query the Twitter API"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef authenticate(self, verify=True):\n\n        self.__oauth = OAuth1(self.__consumer_key,\n                              client_secret=self.__consumer_secret,\n                              resource_owner_key=self.__access_token,\n                              resource_owner_secret=self.__access_token_secret)\n\n        if verify:\n            r = requests.get(self._base_url + self._verify_url,\n                             auth=self.__oauth,\n                             proxies={\"https\": self.__proxy})\n            self.check_http_status(r.status_code)", "response": "Creates an authenticated and internal oauth2 handler needed for the user to access Twitter."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef check_http_status(self, http_status):\n\n        if http_status in self.exceptions:\n            raise TwitterSearchException(http_status,\n                                         self.exceptions[http_status])", "response": "Checks if given HTTP status code is within the list at the specified \\\n         \\ TwitterSearch. exceptions and raises a TwitterSearchException if it is not."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef search_tweets_iterable(self, order, callback=None):\n\n        if callback:\n            if not callable(callback):\n                raise TwitterSearchException(1018)\n            self.__callback = callback\n\n        self.search_tweets(order)\n        return self", "response": "Returns itself and queries the Twitter API."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the minimal tweet ID of the current response", "response": "def get_minimal_id(self):\n        \"\"\" Returns the minimal tweet ID of the current response\n\n        :returns: minimal tweet identification number\n        :raises: TwitterSearchException\n        \"\"\"\n\n        if not self.__response:\n            raise TwitterSearchException(1013)\n\n        return min(\n            self.__response['content']['statuses'] if self.__order_is_search\n            else self.__response['content'],\n            key=lambda i: i['id']\n            )['id'] - 1"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nqueries the Twitter API with a given query string and stores the results internally.", "response": "def send_search(self, url):\n        \"\"\" Queries the Twitter API with a given query string and \\\n        stores the results internally. Also validates returned HTTP status \\\n        code and throws an exception in case of invalid HTTP states. \\\n        Example usage ``sendSearch('?q=One+Two&count=100')``\n\n        :param url: A string of the URL to send the query to\n        :raises: TwitterSearchException\n        \"\"\"\n\n        if not isinstance(url, str if py3k else basestring):\n            raise TwitterSearchException(1009)\n\n        endpoint = self._base_url + (self._search_url\n                                     if self.__order_is_search\n                                     else self._user_url)\n\n        r = requests.get(endpoint + url,\n                         auth=self.__oauth,\n                         proxies={\"https\": self.__proxy})\n\n        self.__response['meta'] = r.headers\n\n        self.check_http_status(r.status_code)\n\n        self.__response['content'] = r.json()\n\n        # update statistics if everything worked fine so far\n        seen_tweets = self.get_amount_of_tweets()\n        self.__statistics[0] += 1\n        self.__statistics[1] += seen_tweets\n\n        # call callback if available\n        if self.__callback:\n            self.__callback(self)\n\n        # if we've seen the correct amount of tweets there may be some more\n        # using IDs to request more results\n        # (former versions used page parameter)\n        # see https://dev.twitter.com/docs/working-with-timelines\n\n        # a leading ? char does \"confuse\" parse_qs()\n        if url[0] == '?':\n            url = url[1:]\n        given_count = int(parse_qs(url)['count'][0])\n\n        # Search API does have valid count values\n        if self.__order_is_search and seen_tweets == given_count:\n            self.__next_max_id = self.get_minimal_id()\n\n        # Timelines doesn't have valid count values\n        # see: https://dev.twitter.com/docs/faq\n        # see section: \"How do I properly navigate a timeline?\"\n        elif (not self.__order_is_search and\n              len(self.__response['content']) > 0):\n            self.__next_max_id = self.get_minimal_id()\n\n        else:  # we got less tweets than requested -> no more results in API\n            self.__next_max_id = None\n\n        return self.__response['meta'], self.__response['content']"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate an query string through a given TwitterSearchOrder \\ instance and takes care that it is send to the Twitter API. \\ This method queries the Twitter API **without** iterating or \\ reloading of further results and returns response. \\ See `Advanced usage <advanced_usage.html>`_ for example :param order: A TwitterOrder instance. \\ Can be either TwitterSearchOrder or TwitterUserOrder :returns: Unmodified response as ``dict``. :raises: TwitterSearchException", "response": "def search_tweets(self, order):\n        \"\"\" Creates an query string through a given TwitterSearchOrder \\\n        instance and takes care that it is send to the Twitter API. \\\n        This method queries the Twitter API **without** iterating or \\\n        reloading of further results and returns response. \\\n        See `Advanced usage <advanced_usage.html>`_ for example\n\n        :param order: A TwitterOrder instance. \\\n        Can be either TwitterSearchOrder or TwitterUserOrder\n        :returns: Unmodified response as ``dict``.\n        :raises: TwitterSearchException\n        \"\"\"\n\n        if isinstance(order, TwitterUserOrder):\n            self.__order_is_search = False\n        elif isinstance(order, TwitterSearchOrder):\n            self.__order_is_search = True\n        else:\n            raise TwitterSearchException(1018)\n\n        self._start_url = order.create_search_url()\n        self.send_search(self._start_url)\n        return self.__response"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef search_next_results(self):\n\n        if not self.__next_max_id:\n            raise TwitterSearchException(1011)\n\n        self.send_search(\n            \"%s&max_id=%i\" % (self._start_url, self.__next_max_id)\n        )\n        return True", "response": "Triggers the search for more results using the Twitter API."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the amount of tweets currently available within this instance.", "response": "def get_amount_of_tweets(self):\n        \"\"\" Returns current amount of tweets available within this instance\n\n        :returns: The amount of tweets currently available\n        :raises: TwitterSearchException\n        \"\"\"\n\n        if not self.__response:\n            raise TwitterSearchException(1013)\n\n        return (len(self.__response['content']['statuses'])\n                if self.__order_is_search\n                else len(self.__response['content']))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_supported_languages(self, order):\n\n        if not isinstance(order, TwitterSearchOrder):\n            raise TwitterSearchException(1010)\n\n        r = requests.get(self._base_url + self._lang_url,\n                         auth=self.__oauth,\n                         proxies={\"https\": self.__proxy})\n\n        self.__response['meta'] = r.headers\n        self.check_http_status(r.status_code)\n        self.__response['content'] = r.json()\n\n        order.iso_6391 = []\n        for lang in self.__response['content']:\n            order.iso_6391.append(lang['code'])", "response": "Loads currently supported languages from Twitter API and sets them in a given TwitterSearchOrder instance."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_since_id(self, twid):\n\n        if py3k:\n            if not isinstance(twid, int):\n                raise TwitterSearchException(1004)\n        else:\n            if not isinstance(twid, (int, long)):\n                raise TwitterSearchException(1004)\n\n        if twid > 0:\n            self.arguments.update({'since_id': '%s' % twid})\n        else:\n            raise TwitterSearchException(1004)", "response": "Sets the since_id parameter used to return only results \\ where the specified ID is greater than or equal to the specified ID."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_count(self, cnt):\n\n        if isinstance(cnt, int) and cnt > 0 and cnt <= 100:\n            self.arguments.update({'count': '%s' % cnt})\n        else:\n            raise TwitterSearchException(1004)", "response": "Sets the count parameter used to define the number of tweets per page. Maximum and default value is 100."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the include entities parameter to either \\ n or \\ n exclude the entities node within the results", "response": "def set_include_entities(self, include):\n        \"\"\" Sets 'include entities' parameter to either \\\n        include or exclude the entities node within the results\n\n        :param include: Boolean to trigger the 'include entities' parameter\n        :raises: TwitterSearchException\n        \"\"\"\n\n        if not isinstance(include, bool):\n            raise TwitterSearchException(1008)\n        self.arguments.update(\n            {'include_entities': 'true' if include else 'false'}\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset trim_user parameter. When set to True each tweet returned in a timeline will include only the status authors numerical ID. When set to False each tweet returned in a timeline will include only the status authors numerical ID.", "response": "def set_trim_user(self, trim):\n        \"\"\" Sets 'trim_user' parameter. When set to True, \\\n        each tweet returned in a timeline will include a \\\n        user object including only the status authors numerical ID\n\n        :param trim: Boolean triggering the usage of the parameter\n        :raises: TwitterSearchException\n        \"\"\"\n\n        if not isinstance(trim, bool):\n            raise TwitterSearchException(1008)\n        self.arguments.update({'trim_user': 'true' if trim else 'false'})"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets include_rts parameter. When set to False the timeline will strip native retweets from the returned timeline.", "response": "def set_include_rts(self, rts):\n        \"\"\" Sets 'include_rts' parameter. When set to False, \\\n        the timeline will strip any native retweets from the returned timeline\n\n        :param rts: Boolean triggering the usage of the parameter\n        :raises: TwitterSearchException\n        \"\"\"\n\n        if not isinstance(rts, bool):\n            raise TwitterSearchException(1008)\n        self.arguments.update({'include_rts': 'true' if rts else 'false'})"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the exclude_replies parameter used to \\ prevent replies from appearing in the returned timeline .", "response": "def set_exclude_replies(self, exclude):\n        \"\"\" Sets 'exclude_replies' parameter used to \\\n        prevent replies from appearing in the returned timeline\n\n        :param exclude: Boolean triggering the usage of the parameter\n        :raises: TwitterSearchException\n        \"\"\"\n\n        if not isinstance(exclude, bool):\n            raise TwitterSearchException(1008)\n        self.arguments.update({'exclude_replies': 'true'\n                                                  if exclude\n                                                  else 'false'})"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_search_url(self):\n\n        url = '?'\n        for key, value in self.arguments.items():\n            url += '%s=%s&' % (quote_plus(key), quote_plus(value))\n        self.url = url[:-1]\n        return self.url", "response": "Generates a url - encoded query string from stored key - values tuples"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_search_url(self, url):\n\n        if url[0] == '?':\n            url = url[1:]\n\n        self.arguments = {}\n        for key, value in parse_qs(url).items():\n            self.arguments.update({key: unquote(value[0])})", "response": "Reads given query string and stores key - value tuples in self. arguments"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nresolve a URL in a template using subdomain - based URL resolution.", "response": "def url(context, view, subdomain=UNSET, *args, **kwargs):\n    \"\"\"\n    Resolves a URL in a template, using subdomain-based URL resolution.\n\n    If no subdomain is provided and a ``request`` is in the template context\n    when rendering, the URL will be resolved relative to the current request's\n    subdomain. If no ``request`` is provided, the URL will be resolved relative\n    to current domain with the ``settings.ROOT_URLCONF``.\n\n    Usage::\n\n        {% load subdomainurls %}\n        {% url 'view-name' subdomain='subdomain' %}\n\n    .. note:: This tag uses the variable URL syntax introduced in Django\n       1.3 as ``{% load url from future %}`` and was made the standard in Django\n       1.5. If you are upgrading a legacy application from one of the previous\n       template tag formats, make sure to quote your constant string URL names\n       to avoid :exc:`~django.core.urlresolver.NoReverseMatch` errors during\n       template rendering.\n\n    \"\"\"\n    if subdomain is UNSET:\n        request = context.get('request')\n        if request is not None:\n            subdomain = getattr(request, 'subdomain', None)\n        else:\n            subdomain = None\n    elif subdomain is '':\n        subdomain = None\n\n    return reverse(view, subdomain=subdomain, args=args, kwargs=kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\njoins a domain path and scheme part together returning a full URL.", "response": "def urljoin(domain, path=None, scheme=None):\n    \"\"\"\n    Joins a domain, path and scheme part together, returning a full URL.\n\n    :param domain: the domain, e.g. ``example.com``\n    :param path: the path part of the URL, e.g. ``/example/``\n    :param scheme: the scheme part of the URL, e.g. ``http``, defaulting to the\n        value of ``settings.DEFAULT_URL_SCHEME``\n    :returns: a full URL\n    \"\"\"\n    if scheme is None:\n        scheme = getattr(settings, 'DEFAULT_URL_SCHEME', 'http')\n\n    return urlunparse((scheme, domain, path or '', None, None, None))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreversing a URL from the given parameters in a similar fashion to the base URL.", "response": "def reverse(viewname, subdomain=None, scheme=None, args=None, kwargs=None,\n        current_app=None):\n    \"\"\"\n    Reverses a URL from the given parameters, in a similar fashion to\n    :meth:`django.core.urlresolvers.reverse`.\n\n    :param viewname: the name of URL\n    :param subdomain: the subdomain to use for URL reversing\n    :param scheme: the scheme to use when generating the full URL\n    :param args: positional arguments used for URL reversing\n    :param kwargs: named arguments used for URL reversing\n    :param current_app: hint for the currently executing application\n    \"\"\"\n    urlconf = settings.SUBDOMAIN_URLCONFS.get(subdomain, settings.ROOT_URLCONF)\n\n    domain = get_domain()\n    if subdomain is not None:\n        domain = '%s.%s' % (subdomain, domain)\n\n    path = simple_reverse(viewname, urlconf=urlconf, args=args, kwargs=kwargs,\n        current_app=current_app)\n    return urljoin(domain, path, scheme=scheme)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef process_request(self, request):\n        domain, host = map(lower,\n            (self.get_domain_for_request(request), request.get_host()))\n\n        pattern = r'^(?:(?P<subdomain>.*?)\\.)?%s(?::.*)?$' % re.escape(domain)\n        matches = re.match(pattern, host)\n\n        if matches:\n            request.subdomain = matches.group('subdomain')\n        else:\n            request.subdomain = None\n            logger.warning('The host %s does not belong to the domain %s, '\n                'unable to identify the subdomain for this request',\n                request.get_host(), domain)", "response": "Processes the request and sets the subdomain attribute of the request."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the current request s urlconf attribute to the urlconf associated with the subdomain if it is listed in TracingEnabled settings. SUBDOMAIN_URLCONFS.", "response": "def process_request(self, request):\n        \"\"\"\n        Sets the current request's ``urlconf`` attribute to the urlconf\n        associated with the subdomain, if it is listed in\n        ``settings.SUBDOMAIN_URLCONFS``.\n        \"\"\"\n        super(SubdomainURLRoutingMiddleware, self).process_request(request)\n\n        subdomain = getattr(request, 'subdomain', UNSET)\n\n        if subdomain is not UNSET:\n            urlconf = settings.SUBDOMAIN_URLCONFS.get(subdomain)\n            if urlconf is not None:\n                logger.debug(\"Using urlconf %s for subdomain: %s\",\n                    repr(urlconf), repr(subdomain))\n                request.urlconf = urlconf"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd the HTTP Vary header to the response to avoid having responses cached across subdomains.", "response": "def process_response(self, request, response):\n        \"\"\"\n        Forces the HTTP ``Vary`` header onto requests to avoid having responses\n        cached across subdomains.\n        \"\"\"\n        if getattr(settings, 'FORCE_VARY_ON_HOST', True):\n            patch_vary_headers(response, ('Host',))\n\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprocess the docstring for a given python object.", "response": "def process_docstring(app, what, name, obj, options, lines):\n    \"\"\"\n    Process the docstring for a given python object.\n    Note that the list 'lines' is changed in this function. Sphinx\n    uses the altered content of the list.\n    \"\"\"\n    result = [re.sub(r'U\\{([^}]*)\\}', r'\\1',\n                     re.sub(r'(L|C)\\{([^}]*)\\}', r':py:obj:`\\2`',\n                            re.sub(r'@(' + '|'.join(FIELDS) + r')', r':\\1',\n                                   l)))\n              for l in lines]\n    lines[:] = result[:]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a dictionary mapping pairs of constrained variables to arcs.", "response": "def getArcs(domains, constraints):\n    \"\"\"\n    Return a dictionary mapping pairs (arcs) of constrained variables\n\n    @attention: Currently unused.\n    \"\"\"\n    arcs = {}\n    for x in constraints:\n        constraint, variables = x\n        if len(variables) == 2:\n            variable1, variable2 = variables\n            arcs.setdefault(variable1, {}).setdefault(variable2, []).append(x)\n            arcs.setdefault(variable2, {}).setdefault(variable1, []).append(x)\n    return arcs"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nperforming the ARC - 8 arc checking algorithm and prune domains", "response": "def doArc8(arcs, domains, assignments):\n    \"\"\"\n    Perform the ARC-8 arc checking algorithm and prune domains\n\n    @attention: Currently unused.\n    \"\"\"\n    check = dict.fromkeys(domains, True)\n    while check:\n        variable, _ = check.popitem()\n        if variable not in arcs or variable in assignments:\n            continue\n        domain = domains[variable]\n        arcsvariable = arcs[variable]\n        for othervariable in arcsvariable:\n            arcconstraints = arcsvariable[othervariable]\n            if othervariable in assignments:\n                otherdomain = [assignments[othervariable]]\n            else:\n                otherdomain = domains[othervariable]\n            if domain:\n                # changed = False\n                for value in domain[:]:\n                    assignments[variable] = value\n                    if otherdomain:\n                        for othervalue in otherdomain:\n                            assignments[othervariable] = othervalue\n                            for constraint, variables in arcconstraints:\n                                if not constraint(\n                                    variables, domains, assignments, True\n                                ):\n                                    break\n                            else:\n                                # All constraints passed. Value is safe.\n                                break\n                        else:\n                            # All othervalues failed. Kill value.\n                            domain.hideValue(value)\n                            # changed = True\n                        del assignments[othervariable]\n                del assignments[variable]\n                # if changed:\n                #     check.update(dict.fromkeys(arcsvariable))\n            if not domain:\n                return False\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef addVariable(self, variable, domain):\n        if variable in self._variables:\n            msg = \"Tried to insert duplicated variable %s\" % repr(variable)\n            raise ValueError(msg)\n        if isinstance(domain, Domain):\n            domain = copy.deepcopy(domain)\n        elif hasattr(domain, \"__getitem__\"):\n            domain = Domain(domain)\n        else:\n            msg = \"Domains must be instances of subclasses of the Domain class\"\n            raise TypeError(msg)\n        if not domain:\n            raise ValueError(\"Domain is empty\")\n        self._variables[variable] = domain", "response": "Add a variable to the problem."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd one or more variables to the problem", "response": "def addVariables(self, variables, domain):\n        \"\"\"\n        Add one or more variables to the problem\n\n        Example:\n\n        >>> problem = Problem()\n        >>> problem.addVariables([\"a\", \"b\"], [1, 2, 3])\n        >>> solutions = problem.getSolutions()\n        >>> len(solutions)\n        9\n        >>> {'a': 3, 'b': 1} in solutions\n        True\n\n        @param variables: Any object containing a sequence of objects\n                          represeting problem variables\n        @type  variables: sequence of hashable objects\n        @param domain: Set of items defining the possible values that\n                       the given variables may assume\n        @type  domain: list, tuple, or instance of C{Domain}\n        \"\"\"\n        for variable in variables:\n            self.addVariable(variable, domain)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a constraint to the problem.", "response": "def addConstraint(self, constraint, variables=None):\n        \"\"\"\n        Add a constraint to the problem\n\n        Example:\n\n        >>> problem = Problem()\n        >>> problem.addVariables([\"a\", \"b\"], [1, 2, 3])\n        >>> problem.addConstraint(lambda a, b: b == a+1, [\"a\", \"b\"])\n        >>> solutions = problem.getSolutions()\n        >>>\n\n        @param constraint: Constraint to be included in the problem\n        @type  constraint: instance a L{Constraint} subclass or a\n                           function to be wrapped by L{FunctionConstraint}\n        @param variables: Variables affected by the constraint (default to\n                          all variables). Depending on the constraint type\n                          the order may be important.\n        @type  variables: set or sequence of variables\n        \"\"\"\n        if not isinstance(constraint, Constraint):\n            if callable(constraint):\n                constraint = FunctionConstraint(constraint)\n            else:\n                msg = \"Constraints must be instances of subclasses \" \"of the Constraint class\"\n                raise ValueError(msg)\n        self._constraints.append((constraint, variables))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getSolution(self):\n        domains, constraints, vconstraints = self._getArgs()\n        if not domains:\n            return None\n        return self._solver.getSolution(domains, constraints, vconstraints)", "response": "Find and return a solution to the problem\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfinds and return all solutions for the current problem", "response": "def getSolutions(self):\n        \"\"\"\n        Find and return all solutions to the problem\n\n        Example:\n\n        >>> problem = Problem()\n        >>> problem.getSolutions() == []\n        True\n        >>> problem.addVariables([\"a\"], [42])\n        >>> problem.getSolutions()\n        [{'a': 42}]\n\n        @return: All solutions for the problem\n        @rtype: list of dictionaries mapping variables to values\n        \"\"\"\n        domains, constraints, vconstraints = self._getArgs()\n        if not domains:\n            return []\n        return self._solver.getSolutions(domains, constraints, vconstraints)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning an iterator to the solutions of the problem.", "response": "def getSolutionIter(self):\n        \"\"\"\n        Return an iterator to the solutions of the problem\n\n        Example:\n\n        >>> problem = Problem()\n        >>> list(problem.getSolutionIter()) == []\n        True\n        >>> problem.addVariables([\"a\"], [42])\n        >>> iter = problem.getSolutionIter()\n        >>> next(iter)\n        {'a': 42}\n        >>> next(iter)\n        Traceback (most recent call last):\n          File \"<stdin>\", line 1, in ?\n        StopIteration\n        \"\"\"\n        domains, constraints, vconstraints = self._getArgs()\n        if not domains:\n            return iter(())\n        return self._solver.getSolutionIter(domains, constraints, vconstraints)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn one solution for the given problem", "response": "def getSolution(self, domains, constraints, vconstraints):\n        \"\"\"\n        Return one solution for the given problem\n\n        @param domains: Dictionary mapping variables to their domains\n        @type  domains: dict\n        @param constraints: List of pairs of (constraint, variables)\n        @type  constraints: list\n        @param vconstraints: Dictionary mapping variables to a list of\n                             constraints affecting the given variables.\n        @type  vconstraints: dict\n        \"\"\"\n        msg = \"%s is an abstract class\" % self.__class__.__name__\n        raise NotImplementedError(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nresetting the state of the current instance to the original state.", "response": "def resetState(self):\n        \"\"\"\n        Reset to the original domain state, including all possible values\n        \"\"\"\n        self.extend(self._hidden)\n        del self._hidden[:]\n        del self._states[:]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrestore domain state from the top of the stack.", "response": "def popState(self):\n        \"\"\"\n        Restore domain state from the top of the stack\n\n        Variables hidden since the last popped state are then available\n        again.\n        \"\"\"\n        diff = self._states.pop() - len(self)\n        if diff:\n            self.extend(self._hidden[-diff:])\n            del self._hidden[-diff:]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef hideValue(self, value):\n        list.remove(self, value)\n        self._hidden.append(value)", "response": "Hide the given value from the domain\n        list."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef block_view(request):\n    blocked_ip_list = get_blocked_ips()\n    blocked_username_list = get_blocked_usernames()\n\n    context = {'blocked_ip_list': blocked_ip_list,\n               'blocked_username_list': blocked_username_list}\n    return render(request, 'defender/admin/blocks.html', context)", "response": "List the blocked IP and Usernames"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks Validity of an IP address", "response": "def is_valid_ip(ip_address):\n    \"\"\" Check Validity of an IP address \"\"\"\n    if not ip_address:\n        return False\n    ip_address = ip_address.strip()\n    try:\n        validate_ipv46_address(ip_address)\n        return True\n    except ValidationError:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_ip_address_from_request(request):\n    remote_addr = request.META.get('REMOTE_ADDR', '')\n    if remote_addr and is_valid_ip(remote_addr):\n        return remote_addr.strip()\n    return '127.0.0.1'", "response": "Makes a best attempt to get the client s real IP or return\n        the loopback"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the ip address from the request", "response": "def get_ip(request):\n    \"\"\" get the ip address from the request \"\"\"\n    if config.BEHIND_REVERSE_PROXY:\n        ip_address = request.META.get(config.REVERSE_PROXY_HEADER, '')\n        ip_address = ip_address.split(\",\", 1)[0].strip()\n        if ip_address == '':\n            ip_address = get_ip_address_from_request(request)\n    else:\n        ip_address = get_ip_address_from_request(request)\n    return ip_address"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_blocked_ips():\n    if config.DISABLE_IP_LOCKOUT:\n        # There are no blocked IP's since we disabled them.\n        return []\n    key = get_ip_blocked_cache_key(\"*\")\n    key_list = [redis_key.decode('utf-8')\n                for redis_key in REDIS_SERVER.keys(key)]\n    return strip_keys(key_list)", "response": "get a list of blocked ips from redis"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_blocked_usernames():\n    if config.DISABLE_USERNAME_LOCKOUT:\n        # There are no blocked usernames since we disabled them.\n        return []\n    key = get_username_blocked_cache_key(\"*\")\n    key_list = [redis_key.decode('utf-8')\n                for redis_key in REDIS_SERVER.keys(key)]\n    return strip_keys(key_list)", "response": "get a list of blocked usernames from redis"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef increment_key(key):\n    pipe = REDIS_SERVER.pipeline()\n    pipe.incr(key, 1)\n    if config.COOLOFF_TIME:\n        pipe.expire(key, config.COOLOFF_TIME)\n    new_value = pipe.execute()[0]\n    return new_value", "response": "given a key increments the value and returns it"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef username_from_request(request):\n    if config.USERNAME_FORM_FIELD in request.POST:\n        return request.POST[config.USERNAME_FORM_FIELD][:255]\n    return None", "response": "unloads username from default POST request"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_user_attempts(request, get_username=get_username_from_request, username=None):\n    ip_address = get_ip(request)\n\n    username = lower_username(username or get_username(request))\n\n    # get by IP\n    ip_count = REDIS_SERVER.get(get_ip_attempt_cache_key(ip_address))\n    if not ip_count:\n        ip_count = 0\n    ip_count = int(ip_count)\n\n    # get by username\n    username_count = REDIS_SERVER.get(get_username_attempt_cache_key(username))\n    if not username_count:\n        username_count = 0\n    username_count = int(username_count)\n\n    # return the larger of the two.\n    return max(ip_count, username_count)", "response": "Returns the number of access attempts for this ip username"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef block_ip(ip_address):\n    if not ip_address:\n        # no reason to continue when there is no ip\n        return\n    if config.DISABLE_IP_LOCKOUT:\n        # no need to block, we disabled it.\n        return\n    key = get_ip_blocked_cache_key(ip_address)\n    if config.COOLOFF_TIME:\n        REDIS_SERVER.set(key, 'blocked', config.COOLOFF_TIME)\n    else:\n        REDIS_SERVER.set(key, 'blocked')\n    send_ip_block_signal(ip_address)", "response": "given the ip block it"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef block_username(username):\n    if not username:\n        # no reason to continue when there is no username\n        return\n    if config.DISABLE_USERNAME_LOCKOUT:\n        # no need to block, we disabled it.\n        return\n    key = get_username_blocked_cache_key(username)\n    if config.COOLOFF_TIME:\n        REDIS_SERVER.set(key, 'blocked', config.COOLOFF_TIME)\n    else:\n        REDIS_SERVER.set(key, 'blocked')\n    send_username_block_signal(username)", "response": "block the given username"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nrecords the failed login attempt", "response": "def record_failed_attempt(ip_address, username):\n    \"\"\" record the failed login attempt, if over limit return False,\n    if not over limit return True \"\"\"\n    # increment the failed count, and get current number\n    ip_block = False\n    if not config.DISABLE_IP_LOCKOUT:\n        # we only want to increment the IP if this is disabled.\n        ip_count = increment_key(get_ip_attempt_cache_key(ip_address))\n        # if over the limit, add to block\n        if ip_count > config.IP_FAILURE_LIMIT:\n            block_ip(ip_address)\n            ip_block = True\n\n    user_block = False\n    if username and not config.DISABLE_USERNAME_LOCKOUT:\n        user_count = increment_key(get_username_attempt_cache_key(username))\n        # if over the limit, add to block\n        if user_count > config.USERNAME_FAILURE_LIMIT:\n            block_username(username)\n            user_block = True\n\n    # if we have this turned on, then there is no reason to look at ip_block\n    # we will just look at user_block, and short circut the result since\n    # we don't need to continue.\n    if config.DISABLE_IP_LOCKOUT:\n        # if user_block is True, it means it was blocked\n        # we need to return False\n        return not user_block\n\n    if config.DISABLE_USERNAME_LOCKOUT:\n        # The same as DISABLE_IP_LOCKOUT\n        return not ip_block\n\n    # we want to make sure both the IP and user is blocked before we\n    # return False\n    # this is mostly used when a lot of your users are using proxies,\n    # and you don't want one user to block everyone on that one IP.\n    if config.LOCKOUT_BY_IP_USERNAME:\n        # both ip_block and user_block need to be True in order\n        # to return a False.\n        return not (ip_block and user_block)\n\n    # if any blocks return False, no blocks. return True\n    return not (ip_block or user_block)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef unblock_ip(ip_address, pipe=None):\n    do_commit = False\n    if not pipe:\n        pipe = REDIS_SERVER.pipeline()\n        do_commit = True\n    if ip_address:\n        pipe.delete(get_ip_attempt_cache_key(ip_address))\n        pipe.delete(get_ip_blocked_cache_key(ip_address))\n        if do_commit:\n            pipe.execute()", "response": "unblock the given IP"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef unblock_username(username, pipe=None):\n    do_commit = False\n    if not pipe:\n        pipe = REDIS_SERVER.pipeline()\n        do_commit = True\n    if username:\n        pipe.delete(get_username_attempt_cache_key(username))\n        pipe.delete(get_username_blocked_cache_key(username))\n        if do_commit:\n            pipe.execute()", "response": "unblock the given Username"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreset the failed attempts for these ip s and usernames.", "response": "def reset_failed_attempts(ip_address=None, username=None):\n    \"\"\" reset the failed attempts for these ip's and usernames\n    \"\"\"\n    pipe = REDIS_SERVER.pipeline()\n\n    unblock_ip(ip_address, pipe=pipe)\n    unblock_username(username, pipe=pipe)\n\n    pipe.execute()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_user_already_locked(username):\n    if username is None:\n        return False\n    if config.DISABLE_USERNAME_LOCKOUT:\n        return False\n    return REDIS_SERVER.get(get_username_blocked_cache_key(username))", "response": "Is this username already locked?"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nbeing this IP already locked?", "response": "def is_source_ip_already_locked(ip_address):\n    \"\"\"Is this IP already locked?\"\"\"\n    if ip_address is None:\n        return False\n    if config.DISABLE_IP_LOCKOUT:\n        return False\n    return REDIS_SERVER.get(get_ip_blocked_cache_key(ip_address))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse the username & IP from the request and see if it s already locked.", "response": "def is_already_locked(request, get_username=get_username_from_request, username=None):\n    \"\"\"Parse the username & IP from the request, and see if it's\n    already locked.\"\"\"\n    user_blocked = is_user_already_locked(username or get_username(request))\n    ip_blocked = is_source_ip_already_locked(get_ip(request))\n\n    if config.LOCKOUT_BY_IP_USERNAME:\n        # if both this IP and this username are present the request is blocked\n        return ip_blocked and user_blocked\n\n    return ip_blocked or user_blocked"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check_request(request, login_unsuccessful,\n                  get_username=get_username_from_request,\n                  username=None):\n    \"\"\" check the request, and process results\"\"\"\n    ip_address = get_ip(request)\n    username = username or get_username(request)\n\n    if not login_unsuccessful:\n        # user logged in -- forget the failed attempts\n        reset_failed_attempts(ip_address=ip_address, username=username)\n        return True\n    else:\n        # add a failed attempt for this user\n        return record_failed_attempt(ip_address, username)", "response": "check the request and process results"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a login attempt to the database.", "response": "def add_login_attempt_to_db(request, login_valid,\n                            get_username=get_username_from_request,\n                            username=None):\n    \"\"\" Create a record for the login attempt If using celery call celery\n    task, if not, call the method normally \"\"\"\n\n    if not config.STORE_ACCESS_ATTEMPTS:\n        # If we don't want to store in the database, then don't proceed.\n        return\n\n    username = username or get_username(request)\n\n    user_agent = request.META.get('HTTP_USER_AGENT', '<unknown>')[:255]\n    ip_address = get_ip(request)\n    http_accept = request.META.get('HTTP_ACCEPT', '<unknown>')\n    path_info = request.META.get('PATH_INFO', '<unknown>')\n\n    if config.USE_CELERY:\n        from .tasks import add_login_attempt_task\n        add_login_attempt_task.delay(user_agent, ip_address, username,\n                                     http_accept, path_info, login_valid)\n    else:\n        store_login_attempt(user_agent, ip_address, username,\n                            http_accept, path_info, login_valid)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_login_attempt_task(user_agent, ip_address, username,\n                           http_accept, path_info, login_valid):\n    \"\"\" Create a record for the login attempt \"\"\"\n    store_login_attempt(user_agent, ip_address, username,\n                        http_accept, path_info, login_valid)", "response": "Add a login attempt record to the log."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nuses to decorate the django.contrib.admin.site.login method or any other function you want to protect by brute forcing. To make it work on normal functions just pass the status code that should indicate a failure and/or a string that will be checked within the response body.", "response": "def watch_login(status_code=302, msg='',\n                get_username=utils.get_username_from_request):\n    \"\"\"\n    Used to decorate the django.contrib.admin.site.login method or\n    any other function you want to protect by brute forcing.\n    To make it work on normal functions just pass the status code that should\n    indicate a failure and/or a string that will be checked within the\n    response body.\n    \"\"\"\n    def decorated_login(func):\n        @functools.wraps(func)\n        def wrapper(request, *args, **kwargs):\n            # if the request is currently under lockout, do not proceed to the\n            # login function, go directly to lockout url, do not pass go,\n            # do not collect messages about this login attempt\n            if utils.is_already_locked(request):\n                return utils.lockout_response(request)\n\n            # call the login function\n            response = func(request, *args, **kwargs)\n\n            if request.method == 'POST':\n                # see if the login was successful\n                if status_code == 302:  # standard Django login view\n                    login_unsuccessful = (\n                        response and\n                        not response.has_header('location') and\n                        response.status_code != status_code\n                    )\n                else:\n                    # If msg is not passed the last condition will be evaluated\n                    # always to True so the first 2 will decide the result.\n                    login_unsuccessful = (\n                        response and response.status_code == status_code\n                        and msg in response.content.decode('utf-8')\n                    )\n\n                # ideally make this background task, but to keep simple,\n                # keeping it inline for now.\n                utils.add_login_attempt_to_db(request, not login_unsuccessful,\n                                              get_username)\n\n                if utils.check_request(request, login_unsuccessful,\n                                       get_username):\n                    return response\n\n                return utils.lockout_response(request)\n\n            return response\n\n        return wrapper\n    return decorated_login"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef store_login_attempt(user_agent, ip_address, username,\n                        http_accept, path_info, login_valid):\n    \"\"\" Store the login attempt to the db. \"\"\"\n    AccessAttempt.objects.create(\n        user_agent=user_agent,\n        ip_address=ip_address,\n        username=username,\n        http_accept=http_accept,\n        path_info=path_info,\n        login_valid=login_valid,\n    )", "response": "Store the login attempt in the database."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_redis_connection():\n    if config.MOCK_REDIS:  # pragma: no cover\n        import mockredis\n        return mockredis.mock_strict_redis_client()  # pragma: no cover\n    elif config.DEFENDER_REDIS_NAME:  # pragma: no cover\n        try:\n            cache = caches[config.DEFENDER_REDIS_NAME]\n        except InvalidCacheBackendError:\n            raise KeyError(INVALID_CACHE_ERROR_MSG.format(\n                config.DEFENDER_REDIS_NAME))\n        # every redis backend implement it own way to get the low level client\n        try:\n            # redis_cache.RedisCache case (django-redis-cache package)\n            return cache.get_master_client()\n        except AttributeError:\n            # django_redis.cache.RedisCache case (django-redis package)\n            return cache.client.get_client(True)\n    else:  # pragma: no cover\n        redis_config = parse_redis_url(config.DEFENDER_REDIS_URL)\n        return redis.StrictRedis(\n            host=redis_config.get('HOST'),\n            port=redis_config.get('PORT'),\n            db=redis_config.get('DB'),\n            password=redis_config.get('PASSWORD'),\n            ssl=redis_config.get('SSL'))", "response": "Get the redis connection if not using mock"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing a redis URL and returns a dictionary with the keys DB password HOST PORT and SSL set to True.", "response": "def parse_redis_url(url):\n    \"\"\"Parses a redis URL.\"\"\"\n\n    # create config with some sane defaults\n    redis_config = {\n        \"DB\": 0,\n        \"PASSWORD\": None,\n        \"HOST\": \"localhost\",\n        \"PORT\": 6379,\n        \"SSL\": False\n    }\n\n    if not url:\n        return redis_config\n\n    url = urlparse.urlparse(url)\n    # Remove query strings.\n    path = url.path[1:]\n    path = path.split('?', 2)[0]\n\n    if path:\n        redis_config.update({\"DB\": int(path)})\n    if url.password:\n        redis_config.update({\"PASSWORD\": url.password})\n    if url.hostname:\n        redis_config.update({\"HOST\": url.hostname})\n    if url.port:\n        redis_config.update({\"PORT\": int(url.port)})\n    if url.scheme in ['https', 'rediss']:\n        redis_config.update({\"SSL\": True})\n\n    return redis_config"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef handle(self, **options):\n        print(\"Starting clean up of django-defender table\")\n        now = timezone.now()\n        cleanup_delta = timedelta(hours=config.ACCESS_ATTEMPT_EXPIRATION)\n        min_attempt_time = now - cleanup_delta\n\n        attempts_to_clean = AccessAttempt.objects.filter(\n            attempt_time__lt=min_attempt_time,\n        )\n        attempts_to_clean_count = attempts_to_clean.count()\n\n        attempts_to_clean.delete()\n\n        print(\n            \"Finished. Removed {0} AccessAttempt entries.\".format(\n                attempts_to_clean_count)\n        )", "response": "Remove any entries in the django - defender table that are older than DEFENDER_ACCESS_ATTEMPT_EXPIRATION."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting a steady cached DB - API 2 connection from the pool.", "response": "def connection(self, shareable=True):\n        \"\"\"Get a steady, cached DB-API 2 connection from the pool.\n\n        If shareable is set and the underlying DB-API 2 allows it,\n        then the connection may be shared with other threads.\n\n        \"\"\"\n        if shareable and self._maxshared:\n            self._lock.acquire()\n            try:\n                while (not self._shared_cache and self._maxconnections\n                        and self._connections >= self._maxconnections):\n                    self._wait_lock()\n                if len(self._shared_cache) < self._maxshared:\n                    # shared cache is not full, get a dedicated connection\n                    try:  # first try to get it from the idle cache\n                        con = self._idle_cache.pop(0)\n                    except IndexError:  # else get a fresh connection\n                        con = self.steady_connection()\n                    else:\n                        con._ping_check()  # check this connection\n                    con = SharedDBConnection(con)\n                    self._connections += 1\n                else:  # shared cache full or no more connections allowed\n                    self._shared_cache.sort()  # least shared connection first\n                    con = self._shared_cache.pop(0)  # get it\n                    while con.con._transaction:\n                        # do not share connections which are in a transaction\n                        self._shared_cache.insert(0, con)\n                        self._wait_lock()\n                        self._shared_cache.sort()\n                        con = self._shared_cache.pop(0)\n                    con.con._ping_check()  # check the underlying connection\n                    con.share()  # increase share of this connection\n                # put the connection (back) into the shared cache\n                self._shared_cache.append(con)\n                self._lock.notify()\n            finally:\n                self._lock.release()\n            con = PooledSharedDBConnection(self, con)\n        else:  # try to get a dedicated connection\n            self._lock.acquire()\n            try:\n                while (self._maxconnections\n                        and self._connections >= self._maxconnections):\n                    self._wait_lock()\n                # connection limit not reached, get a dedicated connection\n                try:  # first try to get it from the idle cache\n                    con = self._idle_cache.pop(0)\n                except IndexError:  # else get a fresh connection\n                    con = self.steady_connection()\n                else:\n                    con._ping_check()  # check connection\n                con = PooledDedicatedDBConnection(self, con)\n                self._connections += 1\n            finally:\n                self._lock.release()\n        return con"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef unshare(self, con):\n        self._lock.acquire()\n        try:\n            con.unshare()\n            shared = con.shared\n            if not shared:  # connection is idle,\n                try:  # so try to remove it\n                    self._shared_cache.remove(con)  # from shared cache\n                except ValueError:\n                    pass  # pool has already been closed\n        finally:\n            self._lock.release()\n        if not shared:  # connection has become idle,\n            self.cache(con.con)", "response": "Decrease the share of a connection in the shared cache."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nputting a dedicated connection back into the idle cache.", "response": "def cache(self, con):\n        \"\"\"Put a dedicated connection back into the idle cache.\"\"\"\n        self._lock.acquire()\n        try:\n            if not self._maxcached or len(self._idle_cache) < self._maxcached:\n                con._reset(force=self._reset)  # rollback possible transaction\n                # the idle cache is not full, so put it there\n                self._idle_cache.append(con)  # append it to the idle cache\n            else:  # if the idle cache is already full,\n                con.close()  # then close the connection\n            self._connections -= 1\n            self._lock.notify()\n        finally:\n            self._lock.release()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef close(self):\n        self._lock.acquire()\n        try:\n            while self._idle_cache:  # close all idle connections\n                con = self._idle_cache.pop(0)\n                try:\n                    con.close()\n                except Exception:\n                    pass\n            if self._maxshared:  # close all shared connections\n                while self._shared_cache:\n                    con = self._shared_cache.pop(0).con\n                    try:\n                        con.close()\n                    except Exception:\n                        pass\n                    self._connections -= 1\n            self._lock.notifyAll()\n        finally:\n            self._lock.release()", "response": "Close all connections in the pool."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nclosing the pooled shared connection.", "response": "def close(self):\n        \"\"\"Close the pooled shared connection.\"\"\"\n        # Instead of actually closing the connection,\n        # unshare it and/or return it to the pool.\n        if self._con:\n            self._pool.unshare(self._shared_con)\n            self._shared_con = self._con = None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget a steady non - persistent DB - API 2 connection.", "response": "def steady_connection(self):\n        \"\"\"Get a steady, non-persistent DB-API 2 connection.\"\"\"\n        return connect(\n            self._creator, self._maxusage, self._setsession,\n            self._failures, self._ping, self._closeable,\n            *self._args, **self._kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef connection(self, shareable=False):\n        try:\n            con = self.thread.connection\n        except AttributeError:\n            con = self.steady_connection()\n            if not con.threadsafety():\n                raise NotSupportedError(\"Database module is not thread-safe.\")\n            self.thread.connection = con\n        con._ping_check()\n        return con", "response": "Get a steady persistent DB - API 2 connection."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef close(self):\n        # Instead of actually closing the connection,\n        # return it to the pool so it can be reused.\n        if self._con is not None:\n            self._pool.returnConnection(self._con)\n            self._con = None", "response": "Close the pooled connection."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget a connection from the pool.", "response": "def _threadsafe_get_connection(self):\n        \"\"\"Get a connection from the pool.\"\"\"\n        self._lock.acquire()\n        try:\n            next = self._nextConnection\n            con = PooledDBConnection(self, self._connections[next])\n            next += 1\n            if next >= len(self._connections):\n                next = 0\n            self._nextConnection = next\n            return con\n        finally:\n            self._lock.release()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a steady non - persistent PyGreSQL connection.", "response": "def steady_connection(self):\n        \"\"\"Get a steady, non-persistent PyGreSQL connection.\"\"\"\n        return SteadyPgConnection(\n            self._maxusage, self._setsession, self._closeable,\n            *self._args, **self._kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef connection(self):\n        try:\n            con = self.thread.connection\n        except AttributeError:\n            con = self.steady_connection()\n            self.thread.connection = con\n        return con", "response": "Get a steady persistent PyGreSQL connection."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nclosing the pooled connection.", "response": "def close(self):\n        \"\"\"Close the pooled connection.\"\"\"\n        # Instead of actually closing the connection,\n        # return it to the pool so it can be reused.\n        if self._con is not None:\n            self._pool.cache(self._con)\n            self._con = None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate version string. For a sequence containing version information such as (2, 0, 0, 'pre'), this returns a printable string such as '2.0pre'. The micro version number is only excluded from the string if it is zero.", "response": "def versionString(version):\n    \"\"\"Create version string.\n\n    For a sequence containing version information such as (2, 0, 0, 'pre'),\n    this returns a printable string such as '2.0pre'.\n    The micro version number is only excluded from the string if it is zero.\n\n    \"\"\"\n    ver = list(map(str, version))\n    numbers, rest = ver[:2 if ver[2] == '0' else 3], ver[3:]\n    return '.'.join(numbers) + '-'.join(rest)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef steady_connection(self):\n        return SteadyPgConnection(self._maxusage, self._setsession, True,\n                                  *self._args, **self._kwargs)", "response": "Get a steady unpooled PostgreSQL connection."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef connection(self):\n        if self._connections:\n            if not self._connections.acquire(self._blocking):\n                raise TooManyConnections\n        try:\n            con = self._cache.get(0)\n        except Empty:\n            con = self.steady_connection()\n        return PooledPgConnection(self, con)", "response": "Get a steady cached PostgreSQL connection from the pool."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nput a connection back into the pool cache.", "response": "def cache(self, con):\n        \"\"\"Put a connection back into the pool cache.\"\"\"\n        try:\n            if self._reset == 2:\n                con.reset()  # reset the connection completely\n            else:\n                if self._reset or con._transaction:\n                    try:\n                        con.rollback()  # rollback a possible transaction\n                    except Exception:\n                        pass\n            self._cache.put(con, 0)  # and then put it back into the cache\n        except Full:\n            con.close()\n        if self._connections:\n            self._connections.release()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncloses all connections in the pool.", "response": "def close(self):\n        \"\"\"Close all connections in the pool.\"\"\"\n        while 1:\n            try:\n                con = self._cache.get(0)\n                try:\n                    con.close()\n                except Exception:\n                    pass\n                if self._connections:\n                    self._connections.release()\n            except Empty:\n                break"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreopening the pooled connection.", "response": "def reopen(self):\n        \"\"\"Reopen the pooled connection.\"\"\"\n        # If the connection is already back in the pool,\n        # get another connection from the pool,\n        # otherwise reopen the underlying connection.\n        if self._con:\n            self._con.reopen()\n        else:\n            self._con = self._pool.connection()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nexecute the SQL commands for session preparation.", "response": "def _setsession(self):\n        \"\"\"Execute the SQL commands for session preparation.\"\"\"\n        if self._setsession_sql:\n            for sql in self._setsession_sql:\n                self._con.query(sql)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _close(self):\n        if not self._closed:\n            try:\n                self._con.close()\n            except Exception:\n                pass\n            self._transaction = False\n            self._closed = True", "response": "Close the tough connection."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef close(self):\n        if self._closeable:\n            self._close()\n        elif self._transaction:\n            self.reset()", "response": "Closes the tough connection."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreopens the tough connection.", "response": "def reopen(self):\n        \"\"\"Reopen the tough connection.\n\n        It will not complain if the connection cannot be reopened.\n\n        \"\"\"\n        try:\n            self._con.reopen()\n        except Exception:\n            if self._transcation:\n                self._transaction = False\n                try:\n                    self._con.query('rollback')\n                except Exception:\n                    pass\n        else:\n            self._transaction = False\n            self._closed = False\n            self._setsession()\n            self._usage = 0"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef reset(self):\n        try:\n            self._con.reset()\n            self._transaction = False\n            self._setsession()\n            self._usage = 0\n        except Exception:\n            try:\n                self.reopen()\n            except Exception:\n                try:\n                    self.rollback()\n                except Exception:\n                    pass", "response": "Resets the tough connection."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncommit the current transaction.", "response": "def end(self, sql=None):\n        \"\"\"Commit the current transaction.\"\"\"\n        self._transaction = False\n        try:\n            end = self._con.end\n        except AttributeError:\n            return self._con.query(sql or 'end')\n        else:\n            if sql:\n                return end(sql=sql)\n            else:\n                return end()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef commit(self, sql=None):\n        self._transaction = False\n        try:\n            commit = self._con.commit\n        except AttributeError:\n            return self._con.query(sql or 'commit')\n        else:\n            if sql:\n                return commit(sql=sql)\n            else:\n                return commit()", "response": "Commit the current transaction."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a tough version of a connection class method.", "response": "def _get_tough_method(self, method):\n        \"\"\"Return a \"tough\" version of a connection class method.\n\n        The tough version checks whether the connection is bad (lost)\n        and automatically and transparently tries to reset the connection\n        if this is the case (for instance, the database has been restarted).\n\n        \"\"\"\n        def tough_method(*args, **kwargs):\n            transaction = self._transaction\n            if not transaction:\n                try:  # check whether connection status is bad\n                    if not self._con.db.status:\n                        raise AttributeError\n                    if self._maxusage:  # or connection used too often\n                        if self._usage >= self._maxusage:\n                            raise AttributeError\n                except Exception:\n                    self.reset()  # then reset the connection\n            try:\n                result = method(*args, **kwargs)  # try connection method\n            except Exception:  # error in query\n                if transaction:  # inside a transaction\n                    self._transaction = False\n                    raise  # propagate the error\n                elif self._con.db.status:  # if it was not a connection problem\n                    raise  # then propagate the error\n                else:  # otherwise\n                    self.reset()  # reset the connection\n                    result = method(*args, **kwargs)  # and try one more time\n            self._usage += 1\n            return result\n        return tough_method"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef connect(\n        creator, maxusage=None, setsession=None,\n        failures=None, ping=1, closeable=True, *args, **kwargs):\n    \"\"\"A tough version of the connection constructor of a DB-API 2 module.\n\n    creator: either an arbitrary function returning new DB-API 2 compliant\n        connection objects or a DB-API 2 compliant database module\n    maxusage: maximum usage limit for the underlying DB-API 2 connection\n        (number of database operations, 0 or None means unlimited usage)\n        callproc(), execute() and executemany() count as one operation.\n        When the limit is reached, the connection is automatically reset.\n    setsession: an optional list of SQL commands that may serve to prepare\n        the session, e.g. [\"set datestyle to german\", \"set time zone mez\"]\n    failures: an optional exception class or a tuple of exception classes\n        for which the failover mechanism shall be applied, if the default\n        (OperationalError, InternalError) is not adequate\n    ping: determines when the connection should be checked with ping()\n        (0 = None = never, 1 = default = when _ping_check() is called,\n        2 = whenever a cursor is created, 4 = when a query is executed,\n        7 = always, and all other bit combinations of these values)\n    closeable: if this is set to false, then closing the connection will\n        be silently ignored, but by default the connection can be closed\n    args, kwargs: the parameters that shall be passed to the creator\n        function or the connection constructor of the DB-API 2 module\n\n    \"\"\"\n    return SteadyDBConnection(\n        creator, maxusage, setsession,\n        failures, ping, closeable, *args, **kwargs)", "response": "A tough version of the connection constructor of a DB - API 2 compliant database module."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a new connection using the creator function.", "response": "def _create(self):\n        \"\"\"Create a new connection using the creator function.\"\"\"\n        con = self._creator(*self._args, **self._kwargs)\n        try:\n            try:\n                if self._dbapi.connect != self._creator:\n                    raise AttributeError\n            except AttributeError:\n                # try finding the DB-API 2 module via the connection itself\n                try:\n                    mod = con.__module__\n                except AttributeError:\n                    mod = None\n                while mod:\n                    try:\n                        self._dbapi = sys.modules[mod]\n                        if not callable(self._dbapi.connect):\n                            raise AttributeError\n                    except (AttributeError, KeyError):\n                        pass\n                    else:\n                        break\n                    i = mod.rfind('.')\n                    if i < 0:\n                        mod = None\n                    else:\n                        mod = mod[:i]\n                else:\n                    try:\n                        mod = con.OperationalError.__module__\n                    except AttributeError:\n                        mod = None\n                    while mod:\n                        try:\n                            self._dbapi = sys.modules[mod]\n                            if not callable(self._dbapi.connect):\n                                raise AttributeError\n                        except (AttributeError, KeyError):\n                            pass\n                        else:\n                            break\n                        i = mod.rfind('.')\n                        if i < 0:\n                            mod = None\n                        else:\n                            mod = mod[:i]\n                    else:\n                        self._dbapi = None\n            if self._threadsafety is None:\n                try:\n                    self._threadsafety = self._dbapi.threadsafety\n                except AttributeError:\n                    try:\n                        self._threadsafety = con.threadsafety\n                    except AttributeError:\n                        pass\n            if self._failures is None:\n                try:\n                    self._failures = (\n                        self._dbapi.OperationalError,\n                        self._dbapi.InternalError)\n                except AttributeError:\n                    try:\n                        self._failures = (\n                            self._creator.OperationalError,\n                            self._creator.InternalError)\n                    except AttributeError:\n                        try:\n                            self._failures = (\n                                con.OperationalError, con.InternalError)\n                        except AttributeError:\n                            raise AttributeError(\n                                \"Could not determine failure exceptions\"\n                                \" (please set failures or creator.dbapi).\")\n            if isinstance(self._failures, tuple):\n                self._failure = self._failures[0]\n            else:\n                self._failure = self._failures\n            self._setsession(con)\n        except Exception as error:\n            # the database module could not be determined\n            # or the session could not be prepared\n            try:  # close the connection first\n                con.close()\n            except Exception:\n                pass\n            raise error  # re-raise the original error again\n        return con"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _setsession(self, con=None):\n        if con is None:\n            con = self._con\n        if self._setsession_sql:\n            cursor = con.cursor()\n            for sql in self._setsession_sql:\n                cursor.execute(sql)\n            cursor.close()", "response": "Execute the SQL commands for session preparation."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _store(self, con):\n        self._con = con\n        self._transaction = False\n        self._closed = False\n        self._usage = 0", "response": "Store a database connection for subsequent use."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _reset(self, force=False):\n        if not self._closed and (force or self._transaction):\n            try:\n                self.rollback()\n            except Exception:\n                pass", "response": "Reset a tough connection. Rollback if forced or the connection was in a transaction."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks whether the underlying connection is still alive.", "response": "def _ping_check(self, ping=1, reconnect=True):\n        \"\"\"Check whether the connection is still alive using ping().\n\n        If the the underlying connection is not active and the ping\n        parameter is set accordingly, the connection will be recreated\n        unless the connection is currently inside a transaction.\n\n        \"\"\"\n        if ping & self._ping:\n            try:  # if possible, ping the connection\n                alive = self._con.ping()\n            except (AttributeError, IndexError, TypeError, ValueError):\n                self._ping = 0  # ping() is not available\n                alive = None\n                reconnect = False\n            except Exception:\n                alive = False\n            else:\n                if alive is None:\n                    alive = True\n                if alive:\n                    reconnect = False\n            if reconnect and not self._transaction:\n                try:  # try to reopen the connection\n                    con = self._create()\n                except Exception:\n                    pass\n                else:\n                    self._close()\n                    self._store(con)\n                    alive = True\n            return alive"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef close(self):\n        if self._closeable:\n            self._close()\n        elif self._transaction:\n            self._reset()", "response": "Closes the tough connection."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef begin(self, *args, **kwargs):\n        self._transaction = True\n        try:\n            begin = self._con.begin\n        except AttributeError:\n            pass\n        else:\n            begin(*args, **kwargs)", "response": "Indicate the beginning of a transaction."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef commit(self):\n        self._transaction = False\n        try:\n            self._con.commit()\n        except self._failures as error:  # cannot commit\n            try:  # try to reopen the connection\n                con = self._create()\n            except Exception:\n                pass\n            else:\n                self._close()\n                self._store(con)\n            raise error", "response": "Commit any pending transaction."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncancels a long - running transaction.", "response": "def cancel(self):\n        \"\"\"Cancel a long-running transaction.\n\n        If the underlying driver supports this method, it will be called.\n\n        \"\"\"\n        self._transaction = False\n        try:\n            cancel = self._con.cancel\n        except AttributeError:\n            pass\n        else:\n            cancel()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _cursor(self, *args, **kwargs):\n        # The args and kwargs are not part of the standard,\n        # but some database modules seem to use these.\n        transaction = self._transaction\n        if not transaction:\n            self._ping_check(2)\n        try:\n            if self._maxusage:\n                if self._usage >= self._maxusage:\n                    # the connection was used too often\n                    raise self._failure\n            cursor = self._con.cursor(*args, **kwargs)  # try to get a cursor\n        except self._failures as error:  # error in getting cursor\n            try:  # try to reopen the connection\n                con = self._create()\n            except Exception:\n                pass\n            else:\n                try:  # and try one more time to get a cursor\n                    cursor = con.cursor(*args, **kwargs)\n                except Exception:\n                    pass\n                else:\n                    self._close()\n                    self._store(con)\n                    if transaction:\n                        raise error  # re-raise the original error again\n                    return cursor\n                try:\n                    con.close()\n                except Exception:\n                    pass\n            if transaction:\n                self._transaction = False\n            raise error  # re-raise the original error again\n        return cursor", "response": "A tough version of the cursor method."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset stored input and output sizes for cursor execution.", "response": "def _setsizes(self, cursor=None):\n        \"\"\"Set stored input and output sizes for cursor execution.\"\"\"\n        if cursor is None:\n            cursor = self._cursor\n        if self._inputsizes:\n            cursor.setinputsizes(self._inputsizes)\n        for column, size in self._outputsizes.items():\n            if column is None:\n                cursor.setoutputsize(size)\n            else:\n                cursor.setoutputsize(size, column)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef close(self):\n        if not self._closed:\n            try:\n                self._cursor.close()\n            except Exception:\n                pass\n            self._closed = True", "response": "Close the tough cursor."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a tough version of the given cursor method.", "response": "def _get_tough_method(self, name):\n        \"\"\"Return a \"tough\" version of the given cursor method.\"\"\"\n        def tough_method(*args, **kwargs):\n            execute = name.startswith('execute')\n            con = self._con\n            transaction = con._transaction\n            if not transaction:\n                con._ping_check(4)\n            try:\n                if con._maxusage:\n                    if con._usage >= con._maxusage:\n                        # the connection was used too often\n                        raise con._failure\n                if execute:\n                    self._setsizes()\n                method = getattr(self._cursor, name)\n                result = method(*args, **kwargs)  # try to execute\n                if execute:\n                    self._clearsizes()\n            except con._failures as error:  # execution error\n                if not transaction:\n                    try:\n                        cursor2 = con._cursor(\n                            *self._args, **self._kwargs)  # open new cursor\n                    except Exception:\n                        pass\n                    else:\n                        try:  # and try one more time to execute\n                            if execute:\n                                self._setsizes(cursor2)\n                            method = getattr(cursor2, name)\n                            result = method(*args, **kwargs)\n                            if execute:\n                                self._clearsizes()\n                        except Exception:\n                            pass\n                        else:\n                            self.close()\n                            self._cursor = cursor2\n                            con._usage += 1\n                            return result\n                        try:\n                            cursor2.close()\n                        except Exception:\n                            pass\n                try:  # try to reopen the connection\n                    con2 = con._create()\n                except Exception:\n                    pass\n                else:\n                    try:\n                        cursor2 = con2.cursor(\n                            *self._args, **self._kwargs)  # open new cursor\n                    except Exception:\n                        pass\n                    else:\n                        if transaction:\n                            self.close()\n                            con._close()\n                            con._store(con2)\n                            self._cursor = cursor2\n                            raise error  # raise the original error again\n                        error2 = None\n                        try:  # try one more time to execute\n                            if execute:\n                                self._setsizes(cursor2)\n                            method2 = getattr(cursor2, name)\n                            result = method2(*args, **kwargs)\n                            if execute:\n                                self._clearsizes()\n                        except error.__class__:  # same execution error\n                            use2 = False\n                            error2 = error\n                        except Exception as error:  # other execution errors\n                            use2 = True\n                            error2 = error\n                        else:\n                            use2 = True\n                        if use2:\n                            self.close()\n                            con._close()\n                            con._store(con2)\n                            self._cursor = cursor2\n                            con._usage += 1\n                            if error2:\n                                raise error2  # raise the other error\n                            return result\n                        try:\n                            cursor2.close()\n                        except Exception:\n                            pass\n                    try:\n                        con2.close()\n                    except Exception:\n                        pass\n                if transaction:\n                    self._transaction = False\n                raise error  # re-raise the original error again\n            else:\n                con._usage += 1\n                return result\n        return tough_method"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef train_punkt(ctx, input, output, abbr, colloc):\n    click.echo('chemdataextractor.tokenize.train_punkt')\n    import pickle\n    from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n    punkt = PunktTrainer()\n    # Set these to true to include collocations more leniently, then increase MIN_COLLOC_FREQ to restrict again\n    # punkt.INCLUDE_ALL_COLLOCS = False\n    # punkt.INCLUDE_ABBREV_COLLOCS = False\n    # punkt.MIN_COLLOC_FREQ = 1\n    # Don't train on titles. They may contain abbreviations, but basically never have actual sentence boundaries.\n    for fin in input:\n        click.echo('Training on %s' % fin.name)\n        sentences = fin.read()  #.replace('.\\n', '. \\n\\n')\n        punkt.train(sentences, finalize=False, verbose=True)\n    punkt.finalize_training(verbose=True)\n    if abbr:\n        abbreviations = abbr.read().strip().split('\\n')\n        click.echo('Manually adding abbreviations: %s' % abbreviations)\n        punkt._params.abbrev_types.update(abbreviations)\n    if colloc:\n        collocations = [tuple(l.split('. ', 1)) for l in colloc.read().strip().split('\\n')]\n        click.echo('Manually adding collocs: %s' % collocations)\n        punkt._params.collocations.update(collocations)\n    model = PunktSentenceTokenizer(punkt.get_params())\n    pickle.dump(model, output, protocol=pickle.HIGHEST_PROTOCOL)", "response": "Train Punkt sentence splitter using sentences in input."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread input document and output sentences.", "response": "def sentences(ctx, input, output):\n    \"\"\"Read input document, and output sentences.\"\"\"\n    log.info('chemdataextractor.read.elements')\n    log.info('Reading %s' % input.name)\n    doc = Document.from_file(input)\n    for element in doc.elements:\n        if isinstance(element, Text):\n            for raw_sentence in element.raw_sentences:\n                output.write(raw_sentence.strip())\n                output.write(u'\\n')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef words(ctx, input, output):\n    log.info('chemdataextractor.read.elements')\n    log.info('Reading %s' % input.name)\n    doc = Document.from_file(input)\n    for element in doc.elements:\n        if isinstance(element, Text):\n            for sentence in element.sentences:\n                output.write(u' '.join(sentence.raw_tokens))\n                output.write(u'\\n')", "response": "Read input document and output words."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _in_stoplist(self, entity):\n        start = 0\n        end = len(entity)\n        # Adjust boundaries to exclude disallowed prefixes/suffixes\n        for prefix in IGNORE_PREFIX:\n            if entity.startswith(prefix):\n                # print('%s removing %s' % (currenttext, prefix))\n                start += len(prefix)\n                break\n        for suffix in IGNORE_SUFFIX:\n            if entity.endswith(suffix):\n                # print('%s removing %s' % (currenttext, suffix))\n                end -= len(suffix)\n                break\n        # Return True if entity has been reduced to nothing by adjusting boundaries\n        if start >= end:\n            return True\n        # Return True if adjusted entity is in the literal stoplist\n        entity = entity[start:end]\n        if entity in STOPLIST:\n            return True\n        # log.debug('Entity: %s', entity)\n        for stop_re in STOP_RES:\n            if re.search(stop_re, entity):\n                log.debug('Killed: %s', entity)\n                return True", "response": "Return True if the entity is in the stoplist."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrun individual chemical entity mention taggers and return union of matches with some postprocessing.", "response": "def tag(self, tokens):\n        \"\"\"Run individual chemical entity mention taggers and return union of matches, with some postprocessing.\"\"\"\n        # Combine output from individual taggers\n        tags = [None] * len(tokens)\n        just_tokens = [t[0] for t in tokens]\n        for tagger in self.taggers:\n            tag_gen = tagger.tag(tokens) if isinstance(tagger, CrfCemTagger) else tagger.tag(just_tokens)\n            for i, (token, newtag) in enumerate(tag_gen):\n                if newtag == 'I-CM' and not (i == 0 or tag_gen[i - 1][1] not in {'B-CM', 'I-CM'}):\n                    tags[i] = 'I-CM'  # Always overwrite I-CM\n                elif newtag == 'B-CM' and tags[i] is None:\n                    tags[i] = 'B-CM'  # Only overwrite B-CM over None\n        # Postprocess the combined output\n        for i, tag in enumerate(tags):\n            token, pos = tokens[i]\n            lex = self.lexicon[token]\n            nexttag = tags[i+1] if i < len(tags) - 1 else None\n            # Trim disallowed first tokens\n            if tag == 'B-CM' and lex.lower in STRIP_START:\n                tags[i] = None\n                if nexttag == 'I-CM':\n                    tags[i+1] = 'B-CM'\n            # Trim disallowed final tokens\n            if nexttag is None and lex.lower in STRIP_END:\n                tags[i] = None\n        # Filter certain entities\n        for i, tag in enumerate(tags):\n            token, pos = tokens[i]\n            if tag == 'B-CM':\n                entity_tokens = [self.lexicon[token].lower]\n                end_i = i + 1\n                for j, subsequent in enumerate(tags[i+1:]):\n                    if subsequent == 'I-CM':\n                        end_i += 1\n                        entity_tokens.append(self.lexicon[tokens[i+j+1][0]].lower)\n                    else:\n                        break\n\n                # Fix combined '1H NMR' on end  # TODO: Also 13C, etc.?\n                if len(entity_tokens) > 2 and entity_tokens[-1] == 'nmr' and entity_tokens[-2] == '1h':\n                    tags[end_i-2] = 'B-CM'\n                    tags[end_i-1] = None\n                    entity_tokens = entity_tokens[:-2]\n\n                entity = ' '.join(entity_tokens)\n                if any(e in STOP_TOKENS for e in entity_tokens) or self._in_stoplist(entity):\n                    tags[i:end_i] = [None] * (end_i - i)\n                else:\n                    bl = bracket_level(entity)\n                    # Try and add on brackets in neighbouring tokens if they form part of the name\n                    # TODO: Check bracket type matches before adding on\n                    if bl == 1 and len(tokens) > end_i and bracket_level(tokens[end_i][0]) == -1:\n                        #print('BLADJUST: %s - %s' % (entity, tokens[end_i][0]))\n                        tags[end_i] = 'I-CM'\n                    elif bl == -1 and i > 0 and bracket_level(tokens[i-1][0]) == 1:\n                        #print('BLADJUST: %s - %s' % (tokens[i-1][0], entity))\n                        tags[i-1] = 'B-CM'\n                        tags[i] = 'I-CM'\n                    elif not bracket_level(entity) == 0:\n                        # Filter entities that overall don't have balanced brackets\n                        tags[i:end_i] = [None] * (end_i - i)\n                    else:\n                        # Remove bracketed alphanumeric from end\n                        if len(entity_tokens) >= 4 and entity_tokens[-1] == ')' and entity_tokens[-3] == '(':\n                            if re.match('^(\\d{1,2}[A-Za-z]?|I|II|III|IV|V|VI|VII|VIII|IX)$', entity_tokens[-2]):\n                                log.debug('Removing %s from end of CEM', entity_tokens[-2])\n                                tags[end_i-3:end_i] = [None, None, None]\n        tokentags = list(six.moves.zip(tokens, tags))\n        return tokentags"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _process_name(name):\n\n    # Unescape HTML entities\n    name = unescape(name)\n\n    # Remove bracketed stuff on the end\n    name = NG_RE.sub('', name).strip()  # Nomenclature groups\n    name = END_RE.sub('', name).strip(', ')  # Words\n    name = RATIO_RE.sub('', name).strip(', ')  # Ratios\n\n    # Remove stuff off start\n    name = START_RE.sub('', name).strip()\n\n    # Remove balanced start and end brackets if none in between\n    name = BRACKET_RE.sub('\\g<1>', name)\n\n    # Un-invert CAS style names\n    comps = name.split(', ')\n    if len(comps) == 2:\n        if comps[1].endswith('-'):\n            name = comps[0]\n            name = '%s%s' % (comps[1], name)\n    elif len(comps) > 2:\n        name = comps[0]\n        for i in range(1, len(comps)):\n            if comps[i].endswith('-'):\n                name = '%s%s' % (comps[i], name)\n            else:\n                name = '%s %s' % (name, comps[i])\n    return name", "response": "Fix issues with Jochem names."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _filter_name(name):\n    # Remove if length 3 or less\n    if len(name) <= 3:\n        return False\n    # Remove if starts with IL-\n    if name.startswith('IL-'):\n        return False\n    lowname = name.lower()\n    # Remove if contains certain sequences\n    if any(c in lowname for c in STOP_SUB):\n        return False\n    # Remove if (case-insensitive) exact match to stoplist\n    if lowname in STOPLIST:\n        return False\n    comps = re.split('[ -]', lowname)\n    # Remove if just single character + digits separated by spaces or hyphens (or the word compound)\n    if all(c.isdigit() or len(c) == 1 or c == 'compound' for c in comps):\n        return False\n    # Remove if 3 or fewer letters with 2 or fewer digits\n    if len(comps) == 2 and len(comps[0]) <= 3 and comps[0].isalpha() and len(comps[1]) <= 3 and comps[1].isdigit():\n        return False\n    # Remove if just greek characters and numbrs\n    if re.match('^[\u0391-\u03a9\u03b1-\u03c90-9]+$', name):\n        return False\n    # Filter registry numbers? No real size benefit in DAWG.\n    # if REG_RE.search(name):\n    #     keep = False\n    # Handle this at the token level\n    # if name.endswith(' derivative') or name.endswith(' analog') or name.endswith(' solution'):\n    #     keep = False\n    # Filter this after matching and expanding boundaries\n    # if name.startswith('-') or name.endswith('-'):\n    #     keep = False\n    # Filter this after matching and expanding boundaries\n    # if not bracket_level(name) == 0:\n    #     print(name)\n    return True", "response": "Filter words when adding to Dictionary. Return True if name should be added. Return False if name should be added. Return True if name should be added. Return True if name should be added. Return False if name should be added."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_variants(name):\n    names = [name]\n    oldname = name\n    # Map greek words to unicode characters\n    if DOT_GREEK_RE.search(name):\n        wordname = name\n        while True:\n            m = DOT_GREEK_RE.search(wordname)\n            if m:\n                wordname = wordname[:m.start(1)-1] + m.group(1) + wordname[m.end(1)+1:]\n            else:\n                break\n        symbolname = name\n        while True:\n            m = DOT_GREEK_RE.search(symbolname)\n            if m:\n                symbolname = symbolname[:m.start(1)-1] + GREEK_WORDS[m.group(1)] + symbolname[m.end(1)+1:]\n            else:\n                break\n        names = [wordname, symbolname]\n    else:\n        while True:\n            m = GREEK_RE.search(name)\n            if m:\n                name = name[:m.start(2)] + GREEK_WORDS[m.group(2)] + name[m.end(2):]\n            else:\n                break\n        while True:\n            m = UNAMBIGUOUS_GREEK_RE.search(name)\n            if m:\n                name = name[:m.start(1)] + GREEK_WORDS[m.group(1)] + name[m.end(1):]\n            else:\n                break\n        if not name == oldname:\n            names.append(name)\n    newnames = []\n    for name in names:\n        # If last word \\d+, add variants with hyphen and no space preceding\n        if NUM_END_RE.search(name):\n            newnames.append(NUM_END_RE.sub('-\\g<1>', name))\n            newnames.append(NUM_END_RE.sub('\\g<1>', name))\n        # If last word [A-Za-z]\\d* add variants with hyphen preceding.\n        if ALPHANUM_END_RE.search(name):\n            newnames.append(ALPHANUM_END_RE.sub('-\\g<1>', name))\n    names.extend(newnames)\n    return names", "response": "Return a list of chemical names and their variants."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef prepare_jochem(ctx, jochem, output, csoutput):\n    click.echo('chemdataextractor.dict.prepare_jochem')\n    for i, line in enumerate(jochem):\n        print('JC%s' % i)\n        if line.startswith('TM '):\n            if line.endswith('\t@match=ci\\n'):\n                for tokens in _make_tokens(line[3:-11]):\n                    output.write(' '.join(tokens))\n                    output.write('\\n')\n            else:\n                for tokens in _make_tokens(line[3:-1]):\n                    csoutput.write(' '.join(tokens))\n                    csoutput.write('\\n')", "response": "Process and filter jochem file to produce list of names for dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprocesses and filter include file to produce list of names for dictionary.", "response": "def prepare_include(ctx, include, output):\n    \"\"\"Process and filter include file to produce list of names for dictionary.\"\"\"\n    click.echo('chemdataextractor.dict.prepare_include')\n    for i, line in enumerate(include):\n        print('IN%s' % i)\n        for tokens in _make_tokens(line.strip()):\n            output.write(u' '.join(tokens))\n            output.write(u'\\n')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef build(ctx, inputs, output, cs):\n    click.echo('chemdataextractor.dict.build')\n    dt = DictionaryTagger(lexicon=ChemLexicon(), case_sensitive=cs)\n    names = []\n    for input in inputs:\n        for line in input:\n            tokens = line.split()\n            names.append(tokens)\n    dt.build(words=names)\n    dt.save(output)", "response": "Build chemical name dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef tag(ctx, model, cs, corpus, output):\n    click.echo('chemdataextractor.dict.tag')\n    tagger = CsDictCemTagger(model=model) if cs else CiDictCemTagger(model=model)\n    for line in corpus:\n        sentence = []\n        goldsentence = []\n        for t in line.split():\n            token, tag = t.rsplit(u'/', 1)\n            goldsentence.append((token, tag))\n            sentence.append(token)\n        if sentence:\n            tokentags = tagger.tag(sentence)\n            for i, tokentag in enumerate(tokentags):\n                goldtokentag = goldsentence[i]\n                if goldtokentag[1] not in {u'B-CM', u'I-CM'} and tokentag[1] in {u'B-CM', u'I-CM'}:\n                    print(line)\n                    print(tokentag[0])\n\n            output.write(u' '.join(u'/'.join(tokentag) for tokentag in tagger.tag(sentence)))\n            output.write(u'\\n')\n        else:\n            output.write(u'\\n')", "response": "Tag chemical entities and write CHEMDNER annotations predictions file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _parse_table_footnotes(self, fns, refs, specials):\n        footnotes = []\n        for fn in fns:\n            footnote = self._parse_text(fn, refs=refs, specials=specials, element_cls=Footnote)[0]\n            footnote += Footnote('', id=fn.getprevious().get('id'))\n            footnotes.append(footnote)\n        return footnotes", "response": "Override to account for awkward RSC table footnotes."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nextract melting points from patents.", "response": "def extract():\n    \"\"\"Extract melting points from patents.\"\"\"\n    Paragraph.parsers = [CompoundParser(), ChemicalLabelParser(), MpParser()]\n    Table.parsers = []\n    patents = []\n    for root, dirs, files in os.walk('../examples/mp/grants'):\n        for filename in files:\n            if not filename.endswith('.xml'):\n                continue\n            path = os.path.abspath(os.path.join(root, filename))\n            size = os.path.getsize(path)\n            patents.append((path, filename, size))\n\n    patents = sorted(patents, key=lambda p: p[2])\n\n    for path, filename, size in patents:\n        print(path)\n        shutil.copyfile(path, '../examples/mp/used/%s' % filename)\n        with open(path) as f:\n            d = Document.from_file(f)\n        if os.path.isfile('../examples/mp/results/%s.json' % filename):\n            continue\n        records = [r.serialize() for r in d.records if len(r.melting_points) == 1]\n        with open('../examples/mp/results/%s.json' % filename, 'w') as fout:\n            fout.write(json.dumps(records, ensure_ascii=False, indent=2).encode('utf8'))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef make_sdf():\n    # import numpy as np\n    # my_results_by_inchikey = defaultdict(list)\n    # result_dir = '../examples/mp/standardized_results'\n    # fout = open('../examples/mp/sdf/chemdataextractor-melting-points.sdf', 'w')\n    # writer = Chem.SDWriter(fout)\n    # for filename in os.listdir(result_dir):\n    #     if not filename.endswith('.json'):\n    #         continue\n    #     patent_id = filename[:-5]\n    #     with open('%s/%s' % (result_dir, filename)) as fin:\n    #         results = json.loads(fin.read().decode('utf8'))\n    #         for m in results:\n    #             m['patent_id'] = patent_id\n    #             mol = Chem.MolFromSmiles(m['smiles'])\n    #             mol.SetProp(b'_Name', m['name'].encode('utf-8'))\n    #             mol.SetProp(b'OriginalText', m['value'].encode('utf-8'))\n    #             mol.SetProp(b'Value', b'%s' % m['float_value'])\n    #             mol.SetProp(b'Patent', m['patent_id'].encode('utf-8'))\n    #             mol.SetProp(b'SMILES', Chem.MolToSmiles(mol, isomericSmiles=True))\n    #             mol.SetProp(b'QuantityType', b'MeltingPoint')\n    #             mol.SetProp(b'StdInChIKey', Chem.InchiToInchiKey(Chem.MolToInchi(mol)))\n    #             if not mol:\n    #                 print('WARNING: %s' % m)\n    #                 return\n    #             AllChem.Compute2DCoords(mol)\n    #             writer.write(mol)\n    #             my_results_by_inchikey[m['inchikey']].append(m)\n    # writer.close()\n    # fout.close()\n    #\n    # fout = open('../examples/mp/sdf/chemdataextractor-melting-points-filtered.sdf', 'w')\n    # writer = Chem.SDWriter(fout)\n    # for inchikey, results in my_results_by_inchikey.items():\n    #     if len(results) == 1:\n    #         m = results[0]\n    #     elif len(results) == 2:\n    #         sorted_results = sorted(results, key=lambda x: x['float_value'])\n    #         m = sorted_results[0]\n    #     else:\n    #         median = np.median([r['float_value'] for r in results])\n    #         chosen = results[0]\n    #         for result in results:\n    #             if result['float_value'] - median < chosen['float_value'] < median:\n    #                 m = result\n    #     mol = Chem.MolFromSmiles(m['smiles'])\n    #     mol.SetProp(b'_Name', m['name'].encode('utf-8'))\n    #     mol.SetProp(b'OriginalText', m['value'].encode('utf-8'))\n    #     mol.SetProp(b'Value', b'%s' % m['float_value'])\n    #     mol.SetProp(b'Patent', m['patent_id'].encode('utf-8'))\n    #     mol.SetProp(b'SMILES', Chem.MolToSmiles(mol, isomericSmiles=True))\n    #     mol.SetProp(b'QuantityType', b'MeltingPoint')\n    #     mol.SetProp(b'StdInChIKey', Chem.InchiToInchiKey(Chem.MolToInchi(mol)))\n    #     if not mol:\n    #         print('WARNING: %s' % m)\n    #         return\n    #     AllChem.Compute2DCoords(mol)\n    #     writer.write(mol)\n\n\n    with open('../examples/mp/sdf/chemdataextractor-melting-points.sdf', 'rb') as f_in, gzip.open('../examples/mp/sdf/chemdataextractor-melting-points.sdf.gz', 'wb') as f_out:\n        shutil.copyfileobj(f_in, f_out)\n\n    with open('../examples/mp/sdf/chemdataextractor-melting-points-filtered.sdf', 'rb') as f_in, gzip.open('../examples/mp/sdf/chemdataextractor-melting-points-filtered.sdf.gz', 'wb') as f_out:\n        shutil.copyfileobj(f_in, f_out)", "response": "Produce SDF of ChemDataExtractor and Tetko sample melting points."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef extract(ctx, input, output):\n    log.info('chemdataextractor.extract')\n    log.info('Reading %s' % input.name)\n    doc = Document.from_file(input, fname=input.name)\n    records = [record.serialize(primitive=True) for record in doc.records]\n    jsonstring = json.dumps(records, indent=2, ensure_ascii=False)\n    output.write(jsonstring)", "response": "Run ChemDataExtractor on a document."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read(ctx, input, output):\n    log.info('chemdataextractor.read')\n    log.info('Reading %s' % input.name)\n    doc = Document.from_file(input)\n    for element in doc.elements:\n        output.write(u'%s : %s\\n=====\\n' % (element.__class__.__name__, six.text_type(element)))", "response": "Read a sequence of XML documents."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef extract_smiles(s):\n    # TODO: This still gets a lot of false positives.\n    smiles = []\n    for t in s.split():\n        if len(t) > 2 and SMILES_RE.match(t) and not t.endswith('.') and bracket_level(t) == 0:\n            smiles.append(t)\n    return smiles", "response": "Return a list of SMILES identifiers extracted from the string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef could_be(self, other):\n        # TODO: Some suffix and title differences should be allowed\n        if type(other) is not type(self):\n            return NotImplemented\n        if self == other:\n            return True\n        for attr in ['title', 'firstname', 'middlename', 'nickname', 'prefix', 'lastname', 'suffix']:\n            if attr not in self or attr not in other:\n                continue\n            puncmap = dict((ord(char), None) for char in string.punctuation)\n            s = self[attr].lower().translate(puncmap)\n            o = other[attr].lower().translate(puncmap)\n            if s == o:\n                continue\n            if attr in {'firstname', 'middlename', 'lastname'}:\n                if (({len(comp) for comp in s.split()} == {1} and [el[0] for el in o.split()] == s.split()) or\n                        ({len(comp) for comp in o.split()} == {1} and [el[0] for el in s.split()] == o.split())):\n                    continue\n            return False\n        return True", "response": "Return True if the other PersonName is not explicitly inconsistent."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _is_suffix(self, t):\n        return t not in NOT_SUFFIX and (t.replace('.', '') in SUFFIXES or t.replace('.', '') in SUFFIXES_LOWER)", "response": "Return true if t is a suffix."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsplit name on spaces unless inside curly brackets or quotes.", "response": "def _tokenize(self, comps):\n        \"\"\"Split name on spaces, unless inside curly brackets or quotes.\"\"\"\n        ps = []\n        for comp in comps:\n            ps.extend([c.strip(' ,') for c in re.split(r'\\s+(?=[^{}]*(?:\\{|$))', comp)])\n        return [p for p in ps if p]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _clean(self, t, capitalize=None):\n        if self._from_bibtex:\n            t = latex_to_unicode(t, capitalize=capitalize)\n        t = ' '.join([el.rstrip('.') if el.count('.') == 1 else el for el in t.split()])\n        return t", "response": "Convert to normalized unicode and strip trailing full stops."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nstrip off contiguous tokens from the start or end of the list that meet the criteria.", "response": "def _strip(self, tokens, criteria, prop, rev=False):\n        \"\"\"Strip off contiguous tokens from the start or end of the list that meet the criteria.\"\"\"\n        num = len(tokens)\n        res = []\n        for i, token in enumerate(reversed(tokens) if rev else tokens):\n            if criteria(token) and num > i + 1:\n                res.insert(0, tokens.pop()) if rev else res.append(tokens.pop(0))\n            else:\n                break\n        if res:\n            self[prop] = self._clean(' '.join(res))\n        return tokens"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _parse(self, fullname):\n        n = ' '.join(fullname.split()).strip(',')\n        if not n:\n            return\n        comps = [p.strip() for p in n.split(',')]\n        if len(comps) > 1 and not all([self._is_suffix(comp) for comp in comps[1:]]):\n            vlj = []\n            while True:\n                vlj.append(comps.pop(0))\n                if not self._is_suffix(comps[0]):\n                    break\n            ltokens = self._tokenize(vlj)\n            ltokens = self._strip(ltokens, self._is_prefix, 'prefix')\n            ltokens = self._strip(ltokens, self._is_suffix, 'suffix', True)\n            self['lastname'] = self._clean(' '.join(ltokens), capitalize='name')\n        tokens = self._tokenize(comps)\n        tokens = self._strip(tokens, self._is_title, 'title')\n        if not 'lastname' in self:\n            tokens = self._strip(tokens, self._is_suffix, 'suffix', True)\n        voni = []\n        end = len(tokens) - 1\n        if not 'prefix' in self:\n            for i, token in enumerate(reversed(tokens)):\n                if self._is_prefix(token):\n                    if (i == 0 and end > 0) or (not 'lastname' in self and not i == end):\n                        voni.append(end - i)\n                else:\n                    if (i == 0 and 'lastname' in self) or voni:\n                        break\n        if voni:\n            if not 'lastname' in self:\n                self['lastname'] = self._clean(' '.join(tokens[voni[0]+1:]), capitalize='name')\n            self['prefix'] = self._clean(' '.join(tokens[voni[-1]:voni[0]+1]))\n            tokens = tokens[:voni[-1]]\n        else:\n            if not 'lastname' in self:\n                self['lastname'] = self._clean(tokens.pop(), capitalize='name')\n        if tokens:\n            self['firstname'] = self._clean(tokens.pop(0), capitalize='name')\n        if tokens:\n            nicki = []\n            for i, token in enumerate(tokens):\n                if token[0] in QUOTES:\n                    for j, token2 in enumerate(tokens[i:]):\n                        if token2[-1] in QUOTES:\n                            nicki = range(i, i+j+1)\n                            break\n            if nicki:\n                self['nickname'] = self._clean(' '.join(tokens[nicki[0]:nicki[-1]+1]).strip(''.join(QUOTES)),\n                                                     capitalize='name')\n                tokens[nicki[0]:nicki[-1]+1] = []\n        if tokens:\n            self['middlename'] = self._clean(' '.join(tokens), capitalize='name')\n        namelist = []\n        for attr in ['title', 'firstname', 'middlename', 'nickname', 'prefix', 'lastname', 'suffix']:\n            if attr in self:\n                namelist.append('\"%s\"' % self[attr] if attr == 'nickname' else self[attr])\n        self['fullname'] = ' '.join(namelist)", "response": "Perform the parsing of the full name."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _parse_element_r(self, el, specials, refs, id=None, element_cls=Paragraph):\n        elements = []\n        if el.tag in {etree.Comment, etree.ProcessingInstruction}:\n            return []\n        # if el in refs:\n        #     return [element_cls('', references=refs[el])]\n        if el in specials:\n            return specials[el]\n        id = el.get('id', id)\n        references = refs.get(el, [])\n        if el.text is not None:\n            elements.append(element_cls(six.text_type(el.text), id=id, references=references))\n        elif references:\n            elements.append(element_cls('', id=id, references=references))\n        for child in el:\n            # br is a special case - technically inline, but we want to split\n            if child.tag not in {etree.Comment, etree.ProcessingInstruction} and child.tag.lower() == 'br':\n                elements.append(element_cls(''))\n\n            child_elements = self._parse_element_r(child, specials=specials, refs=refs, id=id, element_cls=element_cls)\n            if (self._is_inline(child) and len(elements) > 0 and len(child_elements) > 0 and\n                    isinstance(elements[-1], (Text, Sentence)) and isinstance(child_elements[0], (Text, Sentence)) and\n                    type(elements[-1]) == type(child_elements[0])):\n                elements[-1] += child_elements.pop(0)\n            elements.extend(child_elements)\n            if child.tail is not None:\n                if self._is_inline(child) and len(elements) > 0 and isinstance(elements[-1], element_cls):\n                    elements[-1] += element_cls(six.text_type(child.tail), id=id)\n                else:\n                    elements.append(element_cls(six.text_type(child.tail), id=id))\n        return elements", "response": "Recursively parse HTML or XML element and its children into a list of Document elements."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _parse_text(self, el, refs=None, specials=None,  element_cls=Paragraph):\n        if specials is None:\n            specials = {}\n        if refs is None:\n            refs = {}\n        elements = self._parse_element_r(el, specials=specials, refs=refs, element_cls=element_cls)\n        # This occurs if the input element is self-closing... (some table td in NLM XML)\n        if not elements:\n            return [element_cls('')]\n        element = elements[0]\n        for next_element in elements[1:]:\n            element += element_cls(' ') + next_element\n        return [element]", "response": "Like _parse_element but ensure a single element."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn reference ID from href or text content.", "response": "def _parse_reference(self, el):\n        \"\"\"Return reference ID from href or text content.\"\"\"\n        if '#' in el.get('href', ''):\n            return [el.get('href').split('#', 1)[1]]\n        elif 'rid' in el.attrib:\n            return [el.attrib['rid']]\n        elif 'idref' in el.attrib:\n            return [el.attrib['idref']]\n        else:\n            return [''.join(el.itertext()).strip()]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _is_inline(self, element):\n        if element.tag not in {etree.Comment, etree.ProcessingInstruction} and element.tag.lower() in self.inline_elements:\n            return True\n        return False", "response": "Return True if an element is inline."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nincrements _token to the next token and return it.", "response": "def _next_token(self, skipws=True):\n        \"\"\"Increment _token to the next token and return it.\"\"\"\n        self._token = next(self._tokens).group(0)\n        return self._next_token() if skipws and self._token.isspace() else self._token"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _parse_string(self):\n        if self._next_token() in ['{', '(']:\n            field = self._parse_field()\n            if field:\n                self.definitions[field[0]] = field[1]", "response": "Parse a string entry and store the definition."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _parse_value(self):\n        val = []\n        while True:\n            t = self._next_token()\n            if t == '\"':\n                brac_counter = 0\n                while True:\n                    t = self._next_token(skipws=False)\n                    if t == '{':\n                        brac_counter += 1\n                    if t == '}':\n                        brac_counter -= 1\n                    if t == '\"' and brac_counter <= 0:\n                        break\n                    else:\n                        val.append(t)\n            elif t == '{':\n                brac_counter = 0\n                while True:\n                    t = self._next_token(skipws=False)\n                    if t == '{':\n                        brac_counter += 1\n                    if t == '}':\n                        brac_counter -= 1\n                    if brac_counter < 0:\n                        break\n                    else:\n                        val.append(t)\n            elif re.match(r'\\w', t):\n                val.extend([self.definitions.get(t, t), ' '])\n            elif t.isdigit():\n                val.append([t, ' '])\n            elif t == '#':\n                pass\n            else:\n                break\n\n        value = ' '.join(''.join(val).split())\n        return value", "response": "Parse a value. Digits definitions and the contents of double quotes or curly brackets."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse a string of names separated by and like in a BibTeX authors field.", "response": "def parse_names(cls, names):\n        \"\"\"Parse a string of names separated by \"and\" like in a BibTeX authors field.\"\"\"\n        names = [latex_to_unicode(n) for n in re.split(r'\\sand\\s(?=[^{}]*(?:\\{|$))', names) if n]\n        return names"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef metadata(self):\n        auto = {u'records': self.size}\n        auto.update(self.meta)\n        return auto", "response": "Return the metadata for the parsed collection of records."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef json(self):\n        return json.dumps(OrderedDict([('metadata', self.metadata), ('records', self.records.values())]))", "response": "Return a list of records as a JSON string. Follows the BibJSON convention."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _flush(self):\n        d = os.path.dirname(self.path)\n        if not os.path.isdir(d):\n            os.makedirs(d)\n        with io.open(self.path, 'w', encoding='utf8') as f:\n            yaml.safe_dump(self._data, f, default_flow_style=False, encoding=None)", "response": "Save the contents of data to the file on disk."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _is_allowed_abbr(self, tokens):\n        if len(tokens) <= 2:\n            abbr_text = ''.join(tokens)\n            if self.abbr_min <= len(abbr_text) <= self.abbr_max and bracket_level(abbr_text) == 0:\n                if abbr_text[0].isalnum() and any(c.isalpha() for c in abbr_text):\n                    # Disallow property values\n                    if re.match('^\\d+(\\.\\d+)?(g|m[lL]|cm)$', abbr_text):\n                        return False\n                    return True\n        return False", "response": "Return True if text is an allowed abbreviation."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndiscards if long shorter than abbr or long token is in the long token.", "response": "def _filter_candidates(self, tokens, candidates):\n        \"\"\"Discard if long shorter than abbr, or if abbr token(s) are in the long token(s).\"\"\"\n        results = []\n        for abbr_span, long_span in candidates:\n            abbr = tokens[abbr_span[0]:abbr_span[1]]\n            long = tokens[long_span[0]:long_span[1]]\n            if not all(a in long for a in abbr) and len(''.join(long)) > len(''.join(abbr)):\n                results.append((abbr_span, long_span))\n        return results"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef detect(self, tokens):\n        results = []\n        for abbr_span, long_span in self.detect_spans(tokens):\n            results.append((tokens[abbr_span[0]:abbr_span[1]], tokens[long_span[0]:long_span[1]]))\n        return results", "response": "Return a list of tuples for each abbreviation definition."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndetects the spans of each abbreviation definition in the given list of tokens.", "response": "def detect_spans(self, tokens):\n        \"\"\"Return (abbr_span, long_span) pair for each abbreviation definition.\n\n        abbr_span and long_span are (int, int) spans defining token ranges.\n        \"\"\"\n        candidates = self._get_candidates(tokens)\n        results = self._filter_candidates(tokens, candidates)\n        return results"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _post_scrape(self, value, processor=None):\n        # Pass each value through the field's clean method\n        value = [self.process(v) for v in value]\n        # Filter None values\n        value = [v for v in value if v is not None]\n        # Pass each value through processors defined on the entity\n        if processor:\n            value = [processor(v) for v in value]\n            value = [v for v in value if v is not None]\n        # Take first unless all is specified\n        if not self.all:\n            value = value[0] if value else None\n        log.debug('Scraped %s: %s from %s' % (self.name, value, self.selection))\n        return value", "response": "Apply processing to the scraped value."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nscrapes the value for this field from the selector.", "response": "def scrape(self, selector, cleaner=None, processor=None):\n        \"\"\"Scrape the value for this field from the selector.\"\"\"\n        # Apply CSS or XPath expression to the selector\n        selected = selector.xpath(self.selection) if self.xpath else selector.css(self.selection)\n        # Extract the value and apply regular expression if specified\n        value = selected.re(self.re) if self.re else selected.extract(raw=self.raw, cleaner=cleaner)\n        return self._post_scrape(value, processor=processor)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _make_bound_method(func, self):\n    class Foo(object):\n        def meth(self): pass\n    f = Foo()\n    bound_method = type(f.meth)\n    try:\n        return bound_method(func, self, self.__class__)\n    except TypeError: # python3\n        return bound_method(func, self)", "response": "Magic for creating bound methods ( used for _load"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the list of Entity objects for the given selector.", "response": "def scrape(cls, selector, root, xpath=False):\n        \"\"\"Return EntityList for the given selector.\"\"\"\n        log.debug('Called scrape classmethod with root: %s' % root)\n        roots = selector.xpath(root) if xpath else selector.css(root)\n        results = [cls(r) for r in roots]\n        return EntityList(*results)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef serialize(self):\n        # Serialize fields to a dict\n        data = {}\n        for field_name in self:\n            value = self._values.get(field_name)\n            field = self.fields.get(field_name)\n            if value is not None:\n                if field.all:\n                    value = [field.serialize(v) for v in value]\n                else:\n                    value = field.serialize(value)\n            # Skip empty fields unless field.null\n            if not field.null and ((field.all and value == []) or (not field.all and value in {None, ''})):\n                continue\n            data[field.name] = value\n        return data", "response": "Convert the entity to a python dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef to_json(self, *args, **kwargs):\n        return json.dumps(self.serialize(), *args, **kwargs)", "response": "Convert the current object to JSON."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef from_file(cls, f, fname=None, readers=None):\n        if isinstance(f, six.string_types):\n            f = io.open(f, 'rb')\n        if not fname and hasattr(f, 'name'):\n            fname = f.name\n        return cls.from_string(f.read(), fname=fname, readers=readers)", "response": "Create a Document object from a file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a Document object from a byte string containing the contents of a file.", "response": "def from_string(cls, fstring, fname=None, readers=None):\n        \"\"\"Create a Document from a byte string containing the contents of a file.\n\n        Usage::\n\n            contents = open('paper.html', 'rb').read()\n            doc = Document.from_string(contents)\n\n        .. note::\n\n            This method expects a byte string, not a unicode string (in contrast to most methods in ChemDataExtractor).\n\n        :param bytes fstring: A byte string containing the contents of a file.\n        :param string fname: (Optional) The filename. Used to help determine file format.\n        :param list[chemdataextractor.reader.base.BaseReader] readers: (Optional) List of readers to use.\n        \"\"\"\n        if readers is None:\n            from ..reader import DEFAULT_READERS\n            readers = DEFAULT_READERS\n\n        if isinstance(fstring, six.text_type):\n            raise ReaderError('from_string expects a byte string, not a unicode string')\n\n        for reader in readers:\n            # Skip reader if we don't think it can read file\n            if not reader.detect(fstring, fname=fname):\n                continue\n            try:\n                d = reader.readstring(fstring)\n                log.debug('Parsed document with %s' % reader.__class__.__name__)\n                return d\n            except ReaderError:\n                pass\n        raise ReaderError('Unable to read document')"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn chemical records extracted from this document.", "response": "def records(self):\n        \"\"\"Return chemical records extracted from this document.\"\"\"\n        records = ModelList()\n        contextual_records = []\n        head_def_record = None\n        head_def_record_i = None\n        last_product_record = None\n        title_record = None\n        for i, el in enumerate(self.elements):\n            last_id_record = None\n\n            # Save title compound\n            if isinstance(el, Title):\n                el_records = el.records\n                if len(el_records) == 1 and el_records[0].is_id_only:\n                    title_record = el_records[0]\n            \n            # Reset head_def_record unless consecutive heading with no records\n            if isinstance(el, Heading) and head_def_record is not None:\n                if not (i == head_def_record_i + 1 and len(el.records) == 0):\n                    head_def_record = None\n                    head_def_record_i = None\n\n            # Paragraph with single sentence with single ID record considered a head_def_record like heading\n            if isinstance(el, Paragraph) and len(el.sentences) == 1:\n                el_records = el.records\n                if len(el_records) == 1 and el_records[0].is_id_only:\n                    head_def_record = el_records[0]\n                    head_def_record_i = i\n            elif isinstance(el, Paragraph) and len(el.sentences) > 0 and not (head_def_record_i == i - 1 and isinstance(self.elements[i - 1], Heading)):\n                # head_def_record from first sentence in Paragraph with single ID record unless right after heading with previous head_def_record\n                first_sent_records = el.sentences[0].records\n                if len(first_sent_records) == 1 and first_sent_records[0].is_id_only:\n                    sent_record = first_sent_records[0]\n                    if sent_record.labels or (sent_record.names and len(sent_record.names[0]) > len(el.sentences[0].text) / 2):\n                        head_def_record = sent_record\n                        head_def_record_i = i\n\n            for record in el.records:\n                # Keep track of the most recent record with labels\n                if isinstance(el, Paragraph) and record.labels:\n                    last_id_record = record\n                # Keep track of the most recent 'product' record\n                if 'product' in record.roles:\n                    last_product_record = record\n                if isinstance(el, Heading) and (record.labels or record.names):\n                    head_def_record = record\n                    head_def_record_i = i\n\n                    # If 2 consecutive headings with compound ID, merge in from previous\n                    if i > 0 and isinstance(self.elements[i - 1], Heading):\n                        prev = self.elements[i - 1]\n                        if (len(el.records) == 1 and record.is_id_only and len(prev.records) == 1 and\n                                prev.records[0].is_id_only and not (record.labels and prev.records[0].labels) and\n                                not (record.names and prev.records[0].names)):\n                            record.names.extend(prev.records[0].names)\n                            record.labels.extend(prev.records[0].labels)\n                            record.roles.extend(prev.records[0].roles)\n\n                if record.is_unidentified:\n                    if record.is_contextual:\n                        # Add contextual record to a list of all from the document for later merging\n                        contextual_records.append(record)\n                        continue\n                    else:\n                        # print(record.serialize())\n                        # We have property values but no names or labels... try merge those from previous\n                        if isinstance(el, Paragraph) and (head_def_record or last_product_record or last_id_record or title_record):\n                            # head_def_record from heading takes priority if the heading directly precedes the paragraph ( NOPE: or the last_id_record has no name)\n                            if head_def_record_i and head_def_record_i + 1 == i: # or (last_id_record and not last_id_record.names)):\n                                if head_def_record:\n                                    record.names = head_def_record.names\n                                    record.labels = head_def_record.labels\n                                    record.roles = head_def_record.roles\n                                elif last_id_record:\n                                    record.names = last_id_record.names\n                                    record.labels = last_id_record.labels\n                                    record.roles = last_id_record.roles\n                                elif last_product_record:\n                                    record.names = last_product_record.names\n                                    record.labels = last_product_record.labels\n                                    record.roles = last_product_record.roles\n                                elif title_record:\n                                    record.names = title_record.names\n                                    record.labels = title_record.labels\n                                    record.roles = title_record.roles\n                            else:\n                                if last_id_record:\n                                    record.names = last_id_record.names\n                                    record.labels = last_id_record.labels\n                                    record.roles = last_id_record.roles\n                                elif head_def_record:\n                                    record.names = head_def_record.names\n                                    record.labels = head_def_record.labels\n                                    record.roles = head_def_record.roles\n                                elif last_product_record:\n                                    record.names = last_product_record.names\n                                    record.labels = last_product_record.labels\n                                    record.roles = last_product_record.roles\n                                elif title_record:\n                                    record.names = title_record.names\n                                    record.labels = title_record.labels\n                                    record.roles = title_record.roles\n                        else:\n                            # Consider continue here to filter records missing name/label...\n                            pass\n                records.append(record)\n\n        for record in records:\n            for contextual_record in contextual_records:\n                record.merge_contextual(contextual_record)\n\n        for record in records:\n            for short, long, entity in self.abbreviation_definitions:\n                if entity == 'CM':\n                    name = ' '.join(long)\n                    abbrev = ' '.join(short)\n                    if name in record.names and not abbrev in record.names:\n                        record.names.append(abbrev)\n                    if abbrev in record.names and not name in record.names:\n                        record.names.append(name)\n\n        # Merge records with any shared name/label\n        len_l = len(records)\n        i = 0\n        while i < (len_l - 1):\n            for j in range(i + 1, len_l):\n                r = records[i]\n                other_r = records[j]\n\n                # Strip whitespace and lowercase to compare names\n                rnames_std = {''.join(n.split()).lower() for n in r.names}\n                onames_std = {''.join(n.split()).lower() for n in other_r.names}\n\n                # Clashing labels, don't merge\n                if len(set(r.labels) - set(other_r.labels)) > 0 and len(set(other_r.labels) - set(r.labels)) > 0:\n                    continue\n\n                if any(n in rnames_std for n in onames_std) or any(l in r.labels for l in other_r.labels):\n                    records.pop(j)\n                    records.pop(i)\n                    records.append(r.merge(other_r))\n                    len_l -= 1\n                    i -= 1\n                    break\n            i += 1\n        return records"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the element with the specified ID.", "response": "def get_element_with_id(self, id):\n        \"\"\"Return the element with the specified ID.\"\"\"\n        # Should we maintain a hashmap of ids to make this more efficient? Probably overkill.\n        # TODO: Elements can contain nested elements (captions, footnotes, table cells, etc.)\n        return next((el for el in self.elements if el.id == id), None)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef serialize(self):\n        # Serialize fields to a dict\n        elements = []\n        for element in self.elements:\n            elements.append(element.serialize())\n        data = {'type': 'document', 'elements': elements}\n        return data", "response": "Convert Document to python dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse(self, xmp):\n        tree = etree.fromstring(xmp)\n        rdf_tree = tree.find(RDF_NS + 'RDF')\n        meta = defaultdict(dict)\n        for desc in rdf_tree.findall(RDF_NS + 'Description'):\n            for el in desc.getchildren():\n                ns, tag = self._parse_tag(el)\n                value = self._parse_value(el)\n                meta[ns][tag] = value\n        return dict(meta)", "response": "Run parser and return a dictionary of all the parsed metadata."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _parse_tag(self, el):\n        ns = None\n        tag = el.tag\n        if tag[0] == '{':\n            ns, tag = tag[1:].split('}', 1)\n            if self.ns_map and ns in self.ns_map:\n                ns = self.ns_map[ns]\n        return ns, tag", "response": "Extract the namespace and tag from an element."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _parse_value(self, el):\n        if el.find(RDF_NS + 'Bag') is not None:\n            value = []\n            for li in el.findall(RDF_NS + 'Bag/' + RDF_NS + 'li'):\n                value.append(li.text)\n        elif el.find(RDF_NS + 'Seq') is not None:\n            value = []\n            for li in el.findall(RDF_NS + 'Seq/' + RDF_NS + 'li'):\n                value.append(li.text)\n        elif el.find(RDF_NS + 'Alt') is not None:\n            value = {}\n            for li in el.findall(RDF_NS + 'Alt/' + RDF_NS + 'li'):\n                value[li.get(XML_NS + 'lang')] = li.text\n        else:\n            value = el.text\n        return value", "response": "Extract the metadata value from an element."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert LaTeX entities to Unicode.", "response": "def latex_to_unicode(text, capitalize=False):\n    \"\"\"Replace LaTeX entities with the equivalent unicode and optionally capitalize.\n\n    :param text: The LaTeX string to be converted\n    :param capitalize: Can be 'sentence', 'name', 'title', 'upper', 'lower'\n    \"\"\"\n    if capitalize:\n        res = []\n        brac_count = 0\n        for i, c in enumerate(text):\n            if c == '{':\n                brac_count += 1\n            if c == '}':\n                brac_count -= 1\n            if brac_count > 0:\n                res.append(c)\n            elif capitalize == 'upper' or (i == 0 and not capitalize == 'lower'):\n                res.append(c.upper())\n            elif capitalize == 'sentence' and (i > 2 and text[i - 1] == ' ' and text[i - 2] == '.'):\n                res.append(c.upper())\n            elif (capitalize == 'name' and text[i - 1] in [' ', '-']) or (capitalize == 'title' and text[i - 1] == ' '):\n                nextword = text[i:].split(' ', 1)[0].rstrip(string.punctuation)\n                nextword = nextword[:1].lower() + nextword[1:] if text else ''\n                if capitalize == 'name' and nextword in NAME_SMALL:\n                    res.append(c.lower())\n                elif capitalize == 'title' and nextword in SMALL:\n                    res.append(c.lower())\n                else:\n                    res.append(c.upper())\n            elif capitalize == 'name' and c == c.upper():\n                n1 = text[i - 1] if i > 0 else None\n                n2 = text[i - 2] if i > 1 else None\n                n3 = text[i - 3] if i > 2 else None\n                n4 = text[i - 4] if i > 3 else None\n                if n2 == 'M' and n1 == 'c' and (not n3 or n3 == ' '):\n                    res.append(c)  # McCartney\n                elif n2 == 'O' and n1 == '\\'' and (not n3 or n3 == ' '):\n                    res.append(c)  # O'Boyle\n                elif n3 == 'M' and n2 == 'a' and n1 == 'c' and (not n4 or n4 == ' '):\n                    res.append(c)  # MacGarry\n                else:\n                    res.append(c.lower())\n            else:\n                res.append(c.lower())\n        text = ''.join(res)\n    if any(i in text for i in ['\\\\', '{', '}', '$', '&', '%', '#', '_']):\n        for k, v in six.iteritems(LATEX_MAPPINGS):\n            text = text.replace(k, v)\n        for k, v in six.iteritems(LATEX_SUB_MAPPINGS):\n            text = text.replace(k, v)\n        for mod in ['mathbb', 'mathbf', 'mathbit', 'mathfrak', 'mathrm', 'mathscr', 'mathsf', 'mathsfbf', 'mathsfbfsl', \n                    'mathsfsl', 'mathsl', 'mathslbb', 'mathtt']:\n            text = re.sub(r'\\\\%s\\{([\\\\\\w]+)\\}' % mod, r'\\1', text)\n        for k, v in six.iteritems(LATEX_SUB_SUB_MAPPINGS):\n            text = text.replace(k, v)\n        for k, v in six.iteritems(LATEX_COMBINING_CHARS):\n            text = re.sub(r'%s\\{?(\\w)\\}?' % k, r'\\1%s' % v, text)\n        text = re.sub(r'\\\\noopsort\\{.*?\\}', r'', text)\n        text = re.sub(r'\\\\path\\|(.*?)\\|', r'\\1', text)\n        text = re.sub(r'(?<!\\\\)[{}$]', r'', text)\n        text = re.sub(r'\\\\([{}$&_])', r'\\1', text)\n    return text"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreplaces image characters with unicode equivalents.", "response": "def replace_rsc_img_chars(document):\n    \"\"\"Replace image characters with unicode equivalents.\"\"\"\n    image_re = re.compile('http://www.rsc.org/images/entities/(?:h[23]+_)?(?:[ib]+_)?char_([0-9a-f]{4})(?:_([0-9a-f]{4}))?\\.gif')\n    for img in document.xpath('.//img[starts-with(@src, \"http://www.rsc.org/images/entities/\")]'):\n        m = image_re.match(img.get('src'))\n        if m:\n            u1, u2 = m.group(1), m.group(2)\n            if not u2 and u1 in RSC_IMG_CHARS:\n                rep = RSC_IMG_CHARS[u1]\n            else:\n                rep = ('\\\\u%s' % u1).encode('ascii').decode('unicode-escape')\n                if u2:\n                    rep += ('\\\\u%s' % u2).encode('ascii').decode('unicode-escape')\n            if img.tail is not None:\n                rep += img.tail  # Make sure we don't remove any tail text\n            parent = img.getparent()\n            if parent is not None:\n                previous = img.getprevious()\n                if previous is not None:\n                    previous.tail = (previous.tail or '') + rep\n                else:\n                    parent.text = (parent.text or '') + rep\n                parent.remove(img)\n    return document"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef space_references(document):\n    for ref in document.xpath('.//a/sup/span[@class=\"sup_ref\"]'):\n        a = ref.getparent().getparent()\n        if a is not None:\n            atail = a.tail or ''\n            if not atail.startswith(')') and not atail.startswith(',') and not atail.startswith(' '):\n                a.tail = ' ' + atail\n    return document", "response": "Ensure a space around reference links so there s a gap when they are removed."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread clusters from file and save to model file.", "response": "def load(ctx, input, output):\n    \"\"\"Read clusters from file and save to model file.\"\"\"\n    log.debug('chemdataextractor.cluster.load')\n    import pickle\n    click.echo('Reading %s' % input.name)\n    clusters = {}\n    for line in input.readlines():\n        cluster, word, freq = line.split()\n        clusters[word] = cluster\n    pickle.dump(clusters, output, protocol=pickle.HIGHEST_PROTOCOL)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef space_labels(document):\n    for label in document.xpath('.//bold'):\n        # TODO: Make this more permissive to match chemical_label in parser\n        if not label.text or not re.match('^\\(L?\\d\\d?[a-z]?\\):?$', label.text, re.I):\n            continue\n        parent = label.getparent()\n        previous = label.getprevious()\n        if previous is None:\n            text = parent.text or ''\n            if not text.endswith(' '):\n                parent.text = text + ' '\n        else:\n            text = previous.tail or ''\n            if not text.endswith(' '):\n                previous.tail = text + ' '\n        text = label.tail or ''\n        if not text.endswith(' '):\n            label.tail = text + ' '\n    return document", "response": "Ensure space around bold compound labels."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove punctuation around references like brackets commas hyphens.", "response": "def tidy_nlm_references(document):\n    \"\"\"Remove punctuation around references like brackets, commas, hyphens.\"\"\"\n\n    def strip_preceding(text):\n        stext = text.rstrip()\n        if stext.endswith('[') or stext.endswith('('):\n            #log.debug('%s -> %s' % (text, stext[:-1]))\n            return stext[:-1]\n        return text\n\n    def strip_between(text):\n        stext = text.strip()\n        if stext in {',', '-', '\\u2013', '\\u2212'}:\n            #log.debug('%s -> %s' % (text, ''))\n            return ''\n        return text\n\n    def strip_following(text):\n        stext = text.lstrip()\n        if stext.startswith(']') or stext.startswith(')'):\n            #log.debug('%s -> %s' % (text, stext[1:]))\n            return stext[1:]\n        return text\n\n    for ref in document.xpath('.//xref[@ref-type=\"bibr\"]'):\n        parent = ref.getparent()\n        previous = ref.getprevious()\n        next = ref.getnext()\n        if previous is None:\n            parent.text = strip_preceding(parent.text or '')\n        else:\n            previous.tail = strip_preceding(previous.tail or '')\n        if next is not None and next.tag == 'xref' and next.get('ref-type') == 'bibr':\n            ref.tail = strip_between(ref.tail or '')\n        ref.tail = strip_following(ref.tail or '')\n    return document"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef regex_span_tokenize(s, regex):\n    left = 0\n    for m in re.finditer(regex, s, re.U):\n        right, next = m.span()\n        if right != 0:\n            yield left, right\n        left = next\n    yield left, len(s)", "response": "Return spans that identify tokens in s split using regex."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef tokenize(self, s):\n        return [s[start:end] for start, end in self.span_tokenize(s)]", "response": "Return a list of token strings from the given sentence."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of integer offsets that identify sentences in the given text.", "response": "def span_tokenize(self, s):\n        \"\"\"Return a list of integer offsets that identify sentences in the given text.\n\n        :param string s: The text to tokenize into sentences.\n        :rtype: iter(tuple(int, int))\n        \"\"\"\n        if self._tokenizer is None:\n            self._tokenizer = load_model(self.model)\n        # for debug in tokenizer.debug_decisions(s):\n        #     log.debug(format_debug_decision(debug))\n        return self._tokenizer.span_tokenize(s)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsplitting a span into two or three separate spans at certain indices.", "response": "def _split_span(self, span, index, length=0):\n        \"\"\"Split a span into two or three separate spans at certain indices.\"\"\"\n        offset = span[1] + index if index < 0 else span[0] + index\n        # log.debug([(span[0], offset), (offset, offset + length), (offset + length, span[1])])\n        return [(span[0], offset), (offset, offset + length), (offset + length, span[1])]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _subspan(self, s, span, nextspan):\n        text = s[span[0]:span[1]]\n        lowertext = text.lower()\n\n        # Skip if only a single character or a split sequence\n        if span[1] - span[0] < 2 or text in self.SPLIT or text in self.SPLIT_END_WORD or text in self.SPLIT_START_WORD or lowertext in self.NO_SPLIT:\n            return [span]\n\n        # Skip if it looks like URL\n        if text.startswith('http://') or text.startswith('ftp://') or text.startswith('www.'):\n            return [span]\n\n        # Split full stop at end of final token (allow certain characters to follow) unless ellipsis\n        if self.split_last_stop and nextspan is None and text not in self.NO_SPLIT_STOP and not text[-3:] == '...':\n            if text[-1] == '.':\n                return self._split_span(span, -1)\n            ind = text.rfind('.')\n            if ind > -1 and all(t in '\\'\u2018\u2019\"\u201c\u201d)]}' for t in text[ind + 1:]):\n                return self._split_span(span, ind, 1)\n\n        # Split off certain sequences at the end of a word\n        for spl in self.SPLIT_END_WORD:\n            if text.endswith(spl) and len(text) > len(spl) and text[-len(spl) - 1].isalpha():\n                return self._split_span(span, -len(spl), 0)\n\n        # Split off certain sequences at the start of a word\n        for spl in self.SPLIT_START_WORD:\n            if text.startswith(spl) and len(text) > len(spl) and text[-len(spl) - 1].isalpha():\n                return self._split_span(span, len(spl), 0)\n\n        # Split around certain sequences\n        for spl in self.SPLIT:\n            ind = text.find(spl)\n            if ind > -1:\n                return self._split_span(span, ind, len(spl))\n\n        # Split around certain sequences unless followed by a digit\n        for spl in self.SPLIT_NO_DIGIT:\n            ind = text.rfind(spl)\n            if ind > -1 and (len(text) <= ind + len(spl) or not text[ind + len(spl)].isdigit()):\n                return self._split_span(span, ind, len(spl))\n\n        # Characters to split around, but with exceptions\n        for i, char in enumerate(text):\n            if char == '-':\n                before = lowertext[:i]\n                after = lowertext[i+1:]\n                # By default we split on hyphens\n                split = True\n                if before in self.NO_SPLIT_PREFIX or after in self.NO_SPLIT_SUFFIX:\n                    split = False  # Don't split if prefix or suffix in list\n                elif not before.strip(self.NO_SPLIT_CHARS) or not after.strip(self.NO_SPLIT_CHARS):\n                    split = False  # Don't split if prefix or suffix entirely consist of certain characters\n                if split:\n                    return self._split_span(span, i, 1)\n\n        # Split contraction words\n        for contraction in self.CONTRACTIONS:\n            if lowertext == contraction[0]:\n                return self._split_span(span, contraction[1])\n        return [span]", "response": "Recursively subdivides spans based on a series of rules."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the index of the closing bracket that matches the opening bracket at the start of the text.", "response": "def _closing_bracket_index(self, text, bpair=('(', ')')):\n        \"\"\"Return the index of the closing bracket that matches the opening bracket at the start of the text.\"\"\"\n        level = 1\n        for i, char in enumerate(text[1:]):\n            if char == bpair[0]:\n                level += 1\n            elif char == bpair[1]:\n                level -= 1\n            if level == 0:\n                return i + 1"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _opening_bracket_index(self, text, bpair=('(', ')')):\n        level = 1\n        for i, char in enumerate(reversed(text[:-1])):\n            if char == bpair[1]:\n                level += 1\n            elif char == bpair[0]:\n                level -= 1\n            if level == 0:\n                return len(text) - i - 2", "response": "Return the index of the opening bracket that matches the closing bracket at the end of the text."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _is_saccharide_arrow(self, before, after):\n        if (before and after and before[-1].isdigit() and after[0].isdigit() and\n            before.rstrip('0123456789').endswith('(') and after.lstrip('0123456789').startswith(')-')):\n            return True\n        else:\n            return False", "response": "Return True if the arrow is in a chemical name."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning list of every name.", "response": "def get_names(cs):\n    \"\"\"Return list of every name.\"\"\"\n    records = []\n    for c in cs:\n        records.extend(c.get('names', []))\n    return records"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns list of every label.", "response": "def get_labels(cs):\n    \"\"\"Return list of every label.\"\"\"\n    records = []\n    for c in cs:\n        records.extend(c.get('labels', []))\n    return records"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_ids(cs):\n    records = []\n    for c in cs:\n        records.append({k: c[k] for k in c if k in {'names', 'labels'}})\n    return records", "response": "Return chemical identifier records."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef memoized_property(fget):\n    attr_name = '_{}'.format(fget.__name__)\n\n    @functools.wraps(fget)\n    def fget_memoized(self):\n        if not hasattr(self, attr_name):\n            setattr(self, attr_name, fget(self))\n        return getattr(self, attr_name)\n    return property(fget_memoized)", "response": "Decorator to create memoized properties."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef python_2_unicode_compatible(klass):\n    if six.PY2:\n        if '__str__' not in klass.__dict__:\n            raise ValueError(\"Define __str__() on %s to use @python_2_unicode_compatible\" % klass.__name__)\n        if '__repr__' not in klass.__dict__:\n            raise ValueError(\"Define __repr__() on %s to use @python_2_unicode_compatible\" % klass.__name__)\n        klass.__unicode__ = klass.__str__\n        klass._unicode_repr = klass.__repr__\n        klass.__str__ = lambda self: self.__unicode__().encode('utf-8')\n        klass.__repr__ = lambda self: self._unicode_repr().encode('ascii', errors='backslashreplace')\n    return klass", "response": "Fix __str__ and __unicode__ methods under Python 2."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nscrape the value for this field from the selector.", "response": "def scrape(self, selector, cleaner=None, processor=None):\n        \"\"\"Scrape the value for this field from the selector.\"\"\"\n        value = self.entity.scrape(selector, root=self.selection, xpath=self.xpath)\n        return self._post_scrape(value, processor=processor)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef evaluate(self, gold):\n        tagged_sents = self.tag_sents([w for (w, t) in sent] for sent in gold)\n        gold_tokens = sum(gold, [])\n        test_tokens = sum(tagged_sents, [])\n        accuracy = float(sum(x == y for x, y in six.moves.zip(gold_tokens, test_tokens))) / len(test_tokens)\n        return accuracy", "response": "Evaluate the accuracy of this tagger using a gold standard corpus."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef tag(self, tokens):\n        tags = []\n        for token in tokens:\n            normalized = self.lexicon[token].normalized\n            for regex, tag in self.regexes:\n                if regex.match(normalized):\n                    tags.append((token, tag))\n                    break\n            else:\n                tags.append((token, None))\n        return tags", "response": "Return a list of tuples for a given list of tokens."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndotting - product the features and current weights and return the best label.", "response": "def predict(self, features):\n        \"\"\"Dot-product the features and current weights and return the best label.\"\"\"\n        scores = defaultdict(float)\n        for feat in features:\n            if feat not in self.weights:\n                continue\n            weights = self.weights[feat]\n            for label, weight in weights.items():\n                scores[label] += weight\n        # Do a secondary alphabetic sort, for stability\n        return max(self.classes, key=lambda label: (scores[label], label))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update(self, truth, guess, features):\n        def upd_feat(c, f, w, v):\n            param = (f, c)\n            self._totals[param] += (self.i - self._tstamps[param]) * w\n            self._tstamps[param] = self.i\n            self.weights[f][c] = w + v\n\n        self.i += 1\n        if truth == guess:\n            return None\n        for f in features:\n            weights = self.weights.setdefault(f, {})\n            upd_feat(truth, f, weights.get(truth, 0.0), 1.0)\n            upd_feat(guess, f, weights.get(guess, 0.0), -1.0)\n        return None", "response": "Update the feature weights."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\naverage weights from all iterations.", "response": "def average_weights(self):\n        \"\"\"Average weights from all iterations.\"\"\"\n        for feat, weights in self.weights.items():\n            new_feat_weights = {}\n            for clas, weight in weights.items():\n                param = (feat, clas)\n                total = self._totals[param]\n                total += (self.i - self._tstamps[param]) * weight\n                averaged = round(total / float(self.i), 3)\n                if averaged:\n                    new_feat_weights[clas] = averaged\n            self.weights[feat] = new_feat_weights\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef save(self, path):\n        with io.open(path, 'wb') as fout:\n            return pickle.dump(dict(self.weights), fout)", "response": "Save the pickled model weights."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nloading the pickled model weights.", "response": "def load(self, path):\n        \"\"\"Load the pickled model weights.\"\"\"\n        with io.open(path, 'rb') as fin:\n            self.weights = pickle.load(fin)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef tag(self, tokens):\n        # Lazy load model first time we tag\n        if not self.classes:\n            self.load(self.model)\n        prev, prev2 = self.START\n        tags = []\n        for i, token in enumerate(tokens):\n            tag = self.tagdict.get(token)\n            if not tag:\n                features = self._get_features(i, tokens, prev, prev2)\n                tag = self.perceptron.predict(features)\n            tags.append((token, tag))\n            prev2 = prev\n            prev = tag\n        return tags", "response": "Return a list of tuples for a given list of tokens."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntrain a model from a list of sentences.", "response": "def train(self, sentences, nr_iter=5):\n        \"\"\"Train a model from sentences.\n\n        :param sentences: A list of sentences, each of which is a list of (token, tag) tuples.\n        :param nr_iter: Number of training iterations.\n        \"\"\"\n        self._make_tagdict(sentences)\n        self.perceptron.classes = self.classes\n        for iter_ in range(nr_iter):\n            c = 0\n            n = 0\n            for sentence in sentences:\n                prev, prev2 = self.START\n                context = [t[0] for t in sentence]\n                for i, (token, tag) in enumerate(sentence):\n                    guess = self.tagdict.get(token)\n                    if not guess:\n                        feats = self._get_features(i, context, prev, prev2)\n                        guess = self.perceptron.predict(feats)\n                        self.perceptron.update(tag, guess, feats)\n                    prev2 = prev\n                    prev = guess\n                    c += guess == tag\n                    n += 1\n            random.shuffle(sentences)\n            log.debug('Iter %s: %s/%s=%s' % (iter_, c, n, (float(c) / n) * 100))\n        self.perceptron.average_weights()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef save(self, f):\n        return pickle.dump((self.perceptron.weights, self.tagdict, self.classes, self.clusters), f, protocol=pickle.HIGHEST_PROTOCOL)", "response": "Save pickled model to file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef tag(self, tokens):\n        # Lazy load model first time we tag\n        if not self._loaded_model:\n            self.load(self.model)\n        features = [self._get_features(tokens, i) for i in range(len(tokens))]\n        labels = self._tagger.tag(features)\n        tagged_sent = list(zip(tokens, labels))\n        return tagged_sent", "response": "Return a list of tuples for a given list of tokens."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef train(self, sentences, model):\n        trainer = pycrfsuite.Trainer(verbose=True)\n        trainer.set_params(self.params)\n        for sentence in sentences:\n            tokens, labels = zip(*sentence)\n            features = [self._get_features(tokens, i) for i in range(len(tokens))]\n            trainer.append(features, labels)\n        trainer.train(model)\n        self.load(model)", "response": "Train the CRF tagger using the specified model."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load(self, model):\n        self._dawg.load(find_data(model))\n        self._loaded_model = True", "response": "Load pickled DAWG from disk."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconstructs dictionary DAWG from tokenized words.", "response": "def build(self, words):\n        \"\"\"Construct dictionary DAWG from tokenized words.\"\"\"\n        words = [self._normalize(tokens) for tokens in words]\n        self._dawg = dawg.CompletionDAWG(words)\n        self._loaded_model = True"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nnormalizes transform to apply to both dictionary words and input tokens.", "response": "def _normalize(self, tokens):\n        \"\"\"Normalization transform to apply to both dictionary words and input tokens.\"\"\"\n        if self.case_sensitive:\n            return ' '.join(self.lexicon[t].normalized for t in tokens)\n        else:\n            return ' '.join(self.lexicon[t].lower for t in tokens)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of tuples for a given list of tokens.", "response": "def tag(self, tokens):\n        \"\"\"Return a list of (token, tag) tuples for a given list of tokens.\"\"\"\n        if not self._loaded_model:\n            self.load(self.model)\n        tags = [None] * len(tokens)\n        norm = self._normalize(tokens)\n        length = len(norm)\n        # A set of allowed indexes for matches to start or end at\n        delims = [0] + [i for span in [m.span() for m in self.delimiters.finditer(norm)] for i in span] + [length]\n        # Token indices\n        token_at_index = []\n        for i, t in enumerate(tokens):\n            token_at_index.extend([i] * (len(self.lexicon[t].normalized) + 1))\n        start_i = 0\n        end_i = 1\n        matches = {}\n        next_start = end_i\n        # TODO: This could be a little more efficient by skipping indexes forward to next delim points.\n        while True:\n            current = norm[start_i:end_i]\n            if self._dawg.has_keys_with_prefix(current):\n                # print('%s:%s:%s' % (start_i, end_i, current))\n                # If the current span is in the dawg, and isn't followed by an alphanumeric character\n                if current in self._dawg and start_i in delims and end_i in delims:\n                    # print(current)\n                    # Subsequent longer matches with same start_i will overwrite values in matches dict\n                    matches[start_i] = (start_i, end_i, current)\n                    # We can skip forward to after this match next time we increment start_i\n                    next_start = end_i\n                # Increment end_i provided we aren't already at the end of the input\n                if end_i < length:\n                    end_i += 1\n                    continue\n            # Increment start_i provided we aren't already at the end of the input\n            start_i = next_start\n            if start_i >= length - 1:\n                break\n            end_i = start_i + 1\n            next_start = end_i\n        # Apply matches as tags to the relevant tokens\n        for start_i, end_i, current in matches.values():\n            start_token = token_at_index[start_i]\n            end_token = token_at_index[end_i]\n            # Possible for match to start in 'I' token from prev match. Merge matches by not overwriting to 'B'.\n            if not tags[start_token] == 'I-%s' % self.entity:\n                tags[start_token] = 'B-%s' % self.entity\n            tags[start_token+1:end_token+1] = ['I-%s' % self.entity] * (end_token - start_token)\n        tokentags = list(zip(tokens, tags))\n        return tokentags"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef standardize_role(role):\n    role = role.lower()\n    if any(c in role for c in {'synthesis', 'give', 'yield', 'afford', 'product', 'preparation of'}):\n        return 'product'\n    return role", "response": "Convert role text into standardized form."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef list(ctx):\n    log.debug('chemdataextractor.data.list')\n    click.echo('Downloaded\\tPackage')\n    for package in PACKAGES:\n        click.echo('%s\\t%s' % (package.local_exists(), package.path))", "response": "List active data packages."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nextracts units from bracketed after nu", "response": "def extract_units(tokens, start, result):\n    \"\"\"Extract units from bracketed after nu\"\"\"\n    for e in result:\n        for child in e.iter():\n            if 'cm\u22121' in child.text:\n                return [E('units', 'cm\u22121')]\n    return []"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef find_data(path, warn=True):\n    full_path = os.path.join(get_data_dir(), path)\n    if warn and not os.path.isfile(full_path):\n        for package in PACKAGES:\n            if path == package.path:\n                log.warn('%s doesn\\'t exist. Run `cde data download` to get it.' % path)\n                break\n    return full_path", "response": "Return the absolute path to a data file within the data directory."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_model(path):\n    abspath = find_data(path)\n    cached = _model_cache.get(abspath)\n    if cached is not None:\n        log.debug('Using cached copy of %s' % path)\n        return cached\n    log.debug('Loading model %s' % path)\n    try:\n        with io.open(abspath, 'rb') as f:\n            model = six.moves.cPickle.load(f)\n    except IOError:\n        raise ModelNotFoundError('Could not load %s. Have you run `cde data download`?' % path)\n    _model_cache[abspath] = model\n    return model", "response": "Load a model from a pickle file in the data directory. Cached so model is only loaded once."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef normalize(self, text):\n        # Normalize to canonical unicode (using NFKC by default)\n        if self.form is not None:\n            text = unicodedata.normalize(self.form, text)\n\n        # Strip out any control characters (they occasionally creep in somehow)\n        for control in CONTROLS:\n            text = text.replace(control, '')\n\n        # Normalize unusual whitespace not caught by unicodedata\n        text = text.replace('\\u000b', ' ').replace('\\u000c', ' ').replace(u'\\u0085', ' ')\n        text = text.replace('\\u2028', '\\n').replace('\\u2029', '\\n').replace('\\r\\n', '\\n').replace('\\r', '\\n')\n\n        # Normalize all hyphens, minuses and dashes to ascii hyphen-minus and remove soft hyphen entirely\n        if self.hyphens:\n            # TODO: Better normalization of em/en dashes to '--' if surrounded by spaces or start/end?\n            for hyphen in HYPHENS | MINUSES:\n                text = text.replace(hyphen, '-')\n            text = text.replace('\\u00ad', '')\n\n        # Normalize all quotes and primes to ascii apostrophe and quotation mark\n        if self.quotes:\n            for double_quote in DOUBLE_QUOTES:\n                text = text.replace(double_quote, '\"')  # \\u0022\n            for single_quote in (SINGLE_QUOTES | APOSTROPHES | ACCENTS):\n                text = text.replace(single_quote, \"'\")  # \\u0027\n            text = text.replace('\u2032', \"'\")     # \\u2032 prime\n            text = text.replace('\u2035', \"'\")     # \\u2035 reversed prime\n            text = text.replace('\u2033', \"''\")    # \\u2033 double prime\n            text = text.replace('\u2036', \"''\")    # \\u2036 reversed double prime\n            text = text.replace('\u2034', \"'''\")   # \\u2034 triple prime\n            text = text.replace('\u2037', \"'''\")   # \\u2037 reversed triple prime\n            text = text.replace('\u2057', \"''''\")  # \\u2057 quadruple prime\n\n        if self.ellipsis:\n            text = text.replace('\u2026', '...').replace(' . . . ', ' ... ')  # \\u2026\n\n        if self.slashes:\n            for slash in SLASHES:\n                text = text.replace(slash, '/')\n\n        if self.tildes:\n            for tilde in TILDES:\n                text = text.replace(tilde, '~')\n\n        if self.strip:\n            text = text.strip()\n\n        # Collapse all whitespace down to a single space\n        if self.collapse:\n            text = ' '.join(text.split())\n        return text", "response": "Run the Normalizer on a string."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef normalize(self, text):\n        text = super(ChemNormalizer, self).normalize(text)\n        # Normalize element spelling\n        if self.chem_spell:\n            text = re.sub(r'sulph', r'sulf', text, flags=re.I)\n            text = re.sub(r'aluminum', r'aluminium', text, flags=re.I)\n            text = re.sub(r'cesium', r'caesium', text, flags=re.I)\n        return text", "response": "Normalize unicode hyphens whitespace and some chemistry terms and formatting."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add(self, text):\n        # logging.debug('Adding to lexicon: %s' % text)\n        if text not in self.lexemes:\n            normalized = self.normalized(text)\n            self.lexemes[text] = Lexeme(\n                text=text,\n                normalized=normalized,\n                lower=self.lower(normalized),\n                first=self.first(normalized),\n                suffix=self.suffix(normalized),\n                shape=self.shape(normalized),\n                length=self.length(normalized),\n                upper_count=self.upper_count(normalized),\n                lower_count=self.lower_count(normalized),\n                digit_count=self.digit_count(normalized),\n                is_alpha=self.is_alpha(normalized),\n                is_ascii=self.is_ascii(normalized),\n                is_digit=self.is_digit(normalized),\n                is_lower=self.is_lower(normalized),\n                is_upper=self.is_upper(normalized),\n                is_title=self.is_title(normalized),\n                is_punct=self.is_punct(normalized),\n                is_hyphenated=self.is_hyphenated(normalized),\n                like_url=self.like_url(normalized),\n                like_number=self.like_number(normalized),\n                cluster=self.cluster(normalized)\n            )", "response": "Add a text to the lexicon."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting Table element to python dictionary.", "response": "def serialize(self):\n        \"\"\"Convert Table element to python dictionary.\"\"\"\n        data = {\n            'type': self.__class__.__name__,\n            'caption': self.caption.serialize(),\n            'headings': [[cell.serialize() for cell in hrow] for hrow in self.headings],\n            'rows': [[cell.serialize() for cell in row] for row in self.rows],\n        }\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef records(self):\n        caption_records = self.caption.records\n        # Parse headers to extract contextual data and determine value parser for the column\n        value_parsers = {}\n        header_compounds = defaultdict(list)\n        table_records = ModelList()\n        seen_compound_col = False\n        log.debug('Parsing table headers')\n\n        for i, col_headings in enumerate(zip(*self.headings)):\n            # log.info('Considering column %s' % i)\n            for parsers in self.parsers:\n                log.debug(parsers)\n                heading_parser = parsers[0]\n                value_parser = parsers[1] if len(parsers) > 1 else None\n                disallowed_parser = parsers[2] if len(parsers) > 2 else None\n                allowed = False\n                disallowed = False\n                for cell in col_headings:\n                    log.debug(cell.tagged_tokens)\n                    results = list(heading_parser.parse(cell.tagged_tokens))\n                    if results:\n                        allowed = True\n                        log.debug('Heading column %s: Match %s: %s' % (i, heading_parser.__class__.__name__, [c.serialize() for c in results]))\n                    # Results from every parser are stored as header compounds\n                    header_compounds[i].extend(results)\n                    # Referenced footnote records are also stored\n                    for footnote in self.footnotes:\n                        # print('%s - %s - %s' % (footnote.id, cell.references, footnote.id in cell.references))\n                        if footnote.id in cell.references:\n                            log.debug('Adding footnote %s to column %s: %s' % (footnote.id, i, [c.serialize() for c in footnote.records]))\n                            # print('Footnote records: %s' % [c.to_primitive() for c in footnote.records])\n                            header_compounds[i].extend(footnote.records)\n                    # Check if the disallowed parser matches this cell\n                    if disallowed_parser and list(disallowed_parser.parse(cell.tagged_tokens)):\n                        log.debug('Column %s: Disallowed %s' % (i, heading_parser.__class__.__name__))\n                        disallowed = True\n                # If heading parser matches and disallowed parser doesn't, store the value parser\n                if allowed and not disallowed and value_parser and i not in value_parsers:\n                    if isinstance(value_parser, CompoundCellParser):\n                        # Only take the first compound col\n                        if seen_compound_col:\n                            continue\n                        seen_compound_col = True\n                    log.debug('Column %s: Value parser: %s' % (i, value_parser.__class__.__name__))\n                    value_parsers[i] = value_parser\n                    # Stop after value parser is assigned?\n\n        # for hrow in self.headings:\n        #     for i, cell in enumerate(hrow):\n        #         log.debug(cell.tagged_tokens)\n        #         for heading_parser, value_parser in self.parsers:\n        #             results = list(heading_parser.parse(cell.tagged_tokens))\n        #             if results:\n        #                 log.debug('Heading column %s: Match %s: %s' % (i, heading_parser.__class__.__name__, [c.to_primitive() for c in results]))\n        #             # Results from every parser are stored as header compounds\n        #             header_compounds[i].extend(results)\n        #             if results and value_parser and i not in value_parsers:\n        #                 if isinstance(value_parser, CompoundCellParser):\n        #                     # Only take the first compound col\n        #                     if seen_compound_col:\n        #                         continue\n        #                     seen_compound_col = True\n        #                 value_parsers[i] = value_parser\n        #                 break  # Stop after first heading parser matches\n        #         # Referenced footnote records are also stored\n        #         for footnote in self.footnotes:\n        #             # print('%s - %s - %s' % (footnote.id, cell.references, footnote.id in cell.references))\n        #             if footnote.id in cell.references:\n        #                 log.debug('Adding footnote %s to column %s: %s' % (footnote.id, i, [c.to_primitive() for c in footnote.records]))\n        #                 # print('Footnote records: %s' % [c.to_primitive() for c in footnote.records])\n        #                 header_compounds[i].extend(footnote.records)\n\n        # If no parsers, skip processing table\n        if value_parsers:\n\n            # If no CompoundCellParser() in value_parsers and value_parsers[0] == [] then set CompoundCellParser()\n            if not seen_compound_col and 0 not in value_parsers:\n                log.debug('No compound column found in table, assuming first column')\n                value_parsers[0] = CompoundCellParser()\n\n            for row in self.rows:\n                row_compound = Compound()\n                # Keep cell records that are contextual to merge at the end\n                contextual_cell_compounds = []\n                for i, cell in enumerate(row):\n                    log.debug(cell.tagged_tokens)\n                    if i in value_parsers:\n                        results = list(value_parsers[i].parse(cell.tagged_tokens))\n                        if results:\n                            log.debug('Cell column %s: Match %s: %s' % (i, value_parsers[i].__class__.__name__, [c.serialize() for c in results]))\n                        # For each result, merge in values from elsewhere\n                        for result in results:\n                            # Merge each header_compounds[i]\n                            for header_compound in header_compounds[i]:\n                                if header_compound.is_contextual:\n                                    result.merge_contextual(header_compound)\n                            # Merge footnote compounds\n                            for footnote in self.footnotes:\n                                if footnote.id in cell.references:\n                                    for footnote_compound in footnote.records:\n                                        result.merge_contextual(footnote_compound)\n                            if result.is_contextual:\n                                # Don't merge cell as a value compound if there are no values\n                                contextual_cell_compounds.append(result)\n                            else:\n                                row_compound.merge(result)\n                # Merge contextual information from cells\n                for contextual_cell_compound in contextual_cell_compounds:\n                    row_compound.merge_contextual(contextual_cell_compound)\n                # If no compound name/label, try take from previous row\n                if not row_compound.names and not row_compound.labels and table_records:\n                    prev = table_records[-1]\n                    row_compound.names = prev.names\n                    row_compound.labels = prev.labels\n                # Merge contextual information from caption into the full row\n                for caption_compound in caption_records:\n                    if caption_compound.is_contextual:\n                        row_compound.merge_contextual(caption_compound)\n                # And also merge from any footnotes that are referenced from the caption\n                for footnote in self.footnotes:\n                    if footnote.id in self.caption.references:\n                        # print('Footnote records: %s' % [c.to_primitive() for c in footnote.records])\n                        for fn_compound in footnote.records:\n                            row_compound.merge_contextual(fn_compound)\n\n                log.debug(row_compound.serialize())\n                if row_compound.serialize():\n                    table_records.append(row_compound)\n\n        # TODO: If no rows have name or label, see if one is in the caption\n\n        # Include non-contextual caption records in the final output\n        caption_records = [c for c in caption_records if not c.is_contextual]\n        table_records += caption_records\n        return table_records", "response": "Return a list of chemical records that have been parsed from the table."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmerge data from another Compound into this Compound.", "response": "def merge(self, other):\n        \"\"\"Merge data from another Compound into this Compound.\"\"\"\n        log.debug('Merging: %s and %s' % (self.serialize(), other.serialize()))\n        for k in self.keys():\n            for new_item in other[k]:\n                if new_item not in self[k]:\n                    self[k].append(new_item)\n        log.debug('Result: %s' % self.serialize())\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmerge in contextual info from a template Compound.", "response": "def merge_contextual(self, other):\n        \"\"\"Merge in contextual info from a template Compound.\"\"\"\n        # TODO: This is currently dependent on our data model? Make more robust to schema changes\n        # Currently we assume all lists at Compound level, with 1 further potential nested level of lists\n        for k in self.keys():\n            # print('key: %s' % k)\n            for item in self[k]:\n                # print('item: %s' % item)\n                for other_item in other.get(k, []):\n                    # Skip text properties (don't merge names, labels, roles)\n                    if isinstance(other_item, six.text_type):\n                        continue\n                    for otherk in other_item.keys():\n                        if isinstance(other_item[otherk], list):\n                            if len(other_item[otherk]) > 0 and len(item[otherk]) > 0:\n                                other_nested_item = other_item[otherk][0]\n                                for othernestedk in other_nested_item.keys():\n                                    for nested_item in item[otherk]:\n                                        if not nested_item[othernestedk]:\n                                            nested_item[othernestedk] = other_nested_item[othernestedk]\n                        elif not item[otherk]:\n                            item[otherk] = other_item[otherk]\n        log.debug('Result: %s' % self.serialize())\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning True if identifier information only.", "response": "def is_id_only(self):\n        \"\"\"Return True if identifier information only.\"\"\"\n        for key, value in self.items():\n            if key not in {'names', 'labels', 'roles'} and value:\n                return False\n        if self.names or self.labels:\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef join(tokens, start, result):\n    texts = []\n    if len(result) > 0:\n        for e in result:\n            for child in e.iter():\n                if child.text is not None:\n                    texts.append(child.text)\n        return [E(result[0].tag, ' '.join(texts))]", "response": "Join tokens into a single string with spaces between."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef strip_stop(tokens, start, result):\n    for e in result:\n        for child in e.iter():\n            if child.text.endswith('.'):\n                child.text = child.text[:-1]\n    return result", "response": "Remove trailing full stop from tokens."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfix whitespace around hyphens and commas. Can be used to remove whitespace tokenization artefacts.", "response": "def fix_whitespace(tokens, start, result):\n    \"\"\"Fix whitespace around hyphens and commas. Can be used to remove whitespace tokenization artefacts.\"\"\"\n    for e in result:\n        for child in e.iter():\n            child.text = child.text.replace(' , ', ', ')\n            for hyphen in HYPHENS:\n                child.text = child.text.replace(' %s ' % hyphen, '%s' % hyphen)\n            child.text = re.sub(r'- (.) -', r'-\\1-', child.text)\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndetecting if a file is at most files.", "response": "def detect(self, fstring, fname=None):\n        \"\"\"Have a stab at most files.\"\"\"\n        if fname is not None and '.' in fname:\n            extension = fname.rsplit('.', 1)[1]\n            if extension in {'pdf', 'html', 'xml'}:\n                return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _process_layout(self, layout):\n        # Here we just group text into paragraphs\n        elements = []\n        for lt_obj in layout:\n            if isinstance(lt_obj, LTTextBox) or isinstance(lt_obj, LTTextLine):\n                elements.append(Paragraph(lt_obj.get_text().strip()))\n            elif isinstance(lt_obj, LTFigure):\n                # Recursive...\n                elements.extend(self._process_layout(lt_obj))\n        return elements", "response": "Process an LTPage layout and return a list of elements."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_encoding(input_string, guesses=None, is_html=False):\n    converted = UnicodeDammit(input_string, override_encodings=[guesses] if guesses else [], is_html=is_html)\n    return converted.original_encoding", "response": "Returns the encoding of a byte string. Uses bs4 UnicodeDammit."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef levenshtein(s1, s2, allow_substring=False):\n    len1, len2 = len(s1), len(s2)\n    lev = []\n    for i in range(len1 + 1):\n        lev.append([0] * (len2 + 1))\n    for i in range(len1 + 1):\n        lev[i][0] = i\n    for j in range(len2 + 1):\n        lev[0][j] = 0 if allow_substring else j\n    for i in range(len1):\n        for j in range(len2):\n            lev[i + 1][j + 1] = min(lev[i][j + 1] + 1, lev[i + 1][j] + 1, lev[i][j] + (s1[i] != s2[j]))\n    return min(lev[len1]) if allow_substring else lev[len1][len2]", "response": "Returns the Levenshtein distance between two strings."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef bracket_level(text, open={'(', '[', '{'}, close={')', ']', '}'}):\n    level = 0\n    for c in text:\n        if c in open:\n            level += 1\n        elif c in close:\n            level -= 1\n    return level", "response": "Return the number of brackets in a string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nlist all config values.", "response": "def list(ctx):\n    \"\"\"List all config values.\"\"\"\n    log.debug('chemdataextractor.config.list')\n    for k in config:\n        click.echo('%s : %s' % (k, config[k]))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntrains CRF CEM recognizer.", "response": "def train_crf(ctx, input, output, clusters):\n    \"\"\"Train CRF CEM recognizer.\"\"\"\n    click.echo('chemdataextractor.crf.train')\n    sentences = []\n    for line in input:\n        sentence = []\n        for t in line.split():\n            token, tag, iob = t.rsplit('/', 2)\n            sentence.append(((token, tag), iob))\n        if sentence:\n            sentences.append(sentence)\n\n    tagger = CrfCemTagger(clusters=clusters)\n    tagger.train(sentences, output)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a list of Sentences that make up this text passage.", "response": "def sentences(self):\n        \"\"\"Return a list of Sentences that make up this text passage.\"\"\"\n        sents = []\n        spans = self.sentence_tokenizer.span_tokenize(self.text)\n        for span in spans:\n            sent = Sentence(\n                text=self.text[span[0]:span[1]],\n                start=span[0],\n                end=span[1],\n                word_tokenizer=self.word_tokenizer,\n                lexicon=self.lexicon,\n                abbreviation_detector=self.abbreviation_detector,\n                pos_tagger=self.pos_tagger,\n                ner_tagger=self.ner_tagger,\n                parsers=self.parsers,\n                document=self.document\n            )\n            sents.append(sent)\n        return sents"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of records for this text passage.", "response": "def records(self):\n        \"\"\"Return a list of records for this text passage.\"\"\"\n        return ModelList(*[r for sent in self.sentences for r in sent.records])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef tokens(self):\n        spans = self.word_tokenizer.span_tokenize(self.text)\n        toks = [Token(\n            text=self.text[span[0]:span[1]],\n            start=span[0] + self.start,\n            end=span[1] + self.start,\n            lexicon=self.lexicon\n        ) for span in spans]\n        return toks", "response": "Return a list of token Spans for this sentence."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list of tuples that contain the abbreviation long and ner_tag.", "response": "def abbreviation_definitions(self):\n        \"\"\"Return a list of (abbreviation, long, ner_tag) tuples.\"\"\"\n        abbreviations = []\n        if self.abbreviation_detector:\n            # log.debug('Detecting abbreviations')\n            ners = self.unprocessed_ner_tags\n            for abbr_span, long_span in self.abbreviation_detector.detect_spans(self.raw_tokens):\n                abbr = self.raw_tokens[abbr_span[0]:abbr_span[1]]\n                long = self.raw_tokens[long_span[0]:long_span[1]]\n                # Check if long is entirely tagged as one named entity type\n                long_tags = ners[long_span[0]:long_span[1]]\n                unique_tags = set([tag[2:] for tag in long_tags if tag is not None])\n                tag = long_tags[0][2:] if None not in long_tags and len(unique_tags) == 1 else None\n                abbreviations.append((abbr, long, tag))\n        return abbreviations"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef tags(self):\n        tags = self.pos_tags\n        for i, tag in enumerate(self.ner_tags):\n            if tag is not None:\n                tags[i] = tag\n        return tags", "response": "Return combined POS and NER tags."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of records for this sentence.", "response": "def records(self):\n        \"\"\"Return a list of records for this sentence.\"\"\"\n        compounds = ModelList()\n        seen_labels = set()\n        # Ensure no control characters are sent to a parser (need to be XML compatible)\n        tagged_tokens = [(CONTROL_RE.sub('', token), tag) for token, tag in self.tagged_tokens]\n        for parser in self.parsers:\n            for record in parser.parse(tagged_tokens):\n                p = record.serialize()\n                if not p:  # TODO: Potential performance issues?\n                    continue\n                # Skip duplicate records\n                if record in compounds:\n                    continue\n                # Skip just labels that have already been seen (bit of a hack)\n                if all(k in {'labels', 'roles'} for k in p.keys()) and set(record.labels).issubset(seen_labels):\n                    continue\n                seen_labels.update(record.labels)\n                compounds.append(record)\n        return compounds"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef prepare_gold(ctx, annotations, gout):\n    click.echo('chemdataextractor.chemdner.prepare_gold')\n    for line in annotations:\n        pmid, ta, start, end, text, category = line.strip().split('\\t')\n        gout.write('%s\\t%s:%s:%s\\n' % (pmid, ta, start, end))", "response": "Prepare bc - evaluate gold file from annotations supplied by CHEMDNER."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef prepare_tokens(ctx, input, annotations, tout, lout):\n    click.echo('chemdataextractor.chemdner.prepare_tokens')\n    # Collect the annotations into a dict\n    anndict = defaultdict(list)\n    for line in annotations:\n        pmid, ta, start, end, text, category = line.strip().split('\\t')\n        anndict[(pmid, ta)].append((int(start), int(end), text))\n    # Process the corpus\n    for line in input:\n        pmid, title, abstract = line.strip().split(u'\\t')\n        for t, section, anns in [(Title(title), 'T', anndict.get((pmid, u'T'), [])), (Paragraph(abstract), u'A', anndict.get((pmid, u'A'), []))]:\n            # Write our tokens with POS and IOB tags\n            tagged = _prep_tags(t, anns)\n            for i, sentence in enumerate(tagged):\n                tout.write(u' '.join([u'/'.join([token, tag, label]) for token, tag, label in sentence]))\n                lout.write(u' '.join([u'/'.join([token, label]) for token, tag, label in sentence]))\n                tout.write(u'\\n')\n                lout.write(u'\\n')\n            tout.write(u'\\n')\n            lout.write(u'\\n')", "response": "Prepare tokenized and tagged corpus file from those supplied by CHEMDNER."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\napplies IOB chemical entity tags and POS tags to text.", "response": "def _prep_tags(t, annotations):\n    \"\"\"Apply IOB chemical entity tags and POS tags to text.\"\"\"\n    tags = [['O' for _ in sent.tokens] for sent in t.sentences]\n    for start, end, text in annotations:\n        done_first = False\n        for i, sent in enumerate(t.sentences):\n            for j, token in enumerate(sent.tokens):\n                if start <= token.start < end or start < token.end <= end:\n                    # Token start or end occurs within the annotation\n                    tags[i][j] = 'I-CM' if done_first else 'B-CM'\n                    done_first = True\n    tagged = [[(token[0], token[1], tags[i][j]) for j, token in enumerate(sentence.pos_tagged_tokens)] for i, sentence in enumerate(t.sentences)]\n    return tagged"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef tag(ctx, corpus, output):\n    click.echo('chemdataextractor.chemdner.tag')\n    for line in corpus:\n        pmid, title, abstract = line.strip().split(u'\\t')\n        # print(pmid)\n        counter = 1\n        d = Document(Title(title), Paragraph(abstract))\n        for t, section in [(d.elements[0], u'T'), (d.elements[1], u'A')]:\n            for cem in t.cems:\n                code = u'%s:%s:%s' % (section, cem.start, cem.end)\n                output.write(u'\\t'.join([pmid, code, six.text_type(counter), u'1']))\n                output.write(u'\\n')\n                counter += 1", "response": "Tag chemical entities and write CHEMDNER annotations predictions file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntrain POS tagger on WSJ GENIA and both. With and without cluster features.", "response": "def train_all(ctx, output):\n    \"\"\"Train POS tagger on WSJ, GENIA, and both. With and without cluster features.\"\"\"\n    click.echo('chemdataextractor.pos.train_all')\n    click.echo('Output: %s' % output)\n    ctx.invoke(train, output='%s_wsj_nocluster.pickle' % output, corpus='wsj', clusters=False)\n    ctx.invoke(train, output='%s_wsj.pickle' % output, corpus='wsj', clusters=True)\n    ctx.invoke(train, output='%s_genia_nocluster.pickle' % output, corpus='genia', clusters=False)\n    ctx.invoke(train, output='%s_genia.pickle' % output, corpus='genia', clusters=True)\n    ctx.invoke(train, output='%s_wsj_genia_nocluster.pickle' % output, corpus='wsj+genia', clusters=False)\n    ctx.invoke(train, output='%s_wsj_genia.pickle' % output, corpus='wsj+genia', clusters=True)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef evaluate_all(ctx, model):\n    click.echo('chemdataextractor.pos.evaluate_all')\n    click.echo('Model: %s' % model)\n    ctx.invoke(evaluate, model='%s_wsj_nocluster.pickle' % model, corpus='wsj', clusters=False)\n    ctx.invoke(evaluate, model='%s_wsj_nocluster.pickle' % model, corpus='genia', clusters=False)\n    ctx.invoke(evaluate, model='%s_wsj.pickle' % model, corpus='wsj', clusters=True)\n    ctx.invoke(evaluate, model='%s_wsj.pickle' % model, corpus='genia', clusters=True)\n    ctx.invoke(evaluate, model='%s_genia_nocluster.pickle' % model, corpus='wsj', clusters=False)\n    ctx.invoke(evaluate, model='%s_genia_nocluster.pickle' % model, corpus='genia', clusters=False)\n    ctx.invoke(evaluate, model='%s_genia.pickle' % model, corpus='wsj', clusters=True)\n    ctx.invoke(evaluate, model='%s_genia.pickle' % model, corpus='genia', clusters=True)\n    ctx.invoke(evaluate, model='%s_wsj_genia_nocluster.pickle' % model, corpus='wsj', clusters=False)\n    ctx.invoke(evaluate, model='%s_wsj_genia_nocluster.pickle' % model, corpus='genia', clusters=False)\n    ctx.invoke(evaluate, model='%s_wsj_genia.pickle' % model, corpus='wsj', clusters=True)\n    ctx.invoke(evaluate, model='%s_wsj_genia.pickle' % model, corpus='genia', clusters=True)", "response": "Evaluate POS taggers on WSJ and GENIA."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef evaluate(ctx, model, corpus, clusters):\n    click.echo('chemdataextractor.pos.evaluate')\n    if corpus == 'wsj':\n        evaluation = wsj_evaluation\n        sents = list(evaluation.tagged_sents())\n        for i, wsj_sent in enumerate(sents):\n            sents[i] = [t for t in wsj_sent if not t[1] == '-NONE-']\n    elif corpus == 'genia':\n        evaluation = genia_evaluation\n        sents = list(evaluation.tagged_sents())\n        # Translate GENIA bracket tags\n        for i, genia_sent in enumerate(sents):\n            for j, (token, tag) in enumerate(genia_sent):\n                if tag == '(':\n                    sents[i][j] = (token, '-LRB-')\n                elif tag == ')':\n                    sents[i][j] = (token, '-RRB-')\n    else:\n        raise click.ClickException('Invalid corpus')\n    tagger = ChemCrfPosTagger(model=model, clusters=clusters)\n    accuracy = tagger.evaluate(sents)\n    click.echo('%s on %s: %s' % (model, evaluation, accuracy))", "response": "Evaluate performance of POS Tagger."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef evaluate_perceptron(ctx, model, corpus):\n    click.echo('chemdataextractor.pos.evaluate')\n    if corpus == 'wsj':\n        evaluation = wsj_evaluation\n        sents = list(evaluation.tagged_sents())\n        for i, wsj_sent in enumerate(sents):\n            sents[i] = [t for t in wsj_sent if not t[1] == u'-NONE-']\n    elif corpus == 'genia':\n        evaluation = genia_evaluation\n        sents = list(evaluation.tagged_sents())\n        # Translate GENIA bracket tags\n        for i, genia_sent in enumerate(sents):\n            for j, (token, tag) in enumerate(genia_sent):\n                if tag == u'(':\n                    sents[i][j] = (token, u'-LRB-')\n                elif tag == u')':\n                    sents[i][j] = (token, u'-RRB-')\n    else:\n        raise click.ClickException('Invalid corpus')\n    tagger = ChemApPosTagger(model=model)\n    accuracy = tagger.evaluate(sents)\n    click.echo('%s on %s: %s' % (model, evaluation, accuracy))", "response": "Evaluate performance of Averaged Perceptron POS Tagger."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntagging POS - tagged tokens.", "response": "def tag(ctx, input, output):\n    \"\"\"Output POS-tagged tokens.\"\"\"\n    log.info('chemdataextractor.pos.tag')\n    log.info('Reading %s' % input.name)\n    doc = Document.from_file(input)\n    for element in doc.elements:\n        if isinstance(element, Text):\n            for sentence in element.sentences:\n                output.write(u' '.join(u'/'.join([token, tag]) for token, tag in sentence.pos_tagged_tokens))\n                output.write(u'\\n')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_features(self, i, context, prev, prev2):\n        w = self.lexicon[context[i]]\n        features = [\n            'bias',\n            'w:shape=%s' % w.shape,\n            'w:lower=%s' % w.lower,\n            'p1:tag=%s' % prev,\n            'p2:tag=%s' % prev2,\n            'p1:tag+w:lower=%s+%s' % (prev, w.lower),\n            'p1:tag+p2:tag=%s+%s' % (prev, prev2),\n        ]\n        if w.like_number:\n            features.append('w:like_number')\n        elif w.is_punct:\n            features.append('w:is_punct')\n        elif w.like_url:\n            features.append('w:like_url')\n        else:\n            features.extend([\n                'w:suffix2=%s' % w.lower[-2:],\n                'w:suffix3=%s' % w.lower[-3:],\n                'w:suffix4=%s' % w.lower[-4:],\n                'w:suffix5=%s' % w.lower[-5:],\n                'w:prefix1=%s' % w.lower[:1],\n                'w:prefix2=%s' % w.lower[:2],\n                'w:prefix3=%s' % w.lower[:3],\n            ])\n            if w.is_alpha:\n                features.append('w:is_alpha')\n            elif w.is_hyphenated:\n                features.append('w:is_hyphenated')\n            if w.is_upper:\n                features.append('w:is_upper')\n            elif w.is_lower:\n                features.append('w:is_lower')\n            elif w.is_title:\n                features.append('w:is_title')\n        if self.clusters and w.cluster:\n            features.extend([\n                'w:cluster4=%s' % w.cluster[:4],\n                'w:cluster6=%s' % w.cluster[:6],\n                'w:cluster10=%s' % w.cluster[:10],\n                'w:cluster20=%s' % w.cluster[:20],\n            ])\n        # Add features for previous tokens if present\n        if i > 0:\n            p1 = self.lexicon[context[i-1]]\n            features.extend([\n                'p1:lower=%s' % p1.lower,\n                'p1:shape=%s' % p1.shape,\n            ])\n            if not (p1.like_number or p1.is_punct or p1.like_url):\n                features.append('p1:suffix3=%s' % p1.lower[-3:])\n            if self.clusters and p1.cluster:\n                features.extend([\n                    'p1:cluster4=%s' % p1.cluster[:4],\n                    'p1:cluster6=%s' % p1.cluster[:6],\n                    'p1:cluster10=%s' % p1.cluster[:10],\n                    'p1:cluster20=%s' % p1.cluster[:20],\n                ])\n            if i > 1:\n                p2 = self.lexicon[context[i-2]]\n                features.extend([\n                    'p2:lower=%s' % p2.lower,\n                    'p2:shape=%s' % p2.shape,\n                ])\n                if self.clusters and p2.cluster:\n                    features.extend([\n                        'p2:cluster4=%s' % p2.cluster[:4],\n                        'p2:cluster6=%s' % p2.cluster[:6],\n                        'p2:cluster10=%s' % p2.cluster[:10],\n                        'p2:cluster20=%s' % p2.cluster[:20],\n                    ])\n        # Add features for next tokens if present\n        end = len(context) - 1\n        if i < end:\n            n1 = self.lexicon[context[i+1]]\n            features.extend([\n                'n1:lower=%s' % n1.lower,\n                'n1:shape=%s' % n1.shape\n            ])\n            if not (n1.like_number or n1.is_punct or n1.like_url):\n                features.append('n1:suffix3=%s' % n1.lower[-3:])\n            if self.clusters and n1.cluster:\n                features.extend([\n                    'n1:cluster4=%s' % n1.cluster[:4],\n                    'n1:cluster6=%s' % n1.cluster[:6],\n                    'n1:cluster10=%s' % n1.cluster[:10],\n                    'n1:cluster20=%s' % n1.cluster[:20],\n                ])\n            if i < end - 1:\n                n2 = self.lexicon[context[i+2]]\n                features.extend([\n                    'n2:lower=%s' % n2.lower,\n                    'n2:shape=%s' % n2.shape\n                ])\n                if self.clusters and n2.cluster:\n                    features.extend([\n                        'n2:cluster4=%s' % n2.cluster[:4],\n                        'n2:cluster6=%s' % n2.cluster[:6],\n                        'n2:cluster10=%s' % n2.cluster[:10],\n                        'n2:cluster20=%s' % n2.cluster[:20],\n                    ])\n        # Add position features\n        if i == 0:\n            features.append('-firsttoken-')\n        elif i == 1:\n            features.append('-secondtoken-')\n        elif i == end - 1:\n            features.append('-secondlasttoken-')\n        elif i == end:\n            features.append('-lasttoken-')\n        return features", "response": "Map tokens into a feature representation."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmake a HTTP GET request.", "response": "def make_request(self, session, url, **kwargs):\n        \"\"\"Make a HTTP GET request.\n\n        :param url: The URL to get.\n        :returns: The response to the request.\n        :rtype: requests.Response\n        \"\"\"\n        log.debug('Making request: GET %s %s' % (url, kwargs))\n        return session.get(url, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef make_request(self, session, url, **kwargs):\n        log.debug('Making request: POST %s %s' % (url, kwargs))\n        return session.post(url, **kwargs)", "response": "Make a HTTP POST request."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrequesting URL scrape response and return an EntityList.", "response": "def run(self, url):\n        \"\"\"Request URL, scrape response and return an EntityList.\"\"\"\n        url = self.process_url(url)\n        if not url:\n            return\n        response = self.make_request(self.http, url)\n        selector = self.process_response(response)\n        entities = []\n        for root in self.get_roots(selector):\n            entity = self.entity(root)\n            entity = self.process_entity(entity)\n            if entity:\n                entities.append(entity)\n        return EntityList(*entities)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\napplying Cleaner to HTML string or document and return a cleaned string or document.", "response": "def clean_html(self, html):\n        \"\"\"Apply ``Cleaner`` to HTML string or document and return a cleaned string or document.\"\"\"\n        result_type = type(html)\n        if isinstance(html, six.string_types):\n            doc = html_fromstring(html)\n        else:\n            doc = copy.deepcopy(html)\n        self(doc)\n        if issubclass(result_type, six.binary_type):\n            return tostring(doc, encoding='utf-8')\n        elif issubclass(result_type, six.text_type):\n            return tostring(doc, encoding='unicode')\n        else:\n            return doc"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\napplies Cleaner to markup string or document and return a cleaned string or document.", "response": "def clean_markup(self, markup, parser=None):\n        \"\"\"Apply ``Cleaner`` to markup string or document and return a cleaned string or document.\"\"\"\n        result_type = type(markup)\n        if isinstance(markup, six.string_types):\n            doc = fromstring(markup, parser=parser)\n        else:\n            doc = copy.deepcopy(markup)\n        self(doc)\n        if issubclass(result_type, six.binary_type):\n            return tostring(doc, encoding='utf-8')\n        elif issubclass(result_type, six.text_type):\n            return tostring(doc, encoding='unicode')\n        else:\n            return doc"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef floats(s):\n    try:\n        return float(s)\n    except ValueError:\n        s = re.sub(r'(\\d)\\s*\\(\\d+(\\.\\d+)?\\)', r'\\1', s)        # Remove bracketed numbers from end\n        s = re.sub(r'(\\d)\\s*\u00b1\\s*\\d+(\\.\\d+)?', r'\\1', s)        # Remove uncertainties from end\n        s = s.rstrip('\\'\"+-=<>/,.:;!?)]}\u2026\u223c~\u2248\u00d7*_\u2265\u2264')             # Remove trailing punctuation\n        s = s.lstrip('\\'\"+=<>/([{\u223c~\u2248\u00d7*_\u2265\u2264\u00a3$\u20ac#\u00a7')                # Remove leading punctuation\n        s = s.replace(',', '')                                 # Remove commas\n        s = ''.join(s.split())                                  # Strip whitespace\n        s = re.sub(r'(\\d)\\s*[\u00d7x]\\s*10\\^?(-?\\d)', r'\\1e\\2', s)  # Convert scientific notation\n        return float(s)", "response": "Convert string to float. Handles more string formats that the standard python conversion is used."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef strip_querystring(url):\n    p = six.moves.urllib.parse.urlparse(url)\n    return p.scheme + \"://\" + p.netloc + p.path", "response": "Remove the querystring from the end of a URL."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef extract_emails(text):\n    text = text.replace(u'\\u2024', '.')\n    emails = []\n    for m in EMAIL_RE.findall(text):\n        emails.append(m[0])\n    return emails", "response": "Return a list of email addresses extracted from the string."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef unapostrophe(text):\n    text = re.sub(r'[%s]s?$' % ''.join(APOSTROPHES), '', text)\n    return text", "response": "Strip apostrophe and s from the end of a string."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the time in the local timezone from date and time", "response": "def getLocalTime(date, time, *args, **kwargs):\n    \"\"\"\n    Get the time in the local timezone from date and time\n    \"\"\"\n    if time is not None:\n        return getLocalDateAndTime(date, time, *args, **kwargs)[1]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the date and time in the local timezone from date and optionally time", "response": "def getLocalDateAndTime(date, time, *args, **kwargs):\n    \"\"\"\n    Get the date and time in the local timezone from date and optionally time\n    \"\"\"\n    localDt = getLocalDatetime(date, time, *args, **kwargs)\n    if time is not None:\n        return (localDt.date(), localDt.timetz())\n    else:\n        return (localDt.date(), None)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a datetime in the local timezone from date and optionally time", "response": "def getLocalDatetime(date, time, tz=None, timeDefault=dt.time.max):\n    \"\"\"\n    Get a datetime in the local timezone from date and optionally time\n    \"\"\"\n    localTZ = timezone.get_current_timezone()\n    if tz is None or tz == localTZ:\n        localDt = getAwareDatetime(date, time, tz, timeDefault)\n    else:\n        # create in event's time zone\n        eventDt = getAwareDatetime(date, time, tz, timeDefault)\n        # convert to local time zone\n        localDt = eventDt.astimezone(localTZ)\n        if time is None:\n            localDt = getAwareDatetime(localDt.date(), None, localTZ, timeDefault)\n    return localDt"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a datetime in the given timezone from date and optionally time.", "response": "def getAwareDatetime(date, time, tz, timeDefault=dt.time.max):\n    \"\"\"\n    Get a datetime in the given timezone from date and optionally time.\n    If time is not given it will default to timeDefault if that is given\n    or if not then to the end of the day.\n    \"\"\"\n    if time is None:\n        time = timeDefault\n    datetime = dt.datetime.combine(date, time)\n    # arbitary rule to handle DST transitions:\n    # if daylight savings causes an error then use standard time\n    datetime = timezone.make_aware(datetime, tz, is_dst=False)\n    return datetime"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nformatting the times time_from and optionally time_to", "response": "def timeFormat(time_from, time_to=None, prefix=\"\", infix=None):\n    \"\"\"\n    Format the times time_from and optionally time_to, e.g. 10am\n    \"\"\"\n    retval = \"\"\n    if time_from != \"\" and time_from is not None:\n        retval += prefix\n        retval += dateformat.time_format(time_from, \"fA\").lower()\n    if time_to != \"\" and time_to is not None:\n        to = format(dateformat.time_format(time_to, \"fA\").lower())\n        if infix is not None:\n            retval = \"{} {} {}\".format(retval, infix, to)\n        else:\n            retval = _(\"{fromTime} to {toTime}\").format(fromTime=retval,\n                                                        toTime=to)\n    return retval.strip()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dateFormat(when):\n    retval = \"\"\n    if when is not None:\n        dow = dateformat.format(when, \"l\")\n        dom = dateformat.format(when, \"jS\")\n        month = dateformat.format(when, \"F\")\n        if when.year != dt.date.today().year:\n            retval = _(\"{weekday} {day} of {month} {year}\")  \\\n                    .format(weekday=dow, day=dom, month=month, year=when.year)\n        else:\n            retval = _(\"{weekday} {day} of {month}\")  \\\n                    .format(weekday=dow, day=dom, month=month)\n    return retval", "response": "Formats the date when in the order of the current date."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _iso_to_gregorian(iso_year, iso_week, iso_day):\n    \"Gregorian calendar date for the given ISO year, week and day\"\n    year_start = _iso_year_start(iso_year)\n    return year_start + dt.timedelta(days=iso_day-1, weeks=iso_week-1)", "response": "Gregorian calendar date for the given ISO year week and day"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _iso_num_weeks(iso_year):\n    \"Get the number of ISO-weeks in this year\"\n    year_start = _iso_year_start(iso_year)\n    next_year_start = _iso_year_start(iso_year+1)\n    year_num_weeks = ((next_year_start - year_start).days) // 7\n    return year_num_weeks", "response": "Get the number of ISO - weeks in this year"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _iso_info(iso_year, iso_week):\n    \"Give all the iso info we need from one calculation\"\n    prev_year_start = _iso_year_start(iso_year-1)\n    year_start = _iso_year_start(iso_year)\n    next_year_start = _iso_year_start(iso_year+1)\n    first_day = year_start + dt.timedelta(weeks=iso_week-1)\n    last_day = first_day + dt.timedelta(days=6)\n    prev_year_num_weeks = ((year_start - prev_year_start).days) // 7\n    year_num_weeks = ((next_year_start - year_start).days) // 7\n    return (first_day, last_day, prev_year_num_weeks, year_num_weeks)", "response": "Give all the iso info we need from one calculation"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _ssweek_year_start(ssweek_year):\n    \"The gregorian calendar date of the first day of the given Sundaystarting-week year\"\n    fifth_jan = dt.date(ssweek_year, 1, 5)\n    delta = dt.timedelta(fifth_jan.weekday()+1)\n    return fifth_jan - delta", "response": "The gregorian calendar date of the first day of the given Sundaystarting - week year"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _ssweek_to_gregorian(ssweek_year, ssweek_week, ssweek_day):\n    \"Gregorian calendar date for the given Sundaystarting-week year, week and day\"\n    year_start = _ssweek_year_start(ssweek_year)\n    return year_start + dt.timedelta(days=ssweek_day-1, weeks=ssweek_week-1)", "response": "Gregorian calendar date for the given Sundaystarting - week year week and day"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the number of Sundaystarting - weeks in this year", "response": "def _ssweek_num_weeks(ssweek_year):\n    \"Get the number of Sundaystarting-weeks in this year\"\n    year_start = _ssweek_year_start(ssweek_year)\n    next_year_start = _ssweek_year_start(ssweek_year+1)\n    year_num_weeks = ((next_year_start - year_start).days) // 7\n    return year_num_weeks"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _ssweek_info(ssweek_year, ssweek_week):\n    \"Give all the ssweek info we need from one calculation\"\n    prev_year_start = _ssweek_year_start(ssweek_year-1)\n    year_start = _ssweek_year_start(ssweek_year)\n    next_year_start = _ssweek_year_start(ssweek_year+1)\n    first_day = year_start + dt.timedelta(weeks=ssweek_week-1)\n    last_day = first_day + dt.timedelta(days=6)\n    prev_year_num_weeks = ((year_start - prev_year_start).days) // 7\n    year_num_weeks = ((next_year_start - year_start).days) // 7\n    return (first_day, last_day, prev_year_num_weeks, year_num_weeks)", "response": "Give all the ssweek info we need from one calculation"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _gregorian_to_ssweek(date_value):\n    \"Sundaystarting-week year, week and day for the given  Gregorian calendar date\"\n    yearStart = _ssweek_year_start(date_value.year)\n    weekNum = ((date_value - yearStart).days) // 7 + 1\n    dayOfWeek = date_value.weekday()+1\n    return (date_value.year, weekNum, dayOfWeek)", "response": "Sundaystarting - week year week and day for the given  Gregorian calendar date"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _ssweek_of_month(date_value):\n    \"0-starting index which Sundaystarting-week in the month this date is\"\n    weekday_of_first = (date_value.replace(day=1).weekday() + 1) % 7\n    return (date_value.day + weekday_of_first - 1) // 7", "response": "0 - starting index which Sundaystarting - week in the month this date is"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the group model from the JOYOUS_GROUP_MODEL setting.", "response": "def get_group_model():\n    \"\"\"\n    Get the group model from the ``JOYOUS_GROUP_MODEL`` setting.\n    Useful for developers that need the group model.\n    Defaults to the standard :class:`ls.joyous.models.GroupPage` model\n    if no custom model is defined.\n    \"\"\"\n    from django.apps import apps\n    model_string = get_group_model_string()\n    try:\n        return apps.get_model(model_string)\n    except ValueError:\n        raise ImproperlyConfigured(\"JOYOUS_GROUP_MODEL must be of the form 'app_label.model_name'\")\n    except LookupError:\n        raise ImproperlyConfigured(\n            \"JOYOUS_GROUP_MODEL refers to model '%s' that has not been installed\" % model_string\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef frequency(self):\n        freqOptions = (\"YEARLY\", \"MONTHLY\", \"WEEKLY\", \"DAILY\")\n        if self.rule._freq < len(freqOptions):\n            return freqOptions[self.rule._freq]\n        else:\n            return \"unsupported_frequency_{}\".format(self.rule._freq)", "response": "Returns the frequency of the recurrence repeats."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef byweekday(self):\n        retval = []\n        if self.rule._byweekday:\n            retval += [Weekday(day) for day in self.rule._byweekday]\n        if self.rule._bynweekday:\n            retval += [Weekday(day, n) for day, n in self.rule._bynweekday]\n        return retval", "response": "Returns a list of weekdays that are applied by the given weekday."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a list of the calendar names that are in the month of the year.", "response": "def bymonthday(self):\n        \"\"\"\n        The month days where the recurrence will be applied.\n        \"\"\"\n        retval = []\n        if self.rule._bymonthday:\n            retval += self.rule._bymonthday\n        if self.rule._bynmonthday:\n            retval += self.rule._bynmonthday\n        return retval"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef toOrdinal(n):\n    if n == -1:\n        retval = _(\"last\")\n    elif n == -2:\n        retval = _(\"penultimate\")\n    elif 1 <= n <= 5:\n        retval = _n2w(n, to=\"ordinal\")\n    else:\n        retval = _n2w(n, to=\"ordinal_num\")\n    return retval", "response": "Returns the ordinal name of a number\n    e. g. first last penultimate or ordinal_num"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the name of a number n in the ordinal order e. g. the first second third fourth four or the second", "response": "def toTheOrdinal(n, inTitleCase=True):\n    \"\"\"\n    Returns the definite article with the ordinal name of a number\n    e.g. 'the second'\n    Becomes important for languages with multiple definite articles (e.g. French)\n    \"\"\"\n    if n == -1:\n        retval = _(\"the last\")\n    elif n == -2:\n        retval = _(\"the penultimate\")\n    elif n == 1:\n        retval = _(\"the first\")\n    elif n == 2:\n        retval = _(\"the second\")\n    elif n == 3:\n        retval = _(\"the third\")\n    elif n == 4:\n        retval = _(\"the fourth\")\n    elif n == 5:\n        retval = _(\"the fifth\")\n    else:\n        # TODO: support other definite articles depending on gender, etc.\n        retval = _(\"the\")\n        if inTitleCase:\n            retval = retval.title()\n        retval += \" \"+_n2w(n, to=\"ordinal_num\")\n        return retval\n    if inTitleCase:\n        retval = retval.title()\n    return retval"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef hrJoin(items):\n    conjuction = \" \"+_(\"and\")+\" \"\n    if len(items) <= 2:\n        return conjuction.join(items)\n    else:\n        return \", \".join(items[:-1]) + conjuction + items[-1]", "response": "Joins items together in a human readable string e. g. wind ice and fire"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_context(self, name, value, attrs):\n        context = super().get_context(name, value, attrs)\n        context['widget']['value_s'] = str(value)\n        context['widget']['value_r'] = repr(value)\n        return context", "response": "Return the context to use with the widget s template"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef routeDefault(self, request, year=None):\n        eventsView = request.GET.get('view', self.default_view)\n        if eventsView in (\"L\", \"list\"):\n            return self.serveUpcoming(request)\n        elif eventsView in (\"W\", \"weekly\"):\n            return self.serveWeek(request, year)\n        else:\n            return self.serveMonth(request, year)", "response": "Route a request to the default calendar view."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrouting a request with a month abbreviation to the monthly view.", "response": "def routeByMonthAbbr(self, request, year, monthAbbr):\n        \"\"\"Route a request with a month abbreviation to the monthly view.\"\"\"\n        month = (DatePictures['Mon'].index(monthAbbr.lower()) // 4) + 1\n        return self.serveMonth(request, year, month)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nserving data for the MiniMonth template tag.", "response": "def serveMiniMonth(self, request, year=None, month=None):\n        \"\"\"Serve data for the MiniMonth template tag.\"\"\"\n        if not request.is_ajax():\n            raise Http404(\"/mini/ is for ajax requests only\")\n\n        today = timezone.localdate()\n        if year is None: year = today.year\n        if month is None: month = today.month\n        year = int(year)\n        month = int(month)\n\n        # TODO Consider changing to a TemplateResponse\n        # https://stackoverflow.com/questions/38838601\n        return render(request, \"joyous/includes/minicalendar.html\",\n                      {'self':         self,\n                       'page':         self,\n                       'version':      __version__,\n                       'today':        today,\n                       'year':         year,\n                       'month':        month,\n                       'calendarUrl':  self.get_url(request),\n                       'monthName':    MONTH_NAMES[month],\n                       'weekdayInfo':  zip(weekday_abbr, weekday_name),\n                       'events':       self._getEventsByWeek(request, year, month)})"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef peers(cls):\n        contentType = ContentType.objects.get_for_model(cls)\n        return cls.objects.filter(content_type=contentType)", "response": "Return others of the same concrete type."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns all the events in this site for a given day.", "response": "def _getEventsOnDay(self, request, day):\n        \"\"\"Return all the events in this site for a given day.\"\"\"\n        home = request.site.root_page\n        return getAllEventsByDay(request, day, day, home=home)[0]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the events in this site grouped by day.", "response": "def _getEventsByDay(self, request, firstDay, lastDay):\n        \"\"\"\n        Return the events in this site for the dates given, grouped by day.\n        \"\"\"\n        home = request.site.root_page\n        return getAllEventsByDay(request, firstDay, lastDay, home=home)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the events grouped by week.", "response": "def _getEventsByWeek(self, request, year, month):\n        \"\"\"\n        Return the events in this site for the given month grouped by week.\n        \"\"\"\n        home = request.site.root_page\n        return getAllEventsByWeek(request, year, month, home=home)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _getUpcomingEvents(self, request):\n        home = request.site.root_page\n        return getAllUpcomingEvents(request, home=home)", "response": "Return the upcoming events in this site."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _getPastEvents(self, request):\n        home = request.site.root_page\n        return getAllPastEvents(request, home=home)", "response": "Return the past events in this site."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ntries and find an event with the given UID in this site.", "response": "def _getEventFromUid(self, request, uid):\n        \"\"\"Try and find an event with the given UID in this site.\"\"\"\n        event = getEventFromUid(request, uid) # might raise ObjectDoesNotExist\n        home = request.site.root_page\n        if event.get_ancestors().filter(id=home.id).exists():\n            # only return event if it is in the same site\n            return event"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns all the events in this site.", "response": "def _getAllEvents(self, request):\n        \"\"\"Return all the events in this site.\"\"\"\n        home = request.site.root_page\n        return getAllEvents(request, home=home)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _getEventsOnDay(self, request, day):\n        return getAllEventsByDay(request, day, day, home=self)[0]", "response": "Return my child events for a given day."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _getEventsByDay(self, request, firstDay, lastDay):\n        return getAllEventsByDay(request, firstDay, lastDay, home=self)", "response": "Return my child events grouped by day."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _getEventsByWeek(self, request, year, month):\n        return getAllEventsByWeek(request, year, month, home=self)", "response": "Return my child events grouped by week."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _getEventFromUid(self, request, uid):\n        event = getEventFromUid(request, uid)\n        if event.get_ancestors().filter(id=self.id).exists():\n            # only return event if it is a descendant\n            return event", "response": "Try and find a child event with the given UID."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef minicalendar(context):\n    today = dt.date.today()\n    request = context['request']\n    home = request.site.root_page\n    cal = CalendarPage.objects.live().descendant_of(home).first()\n    calUrl = cal.get_url(request) if cal else None\n    if cal:\n        events = cal._getEventsByWeek(request, today.year, today.month)\n    else:\n        events = getAllEventsByWeek(request, today.year, today.month)\n    return {'request':     request,\n            'today':       today,\n            'year':        today.year,\n            'month':       today.month,\n            'calendarUrl': calUrl,\n            'monthName':   calendar.month_name[today.month],\n            'weekdayInfo': zip(weekday_abbr, weekday_name),\n            'events':      events}", "response": "Displays a little ajax version of the calendar."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndisplay a list of all upcoming events in this site.", "response": "def subsite_upcoming_events(context):\n    \"\"\"\n    Displays a list of all upcoming events in this site.\n    \"\"\"\n    request = context['request']\n    home = request.site.root_page\n    return {'request': request,\n            'events':  getAllUpcomingEvents(request, home=home)}"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndisplays a list of all upcoming events assigned to a specific group.", "response": "def group_upcoming_events(context, group=None):\n    \"\"\"\n    Displays a list of all upcoming events that are assigned to a specific\n    group.  If the group is not specified it is assumed to be the current page.\n    \"\"\"\n    request = context.get('request')\n    if group is None:\n        group = context.get('page')\n    if group:\n        events = getGroupUpcomingEvents(request, group)\n    else:\n        events = []\n    return {'request': request,\n            'events':  events}"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef future_exceptions(context, rrevent=None):\n    request = context['request']\n    if rrevent is None:\n        rrevent = context.get('page')\n    if rrevent:\n        exceptions = rrevent._futureExceptions(request)\n    else:\n        exceptions = []\n    return {'request':    request,\n            'exceptions': exceptions}", "response": "Displays a list of all the future exceptions for a given recurring event."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndisplays when the next occurence of a recurring event will be shown.", "response": "def next_on(context, rrevent=None):\n    \"\"\"\n    Displays when the next occurence of a recurring event will be.  If the\n    recurring event is not specified it is assumed to be the current page.\n    \"\"\"\n    request = context['request']\n    if rrevent is None:\n        rrevent = context.get('page')\n    eventNextOn = getattr(rrevent, '_nextOn', lambda _:None)\n    return eventNextOn(request)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef location_gmap(context, location):\n    gmapq = None\n    if getattr(MapFieldPanel, \"UsingWagtailGMaps\", False):\n        gmapq = location\n    return {'gmapq': gmapq}", "response": "Display a link to Google maps iff we are using WagtailGMaps"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntake a string like NZ [ WTL Nelson AU [ * ] or Northern Ireland and builds a HolidaySum from it.", "response": "def parseHolidays(holidaysStr, holidayMap=None):\n    \"\"\"\n    Takes a string like NZ[WTL,Nelson],AU[*],Northern Ireland and builds a HolidaySum from it\n    \"\"\"\n    if holidayMap is None:\n        holidayMap = _HOLIDAY_MAP\n    retval = holidays.HolidayBase()\n    retval.country = None\n    holidaysStr = holidaysStr.strip()\n    for (country, subdivisions) in HolsRe.findall(holidaysStr):\n        cls = holidayMap.get(country)\n        if cls is not None:\n            if subdivisions:\n                retval += _parseSubdivisions(subdivisions, cls)\n            else:\n                retval += cls()\n    return retval"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getAllEventsByDay(request, fromDate, toDate, *, home=None):\n    qrys = [SimpleEventPage.events(request).byDay(fromDate, toDate),\n            MultidayEventPage.events(request).byDay(fromDate, toDate),\n            RecurringEventPage.events(request).byDay(fromDate, toDate),\n            PostponementPage.events(request).byDay(fromDate, toDate)]\n    # Cancellations and ExtraInfo pages are returned by RecurringEventPage.byDay\n    if home is not None:\n        qrys = [qry.descendant_of(home) for qry in qrys]\n    evods = _getEventsByDay(fromDate, qrys)\n    return evods", "response": "Returns all the events in a given day grouped by date."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef getAllEventsByWeek(request, year, month, *, home=None):\n    return _getEventsByWeek(year, month,\n                            partial(getAllEventsByDay, request, home=home))", "response": "Returns all the events in the given month grouped by week."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns all the upcoming events under home.", "response": "def getAllUpcomingEvents(request, *, home=None):\n    \"\"\"\n    Return all the upcoming events (under home if given).\n\n    :param request: Django request object\n    :param home: only include events that are under this page (if given)\n    :rtype: list of the namedtuple ThisEvent (title, page, url)\n    \"\"\"\n    qrys = [SimpleEventPage.events(request).upcoming().this(),\n            MultidayEventPage.events(request).upcoming().this(),\n            RecurringEventPage.events(request).upcoming().this(),\n            PostponementPage.events(request).upcoming().this(),\n            ExtraInfoPage.events(request).exclude(extra_title=\"\").upcoming()\n                                         .this()]\n    if home is not None:\n        qrys = [qry.descendant_of(home) for qry in qrys]\n    events = sorted(chain.from_iterable(qrys),\n                    key=attrgetter('page._upcoming_datetime_from'))\n    return events"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getGroupUpcomingEvents(request, group):\n    # Get events that are a child of a group page, or a postponement or extra\n    # info a child of the recurring event child of the group\n    rrEvents = RecurringEventPage.events(request).exclude(group_page=group) \\\n                                        .upcoming().child_of(group).this()\n    qrys = [SimpleEventPage.events(request).exclude(group_page=group)\n                                        .upcoming().child_of(group).this(),\n            MultidayEventPage.events(request).exclude(group_page=group)\n                                        .upcoming().child_of(group).this(),\n            rrEvents]\n    for rrEvent in rrEvents:\n        qrys += [PostponementPage.events(request).child_of(rrEvent.page)\n                                         .upcoming().this(),\n                 ExtraInfoPage.events(request).exclude(extra_title=\"\")\n                                 .child_of(rrEvent.page).upcoming().this()]\n\n    # Get events that are linked to a group page, or a postponement or extra\n    # info a child of the recurring event linked to a group\n    rrEvents = group.recurringeventpage_set(manager='events').auth(request)  \\\n                                                        .upcoming().this()\n    qrys += [group.simpleeventpage_set(manager='events').auth(request)\n                                                        .upcoming().this(),\n             group.multidayeventpage_set(manager='events').auth(request)\n                                                        .upcoming().this(),\n             rrEvents]\n    for rrEvent in rrEvents:\n        qrys += [PostponementPage.events(request).child_of(rrEvent.page)\n                                                       .upcoming().this(),\n                 ExtraInfoPage.events(request).exclude(extra_title=\"\")\n                                 .child_of(rrEvent.page).upcoming().this()]\n    events = sorted(chain.from_iterable(qrys),\n                    key=attrgetter('page._upcoming_datetime_from'))\n    return events", "response": "Returns all the upcoming events that are assigned to the specified group."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns all the past events under home.", "response": "def getAllPastEvents(request, *, home=None):\n    \"\"\"\n    Return all the past events (under home if given).\n\n    :param request: Django request object\n    :param home: only include events that are under this page (if given)\n    :rtype: list of the namedtuple ThisEvent (title, page, url)\n    \"\"\"\n    qrys = [SimpleEventPage.events(request).past().this(),\n            MultidayEventPage.events(request).past().this(),\n            RecurringEventPage.events(request).past().this(),\n            PostponementPage.events(request).past().this(),\n            ExtraInfoPage.events(request).exclude(extra_title=\"\").past().this()]\n    if home is not None:\n        qrys = [qry.descendant_of(home) for qry in qrys]\n    events = sorted(chain.from_iterable(qrys),\n                    key=attrgetter('page._past_datetime_from'), reverse=True)\n    return events"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the event by its UID .", "response": "def getEventFromUid(request, uid):\n    \"\"\"\n    Get the event by its UID\n    (returns None if we have no authority, raises ObjectDoesNotExist if it is\n    not found).\n\n    :param request: Django request object\n    :param uid: iCal unique identifier\n    :rtype: list of event pages\n    \"\"\"\n    events = []\n    with suppress(ObjectDoesNotExist):\n        events.append(SimpleEventPage.objects.get(uid=uid))\n    with suppress(ObjectDoesNotExist):\n        events.append(MultidayEventPage.objects.get(uid=uid))\n    with suppress(ObjectDoesNotExist):\n        events.append(RecurringEventPage.objects.get(uid=uid))\n    # Exceptions do not have uids and are not returned by this function\n\n    if len(events) == 1:\n        if events[0].isAuthorized(request):\n            return events[0]\n        else:\n            return None\n    elif len(events) == 0:\n        raise ObjectDoesNotExist(\"No event with uid={}\".format(uid))\n    else:\n        raise MultipleObjectsReturned(\"Multiple events with uid={}\".format(uid))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getAllEvents(request, *, home=None):\n    qrys = [SimpleEventPage.events(request).all(),\n            MultidayEventPage.events(request).all(),\n            RecurringEventPage.events(request).all()]\n    # Does not return exceptions\n    if home is not None:\n        qrys = [qry.descendant_of(home) for qry in qrys]\n    events = sorted(chain.from_iterable(qrys),\n                    key=attrgetter('_first_datetime_from'))\n    return events", "response": "Returns all the events under home."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef removeContentPanels(remove):\n    SimpleEventPage._removeContentPanels(remove)\n    MultidayEventPage._removeContentPanels(remove)\n    RecurringEventPage._removeContentPanels(remove)\n    MultidayRecurringEventPage._removeContentPanels(remove)\n    PostponementPage._removeContentPanels(remove)", "response": "Remove the panels and so hide the fields named."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the group that this event belongs to.", "response": "def group(self):\n        \"\"\"\n        The group this event belongs to.  Adding the event as a child of a\n        group automatically assigns the event to that group.\n        \"\"\"\n        retval = None\n        parent = self.get_parent()\n        Group = get_group_model()\n        if issubclass(parent.specific_class, Group):\n            retval = parent.specific\n        if retval is None:\n            retval = self.group_page\n        return retval"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _removeContentPanels(cls, remove):\n        if type(remove) is str:\n            remove = [remove]\n        cls.content_panels = [panel for panel in cls.content_panels\n                              if getattr(panel, \"field_name\", None) not in remove]", "response": "Remove the panels and so hide the fields named in remove."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning True if the user is authorized for the requested action with this event False otherwise.", "response": "def isAuthorized(self, request):\n        \"\"\"\n        Is the user authorized for the requested action with this event?\n        \"\"\"\n        restrictions = self.get_view_restrictions()\n        if restrictions and request is None:\n            return False\n        else:\n            return all(restriction.accept_request(request)\n                       for restriction in restrictions)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _getLocalWhen(self, date_from, date_to=None):\n        dateFrom, timeFrom = getLocalDateAndTime(date_from, self.time_from,\n                                                 self.tz, dt.time.min)\n\n        if date_to is not None:\n            dateTo, timeTo = getLocalDateAndTime(date_to, self.time_to, self.tz)\n        else:\n            if self.time_to is not None:\n                dateTo, timeTo = getLocalDateAndTime(date_from, self.time_to, self.tz)\n            else:\n                dateTo = dateFrom\n                timeTo = None\n\n        if dateFrom == dateTo:\n            retval = _(\"{date} {atTime}\").format(date=dateFormat(dateFrom),\n                        atTime=timeFormat(timeFrom, timeTo, gettext(\"at \")))\n        else:\n            retval = _(\"{date} {atTime}\").format(date=dateFormat(dateFrom),\n                        atTime=timeFormat(timeFrom, prefix=gettext(\"at \")))\n            retval = _(\"{dateTimeFrom} to {dateTo} {atTimeTo}\").format(\n                        dateTimeFrom=retval.strip(),\n                        dateTo=dateFormat(dateTo),\n                        atTimeTo=timeFormat(timeTo, prefix=gettext(\"at \")))\n        return retval.strip()", "response": "Returns a string describing when the event occurs in the local time zone."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef status(self):\n        myNow = timezone.localtime(timezone=self.tz)\n        if getAwareDatetime(self.date, self.time_to, self.tz) < myNow:\n            return \"finished\"\n        elif getAwareDatetime(self.date, self.time_from, self.tz) < myNow:\n            return \"started\"", "response": "Returns the status of the current event."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the local time that the event starts.", "response": "def _getFromTime(self, atDate=None):\n        \"\"\"\n        Time that the event starts (in the local time zone).\n        \"\"\"\n        return getLocalTime(self.date, self.time_from, self.tz)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the status of the current event.", "response": "def status(self):\n        \"\"\"\n        The current status of the event (started, finished or pending).\n        \"\"\"\n        myNow = timezone.localtime(timezone=self.tz)\n        if getAwareDatetime(self.date_to, self.time_to, self.tz) < myNow:\n            return \"finished\"\n        elif getAwareDatetime(self.date_from, self.time_from, self.tz) < myNow:\n            return \"started\""}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _getFromTime(self, atDate=None):\n        return getLocalTime(self.date_from, self.time_from, self.tz)", "response": "Get the local time that the event starts."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the next date that this event is scheduled to occur in the local time zone.", "response": "def next_date(self):\n        \"\"\"\n        Date when this event is next scheduled to occur in the local time zone\n        (Does not include postponements, but does exclude cancellations)\n        \"\"\"\n        nextDt = self.__localAfter(timezone.localtime(), dt.time.min)\n        if nextDt is not None:\n            return nextDt.date()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _upcoming_datetime_from(self):\n        nextDt = self.__localAfter(timezone.localtime(), dt.time.max,\n                                   excludeCancellations=True,\n                                   excludeExtraInfo=True)\n        return nextDt", "response": "Returns the datetime that this event next starts in the local time zone or None if it is finished."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the date of the previous event in the local time zone.", "response": "def prev_date(self):\n        \"\"\"\n        Date when this event last occurred in the local time zone\n        (Does not include postponements, but does exclude cancellations)\n        \"\"\"\n        prevDt = self.__localBefore(timezone.localtime(), dt.time.min)\n        if prevDt is not None:\n            return prevDt.date()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _past_datetime_from(self):\n        prevDt = self.__localBefore(timezone.localtime(), dt.time.max,\n                                    excludeCancellations=True,\n                                    excludeExtraInfo=True)\n        return prevDt", "response": "Returns the datetime that this event was started in the local time zone or None if it never started."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the datetime this event was started in the local time zone.", "response": "def _first_datetime_from(self):\n        \"\"\"\n        The datetime this event first started in the local time zone, or None if\n        it never did.\n        \"\"\"\n        myFromDt = self._getMyFirstDatetimeFrom()\n        localTZ = timezone.get_current_timezone()\n        return myFromDt.astimezone(localTZ)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef status(self):\n        myNow = timezone.localtime(timezone=self.tz)\n        daysDelta = dt.timedelta(days=self.num_days - 1)\n        # NB: postponements can be created after the until date\n        #     so ignore that\n        todayStart = getAwareDatetime(myNow.date(), dt.time.min, self.tz)\n        eventStart, event = self.__afterOrPostponedTo(todayStart - daysDelta)\n        if eventStart is None:\n            return \"finished\"\n        eventFinish = getAwareDatetime(eventStart.date() + daysDelta,\n                                       event.time_to, self.tz)\n        if event.time_from is None:\n            eventStart += _1day\n        if eventStart < myNow < eventFinish:\n            # if there are two occurences on the same day then we may miss\n            # that one of them has started\n            return \"started\"\n        if (self.repeat.until and eventFinish < myNow and\n            self.__afterOrPostponedTo(myNow)[0] is None):\n            # only just wound up, the last occurence was earlier today\n            return \"finished\"", "response": "Returns the status of the current entry in the calendar."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef when(self):\n        offset   = 0\n        timeFrom = dateFrom = timeTo = dateTo = None\n        fromDt   = self._getFromDt()\n        if fromDt is not None:\n            offset = timezone.localtime(fromDt).toordinal() - fromDt.toordinal()\n            dateFrom, timeFrom = getLocalDateAndTime(fromDt.date(), self.time_from,\n                                                     self.tz, dt.time.min)\n            daysDelta = dt.timedelta(days=self.num_days - 1)\n            dateTo, timeTo = getLocalDateAndTime(fromDt.date() + daysDelta,\n                                                 self.time_to, self.tz)\n        if dateFrom == dateTo:\n            retval = _(\"{repeat} {atTime}\").format(\n                    repeat=self.repeat._getWhen(offset),\n                    atTime=timeFormat(timeFrom, timeTo, gettext(\"at \")))\n        else:\n            localNumDays = (dateTo - dateFrom).days + 1\n            retval = _(\"{repeat} {startFinishTime}\").format(\n                    repeat=self.repeat._getWhen(offset, localNumDays),\n                    startFinishTime=timeFormat(timeFrom, timeTo,\n                                               prefix=gettext(\"starting at \"),\n                                               infix=gettext(\"finishing at\")))\n        return retval.strip()", "response": "A string describing when the event occurs."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _getFromTime(self, atDate=None):\n        if atDate is None:\n            atDate = timezone.localdate(timezone=self.tz)\n        return getLocalTime(atDate, self.time_from, self.tz)", "response": "Get the time that this event was from."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the datetime of the next event after or before now.", "response": "def _getFromDt(self):\n        \"\"\"\n        Get the datetime of the next event after or before now.\n        \"\"\"\n        myNow = timezone.localtime(timezone=self.tz)\n        return self.__after(myNow) or self.__before(myNow)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _futureExceptions(self, request):\n        retval = []\n        # We know all future exception dates are in the parent time zone\n        myToday = timezone.localdate(timezone=self.tz)\n\n        for extraInfo in ExtraInfoPage.events(request).child_of(self)         \\\n                                      .filter(except_date__gte=myToday):\n            retval.append(extraInfo)\n        for cancellation in CancellationPage.events(request).child_of(self)   \\\n                                            .filter(except_date__gte=myToday):\n            postponement = getattr(cancellation, \"postponementpage\", None)\n            if postponement:\n                retval.append(postponement)\n            else:\n                retval.append(cancellation)\n        retval.sort(key=attrgetter('except_date'))\n        # notice these are events not ThisEvents\n        return retval", "response": "Returns all future exception dates for this recurring event."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _nextOn(self, request):\n        retval = None\n        nextDt, event = self.__localAfterOrPostponedTo(timezone.localtime(),\n                                                       dt.time.min)\n        if nextDt is not None:\n            timeFrom = nextDt.time() if event.time_from is not None else None\n            retval = \"{} {}\".format(dateFormat(nextDt.date()),\n                                    timeFormat(timeFrom, prefix=gettext(\"at \")))\n            if event is not self and event.isAuthorized(request):\n                retval = format_html('<a class=\"inline-link\" href=\"{}\">{}</a>',\n                                     event.url, retval)\n        return retval", "response": "Returns the next on of the resource."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns true iff an occurence of this event starts on myDate.", "response": "def _occursOn(self, myDate):\n        \"\"\"\n        Returns true iff an occurence of this event starts on this date\n        (given in the event's own timezone).\n\n        (Does not include postponements, but does exclude cancellations.)\n        \"\"\"\n        # TODO analyse which is faster (rrule or db) and test that first\n        if myDate not in self.repeat:\n            return False\n        if CancellationPage.events.child_of(self)                            \\\n                           .filter(except_date=myDate).exists():\n            return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the datetime that this event was started.", "response": "def _getMyFirstDatetimeFrom(self):\n        \"\"\"\n        The datetime this event first started, or None if it never did.\n        \"\"\"\n        myStartDt = getAwareDatetime(self.repeat.dtstart, None,\n                                     self.tz, dt.time.min)\n        return self.__after(myStartDt, excludeCancellations=False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the datetime that this event was first finished or None if it never finished.", "response": "def _getMyFirstDatetimeTo(self):\n        \"\"\"\n        The datetime this event first finished, or None if it never did.\n        \"\"\"\n        myFirstDt = self._getMyFirstDatetimeFrom()\n        if myFirstDt is not None:\n            daysDelta = dt.timedelta(days=self.num_days - 1)\n            return getAwareDatetime(myFirstDt.date() + daysDelta,\n                                    self.time_to,\n                                    self.tz, dt.time.max)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef local_title(self):\n        name = self.title.partition(\" for \")[0]\n        exceptDate = getLocalDate(self.except_date, self.time_from, self.tz)\n        title = _(\"{exception} for {date}\").format(exception=_(name),\n                                                   date=dateFormat(exceptDate))\n        return title", "response": "Localised version of the human - readable title of the page."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a string describing when the event occurs in the local time zone.", "response": "def _getLocalWhen(self, date_from, num_days=1):\n        \"\"\"\n        Returns a string describing when the event occurs (in the local time zone).\n        \"\"\"\n        dateFrom, timeFrom = getLocalDateAndTime(date_from, self.time_from,\n                                                 self.tz, dt.time.min)\n        if num_days > 1 or self.time_to is not None:\n            daysDelta = dt.timedelta(days=self.num_days - 1)\n            dateTo, timeTo = getLocalDateAndTime(date_from + daysDelta,\n                                                 self.time_to, self.tz)\n        else:\n            dateTo = dateFrom\n            timeTo = None\n\n        if dateFrom == dateTo:\n            retval = _(\"{date} {atTime}\").format(\n                    date=dateFormat(dateFrom),\n                    atTime=timeFormat(timeFrom, timeTo, gettext(\"at \")))\n        else:\n            # Friday the 10th of April for 3 days at 1pm to 10am\n            localNumDays = (dateTo - dateFrom).days + 1\n            retval = _(\"{date} for {n} days {startTime}\").format(\n                    date=dateFormat(dateFrom),\n                    n=localNumDays,\n                    startTime=timeFormat(timeFrom, prefix=gettext(\"starting at \")))\n            retval = _(\"{dateAndStartTime} {finishTime}\").format(\n                    dateAndStartTime=retval.strip(),\n                    finishTime=timeFormat(timeTo, prefix=gettext(\"finishing at \")))\n        return retval.strip()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the local time that the event starts.", "response": "def _getFromTime(self, atDate=None):\n        \"\"\"\n        Time that the event starts (in the local time zone).\n        \"\"\"\n        return getLocalTime(self.except_date, self.time_from, self.tz)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef full_clean(self, *args, **kwargs):\n        name = getattr(self, 'name', self.slugName.title())\n        self.title = \"{} for {}\".format(name, dateFormat(self.except_date))\n        self.slug = \"{}-{}\".format(self.except_date, self.slugName)\n        super().full_clean(*args, **kwargs)", "response": "Apply fixups that need to happen before per - field validation occurs. Sets the page s slug."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef status(self):\n        myNow = timezone.localtime(timezone=self.tz)\n        fromDt = getAwareDatetime(self.except_date, self.time_from, self.tz)\n        daysDelta = dt.timedelta(days=self.num_days - 1)\n        toDt = getAwareDatetime(self.except_date + daysDelta, self.time_to, self.tz)\n        if toDt < myNow:\n            return \"finished\"\n        elif fromDt < myNow:\n            return \"started\"", "response": "Returns the status of the event."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a string depending what the event has been changed to.", "response": "def what(self):\n        \"\"\"\n        May return a 'postponed' or 'rescheduled' string depending what\n        the start and finish time of the event has been changed to.\n        \"\"\"\n        originalFromDt = dt.datetime.combine(self.except_date,\n                                             timeFrom(self.overrides.time_from))\n        changedFromDt = dt.datetime.combine(self.date, timeFrom(self.time_from))\n        originalDaysDelta = dt.timedelta(days=self.overrides.num_days - 1)\n        originalToDt = getAwareDatetime(self.except_date + originalDaysDelta,\n                                        self.overrides.time_to, self.tz)\n        changedDaysDelta = dt.timedelta(days=self.num_days - 1)\n        changedToDt = getAwareDatetime(self.except_date + changedDaysDelta,\n                                        self.time_to, self.tz)\n        if originalFromDt < changedFromDt:\n            return _(\"Postponed\")\n        elif originalFromDt > changedFromDt or originalToDt != changedToDt:\n            return _(\"Rescheduled\")\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the date that the event was postponed from.", "response": "def postponed_from(self):\n        \"\"\"\n        Date that the event was postponed from (in the local time zone).\n        \"\"\"\n        fromDate = getLocalDate(self.except_date, self.time_from, self.tz)\n        return dateFormat(fromDate)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the date that the event was postponed to.", "response": "def postponed_to(self):\n        \"\"\"\n        Date that the event was postponed to (in the local time zone).\n        \"\"\"\n        toDate = getLocalDate(self.date, self.time_from, self.tz)\n        return dateFormat(toDate)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _convertTZ(self):\n        tz = timezone.get_current_timezone()\n        dtstart = self['DTSTART']\n        dtend   = self['DTEND']\n        if dtstart.zone() == \"UTC\":\n            dtstart.dt = dtstart.dt.astimezone(tz)\n        if dtend.zone() == \"UTC\":\n            dtend.dt = dtend.dt.astimezone(tz)", "response": "Will convert UTC datetimes to the current local timezone"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_naive_utc(dtime):\n    if not hasattr(dtime, 'tzinfo') or dtime.tzinfo is None:\n        return dtime\n\n    dtime_utc = dtime.astimezone(pytz.UTC)\n    dtime_naive = dtime_utc.replace(tzinfo=None)\n    return dtime_naive", "response": "convert a datetime object to UTC and than remove the tzinfo"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates an icalendar vtimezone from a pytz. tzinfoinfo object.", "response": "def create_timezone(tz, first_date=None, last_date=None):\n    \"\"\"\n    create an icalendar vtimezone from a pytz.tzinfo object\n\n    :param tz: the timezone\n    :type tz: pytz.tzinfo\n    :param first_date: the very first datetime that needs to be included in the\n    transition times, typically the DTSTART value of the (first recurring)\n    event\n    :type first_date: datetime.datetime\n    :param last_date: the last datetime that needs to included, typically the\n    end of the (very last) event (of a recursion set)\n    :returns: timezone information\n    :rtype: icalendar.Timezone()\n\n    we currently have a problem here:\n\n       pytz.timezones only carry the absolute dates of time zone transitions,\n       not their RRULEs. This will a) make for rather bloated VTIMEZONE\n       components, especially for long recurring events, b) we'll need to\n       specify for which time range this VTIMEZONE should be generated and c)\n       will not be valid for recurring events that go into eternity.\n\n    Possible Solutions:\n\n    As this information is not provided by pytz at all, there is no\n    easy solution, we'd really need to ship another version of the OLSON DB.\n\n    \"\"\"\n    if isinstance(tz, pytz.tzinfo.StaticTzInfo):\n        return _create_timezone_static(tz)\n\n    # TODO last_date = None, recurring to infinity\n\n    first_date = dt.datetime.today() if not first_date else to_naive_utc(first_date)\n    last_date = dt.datetime.today() if not last_date else to_naive_utc(last_date)\n    timezone = icalendar.Timezone()\n    timezone.add('TZID', tz)\n\n    # This is not a reliable way of determining if a transition is for\n    # daylight savings.\n    # From 1927 to 1941 New Zealand had GMT+11:30 (NZ Mean Time) as standard\n    # and GMT+12:00 (NZ Summer Time) as daylight savings time.\n    # From 1941 GMT+12:00 (NZ Standard Time) became standard time.\n    # So NZST (NZ Summer/Standard Time) can refer to standard or daylight\n    # savings time.  And this code depends on the random order the _tzinfos\n    # are returned.\n    # dst = {\n    #     one[2]: 'DST' in two.__repr__()\n    #     for one, two in iter(tz._tzinfos.items())\n    # }\n    # bst = {\n    #     one[2]: 'BST' in two.__repr__()\n    #     for one, two in iter(tz._tzinfos.items())\n    # }\n    # ...\n    #   if dst[name] or bst[name]:\n\n    # looking for the first and last transition time we need to include\n    first_num, last_num = 0, len(tz._utc_transition_times) - 1\n    first_tt = tz._utc_transition_times[0]\n    last_tt = tz._utc_transition_times[-1]\n    for num, transtime in enumerate(tz._utc_transition_times):\n        if transtime > first_tt and transtime < first_date:\n            first_num = num\n            first_tt = transtime\n        if transtime < last_tt and transtime > last_date:\n            last_num = num\n            last_tt = transtime\n\n    timezones = dict()\n    for num in range(first_num, last_num + 1):\n        name = tz._transition_info[num][2]\n        if name in timezones:\n            ttime = tz.fromutc(tz._utc_transition_times[num]).replace(tzinfo=None)\n            if 'RDATE' in timezones[name]:\n                timezones[name]['RDATE'].dts.append(\n                    icalendar.prop.vDDDTypes(ttime))\n            else:\n                timezones[name].add('RDATE', ttime)\n            continue\n\n        if tz._transition_info[num][1]:\n            subcomp = icalendar.TimezoneDaylight()\n        else:\n            subcomp = icalendar.TimezoneStandard()\n\n        subcomp.add('TZNAME', tz._transition_info[num][2])\n        subcomp.add(\n            'DTSTART',\n            tz.fromutc(tz._utc_transition_times[num]).replace(tzinfo=None))\n        subcomp.add('TZOFFSETTO', tz._transition_info[num][0])\n        subcomp.add('TZOFFSETFROM', tz._transition_info[num - 1][0])\n        timezones[name] = subcomp\n\n    for subcomp in timezones.values():\n        timezone.add_component(subcomp)\n\n    return timezone"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _create_timezone_static(tz):\n    timezone = icalendar.Timezone()\n    timezone.add('TZID', tz)\n    subcomp = icalendar.TimezoneStandard()\n    subcomp.add('TZNAME', tz)\n    subcomp.add('DTSTART', dt.datetime(1601, 1, 1))\n    subcomp.add('RDATE', dt.datetime(1601, 1, 1))\n    subcomp.add('TZOFFSETTO', tz._utcoffset)\n    subcomp.add('TZOFFSETFROM', tz._utcoffset)\n    timezone.add_component(subcomp)\n    return timezone", "response": "create an icalendar vtimezone from a pytz. tzinfo. StaticTzInfo object"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef protect(self, protect_against=None):\n        return ProtectedF(self._func,\n                          *self._args,\n                          protect_against=protect_against,\n                          **self._kwargs)", "response": "Return a ProtectedF with the same parameters and with the given\n        protect_against."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating the list of plots for the current object.", "response": "def make_plots(datadf, settings):\n    '''\n    Call plotting functions from nanoplotter\n    settings[\"lengths_pointer\"] is a column in the DataFrame specifying which lengths to use\n    '''\n    plot_settings = dict(font_scale=settings[\"font_scale\"])\n    nanoplotter.plot_settings(plot_settings, dpi=settings[\"dpi\"])\n    color = nanoplotter.check_valid_color(settings[\"color\"])\n    plotdict = {type: settings[\"plots\"].count(type) for type in [\"kde\", \"hex\", \"dot\", 'pauvre']}\n    plots = []\n    if settings[\"N50\"]:\n        n50 = nanomath.get_N50(np.sort(datadf[\"lengths\"]))\n    else:\n        n50 = None\n    plots.extend(\n        nanoplotter.length_plots(\n            array=datadf[datadf[\"length_filter\"]][\"lengths\"],\n            name=\"Read length\",\n            path=settings[\"path\"],\n            n50=n50,\n            color=color,\n            figformat=settings[\"format\"],\n            title=settings[\"title\"])\n    )\n    logging.info(\"Created length plots\")\n    if \"quals\" in datadf:\n        plots.extend(\n            nanoplotter.scatter(\n                x=datadf[datadf[\"length_filter\"]][settings[\"lengths_pointer\"]],\n                y=datadf[datadf[\"length_filter\"]][\"quals\"],\n                names=['Read lengths', 'Average read quality'],\n                path=settings[\"path\"] + \"LengthvsQualityScatterPlot\",\n                color=color,\n                figformat=settings[\"format\"],\n                plots=plotdict,\n                log=settings[\"logBool\"],\n                title=settings[\"title\"],\n                plot_settings=plot_settings)\n        )\n        logging.info(\"Created LengthvsQual plot\")\n    if \"channelIDs\" in datadf:\n        plots.extend(\n            nanoplotter.spatial_heatmap(\n                array=datadf[\"channelIDs\"],\n                title=settings[\"title\"],\n                path=settings[\"path\"] + \"ActivityMap_ReadsPerChannel\",\n                color=\"Greens\",\n                figformat=settings[\"format\"])\n        )\n        logging.info(\"Created spatialheatmap for succesfull basecalls.\")\n    if \"start_time\" in datadf:\n        plots.extend(\n            nanoplotter.time_plots(\n                df=datadf,\n                path=settings[\"path\"],\n                color=color,\n                figformat=settings[\"format\"],\n                title=settings[\"title\"],\n                log_length=settings[\"logBool\"],\n                plot_settings=plot_settings)\n        )\n        logging.info(\"Created timeplots.\")\n    if \"aligned_lengths\" in datadf and \"lengths\" in datadf:\n        plots.extend(\n            nanoplotter.scatter(\n                x=datadf[datadf[\"length_filter\"]][\"aligned_lengths\"],\n                y=datadf[datadf[\"length_filter\"]][\"lengths\"],\n                names=[\"Aligned read lengths\", \"Sequenced read length\"],\n                path=settings[\"path\"] + \"AlignedReadlengthvsSequencedReadLength\",\n                figformat=settings[\"format\"],\n                plots=plotdict,\n                color=color,\n                title=settings[\"title\"],\n                plot_settings=plot_settings)\n        )\n        logging.info(\"Created AlignedLength vs Length plot.\")\n    if \"mapQ\" in datadf and \"quals\" in datadf:\n        plots.extend(\n            nanoplotter.scatter(\n                x=datadf[\"mapQ\"],\n                y=datadf[\"quals\"],\n                names=[\"Read mapping quality\", \"Average basecall quality\"],\n                path=settings[\"path\"] + \"MappingQualityvsAverageBaseQuality\",\n                color=color,\n                figformat=settings[\"format\"],\n                plots=plotdict,\n                title=settings[\"title\"],\n                plot_settings=plot_settings)\n        )\n        logging.info(\"Created MapQvsBaseQ plot.\")\n        plots.extend(\n            nanoplotter.scatter(\n                x=datadf[datadf[\"length_filter\"]][settings[\"lengths_pointer\"]],\n                y=datadf[datadf[\"length_filter\"]][\"mapQ\"],\n                names=[\"Read length\", \"Read mapping quality\"],\n                path=settings[\"path\"] + \"MappingQualityvsReadLength\",\n                color=color,\n                figformat=settings[\"format\"],\n                plots=plotdict,\n                log=settings[\"logBool\"],\n                title=settings[\"title\"],\n                plot_settings=plot_settings)\n        )\n        logging.info(\"Created Mapping quality vs read length plot.\")\n    if \"percentIdentity\" in datadf:\n        minPID = np.percentile(datadf[\"percentIdentity\"], 1)\n        if \"aligned_quals\" in datadf:\n            plots.extend(\n                nanoplotter.scatter(\n                    x=datadf[\"percentIdentity\"],\n                    y=datadf[\"aligned_quals\"],\n                    names=[\"Percent identity\", \"Average Base Quality\"],\n                    path=settings[\"path\"] + \"PercentIdentityvsAverageBaseQuality\",\n                    color=color,\n                    figformat=settings[\"format\"],\n                    plots=plotdict,\n                    stat=stats.pearsonr,\n                    minvalx=minPID,\n                    title=settings[\"title\"],\n                    plot_settings=plot_settings)\n            )\n            logging.info(\"Created Percent ID vs Base quality plot.\")\n        plots.extend(\n            nanoplotter.scatter(\n                x=datadf[datadf[\"length_filter\"]][settings[\"lengths_pointer\"]],\n                y=datadf[datadf[\"length_filter\"]][\"percentIdentity\"],\n                names=[\"Aligned read length\", \"Percent identity\"],\n                path=settings[\"path\"] + \"PercentIdentityvsAlignedReadLength\",\n                color=color,\n                figformat=settings[\"format\"],\n                plots=plotdict,\n                stat=stats.pearsonr,\n                log=settings[\"logBool\"],\n                minvaly=minPID,\n                title=settings[\"title\"],\n                plot_settings=plot_settings)\n        )\n        logging.info(\"Created Percent ID vs Length plot\")\n    return plots"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a FAT html report based on the previously created files and plots and settings.", "response": "def make_report(plots, settings):\n    '''\n    Creates a fat html report based on the previously created files\n    plots is a list of Plot objects defined by a path and title\n    statsfile is the file to which the stats have been saved,\n    which is parsed to a table (rather dodgy)\n    '''\n    logging.info(\"Writing html report.\")\n    html_content = ['<body>']\n\n    # Hyperlink Table of Contents panel\n    html_content.append('<div class=\"panel panelC\">')\n    if settings[\"filtered\"]:\n        html_content.append(\n            '<p><strong><a href=\"#stats0\">Summary Statistics prior to filtering</a></strong></p>')\n        html_content.append(\n            '<p><strong><a href=\"#stats1\">Summary Statistics after filtering</a></strong></p>')\n    else:\n        html_content.append(\n            '<p><strong><a href=\"#stats0\">Summary Statistics</a></strong></p>')\n    html_content.append('<p><strong><a href=\"#plots\">Plots</a></strong></p>')\n    html_content.extend(['<p style=\"margin-left:20px\"><a href=\"#' +\n                         p.title.replace(' ', '_') + '\">' + p.title + '</a></p>' for p in plots])\n    html_content.append('</div>')\n\n    # The report itself: stats\n    html_content.append('<div class=\"panel panelM\"> <h1>NanoPlot report</h1>')\n    if settings[\"filtered\"]:\n        html_content.append('<h2 id=\"stats0\">Summary statistics prior to filtering</h2>')\n        html_content.append(utils.stats2html(settings[\"statsfile\"][0]))\n        html_content.append('<h2 id=\"stats1\">Summary statistics after filtering</h2>')\n        html_content.append(utils.stats2html(settings[\"statsfile\"][1]))\n    else:\n        html_content.append('<h2 id=\"stats0\">Summary statistics</h2>')\n        html_content.append(utils.stats2html(settings[\"statsfile\"][0]))\n\n    # The report itself: plots\n    html_content.append('<h2 id=\"plots\">Plots</h2>')\n    for plot in plots:\n        html_content.append('\\n<h3 id=\"' + plot.title.replace(' ', '_') + '\">' +\n                            plot.title + '</h3>\\n' + plot.encode())\n        html_content.append('\\n<br>\\n<br>\\n<br>\\n<br>')\n    html_body = '\\n'.join(html_content) + '</div></body></html>'\n    html_str = utils.html_head + html_body\n    htmlreport = settings[\"path\"] + \"NanoPlot-report.html\"\n    with open(htmlreport, \"w\") as html_file:\n        html_file.write(html_str)\n    return htmlreport"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef init_logs(args, tool=\"NanoPlot\"):\n    start_time = dt.fromtimestamp(time()).strftime('%Y%m%d_%H%M')\n    logname = os.path.join(args.outdir, args.prefix + tool + \"_\" + start_time + \".log\")\n    handlers = [logging.FileHandler(logname)]\n    if args.verbose:\n        handlers.append(logging.StreamHandler())\n    logging.basicConfig(\n        format='%(asctime)s %(message)s',\n        handlers=handlers,\n        level=logging.INFO)\n    logging.info('{} {} started with arguments {}'.format(tool, __version__, args))\n    logging.info('Python version is: {}'.format(sys.version.replace('\\n', ' ')))\n    return logname", "response": "Initiate log file and log arguments."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning index of records with length - outliers above 3 standard deviations from the median.", "response": "def flag_length_outliers(df, columnname):\n    \"\"\"Return index of records with length-outliers above 3 standard deviations from the median.\"\"\"\n    return df[columnname] > (np.median(df[columnname]) + 3 * np.std(df[columnname]))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfilter the data based on the arguments set on commandline Perform filtering on the data based on the arguments set on commandline Perform transformation on the data based on the arguments set on commandline", "response": "def filter_and_transform_data(df, settings):\n    '''\n    Perform filtering on the data based on arguments set on commandline\n    - use aligned length or sequenced length (bam mode only)\n    - hide outliers from length plots*\n    - hide reads longer than maxlength or shorter than minlength from length plots*\n    - filter reads with a quality below minqual\n    - use log10 scaled reads rather than normal\n    - use empirical percent accuracy rather than phred score quality\n    - downsample reads to args.downsample\n\n    - always: drop reads which are basecaller artefacts\n              judged by length below 20 and quality above 30\n\n    * using a boolean column length_filter\n    '''\n    df[\"length_filter\"] = True\n    settings[\"filtered\"] = False\n\n    if settings.get(\"alength\") and settings.get(\"bam\"):\n        settings[\"lengths_pointer\"] = \"aligned_lengths\"\n        logging.info(\"Using aligned read lengths for plotting.\")\n    else:\n        settings[\"lengths_pointer\"] = \"lengths\"\n        logging.info(\"Using sequenced read lengths for plotting.\")\n\n    if settings.get(\"drop_outliers\"):\n        num_reads_prior = non_filtered_reads(df)\n        df.loc[flag_length_outliers(df, settings[\"lengths_pointer\"]), \"length_filter\"] = False\n        num_reads_post = non_filtered_reads(df)\n        logging.info(\"Hidding {} length outliers in length plots.\".format(\n            str(num_reads_prior - num_reads_post)))\n\n    if settings.get(\"maxlength\"):\n        num_reads_prior = non_filtered_reads(df)\n        df.loc[df[settings[\"lengths_pointer\"]] > settings[\"maxlength\"], \"length_filter\"] = False\n        num_reads_post = non_filtered_reads(df)\n        logging.info(\"Hidding {} reads longer than {}bp in length plots.\".format(\n            str(num_reads_prior - num_reads_post),\n            str(settings[\"maxlength\"])))\n\n    if settings.get(\"minlength\"):\n        num_reads_prior = non_filtered_reads(df)\n        df.loc[df[settings[\"lengths_pointer\"]] < settings[\"minlength\"], \"length_filter\"] = False\n        num_reads_post = non_filtered_reads(df)\n        logging.info(\"Hidding {} reads shorter than {}bp in length plots.\".format(\n            str(num_reads_prior - num_reads_post),\n            str(settings[\"minlength\"])))\n\n    if settings.get(\"minqual\"):\n        num_reads_prior = non_filtered_reads(df)\n        df = df.loc[df[\"quals\"] > settings[\"minqual\"]].copy()\n        num_reads_post = non_filtered_reads(df)\n        logging.info(\"Removing {} reads with quality below Q{}.\".format(\n            str(num_reads_prior - num_reads_post),\n            str(settings[\"minqual\"])))\n        settings[\"filtered\"] = True\n\n    if settings.get(\"loglength\"):\n        df[\"log_\" + settings[\"lengths_pointer\"]] = np.log10(df[settings[\"lengths_pointer\"]])\n        settings[\"lengths_pointer\"] = \"log_\" + settings[\"lengths_pointer\"]\n        logging.info(\"Using log10 scaled read lengths.\")\n        settings[\"logBool\"] = True\n    else:\n        settings[\"logBool\"] = False\n\n    if settings.get(\"runtime_until\"):\n        num_reads_prior = non_filtered_reads(df)\n        df = df[df.start_time < timedelta(hours=settings[\"runtime_until\"])]\n        num_reads_post = non_filtered_reads(df)\n        logging.info(\"Removing {} reads generated after {} hours in the run.\".format(\n            str(num_reads_prior - num_reads_post),\n            str(settings[\"runtime_until\"])))\n        settings[\"filtered\"] = True\n\n    if \"quals\" in df:\n        num_reads_prior = len(df)\n        df = df.loc[-((df[\"lengths\"] < 20) & (df[\"quals\"] > 30))].copy()\n        num_reads_post = len(df)\n        if num_reads_prior - num_reads_post > 0:\n            logging.info(\n                \"Removed {} artefactual reads with very short length and very high quality.\"\n                .format(num_reads_prior - num_reads_post))\n            settings[\"filtered\"] = True\n\n    if settings.get(\"downsample\"):\n        new_size = min(settings[\"downsample\"], len(df))\n        logging.info(\"Downsampling the dataset from {} to {} reads\".format(\n            len(df), new_size))\n        df = df.sample(new_size)\n        settings[\"filtered\"] = True\n\n    if settings.get(\"percentqual\"):\n        df[\"quals\"] = df[\"quals\"].apply(phred_to_percent)\n        logging.info(\"Converting quality scores to theoretical percent identities.\")\n\n    return(df, settings)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _clear_empty_values(args):\n    '''\n    Scrap junk data from a dict.\n    '''\n    result = {}\n    for param in args:\n        if args[param] is not None:\n            result[param] = args[param]\n    return result", "response": "Clear empty values from a dict."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nvalidate that username and password are valid.", "response": "def authentication_validation(username, password, access_token):\n    '''\n    Only accept one form of authentication.\n    '''\n    if bool(username) is not bool(password):\n        raise Exception(\"Basic authentication requires a username AND\"\n                        \" password.\")\n    if (username and access_token) or (password and access_token):\n        raise Exception(\"Cannot use both Basic Authentication and\"\n                        \" OAuth2.0. Please use only one authentication\"\n                        \" method.\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef datasets(self, limit=0, offset=0, order=None, **kwargs):\n        '''\n        Returns the list of datasets associated with a particular domain.\n        WARNING: Large limits (>1000) will return megabytes of data,\n        which can be slow on low-bandwidth networks, and is also a lot of\n        data to hold in memory.\n\n        This method performs a get request on these type of URLs:\n        https://data.edmonton.ca/api/catalog/v1\n\n            limit: max number of results to return, default is all (0)\n            offset: the offset of result set\n            order: field to sort on, optionally with ' ASC' or ' DESC' suffix\n            ids: list of dataset IDs to consider\n            domains: list of domains to search\n            categories: list of categories\n            tags: list of tags\n            only: list of logical types to return, among `api`, `calendar`,\n                `chart`, `datalens`, `dataset`, `federated_href`, `file`,\n                `filter`, `form`, `href`, `link`, `map`, `measure`, `story`,\n                `visualization`\n            shared_to: list of users IDs or team IDs that datasets have to be\n                shared with, or the string `site` meaning anyone on the domain.\n                Note that you may only specify yourself or a team that you are\n                on.\n                Also note that if you search for assets shared to you, assets\n                owned by you might be not be returned.\n            column_names: list of column names that must be present in the\n                tabular datasets\n            q: text query that will be used by Elasticsearch to match results\n            min_should_match: string specifying the number of words from `q`\n                that should match. Refer to Elasticsearch docs for the format,\n                the default is '3<60%', meaning that 60% of the terms must\n                match, or all of them if there are 3 or fewer.\n            attribution: string specifying the organization datasets must come\n                from\n            license: string used to filter on results having a specific license\n            derived_from: string containing the ID of a dataset that must be a\n                parent of the result datasets (for example, charts are derived\n                from a parent dataset)\n            provenance: string 'official' or 'community'\n            for_user: string containing a user ID that must own the returned\n                datasets\n            visibility: string 'open' or 'internal'\n            public: boolean indicating that all returned datasets should be\n                public (True) or private (False)\n            published: boolean indicating that returned datasets should have\n                been published (True) or not yet published (False)\n            approval_status: string 'pending', 'rejected', 'approved',\n                'not_ready' filtering results by their current status in the\n                approval pipeline\n            explicitly_hidden: boolean filtering out datasets that have been\n                explicitly hidden on a domain (False) or returning only those\n                (True)\n            derived: boolean allowing to search only for derived datasets\n                (True) or only those from which other datasets were derived\n                (False)\n        '''\n        # Those filters can be passed multiple times; this function expects\n        # an iterable for them\n        filter_multiple = set(['ids', 'domains', 'categories', 'tags', 'only',\n                               'shared_to', 'column_names'])\n        # Those filters only get a single value\n        filter_single = set([\n            'q', 'min_should_match', 'attribution', 'license', 'derived_from',\n            'provenance', 'for_user', 'visibility', 'public', 'published',\n            'approval_status', 'explicitly_hidden', 'derived'\n        ])\n        all_filters = filter_multiple.union(filter_single)\n        for key in kwargs:\n            if key not in all_filters:\n                raise TypeError(\"Unexpected keyword argument %s\" % key)\n        params = []\n        if limit:\n            params.append(('limit', limit))\n        for key, value in kwargs.items():\n            if key in filter_multiple:\n                for item in value:\n                    params.append((key, item))\n            elif key in filter_single:\n                params.append((key, value))\n        # TODO: custom domain-specific metadata\n        # https://socratadiscovery.docs.apiary.io/\n        #     #reference/0/find-by-domain-specific-metadata\n\n        if order:\n            params.append(('order', order))\n\n        results = self._perform_request(\"get\", DATASETS_PATH,\n                                        params=params + [('offset', offset)])\n        numResults = results['resultSetSize']\n        # no more results to fetch, or limit reached\n        if (limit >= numResults or limit == len(results['results']) or\n                numResults == len(results['results'])):\n            return results['results']\n\n        if limit != 0:\n            raise Exception(\"Unexpected number of results returned from endpoint.\\\n                    Expected {0}, got {1}.\".format(limit, len(results['results'])))\n\n        # get all remaining results\n        all_results = results['results']\n        while len(all_results) != numResults:\n            offset += len(results[\"results\"])\n            results = self._perform_request(\"get\", DATASETS_PATH,\n                                            params=params + [('offset', offset)])\n            all_results.extend(results['results'])\n\n        return all_results", "response": "Returns the list of datasets associated with a particular domain."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a new dataset in the backend.", "response": "def create(self, name, **kwargs):\n        '''\n        Create a dataset, including the field types. Optionally, specify args such as:\n            description : description of the dataset\n            columns : list of columns (see docs/tests for list structure)\n            category : must exist in /admin/metadata\n            tags : list of tag strings\n            row_identifier : field name of primary key\n            new_backend : whether to create the dataset in the new backend\n\n        WARNING: This api endpoint might be deprecated.\n        '''\n        new_backend = kwargs.pop(\"new_backend\", False)\n        resource = _format_old_api_request(content_type=\"json\")\n        if new_backend:\n            resource += \"?nbe=true\"\n\n        payload = {\"name\": name}\n\n        if \"row_identifier\" in kwargs:\n            payload[\"metadata\"] = {\n                \"rowIdentifier\": kwargs.pop(\"row_identifier\", None)\n            }\n\n        payload.update(kwargs)\n        payload = _clear_empty_values(payload)\n\n        return self._perform_update(\"post\", resource, payload)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_permission(self, dataset_identifier, permission=\"private\", content_type=\"json\"):\n        '''\n        Set a dataset's permissions to private or public\n        Options are private, public\n\n        '''\n        resource = _format_old_api_request(dataid=dataset_identifier, content_type=content_type)\n\n        params = {\n            \"method\": \"setPermission\",\n            \"value\": \"public.read\" if permission == \"public\" else permission\n        }\n\n        return self._perform_request(\"put\", resource, params=params)", "response": "Set a dataset s permissions to private or public. Read"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving the metadata for a particular dataset.", "response": "def get_metadata(self, dataset_identifier, content_type=\"json\"):\n        '''\n        Retrieve the metadata for a particular dataset.\n        '''\n        resource = _format_old_api_request(dataid=dataset_identifier, content_type=content_type)\n        return self._perform_request(\"get\", resource)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update_metadata(self, dataset_identifier, update_fields, content_type=\"json\"):\n        '''\n        Update the metadata for a particular dataset.\n            update_fields is a dictionary containing [metadata key:new value] pairs.\n\n        This method performs a full replace for the key:value pairs listed in `update_fields`, and\n        returns all of the metadata with the updates applied.\n        '''\n        resource = _format_old_api_request(dataid=dataset_identifier, content_type=content_type)\n        return self._perform_update(\"put\", resource, update_fields)", "response": "Update the metadata for a particular dataset."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndownload all of the attachments associated with a dataset. Return the paths of downloaded files.", "response": "def download_attachments(self, dataset_identifier, content_type=\"json\",\n                             download_dir=\"~/sodapy_downloads\"):\n        '''\n        Download all of the attachments associated with a dataset. Return the paths of downloaded\n        files.\n        '''\n        metadata = self.get_metadata(dataset_identifier, content_type=content_type)\n        files = []\n        attachments = metadata['metadata'].get(\"attachments\")\n        if not attachments:\n            logging.info(\"No attachments were found or downloaded.\")\n            return files\n\n        download_dir = os.path.join(os.path.expanduser(download_dir), dataset_identifier)\n        if not os.path.exists(download_dir):\n            os.makedirs(download_dir)\n\n        for attachment in attachments:\n            file_path = os.path.join(download_dir, attachment[\"filename\"])\n            has_assetid = attachment.get(\"assetId\", False)\n            if has_assetid:\n                base = _format_old_api_request(dataid=dataset_identifier)\n                assetid = attachment[\"assetId\"]\n                resource = \"{0}/files/{1}?download=true&filename={2}\"\\\n                           .format(base, assetid, attachment[\"filename\"])\n            else:\n                base = \"/api/assets\"\n                assetid = attachment[\"blobId\"]\n                resource = \"{0}/{1}?download=true\".format(base, assetid)\n\n            uri = \"{0}{1}{2}\".format(self.uri_prefix, self.domain, resource)\n            _download_file(uri, file_path)\n            files.append(file_path)\n\n        logging.info(\"The following files were downloaded:\\n\\t{0}\".format(\"\\n\\t\".join(files)))\n        return files"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef publish(self, dataset_identifier, content_type=\"json\"):\n        '''\n        The create() method creates a dataset in a \"working copy\" state.\n        This method publishes it.\n        '''\n        base = _format_old_api_request(dataid=dataset_identifier)\n        resource = \"{0}/publication.{1}\".format(base, content_type)\n\n        return self._perform_request(\"post\", resource)", "response": "Publishes a dataset in a working copy state."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread data from the requested resource.", "response": "def get(self, dataset_identifier, content_type=\"json\", **kwargs):\n        '''\n        Read data from the requested resource. Options for content_type are json,\n        csv, and xml. Optionally, specify a keyword arg to filter results:\n\n            select : the set of columns to be returned, defaults to *\n            where : filters the rows to be returned, defaults to limit\n            order : specifies the order of results\n            group : column to group results on\n            limit : max number of results to return, defaults to 1000\n            offset : offset, used for paging. Defaults to 0\n            q : performs a full text search for a value\n            query : full SoQL query string, all as one parameter\n            exclude_system_fields : defaults to true. If set to false, the\n                response will include system fields (:id, :created_at, and\n                :updated_at)\n\n        More information about the SoQL parameters can be found at the official\n        docs:\n            http://dev.socrata.com/docs/queries.html\n\n        More information about system fields can be found here:\n            http://dev.socrata.com/docs/system-fields.html\n        '''\n        resource = _format_new_api_request(dataid=dataset_identifier, content_type=content_type)\n        headers = _clear_empty_values({\"Accept\": kwargs.pop(\"format\", None)})\n\n        # SoQL parameters\n        params = {\n            \"$select\": kwargs.pop(\"select\", None),\n            \"$where\": kwargs.pop(\"where\", None),\n            \"$order\": kwargs.pop(\"order\", None),\n            \"$group\": kwargs.pop(\"group\", None),\n            \"$limit\": kwargs.pop(\"limit\", None),\n            \"$offset\": kwargs.pop(\"offset\", None),\n            \"$q\": kwargs.pop(\"q\", None),\n            \"$query\": kwargs.pop(\"query\", None),\n            \"$$exclude_system_fields\": kwargs.pop(\"exclude_system_fields\",\n                                                  None)\n        }\n\n        # Additional parameters, such as field names\n        params.update(kwargs)\n        params = _clear_empty_values(params)\n\n        response = self._perform_request(\"get\", resource, headers=headers,\n                                         params=params)\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef upsert(self, dataset_identifier, payload, content_type=\"json\"):\n        '''\n        Insert, update or delete data to/from an existing dataset. Currently\n        supports json and csv file objects. See here for the upsert\n        documentation:\n            http://dev.socrata.com/publishers/upsert.html\n        '''\n        resource = _format_new_api_request(dataid=dataset_identifier, content_type=content_type)\n\n        return self._perform_update(\"post\", resource, payload)", "response": "Insert update or delete data to an existing dataset."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a new file - based dataset with the name provided in the files tuple.", "response": "def create_non_data_file(self, params, file_data):\n        '''\n        Creates a new file-based dataset with the name provided in the files\n        tuple.  A valid file input would be:\n        files = (\n            {'file': (\"gtfs2\", open('myfile.zip', 'rb'))}\n        )\n        '''\n        api_prefix = '/api/imports2/'\n\n        if not params.get('method', None):\n            params['method'] = 'blob'\n\n        return self._perform_request(\"post\", api_prefix, params=params, files=file_data)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef replace_non_data_file(self, dataset_identifier, params, file_data):\n        '''\n        Same as create_non_data_file, but replaces a file that already exists in a\n        file-based dataset.\n\n        WARNING: a table-based dataset cannot be replaced by a file-based dataset.\n                 Use create_non_data_file in order to replace.\n        '''\n        resource = _format_old_api_request(dataid=dataset_identifier, content_type=\"txt\")\n\n        if not params.get('method', None):\n            params['method'] = 'replaceBlob'\n\n        params['id'] = dataset_identifier\n\n        return self._perform_request(\"post\", resource, params=params, files=file_data)", "response": "Replaces a file in a file - based dataset."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _perform_update(self, method, resource, payload):\n        '''\n        Execute the update task.\n        '''\n\n        # python2/3 compatibility wizardry\n        try:\n            file_type = file\n        except NameError:\n            file_type = IOBase\n\n        if isinstance(payload, (dict, list)):\n            response = self._perform_request(method, resource,\n                                             data=json.dumps(payload))\n        elif isinstance(payload, file_type):\n            headers = {\n                \"content-type\": \"text/csv\",\n            }\n            response = self._perform_request(method, resource, data=payload,\n                                             headers=headers)\n        else:\n            raise Exception(\"Unrecognized payload {0}. Currently only list-, dictionary-,\"\n                            \" and file-types are supported.\".format(type(payload)))\n\n        return response", "response": "Execute the update task."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndeleting the entire dataset or a single row.", "response": "def delete(self, dataset_identifier, row_id=None, content_type=\"json\"):\n        '''\n        Delete the entire dataset, e.g.\n            client.delete(\"nimj-3ivp\")\n        or a single row, e.g.\n            client.delete(\"nimj-3ivp\", row_id=4)\n        '''\n        if row_id:\n            resource = _format_new_api_request(dataid=dataset_identifier, row_id=row_id,\n                                               content_type=content_type)\n        else:\n            resource = _format_old_api_request(dataid=dataset_identifier,\n                                               content_type=content_type)\n\n        return self._perform_request(\"delete\", resource)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _perform_request(self, request_type, resource, **kwargs):\n        '''\n        Utility method that performs all requests.\n        '''\n        request_type_methods = set([\"get\", \"post\", \"put\", \"delete\"])\n        if request_type not in request_type_methods:\n            raise Exception(\"Unknown request type. Supported request types are\"\n                            \": {0}\".format(\", \".join(request_type_methods)))\n\n        uri = \"{0}{1}{2}\".format(self.uri_prefix, self.domain, resource)\n\n        # set a timeout, just to be safe\n        kwargs[\"timeout\"] = self.timeout\n\n        response = getattr(self.session, request_type)(uri, **kwargs)\n\n        # handle errors\n        if response.status_code not in (200, 202):\n            _raise_for_status(response)\n\n        # when responses have no content body (ie. delete, set_permission),\n        # simply return the whole response\n        if not response.text:\n            return response\n\n        # for other request types, return most useful data\n        content_type = response.headers.get('content-type').strip().lower()\n        if re.match(r'application\\/json', content_type):\n            return response.json()\n        elif re.match(r'text\\/csv', content_type):\n            csv_stream = StringIO(response.text)\n            return [line for line in csv.reader(csv_stream)]\n        elif re.match(r'application\\/rdf\\+xml', content_type):\n            return response.content\n        elif re.match(r'text\\/plain', content_type):\n            try:\n                return json.loads(response.text)\n            except ValueError:\n                return response.text\n        else:\n            raise Exception(\"Unknown response format: {0}\"\n                            .format(content_type))", "response": "Utility method that performs all requests."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef exec_request(self, URL):\n\n        ## Throttle request, if need be\n        interval = time.time() - self.__ts_last_req\n        if (interval < self.__min_req_interval):\n            time.sleep( self.__min_req_interval - interval )\n        \n        ## Construct and execute request\n        headers = {\n            \"X-ELS-APIKey\"  : self.api_key,\n            \"User-Agent\"    : self.__user_agent,\n            \"Accept\"        : 'application/json'\n            }\n        if self.inst_token:\n            headers[\"X-ELS-Insttoken\"] = self.inst_token\n        logger.info('Sending GET request to ' + URL)\n        r = requests.get(\n            URL,\n            headers = headers\n            )\n        self.__ts_last_req = time.time()\n        self._status_code=r.status_code\n        if r.status_code == 200:\n            self._status_msg='data retrieved'\n            return json.loads(r.text)\n        else:\n            self._status_msg=\"HTTP \" + str(r.status_code) + \" Error from \" + URL + \" and using headers \" + str(headers) + \": \" + r.text\n            raise requests.HTTPError(\"HTTP \" + str(r.status_code) + \" Error from \" + URL + \"\\nand using headers \" + str(headers) + \":\\n\" + r.text)", "response": "Sends the actual request ; returns response."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef read(self, payloadType, elsClient):\n        if elsClient:\n            self._client = elsClient;\n        elif not self.client:\n            raise ValueError('''Entity object not currently bound to elsClient instance. Call .read() with elsClient argument or set .client attribute.''')\n        try:\n            api_response = self.client.exec_request(self.uri)\n            if isinstance(api_response[payloadType], list):\n                self._data = api_response[payloadType][0]\n            else:\n                self._data = api_response[payloadType]\n            ## TODO: check if URI is the same, if necessary update and log warning.\n            logger.info(\"Data loaded for \" + self.uri)\n            return True\n        except (requests.HTTPError, requests.RequestException) as e:\n            for elm in e.args:\n                logger.warning(elm)\n            return False", "response": "Fetches the latest data for this entity from api. elsevier. com.\n            Returns True if successful ; else False."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrite the data to disk as a. JSON file. Returns True if the data was written to disk and returns False otherwise.", "response": "def write(self):\n        \"\"\"If data exists for the entity, writes it to disk as a .JSON file with\n             the url-encoded URI as the filename and returns True. Else, returns\n             False.\"\"\"\n        if (self.data):\n            dataPath = self.client.local_dir / (urllib.parse.quote_plus(self.uri)+'.json')\n            with dataPath.open(mode='w') as dump_file:\n                json.dump(self.data, dump_file)\n                dump_file.close()\n            logger.info('Wrote ' + self.uri + ' to file')\n            return True\n        else:\n            logger.warning('No data to write for ' + self.uri)\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfetch the list of documents associated with this entity from the elsevier. com. If need be splits the requests in batches to retrieve them all. Returns True if successful ; else False.", "response": "def read_docs(self, payloadType, els_client = None):\n        \"\"\"Fetches the list of documents associated with this entity from\n            api.elsevier.com. If need be, splits the requests in batches to\n            retrieve them all. Returns True if successful; else, False.\n\t\t\tNOTE: this method requires elevated API permissions.\n\t\t\tSee http://bit.ly/2leirnq for more info.\"\"\"\n        if els_client:\n            self._client = els_client;\n        elif not self.client:\n            raise ValueError('''Entity object not currently bound to els_client instance. Call .read() with els_client argument or set .client attribute.''')\n        try:\n            api_response = self.client.exec_request(self.uri + \"?view=documents\")\n            if isinstance(api_response[payloadType], list):\n                data = api_response[payloadType][0]\n            else:\n                data = api_response[payloadType]\n            docCount = int(data[\"documents\"][\"@total\"])\n            self._doc_list = [x for x in data[\"documents\"][\"abstract-document\"]]\n            for i in range (0, docCount//self.client.num_res):\n                try:\n                    api_response = self.client.exec_request(self.uri + \"?view=documents&start=\" + str((i+1) * self.client.num_res+1))\n                    if isinstance(api_response[payloadType], list):\n                        data = api_response[payloadType][0]\n                    else:\n                        data = api_response[payloadType]\n                    self._doc_list = self._doc_list + [x for x in data[\"documents\"][\"abstract-document\"]]\n                except  (requests.HTTPError, requests.RequestException) as e:\n                    if hasattr(self, 'doc_list'):       ## We don't want incomplete doc lists\n                        self._doc_list = None\n                    raise e\n            logger.info(\"Documents loaded for \" + self.uri)\n            return True\n        except (requests.HTTPError, requests.RequestException) as e:\n            logger.warning(e.args)\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwrites the doclist to disk as a. JSON file with the url - encoded URI as the filename and returns True. Otherwise returns False.", "response": "def write_docs(self):\n        \"\"\"If a doclist exists for the entity, writes it to disk as a .JSON file\n             with the url-encoded URI as the filename and returns True. Else,\n             returns False.\"\"\"\n        if self.doc_list:\n            dataPath = self.client.local_dir\n            dump_file = open('data/'\n                             + urllib.parse.quote_plus(self.uri+'?view=documents')\n                             + '.json', mode='w'\n                             )\n            dump_file.write('[' + json.dumps(self.doc_list[0]))\n            for i in range (1, len(self.doc_list)):\n                dump_file.write(',' + json.dumps(self.doc_list[i]))\n            dump_file.write(']')\n            dump_file.close()\n            logger.info('Wrote ' + self.uri + '?view=documents to file')\n            return True\n        else:\n            logger.warning('No doclist to write for ' + self.uri)\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read(self, els_client = None):\n        if ElsProfile.read(self, self.__payload_type, els_client):\n            return True\n        else:\n            return False", "response": "Reads the JSON representation of the author from ELSAPI."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read_metrics(self, els_client = None):\n        try:\n            api_response = els_client.exec_request(self.uri + \"?field=document-count,cited-by-count,citation-count,h-index,dc:identifier\")\n            data = api_response[self.__payload_type][0]\n            if not self.data:\n                self._data = dict()\n                self._data['coredata'] = dict()\n            self._data['coredata']['dc:identifier'] = data['coredata']['dc:identifier']\n            self._data['coredata']['citation-count'] = int(data['coredata']['citation-count'])\n            self._data['coredata']['cited-by-count'] = int(data['coredata']['citation-count'])\n            self._data['coredata']['document-count'] = int(data['coredata']['document-count'])\n            self._data['h-index'] = int(data['h-index'])\n            logger.info('Added/updated author metrics')\n        except (requests.HTTPError, requests.RequestException) as e:\n            logger.warning(e.args)\n            return False\n        return True", "response": "Reads the bibliographic metrics for this author from api. elsevier. com\n             and updates self. data with them. Returns True if successful ; else False."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef read(self, els_client = None):\n        if super().read(self.__payload_type, els_client):\n            return True\n        else:\n            return False", "response": "Reads the JSON representation of the document from ELSAPI."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef execute(self, els_client = None, get_all = False):\n        ## TODO: add exception handling\n        api_response = els_client.exec_request(self._uri)\n        self._tot_num_res = int(api_response['search-results']['opensearch:totalResults'])\n        self._results = api_response['search-results']['entry']\n        if get_all is True:\n            while (self.num_res < self.tot_num_res) and (self.num_res < 5000):\n                for e in api_response['search-results']['link']:\n                    if e['@ref'] == 'next':\n                        next_url = e['@href']\n                api_response = els_client.exec_request(next_url)\n                self._results += api_response['search-results']['entry']", "response": "Executes the search. If get_all = False (default), this retrieves\n            the default number of results specified for the API. If\n            get_all = True, multiple API calls will be made to iteratively get \n            all results for the search, up to a maximum of 5,000."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _extract_resources(elem):\n        resources = collections.defaultdict(list)\n        for child in itertools.islice(elem.iter(), 1, None):\n            try:\n                basename = child.tag.split('}', 1)[-1]\n                if child.text is not None:\n                    child.text = child.text.strip()\n                if child.text:\n                    resources[basename].append(child.text)\n                elif child.get(RDF_RESOURCE) is not None:\n                    resources[basename].append(child.get(RDF_RESOURCE))\n            except AttributeError:\n                pass\n        return dict(resources)", "response": "Extract the children of an element as a key - value mapping."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _iter_rawterms(cls, tree):\n        for elem in tree.iterfind(OWL_CLASS):\n            if RDF_ABOUT not in elem.keys():   # This avoids parsing a class\n                continue                       # created by restriction\n            rawterm = cls._extract_resources(elem)\n            rawterm['id'] = cls._get_id_from_url(elem.get(RDF_ABOUT))\n            yield rawterm", "response": "Iterate through the raw terms in the ontology."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _extract_obo_synonyms(rawterm):\n        synonyms = set()\n        # keys in rawterm that define a synonym\n        keys = set(owl_synonyms).intersection(rawterm.keys())\n        for k in keys:\n            for s in rawterm[k]:\n                synonyms.add(Synonym(s, owl_synonyms[k]))\n        return synonyms", "response": "Extract the synonyms defined in the rawterm."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nextracts the relationships defined in the rawterm.", "response": "def _extract_obo_relation(cls, rawterm):\n        \"\"\"Extract the relationships defined in the rawterm.\n        \"\"\"\n        relations = {}\n        if 'subClassOf' in rawterm:\n            relations[Relationship('is_a')] = l = []\n            l.extend(map(cls._get_id_from_url, rawterm.pop('subClassOf')))\n        return relations"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchanging the keys of d to use Obo labels.", "response": "def _relabel_to_obo(d):\n        \"\"\"Change the keys of ``d`` to use Obo labels.\n        \"\"\"\n        return {\n            owl_to_obo.get(old_k, old_k): old_v\n                for old_k, old_v in six.iteritems(d)\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the complementary relationship of the current object.", "response": "def complement(self):\n        \"\"\"Return the complementary relationship of self.\n\n        Raises:\n            ValueError: if the relationship has a complementary\n                which was not defined.\n\n        Returns:\n            complementary (Relationship): the complementary relationship.\n\n        Example:\n\n            >>> from pronto.relationship import Relationship\n            >>> print(Relationship('has_part').complement())\n            Relationship('part_of')\n            >>> print(Relationship('has_units').complement())\n            None\n\n        \"\"\"\n        if self.complementary:\n\n            #if self.complementary in self._instances.keys():\n            try:\n                return self._instances[self.complementary]\n            except KeyError:\n                raise ValueError('{} has a complementary but it was not defined !')\n\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef topdown(cls):\n        return tuple(unique_everseen(r for r in cls._instances.values() if r.direction=='topdown'))", "response": "Get all topdown Relationship instances."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef bottomup(cls):\n        return tuple(unique_everseen(r for r in cls._instances.values() if r.direction=='bottomup'))", "response": "Get all bottomup Relationship instances."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef obo(self):\n\n        lines = [\n            \"[Typedef]\",\n            \"id: {}\".format(self.obo_name),\n            \"name: {}\".format(self.obo_name)\n        ]\n        if self.complementary is not None:\n            lines.append(\"inverse_of: {}\".format(self.complementary))\n        if self.symmetry is not None:\n            lines.append(\"is_symmetric: {}\".format(self.symmetry).lower())\n        if self.transitivity is not None:\n            lines.append(\"is_transitive: {}\".format(self.transitivity).lower())\n        if self.reflexivity is not None:\n            lines.append(\"is_reflexive: {}\".format(self.reflexivity).lower())\n        if self.comment:\n            lines.append(\"comment: {}\".format(self.comment))\n        return \"\\n\".join(lines)", "response": "str - the Relationship serialized in an [ Typedef ] stanza."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nlists unique elements preserving order. Remember all elements ever seen.", "response": "def unique_everseen(iterable):\n    \"\"\"List unique elements, preserving order. Remember all elements ever seen.\"\"\"\n    # unique_everseen('AAAABBBCCDAABBB')    --> A B C D\n    seen = set()\n    seen_add = seen.add\n\n    for element in six.moves.filterfalse(seen.__contains__, iterable):\n        seen_add(element)\n        yield element"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef output_str(f):\n    if six.PY2:\n        #@functools.wraps(f)\n        def new_f(*args, **kwargs):\n            return f(*args, **kwargs).encode(\"utf-8\")\n    else:\n        new_f = f\n    return new_f", "response": "Create a function that always return instances of str."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef nowarnings(func):\n    @functools.wraps(func)\n    def new_func(*args, **kwargs):\n        with warnings.catch_warnings():\n            warnings.simplefilter('ignore')\n            return func(*args, **kwargs)\n    return new_func", "response": "Create a function wrapped in a context that ignores warnings."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse the given file using available parsers.", "response": "def parse(self, stream, parser=None):\n        \"\"\"Parse the given file using available `BaseParser` instances.\n\n        Raises:\n            TypeError: when the parser argument is not a string or None.\n            ValueError: when the parser argument is a string that does\n                not name a `BaseParser`.\n\n        \"\"\"\n        force, parsers = self._get_parsers(parser)\n\n        try:\n            stream.seek(0)\n            lookup = stream.read(1024)\n            stream.seek(0)\n        except (io.UnsupportedOperation, AttributeError):\n            lookup = None\n\n        for p in parsers:\n            if p.hook(path=self.path, force=force, lookup=lookup):\n                self.meta, self.terms, self.imports, self.typedefs = p.parse(stream)\n                self._parsed_by = p.__name__\n                break"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_parsers(self, name):\n\n        parserlist = BaseParser.__subclasses__()\n        forced = name is None\n\n        if isinstance(name, (six.text_type, six.binary_type)):\n            parserlist = [p for p in parserlist if p.__name__ == name]\n            if not parserlist:\n                raise ValueError(\"could not find parser: {}\".format(name))\n\n        elif name is not None:\n            raise TypeError(\"parser must be {types} or None, not {actual}\".format(\n                types=\" or \".join([six.text_type.__name__, six.binary_type.__name__]),\n                actual=type(parser).__name__,\n            ))\n\n        return not forced, parserlist", "response": "Return the appropriate parser for the given name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef adopt(self):\n        valid_relationships = set(Relationship._instances.keys())\n\n        relationships = [\n            (parent, relation.complement(), term.id)\n                for term in six.itervalues(self.terms)\n                    for relation in term.relations\n                        for parent in term.relations[relation]\n                            if relation.complementary\n                                and relation.complementary in valid_relationships\n        ]\n\n        relationships.sort(key=operator.itemgetter(2))\n\n        for parent, rel, child in relationships:\n\n            if rel is None:\n                break\n\n            try:\n                parent = parent.id\n            except AttributeError:\n                pass\n\n            if parent in self.terms:\n                try:\n                    if child not in self.terms[parent].relations[rel]:\n                        self.terms[parent].relations[rel].append(child)\n                except KeyError:\n                    self[parent].relations[rel] = [child]\n\n        del relationships", "response": "Make terms aware of their children."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmaking relations point to ontology terms instead of term ids.", "response": "def reference(self):\n        \"\"\"Make relations point to ontology terms instead of term ids.\n\n        This is done automatically when using the :obj:`merge` and :obj:`include`\n        methods as well as the :obj:`__init__` method, but it should be called in\n        case of manual changes of the relationships of a Term.\n        \"\"\"\n        for termkey, termval in six.iteritems(self.terms):\n            termval.relations.update(\n                (relkey, TermList(\n                    (self.terms.get(x) or Term(x, '', '')\n                    if not isinstance(x, Term) else x) for x in relval\n                )) for relkey, relval in six.iteritems(termval.relations)\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nresolves the imports of the ontology.", "response": "def resolve_imports(self, imports, import_depth, parser=None):\n        \"\"\"Import required ontologies.\n        \"\"\"\n        if imports and import_depth:\n            for i in list(self.imports):\n                try:\n\n                    if os.path.exists(i) or i.startswith(('http', 'ftp')):\n                        self.merge(Ontology(i, import_depth=import_depth-1, parser=parser))\n\n                    else: # try to look at neighbouring ontologies\n                        self.merge(Ontology( os.path.join(os.path.dirname(self.path), i),\n                                             import_depth=import_depth-1, parser=parser))\n\n                except (IOError, OSError, URLError, HTTPError, _etree.ParseError) as e:\n                    warnings.warn(\"{} occured during import of \"\n                                  \"{}\".format(type(e).__name__, i),\n                                  ProntoWarning)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef include(self, *terms):\n        ref_needed = False\n\n        for term in terms:\n\n            if isinstance(term, TermList):\n                ref_needed = ref_needed or self._include_term_list(term)\n            elif isinstance(term, Term):\n                ref_needed = ref_needed or self._include_term(term)\n            else:\n                raise TypeError('include only accepts <Term> or <TermList> as arguments')\n\n        self.adopt()\n        self.reference()", "response": "Add new terms to the current ontology."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef merge(self, other):\n        if not isinstance(other, Ontology):\n            raise TypeError(\"'merge' requires an Ontology as argument,\"\n                            \" not {}\".format(type(other)))\n\n        self.terms.update(other.terms)\n        self._empty_cache()\n        self.adopt()\n        self.reference()", "response": "Merge another ontology into the current one."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _include_term_list(self, termlist):\n        ref_needed = False\n        for term in termlist:\n            ref_needed = ref_needed or self._include_term(term)\n        return ref_needed", "response": "Add terms from a TermList to the ontology."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _include_term(self, term):\n        ref_needed = False\n\n        if term.relations:\n\n            for k,v in six.iteritems(term.relations):\n                for i,t in enumerate(v):\n\n                    #if isinstance(t, Term):\n                    try:\n\n                        if t.id not in self:\n                            self._include_term(t)\n\n                        v[i] = t.id\n\n                    except AttributeError:\n                        pass\n\n                    ref_needed = True\n\n        self.terms[term.id] = term\n        return ref_needed", "response": "Add a single term to the current ontology."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nemptying the cache associated with each Term instance.", "response": "def _empty_cache(self, termlist=None):\n        \"\"\"Empty the cache associated with each `Term` instance.\n\n        This method is called when merging Ontologies or including\n        new terms in the Ontology to make sure the cache of each\n        term is cleaned and avoid returning wrong memoized values\n        (such as Term.rchildren() TermLists, which get memoized for\n        performance concerns)\n        \"\"\"\n        if termlist is None:\n            for term in six.itervalues(self.terms):\n                term._empty_cache()\n        else:\n            for term in termlist:\n                try:\n                    self.terms[term.id]._empty_cache()\n                except AttributeError:\n                    self.terms[term]._empty_cache()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _obo_meta(self):\n        metatags = (\n            \"format-version\", \"data-version\", \"date\", \"saved-by\",\n            \"auto-generated-by\", \"import\", \"subsetdef\", \"synonymtypedef\",\n            \"default-namespace\", \"namespace-id-rule\", \"idspace\",\n            \"treat-xrefs-as-equivalent\", \"treat-xrefs-as-genus-differentia\",\n            \"treat-xrefs-as-is_a\", \"remark\", \"ontology\"\n        )\n\n        meta = self.meta.copy()\n        meta['auto-generated-by'] = ['pronto v{}'.format(__version__)]\n        meta['date'] = [datetime.datetime.now().strftime('%d:%m:%Y %H:%M')]\n\n        obo_meta = \"\\n\".join(\n\n            [ # official obo tags\n                x.obo if hasattr(x, 'obo') \\\n                    else \"{}: {}\".format(k,x)\n                        for k in metatags[:-1]\n                            for x in meta.get(k, ())\n            ] + [ # eventual other metadata added to remarksmock.patch in production code\n                \"remark: {}: {}\".format(k, x)\n                    for k,v in sorted(six.iteritems(meta), key=operator.itemgetter(0))\n                        for x in v\n                            if k not in metatags\n            ] + (     [\"ontology: {}\".format(x) for x in meta[\"ontology\"]]\n                            if \"ontology\" in meta\n                 else [\"ontology: {}\".format(meta[\"namespace\"][0].lower())]\n                            if \"namespace\" in meta\n                 else [])\n\n        )\n\n        return obo_meta", "response": "Generate the obo metadata header and updates metadata."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef json(self):\n        return json.dumps(self.terms, indent=4, sort_keys=True,\n                          default=operator.attrgetter(\"__deref__\"))", "response": "str : the ontology serialized in json format."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef obo(self):\n        meta = self._obo_meta()\n        meta = [meta] if meta else []\n        newline = \"\\n\\n\" if six.PY3 else \"\\n\\n\".encode('utf-8')\n\n        try: # if 'namespace' in self.meta:\n            return newline.join( meta + [\n                r.obo for r in self.typedefs\n            ] + [\n                t.obo for t in self\n                    if t.id.startswith(self.meta['namespace'][0])\n            ])\n        except KeyError:\n            return newline.join( meta + [\n                r.obo for r in self.typedefs\n            ] + [\n                t.obo for t in self\n            ])", "response": "str : the ontology serialized in obo format."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parents(self):\n        if self._parents is None:\n            bottomups = tuple(Relationship.bottomup())\n\n            self._parents = TermList()\n            self._parents.extend(\n                [ other\n                    for rship,others in six.iteritems(self.relations)\n                        for other in others\n                            if rship in bottomups\n                ]\n\n            )\n\n        return self._parents", "response": "Returns a TermList containing the direct parents of the Term."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef children(self):\n        if self._children is None:\n            topdowns = tuple(Relationship.topdown())\n            self._children = TermList()\n            self._children.extend(\n                [ other\n                    for rship,others in six.iteritems(self.relations)\n                        for other in others\n                            if rship in topdowns\n                ]\n            )\n        return self._children", "response": "Returns a TermList containing the direct children of the Term."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef obo(self):\n        def add_tags(stanza_list, tags):\n            for tag in tags:\n                if tag in self.other:\n                    if isinstance(self.other[tag], list):\n                        for attribute in self.other[tag]:\n                            stanza_list.append(\"{}: {}\".format(tag, attribute))\n                    else:\n                        stanza_list.append(\"{}: {}\".format(tag, self.other[tag]))\n\n        # metatags = [\"id\", \"is_anonymous\", \"name\", \"namespace\",\"alt_id\", \"def\",\"comment\",\n        #             \"subset\",\"synonym\",\"xref\",\"builtin\",\"property_value\",\"is_a\",\n        #             \"intersection_of\",\"union_of\",\"equivalent_to\",\"disjoint_from\",\n        #             \"relationship\",\"created_by\",\"creation_date\",\"is_obsolete\",\n        #             \"replaced_by\", \"consider\"]\n\n        stanza_list = [\"[Term]\"]\n\n        # id\n        stanza_list.append(\"id: {}\".format(self.id))\n\n\n        # name\n        if self.name is not None:\n            stanza_list.append(\"name: {}\".format(self.name))\n        else:\n            stanza_list.append(\"name: \")\n\n        add_tags(stanza_list, ['is_anonymous', 'alt_id'])\n\n        # def\n        if self.desc:\n            stanza_list.append(self.desc.obo)\n\n        # comment, subset\n        add_tags(stanza_list, ['comment', 'subset'])\n\n        # synonyms\n        for synonym in sorted(self.synonyms, key=str):\n            stanza_list.append(synonym.obo)\n\n        add_tags(stanza_list, ['xref'])\n\n        # is_a\n        if Relationship('is_a') in self.relations:\n            for companion in self.relations[Relationship('is_a')]:\n                stanza_list.append(\"is_a: {} ! {}\".format(companion.id, companion.name))\n\n        add_tags(stanza_list, ['intersection_of', 'union_of', 'disjoint_from'])\n\n        for relation in self.relations:\n            if relation.direction==\"bottomup\" and relation is not Relationship('is_a'):\n                stanza_list.extend(\n                    \"relationship: {} {} ! {}\".format(\n                        relation.obo_name, companion.id, companion.name\n                    ) for companion in self.relations[relation]\n                )\n\n        add_tags(stanza_list, ['is_obsolete', 'replaced_by', 'consider',\n                               'builtin', 'created_by', 'creation_date'])\n\n        return \"\\n\".join(stanza_list)", "response": "str - the Term serialized in an Obo - 1. 4 stanza."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nempties the cache of the Term s memoized functions.", "response": "def _empty_cache(self):\n        \"\"\"Empty the cache of the Term's memoized functions.\n        \"\"\"\n        self._children, self._parents = None, None\n        self._rchildren, self._rparents = {}, {}"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a recursive list of children.", "response": "def rchildren(self, level=-1, intermediate=True):\n        \"\"\"Create a recursive list of children.\n\n        Parameters:\n            level (int): The depth level to continue fetching children from\n                (default is -1, to get children to the utter depths)\n            intermediate (bool): Also include the intermediate children\n                (default is True)\n\n        Returns:\n            :obj:`pronto.TermList`:\n            The recursive children of the Term following the parameters\n\n        \"\"\"\n        try:\n            return self._rchildren[(level, intermediate)]\n\n        except KeyError:\n\n            rchildren = []\n\n            if self.children and level:\n\n                if intermediate or level==1:\n                    rchildren.extend(self.children)\n\n                for child in self.children:\n                    rchildren.extend(child.rchildren(level=level-1,\n                                                     intermediate=intermediate))\n\n            rchildren = TermList(unique_everseen(rchildren))\n            self._rchildren[(level, intermediate)] = rchildren\n            return rchildren"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a recursive list of children.", "response": "def rparents(self, level=-1, intermediate=True):\n        \"\"\"Create a recursive list of children.\n\n        Note that the :param:`intermediate` can be used to include every\n        parents to the returned list, not only the most nested ones.\n\n        Parameters:\n            level (int): The depth level to continue fetching parents from\n                (default is -1, to get parents to the utter depths)\n            intermediate (bool): Also include the intermediate parents\n                (default is True)\n\n        Returns:\n            :obj:`pronto.TermList`:\n            The recursive children of the Term following the parameters\n\n        \"\"\"\n        try:\n            return self._rparents[(level, intermediate)]\n\n        except KeyError:\n\n            rparents = []\n\n            if self.parents and level:\n\n                if intermediate or level==1:\n                    rparents.extend(self.parents)\n\n                for parent in self.parents:\n                    rparents.extend(parent.rparents(level=level-1,\n                                                     intermediate=intermediate))\n\n            rparents = TermList(unique_everseen(rparents))\n            self._rparents[(level, intermediate)] = rparents\n            return rparents"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a TermList containing all the children of all the terms in the list.", "response": "def children(self):\n        \"\"\"~TermList: the children of all the terms in the list.\n        \"\"\"\n        return TermList(unique_everseen(\n            y for x in self for y in x.children\n        ))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a TermList containing all the parents of all the terms in the list.", "response": "def parents(self):\n        \"\"\"~TermList: the parents of all the terms in the list.\n        \"\"\"\n        return TermList(unique_everseen(\n            y for x in self for y in x.parents\n        ))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _check_section(line, section):\n        if \"[Term]\" in line:\n            section = OboSection.term\n        elif \"[Typedef]\" in line:\n            section = OboSection.typedef\n        return section", "response": "Update the section being parsed."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _parse_metadata(cls, line, meta, parse_remarks=True):\n        key, value = line.split(':', 1)\n        key, value = key.strip(), value.strip()\n        if parse_remarks and \"remark\" in key:                        # Checking that the ':' is not\n            if 0<value.find(': ')<20:                                # not too far avoid parsing a sentence\n                try:                                                 # containing a ':' as a key: value\n                    cls._parse_metadata(value, meta, parse_remarks)  # obo statement nested in a remark\n                except ValueError:                                   # (20 is arbitrary, it may require\n                    pass                                             # tweaking)\n        else:\n            meta[key].append(value)\n            try:\n                syn_type_def = []\n                for m in meta['synonymtypedef']:\n                    if not isinstance(m, SynonymType):\n                        x = SynonymType.from_obo(m)\n                        syn_type_def.append(x)\n                    else:\n                        syn_type_def.append(m)\n            except KeyError:\n                pass\n            else:\n                meta['synonymtypedef'] = syn_type_def", "response": "Parse a metadata line."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses a typedef line.", "response": "def _parse_typedef(line, _rawtypedef):\n        \"\"\"Parse a typedef line.\n\n        The typedef is organized as a succesion of ``key:value`` pairs\n        that are extracted into the same dictionnary until a new\n        header is encountered\n\n        Arguments:\n            line (str): the line containing a typedef statement\n        \"\"\"\n        if \"[Typedef]\" in line:\n            _rawtypedef.append(collections.defaultdict(list))\n        else:\n            key, value = line.split(':', 1)\n            _rawtypedef[-1][key.strip()].append(value.strip())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing a term line.", "response": "def _parse_term(_rawterms):\n        \"\"\"Parse a term line.\n\n        The term is organized as a succesion of ``key:value`` pairs\n        that are extracted into the same dictionnary until a new\n        header is encountered\n\n        Arguments:\n            line (str): the line containing a term statement\n        \"\"\"\n        line = yield\n        _rawterms.append(collections.defaultdict(list))\n        while True:\n            line = yield\n            if \"[Term]\" in line:\n                _rawterms.append(collections.defaultdict(list))\n            else:\n                key, value = line.split(':', 1)\n                _rawterms[-1][key.strip()].append(value.strip())"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates proper objects out of extracted dictionnaries.", "response": "def _classify(_rawtypedef, _rawterms):\n        \"\"\"Create proper objects out of extracted dictionnaries.\n\n        New Relationship objects are instantiated with the help of\n        the `Relationship._from_obo_dict` alternate constructor.\n\n        New `Term` objects are instantiated by manually extracting id,\n        name, desc and relationships out of the ``_rawterm``\n        dictionnary, and then calling the default constructor.\n        \"\"\"\n        terms = collections.OrderedDict()\n        _cached_synonyms = {}\n\n        typedefs = [\n            Relationship._from_obo_dict( # instantiate a new Relationship\n                {k:v for k,lv in six.iteritems(_typedef) for v in lv}\n            )\n            for _typedef in _rawtypedef\n        ]\n\n        for _term in _rawterms:\n            synonyms = set()\n\n            _id   = _term['id'][0]\n            _name = _term.pop('name', ('',))[0]\n            _desc = _term.pop('def', ('',))[0]\n\n            _relations = collections.defaultdict(list)\n            try:\n                for other in _term.get('is_a', ()):\n                    _relations[Relationship('is_a')].append(other.split('!')[0].strip())\n            except IndexError:\n                pass\n            try:\n                for relname, other in ( x.split(' ', 1) for x in _term.pop('relationship', ())):\n                    _relations[Relationship(relname)].append(other.split('!')[0].strip())\n            except IndexError:\n                pass\n\n            for key, scope in six.iteritems(_obo_synonyms_map):\n                for obo_header in _term.pop(key, ()):\n                    try:\n                        s = _cached_synonyms[obo_header]\n                    except KeyError:\n                         s = Synonym.from_obo(obo_header, scope)\n                         _cached_synonyms[obo_header] = s\n                    finally:\n                        synonyms.add(s)\n\n            desc = Description.from_obo(_desc) if _desc else Description(\"\")\n\n            terms[_id] = Term(_id, _name, desc, dict(_relations), synonyms, dict(_term))\n        return terms, typedefs"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef obo(self):\n        return ' '.join(['synonymtypedef:', self.name,\n                         '\"{}\"'.format(self.desc),\n                         self.scope or '']).strip()", "response": "str : the synonym type serialized in obo format."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef validate(cpf_number):\n\n    _cpf = compat.clear_punctuation(cpf_number)\n\n    if (len(_cpf) != 11 or\n       len(set(_cpf)) == 1):\n        return False\n\n    first_part = _cpf[:9]\n    second_part = _cpf[:10]\n    first_digit = _cpf[9]\n    second_digit = _cpf[10]\n\n    if (first_digit == calc.calculate_first_digit(first_part) and\n       second_digit == calc.calculate_second_digit(second_part)):\n        return True\n\n    return False", "response": "This function validates a CPF number."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef xml_open(filename, expected_root=None):\n\n    # Is the file a zip (.twbx or .tdsx)\n    if zipfile.is_zipfile(filename):\n        tree = get_xml_from_archive(filename)\n    else:\n        tree = ET.parse(filename)\n\n    # Is the file a supported version\n    tree_root = tree.getroot()\n    file_version = Version(tree_root.attrib.get('version', '0.0'))\n\n    if file_version < MIN_SUPPORTED_VERSION:\n        raise TableauVersionNotSupportedException(file_version)\n\n    # Does the root tag match the object type (workbook or data source)\n    if expected_root and (expected_root != tree_root.tag):\n        raise TableauInvalidFileException(\n            \"'{}'' is not a valid '{}' file\".format(filename, expected_root))\n\n    return tree", "response": "Opens the provided filename. Handles detecting if the file is an archive detecting the document version and validating the root tag."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the twb file from a Tableau packaged file format.", "response": "def find_file_in_zip(zip_file):\n    '''Returns the twb/tds file from a Tableau packaged file format. Packaged\n    files can contain cache entries which are also valid XML, so only look for\n    files with a .tds or .twb extension.\n    '''\n\n    candidate_files = filter(lambda x: x.split('.')[-1] in ('twb', 'tds'),\n                             zip_file.namelist())\n\n    for filename in candidate_files:\n        with zip_file.open(filename) as xml_candidate:\n            try:\n                ET.parse(xml_candidate)\n                return filename\n            except ET.ParseError:\n                # That's not an XML file by gosh\n                pass"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef build_archive_file(archive_contents, zip_file):\n\n    # This is tested against Desktop and Server, and reverse engineered by lots\n    # of trial and error. Do not change this logic.\n    for root_dir, _, files in os.walk(archive_contents):\n        relative_dir = os.path.relpath(root_dir, archive_contents)\n        for f in files:\n            temp_file_full_path = os.path.join(\n                archive_contents, relative_dir, f)\n            zipname = os.path.join(relative_dir, f)\n            zip_file.write(temp_file_full_path, arcname=zipname)", "response": "Build a Tableau - compatible archive file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef from_attributes(cls, server, dbname, username, dbclass, port=None, query_band=None,\n                        initial_sql=None, authentication=''):\n        \"\"\"Creates a new connection that can be added into a Data Source.\n        defaults to `''` which will be treated as 'prompt' by Tableau.\"\"\"\n\n        root = ET.Element('connection', authentication=authentication)\n        xml = cls(root)\n        xml.server = server\n        xml.dbname = dbname\n        xml.username = username\n        xml.dbclass = dbclass\n        xml.port = port\n        xml.query_band = query_band\n        xml.initial_sql = initial_sql\n\n        return xml", "response": "Creates a new connection that can be added into a Data Source."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the connection s database name property.", "response": "def dbname(self, value):\n        \"\"\"\n        Set the connection's database name property.\n\n        Args:\n            value:  New name of the database. String.\n\n        Returns:\n            Nothing.\n\n        \"\"\"\n        self._dbname = value\n        self._connectionXML.set('dbname', value)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the connection s server property.", "response": "def server(self, value):\n        \"\"\"\n        Set the connection's server property.\n\n        Args:\n            value:  New server. String.\n\n        Returns:\n            Nothing.\n\n        \"\"\"\n        self._server = value\n        self._connectionXML.set('server', value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef username(self, value):\n        self._username = value\n        self._connectionXML.set('username', value)", "response": "Sets the connection s username property."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dbclass(self, value):\n\n        if not is_valid_dbclass(value):\n            raise AttributeError(\"'{}' is not a valid database type\".format(value))\n\n        self._class = value\n        self._connectionXML.set('class', value)", "response": "Set the connection s dbclass property."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the connection s port property.", "response": "def port(self, value):\n        \"\"\"Set the connection's port property.\n\n        Args:\n            value:  New port value. String.\n\n        Returns:\n            Nothing.\n        \"\"\"\n\n        self._port = value\n        # If port is None we remove the element and don't write it to XML\n        if value is None:\n            try:\n                del self._connectionXML.attrib['port']\n            except KeyError:\n                pass\n        else:\n            self._connectionXML.set('port', value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the connection s query_band property.", "response": "def query_band(self, value):\n        \"\"\"Set the connection's query_band property.\n\n        Args:\n            value:  New query_band value. String.\n\n        Returns:\n            Nothing.\n        \"\"\"\n\n        self._query_band = value\n        # If query band is None we remove the element and don't write it to XML\n        if value is None:\n            try:\n                del self._connectionXML.attrib['query-band-spec']\n            except KeyError:\n                pass\n        else:\n            self._connectionXML.set('query-band-spec', value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef initial_sql(self, value):\n\n        self._initial_sql = value\n        # If initial_sql is None we remove the element and don't write it to XML\n        if value is None:\n            try:\n                del self._connectionXML.attrib['one-time-sql']\n            except KeyError:\n                pass\n        else:\n            self._connectionXML.set('one-time-sql', value)", "response": "Set the connection s initial_sql property."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts an integer into a base36 string.", "response": "def base36encode(number):\n    \"\"\"Converts an integer into a base36 string.\"\"\"\n\n    ALPHABET = \"0123456789abcdefghijklmnopqrstuvwxyz\"\n\n    base36 = ''\n    sign = ''\n\n    if number < 0:\n        sign = '-'\n        number = -number\n\n    if 0 <= number < len(ALPHABET):\n        return sign + ALPHABET[number]\n\n    while number != 0:\n        number, i = divmod(number, len(ALPHABET))\n        base36 = ALPHABET[i] + base36\n\n    return sign + base36"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_connections(self):\n\n        if float(self._dsversion) < 10:\n            connections = self._extract_legacy_connection()\n        else:\n            connections = self._extract_federated_connections()\n        return connections", "response": "Find and return all connections based on file format version."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef from_file(cls, filename):\n\n        dsxml = xml_open(filename, 'datasource').getroot()\n        return cls(dsxml, filename)", "response": "Initialize a new datasource from a file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a new Data Source give a list of Connections.", "response": "def from_connections(cls, caption, connections):\n        \"\"\"Create a new Data Source give a list of Connections.\"\"\"\n\n        root = ET.Element('datasource', caption=caption, version='10.0', inline='true')\n        outer_connection = ET.SubElement(root, 'connection')\n        outer_connection.set('class', 'federated')\n        named_conns = ET.SubElement(outer_connection, 'named-connections')\n        for conn in connections:\n            nc = ET.SubElement(named_conns,\n                               'named-connection',\n                               name=_make_unique_name(conn.dbclass),\n                               caption=conn.server)\n            nc.append(conn._connectionXML)\n        return cls(root)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsaves our file with the name provided.", "response": "def save_as(self, new_filename):\n        \"\"\"\n        Save our file with the name provided.\n\n        Args:\n            new_filename:  New name for the workbook file. String.\n\n        Returns:\n            Nothing.\n\n        \"\"\"\n\n        xfile._save_file(self._filename, self._datasourceTree, new_filename)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef save_as(self, new_filename):\n        xfile._save_file(\n            self._filename, self._workbookTree, new_filename)", "response": "Save our file with the name provided."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprovide a nice name for the field which is derived from the alias caption and id.", "response": "def name(self):\n        \"\"\" Provides a nice name for the field which is derived from the alias, caption, or the id.\n\n        The name resolves as either the alias if it's defined, or the caption if alias is not defined,\n        and finally the id which is the underlying name if neither of the fields exist. \"\"\"\n        alias = getattr(self, 'alias', None)\n        if alias:\n            return alias\n\n        caption = getattr(self, 'caption', None)\n        if caption:\n            return caption\n\n        return self.id"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _check_configuration(self, *attrs):\n        for attr in attrs:\n            if getattr(self, attr, None) is None:\n                raise ConfigurationError(\"{} not configured\".format(attr))", "response": "Check that each named attribute has been configured"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _make_request(self, url, **kwargs):\n        response = requests.post(url, **kwargs)\n        try:\n            return response.json()\n        except ValueError:\n            pass\n        return parse_qs(response.content)", "response": "Make a request to an OAuth2 endpoint"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the url to redirect the user to for user consent", "response": "def authorize_url(self, scope='', **kwargs):\n        \"\"\"\n        Returns the url to redirect the user to for user consent\n        \"\"\"\n        self._check_configuration(\"site\", \"authorization_url\", \"redirect_uri\",\n                                  \"client_id\")\n        if isinstance(scope, (list, tuple, set, frozenset)):\n            self._check_configuration(\"scope_sep\")\n            scope = self.scope_sep.join(scope)\n        oauth_params = {\n            'redirect_uri': self.redirect_uri,\n            'client_id': self.client_id,\n            'scope': scope,\n        }\n        oauth_params.update(kwargs)\n        return \"%s%s?%s\" % (self.site, quote(self.authorization_url),\n                            urlencode(oauth_params))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nrequests an access token", "response": "def get_token(self, code, headers=None, **kwargs):\n        \"\"\"\n        Requests an access token\n        \"\"\"\n        self._check_configuration(\"site\", \"token_url\", \"redirect_uri\",\n                                  \"client_id\", \"client_secret\")\n        url = \"%s%s\" % (self.site, quote(self.token_url))\n        data = {\n            'redirect_uri': self.redirect_uri,\n            'client_id': self.client_id,\n            'client_secret': self.client_secret,\n            'code': code,\n        }\n        data.update(kwargs)\n\n        return self._make_request(url, data=data, headers=headers)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrequest a refreshed token from the API.", "response": "def refresh_token(self, headers=None, **kwargs):\n        \"\"\"\n        Request a refreshed token\n        \"\"\"\n        self._check_configuration(\"site\", \"token_url\", \"client_id\",\n                                  \"client_secret\")\n        url = \"%s%s\" % (self.site, quote(self.token_url))\n        data = {\n            'client_id': self.client_id,\n            'client_secret': self.client_secret,\n        }\n        data.update(kwargs)\n\n        return self._make_request(url, data=data, headers=headers)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrevoke an access token", "response": "def revoke_token(self, token, headers=None, **kwargs):\n        \"\"\"\n        Revoke an access token\n        \"\"\"\n        self._check_configuration(\"site\", \"revoke_uri\")\n        url = \"%s%s\" % (self.site, quote(self.revoke_url))\n        data = {'token': token}\n        data.update(kwargs)\n\n        return self._make_request(url, data=data, headers=headers)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef predict(self, predict_set ):\n        predict_data           = np.array( [instance.features for instance in predict_set ] )\n        \n        return self.update( predict_data )", "response": "This method updates the data for the given set of Instances."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load_network_from_file( filename ):\n        import cPickle\n        \"\"\"\n        Load the complete configuration of a previously stored network.\n        \"\"\"\n        network = NeuralNet( {\"n_inputs\":1, \"layers\":[[0,None]]} )\n    \n        with open( filename , 'rb') as file:\n            store_dict                   = cPickle.load(file)\n        \n            network.n_inputs             = store_dict[\"n_inputs\"]            \n            network.n_weights            = store_dict[\"n_weights\"]           \n            network.layers               = store_dict[\"layers\"]\n            network.weights              = store_dict[\"weights\"]             \n    \n        return network", "response": "Load a previously stored network from a file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate the Hellinger distance between two signals.", "response": "def hellinger_distance( outputs, targets, derivative=False ):\n    \"\"\"\n    The output signals should be in the range [0, 1]\n    \"\"\"\n    root_difference = np.sqrt( outputs ) - np.sqrt( targets )\n    \n    if derivative:\n        return root_difference/( np.sqrt(2) * np.sqrt( outputs ))\n    else:\n        return np.mean(np.sum( np.power(root_difference, 2 ), axis=1) / math.sqrt( 2 ))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates the binary cross entropy cost of a set of signals.", "response": "def binary_cross_entropy_cost( outputs, targets, derivative=False, epsilon=1e-11 ):\n    \"\"\"\n    The output signals should be in the range [0, 1]\n    \"\"\"\n    # Prevent overflow\n    outputs = np.clip(outputs, epsilon, 1 - epsilon)\n    divisor = np.maximum(outputs * (1 - outputs), epsilon)\n    \n    if derivative:\n        return (outputs - targets) / divisor\n    else:\n        return np.mean(-np.sum(targets * np.log( outputs ) + (1 - targets) * np.log(1 - outputs), axis=1))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef softmax_categorical_cross_entropy_cost( outputs, targets, derivative=False, epsilon=1e-11 ):\n    outputs = np.clip(outputs, epsilon, 1 - epsilon)\n    \n    if derivative:\n        return outputs - targets\n    else:\n        return np.mean(-np.sum(targets * np.log( outputs ), axis=1))", "response": "Compute the softmax cross entropy cost of a set of outputs and targets."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a function that combines the input signal with the mean of 0 and scale the signal strength by dividing with the standard deviation.", "response": "def standarize( trainingset ):\n    \"\"\"\n    Morph the input signal to a mean of 0 and scale the signal strength by \n    dividing with the standard deviation (rather that forcing a [0, 1] range)\n    \"\"\"\n    \n    def encoder( dataset ):\n        for instance in dataset:\n            if np.any(stds == 0):\n                nonzero_indexes = np.where(stds!=0)\n                instance.features[nonzero_indexes] = (instance.features[nonzero_indexes] - means[nonzero_indexes]) / stds[nonzero_indexes]\n            else:\n                instance.features = (instance.features - means) / stds\n        return dataset\n    #end\n    \n    training_data = np.array( [instance.features for instance in trainingset ] )\n    means = training_data.mean(axis=0)\n    stds = training_data.std(axis=0)\n    \n    return encoder"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a function that replaces instanced of not a number with the mean of the signal feature or a specific value assigned by replace_nan_with.", "response": "def replace_nan( trainingset, replace_with = None ): # if replace_with = None, replaces with mean value\n    \"\"\"\n    Replace instanced of \"not a number\" with either the mean of the signal feature\n    or a specific value assigned by `replace_nan_with`\n    \"\"\"\n    training_data = np.array( [instance.features for instance in trainingset ] ).astype( np.float64 )\n    \n    def encoder( dataset ):\n        for instance in dataset:\n            instance.features = instance.features.astype( np.float64 )\n            \n            if np.sum(np.isnan( instance.features )):\n                if replace_with == None:\n                    instance.features[ np.isnan( instance.features ) ] = means[ np.isnan( instance.features ) ]\n                else:\n                    instance.features[ np.isnan( instance.features ) ] = replace_with\n        return dataset\n    #end\n    \n    if replace_nan_with == None:\n        means = np.mean( np.nan_to_num(training_data), axis=0 )\n    \n    return encoder"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef normalize( trainingset ):\n    \n    def encoder( dataset ):\n        for instance in dataset:\n            if np.any(stds == 0):\n                nonzero_indexes = np.where(stds!=0)\n                instance.features[nonzero_indexes] = instance.features[nonzero_indexes] / stds[nonzero_indexes]\n            else:\n                instance.features = instance.features / stds\n        return dataset\n    #end\n    \n    training_data = np.array( [instance.features for instance in trainingset ] )\n    stds = training_data.std(axis=0)\n    \n    return encoder", "response": "Normalize the input signal by \n "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef GetDeepComps(self, zws_id, zpid, count=10, rentzestimate=False):\n        url = '%s/GetDeepComps.htm' % (self.base_url)\n        parameters = {'zws-id': zws_id,\n                      'zpid': zpid,\n                      'count': count}\n        if rentzestimate:\n            parameters['rentzestimate'] = 'true'\n\n        resp = self._RequestUrl(url, 'GET', data=parameters)\n        data = resp.content.decode('utf-8')\n\n        # transform the data to an dict-like object\n        xmltodict_data = xmltodict.parse(data)\n\n        # get the principal property data\n        principal_place = Place()\n        principal_data = xmltodict_data.get('Comps:comps')['response']['properties']['principal']\n\n        try:\n            principal_place.set_data(principal_data)\n        except:\n            raise ZillowError({'message': 'No principal data found: %s' % data})\n\n        # get the comps property_data\n        comps = xmltodict_data.get('Comps:comps')['response']['properties']['comparables']['comp']\n\n        comp_places = []\n        for datum in comps:\n            place = Place()\n            try:\n                place.set_data(datum)\n                comp_places.append(place)\n            except:\n                raise ZillowError({'message': 'No valid comp data found %s' % datum})\n\n        output = {\n            'principal': principal_place,\n            'comps': comp_places\n        }\n\n        return output", "response": "This API returns a list of comparable recent sales for a specified property."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _RequestUrl(self, url, verb, data=None):\n        if verb == 'GET':\n            url = self._BuildUrl(url, extra_params=data)\n            try:\n                return requests.get(\n                    url,\n                    auth=self.__auth,\n                    timeout=self._timeout\n                )\n            except requests.RequestException as e:\n                raise ZillowError(str(e))\n        return 0", "response": "Request a url.\n        :param url: The web location we want to retrieve.\n        :param verb: GET only (for now).\n        :param data: A dict of (str, unicode) key/value pairs.\n        :return:A JSON object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nbuilding a URL from the given url.", "response": "def _BuildUrl(self, url, path_elements=None, extra_params=None):\n        \"\"\"\n        Taken from: https://github.com/bear/python-twitter/blob/master/twitter/api.py#L3814-L3836\n        :param url:\n        :param path_elements:\n        :param extra_params:\n        :return:\n        \"\"\"\n        # Break url into constituent parts\n        (scheme, netloc, path, params, query, fragment) = urlparse(url)\n\n        # Add any additional path elements to the path\n        if path_elements:\n            # Filter out the path elements that have a value of None\n            p = [i for i in path_elements if i]\n            if not path.endswith('/'):\n                path += '/'\n            path += '/'.join(p)\n\n        # Add any additional query parameters to the query string\n        if extra_params and len(extra_params) > 0:\n            extra_query = self._EncodeParameters(extra_params)\n            # Add it to the existing query\n            if query:\n                query += '&' + extra_query\n            else:\n                query = extra_query\n\n        # Return the rebuilt URL\n        return urlunparse((scheme, netloc, path, params, query, fragment))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nencoding the parameters in key = value&key = value form.", "response": "def _EncodeParameters(self, parameters):\n        \"\"\"\n        Return a string in key=value&key=value form.\n        :param parameters: A dict of (key, value) tuples, where value is encoded as specified by self._encoding\n        :return:A URL-encoded string in \"key=value&key=value\" form\n        \"\"\"\n\n        if parameters is None:\n            return None\n        else:\n            return urlencode(dict([(k, self._Encode(v)) for k, v in list(parameters.items()) if v is not None]))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_data(self, source_data):\n        self.home_details = source_data['homedetails']\n        try:\n            self.graphs_and_data = source_data['graphsanddata']\n        except:\n            self.graphs_and_data = None\n        self.map_this_home = source_data['mapthishome']\n        self.comparables = source_data['comparables']", "response": "Set the data from the source data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_data(self, source_data):\n        self.street = source_data['street']\n        self.zipcode = source_data['zipcode']\n        self.city = source_data['city']\n        self.state = source_data['state']\n        self.latitude = source_data['latitude']\n        self.longitude = source_data['longitude']", "response": "Set the data from the source data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the data from the source data to the instance properties.", "response": "def set_data(self, source_data):\n        \"\"\"\n        :source_data: Data from data.get('SearchResults:searchresults', None)['response']['results']['result']['zestimate']\n        :return:\n        \"\"\"\n        try:\n            self.amount = int(source_data['amount']['#text'])\n        except:\n            self.amount = None\n        self.amount_currency = source_data['amount']['@currency']\n        self.amount_last_updated = source_data['last-updated']\n        try:\n            self.amount_change_30days = int(source_data['valueChange']['#text'])\n        except:\n            self.amount_change_30days = None\n        try:\n            self.valuation_range_low = int(source_data['valuationRange']['low']['#text'])\n        except:\n            self.valuation_range_low = None\n        try:\n            self.valuation_range_high = int(source_data['valuationRange']['high']['#text'])\n        except:\n            self.valuation_range_high = None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_data(self, source_data):\n        self.region_name = source_data['region']['@name']\n        self.region_id = source_data['region']['@id']\n        self.region_type =  source_data['region']['@type']\n        self.zillow_home_value_index = source_data.get('zindexValue', None)\n        self.overview_link =  source_data['region']['links']['overview']\n        self.fsbo_link =  source_data['region']['links']['forSaleByOwner']\n        self.sale_link =  source_data['region']['links']['forSale']", "response": "Set the data for the current object"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_data(self, source_data):\n\n        self.zpid = source_data.get('zpid', None)\n        self.similarity_score = source_data.get('@score', None)\n        self.links.set_data(source_data['links'])\n        self.full_address.set_data(source_data['address'])\n        self.zestimate.set_data(source_data['zestimate'])\n        self.local_realestate.set_data(source_data['localRealEstate'])\n        if self.has_extended_data:\n            self.extended_data.set_data(source_data)", "response": "Set data from source_data to this object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nskipping null padding in StringPoolChunk.", "response": "def skipNullPadding(self, buff):\n        '''\n        \u4e0d\u65ad\u5730\u5bfb\u627e CHUNK_STRINGPOOL_TYPE\uff0c\u76ee\u524d\u6682\u65f6\u6ca1\u6709\u9047\u5230\u8fd9\u79cd\u6837\u672c\u3002\n        '''\n        def readNext(buff, first_run=True):\n            datas = unpack('<i', buff.read(4))\n            header = datas[0]\n\n            if header == CHUNK_NULL_TYPE and first_run:\n                print(\"Skipping null padding in StringPoolChunk header\")\n                header = readNext(buff, first_run=False)\n            elif header != CHUNK_STRINGPOOL_TYPE:\n                print(\"Invalid StringPoolChunk header\")\n\n            return header\n\n        header = readNext(buff)\n        return header >> 8, header & 0xFF"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_zipfile(filename):\n    result = False\n    try:\n        if hasattr(filename, \"read\"):\n            result = _check_zipfile(fp=filename)\n        else:\n            with open(filename, \"rb\") as fp:\n                result = _check_zipfile(fp)\n    except OSError:\n        pass\n    return result", "response": "Quickly see if a file is a ZIP file by checking the magic number."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef FileHeader(self, zip64=None):\n        dt = self.date_time\n        dosdate = (dt[0] - 1980) << 9 | dt[1] << 5 | dt[2]\n        dostime = dt[3] << 11 | dt[4] << 5 | (dt[5] // 2)\n        if self.flag_bits & 0x08:\n            # Set these to zero because we write them after the file data\n            CRC = compress_size = file_size = 0\n        else:\n            CRC = self.CRC\n            compress_size = self.compress_size\n            file_size = self.file_size\n\n        extra = self.extra\n\n        min_version = 0\n        if zip64 is None:\n            zip64 = file_size > ZIP64_LIMIT or compress_size > ZIP64_LIMIT\n        if zip64:\n            fmt = '<HHQQ'\n            extra = extra + struct.pack(fmt,\n                                        1, struct.calcsize(fmt) - 4, file_size, compress_size)\n        if file_size > ZIP64_LIMIT or compress_size > ZIP64_LIMIT:\n            if not zip64:\n                raise LargeZipFile(\"Filesize would require ZIP64 extensions\")\n            # File is larger than what fits into a 4 byte integer,\n            # fall back to the ZIP64 extension\n            file_size = 0xffffffff\n            compress_size = 0xffffffff\n            min_version = ZIP64_VERSION\n\n        if self.compress_type == ZIP_BZIP2:\n            min_version = max(BZIP2_VERSION, min_version)\n        elif self.compress_type == ZIP_LZMA:\n            min_version = max(LZMA_VERSION, min_version)\n\n        self.extract_version = max(min_version, self.extract_version)\n        self.create_version = max(min_version, self.create_version)\n        filename, flag_bits = self._encodeFilenameFlags()\n        header = struct.pack(structFileHeader, stringFileHeader,\n                             self.extract_version, self.reserved, flag_bits,\n                             self.compress_type, dostime, dosdate, CRC,\n                             compress_size, file_size,\n                             len(filename), len(extra))\n        return header + filename + extra", "response": "Return the per - file header as a string."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads and return a line from the stream.", "response": "def readline(self, limit=-1):\n        \"\"\"Read and return a line from the stream.\n\n        If limit is specified, at most limit bytes will be read.\n        \"\"\"\n\n        if not self._universal and limit < 0:\n            # Shortcut common case - newline found in buffer.\n            i = self._readbuffer.find(b'\\n', self._offset) + 1\n            if i > 0:\n                line = self._readbuffer[self._offset: i]\n                self._offset = i\n                return line\n\n        if not self._universal:\n            return io.BufferedIOBase.readline(self, limit)\n\n        line = b''\n        while limit < 0 or len(line) < limit:\n            readahead = self.peek(2)\n            if readahead == b'':\n                return line\n\n            #\n            # Search for universal newlines or line chunks.\n            #\n            # The pattern returns either a line chunk or a newline, but not\n            # both. Combined with peek(2), we are assured that the sequence\n            # '\\r\\n' is always retrieved completely and never split into\n            # separate newlines - '\\r', '\\n' due to coincidental readaheads.\n            #\n            match = self.PATTERN.search(readahead)\n            newline = match.group('newline')\n            if newline is not None:\n                if self.newlines is None:\n                    self.newlines = []\n                if newline not in self.newlines:\n                    self.newlines.append(newline)\n                self._offset += len(newline)\n                return line + b'\\n'\n\n            chunk = match.group('chunk')\n            if limit >= 0:\n                chunk = chunk[: limit - len(line)]\n\n            self._offset += len(chunk)\n            line += chunk\n\n        return line"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns buffered bytes without advancing the position.", "response": "def peek(self, n=1):\n        \"\"\"Returns buffered bytes without advancing the position.\"\"\"\n        if n > len(self._readbuffer) - self._offset:\n            chunk = self.read(n)\n            if len(chunk) > self._offset:\n                self._readbuffer = chunk + self._readbuffer[self._offset:]\n                self._offset = 0\n            else:\n                self._offset -= len(chunk)\n\n        # Return up to 512 bytes to reduce allocation overhead for tight loops.\n        return self._readbuffer[self._offset: self._offset + 512]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read(self, n=-1):\n        if n is None or n < 0:\n            buf = self._readbuffer[self._offset:]\n            self._readbuffer = b''\n            self._offset = 0\n            while not self._eof:\n                buf += self._read1(self.MAX_N)\n            return buf\n\n        end = n + self._offset\n        if end < len(self._readbuffer):\n            buf = self._readbuffer[self._offset:end]\n            self._offset = end\n            return buf\n\n        n = end - len(self._readbuffer)\n        buf = self._readbuffer[self._offset:]\n        self._readbuffer = b''\n        self._offset = 0\n        while n > 0 and not self._eof:\n            data = self._read1(n)\n            if n < len(data):\n                self._readbuffer = data\n                self._offset = n\n                buf += data[:n]\n                break\n            buf += data\n            n -= len(data)\n        return buf", "response": "Read and return up to n bytes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading in the table of contents for the ZIP file.", "response": "def _RealGetContents(self):\n        \"\"\"Read in the table of contents for the ZIP file.\"\"\"\n        fp = self.fp\n        try:\n            endrec = _EndRecData(fp)\n        except OSError:\n            raise BadZipFile(\"File is not a zip file\")\n        if not endrec:\n            raise BadZipFile(\"File is not a zip file\")\n        if self.debug > 1:\n            print(endrec)\n        size_cd = endrec[_ECD_SIZE]             # bytes in central directory\n        offset_cd = endrec[_ECD_OFFSET]         # offset of central directory\n        self._comment = endrec[_ECD_COMMENT]    # archive comment\n\n        # ---> APK\u6587\u4ef6\u53ea\u6709\u4e00\u4e2a\uff0c\u4e0d\u53ef\u80fd\u5b58\u5728\u989d\u5916\u6570\u636e\u3002\n        # \"concat\" is zero, unless zip was concatenated to another file\n        # concat = endrec[_ECD_LOCATION] - size_cd - offset_cd\n        # if endrec[_ECD_SIGNATURE] == stringEndArchive64:\n        #     # If Zip64 extension structures are present, account for them\n        #     concat -= (sizeEndCentDir64 + sizeEndCentDir64Locator)\n        concat = 0\n\n        if self.debug > 2:\n            inferred = concat + offset_cd\n            print(\"given, inferred, offset\", offset_cd, inferred, concat)\n        # self.start_dir:  Position of start of central directory\n        self.start_dir = offset_cd + concat\n        fp.seek(self.start_dir, 0)\n        data = fp.read(size_cd)\n        fp = io.BytesIO(data)\n        total = 0\n        while total < size_cd:\n            centdir = fp.read(sizeCentralDir)\n            if len(centdir) != sizeCentralDir:\n                raise BadZipFile(\"Truncated central directory\")\n            centdir = struct.unpack(structCentralDir, centdir)\n            if centdir[_CD_SIGNATURE] != stringCentralDir:\n                raise BadZipFile(\"Bad magic number for central directory\")\n            if self.debug > 2:\n                print(centdir)\n            filename = fp.read(centdir[_CD_FILENAME_LENGTH])\n            flags = centdir[5]\n            if flags & 0x800:\n                # UTF-8 file names extension\n                filename = filename.decode('utf-8')\n            else:\n                # Historical ZIP filename encoding\n                filename = filename.decode('cp437')\n            # Create ZipInfo instance to store file information\n            x = ZipInfo(filename)\n            x.extra = fp.read(centdir[_CD_EXTRA_FIELD_LENGTH])\n            x.comment = fp.read(centdir[_CD_COMMENT_LENGTH])\n            x.header_offset = centdir[_CD_LOCAL_HEADER_OFFSET]\n            (x.create_version, x.create_system, x.extract_version, x.reserved,\n             x.flag_bits, x.compress_type, t, d,\n             x.CRC, x.compress_size, x.file_size) = centdir[1:12]\n            if x.extract_version > MAX_EXTRACT_VERSION:\n                raise NotImplementedError(\"zip file version %.1f\" %\n                                          (x.extract_version / 10))\n            x.volume, x.internal_attr, x.external_attr = centdir[15:18]\n            # Convert date/time code to (year, month, day, hour, min, sec)\n            x._raw_time = t\n            x.date_time = ((d >> 9) + 1980, (d >> 5) & 0xF, d & 0x1F,\n                           t >> 11, (t >> 5) & 0x3F, (t & 0x1F) * 2)\n\n            x._decodeExtra()\n            # x.header_offset = x.header_offset + concat\n            self.filelist.append(x)\n            self.NameToInfo[x.filename] = x\n\n            # update total bytes read from central directory\n            total = total + sizeCentralDir + \\\n                centdir[_CD_FILENAME_LENGTH] + \\\n                centdir[_CD_EXTRA_FIELD_LENGTH] + centdir[_CD_COMMENT_LENGTH]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef setpassword(self, pwd):\n        if pwd and not isinstance(pwd, bytes):\n            raise TypeError(\"pwd: expected bytes, got %s\" % type(pwd))\n        if pwd:\n            self.pwd = pwd\n        else:\n            self.pwd = None", "response": "Set default password for encrypted files."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns file bytes for name.", "response": "def read(self, name, pwd=None):\n        \"\"\"Return file bytes (as a string) for name.\"\"\"\n        with self.open(name, \"r\", pwd) as fp:\n            return fp.read()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef open(self, name, mode=\"r\", pwd=None):\n        if mode not in (\"r\", \"U\", \"rU\"):\n            raise RuntimeError('open() requires mode \"r\", \"U\", or \"rU\"')\n        if 'U' in mode:\n            import warnings\n            warnings.warn(\"'U' mode is deprecated\",\n                          DeprecationWarning, 2)\n        if pwd and not isinstance(pwd, bytes):\n            raise TypeError(\"pwd: expected bytes, got %s\" % type(pwd))\n        if not self.fp:\n            raise RuntimeError(\n                \"Attempt to read ZIP archive that was already closed\")\n\n        # Make sure we have an info object\n        if isinstance(name, ZipInfo):\n            # 'name' is already an info object\n            zinfo = name\n        else:\n            # Get info object for name\n            zinfo = self.getinfo(name)\n\n        self._fileRefCnt += 1\n\n        zef_file = _SharedFile(\n            self.fp, zinfo.header_offset, self._fpclose, self._lock)\n\n        try:\n            # Skip the file header:\n            fheader = zef_file.read(sizeFileHeader)\n\n            if len(fheader) != sizeFileHeader:\n                raise BadZipFile(\"Truncated file header\")\n\n            fheader = struct.unpack(structFileHeader, fheader)\n            # print(fheader)\n\n            if fheader[_FH_SIGNATURE] != stringFileHeader:\n                raise BadZipFile(\"Bad magic number for file header\")\n\n            # \u6ce8\u610f\uff1a\u5934\u90e8\u7684\u6587\u4ef6\u957f\u5ea6\u53ef\u80fd\u4f1a\u88ab\u4fee\u6539\n            len_fname = fheader[_FH_FILENAME_LENGTH]\n            if len_fname > 256:\n                # \u81ea\u52a8\u4fee\u6b63\u6587\u4ef6\u540d\u957f\u5ea6\uff0c\u4f46\u662f\uff0c\u4e0d\u80fd\u4fdd\u8bc1\u89e3\u538b\u6210\u529f\n                len_fname = len(zinfo.orig_filename)\n\n            fname = zef_file.read(len_fname)\n\n            if fheader[_FH_EXTRA_FIELD_LENGTH]:\n                zef_file.read(fheader[_FH_EXTRA_FIELD_LENGTH])\n\n            # zinfo.flag_bits ^= zinfo.flag_bits % 2\n\n            if zinfo.flag_bits & 0x20:\n                # Zip 2.7: compressed patched data\n                raise NotImplementedError(\n                    \"compressed patched data (flag bit 5)\")\n\n            if zinfo.flag_bits & 0x40:\n                # strong encryption\n                raise NotImplementedError(\"strong encryption (flag bit 6)\")\n\n            if zinfo.flag_bits & 0x800:\n                # UTF-8 filename\n                fname_str = fname.decode(\"utf-8\")\n            else:\n                fname_str = fname.decode(\"cp437\")\n\n            if fname_str != zinfo.orig_filename:\n                raise BadZipFile(\n                    'File name in directory %r and header %r differ.'\n                    % (zinfo.orig_filename, fname))\n\n            return ZipExtFile(zef_file, mode, zinfo, None, True)\n        except Exception as ignore:\n            zef_file.close()\n            raise", "response": "Open a file - like object for the given name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _sanitize_windows_name(cls, arcname, pathsep):\n        table = cls._windows_illegal_name_trans_table\n        if not table:\n            illegal = ':<>|\"?*'\n            table = str.maketrans(illegal, '_' * len(illegal))\n            cls._windows_illegal_name_trans_table = table\n        arcname = arcname.translate(table)\n        # remove trailing dots\n        arcname = (x.rstrip('.') for x in arcname.split(pathsep))\n        # rejoin, removing empty parts.\n        arcname = pathsep.join(x for x in arcname if x)\n        return arcname", "response": "Replace bad characters and remove trailing dots from parts."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _extract_member(self, member, targetpath, pwd):\n        # build the destination pathname, replacing\n        # forward slashes to platform specific separators.\n        arcname = member.filename.replace('/', os.path.sep)\n\n        if os.path.altsep:\n            arcname = arcname.replace(os.path.altsep, os.path.sep)\n        # interpret absolute pathname as relative, remove drive letter or\n        # UNC path, redundant separators, \".\" and \"..\" components.\n        arcname = os.path.splitdrive(arcname)[1]\n        invalid_path_parts = ('', os.path.curdir, os.path.pardir)\n        arcname = os.path.sep.join(x for x in arcname.split(os.path.sep)\n                                   if x not in invalid_path_parts)\n        if os.path.sep == '\\\\':\n            # filter illegal characters on Windows\n            arcname = self._sanitize_windows_name(arcname, os.path.sep)\n\n        targetpath = os.path.join(targetpath, arcname)\n        targetpath = os.path.normpath(targetpath)\n\n        # Create all upper directories if necessary.\n        upperdirs = os.path.dirname(targetpath)\n        if upperdirs and not os.path.exists(upperdirs):\n            os.makedirs(upperdirs)\n\n        if member.filename[-1] == '/':\n            if not os.path.isdir(targetpath):\n                os.mkdir(targetpath)\n            return targetpath\n\n        with self.open(member, pwd=pwd) as source, \\\n                open(targetpath, \"wb\") as target:\n            shutil.copyfileobj(source, target)\n\n        return targetpath", "response": "Extracts the ZipInfo object member to a physical entry file on the path targetpath."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks for errors before writing a file to the archive.", "response": "def _writecheck(self, zinfo):\n        \"\"\"Check for errors before writing a file to the archive.\"\"\"\n        if zinfo.filename in self.NameToInfo:\n            import warnings\n            warnings.warn('Duplicate name: %r' % zinfo.filename, stacklevel=3)\n        if self.mode not in ('w', 'x', 'a'):\n            raise RuntimeError(\"write() requires mode 'w', 'x', or 'a'\")\n        if not self.fp:\n            raise RuntimeError(\n                \"Attempt to write ZIP archive that was already closed\")\n        _check_compression(zinfo.compress_type)\n        if not self._allowZip64:\n            requires_zip64 = None\n            if len(self.filelist) >= ZIP_FILECOUNT_LIMIT:\n                requires_zip64 = \"Files count\"\n            elif zinfo.file_size > ZIP64_LIMIT:\n                requires_zip64 = \"Filesize\"\n            elif zinfo.header_offset > ZIP64_LIMIT:\n                requires_zip64 = \"Zipfile size\"\n            if requires_zip64:\n                raise LargeZipFile(requires_zip64 +\n                                   \" would require ZIP64 extensions\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrite the bytes from filename into the archive under the name arcname.", "response": "def write(self, filename, arcname=None, compress_type=None):\n        \"\"\"Put the bytes from filename into the archive under the name\n        arcname.\"\"\"\n        if not self.fp:\n            raise RuntimeError(\n                \"Attempt to write to ZIP archive that was already closed\")\n\n        st = os.stat(filename)\n        isdir = stat.S_ISDIR(st.st_mode)\n        mtime = time.localtime(st.st_mtime)\n        date_time = mtime[0:6]\n        # Create ZipInfo instance to store file information\n        if arcname is None:\n            arcname = filename\n        arcname = os.path.normpath(os.path.splitdrive(arcname)[1])\n        while arcname[0] in (os.sep, os.altsep):\n            arcname = arcname[1:]\n        if isdir:\n            arcname += '/'\n        zinfo = ZipInfo(arcname, date_time)\n        zinfo.external_attr = (st[0] & 0xFFFF) << 16      # Unix attributes\n        if compress_type is None:\n            zinfo.compress_type = self.compression\n        else:\n            zinfo.compress_type = compress_type\n\n        zinfo.file_size = st.st_size\n        zinfo.flag_bits = 0x00\n        with self._lock:\n            if self._seekable:\n                self.fp.seek(self.start_dir)\n            zinfo.header_offset = self.fp.tell()    # Start of header bytes\n            if zinfo.compress_type == ZIP_LZMA:\n                # Compressed data includes an end-of-stream (EOS) marker\n                zinfo.flag_bits |= 0x02\n\n            self._writecheck(zinfo)\n            self._didModify = True\n\n            if isdir:\n                zinfo.file_size = 0\n                zinfo.compress_size = 0\n                zinfo.CRC = 0\n                zinfo.external_attr |= 0x10  # MS-DOS directory flag\n                self.filelist.append(zinfo)\n                self.NameToInfo[zinfo.filename] = zinfo\n                self.fp.write(zinfo.FileHeader(False))\n                self.start_dir = self.fp.tell()\n                return\n\n            cmpr = _get_compressor(zinfo.compress_type)\n            if not self._seekable:\n                zinfo.flag_bits |= 0x08\n            with open(filename, \"rb\") as fp:\n                # Must overwrite CRC and sizes with correct data later\n                zinfo.CRC = CRC = 0\n                zinfo.compress_size = compress_size = 0\n                # Compressed size can be larger than uncompressed size\n                zip64 = self._allowZip64 and \\\n                    zinfo.file_size * 1.05 > ZIP64_LIMIT\n                self.fp.write(zinfo.FileHeader(zip64))\n                file_size = 0\n                while 1:\n                    buf = fp.read(1024 * 8)\n                    if not buf:\n                        break\n                    file_size = file_size + len(buf)\n                    CRC = crc32(buf, CRC) & 0xffffffff\n                    if cmpr:\n                        buf = cmpr.compress(buf)\n                        compress_size = compress_size + len(buf)\n                    self.fp.write(buf)\n            if cmpr:\n                buf = cmpr.flush()\n                compress_size = compress_size + len(buf)\n                self.fp.write(buf)\n                zinfo.compress_size = compress_size\n            else:\n                zinfo.compress_size = file_size\n            zinfo.CRC = CRC\n            zinfo.file_size = file_size\n            if zinfo.flag_bits & 0x08:\n                # Write CRC and file sizes after the file data\n                fmt = '<LQQ' if zip64 else '<LLL'\n                self.fp.write(struct.pack(fmt, zinfo.CRC, zinfo.compress_size,\n                                          zinfo.file_size))\n                self.start_dir = self.fp.tell()\n            else:\n                if not zip64 and self._allowZip64:\n                    if file_size > ZIP64_LIMIT:\n                        raise RuntimeError(\n                            'File size has increased during compressing')\n                    if compress_size > ZIP64_LIMIT:\n                        raise RuntimeError(\n                            'Compressed size larger than uncompressed size')\n                # Seek backwards and write file header (which will now include\n                # correct CRC and file sizes)\n                self.start_dir = self.fp.tell()  # Preserve current position in file\n                self.fp.seek(zinfo.header_offset)\n                self.fp.write(zinfo.FileHeader(zip64))\n                self.fp.seek(self.start_dir)\n            self.filelist.append(zinfo)\n            self.NameToInfo[zinfo.filename] = zinfo"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef close(self):\n        if self.fp is None:\n            return\n\n        try:\n            if self.mode in ('w', 'x', 'a') and self._didModify:  # write ending records\n                with self._lock:\n                    if self._seekable:\n                        self.fp.seek(self.start_dir)\n                    self._write_end_record()\n        finally:\n            fp = self.fp\n            self.fp = None\n            self._fpclose(fp)", "response": "Close the file and for mode w x and a write the ending\n            records."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the filename and archivename for the path.", "response": "def _get_codename(self, pathname, basename):\n        \"\"\"Return (filename, archivename) for the path.\n\n        Given a module name path, return the correct file path and\n        archive name, compiling if necessary.  For example, given\n        /python/lib/string, return (/python/lib/string.pyc, string).\n        \"\"\"\n        def _compile(file, optimize=-1):\n            import py_compile\n            if self.debug:\n                print(\"Compiling\", file)\n            try:\n                py_compile.compile(file, doraise=True, optimize=optimize)\n            except py_compile.PyCompileError as err:\n                print(err.msg)\n                return False\n            return True\n\n        file_py = pathname + \".py\"\n        file_pyc = pathname + \".pyc\"\n        pycache_opt0 = importlib.util.cache_from_source(\n            file_py, optimization='')\n        pycache_opt1 = importlib.util.cache_from_source(\n            file_py, optimization=1)\n        pycache_opt2 = importlib.util.cache_from_source(\n            file_py, optimization=2)\n        if self._optimize == -1:\n            # legacy mode: use whatever file is present\n            if (os.path.isfile(file_pyc) and\n                    os.stat(file_pyc).st_mtime >= os.stat(file_py).st_mtime):\n                # Use .pyc file.\n                arcname = fname = file_pyc\n            elif (os.path.isfile(pycache_opt0) and\n                  os.stat(pycache_opt0).st_mtime >= os.stat(file_py).st_mtime):\n                # Use the __pycache__/*.pyc file, but write it to the legacy pyc\n                # file name in the archive.\n                fname = pycache_opt0\n                arcname = file_pyc\n            elif (os.path.isfile(pycache_opt1) and\n                  os.stat(pycache_opt1).st_mtime >= os.stat(file_py).st_mtime):\n                # Use the __pycache__/*.pyc file, but write it to the legacy pyc\n                # file name in the archive.\n                fname = pycache_opt1\n                arcname = file_pyc\n            elif (os.path.isfile(pycache_opt2) and\n                  os.stat(pycache_opt2).st_mtime >= os.stat(file_py).st_mtime):\n                # Use the __pycache__/*.pyc file, but write it to the legacy pyc\n                # file name in the archive.\n                fname = pycache_opt2\n                arcname = file_pyc\n            else:\n                # Compile py into PEP 3147 pyc file.\n                if _compile(file_py):\n                    if sys.flags.optimize == 0:\n                        fname = pycache_opt0\n                    elif sys.flags.optimize == 1:\n                        fname = pycache_opt1\n                    else:\n                        fname = pycache_opt2\n                    arcname = file_pyc\n                else:\n                    fname = arcname = file_py\n        else:\n            # new mode: use given optimization level\n            if self._optimize == 0:\n                fname = pycache_opt0\n                arcname = file_pyc\n            else:\n                arcname = file_pyc\n                if self._optimize == 1:\n                    fname = pycache_opt1\n                elif self._optimize == 2:\n                    fname = pycache_opt2\n                else:\n                    msg = \"invalid value for 'optimize': {!r}\".format(\n                        self._optimize)\n                    raise ValueError(msg)\n            if not (os.path.isfile(fname) and\n                    os.stat(fname).st_mtime >= os.stat(file_py).st_mtime):\n                if not _compile(file_py, optimize=self._optimize):\n                    fname = arcname = file_py\n        archivename = os.path.split(arcname)[1]\n        if basename:\n            archivename = \"%s/%s\" % (basename, archivename)\n        return (fname, archivename)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprint the node s children to stdout.", "response": "def pretty_print(node):\n        \"\"\"\u6f02\u4eae\u5730\u6253\u5370\u4e00\u4e2a\u8282\u70b9\n\n        Args:\n            node (TYPE): Description\n        \"\"\"\n        for pre, _, node in RenderTree(node):\n            print('{}{}'.format(pre, node.name))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_methods(self, limit=10000):\n        if self.methods is None:\n            self._init_methods(limit)\n        return self.methods", "response": "get_methods - Get all available methods"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninitialize the methods of the class and class.", "response": "def _init_methods(self, limit=10000):\n        \"\"\"\u521d\u59cb\u5316\u65b9\u6cd5\n\n        \u67d0\u4e9bAPK\u53ef\u80fd\u5b58\u5728\u5927\u91cf\u7684\u65b9\u6cd5\uff0c\u53ef\u80fd\u4f1a\u76f8\u5f53\u8017\u65f6\uff0c\u6839\u636e\u60c5\u51b5\u52a0\u9650\u5236\n\n        Args:\n            limit (int, optional): \u65b9\u6cd5\u6570\u91cf\u9650\u5236\uff0c\u8d85\u8fc7\u8be5\u503c\uff0c\u5219\u4e0d\u83b7\u53d6\u65b9\u6cd5\n\n        Returns:\n            TYPE: \u65b9\u6cd5\u96c6\u5408\n        \"\"\"\n        methods = set()\n        if not self.dex_files:\n            self._init_dex_files()\n\n        count = 0\n        for dex_file in self.dex_files:\n            count += dex_file.method_ids.size\n        if limit < count:\n            return\n\n        for dex_file in self.dex_files:\n            for dexClass in dex_file.classes:\n                try:\n                    dexClass.parseData()\n                except IndexError:\n                    continue\n\n                for method in dexClass.data.methods:\n                    clsname = method.id.cname.decode()\n                    mtdname = method.id.name.decode()\n                    methods.add(clsname + '/' + mtdname)\n        self.methods = sorted(methods)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of dynsym datas from an ELF file.", "response": "def get_dynsym_datas(elf_data, elf_file, skip_import=True):\r\n    \"\"\"\r\n    \u83b7\u53d6\u7b26\u53f7/\u65b9\u6cd5\u7684\u76f8\u5173\u4fe1\u606f\uff08\u7b26\u53f7\u540d\u3001\u7b26\u53f7\u6570\u636e\uff09\u3002\r\n    \"\"\"\r\n    f = elf_data\r\n    dynsym_datas = []\r\n\r\n    symbol_table = elf_file.get_section_by_name('.dynsym')\r\n    if symbol_table:\r\n        for symbol in symbol_table.iter_symbols():\r\n            if skip_import and symbol.entry.st_size == 0 or symbol.entry.st_info.type != 'STT_FUNC':\r\n                continue\r\n\r\n            f.seek(0)\r\n            symbol_addr = symbol.entry.st_value & 0xFFFE\r\n            f.seek(symbol_addr)\r\n            symbol_hexs = ''\r\n\r\n            size = symbol.entry.st_size\r\n            if symbol.entry.st_size > 80:\r\n                size = 80\r\n\r\n            for x in f.read(size):\r\n                op = str(hex(x)).upper()[2:]\r\n                if len(op) == 1:\r\n                    op = '0' + op\r\n                symbol_hexs = symbol_hexs + op\r\n            item = {}\r\n            item[\"name\"] = symbol.name\r\n            item[\"data\"] = symbol_hexs\r\n            dynsym_datas.append(item)\r\n\r\n    return dynsym_datas"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_rodata_strings(elf_file):\r\n    try:\r\n        return display_string_dump(elf_file, '.rodata')\r\n    except ELFError as ex:\r\n        import sys\r\n        sys.stderr.write('ELF error: %s\\n' % ex)\r\n        sys.exit(1)", "response": "Get the rodata strings from an ELF file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _section_from_spec(elf_file, spec):\r\n    '''\r\n        Retrieve a section given a \"spec\" (either number or name).\r\n        Return None if no such section exists in the file.\r\n    '''\r\n    if isinstance(spec, int):\r\n        num = int(spec)\r\n        if num < elf_file.num_sections():\r\n            return elf_file.get_section(num)\r\n\r\n    # Not a number. Must be a name then\r\n    if isinstance(spec, str):\r\n        try:\r\n            return elf_file.get_section_by_name(spec)\r\n        except AttributeError:\r\n            return None", "response": "Returns a section given a spec."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndisplaying a strings dump of a section.", "response": "def display_string_dump(self, section_spec):\r\n        \"\"\" Display a strings dump of a section. section_spec is either a\r\n            section number or a name.\r\n        \"\"\"\r\n        section = _section_from_spec(self.elf_file, section_spec)\r\n        if section is None:\r\n            print(\"Section '%s' does not exist in the file!\" % section_spec)\r\n            return None\r\n\r\n        data = section.data()\r\n        dataptr = 0\r\n\r\n        strs = []\r\n        while dataptr < len(data):\r\n            while dataptr < len(data) and not 32 <= byte2int(data[dataptr]) <= 127:\r\n                dataptr += 1\r\n\r\n            if dataptr >= len(data):\r\n                break\r\n\r\n            endptr = dataptr\r\n            while endptr < len(data) and byte2int(data[endptr]) != 0:\r\n                endptr += 1\r\n\r\n            strs.append(binascii.b2a_hex(\r\n                data[dataptr:endptr]).decode().upper())\r\n            dataptr = endptr\r\n\r\n        return strs"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a section given a spec.", "response": "def _section_from_spec(self, spec):\r\n        '''\r\n            Retrieve a section given a \"spec\" (either number or name).\r\n            Return None if no such section exists in the file.\r\n        '''\r\n        try:\r\n            num = int(spec)\r\n            if num < self.elf_file.num_sections():\r\n                return self.elf_file.get_section(num)\r\n            else:\r\n                return None\r\n        except ValueError:\r\n            # Not a number. Must be a name then\r\n            return self.elf_file.get_section_by_name(spec)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef remove_invalid_xml_chars(val):\n    val = make_utf8(val)\n    val = re.sub(u'[^\\u0020-\\uD7FF\\u0009\\u000A\\u000D\\uE000-\\uFFFD\\U00010000-\\U0010FFFF]+', '?', val)\n    return val", "response": "Remove invalid XML characters from the input string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _EnvOpen(var, mode):\n  value = os.getenv(var)\n  if value is None:\n    raise ValueError(\"%s is not set\" % var)\n\n  fd = int(value)\n\n  # If running on Windows, convert the file handle to a C file descriptor; see:\n  # https://groups.google.com/forum/#!topic/dev-python/GeN5bFJWfJ4\n  if _WINDOWS:\n    fd = msvcrt.open_osfhandle(fd, 0)\n\n  return os.fdopen(fd, mode)", "response": "Open a file descriptor identified by an environment variable."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef Send(self, message):\n    if not isinstance(message, common_pb2.Message):\n      raise ValueError(\"Send requires a fleetspeak.Message\")\n\n    if message.destination.service_name == \"system\":\n      raise ValueError(\n          \"Only predefined messages can have destination.service_name == \\\"system\\\"\")\n\n    return self._SendImpl(message)", "response": "Sends a message through Fleetspeak."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef Recv(self):\n    size = struct.unpack(_STRUCT_FMT, self._ReadN(_STRUCT_LEN))[0]\n    if size > MAX_SIZE:\n      raise ProtocolError(\"Expected size to be at most %d, got %d\" % (MAX_SIZE,\n                                                                      size))\n    with self._read_lock:\n      buf = self._ReadN(size)\n      self._ReadMagic()\n\n    res = common_pb2.Message()\n    res.ParseFromString(buf)\n\n    return res, len(buf)", "response": "Reads a message from the Fleetspeak."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsends a heartbeat to the Fleetspeak client.", "response": "def Heartbeat(self):\n    \"\"\"Sends a heartbeat to the Fleetspeak client.\n\n    If this daemonservice is configured to use heartbeats, clients that don't\n    call this method often enough are considered faulty and are restarted by\n    Fleetspeak.\n    \"\"\"\n    heartbeat_msg = common_pb2.Message(\n        message_type=\"Heartbeat\",\n        destination=common_pb2.Address(service_name=\"system\"))\n    self._SendImpl(heartbeat_msg)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread n characters from the input stream.", "response": "def _ReadN(self, n):\n    \"\"\"Reads n characters from the input stream, or until EOF.\n\n    This is equivalent to the current CPython implementation of read(n), but\n    not guaranteed by the docs.\n\n    Args:\n      n: int\n\n    Returns:\n      string\n    \"\"\"\n    ret = \"\"\n    while True:\n      chunk = self._read_file.read(n - len(ret))\n      ret += chunk\n\n      if len(ret) == n or not chunk:\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _CompileProtos():\n  proto_files = []\n  for dir_path, _, filenames in os.walk(THIS_DIRECTORY):\n    for filename in filenames:\n      if filename.endswith(\".proto\"):\n        proto_files.append(os.path.join(dir_path, filename))\n  if not proto_files:\n    return\n  protoc_command = [\n      \"python\", \"-m\", \"grpc_tools.protoc\",\n      \"--python_out\", THIS_DIRECTORY,\n      \"--grpc_python_out\", THIS_DIRECTORY,\n      \"--proto_path\", THIS_DIRECTORY,\n  ]\n  protoc_command.extend(proto_files)\n  subprocess.check_output(protoc_command)", "response": "Compiles all Fleetspeak protos."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nattempting an operation until success or deadline has passed.", "response": "def _RetryLoop(self, func, timeout=None):\n    \"\"\"Retries an operation until success or deadline.\n\n    Args:\n\n      func: The function to run. Must take a timeout, in seconds, as a single\n        parameter. If it raises grpc.RpcError and deadline has not be reached,\n        it will be run again.\n\n      timeout: Retries will continue until timeout seconds have passed.\n    \"\"\"\n    timeout = timeout or self.DEFAULT_TIMEOUT\n    deadline = time.time() + timeout\n    sleep = 1\n    while True:\n      try:\n        return func(timeout)\n      except grpc.RpcError:\n        if time.time() + sleep > deadline:\n          raise\n        time.sleep(sleep)\n        sleep *= 2\n        timeout = deadline - time.time()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef InsertMessage(self, message, timeout=None):\n    if not isinstance(message, common_pb2.Message):\n      raise InvalidArgument(\"Attempt to send unexpected message type: %s\" %\n                            message.__class__.__name__)\n\n    if not message.HasField(\"source\"):\n      message.source.service_name = self._service_name\n\n    # Sometimes GRPC reports failure, even though the call succeeded. To prevent\n    # retry logic from creating duplicate messages we fix the message_id.\n    if not message.message_id:\n      message.message_id = os.urandom(32)\n\n    return self._RetryLoop(\n        lambda t: self._stub.InsertMessage(message, timeout=t))", "response": "Inserts a message into the Fleetspeak server."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ListClients(self, request, timeout=None):\n    return self._RetryLoop(\n        lambda t: self._stub.ListClients(request, timeout=t))", "response": "Provides basic information about Fleetspeak clients."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef Send(self, message):\n    if not self.outgoing:\n      raise NotConfigured(\"Send address not provided.\")\n    self.outgoing.InsertMessage(message)", "response": "Send one message.\n\n    Deprecated, users should migrate to call self.outgoing.InsertMessage\n    directly."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef bars(self, symbol='000001', category='9', start='0', offset='100'):\n        '''\n        \u83b7\u53d6\u5b9e\u65f6\u65e5K\u7ebf\u6570\u636e\n\n        :param symbol: \u80a1\u7968\u4ee3\u7801\n        :param category: \u6570\u636e\u7c7b\u522b\n        :param market: \u8bc1\u5238\u5e02\u573a\n        :param start: \u5f00\u59cb\u4f4d\u7f6e\n        :param offset: \u6bcf\u6b21\u83b7\u53d6\u6761\u6570\n        :return: pd.dataFrame or None\n        '''\n        market = get_stock_market(symbol)\n\n        with self.client.connect(*self.bestip):\n            data = self.client.get_security_bars(\n                int(category), int(market), str(symbol), int(start), int(offset))\n            return self.client.to_df(data)", "response": "\u83b7\u53d6\u5b9e\u65f6\u65e5K\u7ebf\u6570\u636e\n\n        :param symbol: \u80a1\u7968\u4ee3\u7801\n        :param category: \u6570\u636e\u7c7b\u522b\n        :param market: \u8bc1\u5238\u5e02\u573a\n        :param start: \u5f00\u59cb\u4f4d\u7f6e\n        :param offset: \u6bcf\u6b21\u83b7\u53d6\u6761\u6570\n        :return: pd.dataFrame or None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef minute_his(self, symbol='', datetime='20161209'):\n        '''\n        \u5206\u65f6\u5386\u53f2\u6570\u636e\n\n        :param market:\n        :param symbol:\n        :param datetime:\n        :return: pd.dataFrame or None\n        '''\n        market = get_stock_market(symbol)\n\n        with self.client.connect(*self.bestip):\n            data = self.client.get_history_minute_time_data(\n                int(market), symbol, datetime)\n            return self.client.to_df(data)", "response": "Get the minute his of a user."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the data from the database", "response": "def trans(self, symbol='', start=0, offset=10):\n        '''\n        \u67e5\u8be2\u5206\u7b14\u6210\u4ea4\n\n        :param market: \u5e02\u573a\u4ee3\u7801\n        :param symbol: \u80a1\u7968\u4ee3\u7801\n        :param start: \u8d77\u59cb\u4f4d\u7f6e\n        :param offset: \u8bf7\u6c42\u6570\u91cf\n        :return: pd.dataFrame or None\n        '''\n        market = get_stock_market(symbol)\n\n        with self.client.connect(*self.bestip):\n            data = self.client.get_transaction_data(\n                int(market), symbol, int(start), int(market))\n            return self.client.to_df(data)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef trans_his(self, symbol='', start=0, offset=10, date=''):\n        '''\n        \u67e5\u8be2\u5386\u53f2\u5206\u7b14\u6210\u4ea4\n\n        :param market: \u5e02\u573a\u4ee3\u7801\n        :param symbol: \u80a1\u7968\u4ee3\u7801\n        :param start: \u8d77\u59cb\u4f4d\u7f6e\n        :param offset: \u6570\u91cf\n        :param date: \u65e5\u671f\n        :return: pd.dataFrame or None\n        '''\n        market = get_stock_market(symbol)\n\n        with self.client.connect(*self.bestip):\n            data = self.client.get_history_transaction_data(\n                int(market), symbol, int(start), int(offset), date)\n            return self.client.to_df(data)", "response": "Get history of a specific symbol"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef company_content(self, symbol='', file='', start=0, offset=10):\n        '''\n        \u8bfb\u53d6\u516c\u53f8\u4fe1\u606f\u8be6\u60c5\n\n        :param market: \u5e02\u573a\u4ee3\u7801\n        :param symbol: \u80a1\u7968\u4ee3\u7801\n        :param file: \u6587\u4ef6\u540d\n        :param start: \u8d77\u59cb\u4f4d\u7f6e\n        :param offset: \u6570\u91cf\n        :return: pd.dataFrame or None\n        '''\n        market = get_stock_market(symbol)\n\n        with self.client.connect(*self.bestip):\n            data = self.client.get_company_info_content(\n                int(market), symbol, file, int(start), int(offset))\n            return self.client.to_df(data)", "response": "\u8bfb\u53d6\u516c\u53f8\u4fe1\u606f\u8be6\u60c5\n\n        :param market: \u5e02\u573a\u4ee3\u7801\n        :param symbol: \u80a1\u7968\u4ee3\u7801\n        :param file: \u6587\u4ef6\u540d\n        :param start: \u8d77\u59cb\u4f4d\u7f6e\n        :param offset: \u6570\u91cf\n        :return: pd.dataFrame or None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef k(self, symbol='', begin=None, end=None):\n        '''\n        \u8bfb\u53d6k\u7ebf\u4fe1\u606f\n\n        :param symbol:\n        :param begin:\n        :param end:\n        :return: pd.dataFrame or None\n        '''\n        with self.client.connect(*self.bestip):\n            data = self.client.get_k_data(symbol, begin, end)\n            return data", "response": "\u8bfb\u53d6k\u7ebf\u4fe1\u606f\n\n        :param symbol:\n        :param begin:\n        :param end:\n        :return: pd.dataFrame or None"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nindexes the data for the specified key.", "response": "def index(\n        self,\n        symbol='000001',\n        market='sh',\n        category='9',\n        start='0',\n        offset='100'):\n        '''\n        \u83b7\u53d6\u6307\u6570k\u7ebf\n\n        K\u7ebf\u79cd\u7c7b:\n        - 0 5\u5206\u949fK\u7ebf\n        - 1 15\u5206\u949fK\u7ebf\n        - 2 30\u5206\u949fK\u7ebf\n        - 3 1\u5c0f\u65f6K\u7ebf\n        - 4 \u65e5K\u7ebf\n        - 5 \u5468K\u7ebf\n        - 6 \u6708K\u7ebf\n        - 7 1\u5206\u949f\n        - 8 1\u5206\u949fK\u7ebf\n        - 9 \u65e5K\u7ebf\n        - 10 \u5b63K\u7ebf\n        - 11 \u5e74K\u7ebf\n\n        :param symbol: \u80a1\u7968\u4ee3\u7801\n        :param category: \u6570\u636e\u7c7b\u522b\n        :param market: \u8bc1\u5238\u5e02\u573a\n        :param start: \u5f00\u59cb\u4f4d\u7f6e\n        :param offset: \u6bcf\u6b21\u83b7\u53d6\u6761\u6570\n        :return: pd.dataFrame or None\n        '''\n        market = 1 if market == 'sh' else 0\n\n        with self.client.connect(*self.bestip):\n            data = self.client.get_index_bars(\n                int(category), int(market), str(symbol), int(start), int(offset))\n            return self.client.to_df(data)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef block(self, tofile=\"block.dat\"):\n        '''\n        \u83b7\u53d6\u8bc1\u5238\u677f\u5757\u4fe1\u606f\n\n        :param tofile:\n        :return: pd.dataFrame or None\n        '''\n        with self.client.connect(*self.bestip):\n            data = self.client.get_and_parse_block_info(tofile)\n            return self.client.to_df(data)", "response": "Get block info from a file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef quote5(self, market=47, symbol=\"IF1709\"):\n        '''\n        \u67e5\u8be2\u4e94\u6863\u884c\u60c5\n\n        :return: pd.dataFrame or None\n        '''\n        with self.client.connect(*self.bestip):\n            data = self.client.get_instrument_quote(market, symbol)\n            return self.client.to_df(data)", "response": "\u67e5\u8be2\u4e94\u6863\u884c\u60c5\n\n        :return: pd.dataFrame or None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef minute(self, market=47, symbol=\"IF1709\"):\n        '''\n        \u67e5\u8be2\u4e94\u6863\u884c\u60c5\n\n        :return: pd.dataFrame or None\n        '''\n        with self.client.connect(*self.bestip):\n            data = self.client.get_minute_time_data(market, symbol)\n            return self.client.to_df(data)", "response": "\u67e5\u8be2\u4e94\u6863\u884c\u60c5\n\n        :return: pd.dataFrame or None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the instrument information for a given set of items.", "response": "def instrument(self, start=0, offset=100):\n        '''\n        \u67e5\u8be2\u4ee3\u7801\u5217\u8868\n        :param start:\n        :param offset:\n        :return: pd.dataFrame or None\n        '''\n        with self.client.connect(*self.bestip):\n            # nums = self.client.get_instrument_count()\n            data = self.client.get_instrument_info(int(start), int(offset))\n            return self.client.to_df(data)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_stock_market(symbol='', string=False):\n    assert isinstance(symbol, str), 'stock code need str type'\n\n    if symbol.startswith(('sh', 'sz')):\n        market = symbol[:2]\n\n    if symbol.startswith(('50', '51', '60', '90', '110', '113', '132', '204')):\n        market = 'sh'\n\n    if symbol.startswith(\n        ('00',\n         '13',\n         '18',\n         '15',\n         '16',\n         '18',\n         '20',\n         '30',\n         '39',\n         '115',\n         '1318')):\n        market = 'sz'\n\n    if symbol.startswith(('5', '6', '9', '7')):\n        market = 'sh'\n\n    if string is False:\n        return 0 if market == 'sz' else 1\n\n    return market", "response": "\u5224\u65ad\u80a1\u7968ID\u5bf9\u5e94\u7684\u8bc1\u5238\u5e02\u573a\n    \u5339\u914d\u89c4\u5219\n    ['50', '51', '60', '90', '110'] \u4e3a sh\n    ['00', '13', '18', '15', '16', '18', '20', '30', '39', '115'] \u4e3a sz\n    ['5', '6', '9'] \u5f00\u5934\u7684\u4e3a sh\uff0c \u5176\u4f59\u4e3a sz\n    :param symbol:\u80a1\u7968ID, \u82e5\u4ee5 'sz', 'sh' \u5f00\u5934\u76f4\u63a5\u8fd4\u56de\u5bf9\u5e94\u7c7b\u578b\uff0c\u5426\u5219\u4f7f\u7528\u5185\u7f6e\u89c4\u5219\u5224\u65ad\n    :return 'sh' or 'sz"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfinding path to the vipdoc file", "response": "def find_path(self, symbol=None, subdir='lday', ext='day'):\n        '''\n        \u5bfb\u627e\u6587\u4ef6\u8def\u5f84\uff0c\u8f85\u52a9\u51fd\u6570\n\n        :param symbol:\n        :param subdir:\n        :param ext:\n        :return: pd.dataFrame or None\n        '''\n        market = get_stock_market(symbol, True)\n        ext = ext if isinstance(ext, list) else [ext]\n\n        for t in ext:\n            vipdoc = 'vipdoc/{}/{}/{}{}.{}'.format(market, subdir, market, symbol, t)\n            vipdoc = os.path.join(self.tdxdir, vipdoc)\n\n            if os.path.exists(vipdoc):\n                return vipdoc\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets daily data frame", "response": "def daily(self, symbol=None):\n        '''\n        \u83b7\u53d6\u65e5\u7ebf\u6570\u636e\n\n        :param symbol:\n        :return: pd.dataFrame or None\n        '''\n        reader = TdxDailyBarReader()\n        vipdoc = self.find_path(symbol=symbol, subdir='lday', ext='day')\n\n        if vipdoc is not None:\n            return reader.get_df(vipdoc)\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the data frame for the 1 - minute symbol", "response": "def minute(self, symbol=None):\n        '''\n        \u83b7\u53d61\u5206\u949f\u7ebf\n\n        :param symbol:\n        :return: pd.dataFrame or None\n        '''\n        symbol = self.find_path(symbol, subdir='minline', ext=['lc1', '1'])\n        reader = TdxLCMinBarReader()\n\n        if symbol is not None:\n            return reader.get_df(symbol)\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef block(self, group=False, custom=False):\n        '''\n        \u83b7\u53d6\u677f\u5757\u6570\u636e\n\n        :param group:\n        :param custom:\n        :return: pd.dataFrame or None\n        '''\n        reader = BlockReader()\n        symbol = os.path.join(self.tdxdir, 'block_zs.dat')\n\n        if symbol is not None:\n            return reader.get_df(symbol, group)\n\n        return None", "response": "get the block data frame"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef index(self, symbol='incon.dat', group=False):\n        '''\n        \u83b7\u53d6\u6307\u6570\u6570\u636e\n\n        :param symbol:\n        :param group:\n        :return: pd.dataFrame or None\n        '''\n        reader = BlockReader()\n        symbol = os.path.join(self.tdxdir, symbol)\n\n        if symbol is not None:\n            return reader.get_df(symbol, group)\n\n        return None", "response": "get the index of a specific symbol"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting daily data frame", "response": "def daily(self, symbol=None):\n        '''\n        \u83b7\u53d6\u65e5\u7ebf\u6570\u636e\n\n        :return: pd.dataFrame or None\n        '''\n        reader = TdxExHqDailyBarReader()\n        symbol = self.find_path(symbol)\n\n        if symbol is not None:\n            return reader.get_df(symbol)\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute the difference between two supercells.", "response": "def diff_matrix(array_1, array_2, cell_size):\n    \"\"\"\n    :param array_1: supercell scaled positions respect unit cell\n    :param array_2: supercell scaled positions respect unit cell\n    :param cell_size: diference between arrays accounting for periodicity\n    :return:\n    \"\"\"\n    array_1_norm = np.array(array_1) / np.array(cell_size, dtype=float)[None,:]\n    array_2_norm = np.array(array_2) / np.array(cell_size, dtype=float)[None,:]\n\n    return array_2_norm - array_1_norm"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _g_a (self, x, a, b, s):\n        return 2*b/(1.0+np.exp(s*(x-a)))", "response": "Asymmetric width term in the log - likelihood."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _function(self, x, a, b, c, d):\n        return c/((a**2-x**2)**2 + (b*x)**2)+d", "response": "Damped harmonic oscillator PS function\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef return_func_to_optimize(path, session, base_learner_origin, default_params,\n                            metric_to_optimize, invert_metric, integers):\n    \"\"\"Creates the function to be optimized by Bayes Optimization.\n\n    The function automatically handles the case of already existing base learners, and if\n    no base learner for the hyperparameters exists yet, creates one and updates it in the\n    usual way.\n\n    Args:\n        path (str): Path to Xcessiv notebook\n\n        session: Database session passed down\n\n        base_learner_origin: BaseLearnerOrigin object\n\n        default_params (dict): Dictionary containing default params of estimator\n\n        metric_to_optimize (str, unicode): String containing name of metric to optimize\n\n        invert_metric (bool): Specifies whether metric should be inverted e.g. losses\n\n        integers (set): Set of strings that specify which hyperparameters are integers\n\n    Returns:\n        func_to_optimize (function): Function to be optimized\n    \"\"\"\n    def func_to_optimize(**params):\n        base_estimator = base_learner_origin.return_estimator()\n        base_estimator.set_params(**default_params)\n        # For integer hyperparameters, make sure they are rounded off\n        params = dict((key, val) if key not in integers else (key, int(val))\n                      for key, val in iteritems(params))\n        base_estimator.set_params(**params)\n        hyperparameters = functions.make_serializable(base_estimator.get_params())\n\n        # Look if base learner already exists\n        base_learner = session.query(models.BaseLearner).\\\n            filter_by(base_learner_origin_id=base_learner_origin.id,\n                      hyperparameters=hyperparameters).first()\n\n        calculate_only = False\n\n        # If base learner exists and has finished, just return its result\n        if base_learner and base_learner.job_status == 'finished':\n            if invert_metric:\n                return -base_learner.individual_score[metric_to_optimize]\n            else:\n                return base_learner.individual_score[metric_to_optimize]\n\n        # else if base learner exists but is unfinished, just calculate the result without storing\n        elif base_learner and base_learner.job_status != 'finished':\n            calculate_only = True\n\n        # else if base learner does not exist, create it\n        else:\n            base_learner = models.BaseLearner(hyperparameters,\n                                              'started',\n                                              base_learner_origin)\n            base_learner.job_id = get_current_job().id\n            session.add(base_learner)\n            session.commit()\n\n        try:\n            est = base_learner.return_estimator()\n            extraction = session.query(models.Extraction).first()\n            X, y = extraction.return_train_dataset()\n            return_splits_iterable = functions.import_object_from_string_code(\n                extraction.meta_feature_generation['source'],\n                'return_splits_iterable'\n            )\n\n            meta_features_list = []\n            trues_list = []\n            for train_index, test_index in return_splits_iterable(X, y):\n                X_train, X_test = X[train_index], X[test_index]\n                y_train, y_test = y[train_index], y[test_index]\n                est = est.fit(X_train, y_train)\n                meta_features_list.append(\n                    getattr(est, base_learner.base_learner_origin.\n                            meta_feature_generator)(X_test)\n                )\n                trues_list.append(y_test)\n            meta_features = np.concatenate(meta_features_list, axis=0)\n            y_true = np.concatenate(trues_list)\n\n            for key in base_learner.base_learner_origin.metric_generators:\n                metric_generator = functions.import_object_from_string_code(\n                    base_learner.base_learner_origin.metric_generators[key],\n                    'metric_generator'\n                )\n                base_learner.individual_score[key] = metric_generator(y_true, meta_features)\n\n            # Only do this if you want to save things\n            if not calculate_only:\n                meta_features_path = base_learner.meta_features_path(path)\n\n                if not os.path.exists(os.path.dirname(meta_features_path)):\n                    os.makedirs(os.path.dirname(meta_features_path))\n\n                np.save(meta_features_path, meta_features, allow_pickle=False)\n                base_learner.job_status = 'finished'\n                base_learner.meta_features_exists = True\n                session.add(base_learner)\n                session.commit()\n\n            if invert_metric:\n                return -base_learner.individual_score[metric_to_optimize]\n            else:\n                return base_learner.individual_score[metric_to_optimize]\n\n        except:\n            session.rollback()\n            base_learner.job_status = 'errored'\n            base_learner.description['error_type'] = repr(sys.exc_info()[0])\n            base_learner.description['error_value'] = repr(sys.exc_info()[1])\n            base_learner.description['error_traceback'] = \\\n                traceback.format_exception(*sys.exc_info())\n            session.add(base_learner)\n            session.commit()\n            raise\n    return func_to_optimize", "response": "Creates the function to be optimized by Bayes Optimization."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef start_naive_bayes(automated_run, session, path):\n    module = functions.import_string_code_as_module(automated_run.source)\n    random_state = 8 if not hasattr(module, 'random_state') else module.random_state\n    assert module.metric_to_optimize in automated_run.base_learner_origin.metric_generators\n\n    # get non-searchable parameters\n    base_estimator = automated_run.base_learner_origin.return_estimator()\n    base_estimator.set_params(**module.default_params)\n    default_params = functions.make_serializable(base_estimator.get_params())\n    non_searchable_params = dict((key, val) for key, val in iteritems(default_params)\n                                 if key not in module.pbounds)\n\n    # get already calculated base learners in search space\n    existing_base_learners = []\n    for base_learner in automated_run.base_learner_origin.base_learners:\n        if not base_learner.job_status == 'finished':\n            continue\n        in_search_space = True\n        for key, val in iteritems(non_searchable_params):\n            if base_learner.hyperparameters[key] != val:\n                in_search_space = False\n                break  # If no match, move on to the next base learner\n        if in_search_space:\n            existing_base_learners.append(base_learner)\n\n    # build initialize dictionary\n    target = []\n    initialization_dict = dict((key, list()) for key in module.pbounds.keys())\n    for base_learner in existing_base_learners:\n        # check if base learner's searchable hyperparameters are all numerical\n        all_numerical = True\n        for key in module.pbounds.keys():\n            if not isinstance(base_learner.hyperparameters[key], numbers.Number):\n                all_numerical = False\n                break\n        if not all_numerical:\n            continue  # if there is a non-numerical hyperparameter, skip this.\n\n        for key in module.pbounds.keys():\n            initialization_dict[key].append(base_learner.hyperparameters[key])\n        target.append(base_learner.individual_score[module.metric_to_optimize])\n    initialization_dict['target'] = target if not module.invert_metric \\\n        else list(map(lambda x: -x, target))\n    print('{} existing in initialization dictionary'.\n          format(len(initialization_dict['target'])))\n\n    # Create function to be optimized\n    func_to_optimize = return_func_to_optimize(\n        path, session, automated_run.base_learner_origin, module.default_params,\n        module.metric_to_optimize, module.invert_metric, set(module.integers)\n    )\n\n    # Create Bayes object\n    bo = BayesianOptimization(func_to_optimize, module.pbounds)\n\n    bo.initialize(initialization_dict)\n\n    np.random.seed(random_state)\n\n    bo.maximize(**module.maximize_config)", "response": "Starts naive bayes automated run."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef start_tpot(automated_run, session, path):\n    module = functions.import_string_code_as_module(automated_run.source)\n    extraction = session.query(models.Extraction).first()\n    X, y = extraction.return_train_dataset()\n\n    tpot_learner =  module.tpot_learner\n\n    tpot_learner.fit(X, y)\n\n    temp_filename = os.path.join(path, 'tpot-temp-export-{}'.format(os.getpid()))\n    tpot_learner.export(temp_filename)\n\n    with open(temp_filename) as f:\n        base_learner_source = f.read()\n\n    base_learner_source = constants.tpot_learner_docstring + base_learner_source\n\n    try:\n        os.remove(temp_filename)\n    except OSError:\n        pass\n\n    blo = models.BaseLearnerOrigin(\n        source=base_learner_source,\n        name='TPOT Learner',\n        meta_feature_generator='predict'\n    )\n\n    session.add(blo)\n    session.commit()", "response": "Starts a TPOT automated run that exports directly to base learner setup"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nstarting an automated ensemble search using Greedy Forward Model Selection.", "response": "def start_greedy_ensemble_search(automated_run, session, path):\n    \"\"\"Starts an automated ensemble search using greedy forward model selection.\n\n    The steps for this search are adapted from \"Ensemble Selection from Libraries of Models\" by\n    Caruana.\n\n    1. Start with the empty ensemble\n\n    2. Add to the ensemble the model in the library that maximizes the ensemmble's\n    performance on the error metric.\n\n    3. Repeat step 2 for a fixed number of iterations or until all models have been used.\n\n    Args:\n        automated_run (xcessiv.models.AutomatedRun): Automated run object\n\n        session: Valid SQLAlchemy session\n\n        path (str, unicode): Path to project folder\n    \"\"\"\n    module = functions.import_string_code_as_module(automated_run.source)\n    assert module.metric_to_optimize in automated_run.base_learner_origin.metric_generators\n\n    best_ensemble = []  # List containing IDs of best performing ensemble for the last round\n\n    secondary_learner = automated_run.base_learner_origin.return_estimator()\n    secondary_learner.set_params(**module.secondary_learner_hyperparameters)\n\n    for i in range(module.max_num_base_learners):\n        best_score = -float('inf')  # Best metric for this round (not in total!)\n        current_ensemble = best_ensemble[:]  # Shallow copy of best ensemble\n        for base_learner in session.query(models.BaseLearner).filter_by(job_status='finished').all():\n            if base_learner in current_ensemble:  # Don't append when learner is already in\n                continue\n            current_ensemble.append(base_learner)\n\n            # Check if our \"best ensemble\" already exists\n            existing_ensemble = session.query(models.StackedEnsemble).\\\n                filter_by(base_learner_origin_id=automated_run.base_learner_origin.id,\n                          secondary_learner_hyperparameters=secondary_learner.get_params(),\n                          base_learner_ids=sorted([bl.id for bl in current_ensemble])).first()\n\n            if existing_ensemble and existing_ensemble.job_status == 'finished':\n                score = existing_ensemble.individual_score[module.metric_to_optimize]\n\n            elif existing_ensemble and existing_ensemble.job_status != 'finished':\n                eval_stacked_ensemble(existing_ensemble, session, path)\n                score = existing_ensemble.individual_score[module.metric_to_optimize]\n\n            else:\n                stacked_ensemble = models.StackedEnsemble(\n                    secondary_learner_hyperparameters=secondary_learner.get_params(),\n                    base_learners=current_ensemble,\n                    base_learner_origin=automated_run.base_learner_origin,\n                    job_status='started'\n                )\n                session.add(stacked_ensemble)\n                session.commit()\n                eval_stacked_ensemble(stacked_ensemble, session, path)\n                score = stacked_ensemble.individual_score[module.metric_to_optimize]\n\n            score = -score if module.invert_metric else score\n\n            if best_score < score:\n                best_score = score\n                best_ensemble = current_ensemble[:]\n\n            current_ensemble.pop()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating data statistics for the given data extraction setup.", "response": "def extraction_data_statistics(path):\n    \"\"\" Generates data statistics for the given data extraction setup stored\n    in Xcessiv notebook.\n\n    This is in rqtasks.py but not as a job yet. Temporarily call this directly\n    while I'm figuring out Javascript lel.\n\n    Args:\n        path (str, unicode): Path to xcessiv notebook\n    \"\"\"\n    with functions.DBContextManager(path) as session:\n        extraction = session.query(models.Extraction).first()\n        X, y = extraction.return_main_dataset()\n        functions.verify_dataset(X, y)\n\n        if extraction.test_dataset['method'] == 'split_from_main':\n            X, X_test, y, y_test = train_test_split(\n                X,\n                y,\n                test_size=extraction.test_dataset['split_ratio'],\n                random_state=extraction.test_dataset['split_seed'],\n                stratify=y\n            )\n        elif extraction.test_dataset['method'] == 'source':\n            if 'source' not in extraction.test_dataset or not extraction.test_dataset['source']:\n                raise exceptions.UserError('Source is empty')\n\n            extraction_code = extraction.test_dataset[\"source\"]\n            extraction_function = functions.\\\n                import_object_from_string_code(extraction_code, \"extract_test_dataset\")\n            X_test, y_test = extraction_function()\n        else:\n            X_test, y_test = None, None\n\n        # test base learner cross-validation\n        extraction_code = extraction.meta_feature_generation['source']\n        return_splits_iterable = functions.import_object_from_string_code(\n            extraction_code,\n            'return_splits_iterable'\n        )\n        number_of_splits = 0\n        test_indices = []\n        try:\n            for train_idx, test_idx in return_splits_iterable(X, y):\n                number_of_splits += 1\n                test_indices.append(test_idx)\n        except Exception as e:\n            raise exceptions.UserError('User code exception', exception_message=str(e))\n\n        # preparation before testing stacked ensemble cross-validation\n        test_indices = np.concatenate(test_indices)\n        X, y = X[test_indices], y[test_indices]\n\n        # test stacked ensemble cross-validation\n        extraction_code = extraction.stacked_ensemble_cv['source']\n        return_splits_iterable = functions.import_object_from_string_code(\n            extraction_code,\n            'return_splits_iterable'\n        )\n        number_of_splits_stacked_cv = 0\n        try:\n            for train_idx, test_idx in return_splits_iterable(X, y):\n                number_of_splits_stacked_cv += 1\n        except Exception as e:\n            raise exceptions.UserError('User code exception', exception_message=str(e))\n\n        data_stats = dict()\n        data_stats['train_data_stats'] = functions.verify_dataset(X, y)\n        if X_test is not None:\n            data_stats['test_data_stats'] = functions.verify_dataset(X_test, y_test)\n        else:\n            data_stats['test_data_stats'] = None\n        data_stats['holdout_data_stats'] = {'number_of_splits': number_of_splits}\n        data_stats['stacked_ensemble_cv_stats'] = {'number_of_splits': number_of_splits_stacked_cv}\n\n        extraction.data_statistics = data_stats\n\n        session.add(extraction)\n        session.commit()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating meta - features for the specified base learner.", "response": "def generate_meta_features(path, base_learner_id):\n    \"\"\"Generates meta-features for specified base learner\n\n    After generation of meta-features, the file is saved into the meta-features folder\n\n    Args:\n        path (str): Path to Xcessiv notebook\n\n        base_learner_id (str): Base learner ID\n    \"\"\"\n    with functions.DBContextManager(path) as session:\n        base_learner = session.query(models.BaseLearner).filter_by(id=base_learner_id).first()\n        if not base_learner:\n            raise exceptions.UserError('Base learner {} '\n                                       'does not exist'.format(base_learner_id))\n\n        base_learner.job_id = get_current_job().id\n        base_learner.job_status = 'started'\n\n        session.add(base_learner)\n        session.commit()\n\n        try:\n            est = base_learner.return_estimator()\n            extraction = session.query(models.Extraction).first()\n            X, y = extraction.return_train_dataset()\n            return_splits_iterable = functions.import_object_from_string_code(\n                extraction.meta_feature_generation['source'],\n                'return_splits_iterable'\n            )\n\n            meta_features_list = []\n            trues_list = []\n            for train_index, test_index in return_splits_iterable(X, y):\n                X_train, X_test = X[train_index], X[test_index]\n                y_train, y_test = y[train_index], y[test_index]\n                est = est.fit(X_train, y_train)\n                meta_features_list.append(\n                    getattr(est, base_learner.base_learner_origin.\n                            meta_feature_generator)(X_test)\n                )\n                trues_list.append(y_test)\n            meta_features = np.concatenate(meta_features_list, axis=0)\n            y_true = np.concatenate(trues_list)\n\n            for key in base_learner.base_learner_origin.metric_generators:\n                metric_generator = functions.import_object_from_string_code(\n                    base_learner.base_learner_origin.metric_generators[key],\n                    'metric_generator'\n                )\n                base_learner.individual_score[key] = metric_generator(y_true, meta_features)\n\n            meta_features_path = base_learner.meta_features_path(path)\n\n            if not os.path.exists(os.path.dirname(meta_features_path)):\n                os.makedirs(os.path.dirname(meta_features_path))\n\n            np.save(meta_features_path, meta_features, allow_pickle=False)\n            base_learner.job_status = 'finished'\n            base_learner.meta_features_exists = True\n            session.add(base_learner)\n            session.commit()\n\n        except:\n            session.rollback()\n            base_learner.job_status = 'errored'\n            base_learner.description['error_type'] = repr(sys.exc_info()[0])\n            base_learner.description['error_value'] = repr(sys.exc_info()[1])\n            base_learner.description['error_traceback'] = \\\n                traceback.format_exception(*sys.exc_info())\n            session.add(base_learner)\n            session.commit()\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef start_automated_run(path, automated_run_id):\n    with functions.DBContextManager(path) as session:\n        automated_run = session.query(models.AutomatedRun).filter_by(id=automated_run_id).first()\n        if not automated_run:\n            raise exceptions.UserError('Automated run {} '\n                                       'does not exist'.format(automated_run_id))\n        automated_run.job_id = get_current_job().id\n        automated_run.job_status = 'started'\n\n        session.add(automated_run)\n        session.commit()\n\n        try:\n            if automated_run.category == 'bayes':\n                automatedruns.start_naive_bayes(automated_run, session, path)\n\n            elif automated_run.category == 'tpot':\n                automatedruns.start_tpot(automated_run, session, path)\n\n            elif automated_run.category == 'greedy_ensemble_search':\n                automatedruns.start_greedy_ensemble_search(automated_run, session, path)\n\n            else:\n                raise Exception('Something went wrong. Invalid category for automated run')\n\n            automated_run.job_status = 'finished'\n            session.add(automated_run)\n            session.commit()\n\n        except:\n            session.rollback()\n            automated_run.job_status = 'errored'\n            automated_run.description['error_type'] = repr(sys.exc_info()[0])\n            automated_run.description['error_value'] = repr(sys.exc_info()[1])\n            automated_run.description['error_traceback'] = \\\n                traceback.format_exception(*sys.exc_info())\n            session.add(automated_run)\n            session.commit()\n            raise", "response": "Starts automated run. This will automatically create base learners until the run finishes or errors out."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nevaluating the ensemble and updates the database when finished", "response": "def evaluate_stacked_ensemble(path, ensemble_id):\n    \"\"\"Evaluates the ensemble and updates the database when finished/\n\n    Args:\n        path (str): Path to Xcessiv notebook\n\n        ensemble_id (str): Ensemble ID\n    \"\"\"\n    with functions.DBContextManager(path) as session:\n        stacked_ensemble = session.query(models.StackedEnsemble).filter_by(\n            id=ensemble_id).first()\n        if not stacked_ensemble:\n            raise exceptions.UserError('Stacked ensemble {} '\n                                       'does not exist'.format(ensemble_id))\n\n        stacked_ensemble.job_id = get_current_job().id\n        stacked_ensemble.job_status = 'started'\n\n        session.add(stacked_ensemble)\n        session.commit()\n\n        try:\n            meta_features_list = []\n            for base_learner in stacked_ensemble.base_learners:\n                mf = np.load(base_learner.meta_features_path(path))\n                if len(mf.shape) == 1:\n                    mf = mf.reshape(-1, 1)\n                meta_features_list.append(mf)\n\n            secondary_features = np.concatenate(meta_features_list, axis=1)\n\n            # Get data\n            extraction = session.query(models.Extraction).first()\n            return_splits_iterable = functions.import_object_from_string_code(\n                extraction.meta_feature_generation['source'],\n                'return_splits_iterable'\n            )\n            X, y = extraction.return_train_dataset()\n\n            #  We need to retrieve original order of meta-features\n            indices_list = [test_index for train_index, test_index in return_splits_iterable(X, y)]\n            indices = np.concatenate(indices_list)\n            X, y = X[indices], y[indices]\n\n            est = stacked_ensemble.return_secondary_learner()\n\n            return_splits_iterable_stacked_ensemble = functions.import_object_from_string_code(\n                extraction.stacked_ensemble_cv['source'],\n                'return_splits_iterable'\n            )\n            preds = []\n            trues_list = []\n            for train_index, test_index in return_splits_iterable_stacked_ensemble(secondary_features, y):\n                X_train, X_test = secondary_features[train_index], secondary_features[test_index]\n                y_train, y_test = y[train_index], y[test_index]\n                est = est.fit(X_train, y_train)\n                preds.append(\n                    getattr(est, stacked_ensemble.base_learner_origin.\n                            meta_feature_generator)(X_test)\n                )\n                trues_list.append(y_test)\n            preds = np.concatenate(preds, axis=0)\n            y_true = np.concatenate(trues_list)\n\n            for key in stacked_ensemble.base_learner_origin.metric_generators:\n                metric_generator = functions.import_object_from_string_code(\n                    stacked_ensemble.base_learner_origin.metric_generators[key],\n                    'metric_generator'\n                )\n                stacked_ensemble.individual_score[key] = metric_generator(y_true, preds)\n\n            stacked_ensemble.job_status = 'finished'\n            session.add(stacked_ensemble)\n            session.commit()\n\n        except:\n            session.rollback()\n            stacked_ensemble.job_status = 'errored'\n            stacked_ensemble.description['error_type'] = repr(sys.exc_info()[0])\n            stacked_ensemble.description['error_value'] = repr(sys.exc_info()[1])\n            stacked_ensemble.description['error_traceback'] = \\\n                traceback.format_exception(*sys.exc_info())\n            session.add(stacked_ensemble)\n            session.commit()\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns SHA256 checksum of a file", "response": "def hash_file(path, block_size=65536):\n    \"\"\"Returns SHA256 checksum of a file\n\n    Args:\n        path (string): Absolute file path of file to hash\n\n        block_size (int, optional): Number of bytes to read per block\n    \"\"\"\n    sha256 = hashlib.sha256()\n    with open(path, 'rb') as f:\n        for block in iter(lambda: f.read(block_size), b''):\n            sha256.update(block)\n    return sha256.hexdigest()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef import_object_from_path(path, object):\n    with open(path) as f:\n        return import_object_from_string_code(f.read(), object)", "response": "Used to import an object from an absolute path."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef import_object_from_string_code(code, object):\n    sha256 = hashlib.sha256(code.encode('UTF-8')).hexdigest()\n    module = imp.new_module(sha256)\n    try:\n        exec_(code, module.__dict__)\n    except Exception as e:\n        raise exceptions.UserError('User code exception', exception_message=str(e))\n    sys.modules[sha256] = module\n    try:\n        return getattr(module, object)\n    except AttributeError:\n        raise exceptions.UserError(\"{} not found in code\".format(object))", "response": "Used to import an object from arbitrary passed Python code."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef import_string_code_as_module(code):\n    sha256 = hashlib.sha256(code.encode('UTF-8')).hexdigest()\n    module = imp.new_module(sha256)\n    try:\n        exec_(code, module.__dict__)\n    except Exception as e:\n        raise exceptions.UserError('User code exception', exception_message=str(e))\n    sys.modules[sha256] = module\n    return module", "response": "Used to run arbitrary passed code as a module and return the module object."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nverifies if a dataset is valid for use i. e. scikit - learn format for a node - level dataset.", "response": "def verify_dataset(X, y):\n    \"\"\"Verifies if a dataset is valid for use i.e. scikit-learn format\n\n    Used to verify a dataset by returning shape and basic statistics of\n    returned data. This will also provide quick and dirty check on\n    capability of host machine to process the data.\n\n    Args:\n        X (array-like): Features array\n\n        y (array-like): Label array\n\n    Returns:\n        X_shape (2-tuple of int): Shape of X returned\n\n        y_shape (1-tuple of int): Shape of y returned\n\n    Raises:\n        AssertionError: `X_shape` must be of length 2 and `y_shape` must be of\n            length 1. `X` must have the same number of elements as `y`\n            i.e. X_shape[0] == y_shape[0]. If any of these conditions are not met,\n            an AssertionError is raised.\n    \"\"\"\n    X_shape, y_shape = np.array(X).shape, np.array(y).shape\n    if len(X_shape) != 2:\n        raise exceptions.UserError(\"X must be 2-dimensional array\")\n    if len(y_shape) != 1:\n        raise exceptions.UserError(\"y must be 1-dimensional array\")\n    if X_shape[0] != y_shape[0]:\n        raise exceptions.UserError(\"X must have same number of elements as y\")\n    return dict(\n        features_shape=X_shape,\n        labels_shape=y_shape\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a sample dataset for the given properties.", "response": "def get_sample_dataset(dataset_properties):\n    \"\"\"Returns sample dataset\n\n    Args:\n        dataset_properties (dict): Dictionary corresponding to the properties of the dataset\n            used to verify the estimator and metric generators.\n\n    Returns:\n        X (array-like): Features array\n\n        y (array-like): Labels array\n\n        splits (iterator): This is an iterator that returns train test splits for\n            cross-validation purposes on ``X`` and ``y``.\n    \"\"\"\n    kwargs = dataset_properties.copy()\n    data_type = kwargs.pop('type')\n    if data_type == 'multiclass':\n        try:\n            X, y = datasets.make_classification(random_state=8, **kwargs)\n            splits = model_selection.StratifiedKFold(n_splits=2, random_state=8).split(X, y)\n        except Exception as e:\n            raise exceptions.UserError(repr(e))\n    elif data_type == 'iris':\n        X, y = datasets.load_iris(return_X_y=True)\n        splits = model_selection.StratifiedKFold(n_splits=2, random_state=8).split(X, y)\n    elif data_type == 'mnist':\n        X, y = datasets.load_digits(return_X_y=True)\n        splits = model_selection.StratifiedKFold(n_splits=2, random_state=8).split(X, y)\n    elif data_type == 'breast_cancer':\n        X, y = datasets.load_breast_cancer(return_X_y=True)\n        splits = model_selection.StratifiedKFold(n_splits=2, random_state=8).split(X, y)\n    elif data_type == 'boston':\n        X, y = datasets.load_boston(return_X_y=True)\n        splits = model_selection.KFold(n_splits=2, random_state=8).split(X)\n    elif data_type == 'diabetes':\n        X, y = datasets.load_diabetes(return_X_y=True)\n        splits = model_selection.KFold(n_splits=2, random_state=8).split(X)\n    else:\n        raise exceptions.UserError('Unknown dataset type {}'.format(dataset_properties['type']))\n    return X, y, splits"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef verify_estimator_class(est, meta_feature_generator, metric_generators, dataset_properties):\n    X, y, splits = get_sample_dataset(dataset_properties)\n\n    if not hasattr(est, \"get_params\"):\n        raise exceptions.UserError('Estimator does not have get_params method')\n    if not hasattr(est, \"set_params\"):\n        raise exceptions.UserError('Estimator does not have set_params method')\n    if not hasattr(est, meta_feature_generator):\n        raise exceptions.UserError('Estimator does not have meta-feature generator'\n                                   ' {}'.format(meta_feature_generator))\n\n    performance_dict = dict()\n\n    true_labels = []\n    preds = []\n\n    try:\n        for train_index, test_index in splits:\n            X_train, X_test = X[train_index], X[test_index]\n            y_train, y_test = y[train_index], y[test_index]\n            est.fit(X_train, y_train)\n            true_labels.append(y_test)\n            preds.append(getattr(est, meta_feature_generator)(X_test))\n        true_labels = np.concatenate(true_labels)\n        preds = np.concatenate(preds, axis=0)\n    except Exception as e:\n        raise exceptions.UserError(repr(e))\n\n    if preds.shape[0] != true_labels.shape[0]:\n        raise exceptions.UserError('Estimator\\'s meta-feature generator '\n                                   'does not produce valid shape')\n\n    for key in metric_generators:\n        metric_generator = import_object_from_string_code(\n            metric_generators[key],\n            'metric_generator'\n        )\n        try:\n            performance_dict[key] = metric_generator(true_labels, preds)\n        except Exception as e:\n            raise exceptions.UserError(repr(e))\n\n    return performance_dict, make_serializable(est.get_params())", "response": "Verify if an estimator object is valid for use by scikit - learn format."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets path from query string", "response": "def get_path_from_query_string(req):\n    \"\"\"Gets path from query string\n\n    Args:\n        req (flask.request): Request object from Flask\n\n    Returns:\n        path (str): Value of \"path\" parameter from query string\n\n    Raises:\n        exceptions.UserError: If \"path\" is not found in query string\n    \"\"\"\n    if req.args.get('path') is None:\n        raise exceptions.UserError('Path not found in query string')\n    return req.args.get('path')"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns main data set from self. main_dataset", "response": "def return_main_dataset(self):\n        \"\"\"Returns main data set from self\n\n        Returns:\n            X (numpy.ndarray): Features\n\n            y (numpy.ndarray): Labels\n        \"\"\"\n        if not self.main_dataset['source']:\n            raise exceptions.UserError('Source is empty')\n\n        extraction_code = self.main_dataset[\"source\"]\n\n        extraction_function = functions.import_object_from_string_code(extraction_code,\n                                                                       \"extract_main_dataset\")\n        try:\n            X, y = extraction_function()\n        except Exception as e:\n            raise exceptions.UserError('User code exception', exception_message=str(e))\n\n        X, y = np.array(X), np.array(y)\n\n        return X, y"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning train data set and train test data set.", "response": "def return_train_dataset(self):\n        \"\"\"Returns train data set\n\n        Returns:\n            X (numpy.ndarray): Features\n\n            y (numpy.ndarray): Labels\n        \"\"\"\n        X, y = self.return_main_dataset()\n\n        if self.test_dataset['method'] == 'split_from_main':\n            X, X_test, y, y_test = train_test_split(\n                X,\n                y,\n                test_size=self.test_dataset['split_ratio'],\n                random_state=self.test_dataset['split_seed'],\n                stratify=y\n            )\n\n        return X, y"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns estimator from base learner origin", "response": "def return_estimator(self):\n        \"\"\"Returns estimator from base learner origin\n\n        Returns:\n            est (estimator): Estimator object\n        \"\"\"\n        extraction_code = self.source\n        estimator = functions.import_object_from_string_code(extraction_code, \"base_learner\")\n\n        return estimator"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef export_as_file(self, filepath, hyperparameters):\n        if not filepath.endswith('.py'):\n            filepath += '.py'\n\n        file_contents = ''\n        file_contents += self.source\n        file_contents += '\\n\\nbase_learner.set_params(**{})\\n'.format(hyperparameters)\n        file_contents += '\\nmeta_feature_generator = \"{}\"\\n'.format(self.meta_feature_generator)\n        with open(filepath, 'wb') as f:\n            f.write(file_contents.encode('utf8'))", "response": "This function generates a Python file with the importable base learner set to hyperparameters"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef return_estimator(self):\n        estimator = self.base_learner_origin.return_estimator()\n        estimator = estimator.set_params(**self.hyperparameters)\n        return estimator", "response": "Returns a base learner using its origin and the given hyperparameters\n           "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef meta_features_path(self, path):\n        return os.path.join(\n                path,\n                app.config['XCESSIV_META_FEATURES_FOLDER'],\n                str(self.id)\n            ) + '.npy'", "response": "Returns absolute path for meta - features of the current class"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef delete_meta_features(self, path):\n        if os.path.exists(self.meta_features_path(path)):\n            os.remove(self.meta_features_path(path))", "response": "Deletes meta - features of base learner if it exists."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a secondary learner using its origin and the given hyperparameters", "response": "def return_secondary_learner(self):\n        \"\"\"Returns secondary learner using its origin and the given hyperparameters\n\n        Returns:\n            est (estimator): Estimator object\n        \"\"\"\n        estimator = self.base_learner_origin.return_estimator()\n        estimator = estimator.set_params(**self.secondary_learner_hyperparameters)\n        return estimator"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef export_as_code(self, cv_source):\n\n        rand_value = ''.join(random.choice(string.ascii_uppercase + string.digits)\n                             for _ in range(25))\n\n        base_learner_code = ''\n        base_learner_code += 'base_learner_list_{} = []\\n'.format(rand_value)\n        base_learner_code += 'meta_feature_generators_list_{} = []\\n\\n'.format(rand_value)\n        for idx, base_learner in enumerate(self.base_learners):\n            base_learner_code += '################################################\\n'\n            base_learner_code += '###### Code for building base learner {} ########\\n'.format(idx+1)\n            base_learner_code += '################################################\\n'\n            base_learner_code += base_learner.base_learner_origin.source\n            base_learner_code += '\\n\\n'\n            base_learner_code += 'base_learner' \\\n                                 '.set_params(**{})\\n'.format(base_learner.hyperparameters)\n            base_learner_code += 'base_learner_list_{}.append(base_learner)\\n'.format(rand_value)\n            base_learner_code += 'meta_feature_generators_list_{}.append(\"{}\")\\n'.format(\n                rand_value,\n                base_learner.base_learner_origin.meta_feature_generator\n            )\n            base_learner_code += '\\n\\n'\n\n        base_learner_code += '################################################\\n'\n        base_learner_code += '##### Code for building secondary learner ######\\n'\n        base_learner_code += '################################################\\n'\n        base_learner_code += self.base_learner_origin.source\n        base_learner_code += '\\n\\n'\n        base_learner_code += 'base_learner' \\\n                             '.set_params(**{})\\n'.format(self.secondary_learner_hyperparameters)\n        base_learner_code += 'secondary_learner_{} = base_learner\\n'.format(rand_value)\n        base_learner_code += '\\n\\n'\n\n        base_learner_code += '################################################\\n'\n        base_learner_code += '############## Code for CV method ##############\\n'\n        base_learner_code += '################################################\\n'\n        base_learner_code += cv_source\n        base_learner_code += '\\n\\n'\n\n        base_learner_code += '################################################\\n'\n        base_learner_code += '######## Code for Xcessiv stacker class ########\\n'\n        base_learner_code += '################################################\\n'\n        stacker_file_loc = os.path.join(os.path.abspath(os.path.dirname(__file__)), 'stacker.py')\n        with open(stacker_file_loc) as f2:\n            base_learner_code += f2.read()\n\n        base_learner_code += '\\n\\n' \\\n                             '    def {}(self, X):\\n' \\\n                             '        return self._process_using_' \\\n                             'meta_feature_generator(X, \"{}\")\\n\\n'\\\n            .format(self.base_learner_origin.meta_feature_generator,\n                    self.base_learner_origin.meta_feature_generator)\n\n        base_learner_code += '\\n\\n'\n\n        base_learner_code += 'base_learner = XcessivStackedEnsemble' \\\n                             '(base_learners=base_learner_list_{},' \\\n                             ' meta_feature_generators=meta_feature_generators_list_{},' \\\n                             ' secondary_learner=secondary_learner_{},' \\\n                             ' cv_function=return_splits_iterable)\\n'.format(\n            rand_value,\n            rand_value,\n            rand_value\n        )\n\n        return base_learner_code", "response": "Returns a string that contains the Python code for the ensemble base - learner - related data."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nexport the ensemble as a single Python file and saves it to file_path.", "response": "def export_as_file(self, file_path, cv_source):\n        \"\"\"Export the ensemble as a single Python file and saves it to `file_path`.\n\n        This is EXPERIMENTAL as putting different modules together would probably wreak havoc\n        especially on modules that make heavy use of global variables.\n\n        Args:\n            file_path (str, unicode): Absolute/local path of place to save file in\n\n            cv_source (str, unicode): String containing actual code for base learner\n                cross-validation used to generate secondary meta-features.\n        \"\"\"\n        if os.path.exists(file_path):\n            raise exceptions.UserError('{} already exists'.format(file_path))\n\n        with open(file_path, 'wb') as f:\n            f.write(self.export_as_code(cv_source).encode('utf8'))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef export_as_package(self, package_path, cv_source):\n        if os.path.exists(package_path):\n            raise exceptions.UserError('{} already exists'.format(package_path))\n\n        package_name = os.path.basename(os.path.normpath(package_path))\n\n        os.makedirs(package_path)\n\n        # Write __init__.py\n        with open(os.path.join(package_path, '__init__.py'), 'wb') as f:\n            f.write('from {}.builder import xcessiv_ensemble'.format(package_name).encode('utf8'))\n\n        # Create package baselearners with each base learner having its own module\n        os.makedirs(os.path.join(package_path, 'baselearners'))\n        open(os.path.join(package_path, 'baselearners', '__init__.py'), 'a').close()\n        for idx, base_learner in enumerate(self.base_learners):\n            base_learner.export_as_file(os.path.join(package_path,\n                                                     'baselearners',\n                                                     'baselearner' + str(idx)))\n\n        # Create metalearner.py containing secondary learner\n        self.base_learner_origin.export_as_file(\n            os.path.join(package_path, 'metalearner'),\n            self.secondary_learner_hyperparameters\n        )\n\n        # Create cv.py containing CV method for getting meta-features\n        with open(os.path.join(package_path, 'cv.py'), 'wb') as f:\n            f.write(cv_source.encode('utf8'))\n\n        # Create stacker.py containing class for Xcessiv ensemble\n        ensemble_source = ''\n        stacker_file_loc = os.path.join(os.path.abspath(os.path.dirname(__file__)), 'stacker.py')\n        with open(stacker_file_loc) as f:\n            ensemble_source += f.read()\n\n        ensemble_source += '\\n\\n' \\\n                           '    def {}(self, X):\\n' \\\n                           '        return self._process_using_' \\\n                           'meta_feature_generator(X, \"{}\")\\n\\n'\\\n            .format(self.base_learner_origin.meta_feature_generator,\n                    self.base_learner_origin.meta_feature_generator)\n\n        with open(os.path.join(package_path, 'stacker.py'), 'wb') as f:\n            f.write(ensemble_source.encode('utf8'))\n\n        # Create builder.py containing file where `xcessiv_ensemble` is instantiated for import\n        builder_source = ''\n\n        for idx, base_learner in enumerate(self.base_learners):\n            builder_source += 'from {}.baselearners import baselearner{}\\n'.format(package_name, idx)\n\n        builder_source += 'from {}.cv import return_splits_iterable\\n'.format(package_name)\n\n        builder_source += 'from {} import metalearner\\n'.format(package_name)\n\n        builder_source += 'from {}.stacker import XcessivStackedEnsemble\\n'.format(package_name)\n\n        builder_source += '\\nbase_learners = [\\n'\n        for idx, base_learner in enumerate(self.base_learners):\n            builder_source += '    baselearner{}.base_learner,\\n'.format(idx)\n        builder_source += ']\\n'\n\n        builder_source += '\\nmeta_feature_generators = [\\n'\n        for idx, base_learner in enumerate(self.base_learners):\n            builder_source += '    baselearner{}.meta_feature_generator,\\n'.format(idx)\n        builder_source += ']\\n'\n\n        builder_source += '\\nxcessiv_ensemble = XcessivStackedEnsemble(base_learners=base_learners,' \\\n                          ' meta_feature_generators=meta_feature_generators,' \\\n                          ' secondary_learner=metalearner.base_learner,' \\\n                          ' cv_function=return_splits_iterable)\\n'\n\n        with open(os.path.join(package_path, 'builder.py'), 'wb') as f:\n            f.write(builder_source.encode('utf8'))", "response": "Exports the ensemble as a Python package and saves it to package_path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef verify_full_extraction():\n    path = functions.get_path_from_query_string(request)\n\n    if request.method == 'POST':\n        rqtasks.extraction_data_statistics(path)\n\n    with functions.DBContextManager(path) as session:\n        extraction = session.query(models.Extraction).first()\n        return jsonify(extraction.data_statistics)", "response": "This endpoint is used to simultaneously verify data statistics and extraction for training test and holdout datasets."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_base_learner(id):\n    path = functions.get_path_from_query_string(request)\n\n    with functions.DBContextManager(path) as session:\n        base_learner_origin = session.query(models.BaseLearnerOrigin).filter_by(id=id).first()\n        if base_learner_origin is None:\n            raise exceptions.UserError('Base learner origin {} not found'.format(id), 404)\n\n        if not base_learner_origin.final:\n            raise exceptions.UserError('Base learner origin {} is not final'.format(id))\n\n        req_body = request.get_json()\n\n        # Retrieve full hyperparameters\n        est = base_learner_origin.return_estimator()\n        hyperparameters = functions.import_object_from_string_code(req_body['source'],\n                                                                   'params')\n        est.set_params(**hyperparameters)\n        hyperparameters = functions.make_serializable(est.get_params())\n\n        base_learners = session.query(models.BaseLearner).\\\n            filter_by(base_learner_origin_id=id,\n                      hyperparameters=hyperparameters).all()\n        if base_learners:\n            raise exceptions.UserError('Base learner exists with given hyperparameters')\n\n        base_learner = models.BaseLearner(hyperparameters,\n                                          'queued',\n                                          base_learner_origin)\n\n        if 'single_searches' not in base_learner_origin.description:\n            base_learner_origin.description['single_searches'] = []\n        base_learner_origin.description['single_searches'] += ([req_body['source']])\n\n        session.add(base_learner)\n        session.add(base_learner_origin)\n        session.commit()\n\n        with Connection(get_redis_connection()):\n            rqtasks.generate_meta_features.delay(path, base_learner.id)\n\n        return jsonify(base_learner.serialize)", "response": "This function creates a single base learner from a base learner origin and queues it up"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a set of base learners from base learner origin using random search and queues them up", "response": "def search_base_learner(id):\n    \"\"\"Creates a set of base learners from base learner origin using grid search\n    and queues them up\n    \"\"\"\n    path = functions.get_path_from_query_string(request)\n    req_body = request.get_json()\n    if req_body['method'] == 'grid':\n        param_grid = functions.import_object_from_string_code(\n            req_body['source'],\n            'param_grid'\n        )\n        iterator = ParameterGrid(param_grid)\n    elif req_body['method'] == 'random':\n        param_distributions = functions.import_object_from_string_code(\n            req_body['source'],\n            'param_distributions'\n        )\n        iterator = ParameterSampler(param_distributions, n_iter=req_body['n_iter'])\n\n    else:\n        raise exceptions.UserError('{} not a valid search method'.format(req_body['method']))\n\n    with functions.DBContextManager(path) as session:\n        base_learner_origin = session.query(models.BaseLearnerOrigin).filter_by(id=id).first()\n        if base_learner_origin is None:\n            raise exceptions.UserError('Base learner origin {} not found'.format(id), 404)\n\n        if not base_learner_origin.final:\n            raise exceptions.UserError('Base learner origin {} is not final'.format(id))\n\n        learners = []\n        for params in iterator:\n            est = base_learner_origin.return_estimator()\n            try:\n                est.set_params(**params)\n            except Exception as e:\n                print(repr(e))\n                continue\n\n            hyperparameters = functions.make_serializable(est.get_params())\n\n            base_learners = session.query(models.BaseLearner).\\\n                filter_by(base_learner_origin_id=id,\n                          hyperparameters=hyperparameters).all()\n            if base_learners:  # already exists\n                continue\n\n            base_learner = models.BaseLearner(hyperparameters,\n                                              'queued',\n                                              base_learner_origin)\n\n            session.add(base_learner)\n            session.commit()\n            with Connection(get_redis_connection()):\n                rqtasks.generate_meta_features.delay(path, base_learner.id)\n            learners.append(base_learner)\n\n        if not learners:\n            raise exceptions.UserError('Created 0 new base learners')\n\n        if req_body['method'] == 'grid':\n            if 'grid_searches' not in base_learner_origin.description:\n                base_learner_origin.description['grid_searches'] = []\n            base_learner_origin.description['grid_searches'] += ([req_body['source']])\n        elif req_body['method'] == 'random':\n            if 'random_searches' not in base_learner_origin.description:\n                base_learner_origin.description['random_searches'] = []\n            base_learner_origin.description['random_searches'] += ([req_body['source']])\n\n        session.add(base_learner_origin)\n        session.commit()\n\n        return jsonify(list(map(lambda x: x.serialize, learners)))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn all automated runs in the database", "response": "def get_automated_runs():\n    \"\"\"Return all automated runs\"\"\"\n    path = functions.get_path_from_query_string(request)\n\n    if request.method == 'GET':\n        with functions.DBContextManager(path) as session:\n            automated_runs = session.query(models.AutomatedRun).all()\n            return jsonify(list(map(lambda x: x.serialize, automated_runs)))\n\n    if request.method == 'POST':\n        req_body = request.get_json()\n        with functions.DBContextManager(path) as session:\n            base_learner_origin = None\n\n            if req_body['category'] == 'bayes' or req_body['category'] == 'greedy_ensemble_search':\n                base_learner_origin = session.query(models.BaseLearnerOrigin).\\\n                    filter_by(id=req_body['base_learner_origin_id']).first()\n                if base_learner_origin is None:\n                    raise exceptions.UserError('Base learner origin {} not found'.format(\n                        req_body['base_learner_origin_id']\n                    ), 404)\n                if not base_learner_origin.final:\n                    raise exceptions.UserError('Base learner origin {} is not final'.format(\n                        req_body['base_learner_origin_id']\n                    ))\n\n            elif req_body['category'] == 'tpot':\n                pass\n\n            else:\n                raise exceptions.UserError('Automated run category'\n                                           ' {} not recognized'.format(req_body['category']))\n\n            # Check for any syntax errors\n            module = functions.import_string_code_as_module(req_body['source'])\n            del module\n\n            automated_run = models.AutomatedRun(req_body['source'],\n                                                'queued',\n                                                req_body['category'],\n                                                base_learner_origin)\n\n            session.add(automated_run)\n            session.commit()\n\n            with Connection(get_redis_connection()):\n                rqtasks.start_automated_run.delay(path, automated_run.id)\n\n            return jsonify(automated_run.serialize)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _process_using_meta_feature_generator(self, X, meta_feature_generator):\n\n        all_learner_meta_features = []\n        for idx, base_learner in enumerate(self.base_learners):\n            single_learner_meta_features = getattr(base_learner,\n                                                   self.meta_feature_generators[idx])(X)\n\n            if len(single_learner_meta_features.shape) == 1:\n                single_learner_meta_features = single_learner_meta_features.reshape(-1, 1)\n            all_learner_meta_features.append(single_learner_meta_features)\n\n        all_learner_meta_features = np.concatenate(all_learner_meta_features, axis=1)\n\n        out = getattr(self.secondary_learner, meta_feature_generator)(all_learner_meta_features)\n\n        return out", "response": "Process using meta - feature generator"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nbuilding the data structure for a new event.", "response": "def NewEvent(\n    type: str, id: UUID = None, data: JsonDict = None, metadata: JsonDict = None\n) -> NewEventData:\n    \"\"\"Build the data structure for a new event.\n\n    Args:\n        type: An event type.\n        id: The uuid identifier for the event.\n        data: A dict containing data for the event. These data\n            must be json serializable.\n        metadata: A dict containing metadata about the event.\n            These must be json serializable.\n    \"\"\"\n\n    return NewEventData(id or uuid4(), type, data, metadata)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a new instance of the class from the bytes.", "response": "def from_bytes(cls, data):\n        \"\"\"\n        I am so sorry.\n        \"\"\"\n        len_username = int.from_bytes(data[0:2], byteorder=\"big\")\n        offset_username = 2 + len_username\n        username = data[2:offset_username].decode(\"UTF-8\")\n        offset_password = 2 + offset_username\n        len_password = int.from_bytes(\n            data[offset_username:offset_password], byteorder=\"big\"\n        )\n        pass_begin = offset_password\n        pass_end = offset_password + len_password\n        password = data[pass_begin:pass_end].decode(\"UTF-8\")\n\n        return cls(username, password)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconnect to a specific node.", "response": "def connect(\n    host=\"localhost\",\n    port=1113,\n    discovery_host=None,\n    discovery_port=2113,\n    username=None,\n    password=None,\n    loop=None,\n    name=None,\n    selector=select_random,\n) -> Client:\n    \"\"\" Create a new client.\n\n        Examples:\n            Since the Client is an async context manager, we can use it in a\n            with block for automatic connect/disconnect semantics.\n\n            >>> async with connect(host='127.0.0.1', port=1113) as c:\n            >>>     await c.ping()\n\n            Or we can call connect at a more convenient moment\n\n            >>> c = connect()\n            >>> await c.connect()\n            >>> await c.ping()\n            >>> await c.close()\n\n            For cluster discovery cases, we can provide a discovery host and\n            port. The host may be an IP or DNS entry. If you provide a DNS\n            entry, discovery will choose randomly from the registered IP\n            addresses for the hostname.\n\n            >>> async with connect(discovery_host=\"eventstore.test\") as c:\n            >>>     await c.ping()\n\n            The discovery host returns gossip data about the cluster. We use the\n            gossip to select a node at random from the avaialble cluster members.\n\n            If you're using\n            :meth:`persistent subscriptions <photonpump.connection.Client.create_subscription>`\n            you will always want to connect to the master node of the cluster.\n            The selector parameter is a function that chooses an available node from\n            the gossip result. To select the master node, use the\n            :func:`photonpump.discovery.prefer_master` function. This function will return\n            the master node if there is a live master, and a random replica otherwise.\n            All requests to the server can be made with the require_master flag which\n            will raise an error if the current node is not a master.\n\n            >>> async with connect(\n            >>>     discovery_host=\"eventstore.test\",\n            >>>     selector=discovery.prefer_master,\n            >>> ) as c:\n            >>>     await c.ping(require_master=True)\n\n            Conversely, you might want to avoid connecting to the master node for reasons\n            of scalability. For this you can use the\n            :func:`photonpump.discovery.prefer_replica` function.\n\n            >>> async with connect(\n            >>>     discovery_host=\"eventstore.test\",\n            >>>     selector=discovery.prefer_replica,\n            >>> ) as c:\n            >>>     await c.ping()\n\n\n            For some operations, you may need to authenticate your requests by\n            providing a username and password to the client.\n\n            >>> async with connect(username='admin', password='changeit') as c:\n            >>>     await c.ping()\n\n            Ordinarily you will create a single Client per application, but for\n            advanced scenarios you might want multiple connections. In this\n            situation, you can name each connection in order to get better logging.\n\n            >>> async with connect(name=\"event-reader\"):\n            >>>     await c.ping()\n\n            >>> async with connect(name=\"event-writer\"):\n            >>>     await c.ping()\n\n\n        Args:\n            host: The IP or DNS entry to connect with, defaults to 'localhost'.\n            port: The port to connect with, defaults to 1113.\n            discovery_host: The IP or DNS entry to use for cluster discovery.\n            discovery_port: The port to use for cluster discovery, defaults to 2113.\n            username: The username to use when communicating with eventstore.\n            password: The password to use when communicating with eventstore.\n            loop:An Asyncio event loop.\n            selector: An optional function that selects one element from a list of\n                :class:`photonpump.disovery.DiscoveredNode` elements.\n\n    \"\"\"\n    discovery = get_discoverer(host, port, discovery_host, discovery_port, selector)\n    dispatcher = MessageDispatcher(name=name, loop=loop)\n    connector = Connector(discovery, dispatcher, name=name)\n\n    credential = msg.Credential(username, password) if username and password else None\n\n    return Client(connector, dispatcher, credential=credential)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nloop forever reading messages and invoking them", "response": "async def start(self):\n        \"\"\"Loop forever reading messages and invoking\n           the operation that caused them\"\"\"\n\n        while True:\n            try:\n                data = await self.reader.read(8192)\n\n                if self._trace_enabled:\n                    self._logger.trace(\n                        \"Received %d bytes from remote server:\\n%s\",\n                        len(data),\n                        msg.dump(data),\n                    )\n                await self.process(data)\n            except asyncio.CancelledError:\n                return\n            except:\n                logging.exception(\"Unhandled error in Message Reader\")\n                raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsends a PING message to the remote server to check liveness.", "response": "async def ping(self, conversation_id: uuid.UUID = None) -> float:\n        \"\"\"\n        Send a message to the remote server to check liveness.\n\n        Returns:\n            The round-trip time to receive a Pong message in fractional seconds\n\n        Examples:\n\n            >>> async with connect() as conn:\n            >>>     print(\"Sending a PING to the server\")\n            >>>     time_secs = await conn.ping()\n            >>>     print(\"Received a PONG after {} secs\".format(time_secs))\n\n        \"\"\"\n        cmd = convo.Ping(conversation_id=conversation_id or uuid.uuid4())\n        result = await self.dispatcher.start_conversation(cmd)\n\n        return await result"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def publish_event(\n        self,\n        stream: str,\n        type: str,\n        body: Optional[Any] = None,\n        id: Optional[uuid.UUID] = None,\n        metadata: Optional[Any] = None,\n        expected_version: int = -2,\n        require_master: bool = False,\n    ) -> None:\n        \"\"\"\n        Publish a single event to the EventStore.\n\n        This method publishes a single event to the remote server and waits\n\n        for acknowledgement.\n\n        Args:\n            stream: The stream to publish the event to.\n            type: the event's type.\n            body: a serializable body for the event.\n            id: a unique id for the event. PhotonPump will automatically generate an\n                id if none is provided.\n            metadata: Optional serializable metadata block for the event.\n            expected_version: Used for concurrency control.\n                If a positive integer is provided, EventStore will check that the stream\n                is at that version before accepting a write.\n\n                There are three magic values:\n                    -4: StreamMustExist. Checks that the stream already exists.\n                    -2: Any. Disables concurrency checks\n                    -1: NoStream. Checks that the stream does not yet exist.\n                    0: EmptyStream. Checks that the stream has been explicitly created but\n                    does not yet contain any events.\n            require_master: If true, slave nodes will reject this message.\n\n        Examples:\n\n            >>> async with connect() as conn:\n            >>>     await conn.publish_event(\n            >>>         \"inventory_item-1\",\n            >>>         \"item_created\",\n            >>>         body={ \"item-id\": 1, \"created-date\": \"2018-08-19\" },\n            >>>         expected_version=ExpectedVersion.StreamMustNotExist\n            >>>     )\n            >>>\n            >>>     await conn.publish_event(\n            >>>         \"inventory_item-1\",\n            >>>         \"item_deleted\",\n            >>>         expected_version=1,\n            >>>         metadata={'deleted-by': 'bob' }\n            >>>     )\n        \"\"\"\n        event = msg.NewEvent(type, id or uuid.uuid4(), body, metadata)\n        conversation = convo.WriteEvents(\n            stream,\n            [event],\n            expected_version=expected_version,\n            require_master=require_master,\n        )\n        result = await self.dispatcher.start_conversation(conversation)\n\n        return await result", "response": "Publishes a single event to the EventStore."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting a single event by stream and event number.", "response": "async def get_event(\n        self,\n        stream: str,\n        event_number: int,\n        resolve_links=True,\n        require_master=False,\n        correlation_id: uuid.UUID = None,\n    ) -> msg.Event:\n        \"\"\"\n        Get a single event by stream and event number.\n\n        Args:\n            stream: The name of the stream containing the event.\n            event_number: The sequence number of the event to read.\n            resolve_links (optional): True if eventstore should\n                automatically resolve Link Events, otherwise False.\n            required_master (optional): True if this command must be\n                sent direct to the master node, otherwise False.\n            correlation_id (optional): A unique identifer for this\n                command.\n\n        Returns:\n            The resolved event if found, else None.\n\n        Examples:\n\n            >>> async with connection() as conn:\n            >>>     await conn.publish(\"inventory_item-1\", \"item_created\")\n            >>>     event = await conn.get_event(\"inventory_item-1\", 1)\n            >>>     print(event)\n        \"\"\"\n        correlation_id = correlation_id or uuid.uuid4()\n        cmd = convo.ReadEvent(\n            stream,\n            event_number,\n            resolve_links,\n            require_master,\n            conversation_id=correlation_id,\n        )\n\n        result = await self.dispatcher.start_conversation(cmd)\n\n        return await result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def get(\n        self,\n        stream: str,\n        direction: msg.StreamDirection = msg.StreamDirection.Forward,\n        from_event: int = 0,\n        max_count: int = 100,\n        resolve_links: bool = True,\n        require_master: bool = False,\n        correlation_id: uuid.UUID = None,\n    ):\n        \"\"\"\n        Read a range of events from a stream.\n\n        Args:\n            stream: The name of the stream to read\n            direction (optional): Controls whether to read events forward or backward.\n              defaults to Forward.\n            from_event (optional): The first event to read.\n              defaults to the beginning of the stream when direction is forward\n              and the end of the stream if direction is backward.\n            max_count (optional): The maximum number of events to return.\n            resolve_links (optional): True if eventstore should\n                automatically resolve Link Events, otherwise False.\n            required_master (optional): True if this command must be\n                sent direct to the master node, otherwise False.\n            correlation_id (optional): A unique identifer for this command.\n\n        Examples:\n\n            Read 5 events from a stream\n\n            >>> async for event in conn.get(\"my-stream\", max_count=5):\n            >>>     print(event)\n\n\n            Read events 21 to 30\n\n            >>> async for event in conn.get(\"my-stream\", max_count=10, from_event=21):\n            >>>     print(event)\n\n            Read 10 most recent events in reverse order\n\n            >>> async for event in conn.get(\n                        \"my-stream\",\n                        max_count=10,\n                        direction=StreamDirection.Backward\n                    ):\n            >>>     print(event)\n        \"\"\"\n        correlation_id = correlation_id\n        cmd = convo.ReadStreamEvents(\n            stream,\n            from_event,\n            max_count,\n            resolve_links,\n            require_master,\n            direction=direction,\n        )\n        result = await self.dispatcher.start_conversation(cmd)\n\n        return await result", "response": "Read a range of events from a stream."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def get_all(\n        self,\n        direction: msg.StreamDirection = msg.StreamDirection.Forward,\n        from_position: Optional[Union[msg.Position, msg._PositionSentinel]] = None,\n        max_count: int = 100,\n        resolve_links: bool = True,\n        require_master: bool = False,\n        correlation_id: uuid.UUID = None,\n    ):\n        \"\"\"\n        Read a range of events from the whole database.\n\n        Args:\n            direction (optional): Controls whether to read events forward or backward.\n              defaults to Forward.\n            from_position (optional): The position to read from.\n              defaults to the beginning of the stream when direction is forward\n              and the end of the stream if direction is backward.\n            max_count (optional): The maximum number of events to return.\n            resolve_links (optional): True if eventstore should\n                automatically resolve Link Events, otherwise False.\n            required_master (optional): True if this command must be\n                sent direct to the master node, otherwise False.\n            correlation_id (optional): A unique identifer for this command.\n\n        Examples:\n\n            Read 5 events\n\n            >>> async for event in conn.get_all(max_count=5):\n            >>>     print(event)\n\n\n            Read 10 most recent events in reverse order\n\n            >>> async for event in conn.get_all(\n                        max_count=10,\n                        direction=StreamDirection.Backward\n                    ):\n            >>>     print(event)\n        \"\"\"\n        correlation_id = correlation_id\n        cmd = convo.ReadAllEvents(\n            msg.Position.for_direction(direction, from_position),\n            max_count,\n            resolve_links,\n            require_master,\n            direction=direction,\n            credentials=self.credential,\n        )\n        result = await self.dispatcher.start_conversation(cmd)\n\n        return await result", "response": "Read a range of events from the whole eventstore."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads through a stream of events and returns an iterator of the events.", "response": "async def iter(\n        self,\n        stream: str,\n        direction: msg.StreamDirection = msg.StreamDirection.Forward,\n        from_event: int = None,\n        batch_size: int = 100,\n        resolve_links: bool = True,\n        require_master: bool = False,\n        correlation_id: uuid.UUID = None,\n    ):\n        \"\"\"\n        Read through a stream of events until the end and then stop.\n\n        Args:\n            stream: The name of the stream to read.\n            direction: Controls whether to read forward or backward through the\n              stream. Defaults to  StreamDirection.Forward\n            from_event: The sequence number of the first event to read from the\n              stream. Reads from the appropriate end of the stream if unset.\n            batch_size: The maximum number of events to read at a time.\n            resolve_links (optional): True if eventstore should\n                automatically resolve Link Events, otherwise False.\n            required_master (optional): True if this command must be\n                sent direct to the master node, otherwise False.\n            correlation_id (optional): A unique identifer for this\n                command.\n\n        Examples:\n\n            Print every event from the stream \"my-stream\".\n\n            >>> with async.connect() as conn:\n            >>>     async for event in conn.iter(\"my-stream\"):\n            >>>         print(event)\n\n            Print every event from the stream \"my-stream\" in reverse order\n\n            >>> with async.connect() as conn:\n            >>>     async for event in conn.iter(\"my-stream\", direction=StreamDirection.Backward):\n            >>>         print(event)\n\n            Skip the first 10 events of the stream\n\n            >>> with async.connect() as conn:\n            >>>     async for event in conn.iter(\"my-stream\", from_event=11):\n            >>>         print(event)\n        \"\"\"\n\n        correlation_id = correlation_id or uuid.uuid4()\n        cmd = convo.IterStreamEvents(\n            stream,\n            from_event,\n            batch_size,\n            resolve_links,\n            direction=direction,\n            credentials=self.credential,\n        )\n        result = await self.dispatcher.start_conversation(cmd)\n        iterator = await result\n        async for event in iterator:\n            yield event"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def iter_all(\n        self,\n        direction: msg.StreamDirection = msg.StreamDirection.Forward,\n        from_position: Optional[Union[msg.Position, msg._PositionSentinel]] = None,\n        batch_size: int = 100,\n        resolve_links: bool = True,\n        require_master: bool = False,\n        correlation_id: Optional[uuid.UUID] = None,\n    ):\n        \"\"\"\n        Read through all the events in the database.\n\n        Args:\n            direction (optional): Controls whether to read forward or backward\n              through the events. Defaults to  StreamDirection.Forward\n            from_position (optional): The position to start reading from.\n              Defaults to photonpump.Beginning when direction is Forward,\n              photonpump.End when direction is Backward.\n            batch_size (optional): The maximum number of events to read at a time.\n            resolve_links (optional): True if eventstore should\n                automatically resolve Link Events, otherwise False.\n            required_master (optional): True if this command must be\n                sent direct to the master node, otherwise False.\n            correlation_id (optional): A unique identifer for this\n                command.\n\n        Examples:\n\n            Print every event from the database.\n\n            >>> with async.connect() as conn:\n            >>>     async for event in conn.iter_all()\n            >>>         print(event)\n\n            Print every event from the database in reverse order\n\n            >>> with async.connect() as conn:\n            >>>     async for event in conn.iter_all(direction=StreamDirection.Backward):\n            >>>         print(event)\n\n            Start reading from a known commit position\n\n            >>> with async.connect() as conn:\n            >>>     async for event in conn.iter_all(from_position=Position(12345))\n            >>>         print(event)\n\n        \"\"\"\n        correlation_id = correlation_id\n\n        cmd = convo.IterAllEvents(\n            msg.Position.for_direction(direction, from_position),\n            batch_size,\n            resolve_links,\n            require_master,\n            direction,\n            self.credential,\n            correlation_id,\n        )\n        result = await self.dispatcher.start_conversation(cmd)\n        iterator = await result\n        async for event in iterator:\n            yield event", "response": "Reads through all the events in the eventstore."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsubscribes to receive new events from a stream.", "response": "async def subscribe_to(\n        self, stream, start_from=-1, resolve_link_tos=True, batch_size: int = 100\n    ):\n        \"\"\"\n        Subscribe to receive notifications when a new event is published\n        to a stream.\n\n        Args:\n            stream: The name of the stream.\n            start_from (optional): The first event to read.\n                This parameter defaults to the magic value -1 which is treated\n                as meaning \"from the end of the stream\". IF this value is used,\n                no historical events will be returned.\n\n                For any other value, photonpump will read all events from\n                start_from until the end of the stream in pages of max_size\n                before subscribing to receive new events as they arrive.\n\n            resolve_links (optional): True if eventstore should\n                automatically resolve Link Events, otherwise False.\n            required_master (optional): True if this command must be\n                sent direct to the master node, otherwise False.\n            correlation_id (optional): A unique identifer for this\n                command.\n            batch_size (optioal): The number of events to pull down from\n                eventstore in one go.\n\n        Returns:\n            A VolatileSubscription.\n\n        Examples:\n\n            >>> async with connection() as conn:\n            >>>     # Subscribe only to NEW events on the cpu-metrics stream\n            >>>     subs = await conn.subscribe_to(\"price-changes\")\n            >>>     async for event in subs.events:\n            >>>         print(event)\n\n            >>> async with connection() as conn:\n            >>>     # Read all historical events and then receive updates as they\n            >>>     # arrive.\n            >>>     subs = await conn.subscribe_to(\"price-changes\", start_from=0)\n            >>>     async for event in subs.events:\n            >>>         print(event)\n        \"\"\"\n\n        if start_from == -1:\n            cmd: convo.Conversation = convo.SubscribeToStream(\n                stream, resolve_link_tos, credentials=self.credential\n            )\n        else:\n            cmd = convo.CatchupSubscription(\n                stream, start_from, batch_size, credential=self.credential\n            )\n\n        future = await self.dispatcher.start_conversation(cmd)\n\n        return await future"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef prefer_master(nodes: List[DiscoveredNode]) -> Optional[DiscoveredNode]:\n    return max(nodes, key=attrgetter(\"state\"))", "response": "Select the master if available otherwise fall back to a replica."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nselect a random replica if any are available or fall back to the master.", "response": "def prefer_replica(nodes: List[DiscoveredNode]) -> Optional[DiscoveredNode]:\n    \"\"\"\n    Select a random replica if any are available or fall back to the master.\n    \"\"\"\n    masters = [node for node in nodes if node.state == NodeState.Master]\n    replicas = [node for node in nodes if node.state != NodeState.Master]\n\n    if replicas:\n        return random.choice(replicas)\n    else:\n        # if you have more than one master then you're on your own, bud.\n\n        return masters[0]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a handler for an event type and return a serializable object with target name", "response": "def create_event_handler(event_type, handler):\n    \"\"\"Register a comm and return a serializable object with target name\"\"\"\n\n    target_name = '{hash}_{event_type}'.format(hash=hash(handler), event_type=event_type)\n\n    def handle_comm_opened(comm, msg):\n        @comm.on_msg\n        def _handle_msg(msg):\n            data = msg['content']['data']\n            event = json.loads(data)\n            return_value = handler(event)\n            if return_value:\n                comm.send(return_value)\n\n        comm.send('Comm target \"{target_name}\" registered by vdom'.format(target_name=target_name))\n\n    # Register a new comm for this event handler\n    if get_ipython():\n        get_ipython().kernel.comm_manager.register_target(target_name, handle_comm_opened)\n\n    # Return a serialized object\n    return target_name"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert an element to JSON", "response": "def to_json(el, schema=None):\n    \"\"\"Convert an element to VDOM JSON\n\n    If you wish to validate the JSON, pass in a schema via the schema keyword\n    argument. If a schema is provided, this raises a ValidationError if JSON\n    does not match the schema.\n    \"\"\"\n    if type(el) is str:\n        json_el = el\n    elif type(el) is list:\n        json_el = list(map(to_json, el))\n    elif type(el) is dict:\n        assert 'tagName' in el\n        json_el = el.copy()\n        if 'attributes' not in el:\n            json_el['attributes'] = {}\n        if 'children' not in el:\n            json_el['children'] = []\n    elif isinstance(el, VDOM):\n        json_el = el.to_dict()\n    else:\n        json_el = el\n\n    if schema:\n        try:\n            validate(instance=json_el, schema=schema, cls=Draft4Validator)\n        except ValidationError as e:\n            raise ValidationError(_validate_err_template.format(schema, e))\n\n    return json_el"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_component(tag_name, allow_children=True):\n\n    def _component(*children, **kwargs):\n        if 'children' in kwargs:\n            children = kwargs.pop('children')\n        else:\n            # Flatten children under specific circumstances\n            # This supports the use case of div([a, b, c])\n            # And allows users to skip the * operator\n            if len(children) == 1 and isinstance(children[0], list):\n                # We want children to be tuples and not lists, so\n                # they can be immutable\n                children = tuple(children[0])\n        style = None\n        event_handlers = None\n        attributes = dict(**kwargs)\n        if 'style' in kwargs:\n            style = kwargs.pop('style')\n        if 'attributes' in kwargs:\n            attributes = kwargs['attributes']\n        for key, value in attributes.items():\n            if callable(value):\n                attributes = attributes.copy()\n                if event_handlers == None:\n                    event_handlers = {key: attributes.pop(key)}\n                else:\n                    event_handlers[key] = attributes.pop(key)\n        if not allow_children and children:\n            # We don't allow children, but some were passed in\n            raise ValueError('<{tag_name} /> cannot have children'.format(tag_name=tag_name))\n\n        v = VDOM(tag_name, attributes, style, children, None, event_handlers)\n        return v\n\n    return _component", "response": "Create a component for an HTML Tag\n           "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef h(tagName, *children, **kwargs):\n    attrs = {}\n    if 'attrs' in kwargs:\n        attrs = kwargs.pop('attrs')\n\n    attrs = attrs.copy()\n    attrs.update(kwargs)\n\n    el = createComponent(tagName)\n    return el(children, **attrs)", "response": "Takes an HTML Tag children string or array or another element and attributes and returns a new element with the same attributes."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nvalidates VDOM against given JSON Schema Raises ValidationError if schema does not match", "response": "def validate(self, schema):\n        \"\"\"\n        Validate VDOM against given JSON Schema\n\n        Raises ValidationError if schema does not match\n        \"\"\"\n        try:\n            validate(instance=self.to_dict(), schema=schema, cls=Draft4Validator)\n        except ValidationError as e:\n            raise ValidationError(_validate_err_template.format(VDOM_SCHEMA, e))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef to_dict(self):\n        attributes = dict(self.attributes.items())\n        if self.style:\n            attributes.update({\"style\": dict(self.style.items())})\n        vdom_dict = {'tagName': self.tag_name, 'attributes': attributes}\n        if self.event_handlers:\n            event_handlers = dict(self.event_handlers.items())\n            for key, value in event_handlers.items():\n                value = create_event_handler(key, value)\n                event_handlers[key] = value\n            vdom_dict['eventHandlers'] = event_handlers\n        if self.key:\n            vdom_dict['key'] = self.key\n        vdom_dict['children'] = [c.to_dict() if isinstance(c, VDOM) else c for c in self.children]\n        return vdom_dict", "response": "Converts this VDOM object to a dictionary that passes our schema\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _to_inline_css(self, style):\n        return \"; \".join(['{}: {}'.format(convert_style_key(k), v) for k, v in style.items()])", "response": "Convert a style dictionary to inline CSS"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _guess_type(mrz_lines):\n        try:\n            if len(mrz_lines) == 3:\n                return 'TD1'\n            elif len(mrz_lines) == 2 and len(mrz_lines[0]) < 40 and len(mrz_lines[1]) < 40:\n                return 'MRVB' if mrz_lines[0][0].upper() == 'V' else 'TD2'\n            elif len(mrz_lines) == 2:\n                return 'MRVA' if mrz_lines[0][0].upper() == 'V' else 'TD3'\n            else:\n                return None\n        except Exception: #pylint: disable=broad-except\n            return None", "response": "Guesses the type of the MRZ from given lines. Returns TD1 TD2 TD3 MRVA MRVB or None."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting this object to an ordered dictionary of field - value pairs.", "response": "def to_dict(self):\n        \"\"\"Converts this object to an (ordered) dictionary of field-value pairs.\n\n        >>> m = MRZ(['IDAUT10000999<6<<<<<<<<<<<<<<<', '7109094F1112315AUT<<<<<<<<<<<6', 'MUSTERFRAU<<ISOLDE<<<<<<<<<<<<']).to_dict()\n        >>> assert m['type'] == 'ID' and m['country'] == 'AUT' and m['number'] == '10000999<'\n        >>> assert m['valid_number'] and m['valid_date_of_birth'] and m['valid_expiration_date'] and not m['valid_composite']\n        \"\"\"\n\n        result = OrderedDict()\n        result['mrz_type'] = self.mrz_type\n        result['valid_score'] = self.valid_score\n        if self.mrz_type is not None:\n            result['type'] = self.type\n            result['country'] = self.country\n            result['number'] = self.number\n            result['date_of_birth'] = self.date_of_birth\n            result['expiration_date'] = self.expiration_date\n            result['nationality'] = self.nationality\n            result['sex'] = self.sex\n            result['names'] = self.names\n            result['surname'] = self.surname\n            if self.mrz_type == 'TD1':\n                result['optional1'] = self.optional1\n                result['optional2'] = self.optional2\n            elif self.mrz_type in ['TD2', 'MRVA', 'MRVB']:\n                result['optional1'] = self.optional1\n            else:\n                result['personal_number'] = self.personal_number\n            result['check_number'] = self.check_number\n            result['check_date_of_birth'] = self.check_date_of_birth\n            result['check_expiration_date'] = self.check_expiration_date\n            if self.mrz_type not in ['MRVA', 'MRVB']:\n                result['check_composite'] = self.check_composite\n            if self.mrz_type == 'TD3':\n                result['check_personal_number'] = self.check_personal_number\n            result['valid_number'] = self.valid_check_digits[0]\n            result['valid_date_of_birth'] = self.valid_check_digits[1]\n            result['valid_expiration_date'] = self.valid_check_digits[2]\n            if self.mrz_type not in ['MRVA', 'MRVB']:\n                result['valid_composite'] = self.valid_check_digits[3]\n            if self.mrz_type == 'TD3':\n                result['valid_personal_number'] = self.valid_check_digits[4]\n            if 'method' in self.aux:\n                result['method'] = self.aux['method']\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef extract_first_jpeg_in_pdf(fstream):\n    parser = PDFParser(fstream)\n    if PY2:\n        document = PDFDocument(parser)\n    else:\n        document = PDFDocument()\n        parser.set_document(document)\n        document.set_parser(parser)\n        document.initialize('')\n    rsrcmgr = PDFResourceManager()\n    device = PDFPageAggregator(rsrcmgr)\n    interpreter = PDFPageInterpreter(rsrcmgr, device)\n    pages = PDFPage.create_pages(document) if PY2 else document.get_pages()\n    for page in pages:\n        interpreter.process_page(page)\n        layout = device.result\n        for el in layout:\n            if isinstance(el, LTFigure):\n                for im in el:\n                    if isinstance(im, LTImage):\n                        # Found one!\n                        st = None\n                        try:\n                            imdata = im.stream.get_data()\n                        except:\n                            # Failed to decode (seems to happen nearly always - there's probably a bug in PDFMiner), oh well...\n                            imdata = im.stream.get_rawdata()\n                        if imdata is not None and imdata.startswith(b'\\xff\\xd8\\xff\\xe0'):\n                            return imdata\n\n    return None", "response": "Reads a given PDF file and scans for the first valid embedded JPEG image."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_component(self, name, callable, provides=None, depends=None):\n        provides = provides or getattr(callable, '__provides__', [])\n        depends = depends or getattr(callable, '__depends__', [])\n        for p in provides:\n            if p in self.whoprovides:\n                raise Exception(\"There is already a component that provides %s\" % p)\n        self.provides[name] = provides\n        self.depends[name] = depends\n        self.components[name] = callable\n        for p in provides:\n            self.whoprovides[p] = name", "response": "Add a given callable to a list of components."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nremove an existing component with a given name invalidating all the values computed by the previous component.", "response": "def remove_component(self, name):\n        \"\"\"Removes an existing component with a given name, invalidating all the values computed by\n        the previous component.\"\"\"\n        if name not in self.components:\n            raise Exception(\"No component named %s\" % name)\n        del self.components[name]\n        del self.depends[name]\n        for p in self.provides[name]:\n            del self.whoprovides[p]\n            self.invalidate(p)\n        del self.provides[name]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nchange an existing component with a given name invalidating all the values computed by the previous component and its successors.", "response": "def replace_component(self, name, callable, provides=None, depends=None):\n        \"\"\"Changes an existing component with a given name, invalidating all the values computed by\n        the previous component and its successors.\"\"\"\n        self.remove_component(name)\n        self.add_component(name, callable, provides, depends)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef invalidate(self, key):\n        if key not in self.data:\n            return\n        del self.data[key]\n\n        # Find all components that used it and invalidate their results\n        for cname in self.components:\n            if key in self.depends[cname]:\n                for downstream_key in self.provides[cname]:\n                    self.invalidate(downstream_key)", "response": "Remove the given data item along with all items that depend on it in the graph."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nruns Tesseract on a given image.", "response": "def ocr(img, mrz_mode=True, extra_cmdline_params=''):\n    \"\"\"Runs Tesseract on a given image. Writes an intermediate tempfile and then runs the tesseract command on the image.\n\n    This is a simplified modification of image_to_string from PyTesseract, which is adapted to SKImage rather than PIL.\n\n    In principle we could have reimplemented it just as well - there are some apparent bugs in PyTesseract, but it works so far :)\n\n    :param mrz_mode: when this is True (default) the tesseract is configured to recognize MRZs rather than arbitrary texts.\n                     When False, no specific configuration parameters are passed (and you are free to provide your own via `extra_cmdline_params`)\n    :param extra_cmdline_params: extra parameters passed to tesseract. When mrz_mode=True, these are appended to whatever is the\n                    \"best known\" configuration at the moment.\n                    \"--oem 0\" is the parameter you might want to pass. This selects the Tesseract's \"legacy\" OCR engine, which often seems\n                    to work better than the new LSTM-based one.\n    \"\"\"\n    input_file_name = '%s.bmp' % _tempnam()\n    output_file_name_base = '%s' % _tempnam()\n    output_file_name = \"%s.txt\" % output_file_name_base\n    try:\n        # Prevent annoying warning about lossy conversion to uint8\n        if str(img.dtype).startswith('float') and np.nanmin(img) >= 0 and np.nanmax(img) <= 1:\n            img = img.astype(np.float64) * (np.power(2.0, 8) - 1) + 0.499999999\n            img = img.astype(np.uint8)\n        imwrite(input_file_name, img)\n\n        if mrz_mode:\n\t\t\t# NB: Tesseract 4.0 does not seem to support tessedit_char_whitelist\n            config = (\"--psm 6 -c tessedit_char_whitelist=ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789><\"\n                      \" -c load_system_dawg=F -c load_freq_dawg=F {}\").format(extra_cmdline_params)\n        else:\n            config = \"{}\".format(extra_cmdline_params)\n\n        pytesseract.run_tesseract(input_file_name,\n                                  output_file_name_base,\n                                  'txt',\n                                  lang=None,\n                                  config=config)\n        \n        if sys.version_info.major == 3:\n            f = open(output_file_name, encoding='utf-8')\n        else:\n            f = open(output_file_name)\n        \n        try:\n            return f.read().strip()\n        finally:\n            f.close()\n    finally:\n        pytesseract.cleanup(input_file_name)\n        pytesseract.cleanup(output_file_name)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef approx_equal(self, center, width, height, angle, tol=1e-6):\n        \"Method mainly useful for testing\"\n        return abs(self.cx - center[0]) < tol and abs(self.cy - center[1]) < tol and abs(self.width - width) < tol and \\\n               abs(self.height - height) < tol and abs(self.angle - angle) < tol", "response": "Method mainly useful for testing"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a RotatedBox that is obtained by rotating this box around a given center by a given angle.", "response": "def rotated(self, rotation_center, angle):\n        \"\"\"Returns a RotatedBox that is obtained by rotating this box around a given center by a given angle.\n\n        >>> assert RotatedBox([2, 2], 2, 1, 0.1).rotated([1, 1], np.pi/2).approx_equal([0, 2], 2, 1, np.pi/2+0.1)\n        \"\"\"\n        rot = np.array([[np.cos(angle), np.sin(angle)], [-np.sin(angle), np.cos(angle)]])\n        t = np.asfarray(rotation_center)\n        new_c = np.dot(rot.T, (self.center - t)) + t\n        return RotatedBox(new_c, self.width, self.height, (self.angle+angle) % (np.pi*2))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting this box to a polygon i. e. 4x2 array representing the four corners starting from lower left to upper left counterclockwise.", "response": "def as_poly(self, margin_width=0, margin_height=0):\n        \"\"\"Converts this box to a polygon, i.e. 4x2 array, representing the four corners starting from lower left to upper left counterclockwise.\n\n        :param margin_width: The additional \"margin\" that will be added to the box along its width dimension (from both sides) before conversion.\n        :param margin_height: The additional \"margin\" that will be added to the box along its height dimension (from both sides) before conversion.\n\n        >>> RotatedBox([0, 0], 4, 2, 0).as_poly()\n        array([[-2., -1.],\n               [ 2., -1.],\n               [ 2.,  1.],\n               [-2.,  1.]])\n        >>> RotatedBox([0, 0], 4, 2, np.pi/4).as_poly()\n        array([[-0.707..., -2.121...],\n               [ 2.121...,  0.707...],\n               [ 0.707...,  2.121...],\n               [-2.121..., -0.707...]])\n        >>> RotatedBox([0, 0], 4, 2, np.pi/2).as_poly()\n        array([[ 1., -2.],\n               [ 1.,  2.],\n               [-1.,  2.],\n               [-1., -2.]])\n        >>> RotatedBox([0, 0], 0, 0, np.pi/2).as_poly(2, 1)\n        array([[ 1., -2.],\n               [ 1.,  2.],\n               [-1.,  2.],\n               [-1., -2.]])\n        \"\"\"\n        v_hor = (self.width/2 + margin_width)*np.array([np.cos(self.angle), np.sin(self.angle)])\n        v_vert = (self.height/2 + margin_height)*np.array([-np.sin(self.angle), np.cos(self.angle)])\n        c = np.array([self.cx, self.cy])\n        return np.vstack([c - v_hor - v_vert, c + v_hor - v_vert, c + v_hor + v_vert, c - v_hor + v_vert])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nvisualizing the box on a matplotlib plot.", "response": "def plot(self, mode='image', ax=None, **kwargs):\n        \"\"\"Visualize the box on a matplotlib plot.\n        :param mode: How should the box coordinates and angle be interpreted.\n            - mode `'image'` corresponds to the situation where x coordinate of the box\n              denotes the \"row of an image\" (ie. the Y coordinate of the plot, arranged downwards)\n              and y coordinate of the box corresponds to the \"column of an image\",\n              (ie X coordinate of the plot). In other words, box's x goes downwards and y - rightwards.\n            - mode `'math'` corresponds to the \"mathematics\" situation where box's x and y correspond to the X and Y axes of the plot.\n        :param ax: the matplotlib axis to draw on. If unspecified, the current axis is used.\n        :param kwargs: arguments passed to the matplotlib's `Polygon` patch object. By default, fill is set to False, color to red and lw to 2.\n        :return: The created Polygon object.\n        \"\"\"\n        ax = ax or plt.gca()\n        poly = self.as_poly()\n        if mode == 'image':\n            poly = poly[:,[1,0]]\n        kwargs.setdefault('fill', False)\n        kwargs.setdefault('color', 'r')\n        kwargs.setdefault('lw', 2)\n        p = patches.Polygon(poly, **kwargs)\n        ax.add_patch(p)\n        return p"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nextract the contents of this box from a given image.", "response": "def extract_from_image(self, img, scale=1.0, margin_width=5, margin_height=5):\n        \"\"\"Extracts the contents of this box from a given image.\n        For that the image is \"unrotated\" by the appropriate angle, and the corresponding part is extracted from it.\n\n        Returns an image with dimensions height*scale x width*scale.\n        Note that the box coordinates are interpreted as \"image coordinates\" (i.e. x is row and y is column),\n        and box angle is considered to be relative to the vertical (i.e. np.pi/2 is \"normal orientation\")\n\n        :param img: a numpy ndarray suitable for image processing via skimage.\n        :param scale: the RotatedBox is scaled by this value before performing the extraction.\n            This is necessary when, for example, the location of a particular feature is determined using a smaller image,\n            yet then the corresponding area needs to be extracted from the original, larger image.\n            The scale parameter in this case should be width_of_larger_image/width_of_smaller_image.\n        :param margin_width: The margin that should be added to the width dimension of the box from each size.\n            This value is given wrt actual box dimensions (i.e. not scaled).\n        :param margin_height: The margin that should be added to the height dimension of the box from each side.\n        :return: a numpy ndarray, corresponding to the extracted region (aligned straight).\n\n        TODO: This could be made more efficient if we avoid rotating the full image and cut out the ROI from it beforehand.\n        \"\"\"\n        rotate_by = (np.pi/2 - self.angle)*180/np.pi\n        img_rotated = transform.rotate(img, angle=rotate_by, center=[self.center[1]*scale, self.center[0]*scale], resize=True)\n        # The resizeable transform will shift the resulting image somewhat wrt original coordinates.\n        # When we cut out the box we will compensate for this shift.\n        shift_c, shift_r = self._compensate_rotation_shift(img, scale)\n\n        r1 = max(int((self.center[0] - self.height/2 - margin_height)*scale - shift_r), 0)\n        r2 = int((self.center[0] + self.height/2 + margin_height)*scale - shift_r)\n        c1 = max(int((self.center[1] - self.width/2 - margin_width)*scale - shift_c), 0)\n        c2 = int((self.center[1] + self.width/2 + margin_width)*scale - shift_c)\n        return img_rotated[r1:r2, c1:c2]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _compensate_rotation_shift(self, img, scale):\n        ctr = np.asarray([self.center[1]*scale, self.center[0]*scale])\n        tform1 = transform.SimilarityTransform(translation=ctr)\n        tform2 = transform.SimilarityTransform(rotation=np.pi/2 - self.angle)\n        tform3 = transform.SimilarityTransform(translation=-ctr)\n        tform = tform3 + tform2 + tform1\n\n        rows, cols = img.shape[0], img.shape[1]\n        corners = np.array([\n            [0, 0],\n            [0, rows - 1],\n            [cols - 1, rows - 1],\n            [cols - 1, 0]\n        ])\n        corners = tform.inverse(corners)\n        minc = corners[:, 0].min()\n        minr = corners[:, 1].min()\n        maxc = corners[:, 0].max()\n        maxr = corners[:, 1].max()\n\n        # SKImage 0.11 version\n        out_rows = maxr - minr + 1\n        out_cols = maxc - minc + 1\n\n        # fit output image in new shape\n        return ((cols - out_cols) / 2., (rows - out_rows) / 2.)", "response": "This method compensates the rotation of the image by compensating the shift."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninterprets a set of points as a RotatedBox using PCA to determine the potential orientation of the longest component.", "response": "def from_points(points, box_type='bb'):\n        \"\"\"\n        Interpret a given point cloud as a RotatedBox, using PCA to determine the potential orientation (the longest component becomes width)\n        This is basically an approximate version of a min-area-rectangle algorithm.\n        TODO: Test whether using a true min-area-rectangle algorithm would be more precise or faster.\n\n        :param points: An n x 2 numpy array of coordinates.\n        :param box_type: The kind of method used to estimate the \"box\".\n            Possible values:\n                - `'bb'`, denoting the \"bounding box\" approach (min/max coordinates of the points correspond to box limits)\n                - `'mrz`, denoting a slightly modified technique, suited for MRZ zone detection from contour images.\n                          Here the assumption is that the upper and lower bounds of the box are better estimated as the\n                          10% and 90% quantile of the corresponding coordinates (rather than 0% and 100%, i.e. min and max).\n                          This helps against accidental noise in the contour.\n                          The `'mrz'` correction is only applied when there are at least 10 points in the set.\n        :returns: a RotatedBox, bounding the given set of points, oriented according to the principal components.\n\n        >>> RotatedBox.from_points([[0,0]])\n        RotatedBox(cx=0.0, cy=0.0, width=0.0, height=0.0, angle=0.0)\n        >>> assert RotatedBox.from_points([[0,0], [1,1], [2,2]]).approx_equal([1, 1], np.sqrt(8), 0, np.pi/4)\n        >>> assert RotatedBox.from_points([[0,0], [1,1], [0,1], [1,0]]).approx_equal([0.5, 0.5], 1, 1, 0.0) # The angle is rather arbitrary here\n        >>> assert RotatedBox.from_points([[0,0], [2,1], [0,1], [2,0]]).approx_equal([1, 0.5], 2, 1, 0)\n        >>> assert RotatedBox.from_points([[0,0], [2,4], [0,4], [2,0]]).approx_equal([1, 2], 4, 2, np.pi/2)\n        >>> assert RotatedBox.from_points([[0,0], [1,1.5], [2,0]]).approx_equal([1, 0.75], 2, 1.5, 0)\n        >>> assert RotatedBox.from_points([[0,0], [0,1], [1,1]]).approx_equal([0.25, 0.75], np.sqrt(2), np.sqrt(2)/2, np.pi/4)\n        \"\"\"\n        points = np.asfarray(points)\n        if points.shape[0] == 1:\n            return RotatedBox(points[0], width=0.0, height=0.0, angle=0.0, points=points)\n\n        m = PCA(2).fit(points)\n        # Find the angle\n        angle = (np.arctan2(m.components_[0,1], m.components_[0,0]) % np.pi)\n        if abs(angle - np.pi) < angle:\n            # Here the angle is always between -pi and pi\n            # If the principal component happened to be oriented so that the angle happens to be > pi/2 by absolute value,\n            # we flip the direction\n            angle = angle - np.pi if angle > 0 else angle + np.pi\n        points_transformed = m.transform(points)\n        ll = np.min(points_transformed, 0)\n        ur = np.max(points_transformed, 0)\n        wh = ur - ll\n\n        # Now compute and return the bounding box\n        if box_type == 'bb' or (box_type == 'mrz' and points.shape[0] < 10):\n            # We know that if we rotate the points around m.mean_, we get a box with bounds ur and ll\n            # The center of this box is (ur+ll)/2 + mean, which is not the same as the mean,\n            # hence to get the center of the original box we need to \"unrotate\" this box back.\n            return RotatedBox(np.dot(m.components_.T, (ll+ur)/2) + m.mean_, width=wh[0], height=wh[1], angle=angle, points=points)\n        elif box_type == 'mrz':\n            # When working with MRZ detection from contours, we may have minor \"bumps\" in the contour,\n            # that should be ignored at least along the long (\"horizontal\") side.\n            # To do that, we will use 10% and 90% quantiles as the bounds of the box instead of the max and min.\n            # We drop all points which lie beyond and simply repeat the estimation (now 'bb-style') without them.\n            h_coord = sorted(points_transformed[:,1])\n            n = len(h_coord)\n            bottom, top = h_coord[n/10], h_coord[n*9/10]\n            valid_points = np.logical_and(points_transformed[:,1]>=bottom, points_transformed[:,1]<=top)\n            rb = RotatedBox.from_points(points[valid_points, :], 'bb')\n            rb.points = points\n            return rb\n        else:\n            raise ValueError(\"Unknown parameter value: box_type=%s\" % box_type)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read_mrz(file, save_roi=False, extra_cmdline_params=''):\n    p = MRZPipeline(file, extra_cmdline_params)\n    mrz = p.result\n\n    if mrz is not None:\n        mrz.aux['text'] = p['text']\n        if save_roi:\n            mrz.aux['roi'] = p['roi']\n    return mrz", "response": "This function is used to read the MRZ from an image file. It is the main interface function to this module. It is responsible for reading the MRZ from the file and then parsing it into a MRZ object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _imread(self, file):\n        # For now, we have to select the imageio plugin to read image from byte stream\n        # When ski-image v0.15 is released, imageio will be the default plugin, so this\n        # code can be simplified at that time.  See issue report and pull request:\n        # https://github.com/scikit-image/scikit-image/issues/2889\n        # https://github.com/scikit-image/scikit-image/pull/3126\n        img = skimage_io.imread(file, as_gray=self.as_gray, plugin='imageio')\n        if img is not None and len(img.shape) != 2:\n            # The PIL plugin somewhy fails to load some images\n            img = skimage_io.imread(file, as_gray=self.as_gray, plugin='matplotlib')\n        return img", "response": "Proxy to skimage. io. imread with some fixes."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _are_aligned_angles(self, b1, b2):\n        \"Are two boxes aligned according to their angle?\"\n        return abs(b1 - b2) <= self.angle_tol or abs(np.pi - abs(b1 - b2)) <= self.angle_tol", "response": "Are two boxes aligned according to their angle?\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _are_nearby_parallel_boxes(self, b1, b2):\n        \"Are two boxes nearby, parallel, and similar in width?\"\n        if not self._are_aligned_angles(b1.angle, b2.angle):\n            return False\n        # Otherwise pick the smaller angle and see whether the two boxes are close according to the \"up\" direction wrt that angle\n        angle = min(b1.angle, b2.angle)\n        return abs(np.dot(b1.center - b2.center, [-np.sin(angle), np.cos(angle)])) < self.lineskip_tol * (\n            b1.height + b2.height) and (b1.width > 0) and (b2.width > 0) and (0.5 < b1.width / b2.width < 2.0)", "response": "Are two boxes nearby parallel and similar in width?"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _merge_any_two_boxes(self, box_list):\n        n = len(box_list)\n        for i in range(n):\n            for j in range(i + 1, n):\n                if self._are_nearby_parallel_boxes(box_list[i], box_list[j]):\n                    # Remove the two boxes from the list, add a new one\n                    a, b = box_list[i], box_list[j]\n                    merged_points = np.vstack([a.points, b.points])\n                    merged_box = RotatedBox.from_points(merged_points, self.box_type)\n                    if merged_box.width / merged_box.height >= self.min_box_aspect:\n                        box_list.remove(a)\n                        box_list.remove(b)\n                        box_list.append(merged_box)\n                        return True\n        return False", "response": "Given a list of boxes find two nearby parallel ones and merges them. Returns True if any of the boxes are found. Returns False if none are found. Returns True if any of the boxes are not nearby parallel ones. Returns False if none are found."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nattempts to improve the OCR result by scaling the image. Returns the old mrz if the new mrz is better.", "response": "def _try_larger_image(self, roi, cur_text, cur_mrz, filter_order=3):\n        \"\"\"Attempts to improve the OCR result by scaling the image. If the new mrz is better, returns it, otherwise returns\n        the old mrz.\"\"\"\n        if roi.shape[1] <= 700:\n            scale_by = int(1050.0 / roi.shape[1] + 0.5)\n            roi_lg = transform.rescale(roi, scale_by, order=filter_order, mode='constant', multichannel=False,\n                                       anti_aliasing=True)\n            new_text = ocr(roi_lg, extra_cmdline_params=self.extra_cmdline_params)\n            new_mrz = MRZ.from_ocr(new_text)\n            new_mrz.aux['method'] = 'rescaled(%d)' % filter_order\n            if new_mrz.valid_score > cur_mrz.valid_score:\n                cur_mrz = new_mrz\n                cur_text = new_text\n        return cur_text, cur_mrz"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprocesses a file and returns the parsed MRZ.", "response": "def process_file(params):\n    \"\"\"\n    Processes a file and returns the parsed MRZ (or None if no candidate regions were even found).\n\n    The input argument is a list (filename, save_roi, extra_params).\n    (Because we need to use this function within imap_unordered)\n    \"\"\"\n    tic = time.time()\n    filename, save_roi, extra_params = params\n    result = read_mrz(filename, save_roi=save_roi, extra_cmdline_params=extra_params)\n    walltime = time.time() - tic\n    return (filename, result, walltime)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef mrz():\n    parser = argparse.ArgumentParser(description='Run the MRZ OCR recognition algorithm on the given image.')\n    parser.add_argument('filename')\n    parser.add_argument('--json', action='store_true', help='Produce JSON (rather than tabular) output')\n    parser.add_argument('--legacy', action='store_true',\n                        help='Use the \"legacy\" Tesseract OCR engine (--oem 0). Despite the name, it most often results in better '\n                        'results. It is not the default option, because it will only work if '\n                        'your Tesseract installation includes the legacy *.traineddata files. You can download them at '\n                        'https://github.com/tesseract-ocr/tesseract/wiki/Data-Files#data-files-for-version-400-november-29-2016')\n    parser.add_argument('-r', '--save-roi', default=None,\n                        help='Output the region of the image that is detected to contain the MRZ to the given png file')\n    parser.add_argument('--version', action='version', version='PassportEye MRZ v%s' % passporteye.__version__)\n    args = parser.parse_args()\n\n    try:\n        extra_params = '--oem 0' if args.legacy else ''\n        filename, mrz_, walltime = process_file((args.filename, args.save_roi is not None, extra_params))\n    except TesseractNotFoundError:\n        sys.stderr.write(\"ERROR: The tesseract executable was not found.\\n\"\n                         \"Please, make sure Tesseract is installed and the appropriate directory is included \"\n                         \"in your PATH environment variable.\\n\")\n        sys.exit(1)\n    except TesseractError as ex:\n        sys.stderr.write(\"ERROR: %s\" % ex.message)\n        sys.exit(ex.status)\n    \n    d = mrz_.to_dict() if mrz_ is not None else {'mrz_type': None, 'valid': False, 'valid_score': 0}\n    d['walltime'] = walltime\n    d['filename'] = filename\n\n    if args.save_roi is not None and mrz_ is not None and 'roi' in mrz_.aux:\n        io.imsave(args.save_roi, mrz_.aux['roi'])\n\n    if not args.json:\n        for k in d:\n            print(\"%s\\t%s\" % (k, str(d[k])))\n    else:\n        print(json.dumps(d, indent=2))", "response": "Command - line script for extracting MRZ from a given image."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the screen size of the specified monitor.", "response": "def getScreenBounds(self, screenId):\n        \"\"\" Returns the screen size of the specified monitor (0 being the main monitor). \"\"\"\n        screen_details = self.getScreenDetails()\n        if not isinstance(screenId, int) or screenId < -1 or screenId >= len(screen_details):\n            raise ValueError(\"Invalid screen ID\")\n        if screenId == -1:\n            # -1 represents the entire virtual screen\n            return self._getVirtualScreenRect()\n        return screen_details[screenId][\"rect\"]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns list of attached monitors as positioned in virtual screen.", "response": "def getScreenDetails(self):\n        \"\"\" Return list of attached monitors\n\n        For each monitor (as dict), ``monitor[\"rect\"]`` represents the screen as positioned\n        in virtual screen. List is returned in device order, with the first element (0)\n        representing the primary monitor.\n        \"\"\"\n        monitors = self._getMonitorInfo()\n        primary_screen = None\n        screens = []\n        for monitor in monitors:\n            # Convert screen rect to Lackey-style rect (x,y,w,h) as position in virtual screen\n            screen = {\n                \"rect\": (\n                    monitor[\"rect\"][0],\n                    monitor[\"rect\"][1],\n                    monitor[\"rect\"][2] - monitor[\"rect\"][0],\n                    monitor[\"rect\"][3] - monitor[\"rect\"][1]\n                )\n            }\n            screens.append(screen)\n        return screens"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if a point is visible on any monitor.", "response": "def isPointVisible(self, x, y):\n        \"\"\" Checks if a point is visible on any monitor. \"\"\"\n        class POINT(ctypes.Structure):\n            _fields_ = [(\"x\", ctypes.c_long), (\"y\", ctypes.c_long)]\n        pt = POINT()\n        pt.x = x\n        pt.y = y\n        MONITOR_DEFAULTTONULL = 0\n        hmon = self._user32.MonitorFromPoint(pt, MONITOR_DEFAULTTONULL)\n        if hmon == 0:\n            return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncapture a bitmap from the given monitor device name and returns a PIL Image.", "response": "def _captureScreen(self, device_name):\n        \"\"\" Captures a bitmap from the given monitor device name\n\n        Returns as a PIL Image (BGR rather than RGB, for compatibility with OpenCV)\n        \"\"\"\n\n        ## Define constants/structs\n        class HBITMAP(ctypes.Structure):\n            _fields_ = [(\"bmType\", ctypes.c_long),\n                        (\"bmWidth\", ctypes.c_long),\n                        (\"bmHeight\", ctypes.c_long),\n                        (\"bmWidthBytes\", ctypes.c_long),\n                        (\"bmPlanes\", ctypes.wintypes.WORD),\n                        (\"bmBitsPixel\", ctypes.wintypes.WORD),\n                        (\"bmBits\", ctypes.wintypes.LPVOID)]\n        class BITMAPINFOHEADER(ctypes.Structure):\n            _fields_ = [(\"biSize\", ctypes.wintypes.DWORD),\n                        (\"biWidth\", ctypes.c_long),\n                        (\"biHeight\", ctypes.c_long),\n                        (\"biPlanes\", ctypes.wintypes.WORD),\n                        (\"biBitCount\", ctypes.wintypes.WORD),\n                        (\"biCompression\", ctypes.wintypes.DWORD),\n                        (\"biSizeImage\", ctypes.wintypes.DWORD),\n                        (\"biXPelsPerMeter\", ctypes.c_long),\n                        (\"biYPelsPerMeter\", ctypes.c_long),\n                        (\"biClrUsed\", ctypes.wintypes.DWORD),\n                        (\"biClrImportant\", ctypes.wintypes.DWORD)]\n        class BITMAPINFO(ctypes.Structure):\n            _fields_ = [(\"bmiHeader\", BITMAPINFOHEADER),\n                        (\"bmiColors\", ctypes.wintypes.DWORD*3)]\n        HORZRES = ctypes.c_int(8)\n        VERTRES = ctypes.c_int(10)\n        SRCCOPY =    0x00CC0020\n        CAPTUREBLT = 0x40000000\n        DIB_RGB_COLORS = 0\n\n        ## Begin logic\n        self._gdi32.CreateDCW.restype = ctypes.c_void_p\n        hdc = self._gdi32.CreateDCW(ctypes.c_wchar_p(str(device_name)), 0, 0, 0) # Convert to bytestring for c_wchar_p type\n        if hdc == 0:\n            raise ValueError(\"Empty hdc provided\")\n\n        # Get monitor specs\n        self._gdi32.GetDeviceCaps.argtypes = [ctypes.c_void_p, ctypes.c_int]\n        screen_width = self._gdi32.GetDeviceCaps(hdc, HORZRES)\n        screen_height = self._gdi32.GetDeviceCaps(hdc, VERTRES)\n\n        # Create memory device context for monitor\n        self._gdi32.CreateCompatibleDC.restype = ctypes.c_void_p\n        self._gdi32.CreateCompatibleDC.argtypes = [ctypes.c_void_p]\n        hCaptureDC = self._gdi32.CreateCompatibleDC(hdc)\n        if hCaptureDC == 0:\n            raise WindowsError(\"gdi:CreateCompatibleDC failed\")\n\n        # Create bitmap compatible with monitor\n        self._gdi32.CreateCompatibleBitmap.restype = ctypes.c_void_p\n        self._gdi32.CreateCompatibleBitmap.argtypes = [ctypes.c_void_p, ctypes.c_int, ctypes.c_int]\n        hCaptureBmp = self._gdi32.CreateCompatibleBitmap(hdc, screen_width, screen_height)\n        if hCaptureBmp == 0:\n            raise WindowsError(\"gdi:CreateCompatibleBitmap failed\")\n\n        # Select hCaptureBmp into hCaptureDC device context\n        self._gdi32.SelectObject.argtypes = [ctypes.c_void_p, ctypes.c_void_p]\n        self._gdi32.SelectObject(hCaptureDC, hCaptureBmp)\n\n        # Perform bit-block transfer from screen to device context (and thereby hCaptureBmp)\n        self._gdi32.BitBlt.argtypes = [\n            ctypes.c_void_p,\n            ctypes.c_int,\n            ctypes.c_int,\n            ctypes.c_int,\n            ctypes.c_int,\n            ctypes.c_void_p,\n            ctypes.c_int,\n            ctypes.c_int,\n            ctypes.c_ulong\n        ]\n        self._gdi32.BitBlt(hCaptureDC, 0, 0, screen_width, screen_height, hdc, 0, 0, SRCCOPY | CAPTUREBLT)\n\n        # Capture image bits from bitmap\n        img_info = BITMAPINFO()\n        img_info.bmiHeader.biSize = ctypes.sizeof(BITMAPINFOHEADER)\n        img_info.bmiHeader.biWidth = screen_width\n        img_info.bmiHeader.biHeight = screen_height\n        img_info.bmiHeader.biPlanes = 1\n        img_info.bmiHeader.biBitCount = 32\n        img_info.bmiHeader.biCompression = 0\n        img_info.bmiHeader.biClrUsed = 0\n        img_info.bmiHeader.biClrImportant = 0\n\n        buffer_length = screen_width * 4 * screen_height\n        image_data = ctypes.create_string_buffer(buffer_length)\n\n        self._gdi32.GetDIBits.restype = ctypes.c_int\n        self._gdi32.GetDIBits.argtypes = [\n            ctypes.c_void_p,\n            ctypes.c_void_p,\n            ctypes.c_uint,\n            ctypes.c_uint,\n            ctypes.c_void_p,\n            ctypes.c_void_p,\n            ctypes.c_uint\n        ]\n        scanlines = self._gdi32.GetDIBits(\n            hCaptureDC,\n            hCaptureBmp,\n            0,\n            screen_height,\n            ctypes.byref(image_data),\n            ctypes.byref(img_info),\n            DIB_RGB_COLORS)\n        if scanlines != screen_height:\n            raise WindowsError(\"gdi:GetDIBits failed\")\n        final_image = ImageOps.flip(\n            Image.frombuffer(\n                \"RGBX\",\n                (screen_width, screen_height),\n                image_data,\n                \"raw\",\n                \"RGBX\",\n                0,\n                1))\n        # Destroy created device context & GDI bitmap\n        self._gdi32.DeleteObject.argtypes = [ctypes.c_void_p]\n        self._gdi32.DeleteObject(hdc)\n        self._gdi32.DeleteObject(hCaptureDC)\n        self._gdi32.DeleteObject(hCaptureBmp)\n        return final_image"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning info about the attached monitors in device order", "response": "def _getMonitorInfo(self):\n        \"\"\" Returns info about the attached monitors, in device order\n\n        [0] is always the primary monitor\n        \"\"\"\n        monitors = []\n        CCHDEVICENAME = 32\n        def _MonitorEnumProcCallback(hMonitor, hdcMonitor, lprcMonitor, dwData):\n            class MONITORINFOEX(ctypes.Structure):\n                _fields_ = [(\"cbSize\", ctypes.wintypes.DWORD),\n                            (\"rcMonitor\", ctypes.wintypes.RECT),\n                            (\"rcWork\", ctypes.wintypes.RECT),\n                            (\"dwFlags\", ctypes.wintypes.DWORD),\n                            (\"szDevice\", ctypes.wintypes.WCHAR*CCHDEVICENAME)]\n            lpmi = MONITORINFOEX()\n            lpmi.cbSize = ctypes.sizeof(MONITORINFOEX)\n            self._user32.GetMonitorInfoW(hMonitor, ctypes.byref(lpmi))\n            #hdc = self._gdi32.CreateDCA(ctypes.c_char_p(lpmi.szDevice), 0, 0, 0)\n            monitors.append({\n                \"hmon\": hMonitor,\n                #\"hdc\":  hdc,\n                \"rect\": (lprcMonitor.contents.left,\n                         lprcMonitor.contents.top,\n                         lprcMonitor.contents.right,\n                         lprcMonitor.contents.bottom),\n                \"name\": lpmi.szDevice\n                })\n            return True\n        MonitorEnumProc = ctypes.WINFUNCTYPE(\n            ctypes.c_bool,\n            ctypes.c_ulong,\n            ctypes.c_ulong,\n            ctypes.POINTER(ctypes.wintypes.RECT),\n            ctypes.c_int)\n        callback = MonitorEnumProc(_MonitorEnumProcCallback)\n        if self._user32.EnumDisplayMonitors(0, 0, callback, 0) == 0:\n            raise WindowsError(\"Unable to enumerate monitors\")\n        # Clever magic to make the screen with origin of (0,0) [the primary monitor]\n        # the first in the list\n        # Sort by device ID - 0 is primary, 1 is next, etc.\n        monitors.sort(key=lambda x: (not (x[\"rect\"][0] == 0 and x[\"rect\"][1] == 0), x[\"name\"]))\n\n        return monitors"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _getVirtualScreenRect(self):\n        SM_XVIRTUALSCREEN = 76  # Left of virtual screen\n        SM_YVIRTUALSCREEN = 77  # Top of virtual screen\n        SM_CXVIRTUALSCREEN = 78 # Width of virtual screen\n        SM_CYVIRTUALSCREEN = 79 # Height of virtual screen\n\n        return (self._user32.GetSystemMetrics(SM_XVIRTUALSCREEN), \\\n                self._user32.GetSystemMetrics(SM_YVIRTUALSCREEN), \\\n                self._user32.GetSystemMetrics(SM_CXVIRTUALSCREEN), \\\n                self._user32.GetSystemMetrics(SM_CYVIRTUALSCREEN))", "response": "Returns the rect of the virtual screen."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a PIL bitmap of all monitors and virtual screen that is a bit - shaped image.", "response": "def _getVirtualScreenBitmap(self):\n        \"\"\" Returns a PIL bitmap (BGR channel order) of all monitors\n\n        Arranged like the Virtual Screen\n        \"\"\"\n\n        # Collect information about the virtual screen & monitors\n        min_x, min_y, screen_width, screen_height = self._getVirtualScreenRect()\n        monitors = self._getMonitorInfo()\n\n        # Initialize new black image the size of the virtual screen\n        virt_screen = Image.new(\"RGB\", (screen_width, screen_height))\n\n        # Capture images of each of the monitors and overlay on the virtual screen\n        for monitor_id in range(0, len(monitors)):\n            img = self._captureScreen(monitors[monitor_id][\"name\"])\n            # Capture virtscreen coordinates of monitor\n            x1, y1, x2, y2 = monitors[monitor_id][\"rect\"]\n            # Convert to image-local coordinates\n            x = x1 - min_x\n            y = y1 - min_y\n            # Paste on the virtual screen\n            virt_screen.paste(img, (x, y))\n        return virt_screen"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntriggering the OS paste keyboard shortcut", "response": "def osPaste(self):\n        \"\"\" Triggers the OS \"paste\" keyboard shortcut \"\"\"\n        from .InputEmulation import Keyboard\n        k = Keyboard()\n        k.keyDown(\"{CTRL}\")\n        k.type(\"v\")\n        k.keyUp(\"{CTRL}\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a handle for the first window that matches the provided regular expression.", "response": "def getWindowByTitle(self, wildcard, order=0):\n        \"\"\" Returns a handle for the first window that matches the provided \"wildcard\" regex \"\"\"\n        EnumWindowsProc = ctypes.WINFUNCTYPE(\n            ctypes.c_bool,\n            ctypes.POINTER(ctypes.c_int),\n            ctypes.py_object)\n        def callback(hwnd, context):\n            if ctypes.windll.user32.IsWindowVisible(hwnd):\n                length = ctypes.windll.user32.GetWindowTextLengthW(hwnd)\n                buff = ctypes.create_unicode_buffer(length + 1)\n                ctypes.windll.user32.GetWindowTextW(hwnd, buff, length + 1)\n                if re.search(context[\"wildcard\"], buff.value, flags=re.I) != None and not context[\"handle\"]:\n                    if context[\"order\"] > 0:\n                        context[\"order\"] -= 1\n                    else:\n                        context[\"handle\"] = hwnd\n            return True\n        data = {\"wildcard\": wildcard, \"handle\": None, \"order\": order}\n        ctypes.windll.user32.EnumWindows(EnumWindowsProc(callback), ctypes.py_object(data))\n        return data[\"handle\"]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a handle for the first window that matches the provided PID.", "response": "def getWindowByPID(self, pid, order=0):\n        \"\"\" Returns a handle for the first window that matches the provided PID \"\"\"\n        if pid <= 0:\n            return None\n        EnumWindowsProc = ctypes.WINFUNCTYPE(\n            ctypes.c_bool,\n            ctypes.POINTER(ctypes.c_int),\n            ctypes.py_object)\n        def callback(hwnd, context):\n            if ctypes.windll.user32.IsWindowVisible(hwnd):\n                pid = ctypes.c_ulong()\n                ctypes.windll.user32.GetWindowThreadProcessId(hwnd, ctypes.byref(pid))\n                if context[\"pid\"] == int(pid.value) and not context[\"handle\"]:\n                    if context[\"order\"] > 0:\n                        context[\"order\"] -= 1\n                    else:\n                        context[\"handle\"] = hwnd\n            return True\n        data = {\"pid\": pid, \"handle\": None, \"order\": order}\n        ctypes.windll.user32.EnumWindows(EnumWindowsProc(callback), ctypes.py_object(data))\n        return data[\"handle\"]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef getWindowRect(self, hwnd):\n        rect = ctypes.wintypes.RECT()\n        if ctypes.windll.user32.GetWindowRect(hwnd, ctypes.byref(rect)):\n            x1 = rect.left\n            y1 = rect.top\n            x2 = rect.right\n            y2 = rect.bottom\n            return (x1, y1, x2-x1, y2-y1)\n        return None", "response": "Returns a rect ( x y w h for the specified window s area."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef focusWindow(self, hwnd):\n        Debug.log(3, \"Focusing window: \" + str(hwnd))\n        SW_RESTORE = 9\n        if ctypes.windll.user32.IsIconic(hwnd):\n            ctypes.windll.user32.ShowWindow(hwnd, SW_RESTORE)\n        ctypes.windll.user32.SetForegroundWindow(hwnd)", "response": "Focuss the specified window to the front."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the title for the specified window", "response": "def getWindowTitle(self, hwnd):\n        \"\"\" Gets the title for the specified window \"\"\"\n        length = ctypes.windll.user32.GetWindowTextLengthW(hwnd)\n        buff = ctypes.create_unicode_buffer(length + 1)\n        ctypes.windll.user32.GetWindowTextW(hwnd, buff, length + 1)\n        return buff.value"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getWindowPID(self, hwnd):\n        pid = ctypes.c_ulong()\n        ctypes.windll.user32.GetWindowThreadProcessId(hwnd, ctypes.byref(pid))\n        return int(pid.value)", "response": "Gets the process ID that the specified window belongs to."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsimulates a transparent rectangle over the specified rect on the screen.", "response": "def highlight(self, rect, color=\"red\", seconds=None):\n        \"\"\" Simulates a transparent rectangle over the specified ``rect`` on the screen.\n\n        Actually takes a screenshot of the region and displays with a\n        rectangle border in a borderless window (due to Tkinter limitations)\n\n        If a Tkinter root window has already been created somewhere else,\n        uses that instead of creating a new one.\n        \"\"\"\n        if tk._default_root is None:\n            Debug.log(3, \"Creating new temporary Tkinter root\")\n            temporary_root = True\n            root = tk.Tk()\n            root.withdraw()\n        else:\n            Debug.log(3, \"Borrowing existing Tkinter root\")\n            temporary_root = False\n            root = tk._default_root\n        image_to_show = self.getBitmapFromRect(*rect)\n        app = highlightWindow(root, rect, color, image_to_show)\n        if seconds == 0:\n            t = threading.Thread(target=app.do_until_timeout)\n            t.start()\n            return app\n        app.do_until_timeout(seconds)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef isPIDValid(self, pid):\n        ## Slightly copied wholesale from http://stackoverflow.com/questions/568271/how-to-check-if-there-exists-a-process-with-a-given-pid\n        ## Thanks to http://stackoverflow.com/users/1777162/ntrrgc and http://stackoverflow.com/users/234270/speedplane\n        class ExitCodeProcess(ctypes.Structure):\n            _fields_ = [('hProcess', ctypes.c_void_p),\n                        ('lpExitCode', ctypes.POINTER(ctypes.c_ulong))]\n        SYNCHRONIZE = 0x100000\n        PROCESS_QUERY_LIMITED_INFORMATION = 0x1000\n        process = self._kernel32.OpenProcess(SYNCHRONIZE|PROCESS_QUERY_LIMITED_INFORMATION, 0, pid)\n        if not process:\n            return False\n        ec = ExitCodeProcess()\n        out = self._kernel32.GetExitCodeProcess(process, ctypes.byref(ec))\n        if not out:\n            err = self._kernel32.GetLastError()\n            if self._kernel32.GetLastError() == 5:\n                # Access is denied.\n                logging.warning(\"Access is denied to get pid info.\")\n            self._kernel32.CloseHandle(process)\n            return False\n        elif bool(ec.lpExitCode):\n            # There is an exit code, it quit\n            self._kernel32.CloseHandle(process)\n            return False\n        # No exit code, it's running.\n        self._kernel32.CloseHandle(process)\n        return True", "response": "Checks if a PID is associated with a running process."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nkills the process with the specified PID.", "response": "def killProcess(self, pid):\n        \"\"\" Kills the process with the specified PID (if possible) \"\"\"\n        SYNCHRONIZE = 0x00100000\n        PROCESS_TERMINATE = 0x0001\n        hProcess = self._kernel32.OpenProcess(SYNCHRONIZE|PROCESS_TERMINATE, True, pid)\n        result = self._kernel32.TerminateProcess(hProcess, 0)\n        self._kernel32.CloseHandle(hProcess)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef similar(self, similarity):\n        pattern = Pattern(self.path)\n        pattern.similarity = similarity\n        return pattern", "response": "Returns a new Pattern with the specified similarity threshold"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef targetOffset(self, dx, dy):\n        pattern = Pattern(self.path)\n        pattern.similarity = self.similarity\n        pattern.offset = Location(dx, dy)\n        return pattern", "response": "Returns a new Pattern with the given target offset"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the filename of the pattern s image and load it", "response": "def setFilename(self, filename):\n        \"\"\" Set the filename of the pattern's image (and load it) \"\"\"\n        ## Loop through image paths to find the image\n        found = False\n        for image_path in sys.path + [Settings.BundlePath, os.getcwd()] + Settings.ImagePaths:\n            full_path = os.path.join(image_path, filename)\n            if os.path.exists(full_path):\n                # Image file not found\n                found = True\n                break\n        ## Check if path is valid\n        if not found:\n            self.path = filename\n            print(Settings.ImagePaths)\n            raise ImageMissing(ImageMissingEvent(pattern=self, event_type=\"IMAGEMISSING\"))\n        self.path = full_path\n        self.image = cv2.imread(self.path)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nload and displays the image at self. path.", "response": "def debugPreview(self, title=\"Debug\"):\n        \"\"\" Loads and displays the image at ``Pattern.path`` \"\"\"\n        haystack = Image.open(self.path)\n        haystack.show()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the shape of the region as tuple", "response": "def getTuple(self):\n        \"\"\" Returns the shape of the region as (x, y, w, h) \"\"\"\n        return (self.x, self.y, self.w, self.h)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchanges the upper left - hand corner to a new Location object", "response": "def setLocation(self, location):\n        \"\"\" Change the upper left-hand corner to a new ``Location``\n\n        Doesn't change width or height\n        \"\"\"\n        if not location or not isinstance(location, Location):\n            raise ValueError(\"setLocation expected a Location object\")\n        self.x = location.x\n        self.y = location.y\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the ROI of the current region.", "response": "def setROI(self, *args):\n        \"\"\" Set Region of Interest (same as Region.setRect()) \"\"\"\n        if len(args) == 4:\n            x, y, w, h = args\n        elif len(args) == 1:\n            if isinstance(args[0], Region):\n                x, y, w, h = args[0].getTuple()\n            elif isinstance(args[0], tuple):\n                x, y, w, h = args[0]\n            else:\n                raise TypeError(\"Unrecognized argument for Region()\")\n        else:\n            raise TypeError(\"Unrecognized argument(s) for Region()\")\n        self.setX(x)\n        self.setY(y)\n        self.setW(w)\n        self.setH(h)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef contains(self, point_or_region):\n        if isinstance(point_or_region, Location):\n            return (self.x < point_or_region.x < self.x + self.w) and (self.y < point_or_region.y < self.y + self.h)\n        elif isinstance(point_or_region, Region):\n            return ((self.x < point_or_region.getX() < self.x + self.w) and\n                    (self.y < point_or_region.getY() < self.y + self.h) and\n                    (self.x < point_or_region.getX() + point_or_region.getW() < self.x + self.w) and\n                    (self.y < point_or_region.getY() + point_or_region.getH() < self.y + self.h))\n        else:\n            raise TypeError(\"Unrecognized argument type for contains()\")", "response": "Checks if point_or_region is within this region."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nchange shape of this region to match the given Region object", "response": "def morphTo(self, region):\n        \"\"\" Change shape of this region to match the given ``Region`` object \"\"\"\n        if not region or not isinstance(region, Region):\n            raise TypeError(\"morphTo expected a Region object\")\n        self.setROI(region)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the Location of the center of this region", "response": "def getCenter(self):\n        \"\"\" Return the ``Location`` of the center of this region \"\"\"\n        return Location(self.x+(self.w/2), self.y+(self.h/2))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the Location of the bottom right corner of this region", "response": "def getBottomRight(self):\n        \"\"\" Return the ``Location`` of the bottom right corner of this region \"\"\"\n        return Location(self.x+self.w, self.y+self.h)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef offset(self, location, dy=0):\n        if not isinstance(location, Location):\n            # Assume variables passed were dx,dy\n            location = Location(location, dy)\n        r = Region(self.x+location.x, self.y+location.y, self.w, self.h).clipRegionToScreen()\n        if r is None:\n            raise ValueError(\"Specified region is not visible on any screen\")\n            return None\n        return r", "response": "Returns a new Region offset from this one by location and dy."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngrows the region to the specified width and height.", "response": "def grow(self, width, height=None):\n        \"\"\" Expands the region by ``width`` on both sides and ``height`` on the top and bottom.\n\n        If only one value is provided, expands the region by that amount on all sides.\n        Equivalent to ``nearby()``.\n        \"\"\"\n        if height is None:\n            return self.nearby(width)\n        else:\n            return Region(\n                self.x-width,\n                self.y-height,\n                self.w+(2*width),\n                self.h+(2*height)).clipRegionToScreen()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef nearby(self, expand=50):\n        return Region(\n            self.x-expand,\n            self.y-expand,\n            self.w+(2*expand),\n            self.h+(2*expand)).clipRegionToScreen()", "response": "Returns a new Region that includes the nearby neighbourhood of the current region."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a new Region with a width of expand pixels.", "response": "def left(self, expand=None):\n        \"\"\" Returns a new Region left of the current region with a width of ``expand`` pixels.\n\n        Does not include the current region. If range is omitted, it reaches to the left border\n        of the screen. The new region has the same height and y-position as the current region.\n        \"\"\"\n        if expand == None:\n            x = 0\n            y = self.y\n            w = self.x\n            h = self.h\n        else:\n            x = self.x-expand\n            y = self.y\n            w = expand\n            h = self.h\n        return Region(x, y, w, h).clipRegionToScreen()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a new Region with a width of expand pixels.", "response": "def right(self, expand=None):\n        \"\"\" Returns a new Region right of the current region with a width of ``expand`` pixels.\n\n        Does not include the current region. If range is omitted, it reaches to the right border\n        of the screen. The new region has the same height and y-position as the current region.\n        \"\"\"\n        if expand == None:\n            x = self.x+self.w\n            y = self.y\n            w = self.getScreen().getBounds()[2] - x\n            h = self.h\n        else:\n            x = self.x+self.w\n            y = self.y\n            w = expand\n            h = self.h\n        return Region(x, y, w, h).clipRegionToScreen()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getBitmap(self):\n        return PlatformManager.getBitmapFromRect(self.x, self.y, self.w, self.h)", "response": "Returns the bitmap of this region"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef debugPreview(self, title=\"Debug\"):\n        region = self\n        haystack = self.getBitmap()\n        if isinstance(region, Match):\n            cv2.circle(\n                haystack,\n                (region.getTarget().x - self.x, region.getTarget().y - self.y),\n                5,\n                255)\n        if haystack.shape[0] > (Screen(0).getBounds()[2]/2) or haystack.shape[1] > (Screen(0).getBounds()[3]/2):\n            # Image is bigger than half the screen; scale it down\n            haystack = cv2.resize(haystack, (0, 0), fx=0.5, fy=0.5)\n        Image.fromarray(haystack).show()", "response": "Displays the region in a preview window."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nhighlighting the region with a colored frame.", "response": "def highlight(self, *args):\n        \"\"\" Highlights the region with a colored frame. Accepts the following parameters:\n\n        highlight([toEnable], [seconds], [color])\n        \n        * toEnable (boolean): Enables or disables the overlay\n        * seconds (number): Seconds to show overlay\n        * color (string): Hex code (\"#XXXXXX\") or color name (\"black\")\n        \"\"\"\n        toEnable = (self._highlighter is None)\n        seconds = 3\n        color = \"red\"\n        if len(args) > 3:\n            raise TypeError(\"Unrecognized argument(s) for highlight()\")\n        for arg in args:\n            if type(arg) == bool:\n                toEnable = arg\n            elif isinstance(arg, Number):\n                seconds = arg\n            elif isinstance(arg, basestring):\n                color = arg\n        if self._highlighter is not None:\n                self._highlighter.close()\n        if toEnable:\n            self._highlighter = PlatformManager.highlight((self.getX(), self.getY(), self.getW(), self.getH()), color, seconds)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef find(self, pattern):\n        findFailedRetry = True\n        while findFailedRetry:\n            match = self.exists(pattern)\n            if match is not None:\n                break\n            path = pattern.path if isinstance(pattern, Pattern) else pattern\n            findFailedRetry = self._raiseFindFailed(\"Could not find pattern '{}'\".format(path))\n            if findFailedRetry:\n                time.sleep(self._repeatWaitTime)\n        return match", "response": "Searches for an image pattern in the given region Returns None if no image was found."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsearches for an image pattern in the given region returns an iterator of Match objects.", "response": "def findAll(self, pattern):\n        \"\"\" Searches for an image pattern in the given region\n\n        Returns ``Match`` object if ``pattern`` exists, empty array otherwise (does not\n        throw exception). Sikuli supports OCR search with a text parameter. This does not (yet).\n        \"\"\"\n        find_time = time.time()\n        r = self.clipRegionToScreen()\n        if r is None:\n            raise ValueError(\"Region outside all visible screens\")\n            return None\n        seconds = self.autoWaitTimeout\n        if not isinstance(pattern, Pattern):\n            if not isinstance(pattern, basestring):\n                raise TypeError(\"find expected a string [image path] or Pattern object\")\n            pattern = Pattern(pattern)\n        needle = cv2.imread(pattern.path)\n        if needle is None:\n            raise ValueError(\"Unable to load image '{}'\".format(pattern.path))\n        needle_height, needle_width, needle_channels = needle.shape\n        positions = []\n        timeout = time.time() + seconds\n\n        # Check TemplateMatcher for valid matches\n        matches = []\n        while time.time() < timeout and len(matches) == 0:\n            matcher = TemplateMatcher(r.getBitmap())\n            matches = matcher.findAllMatches(needle, pattern.similarity)\n            time.sleep(1/self._defaultScanRate if self._defaultScanRate is not None else 1/Settings.WaitScanRate)\n\n        if len(matches) == 0:\n            Debug.info(\"Couldn't find '{}' with enough similarity.\".format(pattern.path))\n            return iter([])\n\n        # Matches found! Turn them into Match objects\n        lastMatches = []\n        for match in matches:\n            position, confidence = match\n            x, y = position\n            lastMatches.append(\n                Match(\n                    confidence,\n                    pattern.offset,\n                    ((x+self.x, y+self.y), (needle_width, needle_height))))\n        self._lastMatches = iter(lastMatches)\n        Debug.info(\"Found match(es) for pattern '{}' at similarity ({})\".format(pattern.path, pattern.similarity))\n        self._lastMatchTime = (time.time() - find_time) * 1000 # Capture find time in milliseconds\n        return self._lastMatches"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsearch for an image pattern in the given region given a specified number of seconds.", "response": "def wait(self, pattern, seconds=None):\n        \"\"\" Searches for an image pattern in the given region, given a specified timeout period\n\n        Functionally identical to find(). If a number is passed instead of a pattern,\n        just waits the specified number of seconds.\n        Sikuli supports OCR search with a text parameter. This does not (yet).\n        \"\"\"\n        if isinstance(pattern, (int, float)):\n            if pattern == FOREVER:\n                while True:\n                    time.sleep(1) # Infinite loop\n            time.sleep(pattern)\n            return None\n\n        if seconds is None:\n            seconds = self.autoWaitTimeout\n        \n        findFailedRetry = True\n        timeout = time.time() + seconds\n        while findFailedRetry:\n            while True:\n                match = self.exists(pattern)\n                if match:\n                    return match\n                if time.time() >= timeout:\n                    break\n            path = pattern.path if isinstance(pattern, Pattern) else pattern\n            findFailedRetry = self._raiseFindFailed(\"Could not find pattern '{}'\".format(path))\n            if findFailedRetry:\n                time.sleep(self._repeatWaitTime)\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef waitVanish(self, pattern, seconds=None):\n        r = self.clipRegionToScreen()\n        if r is None:\n            raise ValueError(\"Region outside all visible screens\")\n            return None\n        if seconds is None:\n            seconds = self.autoWaitTimeout\n        if not isinstance(pattern, Pattern):\n            if not isinstance(pattern, basestring):\n                raise TypeError(\"find expected a string [image path] or Pattern object\")\n            pattern = Pattern(pattern)\n\n        needle = cv2.imread(pattern.path)\n        match = True\n        timeout = time.time() + seconds\n\n        while match and time.time() < timeout:\n            matcher = TemplateMatcher(r.getBitmap())\n            # When needle disappears, matcher returns None\n            match = matcher.findBestMatch(needle, pattern.similarity)\n            time.sleep(1/self._defaultScanRate if self._defaultScanRate is not None else 1/Settings.WaitScanRate)\n        if match:\n            return False", "response": "Waits until the specified pattern is not visible on screen."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsearch for an image pattern in the given region returns None if no match is found.", "response": "def exists(self, pattern, seconds=None):\n        \"\"\" Searches for an image pattern in the given region\n\n        Returns Match if pattern exists, None otherwise (does not throw exception)\n        Sikuli supports OCR search with a text parameter. This does not (yet).\n        \"\"\"\n        find_time = time.time()\n        r = self.clipRegionToScreen()\n        if r is None:\n            raise ValueError(\"Region outside all visible screens\")\n            return None\n        if seconds is None:\n            seconds = self.autoWaitTimeout\n        if isinstance(pattern, int):\n            # Actually just a \"wait\" statement\n            time.sleep(pattern)\n            return\n        if not pattern:\n            time.sleep(seconds)\n        if not isinstance(pattern, Pattern):\n            if not isinstance(pattern, basestring):\n                raise TypeError(\"find expected a string [image path] or Pattern object\")\n            pattern = Pattern(pattern)\n        needle = cv2.imread(pattern.path)\n        if needle is None:\n            raise ValueError(\"Unable to load image '{}'\".format(pattern.path))\n        needle_height, needle_width, needle_channels = needle.shape\n        match = None\n        timeout = time.time() + seconds\n\n        # Consult TemplateMatcher to find needle\n        while not match:\n            matcher = TemplateMatcher(r.getBitmap())\n            match = matcher.findBestMatch(needle, pattern.similarity)\n            time.sleep(1/self._defaultScanRate if self._defaultScanRate is not None else 1/Settings.WaitScanRate)\n            if time.time() > timeout:\n                break\n\n        if match is None:\n            Debug.info(\"Couldn't find '{}' with enough similarity.\".format(pattern.path))\n            return None\n\n        # Translate local position into global screen position\n        position, confidence = match\n        position = (position[0] + self.x, position[1] + self.y)\n        self._lastMatch = Match(\n            confidence,\n            pattern.offset,\n            (position, (needle_width, needle_height)))\n        #self._lastMatch.debug_preview()\n        Debug.info(\"Found match for pattern '{}' at ({},{}) with confidence ({}). Target at ({},{})\".format(\n            pattern.path,\n            self._lastMatch.getX(),\n            self._lastMatch.getY(),\n            self._lastMatch.getScore(),\n            self._lastMatch.getTarget().x,\n            self._lastMatch.getTarget().y))\n        self._lastMatchTime = (time.time() - find_time) * 1000 # Capture find time in milliseconds\n        return self._lastMatch"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmove the cursor to the target location and clicks the default mouse button.", "response": "def click(self, target=None, modifiers=\"\"):\n        \"\"\" Moves the cursor to the target location and clicks the default mouse button. \"\"\"\n        if target is None:\n            target = self._lastMatch or self # Whichever one is not None\n\n        target_location = None\n        if isinstance(target, Pattern):\n            target_location = self.find(target).getTarget()\n        elif isinstance(target, basestring):\n            target_location = self.find(target).getTarget()\n        elif isinstance(target, Match):\n            target_location = target.getTarget()\n        elif isinstance(target, Region):\n            target_location = target.getCenter()\n        elif isinstance(target, Location):\n            target_location = target\n        else:\n            raise TypeError(\"click expected Pattern, String, Match, Region, or Location object\")\n        if modifiers != \"\":\n            keyboard.keyDown(modifiers)\n\n        Mouse.moveSpeed(target_location, Settings.MoveMouseDelay)\n        time.sleep(0.1) # For responsiveness\n        if Settings.ClickDelay > 0:\n            time.sleep(min(1.0, Settings.ClickDelay))\n            Settings.ClickDelay = 0.0\n        Mouse.click()\n        time.sleep(0.1)\n\n        if modifiers != 0:\n            keyboard.keyUp(modifiers)\n        Debug.history(\"Clicked at {}\".format(target_location))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmoving the cursor to the target location", "response": "def hover(self, target=None):\n        \"\"\" Moves the cursor to the target location \"\"\"\n        if target is None:\n            target = self._lastMatch or self # Whichever one is not None\n        target_location = None\n        if isinstance(target, Pattern):\n            target_location = self.find(target).getTarget()\n        elif isinstance(target, basestring):\n            target_location = self.find(target).getTarget()\n        elif isinstance(target, Match):\n            target_location = target.getTarget()\n        elif isinstance(target, Region):\n            target_location = target.getCenter()\n        elif isinstance(target, Location):\n            target_location = target\n        else:\n            raise TypeError(\"hover expected Pattern, String, Match, Region, or Location object\")\n\n        Mouse.moveSpeed(target_location, Settings.MoveMouseDelay)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nstarting a dragDrop operation.", "response": "def drag(self, dragFrom=None):\n        \"\"\" Starts a dragDrop operation.\n\n        Moves the cursor to the target location and clicks the mouse in preparation to drag\n        a screen element \"\"\"\n        if dragFrom is None:\n            dragFrom = self._lastMatch or self # Whichever one is not None\n        dragFromLocation = None\n        if isinstance(dragFrom, Pattern):\n            dragFromLocation = self.find(dragFrom).getTarget()\n        elif isinstance(dragFrom, basestring):\n            dragFromLocation = self.find(dragFrom).getTarget()\n        elif isinstance(dragFrom, Match):\n            dragFromLocation = dragFrom.getTarget()\n        elif isinstance(dragFrom, Region):\n            dragFromLocation = dragFrom.getCenter()\n        elif isinstance(dragFrom, Location):\n            dragFromLocation = dragFrom\n        else:\n            raise TypeError(\"drag expected dragFrom to be Pattern, String, Match, Region, or Location object\")\n        Mouse.moveSpeed(dragFromLocation, Settings.MoveMouseDelay)\n        time.sleep(Settings.DelayBeforeMouseDown)\n        Mouse.buttonDown()\n        Debug.history(\"Began drag at {}\".format(dragFromLocation))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncomplete a dragDrop operation on the current mouse object", "response": "def dropAt(self, dragTo=None, delay=None):\n        \"\"\" Completes a dragDrop operation\n\n        Moves the cursor to the target location, waits ``delay`` seconds, and releases the mouse\n        button \"\"\"\n        if dragTo is None:\n            dragTo = self._lastMatch or self # Whichever one is not None\n        if isinstance(dragTo, Pattern):\n            dragToLocation = self.find(dragTo).getTarget()\n        elif isinstance(dragTo, basestring):\n            dragToLocation = self.find(dragTo).getTarget()\n        elif isinstance(dragTo, Match):\n            dragToLocation = dragTo.getTarget()\n        elif isinstance(dragTo, Region):\n            dragToLocation = dragTo.getCenter()\n        elif isinstance(dragTo, Location):\n            dragToLocation = dragTo\n        else:\n            raise TypeError(\"dragDrop expected dragTo to be Pattern, String, Match, Region, or Location object\")\n\n        Mouse.moveSpeed(dragToLocation, Settings.MoveMouseDelay)\n        time.sleep(delay if delay is not None else Settings.DelayBeforeDrop)\n        Mouse.buttonUp()\n        Debug.history(\"Ended drag at {}\".format(dragToLocation))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nperforming a dragDrop operation.", "response": "def dragDrop(self, target, target2=None, modifiers=\"\"):\n        \"\"\" Performs a dragDrop operation.\n\n        Holds down the mouse button on ``dragFrom``, moves the mouse to ``dragTo``, and releases\n        the mouse button.\n\n        ``modifiers`` may be a typeKeys() compatible string. The specified keys will be held\n        during the drag-drop operation.\n        \"\"\"\n        if modifiers != \"\":\n            keyboard.keyDown(modifiers)\n        \n        if target2 is None:\n            dragFrom = self._lastMatch\n            dragTo = target\n        else:\n            dragFrom = target\n            dragTo = target2\n\n        self.drag(dragFrom)\n        time.sleep(Settings.DelayBeforeDrag)\n        self.dropAt(dragTo)\n\n        if modifiers != \"\":\n            keyboard.keyUp(modifiers)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntype text with modifiers.", "response": "def type(self, *args):\n        \"\"\" Usage: type([PSMRL], text, [modifiers])\n\n        If a pattern is specified, the pattern is clicked first. Doesn't support text paths.\n\n        Special keys can be entered with the key name between brackets, as `\"{SPACE}\"`, or as\n        `Key.SPACE`.\n        \"\"\"\n        pattern = None\n        text = None\n        modifiers = None\n        if len(args) == 1 and isinstance(args[0], basestring):\n            # Is a string (or Key) to type\n            text = args[0]\n        elif len(args) == 2:\n            if not isinstance(args[0], basestring) and isinstance(args[1], basestring):\n                pattern = args[0]\n                text = args[1]\n            else:\n                text = args[0]\n                modifiers = args[1]\n        elif len(args) == 3 and not isinstance(args[0], basestring):\n            pattern = args[0]\n            text = args[1]\n            modifiers = args[2]\n        else:\n            raise TypeError(\"type method expected ([PSMRL], text, [modifiers])\")\n\n        if pattern:\n            self.click(pattern)\n\n        Debug.history(\"Typing '{}' with modifiers '{}'\".format(text, modifiers))\n        kb = keyboard\n        if modifiers:\n            kb.keyDown(modifiers)\n        if Settings.TypeDelay > 0:\n            typeSpeed = min(1.0, Settings.TypeDelay)\n            Settings.TypeDelay = 0.0\n        else:\n            typeSpeed = self._defaultTypeSpeed\n        kb.type(text, typeSpeed)\n        if modifiers:\n            kb.keyUp(modifiers)\n        time.sleep(0.2)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef paste(self, *args):\n        target = None\n        text = \"\"\n        if len(args) == 1 and isinstance(args[0], basestring):\n            text = args[0]\n        elif len(args) == 2 and isinstance(args[1], basestring):\n            self.click(target)\n            text = args[1]\n        else:\n            raise TypeError(\"paste method expected [PSMRL], text\")\n\n        pyperclip.copy(text)\n        # Triggers OS paste for foreground window\n        PlatformManager.osPaste()\n        time.sleep(0.2)", "response": "Paste the specified text into the current keyboard."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef mouseMove(self, PSRML=None, dy=0):\n        if PSRML is None:\n            PSRML = self._lastMatch or self # Whichever one is not None\n        if isinstance(PSRML, Pattern):\n            move_location = self.find(PSRML).getTarget()\n        elif isinstance(PSRML, basestring):\n            move_location = self.find(PSRML).getTarget()\n        elif isinstance(PSRML, Match):\n            move_location = PSRML.getTarget()\n        elif isinstance(PSRML, Region):\n            move_location = PSRML.getCenter()\n        elif isinstance(PSRML, Location):\n            move_location = PSRML\n        elif isinstance(PSRML, int):\n            # Assume called as mouseMove(dx, dy)\n            offset = Location(PSRML, dy)\n            move_location = Mouse.getPos().offset(offset)\n        else:\n            raise TypeError(\"doubleClick expected Pattern, String, Match, Region, or Location object\")\n        Mouse.moveSpeed(move_location)", "response": "Move the mouse to the specified location."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nclick the wheel the specified number of ticks. Use the following parameters : PSRML direction steps stepDelay", "response": "def wheel(self, *args): # [PSRML], direction, steps\n        \"\"\" Clicks the wheel the specified number of ticks. Use the following parameters:\n\n        wheel([PSRML], direction, steps, [stepDelay])\n        \"\"\"\n        if len(args) == 2:\n            PSRML = None\n            direction = int(args[0])\n            steps = int(args[1])\n            stepDelay = None\n        elif len(args) == 3:\n            PSRML = args[0]\n            direction = int(args[1])\n            steps = int(args[2])\n            stepDelay = None\n        elif len(args) == 4:\n            PSRML = args[0]\n            direction = int(args[1])\n            steps = int(args[2])\n            stepDelay = int(args[3])\n        \n        if PSRML is not None:\n            self.mouseMove(PSRML)\n        Mouse.wheel(direction, steps)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn true if the whole region is even partially inside any screen otherwise false", "response": "def isRegionValid(self):\n        \"\"\" Returns false if the whole region is not even partially inside any screen, otherwise true \"\"\"\n        screens = PlatformManager.getScreenDetails()\n        for screen in screens:\n            s_x, s_y, s_w, s_h = screen[\"rect\"]\n            if self.x+self.w >= s_x and s_x+s_w >= self.x and self.y+self.h >= s_y and s_y+s_h >= self.y:\n                # Rects overlap\n                return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the part of the region that is visible on a screen.", "response": "def clipRegionToScreen(self):\n        \"\"\" Returns the part of the region that is visible on a screen\n\n        If the region equals to all visible screens, returns Screen(-1).\n        If the region is visible on multiple screens, returns the screen with the smallest ID.\n        Returns None if the region is outside the screen.\n        \"\"\"\n        if not self.isRegionValid():\n            return None\n        screens = PlatformManager.getScreenDetails()\n        total_x, total_y, total_w, total_h = Screen(-1).getBounds()\n        containing_screen = None\n        for screen in screens:\n            s_x, s_y, s_w, s_h = screen[\"rect\"]\n            if self.x >= s_x and self.x+self.w <= s_x+s_w and self.y >= s_y and self.y+self.h <= s_y+s_h:\n                # Region completely inside screen\n                return self\n            elif self.x+self.w <= s_x or s_x+s_w <= self.x or self.y+self.h <= s_y or s_y+s_h <= self.y:\n                # Region completely outside screen\n                continue\n            elif self.x == total_x and self.y == total_y and self.w == total_w and self.h == total_h:\n                # Region equals all screens, Screen(-1)\n                return self\n            else:\n                # Region partially inside screen\n                x = max(self.x, s_x)\n                y = max(self.y, s_y)\n                w = min(self.w, s_w)\n                h = min(self.h, s_h)\n                return Region(x, y, w, h)\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the raster for the region allowing sections to be indexed by row and column", "response": "def setRaster(self, rows, columns):\n        \"\"\" Sets the raster for the region, allowing sections to be indexed by row/column \"\"\"\n        rows = int(rows)\n        columns = int(columns)\n        if rows <= 0 or columns <= 0:\n            return self\n        self._raster = (rows, columns)\n        return self.getCell(0, 0)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the specified row of the region.", "response": "def getRow(self, row, numberRows=None):\n        \"\"\" Returns the specified row of the region (if the raster is set)\n\n        If numberRows is provided, uses that instead of the raster\n        \"\"\"\n        row = int(row)\n        if self._raster[0] == 0 or self._raster[1] == 0:\n            return self\n        if numberRows is None or numberRows < 1 or numberRows > 9:\n            numberRows = self._raster[0]\n        rowHeight = self.h / numberRows\n        if row < 0:\n            # If row is negative, count backwards from the end\n            row = numberRows - row\n            if row < 0:\n                # Bad row index, return last row\n                return Region(self.x, self.y+self.h-rowHeight, self.w, rowHeight)\n        elif row > numberRows:\n            # Bad row index, return first row\n            return Region(self.x, self.y, self.w, rowHeight)\n        return Region(self.x, self.y + (row * rowHeight), self.w, rowHeight)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getCol(self, column, numberColumns=None):\n        column = int(column)\n        if self._raster[0] == 0 or self._raster[1] == 0:\n            return self\n        if numberColumns is None or numberColumns < 1 or numberColumns > 9:\n            numberColumns = self._raster[1]\n        columnWidth = self.w / numberColumns\n        if column < 0:\n            # If column is negative, count backwards from the end\n            column = numberColumns - column\n            if column < 0:\n                # Bad column index, return last column\n                return Region(self.x+self.w-columnWidth, self.y, columnWidth, self.h)\n        elif column > numberColumns:\n            # Bad column index, return first column\n            return Region(self.x, self.y, columnWidth, self.h)\n        return Region(self.x + (column * columnWidth), self.y, columnWidth, self.h)", "response": "Returns the specified column of the region."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getCell(self, row, column):\n        row = int(row)\n        column = int(column)\n        if self._raster[0] == 0 or self._raster[1] == 0:\n            return self\n        rowHeight = self.h / self._raster[0]\n        columnWidth = self.h / self._raster[1]\n        if column < 0:\n            # If column is negative, count backwards from the end\n            column = self._raster[1] - column\n            if column < 0:\n                # Bad column index, return last column\n                column = self._raster[1]\n        elif column > self._raster[1]:\n            # Bad column index, return first column\n            column = 0\n        if row < 0:\n            # If row is negative, count backwards from the end\n            row = self._raster[0] - row\n            if row < 0:\n                # Bad row index, return last row\n                row = self._raster[0]\n        elif row > self._raster[0]:\n            # Bad row index, return first row\n            row = 0\n        return Region(self.x+(column*columnWidth), self.y+(row*rowHeight), columnWidth, rowHeight)", "response": "Returns the specified cell in the specified row and column."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get(self, part):\n        if part == self.MID_VERTICAL:\n            return Region(self.x+(self.w/4), y, self.w/2, self.h)\n        elif part == self.MID_HORIZONTAL:\n            return Region(self.x, self.y+(self.h/4), self.w, self.h/2)\n        elif part == self.MID_BIG:\n            return Region(self.x+(self.w/4), self.y+(self.h/4), self.w/2, self.h/2)\n        elif isinstance(part, int) and part >= 200 and part <= 999:\n            raster, row, column = str(part)\n            self.setRaster(raster, raster)\n            if row == raster and column == raster:\n                return self\n            elif row == raster:\n                return self.getCol(column)\n            elif column == raster:\n                return self.getRow(row)\n            else:\n                return self.getCell(row,column)\n        else:\n            return self", "response": "Returns a new region with the specified part."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef setRows(self, rows):\n        self._raster[0] = rows\n        if self._raster[1] == 0:\n            self._raster[1] = 1", "response": "Sets the number of rows in the raster"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the number of columns in the raster", "response": "def setCols(self, columns):\n        \"\"\" Sets the number of columns in the raster (if rows have not been initialized, set to 1 as well) \"\"\"\n        self._raster[1] = columns\n        if self._raster[0] == 0:\n            self._raster[0] = 1"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef setCenter(self, loc):\n        offset = self.getCenter().getOffset(loc) # Calculate offset from current center\n        return self.setLocation(self.getTopLeft().offset(offset)) # Move top left corner by the same offset", "response": "Move this region so it is centered on loc"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmoving this region so its top right corner is on loc", "response": "def setTopRight(self, loc):\n        \"\"\" Move this region so its top right corner is on ``loc`` \"\"\"\n        offset = self.getTopRight().getOffset(loc) # Calculate offset from current top right\n        return self.setLocation(self.getTopLeft().offset(offset)) # Move top left corner by the same offset"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmoves this region so its bottom left corner is on loc", "response": "def setBottomLeft(self, loc):\n        \"\"\" Move this region so its bottom left corner is on ``loc`` \"\"\"\n        offset = self.getBottomLeft().getOffset(loc) # Calculate offset from current bottom left\n        return self.setLocation(self.getTopLeft().offset(offset)) # Move top left corner by the same offset"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmoving this region so its bottom right corner is on loc", "response": "def setBottomRight(self, loc):\n        \"\"\" Move this region so its bottom right corner is on ``loc`` \"\"\"\n        offset = self.getBottomRight().getOffset(loc) # Calculate offset from current bottom right\n        return self.setLocation(self.getTopLeft().offset(offset)) # Move top left corner by the same offset"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef setSize(self, w, h):\n        self.setW(w)\n        self.setH(h)\n        return self", "response": "Sets the new size of the region"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef setRect(self, *args):\n        if len(args) == 1:\n            if isinstance(args[0], tuple):\n                x, y, w, h = args[0]\n            elif isinstance(args[0], Region):\n                x = Region.getX()\n                y = Region.getY()\n                w = Region.getW()\n                h = Region.getH()\n            else:\n                raise TypeError(\"Unrecognized arguments for setRect\")\n        elif len(args) == 4:\n            x, y, w, h = args\n        else:\n            raise TypeError(\"Unrecognized arguments for setRect\")\n        self.setX(x)\n        self.setY(y)\n        self.setW(w)\n        self.setH(h)\n        return self", "response": "Sets the rect of the current region. Accepts the following arguments"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef saveScreenCapture(self, path=None, name=None):\n        bitmap = self.getBitmap()\n        target_file = None\n        if path is None and name is None:\n            _, target_file = tempfile.mkstemp(\".png\")\n        elif name is None:\n            _, tpath = tempfile.mkstemp(\".png\")\n            target_file = os.path.join(path, tfile)\n        else:\n            target_file = os.path.join(path, name+\".png\")\n        cv2.imwrite(target_file, bitmap)\n        return target_file", "response": "Saves the region s bitmap to a file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef saveLastScreenImage(self):\n        bitmap = self.getLastScreenImage()\n        _, target_file = tempfile.mkstemp(\".png\")\n        cv2.imwrite(target_file, bitmap)", "response": "Saves the last screen image taken on this region s screen to a temporary file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the location of the right side of the region", "response": "def rightAt(self, offset=0):\n        \"\"\" Returns point in the center of the region's right side (offset to the right\n        by ``offset``) \"\"\"\n        return Location(self.getX() + self.getW() + offset, self.getY() + (self.getH() / 2))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef leftAt(self, offset=0):\n        return Location(self.getX() + offset, self.getY() + (self.getH() / 2))", "response": "Returns the location of the left side of the region"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the point in the center of the region s top side", "response": "def aboveAt(self, offset=0):\n        \"\"\" Returns point in the center of the region's top side (offset to the top\n        by negative ``offset``) \"\"\"\n        return Location(self.getX() + (self.getW() / 2), self.getY() + offset)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef bottomAt(self, offset=0):\n        return Location(self.getX() + (self.getW() / 2), self.getY() + self.getH() + offset)", "response": "Returns the location of the bottom side of the region"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a new region that contains the overlapping portion of this region and the specified region.", "response": "def intersection(ir):\n        \"\"\" Returns a new region that contains the overlapping portion of this region and the specified region (may be None) \"\"\"\n        x = max(self.getX(), ur.getX())\n        y = max(self.getY(), ur.getY())\n        w = min(self.getBottomRight().x, ur.getBottomRight().x) - x\n        h = min(self.getBottomRight().y, ur.getBottomRight().y) - y\n        if w > 0 and h > 0:\n            return Region(x, y, w, h)\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef findAllByRow(self, target):\n        row_matches = []\n        for row_index in range(self._raster[0]):\n            row = self.getRow(row_index)\n            row_matches[row_index] = row.findAll(target)\n        return row_matches", "response": "Returns an array of rows containing all matches in that row for the target pattern."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef findAllBycolumn(self, target):\n        column_matches = []\n        for column_index in range(self._raster[1]):\n            column = self.getRow(column_index)\n            column_matches[column_index] = column.findAll(target)\n        return column_matches", "response": "Returns an array of columns containing all matches in that column for the target pattern."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the best match in the region of the given pattern.", "response": "def findBest(self, pattern):\n        \"\"\" Returns the *best* match in the region (instead of the first match) \"\"\"\n        findFailedRetry = True\n        while findFailedRetry:\n            best_match = None\n            all_matches = self.findAll(pattern)\n            for match in all_matches:\n                if best_match is None or best_match.getScore() < match.getScore():\n                    best_match = match\n            self._lastMatch = best_match\n            if best_match is not None:\n                break\n            path = pattern.path if isinstance(pattern, Pattern) else pattern\n            findFailedRetry = self._raiseFindFailed(\"Could not find pattern '{}'\".format(path))\n            if findFailedRetry:\n                time.sleep(self._repeatWaitTime)\n        return best_match"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nregister an event to call handler when at least min_changed_pixels change in this region.", "response": "def onChange(self, min_changed_pixels=None, handler=None):\n        \"\"\" Registers an event to call ``handler`` when at least ``min_changed_pixels``\n        change in this region.\n\n        (Default for min_changed_pixels is set in Settings.ObserveMinChangedPixels)\n\n        The ``handler`` function should take one parameter, an ObserveEvent object\n        (see below). This event is ignored in the future unless the handler calls\n        the repeat() method on the provided ObserveEvent object.\n\n        Returns the event's ID as a string.\n        \"\"\"\n        if isinstance(min_changed_pixels, int) and (callable(handler) or handler is None):\n            return self._observer.register_event(\n                \"CHANGE\",\n                pattern=(min_changed_pixels, self.getBitmap()),\n                handler=handler)\n        elif (callable(min_changed_pixels) or min_changed_pixels is None) and (callable(handler) or handler is None):\n            handler = min_changed_pixels or handler\n            return self._observer.register_event(\n                \"CHANGE\",\n                pattern=(Settings.ObserveMinChangedPixels, self.getBitmap()),\n                handler=handler)\n        else:\n            raise ValueError(\"Unsupported arguments for onChange method\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef isChanged(self, min_changed_pixels, screen_state):\n        r = self.clipRegionToScreen()\n        current_state = r.getBitmap()\n        diff = numpy.subtract(current_state, screen_state)\n        return (numpy.count_nonzero(diff) >= min_changed_pixels)", "response": "Returns True if at least min_changed_pixels are different between the current state and screen_state."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef observe(self, seconds=None):\n        # Check if observer is already running\n        if self._observer.isRunning:\n            return False # Could not start\n\n        # Set timeout\n        if seconds is not None:\n            timeout = time.time() + seconds\n        else:\n            timeout = None\n\n        # Start observe loop\n        while (not self._observer.isStopped) and (seconds is None or time.time() < timeout):\n            # Check registered events\n            self._observer.check_events()\n            # Sleep for scan rate\n            time.sleep(1/self.getObserveScanRate())\n        return True", "response": "Starts the observer loop."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef stopObserver(self):\n        self._observer.isStopped = True\n        self._observer.isRunning = False", "response": "Stops the observer loop."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of all events that have occurred. Empties the internal queue.", "response": "def getEvents(self):\n        \"\"\" Returns a list of all events that have occurred.\n\n        Empties the internal queue.\n        \"\"\"\n        caught_events = self._observer.caught_events\n        self._observer.caught_events = []\n        for event in caught_events:\n            self._observer.activate_event(event[\"name\"])\n        return caught_events"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the named event.", "response": "def getEvent(self, name):\n        \"\"\" Returns the named event.\n\n        Removes it from the internal queue.\n        \"\"\"\n        to_return = None\n        for event in self._observer.caught_events:\n            if event[\"name\"] == name:\n                to_return = event\n                break\n        if to_return:\n            self._observer.caught_events.remove(to_return)\n            self._observer.activate_event(to_return[\"name\"])\n        return to_return"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _raiseImageMissing(self, pattern):\n        event = ImageMissingEvent(self, pattern=pattern, event_type=\"MISSING\")\n\n        if self._imageMissingHandler is not None:\n            self._imageMissingHandler(event)\n        response = (event._response or self._findFailedResponse)\n        #if response == \"PROMPT\": # Prompt not valid for ImageMissing error\n        #    response = _findFailedPrompt(pattern)\n        if response == \"ABORT\":\n            raise FindFailed(event)\n        elif response == \"SKIP\":\n            return False\n        elif response == \"RETRY\":\n            return True", "response": "Build an ImageMissingEvent and triggers the default handler or the custom handler. Returns True if it should retry False if it should abort."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the response to a FindFailed exception in this region.", "response": "def setFindFailedResponse(self, response):\n        \"\"\" Set the response to a FindFailed exception in this region.\n        \n        Can be ABORT, SKIP, PROMPT, or RETRY. \"\"\"\n        valid_responses = (\"ABORT\", \"SKIP\", \"PROMPT\", \"RETRY\")\n        if response not in valid_responses:\n            raise ValueError(\"Invalid response - expected one of ({})\".format(\", \".join(valid_responses)))\n        self._findFailedResponse = response"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset whether an exception should be thrown for FindFailed operations.", "response": "def setThrowException(self, setting):\n        \"\"\" Defines whether an exception should be thrown for FindFailed operations.\n\n        ``setting`` should be True or False. \"\"\"\n        if setting:\n            self._throwException = True\n            self._findFailedResponse = \"ABORT\"\n        else:\n            self._throwException = False\n            self._findFailedResponse = \"SKIP\""}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nbuilds a FindFailed event and triggers the default handler or the custom handler. Returns True if it should retry False if it should abort. Returns False if it should abort and raises an exception if it should retry.", "response": "def _raiseFindFailed(self, pattern):\n        \"\"\" Builds a FindFailed event and triggers the default handler (or the custom handler,\n        if one has been specified). Returns True if throwing method should retry, False if it\n        should skip, and throws an exception if it should abort. \"\"\"\n        event = FindFailedEvent(self, pattern=pattern, event_type=\"FINDFAILED\")\n\n        if self._findFailedHandler is not None:\n            self._findFailedHandler(event)\n        response = (event._response or self._findFailedResponse)\n        if response == \"PROMPT\":\n            response = _findFailedPrompt(pattern)\n\n        if response == \"ABORT\":\n            raise FindFailed(event)\n        elif response == \"SKIP\":\n            return False\n        elif response == \"RETRY\":\n            return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef register_event(self, event_type, pattern, handler):\n        if event_type not in self._supported_events:\n            raise ValueError(\"Unsupported event type {}\".format(event_type))\n        if event_type != \"CHANGE\" and not isinstance(pattern, Pattern) and not isinstance(pattern, basestring):\n            raise ValueError(\"Expected pattern to be a Pattern or string\")\n        if event_type == \"CHANGE\" and not (len(pattern)==2 and isinstance(pattern[0], int) and isinstance(pattern[1], numpy.ndarray)):\n            raise ValueError(\"For \\\"CHANGE\\\" events, ``pattern`` should be a tuple of ``min_changed_pixels`` and the base screen state.\")\n\n        # Create event object\n        event = {\n            \"pattern\": pattern,\n            \"event_type\": event_type,\n            \"count\": 0,\n            \"handler\": handler,\n            \"name\": uuid.uuid4(),\n            \"active\": True\n        }\n        self._events[event[\"name\"]] = event\n        return event[\"name\"]", "response": "Register an event in the cache."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the location of the match click target", "response": "def getTarget(self):\n        \"\"\" Returns the location of the match click target (center by default, but may be offset) \"\"\"\n        return self.getCenter().offset(self._target.x, self._target.y)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncaptures the region as an image", "response": "def capture(self, *args): #x=None, y=None, w=None, h=None):\n        \"\"\" Captures the region as an image \"\"\"\n        if len(args) == 0:\n            # Capture screen region\n            region = self\n        elif isinstance(args[0], Region):\n            # Capture specified region\n            region = args[0]\n        elif isinstance(args[0], tuple):\n            # Capture region defined by specified tuple\n            region = Region(*args[0])\n        elif isinstance(args[0], basestring):\n            # Interactive mode\n            raise NotImplementedError(\"Interactive capture mode not defined\")\n        elif isinstance(args[0], int):\n            # Capture region defined by provided x,y,w,h\n            region = Region(*args)\n        self.lastScreenImage = region.getBitmap()\n        return self.lastScreenImage"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef showMonitors(cls):\n        Debug.info(\"*** monitor configuration [ {} Screen(s)] ***\".format(cls.getNumberScreens()))\n        Debug.info(\"*** Primary is Screen {}\".format(cls.primaryScreen))\n        for index, screen in enumerate(PlatformManager.getScreenDetails()):\n            Debug.info(\"Screen {}: ({}, {}, {}, {})\".format(index, *screen[\"rect\"]))\n        Debug.info(\"*** end monitor configuration ***\")", "response": "Prints debug information about currently detected screens"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef resetMonitors(self):\n        Debug.error(\"*** BE AWARE: experimental - might not work ***\")\n        Debug.error(\"Re-evaluation of the monitor setup has been requested\")\n        Debug.error(\"... Current Region/Screen objects might not be valid any longer\")\n        Debug.error(\"... Use existing Region/Screen objects only if you know what you are doing!\")\n        self.__init__(self._screenId)\n        self.showMonitors()", "response": "Recalculates screen based on changed monitor setup"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef newRegion(self, loc, width, height):\n        return Region.create(self.getTopLeft().offset(loc), width, height)", "response": "Creates a new region on the current screen at the specified offset with the specified width and height."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef user(self, message):\n        if Settings.UserLogs:\n            self._write_log(Settings.UserLogPrefix, Settings.UserLogTime, message)", "response": "Creates a user log file if user logging is turned on."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrecording an action - level log message.", "response": "def history(self, message):\n        \"\"\" Records an Action-level log message\n\n        Uses the log path defined by ``Debug.setUserLogFile()``. If no log file is\n        defined, sends to STDOUT\n        \"\"\"\n        if Settings.ActionLogs:\n            self._write_log(\"action\", Settings.LogTime, message)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrecording an error - level log message.", "response": "def error(self, message):\n        \"\"\" Records an Error-level log message\n\n        Uses the log path defined by ``Debug.setUserLogFile()``. If no log file is\n        defined, sends to STDOUT\n        \"\"\"\n        if Settings.ErrorLogs:\n            self._write_log(\"error\", Settings.LogTime, message)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef info(self, message):\n        if Settings.InfoLogs:\n            self._write_log(\"info\", Settings.LogTime, message)", "response": "Records an Info - level log message."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef on(self, level):\n        if isinstance(level, int) and level >= 0 and level <= 3:\n            self._debug_level = level", "response": "Turn on all debugging messages up to the specified level."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrecording a debug - level log message.", "response": "def log(self, level, message):\n        \"\"\" Records a Debug-level log message\n        Uses the log path defined by ``Debug.setUserLogFile()``. If no log file is\n        defined, sends to STDOUT\n        \"\"\"\n        if level <= self._debug_level:\n            self._write_log(\"debug\", Settings.LogTime, message)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef setLoggerAll(self, mthd):\n        for key in self._logger_methods:\n            self._logger_methods[key] = mthd", "response": "Send all messages to the logger."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef setLogFile(self, filepath):\n        if filepath is None:\n            self._log_file = None\n            return\n        parsed_path = os.path.abspath(filepath)\n        # Checks if the provided log filename is in a real directory, and that\n        # the filename itself is not a directory.\n        if os.path.isdir(os.path.dirname(parsed_path)) and not os.path.isdir(parsed_path):\n            self._log_file = parsed_path\n        else:\n            raise IOError(\"File not found: \" + filepath)", "response": "Defines the file to which output log messages should be sent."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _write_log(self, log_type, log_time, message):\n        timestamp = datetime.datetime.now().strftime(\" %Y-%m-%d %H:%M:%S\")\n        log_entry = \"[{}{}] {}\".format(log_type, timestamp if log_time else \"\", message)\n        if self._logger and callable(getattr(self._logger, self._logger_methods[log_type], None)):\n            # Check for log handler (sends message only if _logger_no_prefix is True)\n            getattr(\n                self._logger,\n                self._logger_methods[log_type],\n                None\n                )(message if self._logger_no_prefix else log_entry)\n        elif self._log_file:\n            # Otherwise write to file, if a file has been specified\n            with open(self._log_file, 'a') as logfile:\n                try:\n                    logfile.write(unicode(log_entry + \"\\n\"))\n                except NameError: # `unicode` only works in Python 2\n                    logfile.write(log_entry + \"\\n\")\n        else:\n            # Otherwise, print to STDOUT\n            print(log_entry)", "response": "Private method to abstract log writing for different types of logs"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef setBundlePath(path):\n    if os.path.exists(path):\n        Settings.BundlePath = path\n    else:\n        raise OSError(\"File not found: \" + path)", "response": "Convenience function. Changes the path of the \\\\. sikuli bundle."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a path to the list of paths to search for images.", "response": "def addImagePath(new_path):\n    \"\"\" Convenience function. Adds a path to the list of paths to search for images.\n\n    Can be a URL (but must be accessible). \"\"\"\n    if os.path.exists(new_path):\n        Settings.ImagePaths.append(new_path)\n    elif \"http://\" in new_path or \"https://\" in new_path:\n        request = requests.get(new_path)\n        if request.status_code < 400:\n            # Path exists\n            Settings.ImagePaths.append(new_path)\n        else:\n            raise OSError(\"Unable to connect to \" + new_path)\n    else:\n        raise OSError(\"File not found: \" + new_path)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef unzip(from_file, to_folder):\n    with ZipFile(os.path.abspath(from_file), 'r') as to_unzip:\n        to_unzip.extractall(os.path.abspath(to_folder))", "response": "Utility function. unzips files from the zip file from_file into the folder to_folder."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating an info dialog with the specified text.", "response": "def popup(text, title=\"Lackey Info\"):\n    \"\"\" Creates an info dialog with the specified text. \"\"\"\n    root = tk.Tk()\n    root.withdraw()\n    tkMessageBox.showinfo(title, text)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef popError(text, title=\"Lackey Error\"):\n    root = tk.Tk()\n    root.withdraw()\n    tkMessageBox.showerror(title, text)", "response": "Creates an error dialog with the specified text."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef popAsk(text, title=\"Lackey Decision\"):\n    root = tk.Tk()\n    root.withdraw()\n    return tkMessageBox.askyesno(title, text)", "response": "Creates a yes - no dialog with the specified text."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating an input dialog with the specified message and default text.", "response": "def input(msg=\"\", default=\"\", title=\"Lackey Input\", hidden=False):\n    \"\"\" Creates an input dialog with the specified message and default text.\n\n    If `hidden`, creates a password dialog instead. Returns the entered value. \"\"\"\n    root = tk.Tk()\n    input_text = tk.StringVar()\n    input_text.set(default)\n    PopupInput(root, msg, title, hidden, input_text)\n    root.focus_force()\n    root.mainloop()\n    return str(input_text.get())"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a textarea dialog with the specified message and default text.", "response": "def inputText(message=\"\", title=\"Lackey Input\", lines=9, width=20, text=\"\"):\n    \"\"\" Creates a textarea dialog with the specified message and default text.\n\n    Returns the entered value. \"\"\"\n    root = tk.Tk()\n    input_text = tk.StringVar()\n    input_text.set(text)\n    PopupTextarea(root, message, title, lines, width, input_text)\n    root.focus_force()\n    root.mainloop()\n    return str(input_text.get())"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef select(message=\"\", title=\"Lackey Input\", options=None, default=None):\n    if options is None or len(options) == 0:\n        return \"\"\n    if default is None:\n        default = options[0]\n    if default not in options:\n        raise ValueError(\"<<default>> not in options[]\")\n    root = tk.Tk()\n    input_text = tk.StringVar()\n    input_text.set(message)\n    PopupList(root, message, title, options, default, input_text)\n    root.focus_force()\n    root.mainloop()\n    return str(input_text.get())", "response": "Creates a dropdown selection dialog with the specified message and options."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a file selection dialog with the specified message and options. Returns the selected file.", "response": "def popFile(title=\"Lackey Open File\"):\n    \"\"\" Creates a file selection dialog with the specified message and options.\n\n    Returns the selected file. \"\"\"\n    root = tk.Tk()\n    root.withdraw()\n    return str(tkFileDialog.askopenfilename(title=title))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the location of this object to the specified coordinates.", "response": "def setLocation(self, x, y):\n        \"\"\"Set the location of this object to the specified coordinates.\"\"\"\n        self.x = int(x)\n        self.y = int(y)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef offset(self, dx, dy):\n        return Location(self.x+dx, self.y+dy)", "response": "Return a new Location which is dx and dy pixels away horizontally and vertically."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getScreen(self):\n        from .RegionMatching import PlatformManager, Screen\n        screens = PlatformManager.getScreenDetails()\n        for screen in screens:\n            s_x, s_y, s_w, s_h = screen[\"rect\"]\n            if (self.x >= s_x) and (self.x < s_x + s_w) and (self.y >= s_y) and (self.y < s_y + s_h):\n                # Top left corner is inside screen region\n                return Screen(screens.index(screen))\n        return None # Could not find matching screen", "response": "Returns an instance of Screen object this Location is inside."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getMonitor(self):\n        from .RegionMatching import Screen\n        scr = self.getScreen()\n        return scr if scr is not None else Screen(0)", "response": "Returns an instance of the Screen object this Location is inside."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the offset between the given point and this point", "response": "def getOffset(self, loc):\n        \"\"\" Returns the offset between the given point and this point \"\"\"\n        return Location(loc.x - self.x, loc.y - self.y)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a new region around the given point", "response": "def grow(self, *args):\n        \"\"\" Creates a region around the given point Valid arguments:\n\n        * ``grow(wh)`` - Creates a region centered on this point with a width and height of ``wh``.\n        * ``grow(w, h)`` - Creates a region centered on this point with a width of ``w`` and height\n          of ``h``.\n        * ``grow(Region.CREATE_X_DIRECTION, Region.CREATE_Y_DIRECTION, w, h)`` - Creates a region\n          with this point as one corner, expanding in the specified direction\n        \n        \"\"\"\n        if len(args) == 1:\n            return Region.grow(self.x, self.y, args[0], args[0])\n        elif len(args) == 2:\n            return Region(self.x, self.y, args[0], args[1])\n        elif len(args) == 4:\n            return Region.create(self, *args)\n        else:\n            raise ValueError(\"Unrecognized arguments for grow\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a new point with the same offset on the target screen as this point has on the the current screen", "response": "def copyTo(self, screen):\n        \"\"\" Creates a new point with the same offset on the target screen as this point has on the\n        current screen \"\"\"\n        from .RegionMatching import Screen\n        if not isinstance(screen, Screen):\n            screen = RegionMatching.Screen(screen)\n        return screen.getTopLeft().offset(self.getScreen().getTopLeft().getOffset(self))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _focus_instance(self):\n        if self._title:\n            Debug.log(3, \"Focusing app with title like ({})\".format(self._title))\n            PlatformManager.focusWindow(PlatformManager.getWindowByTitle(re.escape(self._title)))\n            if self.getPID() == -1:\n                self.open()\n        elif self._pid and self._pid != -1:\n            Debug.log(3, \"Focusing app with pid ({})\".format(self._pid))\n            PlatformManager.focusWindow(PlatformManager.getWindowByPID(self._pid))\n        return self", "response": "In instances the focus method is replaced with this instance method."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a Region corresponding to the currently focused window.", "response": "def focusedWindow(cls):\n        \"\"\" Returns a Region corresponding to whatever window is in the foreground \"\"\"\n        x, y, w, h = PlatformManager.getWindowRect(PlatformManager.getForegroundWindow())\n        return Region(x, y, w, h)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the title of the currently open app.", "response": "def getWindow(self):\n        \"\"\" Returns the title of the main window of the currently open app.\n\n        Returns an empty string if no match could be found.\n        \"\"\"\n        if self.getPID() != -1:\n            return PlatformManager.getWindowTitle(PlatformManager.getWindowByPID(self.getPID()))\n        else:\n            return \"\""}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the PID for the associated app or - 1 if no app is associated with the associated app or - 1 if no app is running.", "response": "def getPID(self):\n        \"\"\" Returns the PID for the associated app\n        (or -1, if no app is associated or the app is not running)\n        \"\"\"\n        if self._pid is not None:\n            if not PlatformManager.isPIDValid(self._pid):\n                self._pid = -1\n            return self._pid\n        return -1"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef window(self, windowNum=0):\n        if self._pid == -1:\n            return None\n        x,y,w,h = PlatformManager.getWindowRect(PlatformManager.getWindowByPID(self._pid, windowNum))\n        return Region(x,y,w,h).clipRegionToScreen()", "response": "Returns the region corresponding to the specified window."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck if the window with the specified title is running.", "response": "def isRunning(self, waitTime=0):\n        \"\"\" If PID isn't set yet, checks if there is a window with the specified title. \"\"\"\n        waitUntil = time.time() + waitTime\n        while True:\n            if self.getPID() > 0:\n                return True\n            else:\n                self._pid = PlatformManager.getWindowPID(PlatformManager.getWindowByTitle(re.escape(self._title)))\n\n            # Check if we've waited long enough\n            if time.time() > waitUntil:\n                break\n            else:\n                time.sleep(self._defaultScanRate)\n        return self.getPID() > 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getBitmapFromRect(self, x, y, w, h):\n        min_x, min_y, screen_width, screen_height = self._getVirtualScreenRect()\n        img = self._getVirtualScreenBitmap() # TODO\n        # Limit the coordinates to the virtual screen\n        # Then offset so 0,0 is the top left corner of the image\n        # (Top left of virtual screen could be negative)\n        x1 = min(max(min_x, x), min_x+screen_width) - min_x\n        y1 = min(max(min_y, y), min_y+screen_height) - min_y\n        x2 = min(max(min_x, x+w), min_x+screen_width) - min_x\n        y2 = min(max(min_y, y+h), min_y+screen_height) - min_y\n        return numpy.array(img.crop((x1, y1, x2, y2)))", "response": "Capture the specified area of the virtual screen."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the rect of all attached screens as x y w h", "response": "def _getVirtualScreenRect(self):\n        \"\"\" Returns the rect of all attached screens as (x, y, w, h) \"\"\"\n        monitors = self.getScreenDetails()\n        x1 = min([s[\"rect\"][0] for s in monitors])\n        y1 = min([s[\"rect\"][1] for s in monitors])\n        x2 = max([s[\"rect\"][0]+s[\"rect\"][2] for s in monitors])\n        y2 = max([s[\"rect\"][1]+s[\"rect\"][3] for s in monitors])\n        return (x1, y1, x2-x1, y2-y1)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a bitmap of all attached screens.", "response": "def _getVirtualScreenBitmap(self):\n        \"\"\" Returns a bitmap of all attached screens \"\"\"\n        filenames = []\n        screen_details = self.getScreenDetails()\n        for screen in screen_details:\n            fh, filepath = tempfile.mkstemp('.png')\n            filenames.append(filepath)\n            os.close(fh)\n        subprocess.call(['screencapture', '-x'] + filenames)\n\n        min_x, min_y, screen_w, screen_h = self._getVirtualScreenRect()\n        virtual_screen = Image.new(\"RGB\", (screen_w, screen_h))\n        for filename, screen in zip(filenames, screen_details):\n            # Capture virtscreen coordinates of monitor\n            x, y, w, h = screen[\"rect\"]\n            # Convert image size if needed\n            im = Image.open(filename)\n            im.load()\n            if im.size[0] != w or im.size[1] != h:\n                im = im.resize((int(w), int(h)), Image.ANTIALIAS)\n            # Convert to image-local coordinates\n            x = x - min_x\n            y = y - min_y\n            # Paste on the virtual screen\n            virtual_screen.paste(im, (x, y))\n            os.unlink(filename)\n        return virtual_screen"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getScreenDetails(self):\n        primary_screen = None\n        screens = []\n        for monitor in AppKit.NSScreen.screens():\n            # Convert screen rect to Lackey-style rect (x,y,w,h) as position in virtual screen\n            screen = {\n                \"rect\": (\n                    int(monitor.frame().origin.x),\n                    int(monitor.frame().origin.y),\n                    int(monitor.frame().size.width),\n                    int(monitor.frame().size.height)\n                )\n            }\n            screens.append(screen)\n        return screens", "response": "Return list of attached monitors as dict"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if a point is visible on any monitor.", "response": "def isPointVisible(self, x, y):\n        \"\"\" Checks if a point is visible on any monitor. \"\"\"\n        for screen in self.getScreenDetails():\n            s_x, s_y, s_w, s_h = screen[\"rect\"]\n            if (s_x <= x < (s_x + s_w)) and (s_y <= y < (s_y + s_h)):\n                return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntriggers the OS copy keyboard shortcut", "response": "def osCopy(self):\n        \"\"\" Triggers the OS \"copy\" keyboard shortcut \"\"\"\n        k = Keyboard()\n        k.keyDown(\"{CTRL}\")\n        k.type(\"c\")\n        k.keyUp(\"{CTRL}\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a handle for the first window that matches the provided wildcard regex", "response": "def getWindowByTitle(self, wildcard, order=0):\n        \"\"\" Returns a handle for the first window that matches the provided \"wildcard\" regex \"\"\"\n        for w in self._get_window_list():\n            if \"kCGWindowName\" in w and re.search(wildcard, w[\"kCGWindowName\"], flags=re.I):\n                # Matches - make sure we get it in the correct order\n                if order == 0:\n                    return w[\"kCGWindowNumber\"]\n                else:\n                    order -= 1"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef getWindowByPID(self, pid, order=0):\n        for w in self._get_window_list():\n            if \"kCGWindowOwnerPID\" in w and w[\"kCGWindowOwnerPID\"] == pid:\n                # Matches - make sure we get it in the correct order\n                if order == 0:\n                    return w[\"kCGWindowNumber\"]\n                else:\n                    order -= 1\n        raise OSError(\"Could not find window for PID {} at index {}\".format(pid, order))", "response": "Returns a handle for the first window that matches the provided PID."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a rect of the specified window s area", "response": "def getWindowRect(self, hwnd):\n        \"\"\" Returns a rect (x,y,w,h) for the specified window's area \"\"\"\n        for w in self._get_window_list():\n            if \"kCGWindowNumber\" in w and w[\"kCGWindowNumber\"] == hwnd:\n                x = w[\"kCGWindowBounds\"][\"X\"]\n                y = w[\"kCGWindowBounds\"][\"Y\"]\n                width = w[\"kCGWindowBounds\"][\"Width\"]\n                height = w[\"kCGWindowBounds\"][\"Height\"]\n                return (x, y, width, height)\n        raise OSError(\"Unrecognized window number {}\".format(hwnd))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the title for the specified window", "response": "def getWindowTitle(self, hwnd):\n        \"\"\" Gets the title for the specified window \"\"\"\n        for w in self._get_window_list():\n            if \"kCGWindowNumber\" in w and w[\"kCGWindowNumber\"] == hwnd:\n                return w[\"kCGWindowName\"]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the process ID that the specified window belongs to.", "response": "def getWindowPID(self, hwnd):\n        \"\"\" Gets the process ID that the specified window belongs to \"\"\"\n        for w in self._get_window_list():\n            if \"kCGWindowNumber\" in w and w[\"kCGWindowNumber\"] == hwnd:\n                return w[\"kCGWindowOwnerPID\"]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getForegroundWindow(self):\n        active_app = NSWorkspace.sharedWorkspace().frontmostApplication().localizedName()\n        for w in self._get_window_list():\n            if \"kCGWindowOwnerName\" in w and w[\"kCGWindowOwnerName\"] == active_app:\n                return w[\"kCGWindowNumber\"]", "response": "Returns a handle to the window in the foreground"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_window_list(self):\n        window_list = Quartz.CGWindowListCopyWindowInfo(Quartz.kCGWindowListExcludeDesktopElements, Quartz.kCGNullWindowID)\n        return window_list", "response": "Returns a dictionary of details about open windows"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef highlight(self, rect, color=\"red\", seconds=None):\n        self.queue = queue.Queue()\n        if seconds == 0:\n            t = threading.Thread(target=self._do_until_timeout, args=(self.queue,(rect,color,seconds)))\n            t.start()\n            q = self.queue\n            control_obj = lambda: None\n            control_obj.close = lambda: q.put(True)\n            return control_obj\n        self._do_until_timeout(self.queue, (rect,color,seconds))", "response": "Simulates a transparent rectangle over the specified rect on the screen."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsearches all processes for the given PID then returns the originating command", "response": "def getProcessName(self, pid):\n        \"\"\" Searches all processes for the given PID, then returns the originating command \"\"\"\n        ps = subprocess.check_output([\"ps\", \"aux\"]).decode(\"ascii\")\n        processes = ps.split(\"\\n\")\n        cols = len(processes[0].split()) - 1\n        for row in processes[1:]:\n            if row != \"\":\n                proc = row.split(None, cols)\n                if proc[1].strip() == str(pid):\n                    return proc[-1]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmoves the cursor to specified location.", "response": "def move(self, loc, yoff=None):\n        \"\"\" Moves cursor to specified location. Accepts the following arguments:\n\n        * ``move(loc)`` - Move cursor to ``Location``\n        * ``move(xoff, yoff)`` - Move cursor to offset from current location\n\n        \"\"\"\n        from .Geometry import Location\n        self._lock.acquire()\n        if isinstance(loc, Location):\n            mouse.move(loc.x, loc.y)\n        elif yoff is not None:\n            xoff = loc\n            mouse.move(xoff, yoff)\n        else:\n            raise ValueError(\"Invalid argument. Expected either move(loc) or move(xoff, yoff).\")\n        self._last_position = loc\n        self._lock.release()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmoves the mouse cursor to specified location over seconds.", "response": "def moveSpeed(self, location, seconds=0.3):\n        \"\"\" Moves cursor to specified ``Location`` over ``seconds``.\n\n        If ``seconds`` is 0, moves the cursor immediately. Used for smooth\n        somewhat-human-like motion.\n        \"\"\"\n        self._lock.acquire()\n        original_location = mouse.get_position()\n        mouse.move(location.x, location.y, duration=seconds)\n        if mouse.get_position() == original_location and original_location != location.getTuple():\n            raise IOError(\"\"\"\n                Unable to move mouse cursor. This may happen if you're trying to automate a \n                program running as Administrator with a script running as a non-elevated user.\n            \"\"\")\n        self._lock.release()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nclick the specified mouse button.", "response": "def click(self, loc=None, button=mouse.LEFT):\n        \"\"\" Clicks the specified mouse button.\n\n        If ``loc`` is set, move the mouse to that Location first.\n\n        Use button constants Mouse.LEFT, Mouse.MIDDLE, Mouse.RIGHT\n        \"\"\"\n        if loc is not None:\n            self.moveSpeed(loc)\n        self._lock.acquire()\n        mouse.click(button)\n        self._lock.release()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nholds down the specified mouse button.", "response": "def buttonDown(self, button=mouse.LEFT):\n        \"\"\" Holds down the specified mouse button.\n\n        Use Mouse.LEFT, Mouse.MIDDLE, Mouse.RIGHT\n        \"\"\"\n        self._lock.acquire()\n        mouse.press(button)\n        self._lock.release()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef buttonUp(self, button=mouse.LEFT):\n        self._lock.acquire()\n        mouse.release(button)\n        self._lock.release()", "response": "Release the specified mouse button."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef wheel(self, direction, steps):\n        self._lock.acquire()\n        if direction == 1:\n            wheel_moved = steps\n        elif direction == 0:\n            wheel_moved = -1*steps\n        else:\n            raise ValueError(\"Expected direction to be 1 or 0\")\n        self._lock.release()\n        return mouse.wheel(wheel_moved)", "response": "Clicks the wheel in the given direction."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef keyDown(self, keys):\n        if not isinstance(keys, basestring):\n            raise TypeError(\"keyDown expected keys to be a string\")\n        in_special_code = False\n        special_code = \"\"\n        for i in range(0, len(keys)):\n            if keys[i] == \"{\":\n                in_special_code = True\n            elif in_special_code and (keys[i] == \"}\" or keys[i] == \" \" or i == len(keys)-1):\n                # End of special code (or it wasn't a special code after all)\n                in_special_code = False\n                if special_code in self._SPECIAL_KEYCODES.keys():\n                    # Found a special code\n                    keyboard.press(self._SPECIAL_KEYCODES[special_code])\n                else:\n                    # Wasn't a special code, just treat it as keystrokes\n                    self.keyDown(\"{\")\n                    # Press the rest of the keys normally\n                    self.keyDown(special_code)\n                    self.keyDown(keys[i])\n                special_code = \"\"\n            elif in_special_code:\n                special_code += keys[i]\n            elif keys[i] in self._REGULAR_KEYCODES.keys():\n                keyboard.press(keys[i])\n            elif keys[i] in self._UPPERCASE_KEYCODES.keys():\n                keyboard.press(self._SPECIAL_KEYCODES[\"SHIFT\"])\n                keyboard.press(self._UPPERCASE_KEYCODES[keys[i]])", "response": "Handles a string of keys. Holds down all of them."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntranslates a string into a series of keystrokes.", "response": "def type(self, text, delay=0.1):\n        \"\"\" Translates a string into a series of keystrokes.\n\n        Respects Sikuli special codes, like \"{ENTER}\".\n        \"\"\"\n        in_special_code = False\n        special_code = \"\"\n        modifier_held = False\n        modifier_stuck = False\n        modifier_codes = []\n\n        for i in range(0, len(text)):\n            if text[i] == \"{\":\n                in_special_code = True\n            elif in_special_code and (text[i] == \"}\" or text[i] == \" \" or i == len(text)-1):\n                in_special_code = False\n                if special_code in self._SPECIAL_KEYCODES.keys():\n                    # Found a special code\n                    keyboard.press_and_release(self._SPECIAL_KEYCODES[special_code])\n                else:\n                    # Wasn't a special code, just treat it as keystrokes\n                    keyboard.press(self._SPECIAL_KEYCODES[\"SHIFT\"])\n                    keyboard.press_and_release(self._UPPERCASE_KEYCODES[\"{\"])\n                    keyboard.release(self._SPECIAL_KEYCODES[\"SHIFT\"])\n                    # Release the rest of the keys normally\n                    self.type(special_code)\n                    self.type(text[i])\n                special_code = \"\"\n            elif in_special_code:\n                special_code += text[i]\n            elif text[i] in self._REGULAR_KEYCODES.keys():\n                keyboard.press(self._REGULAR_KEYCODES[text[i]])\n                keyboard.release(self._REGULAR_KEYCODES[text[i]])\n            elif text[i] in self._UPPERCASE_KEYCODES.keys():\n                keyboard.press(self._SPECIAL_KEYCODES[\"SHIFT\"])\n                keyboard.press_and_release(self._UPPERCASE_KEYCODES[text[i]])\n                keyboard.release(self._SPECIAL_KEYCODES[\"SHIFT\"])\n            if delay and not in_special_code:\n                time.sleep(delay)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef findBestMatch(self, needle, similarity):\n        method = cv2.TM_CCOEFF_NORMED\n        position = None\n\n        match = cv2.matchTemplate(self.haystack, needle, method)\n        min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(match)\n        if method == cv2.TM_SQDIFF_NORMED or method == cv2.TM_SQDIFF:\n            confidence = min_val\n            if min_val <= 1-similarity:\n                # Confidence checks out\n                position = min_loc\n        else:\n            confidence = max_val\n            if max_val >= similarity:\n                # Confidence checks out\n                position = max_loc\n\n        if not position:\n            return None\n\n        return (position, confidence)", "response": "Find the best match for needle that has a similarity better than or equal to similarity."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfinds all matches for needle with confidence better than or equal to similarity.", "response": "def findAllMatches(self, needle, similarity):\n        \"\"\" Find all matches for ``needle`` with confidence better than or equal to ``similarity``.\n\n        Returns an array of tuples ``(position, confidence)`` if match(es) is/are found,\n        or an empty array otherwise.\n        \"\"\"\n        positions = []\n        method = cv2.TM_CCOEFF_NORMED\n\n        match = cv2.matchTemplate(self.haystack, self.needle, method)\n\n        indices = (-match).argpartition(100, axis=None)[:100] # Review the 100 top matches\n        unraveled_indices = numpy.array(numpy.unravel_index(indices, match.shape)).T\n        for location in unraveled_indices:\n            y, x = location\n            confidence = match[y][x]\n            if method == cv2.TM_SQDIFF_NORMED or method == cv2.TM_SQDIFF:\n                if confidence <= 1-similarity:\n                    positions.append(((x, y), confidence))\n            else:\n                if confidence >= similarity:\n                    positions.append(((x, y), confidence))\n\n        positions.sort(key=lambda x: (x[0][1], x[0][0]))\n        return positions"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind the best match for a given needle and similarity.", "response": "def findBestMatch(self, needle, similarity):\n        \"\"\" Finds the best match using a search pyramid to improve efficiency\n\n        Pyramid implementation unashamedly stolen from https://github.com/stb-tester/stb-tester\n\n        *Developer's Note - Despite the name, this method actually returns the **first** result\n        with enough similarity, not the **best** result.*\n        \"\"\"\n        needle = cv2.cvtColor(needle, cv2.COLOR_BGR2GRAY) # Convert to grayscale\n        haystack = self.haystack\n        # Check if haystack or needle are a solid color - if so, switch to SQDIFF_NORMED\n\n        if self._is_solid_color(needle):\n            print(\"Solid color, using SQDIFF\")\n            method = cv2.TM_SQDIFF_NORMED\n            if self._is_solid_black(needle):\n                print(\"Inverting images\")\n                # Invert needle & haystack before matching\n                needle = numpy.invert(needle)\n                haystack = numpy.invert(haystack)\n        else:\n            #print(\"Not Solid color, using CCOEFF\")\n            method = cv2.TM_CCOEFF_NORMED\n\n        levels = 3\n        needle_pyramid = self._build_pyramid(needle, levels)\n        # Needle will be smaller than haystack, so may not be able to create\n        # ``levels`` smaller versions of itself. If not, create only as many\n        # levels for ``haystack`` as we could for ``needle``.\n        haystack_pyramid = self._build_pyramid(haystack, min(levels, len(needle_pyramid)))\n        roi_mask = None\n\n        # Run through each level in the pyramid, refining found ROIs\n        for level in range(len(haystack_pyramid)):\n            # Populate the heatmap with ones or zeroes depending on the appropriate method\n            lvl_haystack = haystack_pyramid[level]\n            lvl_needle = needle_pyramid[level]\n            if (lvl_needle.shape[0] > lvl_haystack.shape[0]) or (lvl_needle.shape[1] > lvl_haystack.shape[1]):\n                raise ValueError(\"Image to find is larger than search area\")\n            matches_heatmap = (\n                (numpy.ones if method == cv2.TM_SQDIFF_NORMED else numpy.zeros)(\n                    (lvl_haystack.shape[0] - lvl_needle.shape[0] + 1, lvl_haystack.shape[1] - lvl_needle.shape[1] + 1),\n                    dtype=numpy.float32))\n\n            # Scale up region of interest for the next level in the pyramid\n            # (if it's been set and is a valid size)\n            if roi_mask is not None:\n                if any(x < 3 for x in roi_mask.shape):\n                    roi_mask = None\n                else:\n                    roi_mask = cv2.pyrUp(roi_mask)\n\n            # If roi_mask is set, only search the best candidates in haystack\n            # for the needle:\n\n            if roi_mask is None:\n                # Initialize mask to the whole image\n                rois = [(0, 0, matches_heatmap.shape[1], matches_heatmap.shape[0])]\n            else:\n                # Depending on version of OpenCV, findContours returns either a three-tuple\n                # or a two-tuple. Unsure why the install is different (possibly linked to\n                # OS version).\n                try:\n                    _, contours, _ = cv2.findContours(\n                        roi_mask,\n                        cv2.RETR_EXTERNAL,\n                        cv2.CHAIN_APPROX_NONE)\n                except ValueError:\n                    contours, _ = cv2.findContours(\n                        roi_mask,\n                        cv2.RETR_EXTERNAL,\n                        cv2.CHAIN_APPROX_NONE)\n                # Expand contour rect by 1px on all sides with some tuple magic\n                rois = [tuple(sum(y) for y in zip(cv2.boundingRect(x), (-1, -1, 2, 2))) for x in contours]\n\n\n            for roi in rois:\n                # Trim ROI bounds to zero (if negative)\n                x, y, w, h = roi\n                x = max(x, 0)\n                y = max(y, 0)\n                # Add needle dimensions to roi\n                roi = (x, y, w+lvl_needle.shape[1]-1, h+lvl_needle.shape[0]-1)\n                # numpy 2D slice\n                roi_slice = (slice(roi[1], roi[1]+roi[3]), slice(roi[0], roi[0]+roi[2]))\n                # numpy 2D slice\n                r_slice = (slice(y, y+h), slice(x, x+w))\n\n                # Search the region of interest for needle (and update heatmap)\n                matches_heatmap[r_slice] = cv2.matchTemplate(\n                    lvl_haystack[roi_slice],\n                    lvl_needle,\n                    method)\n\n            min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(matches_heatmap)\n            # Reduce similarity to allow for scaling distortion\n            # (unless we are on the original image)\n            pyr_similarity = max(0, similarity - (0.2 if level < len(haystack_pyramid)-1 else 0))\n            position = None\n            confidence = None\n            # Check for a match\n            if method == cv2.TM_SQDIFF_NORMED:\n                confidence = min_val\n                if min_val <= 1-pyr_similarity:\n                    # Confidence checks out\n                    position = min_loc\n            else:\n                confidence = max_val\n                if max_val >= pyr_similarity:\n                    # Confidence checks out\n                    position = max_loc\n\n            if not position:\n                break\n\n            # Find the best regions of interest\n            _, roi_mask = cv2.threshold(\n                # Source image\n                matches_heatmap,\n                # Confidence threshold\n                ((1-pyr_similarity) if method == cv2.TM_SQDIFF_NORMED else pyr_similarity),\n                # Max value\n                255,\n                # Thresholding style\n                (cv2.THRESH_BINARY_INV if method == cv2.TM_SQDIFF_NORMED else cv2.THRESH_BINARY))\n            roi_mask = roi_mask.astype(numpy.uint8)\n\n        # Whew! Let's see if there's a match after all that.\n\n        if not position:\n            Debug.log(3, \"Best match: {} at {}\".format(max_val, max_loc))\n            return None\n\n        # There was a match!\n        if method == cv2.TM_SQDIFF_NORMED:\n            confidence = 1 - confidence # Invert confidence if we used the SQDIFF method\n        return (position, confidence)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef findAllMatches(self, needle, similarity):\n        positions = []\n        # Use findBestMatch to get the best match\n        while True:\n            best_match = self.findBestMatch(needle, similarity)\n            if best_match is None: # No more matches\n                break\n            # Found a match. Add it to our list\n            positions.append(best_match) # (position, confidence)\n\n            # Erase the found match from the haystack.\n            # Repeat this process until no other matches are found\n            x, y = best_match[0]\n            w = needle.shape[1]\n            h = needle.shape[0]\n            roi = (x, y, w, h)\n            # numpy 2D slice\n            roi_slice = (slice(roi[1], roi[1]+roi[3]), slice(roi[0], roi[0]+roi[2]))\n            self.haystack[roi_slice] = 0\n\n        # Whew! Let's see if there's a match after all that.\n        positions.sort(key=lambda x: (x[0][1], x[0][0]))\n        return positions", "response": "Find all matches above similarity in the haystack."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _build_pyramid(self, image, levels):\n        pyramid = [image]\n        for l in range(levels-1):\n            if any(x < 20 for x in pyramid[-1].shape[:2]):\n                break\n            pyramid.append(cv2.pyrDown(pyramid[-1]))\n        return list(reversed(pyramid))", "response": "Builds a list of reduced - size images from smallest to original size"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef count_weights(scope=None, exclude=None, graph=None):\n  if scope:\n    scope = scope if scope.endswith('/') else scope + '/'\n  graph = graph or tf.get_default_graph()\n  vars_ = graph.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n  if scope:\n    vars_ = [var for var in vars_ if var.name.startswith(scope)]\n  if exclude:\n    exclude = re.compile(exclude)\n    vars_ = [var for var in vars_ if not exclude.match(var.name)]\n  shapes = [var.get_shape().as_list() for var in vars_]\n  return int(sum(np.prod(shape) for shape in shapes))", "response": "Count learnable parameters.\n\n  Args:\n    scope: Restrict the count to a variable scope.\n    exclude: Regex to match variable names to exclude.\n    graph: Operate on a graph other than the current default graph.\n\n  Returns:\n    Number of learnable parameters as integer."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _custom_diag_normal_kl(lhs, rhs, name=None):  # pylint: disable=unused-argument\n  with tf.name_scope(name or 'kl_divergence'):\n    mean0 = lhs.mean()\n    mean1 = rhs.mean()\n    logstd0 = tf.log(lhs.stddev())\n    logstd1 = tf.log(rhs.stddev())\n    logstd0_2, logstd1_2 = 2 * logstd0, 2 * logstd1\n    return 0.5 * (\n        tf.reduce_sum(tf.exp(logstd0_2 - logstd1_2), -1) +\n        tf.reduce_sum((mean1 - mean0) ** 2 / tf.exp(logstd1_2), -1) +\n        tf.reduce_sum(logstd1_2, -1) - tf.reduce_sum(logstd0_2, -1) -\n        mean0.shape[-1].value)", "response": "Customized KL divergence of two normals with diagonal covariance."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef feed_forward_categorical(\n    config, action_space, observations, unused_length, state=None):\n  \"\"\"Independent feed forward networks for policy and value.\n\n  The policy network outputs the mean action and the log standard deviation\n  is learned as independent parameter vector.\n\n  Args:\n    config: Configuration object.\n    action_space: Action space of the environment.\n    observations: Sequences of observations.\n    unused_length: Batch of sequence lengths.\n    state: Unused batch of initial recurrent states.\n\n  Raises:\n    ValueError: Unexpected action space.\n\n  Returns:\n    Attribute dictionary containing the policy, value, and unused state.\n  \"\"\"\n  init_output_weights = tf.contrib.layers.variance_scaling_initializer(\n      factor=config.init_output_factor)\n  if not isinstance(action_space, gym.spaces.Discrete):\n    raise ValueError('Network expects discrete actions.')\n  flat_observations = tf.reshape(observations, [\n      tf.shape(observations)[0], tf.shape(observations)[1],\n      functools.reduce(operator.mul, observations.shape.as_list()[2:], 1)])\n  with tf.variable_scope('policy'):\n    x = flat_observations\n    for size in config.policy_layers:\n      x = tf.contrib.layers.fully_connected(x, size, tf.nn.relu)\n    logits = tf.contrib.layers.fully_connected(\n        x, action_space.n, None, weights_initializer=init_output_weights)\n  with tf.variable_scope('value'):\n    x = flat_observations\n    for size in config.value_layers:\n      x = tf.contrib.layers.fully_connected(x, size, tf.nn.relu)\n    value = tf.contrib.layers.fully_connected(x, 1, None)[..., 0]\n  policy = tfd.Categorical(logits)\n  return agents.tools.AttrDict(policy=policy, value=value, state=state)", "response": "Independent feed forward networks for policy and value."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndefines the simulation graph.", "response": "def define_simulation_graph(batch_env, algo_cls, config):\n  \"\"\"Define the algorithm and environment interaction.\n\n  Args:\n    batch_env: In-graph environments object.\n    algo_cls: Constructor of a batch algorithm.\n    config: Configuration object for the algorithm.\n\n  Returns:\n    Object providing graph elements via attributes.\n  \"\"\"\n  # pylint: disable=unused-variable\n  step = tf.Variable(0, False, dtype=tf.int32, name='global_step')\n  is_training = tf.placeholder(tf.bool, name='is_training')\n  should_log = tf.placeholder(tf.bool, name='should_log')\n  do_report = tf.placeholder(tf.bool, name='do_report')\n  force_reset = tf.placeholder(tf.bool, name='force_reset')\n  algo = algo_cls(batch_env, step, is_training, should_log, config)\n  done, score, summary = tools.simulate(\n      batch_env, algo, should_log, force_reset)\n  message = 'Graph contains {} trainable variables.'\n  tf.logging.info(message.format(tools.count_weights()))\n  # pylint: enable=unused-variable\n  return tools.AttrDict(locals())"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef define_batch_env(constructor, num_agents, env_processes):\n  with tf.variable_scope('environments'):\n    if env_processes:\n      envs = [\n          tools.wrappers.ExternalProcess(constructor)\n          for _ in range(num_agents)]\n    else:\n      envs = [constructor() for _ in range(num_agents)]\n    batch_env = tools.BatchEnv(envs, blocking=not env_processes)\n    batch_env = tools.InGraphBatchEnv(batch_env)\n  return batch_env", "response": "Create environments and apply all desired wrappers."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef define_saver(exclude=None):\n  variables = []\n  exclude = exclude or []\n  exclude = [re.compile(regex) for regex in exclude]\n  for variable in tf.global_variables():\n    if any(regex.match(variable.name) for regex in exclude):\n      continue\n    variables.append(variable)\n  saver = tf.train.Saver(variables, keep_checkpoint_every_n_hours=5)\n  return saver", "response": "Create a saver for the variables we want to checkpoint."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef initialize_variables(sess, saver, logdir, checkpoint=None, resume=None):\n  sess.run(tf.group(\n      tf.local_variables_initializer(),\n      tf.global_variables_initializer()))\n  if resume and not (logdir or checkpoint):\n    raise ValueError('Need to specify logdir to resume a checkpoint.')\n  if logdir:\n    state = tf.train.get_checkpoint_state(logdir)\n    if checkpoint:\n      checkpoint = os.path.join(logdir, checkpoint)\n    if not checkpoint and state and state.model_checkpoint_path:\n      checkpoint = state.model_checkpoint_path\n    if checkpoint and resume is False:\n      message = 'Found unexpected checkpoint when starting a new run.'\n      raise RuntimeError(message)\n    if checkpoint:\n      saver.restore(sess, checkpoint)", "response": "Initialize or restore variables in a checkpoint if available."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef save_config(config, logdir=None):\n  if logdir:\n    with config.unlocked:\n      config.logdir = logdir\n    message = 'Start a new run and write summaries and checkpoints to {}.'\n    tf.logging.info(message.format(config.logdir))\n    tf.gfile.MakeDirs(config.logdir)\n    config_path = os.path.join(config.logdir, 'config.yaml')\n    with tf.gfile.FastGFile(config_path, 'w') as file_:\n      yaml.dump(config, file_, default_flow_style=False)\n  else:\n    message = (\n        'Start a new run without storing summaries and checkpoints since no '\n        'logging directory was specified.')\n    tf.logging.info(message)\n  return config", "response": "Save a new configuration by name."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads a configuration from the log directory.", "response": "def load_config(logdir):\n  # pylint: disable=missing-raises-doc\n  \"\"\"Load a configuration from the log directory.\n\n  Args:\n    logdir: The logging directory containing the configuration file.\n\n  Raises:\n    IOError: The logging directory does not contain a configuration file.\n\n  Returns:\n    Configuration object.\n  \"\"\"\n  config_path = logdir and os.path.join(logdir, 'config.yaml')\n  if not config_path or not tf.gfile.Exists(config_path):\n    message = (\n        'Cannot resume an existing run since the logging directory does not '\n        'contain a configuration file.')\n    raise IOError(message)\n  with tf.gfile.FastGFile(config_path, 'r') as file_:\n    config = yaml.load(file_, Loader=yaml.Loader)\n  message = 'Resume run and write summaries and checkpoints to {}.'\n  tf.logging.info(message.format(config.logdir))\n  return config"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_up_logging():\n  tf.logging.set_verbosity(tf.logging.INFO)\n  logging.getLogger('tensorflow').propagate = False", "response": "Configure the TensorFlow logger."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _create_environment(config, outdir):\n  if isinstance(config.env, str):\n    env = gym.make(config.env)\n  else:\n    env = config.env()\n  # Ensure that the environment has the specification attribute set as expected\n  # by the monitor wrapper.\n  if not hasattr(env, 'spec'):\n    setattr(env, 'spec', getattr(env, 'spec', None))\n  if config.max_length:\n    env = tools.wrappers.LimitDuration(env, config.max_length)\n  env = gym.wrappers.Monitor(\n      env, outdir, lambda unused_episode_number: True)\n  if isinstance(env.action_space, gym.spaces.Box):\n    env = tools.wrappers.RangeNormalize(env)\n    env = tools.wrappers.ClipAction(env)\n  elif isinstance(env.action_space, gym.spaces.Discrete):\n    env = tools.wrappers.RangeNormalize(env, action=False)\n  else:\n    message = \"Unsupported action space '{}'\".format(type(env.action_space))\n    raise NotImplementedError(message)\n  env = tools.wrappers.ConvertTo32Bit(env)\n  env = tools.wrappers.CacheSpaces(env)\n  return env", "response": "Create an instance of the environment."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _define_loop(graph, eval_steps):\n  loop = tools.Loop(\n      None, graph.step, graph.should_log, graph.do_report, graph.force_reset)\n  loop.add_phase(\n      'eval', graph.done, graph.score, graph.summary, eval_steps,\n      report_every=eval_steps,\n      log_every=None,\n      checkpoint_every=None,\n      feed={graph.is_training: False})\n  return loop", "response": "Create and configure an evaluation loop."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsimulate a single cluster of videos.", "response": "def visualize(\n    logdir, outdir, num_agents, num_episodes, checkpoint=None,\n    env_processes=True):\n  \"\"\"Recover checkpoint and render videos from it.\n\n  Args:\n    logdir: Logging directory of the trained algorithm.\n    outdir: Directory to store rendered videos in.\n    num_agents: Number of environments to simulate in parallel.\n    num_episodes: Total number of episodes to simulate.\n    checkpoint: Checkpoint name to load; defaults to most recent.\n    env_processes: Whether to step environments in separate processes.\n  \"\"\"\n  config = utility.load_config(logdir)\n  with tf.device('/cpu:0'):\n    batch_env = utility.define_batch_env(\n        lambda: _create_environment(config, outdir),\n        num_agents, env_processes)\n    graph = utility.define_simulation_graph(\n        batch_env, config.algorithm, config)\n    total_steps = num_episodes * config.max_length\n    loop = _define_loop(graph, total_steps)\n  saver = utility.define_saver(\n      exclude=(r'.*_temporary.*', r'global_step'))\n  sess_config = tf.ConfigProto(allow_soft_placement=True)\n  sess_config.gpu_options.allow_growth = True\n  with tf.Session(config=sess_config) as sess:\n    utility.initialize_variables(\n        sess, saver, config.logdir, checkpoint, resume=True)\n    for unused_score in loop.run(sess, saver, total_steps):\n      pass\n  batch_env.close()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading a trained algorithm and render videos.", "response": "def main(_):\n  \"\"\"Load a trained algorithm and render videos.\"\"\"\n  utility.set_up_logging()\n  if not FLAGS.logdir or not FLAGS.outdir:\n    raise KeyError('You must specify logging and outdirs directories.')\n  FLAGS.logdir = os.path.expanduser(FLAGS.logdir)\n  FLAGS.outdir = os.path.expanduser(FLAGS.outdir)\n  visualize(\n      FLAGS.logdir, FLAGS.outdir, FLAGS.num_agents, FLAGS.num_episodes,\n      FLAGS.checkpoint, FLAGS.env_processes)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nresetting all variables in a nested tuple to zeros.", "response": "def reinit_nested_vars(variables, indices=None):\n  \"\"\"Reset all variables in a nested tuple to zeros.\n\n  Args:\n    variables: Nested tuple or list of variables.\n    indices: Batch indices to reset, defaults to all.\n\n  Returns:\n    Operation.\n  \"\"\"\n  if isinstance(variables, (tuple, list)):\n    return tf.group(*[\n        reinit_nested_vars(variable, indices) for variable in variables])\n  if indices is None:\n    return variables.assign(tf.zeros_like(variables))\n  else:\n    zeros = tf.zeros([tf.shape(indices)[0]] + variables.shape[1:].as_list())\n    return tf.scatter_update(variables, indices, zeros)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nassign tensors to matching nested tuple of variables.", "response": "def assign_nested_vars(variables, tensors, indices=None):\n  \"\"\"Assign tensors to matching nested tuple of variables.\n\n  Args:\n    variables: Nested tuple or list of variables to update.\n    tensors: Nested tuple or list of tensors to assign.\n    indices: Batch indices to assign to; default to all.\n\n  Returns:\n    Operation.\n  \"\"\"\n  if isinstance(variables, (tuple, list)):\n    return tf.group(*[\n        assign_nested_vars(variable, tensor)\n        for variable, tensor in zip(variables, tensors)])\n  if indices is None:\n    return variables.assign(tensors)\n  else:\n    return tf.scatter_update(variables, indices, tensors)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndiscounts Monte - Carlo returns.", "response": "def discounted_return(reward, length, discount):\n  \"\"\"Discounted Monte-Carlo returns.\"\"\"\n  timestep = tf.range(reward.shape[1].value)\n  mask = tf.cast(timestep[None, :] < length[:, None], tf.float32)\n  return_ = tf.reverse(tf.transpose(tf.scan(\n      lambda agg, cur: cur + discount * agg,\n      tf.transpose(tf.reverse(mask * reward, [1]), [1, 0]),\n      tf.zeros_like(reward[:, -1]), 1, False), [1, 0]), [1])\n  return tf.check_numerics(tf.stop_gradient(return_), 'return')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef available_gpus():\n  local_device_protos = device_lib.list_local_devices()\n  return [x.name for x in local_device_protos if x.device_type == 'GPU']", "response": "List of GPU device names detected by TensorFlow."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef gradient_summaries(grad_vars, groups=None, scope='gradients'):\n  groups = groups or {r'all': r'.*'}\n  grouped = collections.defaultdict(list)\n  for grad, var in grad_vars:\n    if grad is None:\n      continue\n    for name, pattern in groups.items():\n      if re.match(pattern, var.name):\n        name = re.sub(pattern, name, var.name)\n        grouped[name].append(grad)\n  for name in groups:\n    if name not in grouped:\n      tf.logging.warn(\"No variables matching '{}' group.\".format(name))\n  summaries = []\n  for name, grads in grouped.items():\n    grads = [tf.reshape(grad, [-1]) for grad in grads]\n    grads = tf.concat(grads, 0)\n    summaries.append(tf.summary.histogram(scope + '/' + name, grads))\n  return tf.summary.merge(summaries)", "response": "Create histogram summaries of the gradient."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates histogram summaries for the provided variables.", "response": "def variable_summaries(vars_, groups=None, scope='weights'):\n  \"\"\"Create histogram summaries for the provided variables.\n\n  Summaries can be grouped via regexes matching variables names.\n\n  Args:\n    vars_: List of variables to summarize.\n    groups: Mapping of name to regex for grouping summaries.\n    scope: Name scope for this operation.\n\n  Returns:\n    Summary tensor.\n  \"\"\"\n  groups = groups or {r'all': r'.*'}\n  grouped = collections.defaultdict(list)\n  for var in vars_:\n    for name, pattern in groups.items():\n      if re.match(pattern, var.name):\n        name = re.sub(pattern, name, var.name)\n        grouped[name].append(var)\n  for name in groups:\n    if name not in grouped:\n      tf.logging.warn(\"No variables matching '{}' group.\".format(name))\n  summaries = []\n  # pylint: disable=redefined-argument-from-local\n  for name, vars_ in grouped.items():\n    vars_ = [tf.reshape(var, [-1]) for var in vars_]\n    vars_ = tf.concat(vars_, 0)\n    summaries.append(tf.summary.histogram(scope + '/' + name, vars_))\n  return tf.summary.merge(summaries)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the length of a tensor along the specified dimension.", "response": "def set_dimension(tensor, axis, value):\n  \"\"\"Set the length of a tensor along the specified dimension.\n\n  Args:\n    tensor: Tensor to define shape of.\n    axis: Dimension to set the static shape for.\n    value: Integer holding the length.\n\n  Raises:\n    ValueError: When the tensor already has a different length specified.\n  \"\"\"\n  shape = tensor.shape.as_list()\n  if shape[axis] not in (value, None):\n    message = 'Cannot set dimension {} of tensor {} to {}; is already {}.'\n    raise ValueError(message.format(axis, tensor.name, value, shape[axis]))\n  shape[axis] = value\n  tensor.set_shape(shape)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef default():\n  # General\n  algorithm = algorithms.PPO\n  num_agents = 30\n  eval_episodes = 30\n  use_gpu = False\n  # Environment\n  normalize_ranges = True\n  # Network\n  network = networks.feed_forward_gaussian\n  weight_summaries = dict(\n      all=r'.*', policy=r'.*/policy/.*', value=r'.*/value/.*')\n  policy_layers = 200, 100\n  value_layers = 200, 100\n  init_output_factor = 0.1\n  init_std = 0.35\n  # Optimization\n  update_every = 30\n  update_epochs = 25\n  optimizer = tf.train.AdamOptimizer\n  learning_rate = 1e-4\n  # Losses\n  discount = 0.995\n  kl_target = 1e-2\n  kl_cutoff_factor = 2\n  kl_cutoff_coef = 1000\n  kl_init_penalty = 1\n  return locals()", "response": "Default configuration for PPO."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef cartpole():\n  locals().update(default())\n  # Environment\n  env = 'CartPole-v1'\n  max_length = 500\n  steps = 2e5  # 200k\n  normalize_ranges = False  # The env reports wrong ranges.\n  # Network\n  network = networks.feed_forward_categorical\n  return locals()", "response": "Configuration for the cart pole classic control task."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef reacher():\n  locals().update(default())\n  # Environment\n  env = 'Reacher-v2'\n  max_length = 1000\n  steps = 5e6  # 5M\n  discount = 0.985\n  update_every = 60\n  return locals()", "response": "Configuration for MuJoCo s reacher task."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef bullet_ant():\n  locals().update(default())\n  # Environment\n  import pybullet_envs  # noqa pylint: disable=unused-import\n  env = 'AntBulletEnv-v0'\n  max_length = 1000\n  steps = 3e7  # 30M\n  update_every = 60\n  return locals()", "response": "Configuration for PyBullet s ant task."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nforwarding a batch of actions to the wrapped environments.", "response": "def step(self, actions):\n    \"\"\"Forward a batch of actions to the wrapped environments.\n\n    Args:\n      actions: Batched action to apply to the environment.\n\n    Raises:\n      ValueError: Invalid actions.\n\n    Returns:\n      Batch of observations, rewards, and done flags.\n    \"\"\"\n    for index, (env, action) in enumerate(zip(self._envs, actions)):\n      if not env.action_space.contains(action):\n        message = 'Invalid action at index {}: {}'\n        raise ValueError(message.format(index, action))\n    if self._blocking:\n      transitions = [\n          env.step(action)\n          for env, action in zip(self._envs, actions)]\n    else:\n      transitions = [\n          env.step(action, blocking=False)\n          for env, action in zip(self._envs, actions)]\n      transitions = [transition() for transition in transitions]\n    observs, rewards, dones, infos = zip(*transitions)\n    observ = np.stack(observs)\n    reward = np.stack(rewards)\n    done = np.stack(dones)\n    info = tuple(infos)\n    return observ, reward, done, info"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef reset(self, indices=None):\n    if indices is None:\n      indices = np.arange(len(self._envs))\n    if self._blocking:\n      observs = [self._envs[index].reset() for index in indices]\n    else:\n      observs = [self._envs[index].reset(blocking=False) for index in indices]\n      observs = [observ() for observ in observs]\n    observ = np.stack(observs)\n    return observ", "response": "Reset the environment and convert the resulting observation."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef simulate(batch_env, algo, log=True, reset=False):\n\n  def _define_begin_episode(agent_indices):\n    \"\"\"Reset environments, intermediate scores and durations for new episodes.\n\n    Args:\n      agent_indices: Tensor containing batch indices starting an episode.\n\n    Returns:\n      Summary tensor.\n    \"\"\"\n    assert agent_indices.shape.ndims == 1\n    zero_scores = tf.zeros_like(agent_indices, tf.float32)\n    zero_durations = tf.zeros_like(agent_indices)\n    reset_ops = [\n        batch_env.reset(agent_indices),\n        tf.scatter_update(score, agent_indices, zero_scores),\n        tf.scatter_update(length, agent_indices, zero_durations)]\n    with tf.control_dependencies(reset_ops):\n      return algo.begin_episode(agent_indices)\n\n  def _define_step():\n    \"\"\"Request actions from the algorithm and apply them to the environments.\n\n    Increments the lengths of all episodes and increases their scores by the\n    current reward. After stepping the environments, provides the full\n    transition tuple to the algorithm.\n\n    Returns:\n      Summary tensor.\n    \"\"\"\n    prevob = batch_env.observ + 0  # Ensure a copy of the variable value.\n    agent_indices = tf.range(len(batch_env))\n    action, step_summary = algo.perform(agent_indices, prevob)\n    action.set_shape(batch_env.action.shape)\n    with tf.control_dependencies([batch_env.simulate(action)]):\n      add_score = score.assign_add(batch_env.reward)\n      inc_length = length.assign_add(tf.ones(len(batch_env), tf.int32))\n    with tf.control_dependencies([add_score, inc_length]):\n      agent_indices = tf.range(len(batch_env))\n      experience_summary = algo.experience(\n          agent_indices, prevob, batch_env.action, batch_env.reward,\n          batch_env.done, batch_env.observ)\n    return tf.summary.merge([step_summary, experience_summary])\n\n  def _define_end_episode(agent_indices):\n    \"\"\"Notify the algorithm of ending episodes.\n\n    Also updates the mean score and length counters used for summaries.\n\n    Args:\n      agent_indices: Tensor holding batch indices that end their episodes.\n\n    Returns:\n      Summary tensor.\n    \"\"\"\n    assert agent_indices.shape.ndims == 1\n    submit_score = mean_score.submit(tf.gather(score, agent_indices))\n    submit_length = mean_length.submit(\n        tf.cast(tf.gather(length, agent_indices), tf.float32))\n    with tf.control_dependencies([submit_score, submit_length]):\n      return algo.end_episode(agent_indices)\n\n  def _define_summaries():\n    \"\"\"Reset the average score and duration, and return them as summary.\n\n    Returns:\n      Summary string.\n    \"\"\"\n    score_summary = tf.cond(\n        tf.logical_and(log, tf.cast(mean_score.count, tf.bool)),\n        lambda: tf.summary.scalar('mean_score', mean_score.clear()), str)\n    length_summary = tf.cond(\n        tf.logical_and(log, tf.cast(mean_length.count, tf.bool)),\n        lambda: tf.summary.scalar('mean_length', mean_length.clear()), str)\n    return tf.summary.merge([score_summary, length_summary])\n\n  with tf.name_scope('simulate'):\n    log = tf.convert_to_tensor(log)\n    reset = tf.convert_to_tensor(reset)\n    with tf.variable_scope('simulate_temporary'):\n      score = tf.Variable(\n          lambda: tf.zeros(len(batch_env), dtype=tf.float32),\n          trainable=False, name='score')\n      length = tf.Variable(\n          lambda: tf.zeros(len(batch_env), dtype=tf.int32),\n          trainable=False, name='length')\n    mean_score = streaming_mean.StreamingMean((), tf.float32)\n    mean_length = streaming_mean.StreamingMean((), tf.float32)\n    agent_indices = tf.cond(\n        reset,\n        lambda: tf.range(len(batch_env)),\n        lambda: tf.cast(tf.where(batch_env.done)[:, 0], tf.int32))\n    begin_episode = tf.cond(\n        tf.cast(tf.shape(agent_indices)[0], tf.bool),\n        lambda: _define_begin_episode(agent_indices), str)\n    with tf.control_dependencies([begin_episode]):\n      step = _define_step()\n    with tf.control_dependencies([step]):\n      agent_indices = tf.cast(tf.where(batch_env.done)[:, 0], tf.int32)\n      end_episode = tf.cond(\n          tf.cast(tf.shape(agent_indices)[0], tf.bool),\n          lambda: _define_end_episode(agent_indices), str)\n    with tf.control_dependencies([end_episode]):\n      summary = tf.summary.merge([\n          _define_summaries(), begin_episode, step, end_episode])\n    with tf.control_dependencies([summary]):\n      done, score = tf.identity(batch_env.done), tf.identity(score)\n    return done, score, summary", "response": "A vectorized simulation step of a vectorized algorithm with in - graph environments."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef close(self):\n    try:\n      self._conn.send((self._CLOSE, None))\n      self._conn.close()\n    except IOError:\n      # The connection was already closed.\n      pass\n    self._process.join()", "response": "Send a close message to the external process and join it."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef step(self, action, blocking=True):\n    promise = self.call('step', action)\n    if blocking:\n      return promise()\n    else:\n      return promise", "response": "Step the environment.\n\n    Args:\n      action: The action to apply to the environment.\n      blocking: Whether to wait for the result.\n\n    Returns:\n      Transition tuple when blocking, otherwise callable that returns the\n      transition tuple."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreset the environment. Args: blocking: Whether to wait for the result. Returns: New observation when blocking, otherwise callable that returns the new observation.", "response": "def reset(self, blocking=True):\n    \"\"\"Reset the environment.\n\n    Args:\n      blocking: Whether to wait for the result.\n\n    Returns:\n      New observation when blocking, otherwise callable that returns the new\n      observation.\n    \"\"\"\n    promise = self.call('reset')\n    if blocking:\n      return promise()\n    else:\n      return promise"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _receive(self):\n    message, payload = self._conn.recv()\n    # Re-raise exceptions in the main process.\n    if message == self._EXCEPTION:\n      stacktrace = payload\n      raise Exception(stacktrace)\n    if message == self._RESULT:\n      return payload\n    raise KeyError('Received message of unexpected type {}'.format(message))", "response": "Wait for a message from the worker process and return its payload."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nforward an action to the wrapped environment.", "response": "def step(self, action):\n    \"\"\"Forward action to the wrapped environment.\n\n    Args:\n      action: Action to apply to the environment.\n\n    Raises:\n      ValueError: Invalid action.\n\n    Returns:\n      Converted observation, converted reward, done flag, and info object.\n    \"\"\"\n    observ, reward, done, info = self._env.step(action)\n    observ = self._convert_observ(observ)\n    reward = self._convert_reward(reward)\n    return observ, reward, done, info"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nresets the environment and convert the resulting observation.", "response": "def reset(self):\n    \"\"\"Reset the environment and convert the resulting observation.\n\n    Returns:\n      Converted observation.\n    \"\"\"\n    observ = self._env.reset()\n    observ = self._convert_observ(observ)\n    return observ"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _convert_observ(self, observ):\n    if not np.isfinite(observ).all():\n      raise ValueError('Infinite observation encountered.')\n    if observ.dtype == np.float64:\n      return observ.astype(np.float32)\n    if observ.dtype == np.int64:\n      return observ.astype(np.int32)\n    return observ", "response": "Convert the observation to 32 bits."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _convert_reward(self, reward):\n    if not np.isfinite(reward).all():\n      raise ValueError('Infinite reward encountered.')\n    return np.array(reward, dtype=np.float32)", "response": "Convert the reward to 32 bits."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef value(self):\n    return self._sum / tf.cast(self._count, self._dtype)", "response": "The current value of the mean."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsubmit a single or batch tensor to refine the streaming mean.", "response": "def submit(self, value):\n    \"\"\"Submit a single or batch tensor to refine the streaming mean.\"\"\"\n    # Add a batch dimension if necessary.\n    if value.shape.ndims == self._sum.shape.ndims:\n      value = value[None, ...]\n    return tf.group(\n        self._sum.assign_add(tf.reduce_sum(value, 0)),\n        self._count.assign_add(tf.shape(value)[0]))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef clear(self):\n    value = self._sum / tf.cast(self._count, self._dtype)\n    with tf.control_dependencies([value]):\n      reset_value = self._sum.assign(tf.zeros_like(self._sum))\n      reset_count = self._count.assign(0)\n    with tf.control_dependencies([reset_value, reset_count]):\n      return tf.identity(value)", "response": "Return the mean estimate and reset the streaming statistics."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncombining corresponding elements in multiple nested structures into tuples.", "response": "def zip_(*structures, **kwargs):\n  # pylint: disable=differing-param-doc,missing-param-doc\n  \"\"\"Combine corresponding elements in multiple nested structure to tuples.\n\n  The nested structures can consist of any combination of lists, tuples, and\n  dicts. All provided structures must have the same nesting.\n\n  Args:\n    *structures: Nested structures.\n    flatten: Whether to flatten the resulting structure into a tuple. Keys of\n        dictionaries will be discarded.\n\n  Returns:\n    Nested structure.\n  \"\"\"\n  # Named keyword arguments are not allowed after *args in Python 2.\n  flatten = kwargs.pop('flatten', False)\n  assert not kwargs, 'zip() got unexpected keyword arguments.'\n  return map(\n      lambda *x: x if len(x) > 1 else x[0],\n      *structures,\n      flatten=flatten)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef map_(function, *structures, **kwargs):\n  # pylint: disable=differing-param-doc,missing-param-doc\n  \"\"\"Apply a function to every element in a nested structure.\n\n  If multiple structures are provided as input, their structure must match and\n  the function will be applied to corresponding groups of elements. The nested\n  structure can consist of any combination of lists, tuples, and dicts.\n\n  Args:\n    function: The function to apply to the elements of the structure. Receives\n        one argument for every structure that is provided.\n    *structures: One of more nested structures.\n    flatten: Whether to flatten the resulting structure into a tuple. Keys of\n        dictionaries will be discarded.\n\n  Returns:\n    Nested structure.\n  \"\"\"\n  # Named keyword arguments are not allowed after *args in Python 2.\n  flatten = kwargs.pop('flatten', False)\n  assert not kwargs, 'map() got unexpected keyword arguments.'\n\n  def impl(function, *structures):\n    if len(structures) == 0:  # pylint: disable=len-as-condition\n      return structures\n    if all(isinstance(s, (tuple, list)) for s in structures):\n      if len(set(len(x) for x in structures)) > 1:\n        raise ValueError('Cannot merge tuples or lists of different length.')\n      args = tuple((impl(function, *x) for x in _builtin_zip(*structures)))\n      if hasattr(structures[0], '_fields'):  # namedtuple\n        return type(structures[0])(*args)\n      else:  # tuple, list\n        return type(structures[0])(args)\n    if all(isinstance(s, dict) for s in structures):\n      if len(set(frozenset(x.keys()) for x in structures)) > 1:\n        raise ValueError('Cannot merge dicts with different keys.')\n      merged = {\n          k: impl(function, *(s[k] for s in structures))\n          for k in structures[0]}\n      return type(structures[0])(merged)\n    return function(*structures)\n\n  result = impl(function, *structures)\n  if flatten:\n    result = flatten_(result)\n  return result", "response": "Applies a function to every element in a nested structure."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef flatten_(structure):\n  if isinstance(structure, dict):\n    if structure:\n      structure = zip(*sorted(structure.items(), key=lambda x: x[0]))[1]\n    else:\n      # Zip doesn't work on an the items of an empty dictionary.\n      structure = ()\n  if isinstance(structure, (tuple, list)):\n    result = []\n    for element in structure:\n      result += flatten_(element)\n    return tuple(result)\n  return (structure,)", "response": "Combine all leaves of a nested structure into a flat tuple."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nselecting elements of a nested structure based on a predicate function.", "response": "def filter_(predicate, *structures, **kwargs):\n  # pylint: disable=differing-param-doc,missing-param-doc, too-many-branches\n  \"\"\"Select elements of a nested structure based on a predicate function.\n\n  If multiple structures are provided as input, their structure must match and\n  the function will be applied to corresponding groups of elements. The nested\n  structure can consist of any combination of lists, tuples, and dicts.\n\n  Args:\n    predicate: The function to determine whether an element should be kept.\n        Receives one argument for every structure that is provided.\n    *structures: One of more nested structures.\n    flatten: Whether to flatten the resulting structure into a tuple. Keys of\n        dictionaries will be discarded.\n\n  Returns:\n    Nested structure.\n  \"\"\"\n  # Named keyword arguments are not allowed after *args in Python 2.\n  flatten = kwargs.pop('flatten', False)\n  assert not kwargs, 'filter() got unexpected keyword arguments.'\n\n  def impl(predicate, *structures):\n    if len(structures) == 0:  # pylint: disable=len-as-condition\n      return structures\n    if all(isinstance(s, (tuple, list)) for s in structures):\n      if len(set(len(x) for x in structures)) > 1:\n        raise ValueError('Cannot merge tuples or lists of different length.')\n      # Only wrap in tuples if more than one structure provided.\n      if len(structures) > 1:\n        filtered = (impl(predicate, *x) for x in _builtin_zip(*structures))\n      else:\n        filtered = (impl(predicate, x) for x in structures[0])\n      # Remove empty containers and construct result structure.\n      if hasattr(structures[0], '_fields'):  # namedtuple\n        filtered = (x if x != () else None for x in filtered)\n        return type(structures[0])(*filtered)\n      else:  # tuple, list\n        filtered = (\n            x for x in filtered if not isinstance(x, (tuple, list, dict)) or x)\n        return type(structures[0])(filtered)\n    if all(isinstance(s, dict) for s in structures):\n      if len(set(frozenset(x.keys()) for x in structures)) > 1:\n        raise ValueError('Cannot merge dicts with different keys.')\n      # Only wrap in tuples if more than one structure provided.\n      if len(structures) > 1:\n        filtered = {\n            k: impl(predicate, *(s[k] for s in structures))\n            for k in structures[0]}\n      else:\n        filtered = {k: impl(predicate, v) for k, v in structures[0].items()}\n      # Remove empty containers and construct result structure.\n      filtered = {\n          k: v for k, v in filtered.items()\n          if not isinstance(v, (tuple, list, dict)) or v}\n      return type(structures[0])(filtered)\n    if len(structures) > 1:\n      return structures if predicate(*structures) else ()\n    else:\n      return structures[0] if predicate(structures[0]) else ()\n\n  result = impl(predicate, *structures)\n  if flatten:\n    result = flatten_(result)\n  return result"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_phase(\n      self, name, done, score, summary, steps,\n      report_every=None, log_every=None, checkpoint_every=None, feed=None):\n    \"\"\"Add a phase to the loop protocol.\n\n    If the model breaks long computation into multiple steps, the done tensor\n    indicates whether the current score should be added to the mean counter.\n    For example, in reinforcement learning we only have a valid score at the\n    end of the episode.\n\n    Score and done tensors can either be scalars or vectors, to support\n    single and batched computations.\n\n    Args:\n      name: Name for the phase, used for the summary writer.\n      done: Tensor indicating whether current score can be used.\n      score: Tensor holding the current, possibly intermediate, score.\n      summary: Tensor holding summary string to write if not an empty string.\n      steps: Duration of the phase in steps.\n      report_every: Yield mean score every this number of steps.\n      log_every: Request summaries via `log` tensor every this number of steps.\n      checkpoint_every: Write checkpoint every this number of steps.\n      feed: Additional feed dictionary for the session run call.\n\n    Raises:\n      ValueError: Unknown rank for done or score tensors.\n    \"\"\"\n    done = tf.convert_to_tensor(done, tf.bool)\n    score = tf.convert_to_tensor(score, tf.float32)\n    summary = tf.convert_to_tensor(summary, tf.string)\n    feed = feed or {}\n    if done.shape.ndims is None or score.shape.ndims is None:\n      raise ValueError(\"Rank of 'done' and 'score' tensors must be known.\")\n    writer = self._logdir and tf.summary.FileWriter(\n        os.path.join(self._logdir, name), tf.get_default_graph(),\n        flush_secs=60)\n    op = self._define_step(done, score, summary)\n    batch = 1 if score.shape.ndims == 0 else score.shape[0].value\n    self._phases.append(_Phase(\n        name, writer, op, batch, int(steps), feed, report_every,\n        log_every, checkpoint_every))", "response": "Adds a phase to the loop protocol."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run(self, sess, saver, max_step=None):\n    global_step = sess.run(self._step)\n    steps_made = 1\n    while True:\n      if max_step and global_step >= max_step:\n        break\n      phase, epoch, steps_in = self._find_current_phase(global_step)\n      phase_step = epoch * phase.steps + steps_in\n      if steps_in % phase.steps < steps_made:\n        message = '\\n' + ('-' * 50) + '\\n'\n        message += 'Phase {} (phase step {}, global step {}).'\n        tf.logging.info(message.format(phase.name, phase_step, global_step))\n      # Populate book keeping tensors.\n      phase.feed[self._reset] = (steps_in < steps_made)\n      phase.feed[self._log] = (\n          phase.writer and\n          self._is_every_steps(phase_step, phase.batch, phase.log_every))\n      phase.feed[self._report] = (\n          self._is_every_steps(phase_step, phase.batch, phase.report_every))\n      summary, mean_score, global_step, steps_made = sess.run(\n          phase.op, phase.feed)\n      if self._is_every_steps(phase_step, phase.batch, phase.checkpoint_every):\n        self._store_checkpoint(sess, saver, global_step)\n      if self._is_every_steps(phase_step, phase.batch, phase.report_every):\n        yield mean_score\n      if summary and phase.writer:\n        # We want smaller phases to catch up at the beginnig of each epoch so\n        # that their graphs are aligned.\n        longest_phase = max(phase.steps for phase in self._phases)\n        summary_step = epoch * longest_phase + steps_in\n        phase.writer.add_summary(summary, summary_step)", "response": "Run the loop schedule for a specified number of steps."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndetermines whether a periodic event should happen at this step.", "response": "def _is_every_steps(self, phase_step, batch, every):\n    \"\"\"Determine whether a periodic event should happen at this step.\n\n    Args:\n      phase_step: The incrementing step.\n      batch: The number of steps progressed at once.\n      every: The interval of the period.\n\n    Returns:\n      Boolean of whether the event should happen.\n    \"\"\"\n    if not every:\n      return False\n    covered_steps = range(phase_step, phase_step + batch)\n    return any((step + 1) % every == 0 for step in covered_steps)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndetermining the current phase based on the global step.", "response": "def _find_current_phase(self, global_step):\n    \"\"\"Determine the current phase based on the global step.\n\n    This ensures continuing the correct phase after restoring checkoints.\n\n    Args:\n      global_step: The global number of steps performed across all phases.\n\n    Returns:\n      Tuple of phase object, epoch number, and phase steps within the epoch.\n    \"\"\"\n    epoch_size = sum(phase.steps for phase in self._phases)\n    epoch = int(global_step // epoch_size)\n    steps_in = global_step % epoch_size\n    for phase in self._phases:\n      if steps_in < phase.steps:\n        return phase, epoch, steps_in\n      steps_in -= phase.steps"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndefine a global step for the phase.", "response": "def _define_step(self, done, score, summary):\n    \"\"\"Combine operations of a phase.\n\n    Keeps track of the mean score and when to report it.\n\n    Args:\n      done: Tensor indicating whether current score can be used.\n      score: Tensor holding the current, possibly intermediate, score.\n      summary: Tensor holding summary string to write if not an empty string.\n\n    Returns:\n      Tuple of summary tensor, mean score, and new global step. The mean score\n      is zero for non reporting steps.\n    \"\"\"\n    if done.shape.ndims == 0:\n      done = done[None]\n    if score.shape.ndims == 0:\n      score = score[None]\n    score_mean = streaming_mean.StreamingMean((), tf.float32)\n    with tf.control_dependencies([done, score, summary]):\n      done_score = tf.gather(score, tf.where(done)[:, 0])\n      submit_score = tf.cond(\n          tf.reduce_any(done), lambda: score_mean.submit(done_score), tf.no_op)\n    with tf.control_dependencies([submit_score]):\n      mean_score = tf.cond(self._report, score_mean.clear, float)\n      steps_made = tf.shape(score)[0]\n      next_step = self._step.assign_add(steps_made)\n    with tf.control_dependencies([mean_score, next_step]):\n      return tf.identity(summary), mean_score, next_step, steps_made"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _store_checkpoint(self, sess, saver, global_step):\n    if not self._logdir or not saver:\n      return\n    tf.gfile.MakeDirs(self._logdir)\n    filename = os.path.join(self._logdir, 'model.ckpt')\n    saver.save(sess, filename, global_step)", "response": "Store a checkpoint if a log directory was provided to the constructor."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _create_environment(config):\n  if isinstance(config.env, str):\n    env = gym.make(config.env)\n  else:\n    env = config.env()\n  if config.max_length:\n    env = tools.wrappers.LimitDuration(env, config.max_length)\n  if isinstance(env.action_space, gym.spaces.Box):\n    if config.normalize_ranges:\n      env = tools.wrappers.RangeNormalize(env)\n    env = tools.wrappers.ClipAction(env)\n  elif isinstance(env.action_space, gym.spaces.Discrete):\n    if config.normalize_ranges:\n      env = tools.wrappers.RangeNormalize(env, action=False)\n  else:\n    message = \"Unsupported action space '{}'\".format(type(env.action_space))\n    raise NotImplementedError(message)\n  env = tools.wrappers.ConvertTo32Bit(env)\n  env = tools.wrappers.CacheSpaces(env)\n  return env", "response": "Create an instance of the environment."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _define_loop(graph, logdir, train_steps, eval_steps):\n  loop = tools.Loop(\n      logdir, graph.step, graph.should_log, graph.do_report,\n      graph.force_reset)\n  loop.add_phase(\n      'train', graph.done, graph.score, graph.summary, train_steps,\n      report_every=train_steps,\n      log_every=train_steps // 2,\n      checkpoint_every=None,\n      feed={graph.is_training: True})\n  loop.add_phase(\n      'eval', graph.done, graph.score, graph.summary, eval_steps,\n      report_every=eval_steps,\n      log_every=eval_steps // 2,\n      checkpoint_every=10 * eval_steps,\n      feed={graph.is_training: False})\n  return loop", "response": "Create and configure a training loop with training and evaluation phases."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntraining and evaluate the current is archive.", "response": "def train(config, env_processes):\n  \"\"\"Training and evaluation entry point yielding scores.\n\n  Resolves some configuration attributes, creates environments, graph, and\n  training loop. By default, assigns all operations to the CPU.\n\n  Args:\n    config: Object providing configurations via attributes.\n    env_processes: Whether to step environments in separate processes.\n\n  Yields:\n    Evaluation scores.\n  \"\"\"\n  tf.reset_default_graph()\n  if config.update_every % config.num_agents:\n    tf.logging.warn('Number of agents should divide episodes per update.')\n  with tf.device('/cpu:0'):\n    batch_env = utility.define_batch_env(\n        lambda: _create_environment(config),\n        config.num_agents, env_processes)\n    graph = utility.define_simulation_graph(\n        batch_env, config.algorithm, config)\n    loop = _define_loop(\n        graph, config.logdir,\n        config.update_every * config.max_length,\n        config.eval_episodes * config.max_length)\n    total_steps = int(\n        config.steps / config.update_every *\n        (config.update_every + config.eval_episodes))\n  # Exclude episode related variables since the Python state of environments is\n  # not checkpointed and thus new episodes start after resuming.\n  saver = utility.define_saver(exclude=(r'.*_temporary.*',))\n  sess_config = tf.ConfigProto(allow_soft_placement=True)\n  sess_config.gpu_options.allow_growth = True\n  with tf.Session(config=sess_config) as sess:\n    utility.initialize_variables(sess, saver, config.logdir)\n    for score in loop.run(sess, saver, total_steps):\n      yield score\n  batch_env.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef main(_):\n  utility.set_up_logging()\n  if not FLAGS.config:\n    raise KeyError('You must specify a configuration.')\n  logdir = FLAGS.logdir and os.path.expanduser(os.path.join(\n      FLAGS.logdir, '{}-{}'.format(FLAGS.timestamp, FLAGS.config)))\n  try:\n    config = utility.load_config(logdir)\n  except IOError:\n    config = tools.AttrDict(getattr(configs, FLAGS.config)())\n    config = utility.save_config(config, logdir)\n  for score in train(config, FLAGS.env_processes):\n    tf.logging.info('Score {}.'.format(score))", "response": "Create or load configuration and launch the trainer."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\niterating over nested sequences.", "response": "def iterate_sequences(\n    consumer_fn, output_template, sequences, length, chunk_length=None,\n    batch_size=None, num_epochs=1, padding_value=0):\n  \"\"\"Iterate over batches of chunks of sequences for multiple epochs.\n\n  The batch dimension of the length tensor must be set because it is used to\n  infer buffer sizes.\n\n  Args:\n    consumer_fn: Function creating the operation to process the data.\n    output_template: Nested tensors of same shape and dtype as outputs.\n    sequences: Nested collection of tensors with batch and time dimension.\n    length: Tensor containing the length for each sequence.\n    chunk_length: Split sequences into chunks of this size; optional.\n    batch_size: Split epochs into batches of this size; optional.\n    num_epochs: How many times to repeat over the data.\n    padding_value: Value used for padding the last chunk after the sequence.\n\n  Raises:\n    ValueError: Unknown batch size of the length tensor.\n\n  Returns:\n    Concatenated nested tensors returned by the consumer.\n  \"\"\"\n  if not length.shape[0].value:\n    raise ValueError('Batch size of length tensor must be set.')\n  num_sequences = length.shape[0].value\n  sequences = dict(sequence=sequences, length=length)\n  dataset = tf.data.Dataset.from_tensor_slices(sequences)\n  dataset = dataset.repeat(num_epochs)\n  if chunk_length:\n    dataset = dataset.map(remove_padding).flat_map(\n        # pylint: disable=g-long-lambda\n        lambda x: tf.data.Dataset.from_tensor_slices(\n            chunk_sequence(x, chunk_length, padding_value)))\n    num_chunks = tf.reduce_sum((length - 1) // chunk_length + 1)\n  else:\n    num_chunks = num_sequences\n  if batch_size:\n    dataset = dataset.shuffle(num_sequences // 2)\n  dataset = dataset.batch(batch_size or num_sequences)\n  dataset = dataset.prefetch(num_epochs)\n  iterator = dataset.make_initializable_iterator()\n  with tf.control_dependencies([iterator.initializer]):\n    num_batches = num_epochs * num_chunks // (batch_size or num_sequences)\n    return tf.scan(\n        # pylint: disable=g-long-lambda\n        lambda _1, index: consumer_fn(iterator.get_next()),\n        tf.range(num_batches), output_template, parallel_iterations=1)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef chunk_sequence(sequence, chunk_length=200, padding_value=0):\n  if 'length' in sequence:\n    length = sequence.pop('length')\n  else:\n    length = tf.shape(tools.nested.flatten(sequence)[0])[0]\n  num_chunks = (length - 1) // chunk_length + 1\n  padding_length = chunk_length * num_chunks - length\n  padded = tools.nested.map(\n      # pylint: disable=g-long-lambda\n      lambda tensor: tf.concat([\n          tensor, 0 * tensor[:padding_length] + padding_value], 0),\n      sequence)\n  chunks = tools.nested.map(\n      # pylint: disable=g-long-lambda\n      lambda tensor: tf.reshape(\n          tensor, [num_chunks, chunk_length] + tensor.shape[1:].as_list()),\n      padded)\n  chunks['length'] = tf.concat([\n      chunk_length * tf.ones((num_chunks - 1,), dtype=tf.int32),\n      [chunk_length - padding_length]], 0)\n  return chunks", "response": "Splits a nested dict of sequence tensors into a batch of chunks."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nremove the padding elements from the sequence.", "response": "def remove_padding(sequence):\n  \"\"\"Selects the used frames of a sequence, up to its length.\n\n  This function does not expect a batch of sequences, but a single sequence.\n  The sequence must be a dict with `length` key, which will removed from the\n  result.\n\n  Args:\n    sequence: Nested dict of tensors with time dimension.\n\n  Returns:\n    Nested dict of tensors with padding elements and `length` key removed.\n  \"\"\"\n  length = sequence.pop('length')\n  sequence = tools.nested.map(lambda tensor: tensor[:length], sequence)\n  return sequence"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nnormalizes a single or batch tensor.", "response": "def transform(self, value):\n    \"\"\"Normalize a single or batch tensor.\n\n    Applies the activated transformations in the constructor using current\n    estimates of mean and variance.\n\n    Args:\n      value: Batch or single value tensor.\n\n    Returns:\n      Normalized batch or single value tensor.\n    \"\"\"\n    with tf.name_scope(self._name + '/transform'):\n      no_batch_dim = value.shape.ndims == self._mean.shape.ndims\n      if no_batch_dim:\n        # Add a batch dimension if necessary.\n        value = value[None, ...]\n      if self._center:\n        value -= self._mean[None, ...]\n      if self._scale:\n        # We cannot scale before seeing at least two samples.\n        value /= tf.cond(\n            self._count > 1, lambda: self._std() + 1e-8,\n            lambda: tf.ones_like(self._var_sum))[None]\n      if self._clip:\n        value = tf.clip_by_value(value, -self._clip, self._clip)\n      # Remove batch dimension if necessary.\n      if no_batch_dim:\n        value = value[0]\n      return tf.check_numerics(value, 'value')"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update(self, value):\n    with tf.name_scope(self._name + '/update'):\n      if value.shape.ndims == self._mean.shape.ndims:\n        # Add a batch dimension if necessary.\n        value = value[None, ...]\n      count = tf.shape(value)[0]\n      with tf.control_dependencies([self._count.assign_add(count)]):\n        step = tf.cast(self._count, tf.float32)\n        mean_delta = tf.reduce_sum(value - self._mean[None, ...], 0)\n        new_mean = self._mean + mean_delta / step\n        new_mean = tf.cond(self._count > 1, lambda: new_mean, lambda: value[0])\n        var_delta = (\n            value - self._mean[None, ...]) * (value - new_mean[None, ...])\n        new_var_sum = self._var_sum + tf.reduce_sum(var_delta, 0)\n      with tf.control_dependencies([new_mean, new_var_sum]):\n        update = self._mean.assign(new_mean), self._var_sum.assign(new_var_sum)\n      with tf.control_dependencies(update):\n        if value.shape.ndims == 1:\n          value = tf.reduce_mean(value)\n        return self._summary('value', tf.reduce_mean(value))", "response": "Updates the mean and variance estimates."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nresets the estimates of mean and variance.", "response": "def reset(self):\n    \"\"\"Reset the estimates of mean and variance.\n\n    Resets the full state of this class.\n\n    Returns:\n      Operation.\n    \"\"\"\n    with tf.name_scope(self._name + '/reset'):\n      return tf.group(\n          self._count.assign(0),\n          self._mean.assign(tf.zeros_like(self._mean)),\n          self._var_sum.assign(tf.zeros_like(self._var_sum)))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing the current estimate of the standard deviation.", "response": "def _std(self):\n    \"\"\"Computes the current estimate of the standard deviation.\n\n    Note that the standard deviation is not defined until at least two samples\n    were seen.\n\n    Returns:\n      Tensor of current variance.\n    \"\"\"\n    variance = tf.cond(\n        self._count > 1,\n        lambda: self._var_sum / tf.cast(self._count - 1, tf.float32),\n        lambda: tf.ones_like(self._var_sum) * float('nan'))\n    # The epsilon corrects for small negative variance values caused by\n    # the algorithm. It was empirically chosen to work with all environments\n    # tested.\n    return tf.sqrt(variance + 1e-4)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a scalar or histogram summary matching the rank of the tensor.", "response": "def _summary(self, name, tensor):\n    \"\"\"Create a scalar or histogram summary matching the rank of the tensor.\n\n    Args:\n      name: Name for the summary.\n      tensor: Tensor to summarize.\n\n    Returns:\n      Summary tensor.\n    \"\"\"\n    if tensor.shape.ndims == 0:\n      return tf.summary.scalar(name, tensor)\n    else:\n      return tf.summary.histogram(name, tensor)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a batch tensor holding the current length of episodes.", "response": "def length(self, rows=None):\n    \"\"\"Tensor holding the current length of episodes.\n\n    Args:\n      rows: Episodes to select length from, defaults to all.\n\n    Returns:\n      Batch tensor of sequence lengths.\n    \"\"\"\n    rows = tf.range(self._capacity) if rows is None else rows\n    return tf.gather(self._length, rows)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef append(self, transitions, rows=None):\n    rows = tf.range(self._capacity) if rows is None else rows\n    assert rows.shape.ndims == 1\n    assert_capacity = tf.assert_less(\n        rows, self._capacity,\n        message='capacity exceeded')\n    with tf.control_dependencies([assert_capacity]):\n      assert_max_length = tf.assert_less(\n          tf.gather(self._length, rows), self._max_length,\n          message='max length exceeded')\n    with tf.control_dependencies([assert_max_length]):\n      timestep = tf.gather(self._length, rows)\n      indices = tf.stack([rows, timestep], 1)\n      append_ops = tools.nested.map(\n          lambda var, val: tf.scatter_nd_update(var, indices, val),\n          self._buffers, transitions, flatten=True)\n    with tf.control_dependencies(append_ops):\n      episode_mask = tf.reduce_sum(tf.one_hot(\n          rows, self._capacity, dtype=tf.int32), 0)\n      return self._length.assign_add(episode_mask)", "response": "Appends a batch of transitions to rows of the memory."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef replace(self, episodes, length, rows=None):\n    rows = tf.range(self._capacity) if rows is None else rows\n    assert rows.shape.ndims == 1\n    assert_capacity = tf.assert_less(\n        rows, self._capacity, message='capacity exceeded')\n    with tf.control_dependencies([assert_capacity]):\n      assert_max_length = tf.assert_less_equal(\n          length, self._max_length, message='max length exceeded')\n    with tf.control_dependencies([assert_max_length]):\n      replace_ops = tools.nested.map(\n          lambda var, val: tf.scatter_update(var, rows, val),\n          self._buffers, episodes, flatten=True)\n    with tf.control_dependencies(replace_ops):\n      return tf.scatter_update(self._length, rows, length)", "response": "Replace full episodes with new ones."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\naccess a batch of episodes from the memory.", "response": "def data(self, rows=None):\n    \"\"\"Access a batch of episodes from the memory.\n\n    Padding elements after the length of each episode are unspecified and might\n    contain old data.\n\n    Args:\n      rows: Episodes to select, defaults to all.\n\n    Returns:\n      Tuple containing a tuple of transition quantities with batch and time\n      dimensions, and a batch of sequence lengths.\n    \"\"\"\n    rows = tf.range(self._capacity) if rows is None else rows\n    assert rows.shape.ndims == 1\n    episode = tools.nested.map(lambda var: tf.gather(var, rows), self._buffers)\n    length = tf.gather(self._length, rows)\n    return episode, length"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef clear(self, rows=None):\n    rows = tf.range(self._capacity) if rows is None else rows\n    assert rows.shape.ndims == 1\n    return tf.scatter_update(self._length, rows, tf.zeros_like(rows))", "response": "Reset the episodes in the memory."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef reset(self):\n    observ_dtype = self._parse_dtype(self._env.observation_space)\n    observ = tf.py_func(self._env.reset, [], observ_dtype, name='reset')\n    observ = tf.check_numerics(observ, 'observ')\n    with tf.control_dependencies([\n        self._observ.assign(observ),\n        self._reward.assign(0),\n        self._done.assign(False)]):\n      return tf.identity(observ)", "response": "Reset the environment.\n\n    Returns:\n      Tensor of the current observation."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _parse_shape(self, space):\n    if isinstance(space, gym.spaces.Discrete):\n      return ()\n    if isinstance(space, gym.spaces.Box):\n      return space.shape\n    raise NotImplementedError()", "response": "Returns a tensor shape tuple from a OpenAI Gym space."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _parse_dtype(self, space):\n    if isinstance(space, gym.spaces.Discrete):\n      return tf.int32\n    if isinstance(space, gym.spaces.Box):\n      return tf.float32\n    raise NotImplementedError()", "response": "Returns a TensorFlow data type from a OpenAI Gym space."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef simulate(self, action):\n    with tf.name_scope('environment/simulate'):\n      if action.dtype in (tf.float16, tf.float32, tf.float64):\n        action = tf.check_numerics(action, 'action')\n      observ_dtype = self._parse_dtype(self._batch_env.observation_space)\n      observ, reward, done = tf.py_func(\n          lambda a: self._batch_env.step(a)[:3], [action],\n          [observ_dtype, tf.float32, tf.bool], name='step')\n      observ = tf.check_numerics(observ, 'observ')\n      reward = tf.check_numerics(reward, 'reward')\n      return tf.group(\n          self._observ.assign(observ),\n          self._action.assign(action),\n          self._reward.assign(reward),\n          self._done.assign(done))", "response": "Step the batch of environments."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef reset(self, indices=None):\n    if indices is None:\n      indices = tf.range(len(self._batch_env))\n    observ_dtype = self._parse_dtype(self._batch_env.observation_space)\n    observ = tf.py_func(\n        self._batch_env.reset, [indices], observ_dtype, name='reset')\n    observ = tf.check_numerics(observ, 'observ')\n    reward = tf.zeros_like(indices, tf.float32)\n    done = tf.zeros_like(indices, tf.bool)\n    with tf.control_dependencies([\n        tf.scatter_update(self._observ, indices, observ),\n        tf.scatter_update(self._reward, indices, reward),\n        tf.scatter_update(self._done, indices, done)]):\n      return tf.identity(observ)", "response": "Reset the batch of environments."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nresets the recurrent states and stored episode.", "response": "def begin_episode(self, agent_indices):\n    \"\"\"Reset the recurrent states and stored episode.\n\n    Args:\n      agent_indices: Tensor containing current batch indices.\n\n    Returns:\n      Summary tensor.\n    \"\"\"\n    with tf.name_scope('begin_episode/'):\n      if self._last_state is None:\n        reset_state = tf.no_op()\n      else:\n        reset_state = utility.reinit_nested_vars(\n            self._last_state, agent_indices)\n      reset_buffer = self._current_episodes.clear(agent_indices)\n      with tf.control_dependencies([reset_state, reset_buffer]):\n        return tf.constant('')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef perform(self, agent_indices, observ):\n    with tf.name_scope('perform/'):\n      observ = self._observ_filter.transform(observ)\n      if self._last_state is None:\n        state = None\n      else:\n        state = tools.nested.map(\n            lambda x: tf.gather(x, agent_indices), self._last_state)\n      with tf.device('/gpu:0' if self._use_gpu else '/cpu:0'):\n        output = self._network(\n            observ[:, None], tf.ones(observ.shape[0]), state)\n      action = tf.cond(\n          self._is_training, output.policy.sample, output.policy.mode)\n      logprob = output.policy.log_prob(action)[:, 0]\n      # pylint: disable=g-long-lambda\n      summary = tf.cond(self._should_log, lambda: tf.summary.merge([\n          tf.summary.histogram('mode', output.policy.mode()[:, 0]),\n          tf.summary.histogram('action', action[:, 0]),\n          tf.summary.histogram('logprob', logprob)]), str)\n      # Remember current policy to append to memory in the experience callback.\n      if self._last_state is None:\n        assign_state = tf.no_op()\n      else:\n        assign_state = utility.assign_nested_vars(\n            self._last_state, output.state, agent_indices)\n      remember_last_action = tf.scatter_update(\n          self._last_action, agent_indices, action[:, 0])\n      policy_params = tools.nested.filter(\n          lambda x: isinstance(x, tf.Tensor), output.policy.parameters)\n      assert policy_params, 'Policy has no parameters to store.'\n      remember_last_policy = tools.nested.map(\n          lambda var, val: tf.scatter_update(var, agent_indices, val[:, 0]),\n          self._last_policy, policy_params, flatten=True)\n      with tf.control_dependencies((\n          assign_state, remember_last_action) + remember_last_policy):\n        return action[:, 0], tf.identity(summary)", "response": "Compute batch of actions and a summary for a batch of observations."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprocesses the current transition tuple of the current step.", "response": "def experience(\n      self, agent_indices, observ, action, reward, unused_done, unused_nextob):\n    \"\"\"Process the transition tuple of the current step.\n\n    When training, add the current transition tuple to the memory and update\n    the streaming statistics for observations and rewards. A summary string is\n    returned if requested at this step.\n\n    Args:\n      agent_indices: Tensor containing current batch indices.\n      observ: Batch tensor of observations.\n      action: Batch tensor of actions.\n      reward: Batch tensor of rewards.\n      unused_done: Batch tensor of done flags.\n      unused_nextob: Batch tensor of successor observations.\n\n    Returns:\n      Summary tensor.\n    \"\"\"\n    with tf.name_scope('experience/'):\n      return tf.cond(\n          self._is_training,\n          # pylint: disable=g-long-lambda\n          lambda: self._define_experience(\n              agent_indices, observ, action, reward), str)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nimplements the branch of experience ( entered during training.", "response": "def _define_experience(self, agent_indices, observ, action, reward):\n    \"\"\"Implement the branch of experience() entered during training.\"\"\"\n    update_filters = tf.summary.merge([\n        self._observ_filter.update(observ),\n        self._reward_filter.update(reward)])\n    with tf.control_dependencies([update_filters]):\n      if self._config.train_on_agent_action:\n        # NOTE: Doesn't seem to change much.\n        action = self._last_action\n      policy = tools.nested.map(\n          lambda x: tf.gather(x, agent_indices), self._last_policy)\n      batch = (observ, action, policy, reward)\n      append = self._current_episodes.append(batch, agent_indices)\n    with tf.control_dependencies([append]):\n      norm_observ = self._observ_filter.transform(observ)\n      norm_reward = tf.reduce_mean(self._reward_filter.transform(reward))\n      # pylint: disable=g-long-lambda\n      summary = tf.cond(self._should_log, lambda: tf.summary.merge([\n          update_filters,\n          self._observ_filter.summary(),\n          self._reward_filter.summary(),\n          tf.summary.scalar('memory_size', self._num_finished_episodes),\n          tf.summary.histogram('normalized_observ', norm_observ),\n          tf.summary.histogram('action', self._last_action),\n          tf.summary.scalar('normalized_reward', norm_reward)]), str)\n      return summary"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds episodes to the memory and perform update steps if memory is full.", "response": "def end_episode(self, agent_indices):\n    \"\"\"Add episodes to the memory and perform update steps if memory is full.\n\n    During training, add the collected episodes of the batch indices that\n    finished their episode to the memory. If the memory is full, train on it,\n    and then clear the memory. A summary string is returned if requested at\n    this step.\n\n    Args:\n      agent_indices: Tensor containing current batch indices.\n\n    Returns:\n       Summary tensor.\n    \"\"\"\n    with tf.name_scope('end_episode/'):\n      return tf.cond(\n          self._is_training,\n          lambda: self._define_end_episode(agent_indices), str)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _initialize_policy(self):\n    with tf.device('/gpu:0' if self._use_gpu else '/cpu:0'):\n      network = functools.partial(\n          self._config.network, self._config, self._batch_env.action_space)\n      self._network = tf.make_template('network', network)\n      output = self._network(\n          tf.zeros_like(self._batch_env.observ)[:, None],\n          tf.ones(len(self._batch_env)))\n    if output.policy.event_shape != self._batch_env.action.shape[1:]:\n      message = 'Policy event shape {} does not match action shape {}.'\n      message = message.format(\n          output.policy.event_shape, self._batch_env.action.shape[1:])\n      raise ValueError(message)\n    self._policy_type = type(output.policy)\n    is_tensor = lambda x: isinstance(x, tf.Tensor)\n    policy_params = tools.nested.filter(is_tensor, output.policy.parameters)\n    set_batch_dim = lambda x: utility.set_dimension(x, 0, len(self._batch_env))\n    tools.nested.map(set_batch_dim, policy_params)\n    if output.state is not None:\n      tools.nested.map(set_batch_dim, output.state)\n    return policy_params, output.state", "response": "Initialize the policy.\n\n    Run the policy network on dummy data to initialize its parameters for later\n    reuse and to analyze the policy distribution. Initializes the attributes\n    `self._network` and `self._policy_type`.\n\n    Raises:\n      ValueError: Invalid policy distribution.\n\n    Returns:\n      Parameters of the policy distribution and policy state."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _initialize_memory(self, policy_params):\n    # We store observation, action, policy parameters, and reward.\n    template = (\n        self._batch_env.observ[0],\n        self._batch_env.action[0],\n        tools.nested.map(lambda x: x[0, 0], policy_params),\n        self._batch_env.reward[0])\n    with tf.variable_scope('ppo_temporary'):\n      self._current_episodes = parts.EpisodeMemory(\n          template, len(self._batch_env), self._config.max_length, 'episodes')\n    self._finished_episodes = parts.EpisodeMemory(\n        template, self._config.update_every, self._config.max_length, 'memory')\n    self._num_finished_episodes = tf.Variable(0, False)", "response": "Initialize temporary and permanent memory."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _define_end_episode(self, agent_indices):\n    episodes, length = self._current_episodes.data(agent_indices)\n    space_left = self._config.update_every - self._num_finished_episodes\n    use_episodes = tf.range(tf.minimum(\n        tf.shape(agent_indices)[0], space_left))\n    episodes = tools.nested.map(lambda x: tf.gather(x, use_episodes), episodes)\n    append = self._finished_episodes.replace(\n        episodes, tf.gather(length, use_episodes),\n        use_episodes + self._num_finished_episodes)\n    with tf.control_dependencies([append]):\n      increment_index = self._num_finished_episodes.assign_add(\n          tf.shape(use_episodes)[0])\n    with tf.control_dependencies([increment_index]):\n      memory_full = self._num_finished_episodes >= self._config.update_every\n      return tf.cond(memory_full, self._training, str)", "response": "Implement the branch of end_episode() entered during training."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nperforming multiple training iterations of both policy and value baseline.", "response": "def _training(self):\n    \"\"\"Perform multiple training iterations of both policy and value baseline.\n\n    Training on the episodes collected in the memory. Reset the memory\n    afterwards. Always returns a summary string.\n\n    Returns:\n      Summary tensor.\n    \"\"\"\n    with tf.device('/gpu:0' if self._use_gpu else '/cpu:0'):\n      with tf.name_scope('training'):\n        assert_full = tf.assert_equal(\n            self._num_finished_episodes, self._config.update_every)\n        with tf.control_dependencies([assert_full]):\n          data = self._finished_episodes.data()\n        (observ, action, old_policy_params, reward), length = data\n        # We set padding frames of the parameters to ones to prevent Gaussians\n        # with zero variance. This would result in an infinite KL divergence,\n        # which, even if masked out, would result in NaN gradients.\n        old_policy_params = tools.nested.map(\n            lambda param: self._mask(param, length, 1), old_policy_params)\n        with tf.control_dependencies([tf.assert_greater(length, 0)]):\n          length = tf.identity(length)\n        observ = self._observ_filter.transform(observ)\n        reward = self._reward_filter.transform(reward)\n        update_summary = self._perform_update_steps(\n            observ, action, old_policy_params, reward, length)\n        with tf.control_dependencies([update_summary]):\n          penalty_summary = self._adjust_penalty(\n              observ, old_policy_params, length)\n        with tf.control_dependencies([penalty_summary]):\n          clear_memory = tf.group(\n              self._finished_episodes.clear(),\n              self._num_finished_episodes.assign(0))\n        with tf.control_dependencies([clear_memory]):\n          weight_summary = utility.variable_summaries(\n              tf.trainable_variables(), self._config.weight_summaries)\n          return tf.summary.merge([\n              update_summary, penalty_summary, weight_summary])"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nperforms multiple update steps of value function and policy.", "response": "def _perform_update_steps(\n      self, observ, action, old_policy_params, reward, length):\n    \"\"\"Perform multiple update steps of value function and policy.\n\n    The advantage is computed once at the beginning and shared across\n    iterations. We need to decide for the summary of one iteration, and thus\n    choose the one after half of the iterations.\n\n    Args:\n      observ: Sequences of observations.\n      action: Sequences of actions.\n      old_policy_params: Parameters of the behavioral policy.\n      reward: Sequences of rewards.\n      length: Batch of sequence lengths.\n\n    Returns:\n      Summary tensor.\n    \"\"\"\n    return_ = utility.discounted_return(\n        reward, length, self._config.discount)\n    value = self._network(observ, length).value\n    if self._config.gae_lambda:\n      advantage = utility.lambda_advantage(\n          reward, value, length, self._config.discount,\n          self._config.gae_lambda)\n    else:\n      advantage = return_ - value\n    mean, variance = tf.nn.moments(advantage, axes=[0, 1], keep_dims=True)\n    advantage = (advantage - mean) / (tf.sqrt(variance) + 1e-8)\n    advantage = tf.Print(\n        advantage, [tf.reduce_mean(return_), tf.reduce_mean(value)],\n        'return and value: ')\n    advantage = tf.Print(\n        advantage, [tf.reduce_mean(advantage)],\n        'normalized advantage: ')\n    episodes = (observ, action, old_policy_params, reward, advantage)\n    value_loss, policy_loss, summary = parts.iterate_sequences(\n        self._update_step, [0., 0., ''], episodes, length,\n        self._config.chunk_length,\n        self._config.batch_size,\n        self._config.update_epochs,\n        padding_value=1)\n    print_losses = tf.group(\n        tf.Print(0, [tf.reduce_mean(value_loss)], 'value loss: '),\n        tf.Print(0, [tf.reduce_mean(policy_loss)], 'policy loss: '))\n    with tf.control_dependencies([value_loss, policy_loss, print_losses]):\n      return summary[self._config.update_epochs // 2]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _update_step(self, sequence):\n    observ, action, old_policy_params, reward, advantage = sequence['sequence']\n    length = sequence['length']\n    old_policy = self._policy_type(**old_policy_params)\n    value_loss, value_summary = self._value_loss(observ, reward, length)\n    network = self._network(observ, length)\n    policy_loss, policy_summary = self._policy_loss(\n        old_policy, network.policy, action, advantage, length)\n    network_loss = network.get('loss', 0.0)\n    loss = policy_loss + value_loss + tf.reduce_mean(network_loss)\n    gradients, variables = (\n        zip(*self._optimizer.compute_gradients(loss)))\n    optimize = self._optimizer.apply_gradients(\n        zip(gradients, variables))\n    summary = tf.summary.merge([\n        value_summary, policy_summary,\n        tf.summary.histogram('network_loss', network_loss),\n        tf.summary.scalar('avg_network_loss', tf.reduce_mean(network_loss)),\n        tf.summary.scalar('gradient_norm', tf.global_norm(gradients)),\n        utility.gradient_summaries(zip(gradients, variables))])\n    with tf.control_dependencies([optimize]):\n      return [tf.identity(x) for x in (value_loss, policy_loss, summary)]", "response": "Compute the current combined loss and perform a gradient update step."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute the loss function for the value baseline.", "response": "def _value_loss(self, observ, reward, length):\n    \"\"\"Compute the loss function for the value baseline.\n\n    The value loss is the difference between empirical and approximated returns\n    over the collected episodes. Returns the loss tensor and a summary strin.\n\n    Args:\n      observ: Sequences of observations.\n      reward: Sequences of reward.\n      length: Batch of sequence lengths.\n\n    Returns:\n      Tuple of loss tensor and summary tensor.\n    \"\"\"\n    with tf.name_scope('value_loss'):\n      value = self._network(observ, length).value\n      return_ = utility.discounted_return(\n          reward, length, self._config.discount)\n      advantage = return_ - value\n      value_loss = 0.5 * self._mask(advantage ** 2, length)\n      summary = tf.summary.merge([\n          tf.summary.histogram('value_loss', value_loss),\n          tf.summary.scalar('avg_value_loss', tf.reduce_mean(value_loss))])\n      value_loss = tf.reduce_mean(value_loss)\n      return tf.check_numerics(value_loss, 'value_loss'), summary"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _policy_loss(\n      self, old_policy, policy, action, advantage, length):\n    \"\"\"Compute the policy loss composed of multiple components.\n\n    1. The policy gradient loss is importance sampled from the data-collecting\n       policy at the beginning of training.\n    2. The second term is a KL penalty between the policy at the beginning of\n       training and the current policy.\n    3. Additionally, if this KL already changed more than twice the target\n       amount, we activate a strong penalty discouraging further divergence.\n\n    Args:\n      old_policy: Action distribution of the behavioral policy.\n      policy: Sequences of distribution params of the current policy.\n      action: Sequences of actions.\n      advantage: Sequences of advantages.\n      length: Batch of sequence lengths.\n\n    Returns:\n      Tuple of loss tensor and summary tensor.\n    \"\"\"\n    with tf.name_scope('policy_loss'):\n      kl = tf.contrib.distributions.kl_divergence(old_policy, policy)\n      # Infinite values in the KL, even for padding frames that we mask out,\n      # cause NaN gradients since TensorFlow computes gradients with respect to\n      # the whole input tensor.\n      kl = tf.check_numerics(kl, 'kl')\n      kl = tf.reduce_mean(self._mask(kl, length), 1)\n      policy_gradient = tf.exp(\n          policy.log_prob(action) - old_policy.log_prob(action))\n      surrogate_loss = -tf.reduce_mean(self._mask(\n          policy_gradient * tf.stop_gradient(advantage), length), 1)\n      surrogate_loss = tf.check_numerics(surrogate_loss, 'surrogate_loss')\n      kl_penalty = self._penalty * kl\n      cutoff_threshold = self._config.kl_target * self._config.kl_cutoff_factor\n      cutoff_count = tf.reduce_sum(\n          tf.cast(kl > cutoff_threshold, tf.int32))\n      with tf.control_dependencies([tf.cond(\n          cutoff_count > 0,\n          lambda: tf.Print(0, [cutoff_count], 'kl cutoff! '), int)]):\n        kl_cutoff = (\n            self._config.kl_cutoff_coef *\n            tf.cast(kl > cutoff_threshold, tf.float32) *\n            (kl - cutoff_threshold) ** 2)\n      policy_loss = surrogate_loss + kl_penalty + kl_cutoff\n      entropy = tf.reduce_mean(policy.entropy(), axis=1)\n      if self._config.entropy_regularization:\n        policy_loss -= self._config.entropy_regularization * entropy\n      summary = tf.summary.merge([\n          tf.summary.histogram('entropy', entropy),\n          tf.summary.histogram('kl', kl),\n          tf.summary.histogram('surrogate_loss', surrogate_loss),\n          tf.summary.histogram('kl_penalty', kl_penalty),\n          tf.summary.histogram('kl_cutoff', kl_cutoff),\n          tf.summary.histogram('kl_penalty_combined', kl_penalty + kl_cutoff),\n          tf.summary.histogram('policy_loss', policy_loss),\n          tf.summary.scalar('avg_surr_loss', tf.reduce_mean(surrogate_loss)),\n          tf.summary.scalar('avg_kl_penalty', tf.reduce_mean(kl_penalty)),\n          tf.summary.scalar('avg_policy_loss', tf.reduce_mean(policy_loss))])\n      policy_loss = tf.reduce_mean(policy_loss, 0)\n      return tf.check_numerics(policy_loss, 'policy_loss'), summary", "response": "Compute the policy gradient loss of a single component."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _adjust_penalty(self, observ, old_policy_params, length):\n    old_policy = self._policy_type(**old_policy_params)\n    with tf.name_scope('adjust_penalty'):\n      network = self._network(observ, length)\n      print_penalty = tf.Print(0, [self._penalty], 'current penalty: ')\n      with tf.control_dependencies([print_penalty]):\n        kl_change = tf.reduce_mean(self._mask(\n            tf.contrib.distributions.kl_divergence(old_policy, network.policy),\n            length))\n        kl_change = tf.Print(kl_change, [kl_change], 'kl change: ')\n        maybe_increase = tf.cond(\n            kl_change > 1.3 * self._config.kl_target,\n            # pylint: disable=g-long-lambda\n            lambda: tf.Print(self._penalty.assign(\n                self._penalty * 1.5), [0], 'increase penalty '),\n            float)\n        maybe_decrease = tf.cond(\n            kl_change < 0.7 * self._config.kl_target,\n            # pylint: disable=g-long-lambda\n            lambda: tf.Print(self._penalty.assign(\n                self._penalty / 1.5), [0], 'decrease penalty '),\n            float)\n      with tf.control_dependencies([maybe_increase, maybe_decrease]):\n        return tf.summary.merge([\n            tf.summary.scalar('kl_change', kl_change),\n            tf.summary.scalar('penalty', self._penalty)])", "response": "Adjust the KL policy between the behavioral and current policy."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _mask(self, tensor, length, padding_value=0):\n    with tf.name_scope('mask'):\n      range_ = tf.range(tensor.shape[1].value)\n      mask = range_[None, :] < length[:, None]\n      if tensor.shape.ndims > 2:\n        for _ in range(tensor.shape.ndims - 2):\n          mask = mask[..., None]\n        mask = tf.tile(mask, [1, 1] + tensor.shape[2:].as_list())\n      masked = tf.where(mask, tensor, padding_value * tf.ones_like(tensor))\n      return tf.check_numerics(masked, 'masked')", "response": "Masks a batch of sequences."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nimplementing the actor main loop by waiting forever for messages.", "response": "def main(self, *args, **kwargs):\n        \"\"\"Implement the actor main loop by waiting forever for messages.\"\"\"\n        self.start(*args, **kwargs)\n        try:\n            while 1:\n                body, message = yield self.receive()\n                handler = self.get_handler(message)\n                handler(body, message)\n        finally:\n            self.stop(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef send(self, method, args={}, to=None, nowait=False, **kwargs):\n\n        if to is None:\n            to = self.routing_key\n        r = self.call_or_cast(method, args, routing_key=to,\n                              nowait=nowait, **kwargs)\n        if not nowait:\n            return r.get()", "response": "Send a method to the routing key."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncall the method on one of the agents in round robin.", "response": "def throw(self, method, args={}, nowait=False, **kwargs):\n        \"\"\"Call method on one of the agents in round robin.\n\n        See :meth:`call_or_cast` for a full list of supported\n        arguments.\n\n        If the keyword argument `nowait` is false (default) it\n        will block and return the reply.\n\n        \"\"\"\n        r = self.call_or_cast(method, args, type=ACTOR_TYPE.RR,\n                              nowait=nowait, **kwargs)\n        if not nowait:\n            return r"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef scatter(self, method, args={}, nowait=False, timeout=None, **kwargs):\n        timeout = timeout if timeout is not None else self.default_timeout\n        r = self.call_or_cast(method, args, type=ACTOR_TYPE.SCATTER,\n                              nowait=nowait, timeout=timeout, **kwargs)\n        if not nowait:\n            return r.gather(timeout=timeout, **kwargs)", "response": "Broadcasts a method to all agents."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\napply remote method asynchronously or synchronously depending on the value of nowait.", "response": "def call_or_cast(self, method, args={}, nowait=False, **kwargs):\n        \"\"\"Apply remote `method` asynchronously or synchronously depending\n        on the value of `nowait`.\n\n        :param method: The name of the remote method to perform.\n        :param args: Dictionary of arguments for the method.\n        :keyword nowait: If false the call will block until the result\n           is available and return it (default), if true the call will be\n           non-blocking and no result will be returned.\n        :keyword retry: If set to true then message sending will be retried\n          in the event of connection failures. Default is decided by the\n          :attr:`retry` attributed.\n        :keyword retry_policy: Override retry policies.\n           See :attr:`retry_policy`.  This must be a dictionary, and keys will\n           be merged with the default retry policy.\n        :keyword timeout: Timeout to wait for replies in seconds as a float\n           (**only relevant in blocking mode**).\n        :keyword limit: Limit number of replies to wait for\n           (**only relevant in blocking mode**).\n        :keyword callback: If provided, this callback will be called for every\n          reply received (**only relevant in blocking mode**).\n        :keyword \\*\\*props: Additional message properties.\n           See :meth:`kombu.Producer.publish`.\n\n        \"\"\"\n        return (nowait and self.cast or self.call)(method, args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_direct_queue(self):\n        return Queue(self.id, self.inbox_direct, routing_key=self.routing_key,\n                     auto_delete=True)", "response": "Returns a : class : kombu. Queue instance to be used to listen\n         for messages send to this specific actor instance"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_scatter_queue(self):\n        return Queue('%s.%s.scatter' % (self.name, self.id),\n                     self.inbox_scatter, auto_delete=True)", "response": "Returns a : class : kombu. Queue instance for receiving broadcast\n        commands for this actor type."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_rr_queue(self):\n        return Queue(self.inbox_rr.name + '.rr', self.inbox_rr,\n                     auto_delete=True)", "response": "Returns a : class : kombu. Queue instance for receiving round - robin\n        commands for this actor type."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef Consumer(self, channel, **kwargs):\n        kwargs.setdefault('no_ack', self.no_ack)\n        return Consumer(channel, self.get_queues(),\n                        callbacks=[self.on_message], **kwargs)", "response": "Returns a : class : kombu. Consumer instance for this Actor"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cast(self, method, args={}, declare=None, retry=None,\n             retry_policy=None, type=None, exchange=None, **props):\n        \"\"\"Send message to actor.  Discarding replies.\"\"\"\n        retry = self.retry if retry is None else retry\n        body = {'class': self.name, 'method': method, 'args': args}\n\n        _retry_policy = self.retry_policy\n        if retry_policy:  # merge default and custom policies.\n            _retry_policy = dict(_retry_policy, **retry_policy)\n\n        if type and type not in self.types:\n            raise ValueError('Unsupported type: {0}'.format(type))\n        elif not type:\n            type = ACTOR_TYPE.DIRECT\n\n        props.setdefault('routing_key', self.routing_key)\n        props.setdefault('serializer', self.serializer)\n        exchange = exchange or self.type_to_exchange[type]()\n        declare = (maybe_list(declare) or []) + [exchange]\n        with producers[self._connection].acquire(block=True) as producer:\n            return producer.publish(body, exchange=exchange, declare=declare,\n                                    retry=retry, retry_policy=retry_policy,\n                                    **props)", "response": "Send message to actor. Discarding replies."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsend a message to the same actor and return an AsyncResult.", "response": "def call(self, method, args={}, retry=False, retry_policy=None,\n             ticket=None, **props):\n        \"\"\"Send message to the same actor and return :class:`AsyncResult`.\"\"\"\n        ticket = ticket or uuid()\n        reply_q = self.get_reply_queue(ticket)\n        self.cast(method, args, declare=[reply_q], reply_to=ticket, **props)\n        return self.AsyncResult(ticket, self)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef handle_call(self, body, message):\n        try:\n            r = self._DISPATCH(body, ticket=message.properties['reply_to'])\n        except self.Next:\n            # don't reply, delegate to other agents.\n            pass\n        else:\n            self.reply(message, r)", "response": "Handle a call message."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _DISPATCH(self, body, ticket=None):\n        if ticket:\n            sticket = '%s' % (shortuuid(ticket), )\n        else:\n            ticket = sticket = str(next(self.ticket_counter))\n        try:\n            method, args = itemgetter('method', 'args')(body)\n            self.log.info('#%s --> %s',\n                          sticket, self._reprcall(method, args))\n            act = self.lookup_action(method)\n            r = {'ok': act(args or {})}\n            self.log.info('#%s <-- %s', sticket, reprkwargs(r))\n        except self.Next:\n            raise\n        except Exception as exc:\n            einfo = sys.exc_info()\n            r = {'nok': [safe_repr(exc), self._get_traceback(einfo)]}\n            self.log.error('#%s <-- nok=%r', sticket, exc)\n        return dict(self._default_fields, **r)", "response": "Dispatch a message to the appropriate method\n            in the appropriate state."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef handle_argv(self, prog_name, argv):\n        options, args = self.parse_options(prog_name, argv)\n        return self.run(*args, **vars(options))", "response": "Parses command line arguments from argv and dispatches the appropriate method to run."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing the available options.", "response": "def parse_options(self, prog_name, arguments):\n        \"\"\"Parse the available options.\"\"\"\n        # Don't want to load configuration to just print the version,\n        # so we handle --version manually here.\n        if '--version' in arguments:\n            self.exit_status(self.version, fh=sys.stdout)\n        parser = self.create_parser(prog_name)\n        options, args = parser.parse_args(arguments)\n        return options, args"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn your text, enclosed in ANSI graphics codes. Depends on the keyword arguments 'fg' and 'bg', and the contents of the opts tuple/list. Returns the RESET code if no parameters are given. Valid colors: 'black', 'red', 'green', 'yellow', 'blue', 'magenta', 'cyan', 'white' Valid options: 'bold' 'underscore' 'blink' 'reverse' 'conceal' 'noreset' - string will not be auto-terminated with the RESET code Examples: colorize('hello', fg='red', bg='blue', opts=('blink',)) colorize() colorize('goodbye', opts=('underscore',)) print colorize('first line', fg='red', opts=('noreset',)) print 'this should be red too' print colorize('and so should this') print 'this should not be red'", "response": "def colorize(text='', opts=(), **kwargs):\n    \"\"\"\n    Returns your text, enclosed in ANSI graphics codes.\n\n    Depends on the keyword arguments 'fg' and 'bg', and the contents of\n    the opts tuple/list.\n\n    Returns the RESET code if no parameters are given.\n\n    Valid colors:\n        'black', 'red', 'green', 'yellow', 'blue', 'magenta', 'cyan', 'white'\n\n    Valid options:\n        'bold'\n        'underscore'\n        'blink'\n        'reverse'\n        'conceal'\n        'noreset' - string will not be auto-terminated with the RESET code\n\n    Examples:\n        colorize('hello', fg='red', bg='blue', opts=('blink',))\n        colorize()\n        colorize('goodbye', opts=('underscore',))\n        print colorize('first line', fg='red', opts=('noreset',))\n        print 'this should be red too'\n        print colorize('and so should this')\n        print 'this should not be red'\n    \"\"\"\n    color_names = ('black', 'red', 'green', 'yellow',\n                   'blue', 'magenta', 'cyan', 'white')\n    foreground = dict([(color_names[x], '3%s' % x) for x in range(8)])\n    background = dict([(color_names[x], '4%s' % x) for x in range(8)])\n\n    RESET = '0'\n    opt_dict = {'bold': '1',\n                'underscore': '4',\n                'blink': '5',\n                'reverse': '7',\n                'conceal': '8'}\n\n    text = str(text)\n    code_list = []\n    if text == '' and len(opts) == 1 and opts[0] == 'reset':\n        return '\\x1b[%sm' % RESET\n    for k, v in kwargs.items():\n        if k == 'fg':\n            code_list.append(foreground[v])\n        elif k == 'bg':\n            code_list.append(background[v])\n    for o in opts:\n        if o in opt_dict:\n            code_list.append(opt_dict[o])\n    if 'noreset' not in opts:\n        text = text + '\\x1b[%sm' % RESET\n    return ('\\x1b[%sm' % ';'.join(code_list)) + text"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get(self, **kwargs):\n        \"What kind of arguments should be pass here\"\n        kwargs.setdefault('limit', 1)\n        return self._first(self.gather(**kwargs))", "response": "What kind of arguments should be pass here"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nextracts the value out of the reply message.", "response": "def to_python(self, reply, propagate=True):\n        \"\"\"Extracts the value out of the reply message.\n\n        :param reply: In the case of a successful call the reply message\n            will be::\n\n                {'ok': return_value, **default_fields}\n\n            Therefore the method returns: return_value, **default_fields\n\n            If the method raises an exception the reply message\n            will be::\n\n                {'nok': [repr exc, str traceback], **default_fields}\n\n        :keyword propagate - Propagate exceptions raised instead of returning\n            a result representation of the error.\n\n        \"\"\"\n        try:\n            return reply['ok']\n        except KeyError:\n            error = self.Error(*reply.get('nok') or ())\n            if propagate:\n                raise error\n            return error"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef spawn(self, cls, kwargs={}, nowait=False):\n\n        actor_id = uuid()\n\n        if str(qualname(cls)) == '__builtin__.unicode':\n            name = cls\n        else:\n            name = qualname(cls)\n\n        res = self.call('spawn', {'cls': name, 'id': actor_id,\n                                  'kwargs': kwargs},\n                        type=ACTOR_TYPE.RR, nowait=nowait)\n        return ActorProxy(name, actor_id, res, agent=self,\n                          connection=self.connection, **kwargs)", "response": "Spawns a new actor on a celery worker by sending a remote command to the worker."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the id of already spawned actor with the given class.", "response": "def select(self, cls, **kwargs):\n        \"\"\"Get the id of already spawned actor\n\n        :keyword actor: the name of the :class:`Actor` class\n        \"\"\"\n        name = qualname(cls)\n        id = first_reply(\n            self.scatter('select', {'cls': name}, limit=1), cls)\n        return ActorProxy(name, id, agent=self,\n                          connection=self.connection, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprocess a message from the actor.", "response": "def process_message(self, actor, body, message):\n        \"\"\"Process actor message depending depending on the the worker settings.\n\n        If greenlets are enabled in the worker, the actor message is processed\n        in a greenlet from the greenlet pool,\n        Otherwise, the message is processed by the same thread.\n        The method is invoked from the callback `cell.actors.Actor.on_message`\n         upon receiving of a message.\n\n        :keyword actor: instance of :class:`Actor` or its derivative.\n        The actor instance to process the message.\n\n        For the full list of arguments see\n        :meth:`cell.actors.Actor._on_message`.\n\n        \"\"\"\n        if actor is not self and self.is_green():\n            self.pool.spawn_n(actor._on_message, body, message)\n        else:\n            if not self.is_green() and message.properties.get('reply_to'):\n                warn('Starting a blocking call (%s) on actor (%s) '\n                     'when greenlets are disabled.',\n                     itemgetter('method')(body), actor.__class__)\n            actor._on_message(body, message)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_outer_frame_variables():\n    cur_filename = inspect.getframeinfo(inspect.currentframe()).filename\n    outer_frame = next(f\n                       for f in inspect.getouterframes(inspect.currentframe())\n                       if f.filename != cur_filename)\n    variables = {}\n    variables.update(outer_frame.frame.f_globals)\n    variables.update(outer_frame.frame.f_locals)\n    return variables", "response": "Get a dict of local and global variables of the first outer frame from another file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nextract table names from an SQL query.", "response": "def extract_table_names(query):\n    \"\"\" Extract table names from an SQL query. \"\"\"\n    # a good old fashioned regex. turns out this worked better than actually parsing the code\n    tables_blocks = re.findall(r'(?:FROM|JOIN)\\s+(\\w+(?:\\s*,\\s*\\w+)*)', query, re.IGNORECASE)\n    tables = [tbl\n              for block in tables_blocks\n              for tbl in re.findall(r'\\w+', block)]\n    return set(tables)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwrites a dataframe to the database.", "response": "def write_table(df, tablename, conn):\n    \"\"\" Write a dataframe to the database. \"\"\"\n    with catch_warnings():\n        filterwarnings('ignore',\n                       message='The provided table name \\'%s\\' is not found exactly as such in the database' % tablename)\n        to_sql(df, name=tablename, con=conn,\n               index=not any(name is None for name in df.index.names))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes the benchmarks for Ordianry Kriging", "response": "def make_benchark(n_train, n_test, n_dim=2):\n    \"\"\" Compute the benchmarks for Ordianry Kriging \n\n    Parameters\n    ----------\n    n_train : int\n      number of points in the training set\n    n_test : int\n      number of points in the test set\n    n_dim : int\n      number of dimensions (default=2)\n\n    Returns\n    -------\n    res : dict\n      a dictionary with the timing results\n    \"\"\"\n    X_train = np.random.rand(n_train, n_dim)\n    y_train = np.random.rand(n_train)\n    X_test = np.random.rand(n_test, n_dim)\n\n    res = {}\n\n    for variogram_model in VARIOGRAM_MODELS:\n        tic = time()\n        OK = OrdinaryKriging(X_train[:, 0], X_train[:, 1], y_train,\n                             variogram_model='linear',\n                             verbose=False, enable_plotting=False)\n        res['t_train_{}'.format(variogram_model)] = time() - tic\n\n    # All the following tests are performed with the linear variogram model\n    for backend in BACKENDS:\n        for n_closest_points in N_MOVING_WINDOW:\n\n            if backend == 'vectorized' and n_closest_points is not None:\n                continue  # this is not supported\n\n            tic = time()\n            OK.execute('points', X_test[:, 0], X_test[:, 1],\n                       backend=backend,\n                       n_closest_points=n_closest_points)\n            res['t_test_{}_{}'.format(backend, n_closest_points)] = time() - tic\n\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprint the benchmarks for the training set and test set.", "response": "def print_benchmark(n_train, n_test, n_dim, res):\n    \"\"\" Print the benchmarks\n\n    Parameters\n    ----------\n    n_train : int\n      number of points in the training set\n    n_test : int\n      number of points in the test set\n    n_dim : int\n      number of dimensions (default=2)\n    res : dict\n      a dictionary with the timing results\n    \"\"\"\n    print('='*80)\n    print(' '*10, 'N_dim={}, N_train={}, N_test={}'.format(n_dim,\n                                                           n_train, n_test))\n    print('='*80)\n    print('\\n', '# Training the model', '\\n')\n    print('|'.join(['{:>11} '.format(el) for el in ['t_train (s)'] +\n                    VARIOGRAM_MODELS]))\n    print('-' * (11 + 2) * (len(VARIOGRAM_MODELS) + 1))\n    print('|'.join(['{:>11} '.format('Training')] +\n                   ['{:>11.2} '.format(el) for el in\n                    [res['t_train_{}'.format(mod)]\n                     for mod in VARIOGRAM_MODELS]]))\n\n    print('\\n', '# Predicting kriging points', '\\n')\n    print('|'.join(['{:>11} '.format(el) for el in ['t_test (s)'] + BACKENDS]))\n    print('-' * (11 + 2) * (len(BACKENDS) + 1))\n\n    for n_closest_points in N_MOVING_WINDOW:\n        timing_results = [res.get(\n            't_test_{}_{}'.format(mod, n_closest_points), '')\n                          for mod in BACKENDS]\n        print('|'.join(['{:>11} '.format('N_nn=' + str(n_closest_points))] +\n                       ['{:>11.2} '.format(el) for el in timing_results]))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _calculate_data_point_zscalars(self, x, y, type_='array'):\r\n\r\n        if type_ == 'scalar':\r\n            nx = 1\r\n            ny = 1\r\n            z_scalars = None\r\n        else:\r\n            if x.ndim == 1:\r\n                nx = x.shape[0]\r\n                ny = 1\r\n            else:\r\n                ny = x.shape[0]\r\n                nx = x.shape[1]\r\n            z_scalars = np.zeros(x.shape)\r\n\r\n        for m in range(ny):\r\n            for n in range(nx):\r\n\r\n                if type_ == 'scalar':\r\n                    xn = x\r\n                    yn = y\r\n                else:\r\n                    if x.ndim == 1:\r\n                        xn = x[n]\r\n                        yn = y[n]\r\n                    else:\r\n                        xn = x[m, n]\r\n                        yn = y[m, n]\r\n\r\n                if xn > np.amax(self.external_Z_array_x) or \\\r\n                   xn < np.amin(self.external_Z_array_x) or \\\r\n                   yn > np.amax(self.external_Z_array_y) or \\\r\n                   yn < np.amin(self.external_Z_array_y):\r\n                    raise ValueError(\"External drift array does not cover \"\r\n                                     \"specified kriging domain.\")\r\n\r\n                # bilinear interpolation\r\n                external_x2_index = \\\r\n                    np.amin(np.where(self.external_Z_array_x >= xn)[0])\r\n                external_x1_index = \\\r\n                    np.amax(np.where(self.external_Z_array_x <= xn)[0])\r\n                external_y2_index = \\\r\n                    np.amin(np.where(self.external_Z_array_y >= yn)[0])\r\n                external_y1_index = \\\r\n                    np.amax(np.where(self.external_Z_array_y <= yn)[0])\r\n                if external_y1_index == external_y2_index:\r\n                    if external_x1_index == external_x2_index:\r\n                        z = self.external_Z_array[external_y1_index, external_x1_index]\r\n                    else:\r\n                        z = (self.external_Z_array[external_y1_index, external_x1_index] *\r\n                             (self.external_Z_array_x[external_x2_index] - xn) +\r\n                             self.external_Z_array[external_y2_index, external_x2_index] *\r\n                             (xn - self.external_Z_array_x[external_x1_index])) / \\\r\n                            (self.external_Z_array_x[external_x2_index] -\r\n                             self.external_Z_array_x[external_x1_index])\r\n                elif external_x1_index == external_x2_index:\r\n                    if external_y1_index == external_y2_index:\r\n                        z = self.external_Z_array[external_y1_index, external_x1_index]\r\n                    else:\r\n                        z = (self.external_Z_array[external_y1_index, external_x1_index] *\r\n                             (self.external_Z_array_y[external_y2_index] - yn) +\r\n                             self.external_Z_array[external_y2_index, external_x2_index] *\r\n                             (yn - self.external_Z_array_y[external_y1_index])) / \\\r\n                            (self.external_Z_array_y[external_y2_index] -\r\n                             self.external_Z_array_y[external_y1_index])\r\n                else:\r\n                    z = (self.external_Z_array[external_y1_index, external_x1_index] *\r\n                         (self.external_Z_array_x[external_x2_index] - xn) *\r\n                         (self.external_Z_array_y[external_y2_index] - yn) +\r\n                         self.external_Z_array[external_y1_index, external_x2_index] *\r\n                         (xn - self.external_Z_array_x[external_x1_index]) *\r\n                         (self.external_Z_array_y[external_y2_index] - yn) +\r\n                         self.external_Z_array[external_y2_index, external_x1_index] *\r\n                         (self.external_Z_array_x[external_x2_index] - xn) *\r\n                         (yn - self.external_Z_array_y[external_y1_index]) +\r\n                         self.external_Z_array[external_y2_index, external_x2_index] *\r\n                         (xn - self.external_Z_array_x[external_x1_index]) *\r\n                         (yn - self.external_Z_array_y[external_y1_index])) / \\\r\n                        ((self.external_Z_array_x[external_x2_index] -\r\n                          self.external_Z_array_x[external_x1_index]) *\r\n                         (self.external_Z_array_y[external_y2_index] -\r\n                          self.external_Z_array_y[external_y1_index]))\r\n\r\n                if type_ == 'scalar':\r\n                    z_scalars = z\r\n                else:\r\n                    if z_scalars.ndim == 1:\r\n                        z_scalars[n] = z\r\n                    else:\r\n                        z_scalars[m, n] = z\r\n\r\n        return z_scalars", "response": "Calculates the Z - scalar values at the specified coordinates for use when setting up the kriging matrix."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nallow user to update a variogram model.", "response": "def update_variogram_model(self, variogram_model, variogram_parameters=None,\r\n                               variogram_function=None, nlags=6, weight=False,\r\n                               anisotropy_scaling=1., anisotropy_angle=0.):\r\n        \"\"\"Allows user to update variogram type and/or\r\n        variogram model parameters.\r\n\r\n        Parameters\r\n        ----------\r\n        variogram_model : str\r\n            May be any of the variogram models listed above.\r\n            May also be 'custom', in which case variogram_parameters and\r\n            variogram_function must be specified.\r\n        variogram_parameters : list or dict, optional\r\n            List or dict of variogram model parameters, as explained above.\r\n            If not provided, a best fit model will be calculated as\r\n            described above.\r\n        variogram_function : callable, optional\r\n            A callable function that must be provided if variogram_model is\r\n            specified as 'custom'. See above for more information.\r\n        nlags : int, optional\r\n            Number of averaging bins for the semivariogram. Defualt is 6.\r\n        weight : boolean, optional\r\n            Flag that specifies if semivariance at smaller lags should be\r\n            weighted more heavily when automatically calculating the\r\n            variogram model. See above for more information. True indicates\r\n            that weights will be applied. Default is False.\r\n        anisotropy_scaling : float, optional\r\n            Scalar stretching value to take into account anisotropy.\r\n            Default is 1 (effectively no stretching).\r\n            Scaling is applied in the y-direction.\r\n        anisotropy_angle : float, optional\r\n            CCW angle (in degrees) by which to rotate coordinate system in\r\n            order to take into account anisotropy. Default is 0 (no rotation).\r\n        \"\"\"\r\n\r\n        if anisotropy_scaling != self.anisotropy_scaling or \\\r\n           anisotropy_angle != self.anisotropy_angle:\r\n            if self.verbose:\r\n                print(\"Adjusting data for anisotropy...\")\r\n            self.anisotropy_scaling = anisotropy_scaling\r\n            self.anisotropy_angle = anisotropy_angle\r\n            self.X_ADJUSTED, self.Y_ADJUSTED =\\\r\n                _adjust_for_anisotropy(np.vstack((self.X_ORIG, self.Y_ORIG)).T,\r\n                                       [self.XCENTER, self.YCENTER],\r\n                                       [self.anisotropy_scaling],\r\n                                       [self.anisotropy_angle]).T\r\n\r\n        self.variogram_model = variogram_model\r\n        if self.variogram_model not in self.variogram_dict.keys() and self.variogram_model != 'custom':\r\n            raise ValueError(\"Specified variogram model '%s' is not supported.\" % variogram_model)\r\n        elif self.variogram_model == 'custom':\r\n            if variogram_function is None or not callable(variogram_function):\r\n                raise ValueError(\"Must specify callable function for \"\r\n                                 \"custom variogram model.\")\r\n            else:\r\n                self.variogram_function = variogram_function\r\n        else:\r\n            self.variogram_function = self.variogram_dict[self.variogram_model]\r\n        if self.verbose:\r\n            print(\"Updating variogram mode...\")\r\n\r\n        # See note above about the 'use_psill' kwarg...\r\n        vp_temp = _make_variogram_parameter_list(self.variogram_model,\r\n                                                 variogram_parameters)\r\n        self.lags, self.semivariance, self.variogram_model_parameters = \\\r\n            _initialize_variogram_model(np.vstack((self.X_ADJUSTED,\r\n                                                   self.Y_ADJUSTED)).T,\r\n                                        self.Z, self.variogram_model, vp_temp,\r\n                                        self.variogram_function, nlags,\r\n                                        weight, 'euclidean')\r\n\r\n        if self.verbose:\r\n            if self.variogram_model == 'linear':\r\n                print(\"Using '%s' Variogram Model\" % 'linear')\r\n                print(\"Slope:\", self.variogram_model_parameters[0])\r\n                print(\"Nugget:\", self.variogram_model_parameters[1], '\\n')\r\n            elif self.variogram_model == 'power':\r\n                print(\"Using '%s' Variogram Model\" % 'power')\r\n                print(\"Scale:\", self.variogram_model_parameters[0])\r\n                print(\"Exponent:\", self.variogram_model_parameters[1])\r\n                print(\"Nugget:\", self.variogram_model_parameters[2], '\\n')\r\n            elif self.variogram_model == 'custom':\r\n                print(\"Using Custom Variogram Model\")\r\n            else:\r\n                print(\"Using '%s' Variogram Model\" % self.variogram_model)\r\n                print(\"Partial Sill:\", self.variogram_model_parameters[0])\r\n                print(\"Full Sill:\", self.variogram_model_parameters[0] +\r\n                      self.variogram_model_parameters[2])\r\n                print(\"Range:\", self.variogram_model_parameters[1])\r\n                print(\"Nugget:\", self.variogram_model_parameters[2], '\\n')\r\n        if self.enable_plotting:\r\n            self.display_variogram_model()\r\n\r\n        if self.verbose:\r\n            print(\"Calculating statistics on variogram model fit...\")\r\n        self.delta, self.sigma, self.epsilon = \\\r\n            _find_statistics(np.vstack((self.X_ADJUSTED, self.Y_ADJUSTED)).T,\r\n                             self.Z, self.variogram_function,\r\n                             self.variogram_model_parameters,\r\n                             'euclidean')\r\n        self.Q1 = core.calcQ1(self.epsilon)\r\n        self.Q2 = core.calcQ2(self.epsilon)\r\n        self.cR = core.calc_cR(self.Q2, self.sigma)\r\n        if self.verbose:\r\n            print(\"Q1 =\", self.Q1)\r\n            print(\"Q2 =\", self.Q2)\r\n            print(\"cR =\", self.cR, '\\n')"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndisplays the variogram model with the actual binned data.", "response": "def display_variogram_model(self):\r\n        \"\"\"Displays variogram model with the actual binned data.\"\"\"\r\n        fig = plt.figure()\r\n        ax = fig.add_subplot(111)\r\n        ax.plot(self.lags, self.semivariance, 'r*')\r\n        ax.plot(self.lags,\r\n                self.variogram_function(self.variogram_model_parameters,\r\n                                        self.lags), 'k-')\r\n        plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns both the lags and the variogram function evaluated at each of them.", "response": "def get_variogram_points(self):\r\n        \"\"\"Returns both the lags and the variogram function evaluated at each\r\n        of them.\r\n\r\n        The evaluation of the variogram function and the lags are produced\r\n        internally. This method is convenient when the user wants to access to\r\n        the lags and the resulting variogram (according to the model provided)\r\n        for further analysis.\r\n\r\n        Returns\r\n        -------\r\n        (tuple) tuple containing:\r\n            lags (array) - the lags at which the variogram was evaluated\r\n            variogram (array) - the variogram function evaluated at the lags\r\n        \"\"\"\r\n        return self.lags, self.variogram_function(self.variogram_model_parameters, self.lags)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nplot the epsilon residuals for the variogram fit.", "response": "def plot_epsilon_residuals(self):\r\n        \"\"\"Plots the epsilon residuals for the variogram fit.\"\"\"\r\n        fig = plt.figure()\r\n        ax = fig.add_subplot(111)\r\n        ax.scatter(range(self.epsilon.size), self.epsilon, c='k', marker='*')\r\n        ax.axhline(y=0.0)\r\n        plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprints out the Q1 Q2 and cR statistics for the variogram fit.", "response": "def print_statistics(self):\r\n        \"\"\"Prints out the Q1, Q2, and cR statistics for the variogram fit.\r\n        NOTE that ideally Q1 is close to zero, Q2 is close to 1,\r\n        and cR is as small as possible.\r\n        \"\"\"\r\n        print(\"Q1 =\", self.Q1)\r\n        print(\"Q2 =\", self.Q2)\r\n        print(\"cR =\", self.cR)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_kriging_matrix(self, n, n_withdrifts):\r\n\r\n        xy = np.concatenate((self.X_ADJUSTED[:, np.newaxis],\r\n                             self.Y_ADJUSTED[:, np.newaxis]), axis=1)\r\n        d = cdist(xy, xy, 'euclidean')\r\n        if self.UNBIAS:\r\n            a = np.zeros((n_withdrifts+1, n_withdrifts+1))\r\n        else:\r\n            a = np.zeros((n_withdrifts, n_withdrifts))\r\n        a[:n, :n] = - self.variogram_function(self.variogram_model_parameters, d)\r\n        np.fill_diagonal(a, 0.)\r\n\r\n        i = n\r\n        if self.regional_linear_drift:\r\n            a[:n, i] = self.X_ADJUSTED\r\n            a[i, :n] = self.X_ADJUSTED\r\n            i += 1\r\n            a[:n, i] = self.Y_ADJUSTED\r\n            a[i, :n] = self.Y_ADJUSTED\r\n            i += 1\r\n        if self.point_log_drift:\r\n            for well_no in range(self.point_log_array.shape[0]):\r\n                log_dist = np.log(np.sqrt((self.X_ADJUSTED - self.point_log_array[well_no, 0])**2 +\r\n                                          (self.Y_ADJUSTED - self.point_log_array[well_no, 1])**2))\r\n                if np.any(np.isinf(log_dist)):\r\n                    log_dist[np.isinf(log_dist)] = -100.0\r\n                a[:n, i] = - self.point_log_array[well_no, 2] * log_dist\r\n                a[i, :n] = - self.point_log_array[well_no, 2] * log_dist\r\n                i += 1\r\n        if self.external_Z_drift:\r\n            a[:n, i] = self.z_scalars\r\n            a[i, :n] = self.z_scalars\r\n            i += 1\r\n        if self.specified_drift:\r\n            for arr in self.specified_drift_data_arrays:\r\n                a[:n, i] = arr\r\n                a[i, :n] = arr\r\n                i += 1\r\n        if self.functional_drift:\r\n            for func in self.functional_drift_terms:\r\n                a[:n, i] = func(self.X_ADJUSTED, self.Y_ADJUSTED)\r\n                a[i, :n] = func(self.X_ADJUSTED, self.Y_ADJUSTED)\r\n                i += 1\r\n        if i != n_withdrifts:\r\n            warnings.warn(\"Error in creating kriging matrix. Kriging may fail.\",\r\n                          RuntimeWarning)\r\n        if self.UNBIAS:\r\n            a[n_withdrifts, :n] = 1.0\r\n            a[:n, n_withdrifts] = 1.0\r\n            a[n:n_withdrifts + 1, n:n_withdrifts + 1] = 0.0\r\n\r\n        return a", "response": "Assembles the kriging matrix."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nexecuting a vectorized operation.", "response": "def _exec_vector(self, a, bd, xy, xy_orig, mask,\r\n                     n_withdrifts, spec_drift_grids):\r\n        \"\"\"Solves the kriging system as a vectorized operation. This method\r\n        can take a lot of memory for large grids and/or large datasets.\"\"\"\r\n\r\n        npt = bd.shape[0]\r\n        n = self.X_ADJUSTED.shape[0]\r\n        zero_index = None\r\n        zero_value = False\r\n\r\n        a_inv = scipy.linalg.inv(a)\r\n\r\n        if np.any(np.absolute(bd) <= self.eps):\r\n            zero_value = True\r\n            zero_index = np.where(np.absolute(bd) <= self.eps)\r\n\r\n        if self.UNBIAS:\r\n            b = np.zeros((npt, n_withdrifts+1, 1))\r\n        else:\r\n            b = np.zeros((npt, n_withdrifts, 1))\r\n        b[:, :n, 0] = - self.variogram_function(self.variogram_model_parameters, bd)\r\n        if zero_value:\r\n            b[zero_index[0], zero_index[1], 0] = 0.0\r\n\r\n        i = n\r\n        if self.regional_linear_drift:\r\n            b[:, i, 0] = xy[:, 0]\r\n            i += 1\r\n            b[:, i, 0] = xy[:, 1]\r\n            i += 1\r\n        if self.point_log_drift:\r\n            for well_no in range(self.point_log_array.shape[0]):\r\n                log_dist = np.log(np.sqrt((xy[:, 0] - self.point_log_array[well_no, 0])**2 +\r\n                                          (xy[:, 1] - self.point_log_array[well_no, 1])**2))\r\n                if np.any(np.isinf(log_dist)):\r\n                    log_dist[np.isinf(log_dist)] = -100.0\r\n                b[:, i, 0] = - self.point_log_array[well_no, 2] * log_dist\r\n                i += 1\r\n        if self.external_Z_drift:\r\n            b[:, i, 0] = self._calculate_data_point_zscalars(xy_orig[:, 0], xy_orig[:, 1])\r\n            i += 1\r\n        if self.specified_drift:\r\n            for spec_vals in spec_drift_grids:\r\n                b[:, i, 0] = spec_vals.flatten()\r\n                i += 1\r\n        if self.functional_drift:\r\n            for func in self.functional_drift_terms:\r\n                b[:, i, 0] = func(xy[:, 0], xy[:, 1])\r\n                i += 1\r\n        if i != n_withdrifts:\r\n            warnings.warn(\"Error in setting up kriging system. \"\r\n                          \"Kriging may fail.\", RuntimeWarning)\r\n        if self.UNBIAS:\r\n            b[:, n_withdrifts, 0] = 1.0\r\n\r\n        if (~mask).any():\r\n            mask_b = np.repeat(mask[:, np.newaxis, np.newaxis],\r\n                               n_withdrifts+1, axis=1)\r\n            b = np.ma.array(b, mask=mask_b)\r\n\r\n        if self.UNBIAS:\r\n            x = np.dot(a_inv, b.reshape((npt, n_withdrifts+1)).T).reshape((1, n_withdrifts+1, npt)).T\r\n        else:\r\n            x = np.dot(a_inv, b.reshape((npt, n_withdrifts)).T).reshape((1, n_withdrifts, npt)).T\r\n        zvalues = np.sum(x[:, :n, 0] * self.Z, axis=1)\r\n        sigmasq = np.sum(x[:, :, 0] * -b[:, :, 0], axis=1)\r\n\r\n        return zvalues, sigmasq"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef execute(self, style, xpoints, ypoints, mask=None,\r\n                backend='vectorized', specified_drift_arrays=None):\r\n        \"\"\"Calculates a kriged grid and the associated variance.\r\n        Includes drift terms.\r\n\r\n        This is now the method that performs the main kriging calculation.\r\n        Note that currently measurements (i.e., z values) are considered\r\n        'exact'. This means that, when a specified coordinate for interpolation\r\n        is exactly the same as one of the data points, the variogram evaluated\r\n        at the point is forced to be zero. Also, the diagonal of the kriging\r\n        matrix is also always forced to be zero. In forcing the variogram\r\n        evaluated at data points to be zero, we are effectively saying that\r\n        there is no variance at that point (no uncertainty,\r\n        so the value is 'exact').\r\n\r\n        In the future, the code may include an extra 'exact_values' boolean\r\n        flag that can be adjusted to specify whether to treat the measurements\r\n        as 'exact'. Setting the flag to false would indicate that the variogram\r\n        should not be forced to be zero at zero distance (i.e., when evaluated\r\n        at data points). Instead, the uncertainty in the point will be equal to\r\n        the nugget. This would mean that the diagonal of the kriging matrix\r\n        would be set to the nugget instead of to zero.\r\n\r\n        Parameters\r\n        ----------\r\n        style : str\r\n            Specifies how to treat input kriging points. Specifying 'grid'\r\n            treats xpoints and ypoints as two arrays of x and y coordinates\r\n            that define a rectangular grid. Specifying 'points' treats xpoints\r\n            and ypoints as two arrays that provide coordinate pairs at which\r\n            to solve the kriging system. Specifying 'masked' treats xpoints and\r\n            ypoints as two arrays of x and y coordinates that define a\r\n            rectangular grid and uses mask to only evaluate specific points\r\n            in the grid.\r\n        xpoints : array_like, shape (N,) or (N, 1)\r\n            If style is specific as 'grid' or 'masked', x-coordinates of\r\n            MxN grid. If style is specified as 'points', x-coordinates of\r\n            specific points at which to solve kriging system.\r\n        ypoints : array-like, shape (M,) or (M, 1)\r\n            If style is specified as 'grid' or 'masked', y-coordinates of\r\n            MxN grid. If style is specified as 'points', y-coordinates of\r\n            specific points at which to solve kriging system.\r\n            Note that in this case, xpoints and ypoints must have the same\r\n            dimensions (i.e., M = N).\r\n        mask : boolean array, shape (M, N), optional\r\n            Specifies the points in the rectangular grid defined by xpoints and\r\n            ypoints that are to be excluded in the kriging calculations.\r\n            Must be provided if style is specified as 'masked'. False indicates\r\n            that the point should not be masked, so the kriging system will be\r\n            solved at the point. True indicates that the point should be masked,\r\n            so the kriging system should will not be solved at the point.\r\n        backend : str, optional\r\n            Specifies which approach to use in kriging. Specifying 'vectorized'\r\n            will solve the entire kriging problem at once in a vectorized\r\n            operation. This approach is faster but also can consume a\r\n            significant amount of memory for large grids and/or large datasets.\r\n            Specifying 'loop' will loop through each point at which the kriging\r\n            system is to be solved. This approach is slower but also less\r\n            memory-intensive. Default is 'vectorized'.\r\n            Note that Cython backend is not supported for UK.\r\n        specified_drift_arrays : list of array-like objects, optional\r\n            Specifies the drift values at the points at which the kriging\r\n            system is to be evaluated. Required if 'specified' drift provided\r\n            in the list of drift terms when instantiating the UniversalKriging\r\n            class. Must be a list of arrays in the same order as the list\r\n            provided when instantiating the kriging object. Array(s) must be\r\n            the same dimension as the specified grid or have the same number of\r\n            points as the specified points; i.e., the arrays either must be\r\n            shape (M, N), where M is the number of y grid-points and N is the\r\n            number of x grid-points, or shape (M, ) or (N, 1), where M is the\r\n            number of  points at which to evaluate the kriging system.\r\n\r\n        Returns\r\n        -------\r\n        zvalues : ndarray, shape (M, N) or (N, 1)\r\n            Z-values of specified grid or at the specified set of points.\r\n            If style was specified as 'masked', zvalues will be a numpy\r\n            masked array.\r\n        sigmasq : ndarray, shape (M, N) or (N, 1)\r\n            Variance at specified grid points or at the specified set of points.\r\n            If style was specified as 'masked', sigmasq will be a numpy\r\n            masked array.\r\n        \"\"\"\r\n\r\n        if self.verbose:\r\n            print(\"Executing Universal Kriging...\\n\")\r\n\r\n        if style != 'grid' and style != 'masked' and style != 'points':\r\n            raise ValueError(\"style argument must be 'grid', 'points', \"\r\n                             \"or 'masked'\")\r\n\r\n        n = self.X_ADJUSTED.shape[0]\r\n        n_withdrifts = n\r\n        xpts = np.atleast_1d(np.squeeze(np.array(xpoints, copy=True)))\r\n        ypts = np.atleast_1d(np.squeeze(np.array(ypoints, copy=True)))\r\n        nx = xpts.size\r\n        ny = ypts.size\r\n        if self.regional_linear_drift:\r\n            n_withdrifts += 2\r\n        if self.point_log_drift:\r\n            n_withdrifts += self.point_log_array.shape[0]\r\n        if self.external_Z_drift:\r\n            n_withdrifts += 1\r\n        if self.specified_drift:\r\n            n_withdrifts += len(self.specified_drift_data_arrays)\r\n        if self.functional_drift:\r\n            n_withdrifts += len(self.functional_drift_terms)\r\n        a = self._get_kriging_matrix(n, n_withdrifts)\r\n\r\n        if style in ['grid', 'masked']:\r\n            if style == 'masked':\r\n                if mask is None:\r\n                    raise IOError(\"Must specify boolean masking array when \"\r\n                                  \"style is 'masked'.\")\r\n                if mask.shape[0] != ny or mask.shape[1] != nx:\r\n                    if mask.shape[0] == nx and mask.shape[1] == ny:\r\n                        mask = mask.T\r\n                    else:\r\n                        raise ValueError(\"Mask dimensions do not match \"\r\n                                         \"specified grid dimensions.\")\r\n                mask = mask.flatten()\r\n            npt = ny*nx\r\n            grid_x, grid_y = np.meshgrid(xpts, ypts)\r\n            xpts = grid_x.flatten()\r\n            ypts = grid_y.flatten()\r\n\r\n        elif style == 'points':\r\n            if xpts.size != ypts.size:\r\n                raise ValueError(\"xpoints and ypoints must have same \"\r\n                                 \"dimensions when treated as listing \"\r\n                                 \"discrete points.\")\r\n            npt = nx\r\n        else:\r\n            raise ValueError(\"style argument must be 'grid', 'points', \"\r\n                             \"or 'masked'\")\r\n\r\n        if specified_drift_arrays is None:\r\n            specified_drift_arrays = []\r\n        spec_drift_grids = []\r\n        if self.specified_drift:\r\n            if len(specified_drift_arrays) == 0:\r\n                raise ValueError(\"Must provide drift values for kriging points \"\r\n                                 \"when using 'specified' drift capability.\")\r\n            if type(specified_drift_arrays) is not list:\r\n                raise TypeError(\"Arrays for specified drift terms must be \"\r\n                                \"encapsulated in a list.\")\r\n            for spec in specified_drift_arrays:\r\n                if style in ['grid', 'masked']:\r\n                    if spec.ndim < 2:\r\n                        raise ValueError(\"Dimensions of drift values array do \"\r\n                                         \"not match specified grid dimensions.\")\r\n                    elif spec.shape[0] != ny or spec.shape[1] != nx:\r\n                        if spec.shape[0] == nx and spec.shape[1] == ny:\r\n                            spec_drift_grids.append(np.squeeze(spec.T))\r\n                        else:\r\n                            raise ValueError(\"Dimensions of drift values array \"\r\n                                             \"do not match specified grid \"\r\n                                             \"dimensions.\")\r\n                    else:\r\n                        spec_drift_grids.append(np.squeeze(spec))\r\n                elif style == 'points':\r\n                    if spec.ndim != 1:\r\n                        raise ValueError(\"Dimensions of drift values array do \"\r\n                                         \"not match specified grid dimensions.\")\r\n                    elif spec.shape[0] != xpts.size:\r\n                        raise ValueError(\"Number of supplied drift values in \"\r\n                                         \"array do not match specified number \"\r\n                                         \"of kriging points.\")\r\n                    else:\r\n                        spec_drift_grids.append(np.squeeze(spec))\r\n            if len(spec_drift_grids) != len(self.specified_drift_data_arrays):\r\n                raise ValueError(\"Inconsistent number of specified drift \"\r\n                                 \"terms supplied.\")\r\n        else:\r\n            if len(specified_drift_arrays) != 0:\r\n                warnings.warn(\"Provided specified drift values, but \"\r\n                              \"'specified' drift was not initialized during \"\r\n                              \"instantiation of UniversalKriging class.\",\r\n                              RuntimeWarning)\r\n\r\n        xy_points_original = \\\r\n            np.concatenate((xpts[:, np.newaxis], ypts[:, np.newaxis]), axis=1)\r\n        xpts, ypts = _adjust_for_anisotropy(np.vstack((xpts, ypts)).T,\r\n                                            [self.XCENTER, self.YCENTER],\r\n                                            [self.anisotropy_scaling],\r\n                                            [self.anisotropy_angle]).T\r\n        xy_points = \\\r\n            np.concatenate((xpts[:, np.newaxis], ypts[:, np.newaxis]), axis=1)\r\n        xy_data = np.concatenate((self.X_ADJUSTED[:, np.newaxis],\r\n                                  self.Y_ADJUSTED[:, np.newaxis]), axis=1)\r\n\r\n        if style != 'masked':\r\n            mask = np.zeros(npt, dtype='bool')\r\n\r\n        bd = cdist(xy_points,  xy_data, 'euclidean')\r\n        if backend == 'vectorized':\r\n            zvalues, sigmasq = self._exec_vector(a, bd, xy_points,\r\n                                                 xy_points_original,\r\n                                                 mask, n_withdrifts,\r\n                                                 spec_drift_grids)\r\n        elif backend == 'loop':\r\n            zvalues, sigmasq = self._exec_loop(a, bd, xy_points,\r\n                                               xy_points_original,\r\n                                               mask, n_withdrifts,\r\n                                               spec_drift_grids)\r\n        else:\r\n            raise ValueError('Specified backend {} is not supported '\r\n                             'for 2D universal kriging.'.format(backend))\r\n\r\n        if style == 'masked':\r\n            zvalues = np.ma.array(zvalues, mask=mask)\r\n            sigmasq = np.ma.array(sigmasq, mask=mask)\r\n\r\n        if style in ['masked', 'grid']:\r\n            zvalues = zvalues.reshape((ny, nx))\r\n            sigmasq = sigmasq.reshape((ny, nx))\r\n\r\n        return zvalues, sigmasq", "response": "Executes the main kriging calculation and returns the resulting array of values."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef linear_variogram_model(m, d):\r\n    slope = float(m[0])\r\n    nugget = float(m[1])\r\n    return slope * d + nugget", "response": "Linear model, m is [slope, nugget]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef power_variogram_model(m, d):\r\n    scale = float(m[0])\r\n    exponent = float(m[1])\r\n    nugget = float(m[2])\r\n    return scale * d**exponent + nugget", "response": "Returns the power variogram model of a given variogram."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef spherical_variogram_model(m, d):\r\n    psill = float(m[0])\r\n    range_ = float(m[1])\r\n    nugget = float(m[2])\r\n    return np.piecewise(d, [d <= range_, d > range_],\r\n                        [lambda x: psill * ((3.*x)/(2.*range_) - (x**3.)/(2.*range_**3.)) + nugget, psill + nugget])", "response": "Spherical variogram model m is [ psill range nugget ]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_kriging_matrix(self, n):\r\n\r\n        if self.coordinates_type == 'euclidean':\r\n            xy = np.concatenate((self.X_ADJUSTED[:, np.newaxis],\r\n                                 self.Y_ADJUSTED[:, np.newaxis]), axis=1)\r\n            d = cdist(xy, xy, 'euclidean')\r\n        elif self.coordinates_type == 'geographic':\r\n            d = core.great_circle_distance(self.X_ADJUSTED[:,np.newaxis],\r\n                                           self.Y_ADJUSTED[:,np.newaxis],\r\n                                           self.X_ADJUSTED, self.Y_ADJUSTED)\r\n        a = np.zeros((n+1, n+1))\r\n        a[:n, :n] = - self.variogram_function(self.variogram_model_parameters,\r\n                                              d)\r\n        np.fill_diagonal(a, 0.)\r\n        a[n, :] = 1.0\r\n        a[:, n] = 1.0\r\n        a[n, n] = 0.0\r\n\r\n        return a", "response": "Assembles the kriging matrix."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _exec_vector(self, a, bd, mask):\r\n\r\n        npt = bd.shape[0]\r\n        n = self.X_ADJUSTED.shape[0]\r\n        zero_index = None\r\n        zero_value = False\r\n\r\n        a_inv = scipy.linalg.inv(a)\r\n\r\n        if np.any(np.absolute(bd) <= self.eps):\r\n            zero_value = True\r\n            zero_index = np.where(np.absolute(bd) <= self.eps)\r\n\r\n        b = np.zeros((npt, n+1, 1))\r\n        b[:, :n, 0] = - self.variogram_function(self.variogram_model_parameters, bd)\r\n        if zero_value:\r\n            b[zero_index[0], zero_index[1], 0] = 0.0\r\n        b[:, n, 0] = 1.0\r\n\r\n        if (~mask).any():\r\n            mask_b = np.repeat(mask[:, np.newaxis, np.newaxis], n+1, axis=1)\r\n            b = np.ma.array(b, mask=mask_b)\r\n\r\n        x = np.dot(a_inv, b.reshape((npt, n+1)).T).reshape((1, n+1, npt)).T\r\n        zvalues = np.sum(x[:, :n, 0] * self.Z, axis=1)\r\n        sigmasq = np.sum(x[:, :, 0] * -b[:, :, 0], axis=1)\r\n\r\n        return zvalues, sigmasq", "response": "Solves the kriging system as a vectorized operation."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef execute(self, style, xpoints, ypoints, mask=None, backend='vectorized',\r\n                n_closest_points=None):\r\n        \"\"\"Calculates a kriged grid and the associated variance.\r\n\r\n        This is now the method that performs the main kriging calculation.\r\n        Note that currently measurements (i.e., z values) are considered\r\n        'exact'. This means that, when a specified coordinate for interpolation\r\n        is exactly the same as one of the data points, the variogram evaluated\r\n        at the point is forced to be zero. Also, the diagonal of the kriging\r\n        matrix is also always forced to be zero. In forcing the variogram\r\n        evaluated at data points to be zero, we are effectively saying that\r\n        there is no variance at that point (no uncertainty,\r\n        so the value is 'exact').\r\n\r\n        In the future, the code may include an extra 'exact_values' boolean\r\n        flag that can be adjusted to specify whether to treat the measurements\r\n        as 'exact'. Setting the flag to false would indicate that the\r\n        variogram should not be forced to be zero at zero distance\r\n        (i.e., when evaluated at data points). Instead, the uncertainty in\r\n        the point will be equal to the nugget. This would mean that the\r\n        diagonal of the kriging matrix would be set to\r\n        the nugget instead of to zero.\r\n\r\n        Parameters\r\n        ----------\r\n        style : str\r\n            Specifies how to treat input kriging points. Specifying 'grid'\r\n            treats xpoints and ypoints as two arrays of x and y coordinates\r\n            that define a rectangular grid. Specifying 'points' treats\r\n            xpoints and ypoints as two arrays that provide coordinate pairs\r\n            at which to solve the kriging system. Specifying 'masked'\r\n            treats xpoints and ypoints as two arrays of x and y coordinates\r\n            that define a rectangular grid and uses mask to only evaluate\r\n            specific points in the grid.\r\n        xpoints : array_like, shape (N,) or (N, 1)\r\n            If style is specific as 'grid' or 'masked',\r\n            x-coordinates of MxN grid. If style is specified as 'points',\r\n            x-coordinates of specific points at which to solve\r\n            kriging system.\r\n        ypoints : array_like, shape (M,) or (M, 1)\r\n            If style is specified as 'grid' or 'masked',\r\n            y-coordinates of MxN grid. If style is specified as 'points',\r\n            y-coordinates of specific points at which to solve kriging\r\n            system. Note that in this case, xpoints and ypoints must have\r\n            the same dimensions (i.e., M = N).\r\n        mask : bool, array_like, shape (M, N), optional\r\n            Specifies the points in the rectangular grid defined\r\n            by xpoints and ypoints that are to be excluded in the\r\n            kriging calculations. Must be provided if style is specified\r\n            as 'masked'. False indicates that the point should not be\r\n            masked, so the kriging system will be solved at the point.\r\n            True indicates that the point should be masked, so the kriging\r\n            system should will not be solved at the point.\r\n        backend : str, optional\r\n            Specifies which approach to use in kriging.\r\n            Specifying 'vectorized' will solve the entire kriging problem\r\n            at once in a vectorized operation. This approach is faster but\r\n            also can consume a significant amount of memory for large grids\r\n            and/or large datasets. Specifying 'loop' will loop through each\r\n            point at which the kriging system is to be solved.\r\n            This approach is slower but also less memory-intensive.\r\n            Specifying 'C' will utilize a loop in Cython.\r\n            Default is 'vectorized'.\r\n        n_closest_points : int, optional\r\n            For kriging with a moving window, specifies the number of\r\n            nearby points to use in the calculation. This can speed up the\r\n            calculation for large datasets, but should be used\r\n            with caution. As Kitanidis notes, kriging with a moving window\r\n            can produce unexpected oddities if the variogram model\r\n            is not carefully chosen.\r\n\r\n        Returns\r\n        -------\r\n        zvalues : ndarray, shape (M, N) or (N, 1)\r\n            Z-values of specified grid or at the specified set of points.\r\n            If style was specified as 'masked', zvalues will\r\n            be a numpy masked array.\r\n        sigmasq : ndarray, shape (M, N) or (N, 1)\r\n            Variance at specified grid points or at the specified\r\n            set of points. If style was specified as 'masked', sigmasq\r\n            will be a numpy masked array.\r\n        \"\"\"\r\n\r\n        if self.verbose:\r\n            print(\"Executing Ordinary Kriging...\\n\")\r\n\r\n        if style != 'grid' and style != 'masked' and style != 'points':\r\n            raise ValueError(\"style argument must be 'grid', \"\r\n                             \"'points', or 'masked'\")\r\n\r\n        if n_closest_points is not None and n_closest_points <= 1:\r\n            # If this is not checked, nondescriptive errors emerge\r\n            # later in the code.\r\n            raise ValueError(\"n_closest_points has to be at least two!\")\r\n\r\n        xpts = np.atleast_1d(np.squeeze(np.array(xpoints, copy=True)))\r\n        ypts = np.atleast_1d(np.squeeze(np.array(ypoints, copy=True)))\r\n        n = self.X_ADJUSTED.shape[0]\r\n        nx = xpts.size\r\n        ny = ypts.size\r\n        a = self._get_kriging_matrix(n)\r\n\r\n        if style in ['grid', 'masked']:\r\n            if style == 'masked':\r\n                if mask is None:\r\n                    raise IOError(\"Must specify boolean masking array \"\r\n                                  \"when style is 'masked'.\")\r\n                if mask.shape[0] != ny or mask.shape[1] != nx:\r\n                    if mask.shape[0] == nx and mask.shape[1] == ny:\r\n                        mask = mask.T\r\n                    else:\r\n                        raise ValueError(\"Mask dimensions do not match \"\r\n                                         \"specified grid dimensions.\")\r\n                mask = mask.flatten()\r\n            npt = ny*nx\r\n            grid_x, grid_y = np.meshgrid(xpts, ypts)\r\n            xpts = grid_x.flatten()\r\n            ypts = grid_y.flatten()\r\n\r\n        elif style == 'points':\r\n            if xpts.size != ypts.size:\r\n                raise ValueError(\"xpoints and ypoints must have \"\r\n                                 \"same dimensions when treated as \"\r\n                                 \"listing discrete points.\")\r\n            npt = nx\r\n        else:\r\n            raise ValueError(\"style argument must be 'grid', \"\r\n                             \"'points', or 'masked'\")\r\n\r\n        if self.coordinates_type == 'euclidean':\r\n            xpts, ypts = _adjust_for_anisotropy(np.vstack((xpts, ypts)).T,\r\n                                                [self.XCENTER, self.YCENTER],\r\n                                                [self.anisotropy_scaling],\r\n                                                [self.anisotropy_angle]).T\r\n            # Prepare for cdist:\r\n            xy_data = np.concatenate((self.X_ADJUSTED[:, np.newaxis],\r\n                                      self.Y_ADJUSTED[:, np.newaxis]), axis=1)\r\n            xy_points = np.concatenate((xpts[:, np.newaxis],\r\n                                        ypts[:, np.newaxis]), axis=1)\r\n        elif self.coordinates_type == 'geographic':\r\n            # In spherical coordinates, we do not correct for anisotropy.\r\n            # Also, we don't use scipy.spatial.cdist, so we do not have to\r\n            # format the input data accordingly.\r\n            pass\r\n\r\n        if style != 'masked':\r\n            mask = np.zeros(npt, dtype='bool')\r\n\r\n        c_pars = None\r\n        if backend == 'C':\r\n            try:\r\n                from .lib.cok import _c_exec_loop, _c_exec_loop_moving_window\r\n            except ImportError:\r\n                print('Warning: failed to load Cython extensions.\\n'\r\n                      '   See https://github.com/bsmurphy/PyKrige/issues/8 \\n'\r\n                      '   Falling back to a pure python backend...')\r\n                backend = 'loop'\r\n            except:\r\n                raise RuntimeError(\"Unknown error in trying to \"\r\n                                   \"load Cython extension.\")\r\n\r\n            c_pars = {key: getattr(self, key) for key in ['Z', 'eps', 'variogram_model_parameters', 'variogram_function']}\r\n\r\n        if n_closest_points is not None:\r\n            if self.coordinates_type == 'geographic':\r\n                # To make use of the KDTree, we have to convert the\r\n                # spherical coordinates into three dimensional Euclidean\r\n                # coordinates, since the standard KDTree cannot handle\r\n                # the periodicity.\r\n                # Do the conversion just for the step involving the KDTree:\r\n                lon_d = self.X_ADJUSTED[:, np.newaxis] * np.pi / 180.0\r\n                lat_d = self.Y_ADJUSTED[:, np.newaxis] * np.pi / 180.0\r\n                xy_data = np.concatenate((np.cos(lon_d) * np.cos(lat_d),\r\n                                          np.sin(lon_d) * np.cos(lat_d),\r\n                                          np.sin(lat_d)), axis=1)\r\n                lon_p = xpts[:, np.newaxis] * np.pi / 180.0\r\n                lat_p = ypts[:, np.newaxis] * np.pi / 180.0\r\n                xy_points = np.concatenate((np.cos(lon_p) * np.cos(lat_p),\r\n                                            np.sin(lon_p) * np.cos(lat_p),\r\n                                            np.sin(lat_p)), axis=1)\r\n\r\n            from scipy.spatial import cKDTree\r\n            tree = cKDTree(xy_data)\r\n            bd, bd_idx = tree.query(xy_points, k=n_closest_points, eps=0.0)\r\n\r\n            if self.coordinates_type == 'geographic':\r\n                # Between the nearest neighbours from Euclidean search,\r\n                # calculate the great circle distance using the standard method:\r\n                x_points = np.tile(xpts[:,np.newaxis],(1,n_closest_points))\r\n                y_points = np.tile(ypts[:,np.newaxis],(1,n_closest_points))\r\n                bd = core.great_circle_distance(x_points, y_points,\r\n                                                self.X_ADJUSTED[bd_idx],\r\n                                                self.Y_ADJUSTED[bd_idx])\r\n\r\n            if backend == 'loop':\r\n                zvalues, sigmasq = \\\r\n                    self._exec_loop_moving_window(a, bd, mask, bd_idx)\r\n            elif backend == 'C':\r\n                zvalues, sigmasq = \\\r\n                    _c_exec_loop_moving_window(a, bd, mask.astype('int8'),\r\n                                               bd_idx, self.X_ADJUSTED.shape[0],\r\n                                               c_pars)\r\n            else:\r\n                raise ValueError('Specified backend {} for a moving window '\r\n                                 'is not supported.'.format(backend))\r\n        else:\r\n            if self.coordinates_type == 'euclidean':\r\n                bd = cdist(xy_points,  xy_data, 'euclidean')\r\n            elif self.coordinates_type == 'geographic':\r\n                bd = core.great_circle_distance(xpts[:,np.newaxis],\r\n                                                ypts[:,np.newaxis],\r\n                                                self.X_ADJUSTED, self.Y_ADJUSTED)\r\n\r\n            if backend == 'vectorized':\r\n                zvalues, sigmasq = self._exec_vector(a, bd, mask)\r\n            elif backend == 'loop':\r\n                zvalues, sigmasq = self._exec_loop(a, bd, mask)\r\n            elif backend == 'C':\r\n                zvalues, sigmasq = _c_exec_loop(a, bd, mask.astype('int8'),\r\n                                                self.X_ADJUSTED.shape[0],\r\n                                                c_pars)\r\n            else:\r\n                raise ValueError('Specified backend {} is not supported for '\r\n                                 '2D ordinary kriging.'.format(backend))\r\n\r\n        if style == 'masked':\r\n            zvalues = np.ma.array(zvalues, mask=mask)\r\n            sigmasq = np.ma.array(sigmasq, mask=mask)\r\n\r\n        if style in ['masked', 'grid']:\r\n            zvalues = zvalues.reshape((ny, nx))\r\n            sigmasq = sigmasq.reshape((ny, nx))\r\n\r\n        return zvalues, sigmasq", "response": "This method is called by the kernel to calculate a new grid and the associated variance."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef great_circle_distance(lon1, lat1, lon2, lat2):\n    # Convert to radians:\n    lat1 = np.array(lat1)*np.pi/180.0\n    lat2 = np.array(lat2)*np.pi/180.0\n    dlon = (lon1-lon2)*np.pi/180.0\n\n    # Evaluate trigonometric functions that need to be evaluated more\n    # than once:\n    c1 = np.cos(lat1)\n    s1 = np.sin(lat1)\n    c2 = np.cos(lat2)\n    s2 = np.sin(lat2)\n    cd = np.cos(dlon)\n\n    # This uses the arctan version of the great-circle distance function\n    # from en.wikipedia.org/wiki/Great-circle_distance for increased\n    # numerical stability.\n    # Formula can be obtained from [2] combining eqns. (14)-(16)\n    # for spherical geometry (f=0).\n\n    return 180.0 / np.pi * np.arctan2(np.sqrt((c2*np.sin(dlon))**2 + (c1*s2-s1*c2*cd)**2), s1*s2+c1*c2*cd)", "response": "Calculates the great circle distance between two points given in spherical coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadjust data coordinates to take into account anisotropy.", "response": "def _adjust_for_anisotropy(X, center, scaling, angle):\n    \"\"\"Adjusts data coordinates to take into account anisotropy.\n    Can also be used to take into account data scaling. Angles are CCW about\n    specified axes. Scaling is applied in rotated coordinate system.\n\n    Parameters\n    ----------\n    X : ndarray\n        float array [n_samples, n_dim], the input array of coordinates\n    center : ndarray\n        float array [n_dim], the coordinate of centers\n    scaling : ndarray\n        float array [n_dim - 1], the scaling of last two dimensions\n    angle : ndarray\n        float array [2*n_dim - 3], the anisotropy angle (degrees)\n\n    Returns\n    -------\n    X_adj : ndarray\n        float array [n_samples, n_dim], the X array adjusted for anisotropy.\n    \"\"\"\n\n    center = np.asarray(center)[None, :]\n    angle = np.asarray(angle)*np.pi/180\n\n    X -= center\n\n    Ndim = X.shape[1]\n\n    if Ndim == 1:\n        raise NotImplementedError('Not implemnented yet?')\n    elif Ndim == 2:\n        stretch = np.array([[1, 0], [0, scaling[0]]])\n        rot_tot = np.array([[np.cos(-angle[0]), -np.sin(-angle[0])],\n                           [np.sin(-angle[0]), np.cos(-angle[0])]])\n    elif Ndim == 3:\n        stretch = np.array([[1., 0., 0.], [0., scaling[0], 0.], [0., 0., scaling[1]]])\n        rotate_x = np.array([[1., 0., 0.],\n                             [0., np.cos(-angle[0]), -np.sin(-angle[0])],\n                             [0., np.sin(-angle[0]), np.cos(-angle[0])]])\n        rotate_y = np.array([[np.cos(-angle[1]), 0., np.sin(-angle[1])],\n                             [0., 1., 0.],\n                             [-np.sin(-angle[1]), 0., np.cos(-angle[1])]])\n        rotate_z = np.array([[np.cos(-angle[2]), -np.sin(-angle[2]), 0.],\n                             [np.sin(-angle[2]), np.cos(-angle[2]), 0.],\n                             [0., 0., 1.]])\n        rot_tot = np.dot(rotate_z, np.dot(rotate_y, rotate_x))\n    else:\n        raise ValueError(\"Adjust for anisotropy function doesn't \"\n                         \"support ND spaces where N>3\")\n    X_adj = np.dot(stretch, np.dot(rot_tot, X.T)).T\n\n    X_adj += center\n\n    return X_adj"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert the user input for the variogram model parameters into a list of variogram model parameters.", "response": "def _make_variogram_parameter_list(variogram_model, variogram_model_parameters):\n    \"\"\"Converts the user input for the variogram model parameters into the\n    format expected in the rest of the code.\n\n    Makes a list of variogram model parameters in the expected order if the\n    user has provided the model parameters. If not, returns None, which\n    will ensure that the automatic variogram estimation routine is\n    triggered.\n\n    Parameters\n    ----------\n    variogram_model : str\n        specifies the variogram model type\n    variogram_model_parameters : list, dict, or None\n        parameters provided by the user, can also be None if the user\n        did not specify the variogram model parameters; if None,\n        this function returns None, that way the automatic variogram\n        estimation routine will kick in down the road...\n\n    Returns\n    -------\n    parameter_list : list\n        variogram model parameters stored in a list in the expected order;\n        if variogram_model is 'custom', model parameters should already\n        be encapsulated in a list, so the list is returned unaltered;\n        if variogram_model_parameters was not specified by the user,\n        None is returned; order for internal variogram models is as follows...\n\n        linear - [slope, nugget]\n        power - [scale, exponent, nugget]\n        gaussian - [psill, range, nugget]\n        spherical - [psill, range, nugget]\n        exponential - [psill, range, nugget]\n        hole-effect - [psill, range, nugget]\n\n    \"\"\"\n\n    if variogram_model_parameters is None:\n\n        parameter_list = None\n\n    elif type(variogram_model_parameters) is dict:\n\n        if variogram_model in ['linear']:\n\n            if 'slope' not in variogram_model_parameters.keys() \\\n                    or 'nugget' not in variogram_model_parameters.keys():\n\n                raise KeyError(\"'linear' variogram model requires 'slope' \"\n                               \"and 'nugget' specified in variogram model \"\n                               \"parameter dictionary.\")\n\n            else:\n\n                parameter_list = [variogram_model_parameters['slope'],\n                                  variogram_model_parameters['nugget']]\n\n        elif variogram_model in ['power']:\n\n            if 'scale' not in variogram_model_parameters.keys() \\\n                    or 'exponent' not in variogram_model_parameters.keys() \\\n                    or 'nugget' not in variogram_model_parameters.keys():\n\n                raise KeyError(\"'power' variogram model requires 'scale', \"\n                               \"'exponent', and 'nugget' specified in \"\n                               \"variogram model parameter dictionary.\")\n\n            else:\n\n                parameter_list = [variogram_model_parameters['scale'],\n                                  variogram_model_parameters['exponent'],\n                                  variogram_model_parameters['nugget']]\n\n        elif variogram_model in ['gaussian', 'spherical', 'exponential',\n                                 'hole-effect']:\n\n            if 'range' not in variogram_model_parameters.keys() \\\n                    or 'nugget' not in variogram_model_parameters.keys():\n\n                raise KeyError(\"'%s' variogram model requires 'range', \"\n                               \"'nugget', and either 'sill' or 'psill' \"\n                               \"specified in variogram model parameter \"\n                               \"dictionary.\" % variogram_model)\n\n            else:\n\n                if 'sill' in variogram_model_parameters.keys():\n\n                    parameter_list = [variogram_model_parameters['sill'] -\n                                      variogram_model_parameters['nugget'],\n                                      variogram_model_parameters['range'],\n                                      variogram_model_parameters['nugget']]\n\n                elif 'psill' in variogram_model_parameters.keys():\n\n                    parameter_list = [variogram_model_parameters['psill'],\n                                      variogram_model_parameters['range'],\n                                      variogram_model_parameters['nugget']]\n\n                else:\n\n                    raise KeyError(\"'%s' variogram model requires either \"\n                                   \"'sill' or 'psill' specified in \"\n                                   \"variogram model parameter \"\n                                   \"dictionary.\" % variogram_model)\n\n        elif variogram_model in ['custom']:\n\n            raise TypeError(\"For user-specified custom variogram model, \"\n                            \"parameters must be specified in a list, \"\n                            \"not a dict.\")\n\n        else:\n\n            raise ValueError(\"Specified variogram model must be one of the \"\n                             \"following: 'linear', 'power', 'gaussian', \"\n                             \"'spherical', 'exponential', 'hole-effect', \"\n                             \"'custom'.\")\n\n    elif type(variogram_model_parameters) is list:\n\n        if variogram_model in ['linear']:\n\n            if len(variogram_model_parameters) != 2:\n\n                raise ValueError(\"Variogram model parameter list must have \"\n                                 \"exactly two entries when variogram model \"\n                                 \"set to 'linear'.\")\n\n            parameter_list = variogram_model_parameters\n\n        elif variogram_model in ['power']:\n\n            if len(variogram_model_parameters) != 3:\n\n                raise ValueError(\"Variogram model parameter list must have \"\n                                 \"exactly three entries when variogram model \"\n                                 \"set to 'power'.\")\n\n            parameter_list = variogram_model_parameters\n\n        elif variogram_model in ['gaussian', 'spherical', 'exponential',\n                                 'hole-effect']:\n\n            if len(variogram_model_parameters) != 3:\n\n                raise ValueError(\"Variogram model parameter list must have \"\n                                 \"exactly three entries when variogram model \"\n                                 \"set to '%s'.\" % variogram_model)\n\n            parameter_list = [variogram_model_parameters[0] -\n                              variogram_model_parameters[2],\n                              variogram_model_parameters[1],\n                              variogram_model_parameters[2]]\n\n        elif variogram_model in ['custom']:\n\n            parameter_list = variogram_model_parameters\n\n        else:\n\n            raise ValueError(\"Specified variogram model must be one of the \"\n                             \"following: 'linear', 'power', 'gaussian', \"\n                             \"'spherical', 'exponential', 'hole-effect', \"\n                             \"'custom'.\")\n\n    else:\n\n        raise TypeError(\"Variogram model parameters must be provided in either \"\n                        \"a list or a dict when they are explicitly specified.\")\n\n    return parameter_list"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninitialize the variogram model for kriging.", "response": "def _initialize_variogram_model(X, y, variogram_model,\n                                variogram_model_parameters, variogram_function,\n                                nlags, weight, coordinates_type):\n    \"\"\"Initializes the variogram model for kriging. If user does not specify\n    parameters, calls automatic variogram estimation routine.\n    Returns lags, semivariance, and variogram model parameters.\n\n    Parameters\n    ----------\n    X: ndarray\n        float array [n_samples, n_dim], the input array of coordinates\n    y: ndarray\n        float array [n_samples], the input array of values to be kriged\n    variogram_model: str\n        user-specified variogram model to use\n    variogram_model_parameters: list\n        user-specified parameters for variogram model\n    variogram_function: callable\n        function that will be called to evaluate variogram model\n        (only used if user does not specify variogram model parameters)\n    nlags: int\n        integer scalar, number of bins into which to group inter-point distances\n    weight: bool\n        boolean flag that indicates whether the semivariances at smaller lags\n        should be weighted more heavily in the automatic variogram estimation\n    coordinates_type: str\n        type of coordinates in X array, can be 'euclidean' for standard\n        rectangular coordinates or 'geographic' if the coordinates are lat/lon\n\n    Returns\n    -------\n    lags: ndarray\n        float array [nlags], distance values for bins into which the\n        semivariances were grouped\n    semivariance: ndarray\n        float array [nlags], averaged semivariance for each bin\n    variogram_model_parameters: list\n        parameters for the variogram model, either returned unaffected if the\n        user specified them or returned from the automatic variogram\n        estimation routine\n    \"\"\"\n\n    # distance calculation for rectangular coords now leverages\n    # scipy.spatial.distance's pdist function, which gives pairwise distances\n    # in a condensed distance vector (distance matrix flattened to a vector)\n    # to calculate semivariances...\n    if coordinates_type == 'euclidean':\n        d = pdist(X, metric='euclidean')\n        g = 0.5 * pdist(y[:, None], metric='sqeuclidean')\n\n    # geographic coordinates only accepted if the problem is 2D\n    # assume X[:, 0] ('x') => lon, X[:, 1] ('y') => lat\n    # old method of distance calculation is retained here...\n    # could be improved in the future\n    elif coordinates_type == 'geographic':\n        if X.shape[1] != 2:\n            raise ValueError('Geographic coordinate type only '\n                             'supported for 2D datasets.')\n        x1, x2 = np.meshgrid(X[:, 0], X[:, 0], sparse=True)\n        y1, y2 = np.meshgrid(X[:, 1], X[:, 1], sparse=True)\n        z1, z2 = np.meshgrid(y, y, sparse=True)\n        d = great_circle_distance(x1, y1, x2, y2)\n        g = 0.5 * (z1 - z2)**2.\n        indices = np.indices(d.shape)\n        d = d[(indices[0, :, :] > indices[1, :, :])]\n        g = g[(indices[0, :, :] > indices[1, :, :])]\n\n    else:\n        raise ValueError(\"Specified coordinate type '%s' \"\n                         \"is not supported.\" % coordinates_type)\n\n    # Equal-sized bins are now implemented. The upper limit on the bins\n    # is appended to the list (instead of calculated as part of the\n    # list comprehension) to avoid any numerical oddities\n    # (specifically, say, ending up as 0.99999999999999 instead of 1.0).\n    # Appending dmax + 0.001 ensures that the largest distance value\n    # is included in the semivariogram calculation.\n    dmax = np.amax(d)\n    dmin = np.amin(d)\n    dd = (dmax - dmin) / nlags\n    bins = [dmin + n * dd for n in range(nlags)]\n    dmax += 0.001\n    bins.append(dmax)\n\n    # This old binning method was experimental and doesn't seem\n    # to work too well. Bins were computed such that there are more\n    # at shorter lags. This effectively weights smaller distances more\n    # highly in determining the variogram. As Kitanidis points out,\n    # the variogram fit to the data at smaller lag distances is more\n    # important. However, the value at the largest lag probably ends up\n    # being biased too high for the larger values and thereby throws off\n    # automatic variogram calculation and confuses comparison of the\n    # semivariogram with the variogram model.\n    #\n    # dmax = np.amax(d)\n    # dmin = np.amin(d)\n    # dd = dmax - dmin\n    # bins = [dd*(0.5**n) + dmin for n in range(nlags, 1, -1)]\n    # bins.insert(0, dmin)\n    # bins.append(dmax)\n\n    lags = np.zeros(nlags)\n    semivariance = np.zeros(nlags)\n\n    for n in range(nlags):\n        # This 'if... else...' statement ensures that there are data\n        # in the bin so that numpy can actually find the mean. If we\n        # don't test this first, then Python kicks out an annoying warning\n        # message when there is an empty bin and we try to calculate the mean.\n        if d[(d >= bins[n]) & (d < bins[n + 1])].size > 0:\n            lags[n] = np.mean(d[(d >= bins[n]) & (d < bins[n + 1])])\n            semivariance[n] = np.mean(g[(d >= bins[n]) & (d < bins[n + 1])])\n        else:\n            lags[n] = np.nan\n            semivariance[n] = np.nan\n\n    lags = lags[~np.isnan(semivariance)]\n    semivariance = semivariance[~np.isnan(semivariance)]\n\n    # a few tests the make sure that, if the variogram_model_parameters\n    # are supplied, they have been supplied as expected...\n    # if variogram_model_parameters was not defined, then estimate the variogram\n    if variogram_model_parameters is not None:\n        if variogram_model == 'linear' and len(variogram_model_parameters) != 2:\n            raise ValueError(\"Exactly two parameters required \"\n                             \"for linear variogram model.\")\n        elif variogram_model in ['power', 'spherical', 'exponential',\n                                 'gaussian', 'hole-effect'] \\\n                and len(variogram_model_parameters) != 3:\n            raise ValueError(\"Exactly three parameters required for \"\n                             \"%s variogram model\" % variogram_model)\n    else:\n        if variogram_model == 'custom':\n            raise ValueError(\"Variogram parameters must be specified when \"\n                             \"implementing custom variogram model.\")\n        else:\n            variogram_model_parameters = \\\n                _calculate_variogram_model(lags, semivariance, variogram_model,\n                                           variogram_function, weight)\n\n    return lags, semivariance, variogram_model_parameters"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfunctioning used in variogram model estimation. Returns residuals between calculated variogram and actual data at the specified lags.", "response": "def _variogram_residuals(params, x, y, variogram_function, weight):\n    \"\"\"Function used in variogram model estimation. Returns residuals between\n    calculated variogram and actual data (lags/semivariance).\n    Called by _calculate_variogram_model.\n\n    Parameters\n    ----------\n    params: list or 1D array\n        parameters for calculating the model variogram\n    x: ndarray\n        lags (distances) at which to evaluate the model variogram\n    y: ndarray\n        experimental semivariances at the specified lags\n    variogram_function: callable\n        the actual funtion that evaluates the model variogram\n    weight: bool\n        flag for implementing the crude weighting routine, used in order to\n        fit smaller lags better\n\n    Returns\n    -------\n    resid: 1d array\n        residuals, dimension same as y\n    \"\"\"\n\n    # this crude weighting routine can be used to better fit the model\n    # variogram to the experimental variogram at smaller lags...\n    # the weights are calculated from a logistic function, so weights at small\n    # lags are ~1 and weights at the longest lags are ~0;\n    # the center of the logistic weighting is hard-coded to be at 70% of the\n    # distance from the shortest lag to the largest lag\n    if weight:\n        drange = np.amax(x) - np.amin(x)\n        k = 2.1972 / (0.1 * drange)\n        x0 = 0.7 * drange + np.amin(x)\n        weights = 1. / (1. + np.exp(-k * (x0 - x)))\n        weights /= np.sum(weights)\n        resid = (variogram_function(params, x) - y) * weights\n    else:\n        resid = variogram_function(params, x) - y\n\n    return resid"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfunctioning that fits a variogram model and returns the actual calculated variogram model parameters.", "response": "def _calculate_variogram_model(lags, semivariance, variogram_model,\n                               variogram_function, weight):\n    \"\"\"Function that fits a variogram model when parameters are not specified.\n    Returns variogram model parameters that minimize the RMSE between the\n    specified variogram function and the actual calculated variogram points.\n\n    Parameters\n    ----------\n    lags: 1d array\n        binned lags/distances to use for variogram model parameter estimation\n    semivariance: 1d array\n        binned/averaged experimental semivariances to use for variogram model\n        parameter estimation\n    variogram_model: str/unicode\n        specified variogram model to use for parameter estimation\n    variogram_function: callable\n        the actual funtion that evaluates the model variogram\n    weight: bool\n        flag for implementing the crude weighting routine, used in order to fit\n        smaller lags better this is passed on to the residual calculation\n        cfunction, where weighting is actually applied...\n\n    Returns\n    -------\n    res: list\n        list of estimated variogram model parameters\n\n    NOTE that the estimation routine works in terms of the partial sill\n    (psill = sill - nugget) -- setting bounds such that psill > 0 ensures that\n    the sill will always be greater than the nugget...\n    \"\"\"\n\n    if variogram_model == 'linear':\n        x0 = [(np.amax(semivariance) - np.amin(semivariance)) /\n              (np.amax(lags) - np.amin(lags)), np.amin(semivariance)]\n        bnds = ([0., 0.], [np.inf, np.amax(semivariance)])\n    elif variogram_model == 'power':\n        x0 = [(np.amax(semivariance) - np.amin(semivariance)) /\n              (np.amax(lags) - np.amin(lags)), 1.1, np.amin(semivariance)]\n        bnds = ([0., 0.001, 0.], [np.inf, 1.999, np.amax(semivariance)])\n    else:\n        x0 = [np.amax(semivariance) - np.amin(semivariance),\n              0.25*np.amax(lags), np.amin(semivariance)]\n        bnds = ([0., 0., 0.], [10.*np.amax(semivariance), np.amax(lags),\n                               np.amax(semivariance)])\n\n    # use 'soft' L1-norm minimization in order to buffer against\n    # potential outliers (weird/skewed points)\n    res = least_squares(_variogram_residuals, x0, bounds=bnds, loss='soft_l1',\n                        args=(lags, semivariance, variogram_function, weight))\n\n    return res.x"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting up and solves the ordinary kriging system for the given coordinate pair.", "response": "def _krige(X, y, coords, variogram_function,\n           variogram_model_parameters, coordinates_type):\n    \"\"\"Sets up and solves the ordinary kriging system for the given\n    coordinate pair. This function is only used for the statistics calculations.\n\n    Parameters\n    ----------\n    X: ndarray\n        float array [n_samples, n_dim], the input array of coordinates\n    y: ndarray\n        float array [n_samples], the input array of measurement values\n    coords: ndarray\n        float array [1, n_dim], point at which to evaluate the kriging system\n    variogram_function: callable\n        function that will be called to evaluate variogram model\n    variogram_model_parameters: list\n        user-specified parameters for variogram model\n    coordinates_type: str\n        type of coordinates in X array, can be 'euclidean' for standard\n        rectangular coordinates or 'geographic' if the coordinates are lat/lon\n\n    Returns\n    -------\n    zinterp: float\n        kriging estimate at the specified point\n    sigmasq: float\n        mean square error of the kriging estimate\n    \"\"\"\n\n    zero_index = None\n    zero_value = False\n\n    # calculate distance between points... need a square distance matrix\n    # of inter-measurement-point distances and a vector of distances between\n    # measurement points (X) and the kriging point (coords)\n    if coordinates_type == 'euclidean':\n        d = squareform(pdist(X, metric='euclidean'))\n        bd = np.squeeze(cdist(X, coords[None, :], metric='euclidean'))\n\n    # geographic coordinate distances still calculated in the old way...\n    # assume X[:, 0] ('x') => lon, X[:, 1] ('y') => lat\n    # also assume problem is 2D; check done earlier in initializing variogram\n    elif coordinates_type == 'geographic':\n        x1, x2 = np.meshgrid(X[:, 0], X[:, 0], sparse=True)\n        y1, y2 = np.meshgrid(X[:, 1], X[:, 1], sparse=True)\n        d = great_circle_distance(x1, y1, x2, y2)\n        bd = great_circle_distance(X[:, 0], X[:, 1],\n                                   coords[0] * np.ones(X.shape[0]),\n                                   coords[1] * np.ones(X.shape[0]))\n\n    # this check is done when initializing variogram, but kept here anyways...\n    else:\n        raise ValueError(\"Specified coordinate type '%s' \"\n                         \"is not supported.\" % coordinates_type)\n\n    # check if kriging point overlaps with measurement point\n    if np.any(np.absolute(bd) <= 1e-10):\n        zero_value = True\n        zero_index = np.where(bd <= 1e-10)[0][0]\n\n    # set up kriging matrix\n    n = X.shape[0]\n    a = np.zeros((n+1, n+1))\n    a[:n, :n] = - variogram_function(variogram_model_parameters, d)\n    np.fill_diagonal(a, 0.0)\n    a[n, :] = 1.0\n    a[:, n] = 1.0\n    a[n, n] = 0.0\n\n    # set up RHS\n    b = np.zeros((n+1, 1))\n    b[:n, 0] = - variogram_function(variogram_model_parameters, bd)\n    if zero_value:\n        b[zero_index, 0] = 0.0\n    b[n, 0] = 1.0\n\n    # solve\n    res = np.linalg.solve(a, b)\n    zinterp = np.sum(res[:n, 0] * y)\n    sigmasq = np.sum(res[:, 0] * -b[:, 0])\n\n    return zinterp, sigmasq"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _find_statistics(X, y, variogram_function,\n                     variogram_model_parameters, coordinates_type):\n    \"\"\"Calculates variogram fit statistics.\n    Returns the delta, sigma, and epsilon values for the variogram fit.\n    These arrays are used for statistics calculations.\n\n    Parameters\n    ----------\n    X: ndarray\n        float array [n_samples, n_dim], the input array of coordinates\n    y: ndarray\n        float array [n_samples], the input array of measurement values\n    variogram_function: callable\n        function that will be called to evaluate variogram model\n    variogram_model_parameters: list\n        user-specified parameters for variogram model\n    coordinates_type: str\n        type of coordinates in X array, can be 'euclidean' for standard\n        rectangular coordinates or 'geographic' if the coordinates are lat/lon\n\n    Returns\n    -------\n    delta: ndarray\n        residuals between observed values and kriged estimates for those values\n    sigma: ndarray\n        mean error in kriging estimates\n    epsilon: ndarray\n        residuals normalized by their mean error\n    \"\"\"\n\n    delta = np.zeros(y.shape)\n    sigma = np.zeros(y.shape)\n\n    for i in range(y.shape[0]):\n\n        # skip the first value in the kriging problem\n        if i == 0:\n            continue\n\n        else:\n            k, ss = _krige(X[:i, :], y[:i], X[i, :], variogram_function,\n                           variogram_model_parameters, coordinates_type)\n\n            # if the estimation error is zero, it's probably because\n            # the evaluation point X[i, :] is really close to one of the\n            # kriging system points in X[:i, :]...\n            # in the case of zero estimation error, the results are not stored\n            if np.absolute(ss) < eps:\n                continue\n\n            delta[i] = y[i] - k\n            sigma[i] = np.sqrt(ss)\n\n    # only use non-zero entries in these arrays... sigma is used to pull out\n    # non-zero entries in both cases because it is guaranteed to be positive,\n    # whereas delta can be either positive or negative\n    delta = delta[sigma > eps]\n    sigma = sigma[sigma > eps]\n    epsilon = delta/sigma\n\n    return delta, sigma, epsilon", "response": "Calculates the statistics of the current variogram fit."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncalculate the cR statistic for the variogram fit", "response": "def calc_cR(Q2, sigma):\n    \"\"\"Returns the cR statistic for the variogram fit (see [1]).\"\"\"\n    return Q2 * np.exp(np.sum(np.log(sigma**2))/sigma.shape[0])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate the variogram model and the variogram parameters for the kriging system.", "response": "def update_variogram_model(self, variogram_model, variogram_parameters=None,\n                               variogram_function=None, nlags=6, weight=False,\n                               anisotropy_scaling_y=1., anisotropy_scaling_z=1.,\n                               anisotropy_angle_x=0., anisotropy_angle_y=0.,\n                               anisotropy_angle_z=0.):\n        \"\"\"Changes the variogram model and variogram parameters\n        for the kriging system.\n\n        Parameters\n        ----------\n        variogram_model : str\n            May be any of the variogram models listed above.\n            May also be 'custom', in which case variogram_parameters and\n            variogram_function must be specified.\n        variogram_parameters : list or dict, optional\n            List or dict of variogram model parameters, as explained above.\n            If not provided, a best fit model will be calculated as\n            described above.\n        variogram_function : callable, optional\n            A callable function that must be provided if variogram_model is\n            specified as 'custom'. See above for more information.\n        nlags : int, optional)\n            Number of averaging bins for the semivariogram. Default is 6.\n        weight : boolean, optional\n            Flag that specifies if semivariance at smaller lags should be\n            weighted more heavily when automatically calculating variogram\n            model. See above for more information. True indicates that\n            weights will be applied. Default is False.\n        anisotropy_scaling_y : float, optional\n            Scalar stretching value to take into account anisotropy\n            in y-direction. Default is 1 (effectively no stretching).\n            See above for more information.\n        anisotropy_scaling_z : float, optional\n            Scalar stretching value to take into account anisotropy\n            in z-direction. Default is 1 (effectively no stretching).\n            See above for more information.\n        anisotropy_angle_x : float, optional\n            Angle (in degrees) by which to rotate coordinate system about\n            the x axis in order to take into account anisotropy.\n            Default is 0 (no rotation). See above for more information.\n        anisotropy_angle_y : float, optional\n            Angle (in degrees) by which to rotate coordinate system about\n            the y axis in order to take into account anisotropy.\n            Default is 0 (no rotation). See above for more information.\n        anisotropy_angle_z : float, optional\n            Angle (in degrees) by which to rotate coordinate system about\n            the z axis in order to take into account anisotropy.\n            Default is 0 (no rotation).\n            See above for more information.\n        \"\"\"\n\n        if anisotropy_scaling_y != self.anisotropy_scaling_y or \\\n           anisotropy_scaling_z != self.anisotropy_scaling_z or \\\n           anisotropy_angle_x != self.anisotropy_angle_x or \\\n           anisotropy_angle_y != self.anisotropy_angle_y or \\\n           anisotropy_angle_z != self.anisotropy_angle_z:\n            if self.verbose:\n                print(\"Adjusting data for anisotropy...\")\n            self.anisotropy_scaling_y = anisotropy_scaling_y\n            self.anisotropy_scaling_z = anisotropy_scaling_z\n            self.anisotropy_angle_x = anisotropy_angle_x\n            self.anisotropy_angle_y = anisotropy_angle_y\n            self.anisotropy_angle_z = anisotropy_angle_z\n            self.X_ADJUSTED, self.Y_ADJUSTED, self.Z_ADJUSTED = \\\n                _adjust_for_anisotropy(np.vstack((self.X_ORIG, self.Y_ORIG, self.Z_ORIG)).T,\n                                       [self.XCENTER, self.YCENTER, self.ZCENTER],\n                                       [self.anisotropy_scaling_y, self.anisotropy_scaling_z],\n                                       [self.anisotropy_angle_x, self.anisotropy_angle_y, self.anisotropy_angle_z]).T\n\n        self.variogram_model = variogram_model\n        if self.variogram_model not in self.variogram_dict.keys() and self.variogram_model != 'custom':\n            raise ValueError(\"Specified variogram model '%s' \"\n                             \"is not supported.\" % variogram_model)\n        elif self.variogram_model == 'custom':\n            if variogram_function is None or not callable(variogram_function):\n                raise ValueError(\"Must specify callable function for \"\n                                 \"custom variogram model.\")\n            else:\n                self.variogram_function = variogram_function\n        else:\n            self.variogram_function = self.variogram_dict[self.variogram_model]\n        if self.verbose:\n            print(\"Updating variogram mode...\")\n\n        vp_temp = _make_variogram_parameter_list(self.variogram_model,\n                                                 variogram_parameters)\n        self.lags, self.semivariance, self.variogram_model_parameters = \\\n            _initialize_variogram_model(np.vstack((self.X_ADJUSTED,\n                                                   self.Y_ADJUSTED,\n                                                   self.Z_ADJUSTED)).T,\n                                        self.VALUES, self.variogram_model,\n                                        vp_temp, self.variogram_function,\n                                        nlags, weight, 'euclidean')\n\n        if self.verbose:\n            if self.variogram_model == 'linear':\n                print(\"Using '%s' Variogram Model\" % 'linear')\n                print(\"Slope:\", self.variogram_model_parameters[0])\n                print(\"Nugget:\", self.variogram_model_parameters[1], '\\n')\n            elif self.variogram_model == 'power':\n                print(\"Using '%s' Variogram Model\" % 'power')\n                print(\"Scale:\", self.variogram_model_parameters[0])\n                print(\"Exponent:\", self.variogram_model_parameters[1])\n                print(\"Nugget:\", self.variogram_model_parameters[2], '\\n')\n            elif self.variogram_model == 'custom':\n                print(\"Using Custom Variogram Model\")\n            else:\n                print(\"Using '%s' Variogram Model\" % self.variogram_model)\n                print(\"Partial Sill:\", self.variogram_model_parameters[0])\n                print(\"Full Sill:\", self.variogram_model_parameters[0] +\n                      self.variogram_model_parameters[2])\n                print(\"Range:\", self.variogram_model_parameters[1])\n                print(\"Nugget:\", self.variogram_model_parameters[2], '\\n')\n        if self.enable_plotting:\n            self.display_variogram_model()\n\n        if self.verbose:\n            print(\"Calculating statistics on variogram model fit...\")\n        self.delta, self.sigma, self.epsilon = \\\n            _find_statistics(np.vstack((self.X_ADJUSTED,\n                                        self.Y_ADJUSTED,\n                                        self.Z_ADJUSTED)).T,\n                             self.VALUES, self.variogram_function,\n                             self.variogram_model_parameters, 'euclidean')\n        self.Q1 = core.calcQ1(self.epsilon)\n        self.Q2 = core.calcQ2(self.epsilon)\n        self.cR = core.calc_cR(self.Q2, self.sigma)\n        if self.verbose:\n            print(\"Q1 =\", self.Q1)\n            print(\"Q2 =\", self.Q2)\n            print(\"cR =\", self.cR, '\\n')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_kriging_matrix(self, n, n_withdrifts):\n\n        xyz = np.concatenate((self.X_ADJUSTED[:, np.newaxis], self.Y_ADJUSTED[:, np.newaxis],\n                              self.Z_ADJUSTED[:, np.newaxis]), axis=1)\n        d = cdist(xyz, xyz, 'euclidean')\n        if self.UNBIAS:\n            a = np.zeros((n_withdrifts+1, n_withdrifts+1))\n        else:\n            a = np.zeros((n_withdrifts, n_withdrifts))\n        a[:n, :n] = - self.variogram_function(self.variogram_model_parameters, d)\n        np.fill_diagonal(a, 0.)\n\n        i = n\n        if self.regional_linear_drift:\n            a[:n, i] = self.X_ADJUSTED\n            a[i, :n] = self.X_ADJUSTED\n            i += 1\n            a[:n, i] = self.Y_ADJUSTED\n            a[i, :n] = self.Y_ADJUSTED\n            i += 1\n            a[:n, i] = self.Z_ADJUSTED\n            a[i, :n] = self.Z_ADJUSTED\n            i += 1\n        if self.specified_drift:\n            for arr in self.specified_drift_data_arrays:\n                a[:n, i] = arr\n                a[i, :n] = arr\n                i += 1\n        if self.functional_drift:\n            for func in self.functional_drift_terms:\n                a[:n, i] = func(self.X_ADJUSTED, self.Y_ADJUSTED, self.Z_ADJUSTED)\n                a[i, :n] = func(self.X_ADJUSTED, self.Y_ADJUSTED, self.Z_ADJUSTED)\n                i += 1\n        if i != n_withdrifts:\n            warnings.warn(\"Error in creating kriging matrix. Kriging may fail.\", RuntimeWarning)\n        if self.UNBIAS:\n            a[n_withdrifts, :n] = 1.0\n            a[:n, n_withdrifts] = 1.0\n            a[n:n_withdrifts + 1, n:n_withdrifts + 1] = 0.0\n\n        return a", "response": "Assembles the kriging matrix."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsolve the kriging system as a vectorized operation.", "response": "def _exec_vector(self, a, bd, xyz, mask, n_withdrifts, spec_drift_grids):\n        \"\"\"Solves the kriging system as a vectorized operation. This method\n        can take a lot of memory for large grids and/or large datasets.\"\"\"\n\n        npt = bd.shape[0]\n        n = self.X_ADJUSTED.shape[0]\n        zero_index = None\n        zero_value = False\n\n        a_inv = scipy.linalg.inv(a)\n\n        if np.any(np.absolute(bd) <= self.eps):\n            zero_value = True\n            zero_index = np.where(np.absolute(bd) <= self.eps)\n\n        if self.UNBIAS:\n            b = np.zeros((npt, n_withdrifts+1, 1))\n        else:\n            b = np.zeros((npt, n_withdrifts, 1))\n        b[:, :n, 0] = - self.variogram_function(self.variogram_model_parameters, bd)\n        if zero_value:\n            b[zero_index[0], zero_index[1], 0] = 0.0\n\n        i = n\n        if self.regional_linear_drift:\n            b[:, i, 0] = xyz[:, 2]\n            i += 1\n            b[:, i, 0] = xyz[:, 1]\n            i += 1\n            b[:, i, 0] = xyz[:, 0]\n            i += 1\n        if self.specified_drift:\n            for spec_vals in spec_drift_grids:\n                b[:, i, 0] = spec_vals.flatten()\n                i += 1\n        if self.functional_drift:\n            for func in self.functional_drift_terms:\n                b[:, i, 0] = func(xyz[:, 2], xyz[:, 1], xyz[:, 0])\n                i += 1\n        if i != n_withdrifts:\n            warnings.warn(\"Error in setting up kriging system. \"\n                          \"Kriging may fail.\", RuntimeWarning)\n        if self.UNBIAS:\n            b[:, n_withdrifts, 0] = 1.0\n\n        if (~mask).any():\n            mask_b = np.repeat(mask[:, np.newaxis, np.newaxis],\n                               n_withdrifts+1, axis=1)\n            b = np.ma.array(b, mask=mask_b)\n\n        if self.UNBIAS:\n            x = np.dot(a_inv, b.reshape((npt, n_withdrifts+1)).T).reshape((1, n_withdrifts+1, npt)).T\n        else:\n            x = np.dot(a_inv, b.reshape((npt, n_withdrifts)).T).reshape((1, n_withdrifts, npt)).T\n        kvalues = np.sum(x[:, :n, 0] * self.VALUES, axis=1)\n        sigmasq = np.sum(x[:, :, 0] * -b[:, :, 0], axis=1)\n\n        return kvalues, sigmasq"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _exec_loop(self, a, bd_all, xyz, mask, n_withdrifts, spec_drift_grids):\n\n        npt = bd_all.shape[0]\n        n = self.X_ADJUSTED.shape[0]\n        kvalues = np.zeros(npt)\n        sigmasq = np.zeros(npt)\n\n        a_inv = scipy.linalg.inv(a)\n\n        for j in np.nonzero(~mask)[0]:   # Note that this is the same thing as range(npt) if mask is not defined,\n            bd = bd_all[j]               # otherwise it takes the non-masked elements.\n            if np.any(np.absolute(bd) <= self.eps):\n                zero_value = True\n                zero_index = np.where(np.absolute(bd) <= self.eps)\n            else:\n                zero_value = False\n                zero_index = None\n\n            if self.UNBIAS:\n                b = np.zeros((n_withdrifts+1, 1))\n            else:\n                b = np.zeros((n_withdrifts, 1))\n            b[:n, 0] = - self.variogram_function(self.variogram_model_parameters, bd)\n            if zero_value:\n                b[zero_index[0], 0] = 0.0\n\n            i = n\n            if self.regional_linear_drift:\n                b[i, 0] = xyz[j, 2]\n                i += 1\n                b[i, 0] = xyz[j, 1]\n                i += 1\n                b[i, 0] = xyz[j, 0]\n                i += 1\n            if self.specified_drift:\n                for spec_vals in spec_drift_grids:\n                    b[i, 0] = spec_vals.flatten()[i]\n                    i += 1\n            if self.functional_drift:\n                for func in self.functional_drift_terms:\n                    b[i, 0] = func(xyz[j, 2], xyz[j, 1], xyz[j, 0])\n                    i += 1\n            if i != n_withdrifts:\n                warnings.warn(\"Error in setting up kriging system. \"\n                              \"Kriging may fail.\", RuntimeWarning)\n            if self.UNBIAS:\n                b[n_withdrifts, 0] = 1.0\n\n            x = np.dot(a_inv, b)\n            kvalues[j] = np.sum(x[:n, 0] * self.VALUES)\n            sigmasq[j] = np.sum(x[:, 0] * -b[:, 0])\n\n        return kvalues, sigmasq", "response": "Solves the kriging system by looping over all specified points."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef execute(self, style, xpoints, ypoints, zpoints, mask=None,\n                backend='vectorized', specified_drift_arrays=None):\n        \"\"\"Calculates a kriged grid and the associated variance.\n\n        This is now the method that performs the main kriging calculation.\n        Note that currently measurements (i.e., z values) are\n        considered 'exact'. This means that, when a specified coordinate for\n        interpolation is exactly the same as one of the data points,\n        the variogram evaluated at the point is forced to be zero. Also, the\n        diagonal of the kriging matrix is also always forced to be zero.\n        In forcing the variogram evaluated at data points to be zero, we are\n        effectively saying that there is no variance at that point\n        (no uncertainty, so the value is 'exact').\n\n        In the future, the code may include an extra 'exact_values' boolean\n        flag that can be adjusted to specify whether to treat the measurements\n        as 'exact'. Setting the flag to false would indicate that the variogram\n        should not be forced to be zero at zero distance (i.e., when evaluated\n        at data points). Instead, the uncertainty in the point will be equal\n        to the nugget. This would mean that the diagonal of the kriging matrix\n        would be set to the nugget instead of to zero.\n\n        Parameters\n        ----------\n        style : str\n            Specifies how to treat input kriging points. Specifying 'grid'\n            treats xpoints, ypoints, and zpoints as arrays of x, y, and z\n            coordinates that define a rectangular grid. Specifying 'points'\n            treats xpoints, ypoints, and zpoints as arrays that provide\n            coordinates at which to solve the kriging system. Specifying\n            'masked' treats xpoints, ypoints, and zpoints as arrays of x, y,\n            and z coordinates that define a rectangular grid and uses mask\n            to only evaluate specific points in the grid.\n        xpoints : array_like, shape (N,) or (N, 1)\n            If style is specific as 'grid' or 'masked', x-coordinates of\n            LxMxN grid. If style is specified as 'points', x-coordinates of\n            specific points at which to solve kriging system.\n        ypoints : array_like, shape (M,) or (M, 1)\n            If style is specified as 'grid' or 'masked', y-coordinates of\n            LxMxN grid. If style is specified as 'points', y-coordinates of\n            specific points at which to solve kriging system. Note that in this\n            case, xpoints, ypoints, and zpoints must have the same dimensions\n            (i.e., L = M = N).\n        zpoints : array_like, shape (L,) or (L, 1)\n            If style is specified as 'grid' or 'masked', z-coordinates of\n            LxMxN grid. If style is specified as 'points', z-coordinates of\n            specific points at which to solve kriging system. Note that in this\n            case, xpoints, ypoints, and zpoints must have the same dimensions\n            (i.e., L = M = N).\n        mask : boolean array, shape (L, M, N), optional\n            Specifies the points in the rectangular grid defined by xpoints,\n            ypoints, zpoints that are to be excluded in the kriging\n            calculations. Must be provided if style is specified as 'masked'.\n            False indicates that the point should not be masked, so the kriging\n            system will be solved at the point.\n            True indicates that the point should be masked, so the kriging\n            system will not be solved at the point.\n        backend : string, optional\n            Specifies which approach to use in kriging. Specifying 'vectorized'\n            will solve the entire kriging problem at once in a vectorized\n            operation. This approach is faster but also can consume a\n            significant amount of memory for large grids and/or large datasets.\n            Specifying 'loop' will loop through each point at which the kriging\n            system is to be solved. This approach is slower but also less\n            memory-intensive. Default is 'vectorized'.\n        specified_drift_arrays : list of array-like objects, optional\n            Specifies the drift values at the points at which the kriging\n            system is to be evaluated. Required if 'specified' drift provided\n            in the list of drift terms when instantiating the UniversalKriging3D\n            class. Must be a list of arrays in the same order as the list\n            provided when instantiating the kriging object. Array(s) must be\n            the same dimension as the specified grid or have the same number\n            of points as the specified points; i.e., the arrays either must be\n            shape (L, M, N), where L is the number of z grid-points,\n            M is the number of y grid-points, and N is the number of\n            x grid-points, or shape (N,) or (N, 1), where N is the number of\n            points at which to evaluate the kriging system.\n\n        Returns\n        -------\n        kvalues : ndarray, shape (L, M, N) or (N,) or (N, 1)\n            Interpolated values of specified grid or at the specified set\n            of points. If style was specified as 'masked', kvalues will be a\n            numpy masked array.\n        sigmasq : ndarray, shape (L, M, N) or (N,) or (N, 1)\n            Variance at specified grid points or at the specified set of points.\n            If style was specified as 'masked', sigmasq will be a numpy\n            masked array.\n        \"\"\"\n\n        if self.verbose:\n            print(\"Executing Ordinary Kriging...\\n\")\n\n        if style != 'grid' and style != 'masked' and style != 'points':\n            raise ValueError(\"style argument must be 'grid', 'points', \"\n                             \"or 'masked'\")\n\n        xpts = np.atleast_1d(np.squeeze(np.array(xpoints, copy=True)))\n        ypts = np.atleast_1d(np.squeeze(np.array(ypoints, copy=True)))\n        zpts = np.atleast_1d(np.squeeze(np.array(zpoints, copy=True)))\n        n = self.X_ADJUSTED.shape[0]\n        n_withdrifts = n\n        if self.regional_linear_drift:\n            n_withdrifts += 3\n        if self.specified_drift:\n            n_withdrifts += len(self.specified_drift_data_arrays)\n        if self.functional_drift:\n            n_withdrifts += len(self.functional_drift_terms)\n        nx = xpts.size\n        ny = ypts.size\n        nz = zpts.size\n        a = self._get_kriging_matrix(n, n_withdrifts)\n\n        if style in ['grid', 'masked']:\n            if style == 'masked':\n                if mask is None:\n                    raise IOError(\"Must specify boolean masking array \"\n                                  \"when style is 'masked'.\")\n                if mask.ndim != 3:\n                    raise ValueError(\"Mask is not three-dimensional.\")\n                if mask.shape[0] != nz or mask.shape[1] != ny or mask.shape[2] != nx:\n                    if mask.shape[0] == nx and mask.shape[2] == nz and mask.shape[1] == ny:\n                        mask = mask.swapaxes(0, 2)\n                    else:\n                        raise ValueError(\"Mask dimensions do not match \"\n                                         \"specified grid dimensions.\")\n                mask = mask.flatten()\n            npt = nz * ny * nx\n            grid_z, grid_y, grid_x = np.meshgrid(zpts, ypts, xpts, indexing='ij')\n            xpts = grid_x.flatten()\n            ypts = grid_y.flatten()\n            zpts = grid_z.flatten()\n        elif style == 'points':\n            if xpts.size != ypts.size and ypts.size != zpts.size:\n                raise ValueError(\"xpoints and ypoints must have same \"\n                                 \"dimensions when treated as listing \"\n                                 \"discrete points.\")\n            npt = nx\n        else:\n            raise ValueError(\"style argument must be 'grid', 'points', \"\n                             \"or 'masked'\")\n\n        if specified_drift_arrays is None:\n            specified_drift_arrays = []\n        spec_drift_grids = []\n        if self.specified_drift:\n            if len(specified_drift_arrays) == 0:\n                raise ValueError(\"Must provide drift values for kriging \"\n                                 \"points when using 'specified' drift \"\n                                 \"capability.\")\n            if type(specified_drift_arrays) is not list:\n                raise TypeError(\"Arrays for specified drift terms must \"\n                                \"be encapsulated in a list.\")\n            for spec in specified_drift_arrays:\n                if style in ['grid', 'masked']:\n                    if spec.ndim < 3:\n                        raise ValueError(\"Dimensions of drift values array do \"\n                                         \"not match specified grid dimensions.\")\n                    elif spec.shape[0] != nz or spec.shape[1] != ny or spec.shape[2] != nx:\n                        if spec.shape[0] == nx and spec.shape[2] == nz and spec.shape[1] == ny:\n                            spec_drift_grids.append(np.squeeze(spec.swapaxes(0, 2)))\n                        else:\n                            raise ValueError(\"Dimensions of drift values array \"\n                                             \"do not match specified grid \"\n                                             \"dimensions.\")\n                    else:\n                        spec_drift_grids.append(np.squeeze(spec))\n                elif style == 'points':\n                    if spec.ndim != 1:\n                        raise ValueError(\"Dimensions of drift values array do \"\n                                         \"not match specified grid dimensions.\")\n                    elif spec.shape[0] != xpts.size:\n                        raise ValueError(\"Number of supplied drift values in \"\n                                         \"array do not match specified number \"\n                                         \"of kriging points.\")\n                    else:\n                        spec_drift_grids.append(np.squeeze(spec))\n            if len(spec_drift_grids) != len(self.specified_drift_data_arrays):\n                raise ValueError(\"Inconsistent number of specified \"\n                                 \"drift terms supplied.\")\n        else:\n            if len(specified_drift_arrays) != 0:\n                warnings.warn(\"Provided specified drift values, but \"\n                              \"'specified' drift was not initialized during \"\n                              \"instantiation of UniversalKriging3D class.\",\n                              RuntimeWarning)\n\n        xpts, ypts, zpts = _adjust_for_anisotropy(np.vstack((xpts, ypts, zpts)).T,\n                                                  [self.XCENTER, self.YCENTER, self.ZCENTER],\n                                                  [self.anisotropy_scaling_y, self.anisotropy_scaling_z],\n                                                  [self.anisotropy_angle_x, self.anisotropy_angle_y,\n                                                  self.anisotropy_angle_z]).T\n\n        if style != 'masked':\n            mask = np.zeros(npt, dtype='bool')\n\n        xyz_points = np.concatenate((zpts[:, np.newaxis], ypts[:, np.newaxis],\n                                     xpts[:, np.newaxis]), axis=1)\n        xyz_data = np.concatenate((self.Z_ADJUSTED[:, np.newaxis],\n                                   self.Y_ADJUSTED[:, np.newaxis],\n                                   self.X_ADJUSTED[:, np.newaxis]), axis=1)\n        bd = cdist(xyz_points, xyz_data, 'euclidean')\n\n        if backend == 'vectorized':\n            kvalues, sigmasq = self._exec_vector(a, bd, xyz_points, mask,\n                                                 n_withdrifts, spec_drift_grids)\n        elif backend == 'loop':\n            kvalues, sigmasq = self._exec_loop(a, bd, xyz_points, mask,\n                                               n_withdrifts, spec_drift_grids)\n        else:\n            raise ValueError('Specified backend {} is not supported for '\n                             '3D ordinary kriging.'.format(backend))\n\n        if style == 'masked':\n            kvalues = np.ma.array(kvalues, mask=mask)\n            sigmasq = np.ma.array(sigmasq, mask=mask)\n\n        if style in ['masked', 'grid']:\n            kvalues = kvalues.reshape((nz, ny, nx))\n            sigmasq = sigmasq.reshape((nz, ny, nx))\n\n        return kvalues, sigmasq", "response": "This method is called by the kernel to calculate the kriged grid and the associated variance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfitting the Krige model to the given data.", "response": "def fit(self, x, y, *args, **kwargs):\n        \"\"\"\n        Parameters\n        ----------\n        x: ndarray\n            array of Points, (x, y) pairs of shape (N, 2) for 2d kriging\n            array of Points, (x, y, z) pairs of shape (N, 3) for 3d kriging\n        y: ndarray\n            array of targets (N, )\n        \"\"\"\n\n        points = self._dimensionality_check(x)\n\n        # if condition required to address backward compatibility\n        if self.method in threed_krige:\n            self.model = krige_methods[self.method](\n                val=y,\n                variogram_model=self.variogram_model,\n                nlags=self.nlags,\n                weight=self.weight,\n                verbose=self.verbose,\n                **points\n            )\n        else:\n            self.model = krige_methods[self.method](\n                z=y,\n                variogram_model=self.variogram_model,\n                nlags=self.nlags,\n                weight=self.weight,\n                verbose=self.verbose,\n                **points\n            )"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\npredicts the Kraus - Katiging version of the Kraus - Katiging K at", "response": "def predict(self, x, *args, **kwargs):\n        \"\"\"\n        Parameters\n        ----------\n        x: ndarray\n            array of Points, (x, y) pairs of shape (N, 2) for 2d kriging\n            array of Points, (x, y, z) pairs of shape (N, 3) for 3d kriging\n\n        Returns\n        -------\n        Prediction array\n        \"\"\"\n        if not self.model:\n            raise Exception('Not trained. Train first')\n\n        points = self._dimensionality_check(x, ext='points')\n\n        return self.execute(points, *args, **kwargs)[0]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef execute(self, points, *args, **kwargs):\n        # TODO array of Points, (x, y) pairs of shape (N, 2)\n        \"\"\"\n        Parameters\n        ----------\n        points: dict\n\n        Returns:\n        -------\n        Prediction array\n        Variance array\n        \"\"\"\n        if isinstance(self.model, OrdinaryKriging) or \\\n                isinstance(self.model, OrdinaryKriging3D):\n            prediction, variance = \\\n                self.model.execute('points',\n                                   n_closest_points=self.n_closest_points,\n                                   backend='loop',\n                                   **points)\n        else:\n            print('n_closest_points will be ignored for UniversalKriging')\n            prediction, variance = \\\n                self.model.execute('points', backend='loop', **points)\n\n        return prediction, variance", "response": "Execute the evaluator on a dictionary of points."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfit the regression model and also Krige the residual", "response": "def fit(self, p, x, y):\n        \"\"\"\n        fit the regression method and also Krige the residual\n\n        Parameters\n        ----------\n        p: ndarray\n            (Ns, d) array of predictor variables (Ns samples, d dimensions)\n            for regression\n        x: ndarray\n            ndarray of (x, y) points. Needs to be a (Ns, 2) array\n            corresponding to the lon/lat, for example 2d regression kriging.\n            array of Points, (x, y, z) pairs of shape (N, 3) for 3d kriging\n        y: ndarray\n            array of targets (Ns, )\n        \"\"\"\n        self.regression_model.fit(p, y)\n        ml_pred = self.regression_model.predict(p)\n        print('Finished learning regression model')\n        # residual=y-ml_pred\n        self.krige.fit(x=x, y=y - ml_pred)\n        print('Finished kriging residuals')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef predict(self, p, x):\n\n        return self.krige_residual(x) + self.regression_model.predict(p)", "response": "Predicts the Krige - Residual model for the given set of points."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrite a gridded ASCII file.", "response": "def write_asc_grid(x, y, z, filename='output.asc', style=1):\r\n    \"\"\"Writes gridded data to ASCII grid file (*.asc). This is useful for\r\n    exporting data to a GIS program.\r\n\r\n    Parameters\r\n    ----------\r\n    x : array_like, shape (N,) or (N, 1)\r\n        X-coordinates of grid points at center of cells.\r\n    y : array_like, shape (M,) or (M, 1)\r\n        Y-coordinates of grid points at center of cells.\r\n    z : array_like, shape (M, N)\r\n        Gridded data values. May be a masked array.\r\n    filename : string, optional\r\n        Name of output *.asc file. Default name is 'output.asc'.\r\n    style : int, optional\r\n        Determines how to write the *.asc file header.\r\n        Specifying 1 writes out DX, DY, XLLCENTER, YLLCENTER.\r\n        Specifying 2 writes out CELLSIZE (note DX must be the same as DY),\r\n        XLLCORNER, YLLCORNER. Default is 1.\r\n    \"\"\"\r\n\r\n    if np.ma.is_masked(z):\r\n        z = np.array(z.tolist(-999.))\r\n\r\n    x = np.squeeze(np.array(x))\r\n    y = np.squeeze(np.array(y))\r\n    z = np.squeeze(np.array(z))\r\n    nrows = z.shape[0]\r\n    ncols = z.shape[1]\r\n\r\n    if z.ndim != 2:\r\n        raise ValueError(\"Two-dimensional grid is required to \"\r\n                         \"write *.asc grid.\")\r\n    if x.ndim > 1 or y.ndim > 1:\r\n        raise ValueError(\"Dimensions of X and/or Y coordinate arrays are not \"\r\n                         \"as expected. Could not write *.asc grid.\")\r\n    if z.shape != (y.size, x.size):\r\n        warnings.warn(\"Grid dimensions are not as expected. \"\r\n                      \"Incorrect *.asc file generation may result.\",\r\n                      RuntimeWarning)\r\n    if np.amin(x) != x[0] or np.amin(y) != y[0]:\r\n        warnings.warn(\"Order of X or Y coordinates is not as expected. \"\r\n                      \"Incorrect *.asc file generation may result.\",\r\n                      RuntimeWarning)\r\n\r\n    dx = abs(x[1] - x[0])\r\n    dy = abs(y[1] - y[0])\r\n    if abs((x[-1] - x[0])/(x.shape[0] - 1)) != dx or \\\r\n       abs((y[-1] - y[0])/(y.shape[0] - 1)) != dy:\r\n        raise ValueError(\"X or Y spacing is not constant; *.asc \"\r\n                         \"grid cannot be written.\")\r\n    cellsize = -1\r\n    if style == 2:\r\n        if dx != dy:\r\n            raise ValueError(\"X and Y spacing is not the same. \"\r\n                             \"Cannot write *.asc file in the specified format.\")\r\n        cellsize = dx\r\n\r\n    xllcenter = x[0]\r\n    yllcenter = y[0]\r\n\r\n    # Note that these values are flagged as -1. If there is a problem in trying\r\n    # to write out style 2, the -1 value will appear in the output file.\r\n    xllcorner = -1\r\n    yllcorner = -1\r\n    if style == 2:\r\n        xllcorner = xllcenter - dx/2.0\r\n        yllcorner = yllcenter - dy/2.0\r\n\r\n    no_data = -999.\r\n\r\n    with io.open(filename, 'w') as f:\r\n        if style == 1:\r\n            f.write(\"NCOLS          \" + '{:<10n}'.format(ncols) + '\\n')\r\n            f.write(\"NROWS          \" + '{:<10n}'.format(nrows) + '\\n')\r\n            f.write(\"XLLCENTER      \" + '{:<10.2f}'.format(xllcenter) + '\\n')\r\n            f.write(\"YLLCENTER      \" + '{:<10.2f}'.format(yllcenter) + '\\n')\r\n            f.write(\"DX             \" + '{:<10.2f}'.format(dx) + '\\n')\r\n            f.write(\"DY             \" + '{:<10.2f}'.format(dy) + '\\n')\r\n            f.write(\"NODATA_VALUE   \" + '{:<10.2f}'.format(no_data) + '\\n')\r\n        elif style == 2:\r\n            f.write(\"NCOLS          \" + '{:<10n}'.format(ncols) + '\\n')\r\n            f.write(\"NROWS          \" + '{:<10n}'.format(nrows) + '\\n')\r\n            f.write(\"XLLCORNER      \" + '{:<10.2f}'.format(xllcorner) + '\\n')\r\n            f.write(\"YLLCORNER      \" + '{:<10.2f}'.format(yllcorner) + '\\n')\r\n            f.write(\"CELLSIZE       \" + '{:<10.2f}'.format(cellsize) + '\\n')\r\n            f.write(\"NODATA_VALUE   \" + '{:<10.2f}'.format(no_data) + '\\n')\r\n        else:\r\n            raise ValueError(\"style kwarg must be either 1 or 2.\")\r\n\r\n        for m in range(z.shape[0] - 1, -1, -1):\r\n            for n in range(z.shape[1]):\r\n                f.write('{:<16.2f}'.format(z[m, n]))\r\n            if m != 0:\r\n                f.write('\\n')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef read_asc_grid(filename, footer=0):\r\n\r\n    ncols = None\r\n    nrows = None\r\n    xllcorner = None\r\n    xllcenter = None\r\n    yllcorner = None\r\n    yllcenter = None\r\n    cellsize = None\r\n    dx = None\r\n    dy = None\r\n    no_data = None\r\n    header_lines = 0\r\n    with io.open(filename, 'r') as f:\r\n        while True:\r\n            string, value = f.readline().split()\r\n            header_lines += 1\r\n            if string.lower() == 'ncols':\r\n                ncols = int(value)\r\n            elif string.lower() == 'nrows':\r\n                nrows = int(value)\r\n            elif string.lower() == 'xllcorner':\r\n                xllcorner = float(value)\r\n            elif string.lower() == 'xllcenter':\r\n                xllcenter = float(value)\r\n            elif string.lower() == 'yllcorner':\r\n                yllcorner = float(value)\r\n            elif string.lower() == 'yllcenter':\r\n                yllcenter = float(value)\r\n            elif string.lower() == 'cellsize':\r\n                cellsize = float(value)\r\n            elif string.lower() == 'cell_size':\r\n                cellsize = float(value)\r\n            elif string.lower() == 'dx':\r\n                dx = float(value)\r\n            elif string.lower() == 'dy':\r\n                dy = float(value)\r\n            elif string.lower() == 'nodata_value':\r\n                no_data = float(value)\r\n            elif string.lower() == 'nodatavalue':\r\n                no_data = float(value)\r\n            else:\r\n                raise IOError(\"could not read *.asc file. Error in header.\")\r\n\r\n            if (ncols is not None) and \\\r\n               (nrows is not None) and \\\r\n               (((xllcorner is not None) and (yllcorner is not None)) or\r\n                ((xllcenter is not None) and (yllcenter is not None))) and \\\r\n               ((cellsize is not None) or ((dx is not None) and (dy is not None))) and \\\r\n               (no_data is not None):\r\n                break\r\n\r\n    raw_grid_array = np.genfromtxt(filename, skip_header=header_lines,\r\n                                   skip_footer=footer)\r\n    grid_array = np.flipud(raw_grid_array)\r\n\r\n    if nrows != grid_array.shape[0] or ncols != grid_array.shape[1]:\r\n        raise IOError(\"Error reading *.asc file. Encountered problem \"\r\n                      \"with header: NCOLS and/or NROWS does not match \"\r\n                      \"number of columns/rows in data file body.\")\r\n\r\n    if xllcorner is not None and yllcorner is not None:\r\n        if dx is not None and dy is not None:\r\n            xllcenter = xllcorner + dx/2.0\r\n            yllcenter = yllcorner + dy/2.0\r\n        else:\r\n            xllcenter = xllcorner + cellsize/2.0\r\n            yllcenter = yllcorner + cellsize/2.0\r\n\r\n    if dx is not None and dy is not None:\r\n        x = np.arange(xllcenter, xllcenter + ncols*dx, dx)\r\n        y = np.arange(yllcenter, yllcenter + nrows*dy, dy)\r\n    else:\r\n        x = np.arange(xllcenter, xllcenter + ncols*cellsize, cellsize)\r\n        y = np.arange(yllcenter, yllcenter + nrows*cellsize, cellsize)\r\n\r\n    # Sometimes x and y and can be an entry too long due to imprecision\r\n    # in calculating the upper cutoff for np.arange(); this bit takes care of\r\n    # that potential problem.\r\n    if x.size == ncols + 1:\r\n        x = x[:-1]\r\n    if y.size == nrows + 1:\r\n        y = y[:-1]\r\n\r\n    if cellsize is None:\r\n        cellsize = (dx, dy)\r\n\r\n    return grid_array, x, y, cellsize, no_data", "response": "Reads an ASCII grid file and returns a list of tuples each containing the values of each entry in the array."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_kriging_matrix(self, n):\n\n        xyz = np.concatenate((self.X_ADJUSTED[:, np.newaxis],\n                              self.Y_ADJUSTED[:, np.newaxis],\n                              self.Z_ADJUSTED[:, np.newaxis]), axis=1)\n        d = cdist(xyz, xyz, 'euclidean')\n        a = np.zeros((n+1, n+1))\n        a[:n, :n] = - self.variogram_function(self.variogram_model_parameters, d)\n        np.fill_diagonal(a, 0.)\n        a[n, :] = 1.0\n        a[:, n] = 1.0\n        a[n, n] = 0.0\n\n        return a", "response": "Assembles the kriging matrix."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _exec_loop(self, a, bd_all, mask):\n\n        npt = bd_all.shape[0]\n        n = self.X_ADJUSTED.shape[0]\n        kvalues = np.zeros(npt)\n        sigmasq = np.zeros(npt)\n\n        a_inv = scipy.linalg.inv(a)\n\n        for j in np.nonzero(~mask)[0]:   # Note that this is the same thing as range(npt) if mask is not defined,\n            bd = bd_all[j]               # otherwise it takes the non-masked elements.\n            if np.any(np.absolute(bd) <= self.eps):\n                zero_value = True\n                zero_index = np.where(np.absolute(bd) <= self.eps)\n            else:\n                zero_value = False\n                zero_index = None\n\n            b = np.zeros((n+1, 1))\n            b[:n, 0] = - self.variogram_function(self.variogram_model_parameters, bd)\n            if zero_value:\n                b[zero_index[0], 0] = 0.0\n            b[n, 0] = 1.0\n\n            x = np.dot(a_inv, b)\n            kvalues[j] = np.sum(x[:n, 0] * self.VALUES)\n            sigmasq[j] = np.sum(x[:, 0] * -b[:, 0])\n\n        return kvalues, sigmasq", "response": "Solves the kriging system by looping over all specified points."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _exec_loop_moving_window(self, a_all, bd_all, mask, bd_idx):\n        import scipy.linalg.lapack\n\n        npt = bd_all.shape[0]\n        n = bd_idx.shape[1]\n        kvalues = np.zeros(npt)\n        sigmasq = np.zeros(npt)\n\n        for i in np.nonzero(~mask)[0]:\n            b_selector = bd_idx[i]\n            bd = bd_all[i]\n\n            a_selector = np.concatenate((b_selector, np.array([a_all.shape[0] - 1])))\n            a = a_all[a_selector[:, None], a_selector]\n\n            if np.any(np.absolute(bd) <= self.eps):\n                zero_value = True\n                zero_index = np.where(np.absolute(bd) <= self.eps)\n            else:\n                zero_value = False\n                zero_index = None\n            b = np.zeros((n+1, 1))\n            b[:n, 0] = - self.variogram_function(self.variogram_model_parameters, bd)\n            if zero_value:\n                b[zero_index[0], 0] = 0.0\n            b[n, 0] = 1.0\n\n            x = scipy.linalg.solve(a, b)\n\n            kvalues[i] = x[:n, 0].dot(self.VALUES[b_selector])\n            sigmasq[i] = - x[:, 0].dot(b[:, 0])\n\n        return kvalues, sigmasq", "response": "Execute the kriging system by looping over all specified points."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates a kriged grid and the associated variance. This is now the method that performs the main kriging calculation. Note that currently measurements (i.e., z values) are considered 'exact'. This means that, when a specified coordinate for interpolation is exactly the same as one of the data points, the variogram evaluated at the point is forced to be zero. Also, the diagonal of the kriging matrix is also always forced to be zero. In forcing the variogram evaluated at data points to be zero, we are effectively saying that there is no variance at that point (no uncertainty, so the value is 'exact'). In the future, the code may include an extra 'exact_values' boolean flag that can be adjusted to specify whether to treat the measurements as 'exact'. Setting the flag to false would indicate that the variogram should not be forced to be zero at zero distance (i.e., when evaluated at data points). Instead, the uncertainty in the point will be equal to the nugget. This would mean that the diagonal of the kriging matrix would be set to the nugget instead of to zero. Parameters ---------- style : str Specifies how to treat input kriging points. Specifying 'grid' treats xpoints, ypoints, and zpoints as arrays of x, y, and z coordinates that define a rectangular grid. Specifying 'points' treats xpoints, ypoints, and zpoints as arrays that provide coordinates at which to solve the kriging system. Specifying 'masked' treats xpoints, ypoints, and zpoints as arrays of x, y, and z coordinates that define a rectangular grid and uses mask to only evaluate specific points in the grid. xpoints : array_like, shape (N,) or (N, 1) If style is specific as 'grid' or 'masked', x-coordinates of LxMxN grid. If style is specified as 'points', x-coordinates of specific points at which to solve kriging system. ypoints : array-like, shape (M,) or (M, 1) If style is specified as 'grid' or 'masked', y-coordinates of LxMxN grid. If style is specified as 'points', y-coordinates of specific points at which to solve kriging system. Note that in this case, xpoints, ypoints, and zpoints must have the same dimensions (i.e., L = M = N). zpoints : array-like, shape (L,) or (L, 1) If style is specified as 'grid' or 'masked', z-coordinates of LxMxN grid. If style is specified as 'points', z-coordinates of specific points at which to solve kriging system. Note that in this case, xpoints, ypoints, and zpoints must have the same dimensions (i.e., L = M = N). mask : boolean array, shape (L, M, N), optional Specifies the points in the rectangular grid defined by xpoints, ypoints, zpoints that are to be excluded in the kriging calculations. Must be provided if style is specified as 'masked'. False indicates that the point should not be masked, so the kriging system will be solved at the point. True indicates that the point should be masked, so the kriging system should will not be solved at the point. backend : str, optional Specifies which approach to use in kriging. Specifying 'vectorized' will solve the entire kriging problem at once in a vectorized operation. This approach is faster but also can consume a significant amount of memory for large grids and/or large datasets. Specifying 'loop' will loop through each point at which the kriging system is to be solved. This approach is slower but also less memory-intensive. Default is 'vectorized'. n_closest_points : int, optional For kriging with a moving window, specifies the number of nearby points to use in the calculation. This can speed up the calculation for large datasets, but should be used with caution. As Kitanidis notes, kriging with a moving window can produce unexpected oddities if the variogram model is not carefully chosen. Returns ------- kvalues : ndarray, shape (L, M, N) or (N, 1) Interpolated values of specified grid or at the specified set of points. If style was specified as 'masked', kvalues will be a numpy masked array. sigmasq : ndarray, shape (L, M, N) or (N, 1) Variance at specified grid points or at the specified set of points. If style was specified as 'masked', sigmasq will be a numpy masked array.", "response": "def execute(self, style, xpoints, ypoints, zpoints, mask=None,\n                backend='vectorized', n_closest_points=None):\n        \"\"\"Calculates a kriged grid and the associated variance.\n\n        This is now the method that performs the main kriging calculation.\n        Note that currently measurements (i.e., z values) are\n        considered 'exact'. This means that, when a specified coordinate\n        for interpolation is exactly the same as one of the data points,\n        the variogram evaluated at the point is forced to be zero.\n        Also, the diagonal of the kriging matrix is also always forced\n        to be zero. In forcing the variogram evaluated at data points\n        to be zero, we are effectively saying that there is no variance\n        at that point (no uncertainty, so the value is 'exact').\n\n        In the future, the code may include an extra 'exact_values' boolean\n        flag that can be adjusted to specify whether to treat the\n        measurements as 'exact'. Setting the flag to false would indicate\n        that the variogram should not be forced to be zero at zero distance\n        (i.e., when evaluated at data points). Instead, the uncertainty in the\n        point will be equal to the nugget. This would mean that the diagonal\n        of the kriging matrix would be set to the nugget instead of to zero.\n\n        Parameters\n        ----------\n        style : str\n            Specifies how to treat input kriging points.\n            Specifying 'grid' treats xpoints, ypoints, and zpoints as arrays of\n            x, y, and z coordinates that define a rectangular grid.\n            Specifying 'points' treats xpoints, ypoints, and zpoints as arrays\n            that provide coordinates at which to solve the kriging system.\n            Specifying 'masked' treats xpoints, ypoints, and zpoints as arrays\n            of x, y, and z coordinates that define a rectangular grid and uses\n            mask to only evaluate specific points in the grid.\n        xpoints : array_like, shape (N,) or (N, 1)\n            If style is specific as 'grid' or 'masked', x-coordinates of\n            LxMxN grid. If style is specified as 'points', x-coordinates of\n            specific points at which to solve kriging system.\n        ypoints : array-like, shape (M,) or (M, 1)\n            If style is specified as 'grid' or 'masked', y-coordinates of\n            LxMxN grid. If style is specified as 'points', y-coordinates of\n            specific points at which to solve kriging system.\n            Note that in this case, xpoints, ypoints, and zpoints must have the\n            same dimensions (i.e., L = M = N).\n        zpoints : array-like, shape (L,) or (L, 1)\n            If style is specified as 'grid' or 'masked', z-coordinates of\n            LxMxN grid. If style is specified as 'points', z-coordinates of\n            specific points at which to solve kriging system.\n            Note that in this case, xpoints, ypoints, and zpoints must have the\n            same dimensions (i.e., L = M = N).\n        mask : boolean array, shape (L, M, N), optional\n            Specifies the points in the rectangular grid defined by xpoints,\n            ypoints, zpoints that are to be excluded in the\n            kriging calculations. Must be provided if style is specified\n            as 'masked'. False indicates that the point should not be masked,\n            so the kriging system will be solved at the point.\n            True indicates that the point should be masked, so the kriging\n            system should will not be solved at the point.\n        backend : str, optional\n            Specifies which approach to use in kriging. Specifying 'vectorized'\n            will solve the entire kriging problem at once in a\n            vectorized operation. This approach is faster but also can consume a\n            significant amount of memory for large grids and/or large datasets.\n            Specifying 'loop' will loop through each point at which the kriging\n            system is to be solved. This approach is slower but also less\n            memory-intensive. Default is 'vectorized'.\n        n_closest_points : int, optional\n            For kriging with a moving window, specifies the number of nearby\n            points to use in the calculation. This can speed up the calculation\n            for large datasets, but should be used with caution.\n            As Kitanidis notes, kriging with a moving window can produce\n            unexpected oddities if the variogram model is not carefully chosen.\n\n        Returns\n        -------\n        kvalues : ndarray, shape (L, M, N) or (N, 1)\n            Interpolated values of specified grid or at the specified set\n            of points. If style was specified as 'masked', kvalues will be a\n            numpy masked array.\n        sigmasq : ndarray, shape (L, M, N) or (N, 1)\n            Variance at specified grid points or at the specified set of points.\n            If style was specified as 'masked', sigmasq will be a numpy\n            masked array.\n        \"\"\"\n\n        if self.verbose:\n            print(\"Executing Ordinary Kriging...\\n\")\n\n        if style != 'grid' and style != 'masked' and style != 'points':\n            raise ValueError(\"style argument must be 'grid', 'points', \"\n                             \"or 'masked'\")\n\n        xpts = np.atleast_1d(np.squeeze(np.array(xpoints, copy=True)))\n        ypts = np.atleast_1d(np.squeeze(np.array(ypoints, copy=True)))\n        zpts = np.atleast_1d(np.squeeze(np.array(zpoints, copy=True)))\n        n = self.X_ADJUSTED.shape[0]\n        nx = xpts.size\n        ny = ypts.size\n        nz = zpts.size\n        a = self._get_kriging_matrix(n)\n\n        if style in ['grid', 'masked']:\n            if style == 'masked':\n                if mask is None:\n                    raise IOError(\"Must specify boolean masking array when \"\n                                  \"style is 'masked'.\")\n                if mask.ndim != 3:\n                    raise ValueError(\"Mask is not three-dimensional.\")\n                if mask.shape[0] != nz or mask.shape[1] != ny or mask.shape[2] != nx:\n                    if mask.shape[0] == nx and mask.shape[2] == nz and mask.shape[1] == ny:\n                        mask = mask.swapaxes(0, 2)\n                    else:\n                        raise ValueError(\"Mask dimensions do not match \"\n                                         \"specified grid dimensions.\")\n                mask = mask.flatten()\n            npt = nz * ny * nx\n            grid_z, grid_y, grid_x = np.meshgrid(zpts, ypts, xpts, indexing='ij')\n            xpts = grid_x.flatten()\n            ypts = grid_y.flatten()\n            zpts = grid_z.flatten()\n        elif style == 'points':\n            if xpts.size != ypts.size and ypts.size != zpts.size:\n                raise ValueError(\"xpoints, ypoints, and zpoints must have \"\n                                 \"same dimensions when treated as listing \"\n                                 \"discrete points.\")\n            npt = nx\n        else:\n            raise ValueError(\"style argument must be 'grid', \"\n                             \"'points', or 'masked'\")\n\n        xpts, ypts, zpts = \\\n            _adjust_for_anisotropy(np.vstack((xpts, ypts, zpts)).T,\n                                   [self.XCENTER, self.YCENTER, self.ZCENTER],\n                                   [self.anisotropy_scaling_y, self.anisotropy_scaling_z],\n                                   [self.anisotropy_angle_x, self.anisotropy_angle_y, self.anisotropy_angle_z]).T\n\n        if style != 'masked':\n            mask = np.zeros(npt, dtype='bool')\n\n        xyz_points = np.concatenate((zpts[:, np.newaxis], ypts[:, np.newaxis],\n                                     xpts[:, np.newaxis]), axis=1)\n        xyz_data = np.concatenate((self.Z_ADJUSTED[:, np.newaxis],\n                                   self.Y_ADJUSTED[:, np.newaxis],\n                                   self.X_ADJUSTED[:, np.newaxis]), axis=1)\n        bd = cdist(xyz_points, xyz_data, 'euclidean')\n\n        if n_closest_points is not None:\n            from scipy.spatial import cKDTree\n            tree = cKDTree(xyz_data)\n            bd, bd_idx = tree.query(xyz_points, k=n_closest_points, eps=0.0)\n            if backend == 'loop':\n                kvalues, sigmasq = \\\n                    self._exec_loop_moving_window(a, bd, mask, bd_idx)\n            else:\n                raise ValueError(\"Specified backend '{}' not supported \"\n                                 \"for moving window.\".format(backend))\n        else:\n            if backend == 'vectorized':\n                kvalues, sigmasq = self._exec_vector(a, bd, mask)\n            elif backend == 'loop':\n                kvalues, sigmasq = self._exec_loop(a, bd, mask)\n            else:\n                raise ValueError('Specified backend {} is not supported for '\n                                 '3D ordinary kriging.'.format(backend))\n\n        if style == 'masked':\n            kvalues = np.ma.array(kvalues, mask=mask)\n            sigmasq = np.ma.array(sigmasq, mask=mask)\n\n        if style in ['masked', 'grid']:\n            kvalues = kvalues.reshape((nz, ny, nx))\n            sigmasq = sigmasq.reshape((nz, ny, nx))\n\n        return kvalues, sigmasq"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a dict of information about a given traceback.", "response": "def get_frame_info(tb, context_lines=7, simple=False):\n    \"\"\"\n    Return a dict of information about a given traceback.\n    \"\"\"\n    # line numbers / function / variables\n    lineno = tb.tb_lineno\n    function = tb.tb_frame.f_code.co_name\n    variables = tb.tb_frame.f_locals\n\n    files = {}\n\n    # get filename\n    if simple:\n        fn = tb.tb_frame.f_code.co_filename\n    else:\n        fn = tb.tb_frame.f_globals.get('__file__')\n        if not fn:\n            fn = os.path.realpath(inspect.getsourcefile(tb) or\n                                  inspect.getfile(tb))\n        if fn[-4:] in ('.pyc', '.pyo'):\n            fn = fn[:-1]\n\n    #if filename is existed, then just read the file\n    # get loader\n    loader = None\n    if not os.path.exists(fn):\n        loader = tb.tb_frame.f_globals.get('__loader__')\n        while not loader and tb.tb_next:\n            tb = tb.tb_next\n            loader = tb.tb_frame.f_globals.get('__loader__')\n\n\n    # sourcecode\n    source = ''\n    pre_context, post_context = [], []\n    context_line = raw_context_line = context_lineno = None\n    try:\n        if loader:\n            source = loader.get_source(fn)\n        else:\n            if not fn in files:\n                source = open(fn).read()\n                files[fn] = source\n            else:\n                source = files[fn]\n    except:\n        pass\n    else:\n        try:\n            raw_context_line = source.splitlines()[lineno - 1].strip()\n        except IndexError:\n            pass\n        if not simple:\n            parsed_source = highlight_python(source)\n            lbound = max(0, lineno - context_lines - 1)\n            ubound = lineno + context_lines\n            try:\n                context_line = parsed_source[lineno - 1]\n                pre_context = parsed_source[lbound:lineno - 1]\n                post_context = parsed_source[lineno:ubound]\n            except IndexError as e:\n                pass\n            context_lineno = lbound\n\n    if isinstance(fn, unicode):\n        fn = fn.encode('utf-8')\n    return {\n        'tb':               tb,\n        'filename':         fn,\n        'basename':         os.path.basename(fn),\n        'loader':           loader,\n        'function':         function,\n        'lineno':           lineno,\n        'vars':             variables,\n        'pre_context':      pre_context,\n        'context_line':     context_line,\n        'raw_context_line': raw_context_line,\n        'post_context':     post_context,\n        'context_lineno':   context_lineno,\n        'source':           source\n    }"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of lines that are HTML output.", "response": "def get_html_output(self):\n        \"\"\" Return line generator. \"\"\"\n        def html_splitlines(lines):\n            # this cool function was taken from trac.\n            # http://projects.edgewall.com/trac/\n            open_tag_re = re.compile(r'<(\\w+)(\\s.*)?[^/]?>')\n            close_tag_re = re.compile(r'</(\\w+)>')\n            open_tags = []\n            for line in lines:\n                for tag in open_tags:\n                    line = tag.group(0) + line\n                open_tags = []\n                for tag in open_tag_re.finditer(line):\n                    open_tags.append(tag)\n                open_tags.reverse()\n                for ctag in close_tag_re.finditer(line):\n                    for otag in open_tags:\n                        if otag.group(1) == ctag.group(1):\n                            open_tags.remove(otag)\n                            break\n                for tag in open_tags:\n                    line += '</%s>' % tag.group(1)\n                yield line\n\n        if self.error:\n            return escape(self.raw).splitlines()\n        return list(html_splitlines(self.out.getvalue().splitlines()))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_sort_field(model=None, sort_field='sort', order_name='asc'):\r\n    from uliweb import request\r\n    \r\n    if request.values.getlist('sort'):\r\n        sort_fields = request.values.getlist('sort')\r\n        order_by = []\r\n        orders = request.values.getlist('order')\r\n        for i, f in enumerate(sort_fields):\r\n            _order = 'asc'\r\n            if not orders:\r\n                if f.endswith('.asc') or f.endswith('.desc'):\r\n                    f, _order = f.rsplit('.', 1)\r\n                else:\r\n                    continue\r\n            field = get_column(f, model)\r\n            if field is not None:\r\n                if orders:\r\n                    _order = orders[i]\r\n                if _order == 'asc':\r\n                    order_by.append(field)\r\n                else:\r\n                    order_by.append(field.desc())\r\n    else:\r\n        order_by = None\r\n        \r\n    return order_by", "response": "Returns a list of fields that can be used in order_by in a list of fields in a single word list."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_fields(model, fields, meta=None):\r\n    model = get_model(model)\r\n    if fields is not None:\r\n        f = fields\r\n    elif meta and hasattr(model, meta):\r\n        m = getattr(model, meta)\r\n        if hasattr(m, 'fields'):\r\n            f = m.fields\r\n        else:\r\n            f = model._fields_list\r\n    else:\r\n        f = model._fields_list\r\n        \r\n    fields_list = []\r\n    for x in f:\r\n        field = {}\r\n        if isinstance(x, str):  #so x is field_name\r\n            field['name'] = x\r\n        elif isinstance(x, tuple):\r\n            field['name'] = x[0]\r\n            field['field'] = x[1]\r\n        elif isinstance(x, dict):\r\n            field = x.copy()\r\n        else:\r\n            raise UliwebError('Field definition {!r} is not right, it should be just like (field_name, form_field_obj)'.format(x))\r\n        \r\n        if 'prop' not in field:\r\n            if hasattr(model, field['name']):\r\n                field['prop'] = getattr(model, field['name'])\r\n            else:\r\n                field['prop'] = None\r\n        \r\n        fields_list.append((field['name'], field))\r\n    return fields_list", "response": "Returns a list of fields that are in the model and the given fields."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets model columns list", "response": "def get_columns(model=None, fields=None, meta=None):\r\n    \"\"\"\r\n    Get model columns list\r\n    \"\"\"\r\n    if model:\r\n        M = get_model(model)\r\n    else:\r\n        M = None\r\n    if fields is not None:\r\n        f = fields\r\n\r\n    if M:\r\n        if meta and hasattr(M, meta):\r\n            m = getattr(model, meta)\r\n            if hasattr(m, 'fields'):\r\n                f = m.fields\r\n            else:\r\n                f = M._fields_list\r\n        else:\r\n            f = M._fields_list\r\n\r\n    columns = []\r\n    for x in f:\r\n        if isinstance(x, str):  # so x is field_name\r\n            field_name = x\r\n        elif isinstance(x, dict):\r\n            field_name = x['name']\r\n        else:\r\n            raise UliwebError(\"Field definition is not right, it should be just like str or {'name':xxx}\")\r\n\r\n        if '.' in field_name:\r\n            model_name, field_name = field_name.split('.')\r\n            M = get_model(model_name)\r\n\r\n        if not M:\r\n            raise UliwebError(\"Model can't be empty, because field name not has `model.` prefix\")\r\n\r\n        if field_name in M.c:\r\n            columns.append(M.c[field_name])\r\n\r\n    return columns"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list of fields info according to the model class", "response": "def get_model_columns(model, fields=None, meta='Table'):\r\n    \"\"\"\r\n    Get fields info according model class, the fields just like ListView fields definition\r\n    :param fields: A list\r\n    :param meta: if no fields, it'll use meta\r\n    \"\"\"\r\n    from copy import deepcopy\r\n\r\n    fields = fields or []\r\n    model = functions.get_model(model)\r\n\r\n    if not fields:\r\n        if hasattr(model, meta):\r\n            fields = getattr(model, meta).fields\r\n        else:\r\n            fields = [x for x, y in model._fields_list]\r\n\r\n    fields_list = []\r\n    for x in fields:\r\n        if isinstance(x, (str, unicode)):\r\n            f = get_grid_column(model, x)\r\n        elif isinstance(x, dict):\r\n            name = x['name']\r\n            f = deepcopy(x)\r\n            if 'verbose_name' in x:\r\n                f['title'] = x['verbose_name']\r\n            if 'title' not in f:\r\n                f.update(get_grid_column(model, name))\r\n        else:\r\n            raise ValueError(\"Field should be string or dict type, but {!r} found\".format(x))\r\n        fields_list.append(f)\r\n    return fields_list"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_field(name, model=None):\r\n    if '.' in name:\r\n        m, name = name.split('.')\r\n        model = get_model(m)\r\n\r\n    if model:\r\n        return getattr(model, name, None)", "response": "get model field according to name"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_column(name, model=None):\r\n    if '.' in name:\r\n        m, name = name.split('.')\r\n        model = get_model(m)\r\n\r\n    if model:\r\n        return model.c.get(name)", "response": "get table column according to name"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting model field according to name", "response": "def get_field_model(name, model=None):\r\n    \"\"\"\r\n    get model field according to name\r\n    \"\"\"\r\n    if '.' in name:\r\n        m, name = name.split('.')\r\n        model = get_model(m)\r\n\r\n    if model:\r\n        return getattr(model, name, None), model\r\n    else:\r\n        return None, None"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets model field according to name", "response": "def get_column_model(name, model=None):\r\n    \"\"\"\r\n    get model field according to name\r\n    \"\"\"\r\n    if '.' in name:\r\n        m, name = name.split('.')\r\n        model = get_model(m)\r\n\r\n    if model:\r\n        return model.c.get(name), model\r\n    else:\r\n        return None, None"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmakes form field according field value", "response": "def make_form_field(field, model=None, field_cls=None,\r\n                    use_default_value=True,\r\n                    builds_args_map=None):\r\n    \"\"\"\r\n    make form field according field value\r\n    :param field: such as: str, Form Field instance, dict\r\n        if field is str type, it'll fetch property from model or str is like 'model.name'\r\n        it'll fetch property `name` from `model`\r\n    :param model: if field is str type, it'll may use model value to fetch property\r\n    :param field_cls: if not applied, it'll fetch from property, field_cls sould be Field class\r\n    :param builds_args_map:\r\n    :return: Field instance\r\n    \"\"\"\r\n    import uliweb.form as form\r\n    from uliweb.form.validators import TEST_MAXLENGTH\r\n\r\n    builds_args_map = builds_args_map or {}\r\n\r\n    if model:\r\n        model = get_model(model)\r\n\r\n    if isinstance(field, BaseField):\r\n        return field\r\n    if isinstance(field, dict) and 'field' in field and isinstance(field['field'], BaseField):\r\n        return field['field']\r\n\r\n    field_type = None\r\n    prop = None #model property if existed\r\n\r\n    #process prop\r\n    if isinstance(field, (str, unicode)):\r\n        field = {'name':field}\r\n\r\n    if isinstance(field, dict):\r\n        prop = field.get('prop')\r\n        if not prop and model:\r\n            #if there is field_name, then use it property class\r\n            field_name = field.get('field_name') or field['name']\r\n            prop = get_model_property(model, field_name)\r\n            #if existed field_cls, then it'll use it to create form field\r\n            if not prop and not field_cls:\r\n                raise UliwebError(\"Can't find property %s in Model(%r)\" %\r\n                                  (field_name, model.__name__))\r\n    else:\r\n        raise UliwebError(\"Can't support this field %r type in make_form_field\" % type(field))\r\n\r\n    default_kwargs = {}\r\n    if prop:\r\n        default_kwargs = {\r\n            'label':prop.label or prop.property_name,\r\n            'help_string':prop.hint,\r\n            'placeholder':prop.placeholder,\r\n            'html_attrs':prop.extra.get('html_attrs', {}),\r\n            'required':prop.required,\r\n        }\r\n    kwargs = {\r\n        'label':field.get('label') or field.get('verbose_name') or default_kwargs.get('label'),\r\n        'help_string':field.get('hint') or field.get('help_string') or default_kwargs.get('help_string'),\r\n        'placeholder':field.get('placeholder') or default_kwargs.get('placeholder'),\r\n        'html_attrs':field.get('extra', {}).get('html_attrs', {}) or default_kwargs.get('html_attrs'),\r\n        'required':field.get('required', False) if 'required' in field else default_kwargs.get('required', False)\r\n    }\r\n\r\n    #add data-url support\r\n    if 'data-url' in field:\r\n        kwargs['html_attrs']['data-url'] = field['data-url']\r\n\r\n    if use_default_value:\r\n        v = prop.default_value()\r\n        kwargs['default'] = v\r\n\r\n    if field.get('static'):\r\n        field_type = form.StringField\r\n        kwargs['required'] = False\r\n        kwargs['static'] = True\r\n        if prop.choices is not None:\r\n            kwargs['choices'] = prop.get_choices()\r\n\r\n    if field.get('hidden'):\r\n        field_type = form.HiddenField\r\n        \r\n    if field_cls:\r\n        field_type = field_cls\r\n    elif not field_type:\r\n        cls = prop.__class__\r\n        type_name = prop.type_name\r\n        if type_name == 'BLOG':\r\n            pass\r\n        elif type_name in ('TEXT', 'JSON'):\r\n            field_type = form.TextField\r\n        elif type_name in ('CHAR', 'VARCHAR', 'UUID'):\r\n            if prop.choices is not None:\r\n                field_type = form.SelectField\r\n                kwargs['choices'] = prop.get_choices()\r\n            else:\r\n                if cls.__name__ == 'FileProperty':\r\n                    field_type = form.FileField\r\n                    kwargs['upload_to'] = prop.upload_to\r\n                    kwargs['upload_to_sub'] = prop.upload_to_sub\r\n                else:\r\n                    field_type = form.UnicodeField\r\n        elif type_name == 'BOOL':\r\n            field_type = form.BooleanField\r\n        elif type_name == 'DATE':\r\n#            if not prop.auto_now and not prop.auto_now_add:\r\n            field_type = form.DateField\r\n        elif type_name == 'TIME':\r\n#            if not prop.auto_now and not prop.auto_now_add:\r\n            field_type = form.TimeField\r\n        elif type_name == 'DATETIME':\r\n#            if not prop.auto_now and not prop.auto_now_add:\r\n            field_type = form.DateTimeField\r\n        elif type_name == 'DECIMAL':\r\n            field_type = form.StringField\r\n            if prop.choices is not None:\r\n                field_type = form.SelectField\r\n                kwargs['choices'] = prop.get_choices()\r\n        elif type_name == 'FLOAT':\r\n            field_type = form.FloatField\r\n        elif type_name == 'INTEGER':\r\n            if prop.choices is not None:\r\n                field_type = form.SelectField\r\n                kwargs['choices'] = prop.get_choices()\r\n                kwargs['datetype'] = int\r\n            else:\r\n                field_type = form.IntField\r\n        elif type_name == 'ManyToMany':\r\n            kwargs['model'] = prop.reference_class\r\n            field_type = ManyToManySelectField\r\n        elif type_name in ('Reference', 'OneToOne'):\r\n            #field_type = form.IntField\r\n            kwargs['model'] = prop.reference_class\r\n            kwargs['value_field'] = prop.reference_fieldname\r\n            field_type = ReferenceSelectField\r\n        else:\r\n            raise ValueError(\"Can't support the Property [%s=%s]\" %\r\n                             (field['name'], prop.__class__.__name__))\r\n       \r\n    if field_type:\r\n        build_args = builds_args_map.get(field_type, {})\r\n        #add settings.ini configure support\r\n        #so you could add options in settings.ini like this\r\n        #  [GENERIC_FIELDS_MAPPING]\r\n        #  FormFieldClassName = {'build':'model.NewFormFieldTypeClassName', **other args}\r\n        #  \r\n        #  e.g.\r\n        #  [GENERIC_FIELDS_MAPPING]\r\n        #  DateField = {'build':'jquery.widgets.DatePicker'}\r\n        if not build_args:\r\n            build_args = get_fileds_builds().get(field_type, {})\r\n        kwargs.update(build_args)\r\n        \r\n        #add max_length validator\r\n        if issubclass(prop.__class__, (orm.StringProperty, orm.CharProperty, orm.UnicodeProperty)):\r\n            v = kwargs.setdefault('validators', [])\r\n            if isinstance(prop.max_length, int):\r\n                v.append(TEST_MAXLENGTH(prop.max_length))\r\n\r\n        f = field_type(**kwargs)\r\n    \r\n        return f"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef make_view_field(field, obj=None, types_convert_map=None, fields_convert_map=None, \r\n    value=__default_value__, auto_convert=True):\r\n    \"\"\"\r\n    If auto_convert, then all values will be converted to string format, otherwise\r\n    remain the orignal value\r\n    \"\"\"\r\n    from uliweb.utils.textconvert import text2html\r\n    from uliweb.core.html import Tag\r\n    \r\n    old_value = value\r\n\r\n    types_convert_map = types_convert_map or {}\r\n    fields_convert_map = fields_convert_map or {}\r\n    default_convert_map = {orm.TextProperty:lambda v,o:text2html(v)}\r\n    \r\n    if isinstance(field, dict):\r\n        if 'prop' in field and field.get('prop'):\r\n            prop = field['prop']\r\n        else:\r\n            prop = field\r\n        name = field.get('name')\r\n    else:\r\n        prop = field\r\n        name = prop.property_name\r\n        \r\n    #not real Property instance, then return itself, so if should return\r\n    #just like {'label':xxx, 'value':xxx, 'display':xxx}\r\n    if not isinstance(prop, orm.Property):  \r\n        if old_value is __default_value__:\r\n            value = prop.get('value', '')\r\n        display = prop.get('display', value)\r\n        label = prop.get('label', '') or prop.get('verbose_name', '')\r\n        convert = prop.get('convert', None)\r\n    else:\r\n        if old_value is __default_value__:\r\n            if isinstance(obj, Model):\r\n                value = prop.get_value_for_datastore(obj)\r\n                if value is Lazy:\r\n                    getattr(obj, prop.property_name)\r\n                    value = prop.get_value_for_datastore(obj)\r\n            else:\r\n                value = obj[name]\r\n        if auto_convert or prop.choices:\r\n            display = prop.get_display_value(value)\r\n        else:\r\n            display = value\r\n        \r\n        if isinstance(field, dict):\r\n            initial = field.get('verbose_name', None)\r\n        else:\r\n            initial = ''\r\n        label = initial or prop.verbose_name or name\r\n        \r\n    if name in fields_convert_map:\r\n        convert = fields_convert_map.get(name, None)\r\n    else:\r\n        if isinstance(prop, orm.Property):\r\n            convert = types_convert_map.get(prop.__class__, None)\r\n            if not convert:\r\n                convert = default_convert_map.get(prop.__class__, None)\r\n    \r\n    convert_result = None\r\n    if convert:\r\n        convert_result = convert(value, obj)\r\n    \r\n    if convert_result is None:\r\n        if value is not None:\r\n            if isinstance(prop, orm.ManyToMany):\r\n                s = []\r\n                #support value parameter, the old value is already stored in \"old_value\" variable\r\n                if old_value is not __default_value__:\r\n                    if prop.reference_fieldname == 'id':\r\n                        query = []\r\n                        for _id in old_value:\r\n                            _v = functions.get_cached_object(prop.reference_class, _id)\r\n                            query.append(_v)\r\n                    else:\r\n                        query = prop.reference_class.filter(prop.reference_class.c[prop.reversed_fieldname].in_(old_value))\r\n                else:\r\n                    if prop.reference_fieldname == 'id':\r\n                        query = []\r\n                        _ids = prop.get_value_for_datastore(obj, cached=True)\r\n                        for _id in _ids:\r\n                            _v = functions.get_cached_object(prop.reference_class, _id)\r\n                            if not _v:\r\n                                log.debug(\"Can't find object %s:%d\" % (prop.reference_class.__name__, _id))\r\n                                _v = _id\r\n                            query.append(_v)\r\n                    else:\r\n                        query = getattr(obj, prop.property_name).all()\r\n                        \r\n                for x in query:\r\n                    if isinstance(x, orm.Model):\r\n                        s.append(get_obj_url(x))\r\n                    else:\r\n                        s.append(str(x))\r\n                display = ' '.join(s)\r\n            elif isinstance(prop, orm.ReferenceProperty) or isinstance(prop, orm.OneToOne):\r\n                try:\r\n                    if old_value is not __default_value__:\r\n                        d = prop.reference_class.c[prop.reference_fieldname]\r\n                        if prop.reference_fieldname == 'id':\r\n                            v = functions.get_cached_object(prop.reference_class, old_value)\r\n                        else:\r\n                            v = prop.reference_class.get(d==old_value)\r\n                    if not isinstance(obj, Model):\r\n                        d = prop.reference_class.c[prop.reference_fieldname]\r\n                        if prop.reference_fieldname == 'id':\r\n                            v = functions.get_cached_object(prop.reference_class, value)\r\n                        else:\r\n                            v = prop.reference_class.get(d==value)\r\n                    else:\r\n                        if prop.reference_fieldname == 'id':\r\n                            v = functions.get_cached_object(prop.reference_class, obj.get_datastore_value(prop.property_name))\r\n                        else:\r\n                            v = functions.get_cached_object(prop.reference_class,\r\n                                    condition=prop.reference_class.c[prop.reference_fieldname]==obj.get_datastore_value(prop.property_name))\r\n                except orm.Error:\r\n                    display = prop.get_datastore_value(obj) or ''\r\n                    v = None\r\n                if isinstance(v, Model):\r\n                    display = get_obj_url(v)\r\n                else:\r\n                    display = str(v if v is not None else '')\r\n            elif isinstance(prop, orm.FileProperty):\r\n                url = functions.get_href(value)\r\n                if url:\r\n                    display = str(Tag('a', value, href=url))\r\n                else:\r\n                    display = ''\r\n#            if isinstance(prop, orm.Property) and prop.choices is not None:\r\n#                display = prop.get_display_value(value)\r\n            if prop.__class__ is orm.TextProperty:\r\n                display = text2html(value)\r\n       \r\n    else:\r\n        display = convert_result\r\n        \r\n    if isinstance(display, unicode):\r\n        display = display.encode('utf-8')\r\n    if display is None:\r\n        display = ''\r\n        \r\n    return Storage({'label':label, 'value':value, 'display':display, 'name':name})", "response": "Returns a new field that can be used in viewing the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_value_for_datastore(self, model_instance):\r\n        table_id = getattr(model_instance, self.table_fieldname, None)\r\n        object_id = getattr(model_instance, self.object_fieldname, None)\r\n        return table_id, object_id", "response": "Get key of reference rather than reference itself."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef prepare_static_data(self, data):\r\n        d = data.copy()\r\n        for f in self.get_fields():\r\n            if f['static'] and f['name'] in d:\r\n                d[f['name']] = make_view_field(f, None, self.types_convert_map, self.fields_convert_map, d[f['name']])['display']\r\n        return d", "response": "Prepare static data for the view."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _process_file(self, obj, fobj, field):\r\n        from uliweb import settings\r\n        \r\n        paths = []\r\n        upload_to = self.upload_to or self._get_upload_path(field, 'upload_to', obj)\r\n        if upload_to:\r\n            self.fileserving.to_path = upload_to\r\n        upload_to_sub = self.upload_to_sub or self._get_upload_path(field, 'upload_to_sub', obj)\r\n        if upload_to_sub:\r\n            paths.append(upload_to_sub)\r\n        paths.append(fobj['filename'])\r\n        \r\n        return self.fileserving.save_file(os.path.join(*paths), \r\n            fobj['file'], replace=self.file_replace, \r\n            convert=self.file_convert)", "response": "Process a single file and save it to the file manager"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef prepare_static_data(self, data):\r\n        d = self.obj.to_dict()\r\n        d.update(data.copy())\r\n        for f in self.get_fields():\r\n            if f['static'] and f['name'] in d:\r\n                v = make_view_field(f, self.obj, self.types_convert_map, self.fields_convert_map, d[f['name']])\r\n                d[f['name']] = v['display']\r\n        return d", "response": "Prepare static data for the object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef count(self, query):\r\n        if self.manual:\r\n            return self.total\r\n        \r\n        if isinstance(query, Select):\r\n            q = query.with_only_columns([func.count()]).order_by(None).limit(None).offset(None)\r\n            return do_(q).scalar()\r\n\r\n        return query.count()", "response": "Returns count of all the records in the specified table."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef download(self, filename, timeout=3600, action=None, query=None,\r\n                 fields_convert_map=None, type=None, domain=None,\r\n                 template_filename='', sheet_name='', **kwargs):\r\n        \"\"\"\r\n        Default domain option is PARA/DOMAIN\r\n        :param template_filename: Excel template filename, it'll xltools to writer it, only can be used\r\n            in xlsx\r\n        \"\"\"\r\n        from uliweb import settings\r\n        \r\n        fields_convert_map = fields_convert_map or self.fields_convert_map\r\n        \r\n        t_filename = self.get_real_file(filename)\r\n        if os.path.exists(t_filename):\r\n            if timeout and os.path.getmtime(t_filename) + timeout > time.time():\r\n                return self.downloader.download(filename, action)\r\n            \r\n        if query is None:\r\n            query = self.query_all()\r\n        if not type:\r\n            type = os.path.splitext(filename)[1]\r\n            if type:\r\n                type = type[1:]\r\n            else:\r\n                type = 'csv'\r\n        if type in ('xlt', 'xls'):\r\n            if not domain:\r\n                domain = settings.get_var('PARA/DOMAIN')\r\n            return self.download_xlt(filename, query, action, fields_convert_map, domain, not_tempfile=bool(timeout), **kwargs)\r\n        if type in ('xlsx',):\r\n            if not domain:\r\n                domain = settings.get_var('PARA/DOMAIN')\r\n            return self.download_xlsx(filename, query, action, fields_convert_map, domain,\r\n                                      not_tempfile=bool(timeout), template_filename=template_filename,\r\n                                      sheet_name=sheet_name, **kwargs)\r\n        else:\r\n            return self.download_csv(filename, query, action, fields_convert_map, not_tempfile=bool(timeout), **kwargs)", "response": "Download a single file from the server"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list of data from the table.", "response": "def get_data(self, query, fields_convert_map, encoding='utf-8', auto_convert=True,\r\n                 include_hidden=False, header=None):\r\n        \"\"\"\r\n        If convert=True, will convert field value\r\n        \"\"\"\r\n        fields_convert_map = fields_convert_map or {}\r\n        d = self.fields_convert_map.copy() \r\n        d.update(fields_convert_map)\r\n\r\n        if isinstance(query, Select):\r\n            query = do_(query)\r\n        \r\n        # def get_value(name, value, record):\r\n        #     convert = d.get(name)\r\n        #     if convert:\r\n        #         value = convert(value, record)\r\n        #     return safe_unicode(value, encoding)\r\n        \r\n        for record in query:\r\n            self._cal_sum(record)\r\n            row = []\r\n            record = self._get_record(record)\r\n            if self.before_record_render:\r\n                self.before_record_render(record)\r\n            if isinstance(record, orm.Model):\r\n                model = record.__class__\r\n            else:\r\n                model = None\r\n                \r\n            for i, x in enumerate(self.table_info['fields_list']):\r\n                field = get_field(x['name'], model)\r\n                if not field:\r\n                    field = {'name':x['name']}\r\n                else:\r\n                    field = {'name':x['name'], 'prop':field}\r\n                if not include_hidden and x.get('hidden'):\r\n                    continue\r\n                if isinstance(record, orm.Model):\r\n                    v = make_view_field(field, record, fields_convert_map=d, \r\n                        auto_convert=auto_convert)\r\n                else:\r\n                    v = make_view_field(field, record, fields_convert_map=d, \r\n                        auto_convert=auto_convert, value=record[x['name']])\r\n                value = v['display']\r\n                #value = safe_unicode(v['display'], encoding)\r\n                row.append(value)\r\n                \r\n            if header:\r\n                ret = dict(zip(header, row))\r\n            else:\r\n                ret = row\r\n            yield ret\r\n\r\n        total = self._get_sum()\r\n        if total:\r\n            row = []\r\n            for x in total:\r\n                v = x\r\n                if isinstance(x, str):\r\n                    v = safe_unicode(x, encoding)\r\n                row.append(v)\r\n            if header:\r\n                ret = dict(zip(header, row))\r\n            else:\r\n                ret = row\r\n            yield ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a generator of all processed data", "response": "def objects(self, json_result=False):\r\n        \"\"\"\r\n        Return a generator of all processed data, it just like render\r\n        but it'll not return a table or json format data but just\r\n        data. And the data will be processed by fields_convert_map if passed.\r\n        \"\"\"\r\n        self.rows_num = 0\r\n        query = self.query()\r\n        if not isinstance(query, (orm.Result, list, dict)):\r\n            query = do_(query)\r\n        for record in query:\r\n            self.rows_num += 1\r\n            r = self.object(record, json_result)\r\n            self._cal_sum(record)\r\n            yield r\r\n        total = self._render_sum(True)\r\n        if total:\r\n            yield total"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef query_all(self):\r\n        return self.query_model(self.model, self.condition, order_by=self.order_by,\r\n                                group_by=self.group_by, having=self.having)", "response": "Query all records in the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nqueries all records in the model.", "response": "def query_model(self, model, condition=None, offset=None, limit=None,\r\n                    order_by=None, group_by=None, having=None, fields=None):\r\n        \"\"\"\r\n        Query all records with limit and offset, it's used for pagination query.\r\n        \"\"\"\r\n        from uliweb import settings, request\r\n        from uliweb.utils.common import log\r\n        from uliweb.orm import rawsql\r\n        \r\n        if self._query is not None:\r\n            query = self._query\r\n            if condition is not None and isinstance(query, Result):\r\n                query = query.filter(condition)\r\n        else:\r\n            query = self.model.filter(condition)\r\n\r\n        if request.GET.get('_debug_'):\r\n            log.info(rawsql(query.get_query()))\r\n        \r\n        if self.pagination:\r\n            if offset is not None:\r\n                query = query.offset(int(offset))\r\n            if limit is not None:\r\n                query = query.limit(int(limit))\r\n        if order_by is not None:\r\n            if isinstance(order_by, (tuple, list)):\r\n                query = query.order_by(*order_by)\r\n            else:\r\n                query = query.order_by(order_by)\r\n        if group_by is not None:\r\n            if isinstance(group_by, (tuple, list)):\r\n                query = query.group_by(*group_by)\r\n            else:\r\n                query = query.group_by(group_by)\r\n            if having is not None:\r\n                query = query.having(having)\r\n        return query"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef query_model(self, model, condition=None, offset=None, limit=None,\r\n                    group_by=None, having=None, order_by=None, fields=None):\r\n        \"\"\"\r\n        Query all records with limit and offset, it's used for pagination query.\r\n        \"\"\"\r\n        if self._query is not None:\r\n            query = self._query\r\n        else:\r\n            query = self.get_select()\r\n            \r\n        if condition is not None:\r\n            if isinstance(query, Result):\r\n                query = query.filter(condition)\r\n            else:\r\n                query = query.where(condition)\r\n        if self.pagination:\r\n            if offset is not None:\r\n                query = query.offset(int(offset))\r\n            if limit is not None:\r\n                query = query.limit(int(limit))\r\n        if order_by is not None:\r\n            if isinstance(order_by, (tuple, list)):\r\n                for order in order_by:\r\n                    query = query.order_by(order)\r\n            else:\r\n                query = query.order_by(order_by)\r\n        if group_by is not None:\r\n            if isinstance(group_by, (tuple, list)):\r\n                query = query.group_by(*group_by)\r\n            else:\r\n                query = query.group_by(group_by)\r\n            if having is not None:\r\n                query = query.having(having)\r\n        return query", "response": "Query all records in the specified model."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmake like condition :param column: column object :param format: '%_' '_%' '%_%' :param value: column value :return: condition object", "response": "def _make_like(self, column, format, value):\r\n        \"\"\"\r\n        make like condition\r\n        :param column: column object\r\n        :param format: '%_' '_%' '%_%'\r\n        :param value: column value\r\n        :return: condition object\r\n        \"\"\"\r\n        c = []\r\n        if format.startswith('%'):\r\n            c.append('%')\r\n        c.append(value)\r\n        if format.endswith('%'):\r\n            c.append('%')\r\n        return column.like(''.join(c))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _make_op(self, column, op, value):\r\n        if not op:\r\n            return None\r\n        if op == '>':\r\n            return column>value\r\n        elif op == '<':\r\n            return column<value\r\n        elif op == '>=':\r\n            return column>=value\r\n        elif op == '<=':\r\n            return column<=value\r\n        elif op == '==':\r\n            return column==value\r\n        elif op == '!=':\r\n            return column!=value\r\n        elif op == 'in':\r\n            return column.in_(value)\r\n        else:\r\n            raise KeyError('Not support this op[%s] value' % op)", "response": "make an operation condition object\r\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a flat copy of the dict.", "response": "def copy(self):\n        \"\"\"Create a flat copy of the dict.\"\"\"\n        missing = object()\n        result = object.__new__(self.__class__)\n        for name in self.__slots__:\n            val = getattr(self, name, missing)\n            if val is not missing:\n                setattr(result, name, val)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef list(self):\n        before, after = self.filename_template.split('%s', 1)\n        filename_re = re.compile(r'%s(.{5,})%s$' % (re.escape(before),\n                                                    re.escape(after)))\n        result = []\n        for filename in os.listdir(self.path):\n            #: this is a session that is still being saved.\n            if filename.endswith(_fs_transaction_suffix):\n                continue\n            match = filename_re.match(filename)\n            if match is not None:\n                result.append(match.group(1))\n        return result", "response": "Lists all the sessions in the store."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting a datetime to timezone", "response": "def to_timezone(dt, tzinfo=None):\n    \"\"\"\n    Convert a datetime to timezone\n    \"\"\"\n    if not dt:\n        return dt\n    tz = pick_timezone(tzinfo, __timezone__)\n    if not tz:\n        return dt\n    dttz = getattr(dt, 'tzinfo', None)\n    if not dttz:\n        return dt.replace(tzinfo=tz)\n    else:\n        return dt.astimezone(tz)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_date(dt, tzinfo=None, format=None):\n    d = to_datetime(dt, tzinfo, format)\n    if not d:\n        return d\n    return date(d.year, d.month, d.day)", "response": "Convert a datetime to date with tzinfo"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_time(dt, tzinfo=None, format=None):\n    d = to_datetime(dt, tzinfo, format)\n    if not d:\n        return d\n    return time_(d.hour, d.minute, d.second, d.microsecond, tzinfo=d.tzinfo)", "response": "Convert a datetime to time with tzinfo"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_datetime(dt, tzinfo=None, format=None):\n    if not dt:\n        return dt\n    \n    tz = pick_timezone(tzinfo, __timezone__)\n    \n    if isinstance(dt, (str, unicode)):\n        if not format:\n            formats = DEFAULT_DATETIME_INPUT_FORMATS\n        else:\n            formats = list(format)\n        d = None\n        for fmt in formats:\n            try:\n                d = datetime.strptime(dt, fmt)\n            except ValueError:\n                continue\n        if not d:\n            return None\n        d = d.replace(tzinfo=tz)\n    else:\n        d = datetime(getattr(dt, 'year', 1970), getattr(dt, 'month', 1),\n            getattr(dt, 'day', 1), getattr(dt, 'hour', 0), getattr(dt, 'minute', 0),\n            getattr(dt, 'second', 0), getattr(dt, 'microsecond', 0))\n        if not getattr(dt, 'tzinfo', None):\n            d = d.replace(tzinfo=tz)\n        else:\n            d = d.replace(tzinfo=dt.tzinfo)\n    return to_timezone(d, tzinfo)", "response": "Convert a date or time to datetime with tzinfo"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_time(t):\n    if isinstance(t, (str, unicode)):\n        b = re_time.match(t)\n        if b:\n            v, unit = int(b.group(1)), b.group(2)\n            if unit == 's':\n                return v*1000\n            elif unit == 'm':\n                return v*60*1000\n            elif unit == 'h':\n                return v*60*60*1000\n            else:\n                return v\n        else:\n            raise TimeFormatError(t)\n    elif isinstance(t, (int, long)):\n        return t\n    else:\n        raise TimeFormatError(t)", "response": "Parse string time format to microsecond\nialog"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the current exception info as Traceback object.", "response": "def get_current_traceback(ignore_system_exceptions=False):\n    \"\"\"Get the current exception info as `Traceback` object.  Per default\n    calling this method will reraise system exceptions such as generator exit,\n    system exit or others.  This behavior can be disabled by passing `False`\n    to the function as first parameter.\n    \"\"\"\n    exc_type, exc_value, tb = sys.exc_info()\n    if ignore_system_exceptions and exc_type in system_exceptions:\n        raise\n    return Traceback(exc_type, exc_value, tb)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndownloading a file into a new page.", "response": "def filedown(environ, filename, cache=True, cache_timeout=None,\n    action=None, real_filename=None, x_sendfile=False,\n    x_header_name=None, x_filename=None, fileobj=None,\n    default_mimetype='application/octet-stream'):\n    \"\"\"\n    @param filename: is used for display in download\n    @param real_filename: if used for the real file location\n    @param x_urlfile: is only used in x-sendfile, and be set to x-sendfile header\n    @param fileobj: if provided, then returned as file content\n    @type fileobj: (fobj, mtime, size)\n\n    filedown now support web server controlled download, you should set\n    xsendfile=True, and add x_header, for example:\n\n    nginx\n        ('X-Accel-Redirect', '/path/to/local_url')\n    apache\n        ('X-Sendfile', '/path/to/local_url')\n    \"\"\"\n    from .common import safe_str\n    from werkzeug.http import parse_range_header\n\n    guessed_type = mimetypes.guess_type(filename)\n    mime_type = guessed_type[0] or default_mimetype\n    real_filename = real_filename or filename\n\n    #make common headers\n    headers = []\n    headers.append(('Content-Type', mime_type))\n    d_filename = _get_download_filename(environ, os.path.basename(filename))\n    if action == 'download':\n        headers.append(('Content-Disposition', 'attachment; %s' % d_filename))\n    elif action == 'inline':\n        headers.append(('Content-Disposition', 'inline; %s' % d_filename))\n    if x_sendfile:\n        if not x_header_name or not x_filename:\n            raise Exception(\"x_header_name or x_filename can't be empty\")\n        headers.append((x_header_name, safe_str(x_filename)))\n        return Response('', status=200, headers=headers,\n            direct_passthrough=True)\n    else:\n        request = environ.get('werkzeug.request')\n        if request:\n            range = request.range\n        else:\n            range = parse_range_header(environ.get('HTTP_RANGE'))\n        #when request range,only recognize \"bytes\" as range units\n        if range and range.units==\"bytes\":\n            try:\n                fsize = os.path.getsize(real_filename)\n            except OSError as e:\n                return Response(\"Not found\",status=404)\n            mtime = datetime.utcfromtimestamp(os.path.getmtime(real_filename))\n            mtime_str = http_date(mtime)\n            if cache:\n                etag = _generate_etag(mtime, fsize, real_filename)\n            else:\n                etag = mtime_str\n\n            if_range = environ.get('HTTP_IF_RANGE')\n            if if_range:\n                check_if_range_ok = (if_range.strip('\"')==etag)\n                #print \"check_if_range_ok (%s) = (%s ==%s)\"%(check_if_range_ok,if_range.strip('\"'),etag)\n            else:\n                check_if_range_ok = True\n\n            rbegin,rend = range.ranges[0]\n            if check_if_range_ok and (rbegin+1)<fsize:\n                if rend == None:\n                    rend = fsize\n\n                headers.append(('Content-Length',str(rend-rbegin)))\n                #werkzeug do not count rend with the same way of rfc7233,so -1\n                headers.append(('Content-Range','%s %d-%d/%d' %(range.units,rbegin, rend-1, fsize)))\n                headers.append(('Last-Modified', mtime_str))\n                if cache:\n                    headers.append(('ETag', '\"%s\"' % etag))\n                #for small file, read it to memory and return directly\n                #and this can avoid some issue with google chrome\n                if (rend-rbegin) < FileIterator.chunk_size:\n                    s = \"\".join([chunk for chunk in FileIterator(real_filename,rbegin,rend)])\n                    return Response(s,status=206, headers=headers, direct_passthrough=True)\n                else:\n                    return Response(FileIterator(real_filename,rbegin,rend),\n                        status=206, headers=headers, direct_passthrough=True)\n\n        #process fileobj\n        if fileobj:\n            f, mtime, file_size = fileobj\n        else:\n            f, mtime, file_size = _opener(real_filename)\n        headers.append(('Date', http_date()))\n\n        if cache:\n            etag = _generate_etag(mtime, file_size, real_filename)\n            headers += [\n                ('ETag', '\"%s\"' % etag),\n            ]\n            if cache_timeout:\n                headers += [\n                    ('Cache-Control', 'max-age=%d, public' % cache_timeout),\n                    ('Expires', http_date(time() + cache_timeout))\n                ]\n            if not is_resource_modified(environ, etag, last_modified=mtime):\n                f.close()\n                return Response(status=304, headers=headers)\n        else:\n            headers.append(('Cache-Control', 'public'))\n\n\n        headers.extend((\n            ('Content-Length', str(file_size)),\n            ('Last-Modified', http_date(mtime))\n        ))\n\n        return Response(wrap_file(environ, f), status=200, headers=headers,\n            direct_passthrough=True)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nimport the objects in the specified modules.", "response": "def import_(module, objects=None, py2=None):\n    \"\"\"\n    :param module: py3 compatiable module path\n    :param objects: objects want to imported, it should be a list\n    :param via: for some py2 module, you should give the import path according the\n        objects which you want to imported\n    :return: object or module\n\n    Usage:\n        import_('urllib.parse', 'urlparse')\n        import_('urllib.parse', ['urlparse', 'urljoin'])\n        import_('urllib.parse', py2='urllib2')\n    \"\"\"\n    if not PY2:\n        mod = __import__(module, fromlist=['*'])\n        if objects:\n            if not isinstance(objects, (list, tuple)):\n                objects = [objects]\n\n            r = []\n            for x in objects:\n                r.append(getattr(mod, x))\n            if len(r) > 1:\n                return tuple(r)\n            else:\n                return r[0]\n\n        else:\n            return mod\n\n    else:\n        path = modules_mapping.get(module)\n        if not path:\n            raise Exception(\"Can't find the module {} in mappings.\".format(module))\n\n        if objects:\n            if not isinstance(objects, (list, tuple)):\n                objects = [objects]\n\n            r = []\n            for x in objects:\n                m = path.get(x)\n                if not m:\n                    raise Exception(\"Can't find the object {} in {}.\".format(x, path))\n                mod = __import__(m, fromlist=['*'])\n                r.append(getattr(mod, x))\n            if len(r) > 1:\n                return tuple(r)\n            else:\n                return r[0]\n\n        else:\n            if isinstance(path, (dict, list)):\n                if not py2:\n                    raise Exception(\"You should give py2 parameter to enable import from py2.\")\n                path = py2\n            mod = __import__(path, fromlist=['*'])\n\n            return mod"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nstill process session data when specially Exception", "response": "def process_exception(self, request, e):\n        \"\"\"\n        Still process session data when specially Exception\n        \"\"\"\n        if isinstance(e, RedirectException):\n            response = e.get_response()\n            self.process_response(request, response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprocesses fields_convert_map for ListView", "response": "def _process_fields_convert_map(self, parameters, download=False):\n        \"\"\"\n        process fields_convert_map, ListView doesn't support list type but dict\n\n        fields_convert_map should be define as list or dict\n        for list, it can be:\n            [name, name, ...]\n            [(name, func), (name, func), ...] if func is str, it'll be the property name of class\n        for dict, it can be:\n            {'name':func, ...}\n        :param model: model object\n        :param parameters:\n        :param prefix: it'll used to combine prefix+_convert_xxx to get convert function\n            from class\n        :return:\n        \"\"\"\n        if 'fields_convert_map' in parameters:\n            _f = parameters.get('fields_convert_map') or []\n            parameters['fields_convert_map'] = self._get_fields_convert_map(_f, download)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _list_view(self, model, **kwargs):\n        from uliweb import request\n\n        #add download fields process\n        fields = kwargs.pop('fields', None)\n        meta = kwargs.pop('meta', 'Table')\n        if 'download' in request.GET:\n            if 'download_fields' in kwargs:\n                fields = kwargs.pop('download_fields', fields)\n            if 'download_meta' in kwargs:\n                meta = kwargs.pop('download_meta')\n            else:\n                if hasattr(model, 'Download'):\n                    meta = 'Download'\n                else:\n                    meta = meta\n\n        view = functions.ListView(model, fields=fields, meta=meta, **kwargs)\n        return view", "response": ":param model:\n        :param fields_convert_map: it's different from ListView\n        :param kwargs:\n        :return:"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nqueries view for the current locale.", "response": "def _query_view(self, model, **kwargs):\n        \"\"\"\n        :param model:\n        :return: (query, condition)\n\n        Default use QueryForm\n        \"\"\"\n        QueryForm = functions.get_form('QueryForm')\n\n        if 'form_cls' not in kwargs:\n            kwargs['form_cls'] = QueryForm\n        query = functions.QueryView(model, **kwargs)\n        return query"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse download parameters from kwargs and return parsed data", "response": "def _parse_download_args(self, kwargs, fields):\n        \"\"\"\n        Parse download parameters from kwargs\n        :return: parsed data\n        \"\"\"\n\n        downloads = {}\n        downloads['filename'] = kwargs.pop('download_filename', 'download.xlsx')\n        downloads['action'] = kwargs.pop('download_action', 'download')\n        downloads['fields_convert_map'] = kwargs.pop('download_fields_convert_map',\n                                                     fields)\n        downloads['domain'] = kwargs.pop('download_domain', '')\n        downloads['timeout'] = 0\n        downloads['template_filename'] = kwargs.pop('download_template_filename', '')\n        downloads['template_data'] = kwargs.pop('download_template_data', None)\n        downloads.update(kwargs.pop('download_kwargs', {}))\n        return downloads"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nselecting a list view from a model.", "response": "def _select_list_view(self, model, **kwargs):\n        \"\"\"\n        :param model:\n        :param fields_convert_map: it's different from ListView\n        :param kwargs:\n        :return:\n        \"\"\"\n        from uliweb import request\n\n        # add download fields process\n        fields = kwargs.pop('fields', None)\n        meta = kwargs.pop('meta', 'Table')\n        if 'download' in request.GET:\n            if 'download_fields' in kwargs:\n                fields = kwargs.pop('download_fields', fields)\n            if 'download_meta' in kwargs:\n                meta = kwargs.pop('download_meta')\n            else:\n                if hasattr(model, 'Download'):\n                    meta = 'Download'\n                else:\n                    meta = meta\n\n        view = functions.SelectListView(model, fields=fields, meta=meta, **kwargs)\n        return view"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndefaulting search function :param search_field: Used for search field, default is 'name' :param value_field: Used for id field, default is id :param label_field: Used for label field, default is None, then it'll use unicode() function", "response": "def _search(self, model, condition=None, search_field='name',\n                value_field='id', label_field=None, pagination=True):\n        \"\"\"\n        Default search function\n        :param search_field: Used for search field, default is 'name'\n        :param value_field: Used for id field, default is id\n        :param label_field: Used for label field, default is None, then it'll use unicode() function\n        \"\"\"\n        from uliweb import json, request\n\n        name = request.GET.get('term', '')\n        M = functions.get_model(model)\n\n        def _v(label_field):\n            if label_field:\n                return lambda x: getattr(x, label_field)\n            else:\n                return lambda x: unicode(x)\n\n        v_field = request.values.get('label', 'title')\n        page = int(request.values.get('page') or 1)\n        limit = int(request.values.get('limit') or 10)\n        v_func = _v(label_field)\n        if name:\n            if condition is None:\n                condition = M.c[search_field].like('%' + name + '%')\n            if pagination:\n                query = M.filter(condition)\n                total = query.count()\n                rows = [{'id': getattr(obj, value_field), v_field: v_func(obj)}\n                            for obj in query.limit(limit).offset((page-1)*limit)]\n                result = {'total':total, 'rows':rows}\n            else:\n                result = [{'id': getattr(obj, value_field), v_field: v_func(obj)}\n                      for obj in M.filter(condition)]\n        else:\n            result = []\n        return json(result)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef make_action(app_factory, hostname='localhost', port=5000,\n                threaded=False, processes=1, stream=None,\n                sort_by=('time', 'calls'), restrictions=()):\n    \"\"\"Return a new callback for :mod:`werkzeug.script` that starts a local\n    server with the profiler enabled.\n\n    ::\n\n        from werkzeug.contrib import profiler\n        action_profile = profiler.make_action(make_app)\n    \"\"\"\n    def action(hostname=('h', hostname), port=('p', port),\n               threaded=threaded, processes=processes):\n        \"\"\"Start a new development server.\"\"\"\n        from werkzeug.serving import run_simple\n        app = ProfilerMiddleware(app_factory(), stream, sort_by, restrictions)\n        run_simple(hostname, port, app, False, None, threaded, processes)\n    return action", "response": "Create a callback for a local\n    that starts a development server with the profiler enabled."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef jsonp(data, **json_kwargs):\n    from uliweb import request\n    \n    if 'jsonp' in json_kwargs:\n        cb = json_kwargs.pop('jsonp')\n    else:\n        cb = 'callback'\n        \n    begin = str(request.GET.get(cb))\n    if not begin:\n        raise BadRequest(\"Can't found %s parameter in request's query_string\" % cb)\n    if not r_callback.match(begin):\n        raise BadRequest(\"The callback name is not right, it can be alphabetic, number and underscore only\")\n    \n    if callable(data):\n        @wraps(data)\n        def f(*arg, **kwargs):\n            ret = data(*arg, **kwargs)\n            return Response(begin + '(' + json_dumps(ret) + ');', **json_kwargs)\n        return f\n    else:\n        return Response(begin + '(' + json_dumps(data) + ');', **json_kwargs)", "response": "This function is used to create a jsonp response for a single object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef CORS(func=None):\n\n    def w(r=None):\n        from uliweb import request, response\n\n        if request.method == 'OPTIONS':\n            response = Response(status=204)\n            response.headers['Access-Control-Allow-Credentials'] = 'true'\n            response.headers['Access-Control-Allow-Origin'] = request.headers['Origin']\n            response.headers['Access-Control-Allow-Methods'] = 'GET, POST, PUT, DELETE, OPTIONS'\n            response.headers['Access-Control-Allow-Headers'] = 'DNT,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Range'\n            response.headers['Access-Control-Max-Age'] = 24*3600\n            response.headers['Content-Type'] = 'text/plain; charset=utf-8'\n            response.headers['Content-Length'] = 0\n            return response\n        elif request.method in ('GET', 'POST'):\n            if isinstance(r, Response):\n                response = r\n            response.headers['Access-Control-Allow-Credentials'] = 'true'\n            if 'Origin' in request.headers:\n                response.headers['Access-Control-Allow-Origin'] = request.headers['Origin']\n            response.headers['Access-Control-Allow-Methods'] = 'GET, POST, PUT, DELETE, OPTIONS'\n            response.headers['Access-Control-Allow-Headers'] = 'DNT,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Range'\n            response.headers['Access-Control-Expose-Headers'] = 'Content-Length,Content-Range'\n\n    if callable(func):\n        @wraps(func)\n        def f(*arg, **kwargs):\n            if request.method == 'OPTIONS':\n                return w()\n            ret = func(*arg, **kwargs)\n            w(ret)\n            return ret\n\n        return f\n    else:\n        w()", "response": "A convenience function for handling CORS requests."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_url_adapter(_domain_name):\n    from werkzeug._compat import wsgi_decoding_dance\n\n    domain = application.domains.get(_domain_name, {})\n    server_name = None\n    if domain.get('domain', ''):\n        server_name = domain['domain']\n        try:\n            env = {}\n            environ = request.environ\n            env['url_scheme'] = environ['wsgi.url_scheme']\n            env['default_method'] = environ['REQUEST_METHOD']\n\n            def _get_wsgi_string(name):\n                val = environ.get(name)\n                if val is not None:\n                    return wsgi_decoding_dance(val, \"utf-8\")\n\n            env['script_name'] = _get_wsgi_string('SCRIPT_NAME')\n            env['path_info'] = _get_wsgi_string('PATH_INFO')\n            env['query_args'] = _get_wsgi_string('QUERY_STRING')\n        except:\n            env = {}\n        adapter = url_map.bind(server_name, **env)\n    else:\n        try:\n            env = request.environ\n        except:\n            #this env if for testing only\n            env = {\n                'HTTP_ACCEPT': 'text/html,application/xhtml+xml,application/xml;'\n                               'q=0.9,*/*;q=0.8',\n                'HTTP_ACCEPT_CHARSET': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n                'HTTP_ACCEPT_ENCODING': 'gzip,deflate,sdch',\n                'HTTP_ACCEPT_LANGUAGE': 'uk,en-US;q=0.8,en;q=0.6',\n                'HTTP_CACHE_CONTROL': 'max-age=0',\n                'HTTP_CONNECTION': 'keep-alive',\n                # 'HTTP_HOST': 'localhost:8080',\n                'HTTP_USER_AGENT': 'Mozilla/5.0 (X11; Linux i686)',\n                # 'PATH_INFO': '/',\n                # 'QUERY_STRING': '',\n                'REMOTE_ADDR': '127.0.0.1',\n                'REQUEST_METHOD': 'GET',\n                'REQUEST_URI': '/',\n                'SCRIPT_NAME': '',\n                'SERVER_NAME': 'localhost',\n                'SERVER_PORT': '8080',\n                'SERVER_PROTOCOL': 'HTTP/1.1',\n                'wsgi.errors': None,\n                'wsgi.file_wrapper': None,\n                # 'wsgi.input': BytesIO(ntob('', 'utf-8')),\n                'wsgi.multiprocess': False,\n                'wsgi.multithread': False,\n                'wsgi.run_once': False,\n                'wsgi.url_scheme': 'http',\n                'wsgi.version': (1, 0),\n            }\n        adapter = url_map.bind_to_environ(env)\n    return adapter", "response": "Returns a url_adapter object for the given domain name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting an app s directory", "response": "def get_app_dir(app):\n    \"\"\"\n    Get an app's directory\n    \"\"\"\n    path = __app_dirs__.get(app)\n    if path is not None:\n        return path\n    else:\n        p = app.split('.')\n        try:\n            path = pkg.resource_filename(p[0], '')\n        except ImportError as e:\n            log.error(\"Can't import app %s\" % app)\n            log.exception(e)\n            path = ''\n        if len(p) > 1:\n            path = os.path.join(path, *p[1:])\n        \n        __app_dirs__[app] = path\n        return path"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_file(self, filename, dir='static'):\n        if os.path.exists(filename):\n            return filename\n        dirs = self.apps\n        if dir:\n            fname = os.path.join(dir, filename)\n        else:\n            fname = filename\n        for d in reversed(dirs):\n            path = pkg.resource_filename(d, fname)\n            if os.path.exists(path):\n                return path\n        return None", "response": "get_file will search from apps directory"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nprocessing [ GLOBAL_OBJECTS ] and inject all object to uliweb module so that user can import from uliweb module", "response": "def install_global_objects(self):\n        \"\"\"\n        Process [GLOBAL_OBJECTS], and inject all object to uliweb module, so\n        user can import from uliweb\n        \"\"\"\n        import uliweb\n        for k, v in settings.GLOBAL_OBJECTS.items():\n            setattr(uliweb, k, import_attr(v))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_config(self, config_filename):\n        x = pyini.Ini(lazy=True, basepath=os.path.join(self.project_dir, 'apps'))\n        for p in reversed(self.apps):\n            app_path = get_app_dir(p)\n            filename = os.path.join(app_path, config_filename)\n            if os.path.exists(filename):\n                x.read(filename)\n\n        x.freeze()\n        return x", "response": "Get all config files in all available apps and merge them into a single ini object"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_template_dirs(self):\n        def if_not_empty(dir):\n            if not os.path.exists(dir):\n                return\n            for root, dirs, files in os.walk(dir):\n                if dirs:\n                    return True\n                for f in files:\n                    if f != 'readme.txt':\n                        return True\n                    \n        template_dirs = [os.path.join(self.project_dir, x) for x in settings.GLOBAL.TEMPLATE_DIRS or []]\n        taglibs_dirs = []\n        for p in reversed(self.apps):\n            app_path = get_app_dir(p)\n            path = os.path.join(app_path, 'templates')\n            if if_not_empty(path):\n                template_dirs.append(path)\n\n            path = os.path.join(app_path, 'taglibs')\n            if if_not_empty(path):\n                taglibs_dirs.append(path)\n\n        Dispatcher.template_dirs = template_dirs\n        Dispatcher.taglibs_dirs = taglibs_dirs", "response": "Get templates directory from apps but in reversed order so the same named template directories will be overrided by latter defined app\n           "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_redis(**options):\n    from uliweb import settings\n    from uliweb.utils.common import log\n    import redis\n        \n    options = (options or {})\n    options.update(settings.REDIS)\n    if 'unix_socket_path' in options:\n        client = redis.Redis(unix_socket_path=options['unix_socket_path'])\n    else:\n        global __connection_pool__\n    \n        if not __connection_pool__ or __connection_pool__[0] != options['connection_pool']:\n            d = {'host':'localhost', 'port':6379}\n            d.update(options['connection_pool'])\n            __connection_pool__ = (d, redis.ConnectionPool(**d))\n        client = redis.Redis(connection_pool=__connection_pool__[1])\n        \n    if settings.REDIS.test_first:\n        try:\n            client.info()\n        except Exception as e:\n            log.exception(e)\n            client = None\n    return client", "response": "if no options defined, then it'll use settings options\n    \n    #unix_socket_path = '/tmp/redis.sock'\n    connection_pool = {'host':'localhost', 'port':6379}\n    #if test after created redis client object\n    test_first = False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_lock(key, value=None, expiry_time=60):\n    from uliweb.utils.common import get_uuid\n\n    redis = get_redis()\n    value = value or get_uuid()\n    return redis.set(key, value, ex=expiry_time, nx=True)", "response": "Get a distribute lock"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nforcing to set a distribute lock", "response": "def set_lock(key, value=None, expiry_time=60):\n    \"\"\"Force to set a distribute lock\"\"\"\n    from uliweb.utils.common import get_uuid\n\n    redis = get_redis()\n    value = value or get_uuid()\n    return redis.set(key, value, ex=expiry_time, xx=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks redis version and check redis version", "response": "def after_init_apps(sender):\n    \"\"\"\n    Check redis version\n    \"\"\"\n    from uliweb import settings\n    from uliweb.utils.common import log\n    \n    check = settings.get_var('REDIS/check_version')\n    if check:\n        client = get_redis()\n        try:\n            info = client.info()\n        except Exception as e:\n            log.exception(e)\n            log.error('Redis is not started!')\n            return\n        \n        redis_version = info['redis_version']\n        version = tuple(map(int, redis_version.split('.')))\n        op = re_compare_op.search(check)\n        if op:\n            _op = op.group()\n            _v = check[op.end()+1:].strip()\n        else:\n            _op = '='\n            _v = check\n        nv = tuple(map(int, _v.split('.')))\n        if _op == '=':\n            flag = version[:len(nv)] == nv\n        elif _op == '>=':\n            flag = version >= nv\n        elif _op == '>':\n            flag = version > nv\n        elif _op == '<=':\n            flag = version <= nv\n        elif _op == '<':\n            flag = version < nv\n        else:\n            log.error(\"Can't support operator %s when check redis version\" % _op)\n        if not flag:\n            log.error(\"Redis version %s is not matched what you want %s\" % (redis_version, _v))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_accept_lang_header(lang_string):\r\n    result = []\r\n    pieces = accept_language_re.split(lang_string)\r\n    if pieces[-1]:\r\n        return []\r\n    for i in range(0, len(pieces) - 1, 3):\r\n        first, lang, priority = pieces[i : i + 3]\r\n        if first:\r\n            return []\r\n        priority = priority and float(priority) or 1.0\r\n        result.append((lang, priority))\r\n    result.sort(lambda x, y: -cmp(x[1], y[1]))\r\n    return result", "response": "Parses the Accept - Language header and returns a list of tuples ordered by q - value."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fix_filename(filename, suffix=''):\n    if suffix:\n        f, ext = os.path.splitext(filename)\n        return f+suffix+ext\n    else:\n        return filename", "response": "fix filename to be _40x40. png"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _make_text_block(name, content, content_type=None):\n    if content_type == 'xhtml':\n        return u'<%s type=\"xhtml\"><div xmlns=\"%s\">%s</div></%s>\\n' % \\\n               (name, XHTML_NAMESPACE, content, name)\n    if not content_type:\n        return u'<%s>%s</%s>\\n' % (name, escape(content), name)\n    return u'<%s type=\"%s\">%s</%s>\\n' % (name, content_type,\n                                         escape(content), name)", "response": "Helper function for the builder that creates an XML text block."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _style_range(self, cell, cell_range, border=None, fill=None, font=None, alignment=None):\n\n        from openpyxl.styles import Border, Side\n\n        top = left = right = bottom = Side(border_style='thin', color=self.border_color)\n\n        def border_add(border, top=None, right=None, left=None, bottom=None):\n            top = top or border.top\n            left = left or border.left\n            right = right or border.right\n            bottom = bottom or border.bottom\n            return Border(top=top, left=left, right=right, bottom=bottom)\n\n        cell.alignment = alignment\n        cell.fill = fill\n\n        rows = list(self.sheet[cell_range])\n\n        for cell in rows[0]:\n            cell.border = border_add(cell.border, top=top)\n        for cell in rows[-1]:\n            cell.border = border_add(cell.border, bottom=bottom)\n\n        for row in rows:\n            l = row[0]\n            r = row[-1]\n            l.border = border_add(l.border, left=left)\n            r.border = border_add(r.border, right=right)", "response": "Apply styles to a range of cells as if they were a single cell."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_template(self):\n        rows = []\n        stack = []\n        stack.append(rows)\n        #top\u7528\u6765\u8bb0\u5f55\u5f53\u524d\u6808\n        top = rows\n        for i in range(1, self.sheet.max_row+1):\n            cell = self.sheet.cell(row=i, column=1)\n            #\u662f\u5426\u5b50\u6a21\u677f\u5f00\u59cb\n            if (isinstance(cell.value, (str, unicode)) and\n                    cell.value.startswith('{{for ') and\n                    cell.value.endswith('}}')):\n                row = {'field':cell.value[6:-2].strip(), 'cols':[], 'subs':[]}\n                top.append(row)\n                top = row['subs']\n                stack.append(top)\n                if self.begin == 1:\n                    self.begin = i\n            #\u662f\u5426\u5b50\u6a21\u677f\u7ed3\u675f\n            elif (isinstance(cell.value, (str, unicode)) and\n                    cell.value == '{{end}}'):\n                stack.pop()\n                top = stack[-1]\n            else:\n                row = {'cols':[], 'subs':[]}\n                cols = row['cols']\n                for j in range(1, self.sheet.max_column+1):\n                    cell = self.sheet.cell(row=i, column=j)\n                    v = self.process_cell(i, j, cell)\n                    if v:\n                        cols.append(v)\n                if row['cols'] or row['subs']:\n                    top.append(row)\n\n\n        # pprint(rows)\n        return rows", "response": "get_template - returns a list of template objects"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_cell(self, cell):\n        field = ''\n        if (isinstance(cell.value, (str, unicode)) and\n            cell.value.startswith('{{') and\n            cell.value.endswith('}}')):\n            field = cell.value[2:-2].strip()\n            value = ''\n        else:\n            value = cell.value\n\n        return value, field", "response": "Process the cell field the field format just like the field"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprocess a single cell.", "response": "def process_cell(self, row, column, cell):\n        \"\"\"\n        \u5bf9\u4e8e\u8bfb\u6a21\u677f\uff0c\u53ea\u8bb0\u5f55\u683c\u5f0f\u4e3a {{xxx}} \u7684\u5b57\u6bb5\n        :param cell:\n        :return: \u5982\u679c\u4e0d\u662f\u7b2c\u4e00\u884c\uff0c\u5219\u6bcf\u5217\u8fd4\u56de{'col':\u5217\u503c, 'field'}\uff0c\u5426\u5219\u53ea\u8fd4\u56de {'value':...}\n        \"\"\"\n        value, field = self.parse_cell(cell)\n\n        if value or field:\n            return {'col':column, 'field':field, 'value':value}\n        else:\n            return {}"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmatching the current row with the template.", "response": "def match(self, row, template_row=None):\n        \"\"\"\n        \u5339\u914d\u4e00\u4e2a\u6a21\u677f\u65f6\uff0c\u53ea\u6bd4\u8f83\u8d77\u59cb\u884c\uff0c\u672a\u6765\u8003\u8651\u652f\u6301\u6bd4\u8f83\u5173\u952e\u5b57\u6bb5\u5373\u53ef\uff0c\u73b0\u5728\u662f\u8d77\u59cb\u884c\u7684\u6240\u6709\u5b57\u6bb5\u5168\u5339\u914d\n        :param row:\n        :return:\n        \"\"\"\n        if not template_row:\n            template_cols = self.template[0]['cols']\n        else:\n            template_cols = template_row['cols']\n\n        #check if length of template_cols great than row, then match failed\n        if len(template_cols)>len(row):\n            return False\n\n        for c in template_cols:\n            #\u5982\u679c\u5750\u6807\u5927\u4e8e\u6a21\u677f\u6700\u5927\u957f\u5ea6\uff0c\u5219\u8868\u793a\u4e0d\u5339\u914d\uff0c\u5219\u9000\u51fa\n            text = c['value']\n            if not text or (text and not (text.startswith('{{') and text.endswith('}}'))\n                and row[c['col']-1].value == text):\n                pass\n            else:\n                return False\n        # print 'Matched'\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nextract data from sheet", "response": "def get_data(self, sheet, find=False, begin=1):\n        \"\"\"\n        Extract data from sheet\n        :param find: \u67e5\u627e\u6a21\u5f0f.\u7f3a\u7701\u4e3aFalse.\u4e3aTrue\u65f6,\u5c06\u8fdb\u884c\u9012\u5f52\u67e5\u627e\n        :param begin: \u5f00\u59cb\u884c\u53f7\n        \"\"\"\n        line = begin\n        for row in sheet.rows:\n            if self.match(row):\n                d = self.extract_data(sheet, line)\n                return d\n            else:\n                if find:\n                    line += 1\n                else:\n                    break\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read_data(self, sheet, begin=None):\n        line = self.begin\n        rows = sheet.rows\n        for i in range(line-1):\n            rows.next()\n        template_line = self.template[self.begin-1]\n        if not template_line['subs']:\n            raise ValueError(\"Template definition is not right\")\n\n        for row in rows:\n            d = {}\n            for c in template_line['subs'][0]['cols']:\n                if c['field']:\n                    cell = row[c['col']-1]\n                    if isinstance(cell.value, (str, unicode)):\n                        v = cell.value.strip()\n                    else:\n                        v = cell.value\n                    d[c['field']] = v\n            yield d", "response": "read_data - \u53ea\u5904\u7406\u4e00\u884c\u7684\u6a21\u677f yield all data in the sheet"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef read(self, template, find=False, begin=1):\n        result = []\n        self.matched = False\n        for sheet in self.get_sheet():\n            r = template.get_data(sheet, find=find, begin=begin)\n            if r is not None: #not matched\n                self.matched = True\n                if self.merge:\n                    self.merge.add(r)\n                    result = self.merge.result\n                else:\n                    result.extend(r)\n        return result", "response": "read - Read the data from the template"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef read(self):\n        for sheet in self.get_sheet():\n            for row in self.template.read_data(sheet, begin=self.begin):\n                yield row", "response": "read all the data in the current page"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets a key from the cache", "response": "def get(self, key, default=Empty, creator=Empty, expire=None):\n        \"\"\"\n        :para default: if default is callable then invoke it, save it and return it\n        \"\"\"\n        try:\n            return self.storage.get(key)\n        except KeyError as e:\n            if creator is not Empty:\n                if callable(creator):\n                    v = creator()\n                else:\n                    v = creator\n                self.set(key, v, expire)\n                return v\n            else:\n                if default is not Empty:\n                    if callable(default):\n                        v = default()\n                        return v\n                    return default\n                else:\n                    raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run_migrations_online():\n    from uliweb.manage import make_simple_application\n    from uliweb import orm, settings\n\n#    engine = engine_from_config(\n#                config.get_section(config.config_ini_section), \n#                prefix='sqlalchemy.', \n#                poolclass=pool.NullPool)\n\n    name = config.get_main_option(\"engine_name\")\n    make_simple_application(project_dir='.')\n    target_metadata = orm.get_metadata(name)\n    connection = orm.get_connection(name).connect()\n#    connection = engine.connect()\n    \n    context.configure(\n                connection=connection, \n                target_metadata=target_metadata,\n                compare_server_default=True,\n                include_object=uliweb_include_object,\n#                compare_server_default=uliweb_compare_server_default,\n                )\n\n    try:\n        with context.begin_transaction():\n            context.run_migrations()\n    finally:\n        connection.close()", "response": "Run migrations in online mode."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating an application object", "response": "def make_application(debug=None, apps_dir='apps', project_dir=None,\r\n    include_apps=None, debug_console=True, settings_file=None,\r\n    local_settings_file=None, start=True, default_settings=None,\r\n    dispatcher_cls=None, dispatcher_kwargs=None, debug_cls=None, debug_kwargs=None,\r\n    reuse=True, verbose=False, pythonpath=None, trace_print=False):\r\n    \"\"\"\r\n    Make an application object\r\n    \"\"\"\r\n    from uliweb.utils.common import import_attr\r\n    from uliweb.utils.whocallme import print_frame\r\n    from werkzeug.debug import DebuggedApplication\r\n\r\n    #is reuse, then create application only one\r\n    if reuse and hasattr(SimpleFrame.__global__, 'application') and SimpleFrame.__global__.application:\r\n        return SimpleFrame.__global__.application\r\n\r\n    #process settings and local_settings\r\n    settings_file = settings_file or os.environ.get('SETTINGS', 'settings.ini')\r\n    local_settings_file = local_settings_file or os.environ.get('LOCAL_SETTINGS', 'local_settings.ini')\r\n\r\n    dispatcher_cls = dispatcher_cls or SimpleFrame.Dispatcher\r\n    dispatcher_kwargs = dispatcher_kwargs or {}\r\n\r\n    if project_dir:\r\n        apps_dir = os.path.abspath(os.path.normpath(os.path.join(project_dir, 'apps')))\r\n    if not project_dir:\r\n        project_dir = os.path.abspath(os.path.normpath(os.path.abspath(os.path.join(apps_dir, '..'))))\r\n\r\n    if pythonpath:\r\n        if isinstance(pythonpath, str):\r\n            pythonpath = pythonpath.split(';')\r\n        for x in pythonpath:\r\n            if x not in sys.path:\r\n                sys.path.insert(0, x)\r\n\r\n    if project_dir not in sys.path:\r\n        sys.path.insert(0, project_dir)\r\n\r\n    if apps_dir not in sys.path:\r\n        sys.path.insert(0, apps_dir)\r\n\r\n    install_config(apps_dir)\r\n\r\n    if trace_print:\r\n        output = sys.stdout\r\n\r\n        class MyOut(object):\r\n            def write(self, s):\r\n                output.write(s)\r\n                output.write('\\n')\r\n                print_frame(output)\r\n                if hasattr(output, 'flush'):\r\n                    output.flush()\r\n\r\n        sys.stdout = MyOut()\r\n\r\n    application = app = dispatcher_cls(apps_dir=apps_dir,\r\n        include_apps=include_apps,\r\n        settings_file=settings_file,\r\n        local_settings_file=local_settings_file,\r\n        start=start,\r\n        default_settings=default_settings,\r\n        reset=True,\r\n        **dispatcher_kwargs)\r\n\r\n    if verbose:\r\n        log.info(' * settings file is \"%s\"' % settings_file)\r\n        log.info(' * local settings file is \"%s\"' % local_settings_file)\r\n\r\n    #settings global application object\r\n    SimpleFrame.__global__.application = app\r\n\r\n    #process wsgi middlewares\r\n    middlewares = []\r\n    parameters = {}\r\n    for name, v in uliweb.settings.get('WSGI_MIDDLEWARES', {}).iteritems():\r\n        order, kwargs = 500, {}\r\n        if not v:\r\n            continue\r\n        if isinstance(v, (list, tuple)):\r\n            if len(v) > 3:\r\n                logging.error('WSGI_MIDDLEWARE %s difinition is not right' % name)\r\n                raise uliweb.UliwebError('WSGI_MIDDLEWARE %s difinition is not right' % name)\r\n            cls = v[0]\r\n            if len(v) == 2:\r\n                if isinstance(v[1], int):\r\n                    order = v[1]\r\n                else:\r\n                    kwargs = v[1]\r\n            else:\r\n                order, kwargs = v[1], v[2]\r\n        else:\r\n            cls = v\r\n        middlewares.append((order, name))\r\n        parameters[name] = cls, kwargs\r\n\r\n    middlewares.sort(cmp=lambda x, y: cmp(x[0], y[0]))\r\n    for name in reversed([x[1] for x in middlewares]):\r\n        clspath, kwargs = parameters[name]\r\n        cls = import_attr(clspath)\r\n        app = cls(app, **kwargs)\r\n\r\n    debug_flag = uliweb.settings.GLOBAL.DEBUG\r\n    if debug or (debug is None and debug_flag):\r\n        if not debug_cls:\r\n            debug_cls = DebuggedApplication\r\n\r\n        log.setLevel(logging.DEBUG)\r\n        log.info(' * Loading DebuggedApplication...')\r\n        app.debug = True\r\n        app = debug_cls(app, uliweb.settings.GLOBAL.get('DEBUG_CONSOLE', False))\r\n    return app"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfinds a template and display it if it exists.", "response": "def _find_template(self, template, tree, blocks, with_filename,\r\n                   source, comment):\r\n        \"\"\"\r\n        If tree is true, then will display the track of template extend or include\r\n        \"\"\"\r\n        from uliweb import application\r\n        from uliweb.core.template import _format_code\r\n\r\n        def get_rel_filename(filename, path):\r\n            f1 = os.path.splitdrive(filename)[1]\r\n            f2 = os.path.splitdrive(path)[1]\r\n            f = os.path.relpath(f1, f2).replace('\\\\', '/')\r\n            if f.startswith('..'):\r\n                return filename.replace('\\\\', '/')\r\n            else:\r\n                return f\r\n\r\n        template_file = None\r\n\r\n        if not tree:\r\n            application.template_loader.comment = comment\r\n            files = application.template_loader.find_templates(template)\r\n            if files:\r\n                template_file = files[0]\r\n\r\n                for x in files:\r\n                    print x\r\n\r\n                if source:\r\n                    print\r\n                    print '---------------- source of %s ---------------' % template\r\n                    t = application.template_loader.load(template_file)\r\n                    if t and comment:\r\n                        print _format_code(t.code).rstrip()\r\n                        print\r\n                    else:\r\n                        print t.code\r\n                        print\r\n\r\n            else:\r\n                print 'Not Found'\r\n        else:\r\n            application.template_loader.print_tree(template)\r\n\r\n        if template_file and blocks:\r\n            application.template_loader.print_blocks(template, with_filename)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nvalidating the list of files and return a list of files.", "response": "def _validate_templates(self, app, files, verbose):\r\n        \"\"\"\r\n        If tree is true, then will display the track of template extend or include\r\n        \"\"\"\r\n        from uliweb import application\r\n        from uliweb.core.template import template_file\r\n        from uliweb.utils.common import trim_path\r\n\r\n        app.template_loader.log = None\r\n        for f in files:\r\n            try:\r\n                t = app.template_loader.load(f)\r\n                if verbose:\r\n                    print 'PASSED', f\r\n            except Exception as e:\r\n                print 'FAILED', f, str(e)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef url_parse(url, scheme=None, allow_fragments=True):\n    s = make_literal_wrapper(url)\n    is_text_based = isinstance(url, text_type)\n\n    if scheme is None:\n        scheme = s('')\n    netloc = query = fragment = s('')\n    i = url.find(s(':'))\n    if i > 0 and _scheme_re.match(to_native(url[:i], errors='replace')):\n        # make sure \"iri\" is not actually a port number (in which case\n        # \"scheme\" is really part of the path)\n        rest = url[i + 1:]\n        if not rest or any(c not in s('0123456789') for c in rest):\n            # not a port number\n            scheme, url = url[:i].lower(), rest\n\n    if url[:2] == s('//'):\n        delim = len(url)\n        for c in s('/?#'):\n            wdelim = url.find(c, 2)\n            if wdelim >= 0:\n                delim = min(delim, wdelim)\n        netloc, url = url[2:delim], url[delim:]\n        if ((s('[') in netloc and s(']') not in netloc) or\n            (s(']') in netloc and s('[') not in netloc)):\n            raise ValueError('Invalid IPv6 URL')\n\n    if allow_fragments and s('#') in url:\n        url, fragment = url.split(s('#'), 1)\n    if s('?') in url:\n        url, query = url.split(s('?'), 1)\n\n    result_type = is_text_based and URL or BytesURL\n    return result_type(scheme, netloc, url, query, fragment)", "response": "Parses a URL into a tuple."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef url_quote(string, charset='utf-8', errors='strict', safe='/:'):\n    if not isinstance(string, (text_type, bytes, bytearray)):\n        string = text_type(string)\n    if isinstance(string, text_type):\n        string = string.encode(charset, errors)\n    if isinstance(safe, text_type):\n        safe = safe.encode(charset, errors)\n    safe = frozenset(bytearray(safe) + _always_safe)\n    rv = bytearray()\n    for char in bytearray(string):\n        if char in safe:\n            rv.append(char)\n        else:\n            rv.extend(('%%%02X' % char).encode('ascii'))\n    return to_native(bytes(rv))", "response": "URL encode a single string with a given encoding."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef url_quote_plus(string, charset='utf-8', errors='strict', safe=''):\n    return url_quote(string, charset, errors, safe + ' ').replace(' ', '+')", "response": "URL encode a single string with the given encoding and convert\n    whitespace to +."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef url_unparse(components):\n    scheme, netloc, path, query, fragment = \\\n        normalize_string_tuple(components)\n    s = make_literal_wrapper(scheme)\n    url = s('')\n\n    # We generally treat file:///x and file:/x the same which is also\n    # what browsers seem to do.  This also allows us to ignore a schema\n    # register for netloc utilization or having to differenciate between\n    # empty and missing netloc.\n    if netloc or (scheme and path.startswith(s('/'))):\n        if path and path[:1] != s('/'):\n            path = s('/') + path\n        url = s('//') + (netloc or s('')) + path\n    elif path:\n        url += path\n    if scheme:\n        url = scheme + s(':') + url\n    if query:\n        url = url + s('?') + query\n    if fragment:\n        url = url + s('#') + fragment\n    return url", "response": "This method is used to convert arbitrary URL components into a URL string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef url_unquote(string, charset='utf-8', errors='replace', unsafe=''):\n    rv = _unquote_to_bytes(string, unsafe)\n    if charset is not None:\n        rv = rv.decode(charset, errors)\n    return rv", "response": "URL decode a single string with a given encoding."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfixing the URL of a string.", "response": "def url_fix(s, charset='utf-8'):\n    r\"\"\"Sometimes you get an URL by a user that just isn't a real URL because\n    it contains unsafe characters like ' ' and so on. This function can fix\n    some of the problems in a similar way browsers handle data entered by the\n    user:\n\n    >>> url_fix(u'http://de.wikipedia.org/wiki/Elf (Begriffskl\\xe4rung)')\n    'http://de.wikipedia.org/wiki/Elf%20(Begriffskl%C3%A4rung)'\n\n    :param s: the string with the URL to fix.\n    :param charset: The target charset for the URL if the url was given as\n                    unicode string.\n    \"\"\"\n    scheme, netloc, path, qs, anchor = url_parse(to_unicode(s, charset, 'replace'))\n    path = url_quote(path, charset, safe='/%+$!*\\'(),')\n    qs = url_quote_plus(qs, charset, safe=':&%=+$!*\\'(),')\n    return to_native(url_unparse((scheme, netloc, path, qs, anchor)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef uri_to_iri(uri, charset='utf-8', errors='replace'):\n    if isinstance(uri, tuple):\n        uri = url_unparse(uri)\n    uri = url_parse(to_unicode(uri, charset))\n    path = url_unquote(uri.path, charset, errors, '/;?')\n    query = url_unquote(uri.query, charset, errors, ';/?:@&=+,$')\n    fragment = url_unquote(uri.fragment, charset, errors, ';/?:@&=+,$')\n    return url_unparse((uri.scheme, uri.decode_netloc(),\n                        path, query, fragment))", "response": "r Converts a URI in a given charset to a IRI."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef iri_to_uri(iri, charset='utf-8', errors='strict'):\n    if isinstance(iri, tuple):\n        iri = url_unparse(iri)\n    iri = url_parse(to_unicode(iri, charset, errors))\n\n    netloc = iri.encode_netloc().decode('ascii')\n    path = url_quote(iri.path, charset, errors, '/:~+%')\n    query = url_quote(iri.query, charset, errors, '%&[]:;$*()+,!?*/=')\n    fragment = url_quote(iri.fragment, charset, errors, '=%&[]:;$()+,!?*/')\n\n    return to_native(url_unparse((iri.scheme, netloc,\n                                  path, query, fragment)))", "response": "r Converts any unicode based IRI to an acceptable ASCII URI."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef url_decode(s, charset='utf-8', decode_keys=False, include_empty=True,\n               errors='replace', separator='&', cls=None):\n    \"\"\"\n    Parse a querystring and return it as :class:`MultiDict`.  There is a\n    difference in key decoding on different Python versions.  On Python 3\n    keys will always be fully decoded whereas on Python 2, keys will\n    remain bytestrings if they fit into ASCII.  On 2.x keys can be forced\n    to be unicode by setting `decode_keys` to `True`.\n\n    If the charset is set to `None` no unicode decoding will happen and\n    raw bytes will be returned.\n\n    Per default a missing value for a key will default to an empty key.  If\n    you don't want that behavior you can set `include_empty` to `False`.\n\n    Per default encoding errors are ignored.  If you want a different behavior\n    you can set `errors` to ``'replace'`` or ``'strict'``.  In strict mode a\n    `HTTPUnicodeError` is raised.\n\n    .. versionchanged:: 0.5\n       In previous versions \";\" and \"&\" could be used for url decoding.\n       This changed in 0.5 where only \"&\" is supported.  If you want to\n       use \";\" instead a different `separator` can be provided.\n\n       The `cls` parameter was added.\n\n    :param s: a string with the query string to decode.\n    :param charset: the charset of the query string.  If set to `None`\n                    no unicode decoding will take place.\n    :param decode_keys: Used on Python 2.x to control whether keys should\n                        be forced to be unicode objects.  If set to `True`\n                        then keys will be unicode in all cases. Otherwise,\n                        they remain `str` if they fit into ASCII.\n    :param include_empty: Set to `False` if you don't want empty values to\n                          appear in the dict.\n    :param errors: the decoding error behavior.\n    :param separator: the pair separator to be used, defaults to ``&``\n    :param cls: an optional dict class to use.  If this is not specified\n                       or `None` the default :class:`MultiDict` is used.\n    \"\"\"\n    if cls is None:\n        cls = MultiDict\n    if isinstance(s, text_type) and not isinstance(separator, text_type):\n        separator = separator.decode(charset or 'ascii')\n    elif isinstance(s, bytes) and not isinstance(separator, bytes):\n        separator = separator.encode(charset or 'ascii')\n    return cls(_url_decode_impl(s.split(separator), charset, decode_keys,\n                                include_empty, errors))", "response": "URL decode a string into a MultiDict."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndecode the netloc part into a string.", "response": "def decode_netloc(self):\n        \"\"\"Decodes the netloc part into a string.\"\"\"\n        rv = _decode_idna(self.host or '')\n\n        if ':' in rv:\n            rv = '[%s]' % rv\n        port = self.port\n        if port is not None:\n            rv = '%s:%d' % (rv, port)\n        auth = ':'.join(filter(None, [\n            _url_unquote_legacy(self.raw_username or '', '/:%@'),\n            _url_unquote_legacy(self.raw_password or '', '/:%@'),\n        ]))\n        if auth:\n            rv = '%s@%s' % (auth, rv)\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndecode the URL to a tuple made out of strings.", "response": "def decode(self, charset='utf-8', errors='replace'):\n        \"\"\"Decodes the URL to a tuple made out of strings.  The charset is\n        only being used for the path, query and fragment.\n        \"\"\"\n        return URL(\n            self.scheme.decode('ascii'),\n            self.decode_netloc(),\n            self.path.decode(charset, errors),\n            self.query.decode(charset, errors),\n            self.fragment.decode(charset, errors)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cal_position(self, text, has_toplinks, has_bottomlinks, head_len, head_start):\r\n        if head_len == 0:\r\n            top_start = top_end = bottom_start = bottom_end = 0\r\n        else:\r\n            top_start = top_end = head_start + 6\r\n            bottom_start = bottom_end = head_start + head_len - 7\r\n            \r\n        if has_toplinks:\r\n            t = r_top.search(text)\r\n            if t:\r\n                top_start, top_end = t.span()\r\n        \r\n        if has_bottomlinks:\r\n            t = r_bottom.search(text)\r\n            if t:\r\n                bottom_start, bottom_end = t.span()\r\n\r\n        return top_end, bottom_end", "response": "Calculate the position of the toplinks and bottomlinks of a list of\rCOOKIES."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a relation to the list of related resources.", "response": "def add(self, relation):\n        \"\"\"\n        relation is a string list, just like:\n        ['User.id = Group.user', 'User.username = Group.username']\n        :param relation:\n        :return:\n        \"\"\"\n        r = Relation(relation)\n        key = r.relation_key[0]\n        if key not in self.relations:\n            self.relations[key] = r\n            self.relations[r.relation_key[1]]= r"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_entry(self, name):\n\n        # calculate schema mode\n        # if ':name' or '' or '[]:name' or '[]' found, it'll be treat as multiple Schema query\n        alias = name\n        if ':' in name:\n            name, alias = name.split(':')\n        if name.endswith('[]'):\n            need_list = True\n            name = name[:-2]\n        else:\n            need_list = False\n        return alias, name, need_list", "response": "Parse a query entry name just like a user name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef query_schema(self, name, param):\n\n        alias, name, need_list = self.parse_entry(name)\n\n        if not name:\n            result = self.process_multiple_query(need_list, param)\n        else:\n            result = self.process_single_query(name, need_list, param)\n        return alias, result", "response": "Query the schema for a specific entry."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse condition for a specific column.", "response": "def parse_condition(self, schema, name, v):\n        \"\"\"\n        Parse name = 'value' to condition\n        :param name: column name\n        :param schema: schema name\n        :param v: column value\n        :return:\n        \"\"\"\n\n        S = schema\n        col = S.get_column(name)\n        condition = None\n\n        if col is not None:  # can create condition\n            if isinstance(v, (str, unicode)):\n                if v.startswith('>='):\n                    condition = (col >= eval(v[2:].strip()))\n                elif v.startswith('>'):\n                    condition = (col > eval(v[1:].strip()))\n                elif v.startswith('<='):\n                    condition = (col <= eval(v[2:].strip()))\n                elif v.startswith('<'):\n                    condition = (col < eval(v[1:].strip()))\n                elif v.startswith('='):\n                    condition = (col == eval(v[1:].strip()))\n                elif v.startswith('!='):\n                    condition = (col != eval(v[2:].strip()))\n                elif v.startswith('like'):\n                    condition = col.like(v[4:].strip())\n                elif v.startswith('between'):\n                    _v = eval(v[7:].strip())\n                    if not isinstance(_v, (tuple, list)):\n                        raise ValueError(\"Between operation should be a list, but {!r} found\".format(v))\n                    condition = (col.between(*_v))\n                elif v.startswith('in'):\n                    condition = (col.in_(eval(v[2:].strip())))\n                else:\n                    if '%' in v:  # like\n                        condition = col.like(v)\n                    else:\n                        condition = (col == v)\n            elif isinstance(v, (tuple, list)):\n                condition = (col.in_(v))\n            else:\n                condition = (col == v)\n        return condition"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses schema parameter. It ll return a dictionary of all the keys that are defined in the parameter.", "response": "def parse_param(self, name, param):\n        \"\"\"\n        Parse schema parameter, it'll return\n        {\n            condition\n            columns\n            limit\n            order_by\n            group_by\n            total\n            page\n            table\n            name #schema name\n        }\n        :param name: schema name\n        :param param: schema query parameter\n        :return: dict\n        \"\"\"\n        S = get_schema(name)\n\n        # prepare condition\n        condition = true()\n        fields = []\n        columns = []\n        columns_param = {}\n        limit = __default_limit__\n        order_by = []\n        group_by = []\n        total = None\n        page = 0\n        table = S.__table__\n        relation = None\n\n        for k, v in param.items():\n            if k.startswith('@'):\n                if k == '@columns':\n                    fields = v[:]\n                elif k == '@limit':\n                    limit = v\n                elif k == '@page':\n                    page = v\n                elif k == '@order_by':\n                    if isinstance(v, (str, unicode)):\n                        orders = v.split(',')\n                    else:\n                        orders = v\n                    for c in orders:\n                        if '.' in c:\n                            col_name, dir = c.split('.')\n                        else:\n                            col_name = c\n                            dir = 'asc'\n                        col = S.get_column(col_name)\n                        if dir == 'desc':\n                            order_by.append(col.desc())\n                        else:\n                            order_by.append(col)\n                elif k == '@group_by':\n                    if isinstance(v, (str, unicode)):\n                        groups = v.split(',')\n                    else:\n                        groups = v\n                    for c in groups:\n                        col = S.get_column(c)\n                        group_by.append(col)\n                elif k == '@total':\n                    total = v\n                elif k == '@relation':\n                    relation_key = name, v\n                    relation = get_relation_condition(relation_key)\n            elif k.startswith('$'):  # condition\n                c = self.parse_condition(S, k[1:], v)\n                if c is not None:\n                    condition = c & condition\n            elif isinstance(v, dict):  # guest schema\n                # todo nested schema\n                # if there is not one row, it'll using left join otherwise using standalone\n                # query\n                nested_alias, nested_name, nested_need_list = self.parse_entry(k)\n                nested_config = self.parse_param(nested_name, value)\n                if nested_need_list:\n                    # insert resolve function\n                    pass\n                else:\n                    relation = name, nested_config.name\n                    outerjoin_condition = get_relation_condition(relation)\n                    if outerjoin_condition is None:\n                        raise RelationError(\"Relation between {!r} can not be found\".format(relation))\n                    table.outerjoin(nested_config.table, outerjoin_condition)\n                    condition = nested_config.condition & condition\n                    columns.extend(nested_config.columns)\n\n            else:\n                # columns\n                if k not in fields:\n                    fields.append(k)\n\n        columns.extend([S.get_column(x) for x in fields or S._fields_list])  # used for select\n\n        config = Storage({})\n        config.table = table\n        config.condition = condition\n        config.columns = columns\n        config.columns_param = columns_param\n        config.total = total\n        config.limit = limit\n        config.page = page\n        config.order_by = order_by\n        config.group_by = group_by\n        config.name = name\n        config.schema = S\n        config.relation = relation\n\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nescapes a string so it is valid within HTML or XML.", "response": "def xhtml_escape(value):\n    \"\"\"Escapes a string so it is valid within HTML or XML.\n\n    Escapes the characters ``<``, ``>``, ``\"``, ``'``, and ``&``.\n    When used in attribute values the escaped strings must be enclosed\n    in quotes.\n\n    .. versionchanged:: 3.2\n\n       Added the single quote to the list of escaped characters.\n    \"\"\"\n    return _XHTML_ESCAPE_RE.sub(lambda match: _XHTML_ESCAPE_DICT[match.group(0)],\n                                to_basestring(value))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef url_escape(value, plus=True):\n    quote = urllib_parse.quote_plus if plus else urllib_parse.quote\n    return quote(utf8(value))", "response": "Returns a URL - encoded version of the given value."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert a string argument to a byte string.", "response": "def utf8(value):\n    \"\"\"Converts a string argument to a byte string.\n\n    If the argument is already a byte string or None, it is returned unchanged.\n    Otherwise it must be a unicode string and is encoded as utf8.\n    \"\"\"\n    if isinstance(value, _UTF8_TYPES):\n        return value\n    if not isinstance(value, unicode_type):\n        raise TypeError(\n            \"Expected bytes, unicode, or None; got %r\" % type(value)\n        )\n    return value.encode(\"utf-8\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_unicode(value):\n    if isinstance(value, _TO_UNICODE_TYPES):\n        return value\n    if not isinstance(value, bytes_type):\n        raise TypeError(\n            \"Expected bytes, unicode, or None; got %r\" % type(value)\n        )\n    return value.decode(\"utf-8\")", "response": "Converts a string argument to a unicode string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting a string argument to a subclass of basestring.", "response": "def to_basestring(value):\n    \"\"\"Converts a string argument to a subclass of basestring.\n\n    In python2, byte and unicode strings are mostly interchangeable,\n    so functions that deal with a user-supplied argument in combination\n    with ascii string constants can use either and should return the type\n    the user supplied.  In python3, the two types are not interchangeable,\n    so this method is needed to convert byte strings to unicode.\n    \"\"\"\n    if isinstance(value, _BASESTRING_TYPES):\n        return value\n    if not isinstance(value, bytes_type):\n        raise TypeError(\n            \"Expected bytes, unicode, or None; got %r\" % type(value)\n        )\n    return value.decode(\"utf-8\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef recursive_unicode(obj):\n    if isinstance(obj, dict):\n        return dict((recursive_unicode(k), recursive_unicode(v)) for (k, v) in obj.items())\n    elif isinstance(obj, list):\n        return list(recursive_unicode(i) for i in obj)\n    elif isinstance(obj, tuple):\n        return tuple(recursive_unicode(i) for i in obj)\n    elif isinstance(obj, bytes_type):\n        return to_unicode(obj)\n    else:\n        return obj", "response": "Walks a simple data structure converting byte strings to unicode."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _mixed_join(iterable, sentinel):\n    iterator = iter(iterable)\n    first_item = next(iterator, sentinel)\n    if isinstance(first_item, bytes):\n        return first_item + b''.join(iterator)\n    return first_item + u''.join(iterator)", "response": "concatenate any string type in an intelligent way."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreplace string directly without appending to an empty string.", "response": "def _buf_append(self, string):\n        '''Replace string directly without appending to an empty string,\n        avoiding type issues.'''\n        if not self._buf:\n            self._buf = string\n        else:\n            self._buf += string"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef quote_header_value(value, extra_chars='', allow_token=True):\n    if isinstance(value, bytes):\n        value = bytes_to_wsgi(value)\n    value = str(value)\n    if allow_token:\n        token_chars = _token_chars | set(extra_chars)\n        if set(value).issubset(token_chars):\n            return value\n    return '\"%s\"' % value.replace('\\\\', '\\\\\\\\').replace('\"', '\\\\\"')", "response": "Quote a header value if necessary."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_options_header(value):\n    def _tokenize(string):\n        for match in _option_header_piece_re.finditer(string):\n            key, value = match.groups()\n            key = unquote_header_value(key)\n            if value is not None:\n                value = unquote_header_value(value, key == 'filename')\n            yield key, value\n\n    if not value:\n        return '', {}\n\n    parts = _tokenize(';' + value)\n    name = next(parts)[0]\n    extra = dict(parts)\n    return name, extra", "response": "Parse a Content - Type - like header into a tuple with the content type and options."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse an HTTP basic or digest authorization header transmitted by the web browser.", "response": "def parse_authorization_header(value):\n    \"\"\"Parse an HTTP basic/digest authorization header transmitted by the web\n    browser.  The return value is either `None` if the header was invalid or\n    not given, otherwise an :class:`~werkzeug.datastructures.Authorization`\n    object.\n\n    :param value: the authorization header to parse.\n    :return: a :class:`~werkzeug.datastructures.Authorization` object or `None`.\n    \"\"\"\n    if not value:\n        return\n    value = wsgi_to_bytes(value)\n    try:\n        auth_type, auth_info = value.split(None, 1)\n        auth_type = auth_type.lower()\n    except ValueError:\n        return\n    if auth_type == b'basic':\n        try:\n            username, password = base64.b64decode(auth_info).split(b':', 1)\n        except Exception as e:\n            return\n        return Authorization('basic', {'username':  bytes_to_wsgi(username),\n                                       'password': bytes_to_wsgi(password)})\n    elif auth_type == b'digest':\n        auth_map = parse_dict_header(auth_info)\n        for key in 'username', 'realm', 'nonce', 'uri', 'response':\n            if not key in auth_map:\n                return\n        if 'qop' in auth_map:\n            if not auth_map.get('nc') or not auth_map.get('cnonce'):\n                return\n        return Authorization('digest', auth_map)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing a range header into a : class : ~werkzeug. datastructures. Range object.", "response": "def parse_range_header(value, make_inclusive=True):\n    \"\"\"Parses a range header into a :class:`~werkzeug.datastructures.Range`\n    object.  If the header is missing or malformed `None` is returned.\n    `ranges` is a list of ``(start, stop)`` tuples where the ranges are\n    non-inclusive.\n\n    .. versionadded:: 0.7\n    \"\"\"\n    if not value or '=' not in value:\n        return None\n\n    ranges = []\n    last_end = 0\n    units, rng = value.split('=', 1)\n    units = units.strip().lower()\n\n    for item in rng.split(','):\n        item = item.strip()\n        if '-' not in item:\n            return None\n        if item.startswith('-'):\n            if last_end < 0:\n                return None\n            begin = int(item)\n            end = None\n            last_end = -1\n        elif '-' in item:\n            begin, end = item.split('-', 1)\n            begin = int(begin)\n            if begin < last_end or last_end < 0:\n                return None\n            if end:\n                end = int(end) + 1\n                if begin >= end:\n                    return None\n            else:\n                end = None\n            last_end = end\n        ranges.append((begin, end))\n\n    return Range(units, ranges)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse a content range header into a base content range object.", "response": "def parse_content_range_header(value, on_update=None):\n    \"\"\"Parses a range header into a\n    :class:`~werkzeug.datastructures.ContentRange` object or `None` if\n    parsing is not possible.\n\n    .. versionadded:: 0.7\n\n    :param value: a content range header to be parsed.\n    :param on_update: an optional callable that is called every time a value\n                      on the :class:`~werkzeug.datastructures.ContentRange`\n                      object is changed.\n    \"\"\"\n    if value is None:\n        return None\n    try:\n        units, rangedef = (value or '').strip().split(None, 1)\n    except ValueError:\n        return None\n\n    if '/' not in rangedef:\n        return None\n    rng, length = rangedef.split('/', 1)\n    if length == '*':\n        length = None\n    elif length.isdigit():\n        length = int(length)\n    else:\n        return None\n\n    if rng == '*':\n        return ContentRange(units, None, None, length, on_update=on_update)\n    elif '-' not in rng:\n        return None\n\n    start, stop = rng.split('-', 1)\n    try:\n        start = int(start)\n        stop = int(stop) + 1\n    except ValueError:\n        return None\n\n    if is_byte_range_valid(start, stop, length):\n        return ContentRange(units, start, stop, length, on_update=on_update)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef quote_etag(etag, weak=False):\n    if '\"' in etag:\n        raise ValueError('invalid etag')\n    etag = '\"%s\"' % etag\n    if weak:\n        etag = 'w/' + etag\n    return etag", "response": "Quote an etag.\n\n    :param etag: the etag to quote.\n    :param weak: set to `True` to tag it \"weak\"."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing an etag header.", "response": "def parse_etags(value):\n    \"\"\"Parse an etag header.\n\n    :param value: the tag header to parse\n    :return: an :class:`~werkzeug.datastructures.ETags` object.\n    \"\"\"\n    if not value:\n        return ETags()\n    strong = []\n    weak = []\n    end = len(value)\n    pos = 0\n    while pos < end:\n        match = _etag_re.match(value, pos)\n        if match is None:\n            break\n        is_weak, quoted, raw = match.groups()\n        if raw == '*':\n            return ETags(star_tag=True)\n        elif quoted:\n            raw = quoted\n        if is_weak:\n            weak.append(raw)\n        else:\n            strong.append(raw)\n        pos = match.end()\n    return ETags(strong, weak)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_resource_modified(environ, etag=None, data=None, last_modified=None):\n    if etag is None and data is not None:\n        etag = generate_etag(data)\n    elif data is not None:\n        raise TypeError('both data and etag given')\n    if environ['REQUEST_METHOD'] not in ('GET', 'HEAD'):\n        return False\n\n    unmodified = False\n    if isinstance(last_modified, string_types):\n        last_modified = parse_date(last_modified)\n\n    # ensure that microsecond is zero because the HTTP spec does not transmit\n    # that either and we might have some false positives.  See issue #39\n    if last_modified is not None:\n        last_modified = last_modified.replace(microsecond=0)\n\n    modified_since = parse_date(environ.get('HTTP_IF_MODIFIED_SINCE'))\n\n    if modified_since and last_modified and last_modified <= modified_since:\n        unmodified = True\n    if etag:\n        if_none_match = parse_etags(environ.get('HTTP_IF_NONE_MATCH'))\n        if if_none_match:\n            unmodified = if_none_match.contains_raw(etag)\n\n    return not unmodified", "response": "Returns a boolean indicating whether the resource was modified or not."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a new Set-Cookie header without the ``Set-Cookie`` prefix The parameters are the same as in the cookie Morsel object in the Python standard library but it accepts unicode data, too. On Python 3 the return value of this function will be a unicode string, on Python 2 it will be a native string. In both cases the return value is usually restricted to ascii as the vast majority of values are properly escaped, but that is no guarantee. If a unicode string is returned it's tunneled through latin1 as required by PEP 3333. The return value is not ASCII safe if the key contains unicode characters. This is technically against the specification but happens in the wild. It's strongly recommended to not use non-ASCII values for the keys. :param max_age: should be a number of seconds, or `None` (default) if the cookie should last only as long as the client's browser session. Additionally `timedelta` objects are accepted, too. :param expires: should be a `datetime` object or unix timestamp. :param path: limits the cookie to a given path, per default it will span the whole domain. :param domain: Use this if you want to set a cross-domain cookie. For example, ``domain=\".example.com\"`` will set a cookie that is readable by the domain ``www.example.com``, ``foo.example.com`` etc. Otherwise, a cookie will only be readable by the domain that set it. :param secure: The cookie will only be available via HTTPS :param httponly: disallow JavaScript to access the cookie. This is an extension to the cookie standard and probably not supported by all browsers. :param charset: the encoding for unicode values. :param sync_expires: automatically set expires if max_age is defined but expires not.", "response": "def dump_cookie(key, value='', max_age=None, expires=None, path='/',\n                domain=None, secure=False, httponly=False,\n                charset='utf-8', sync_expires=True):\n    \"\"\"Creates a new Set-Cookie header without the ``Set-Cookie`` prefix\n    The parameters are the same as in the cookie Morsel object in the\n    Python standard library but it accepts unicode data, too.\n\n    On Python 3 the return value of this function will be a unicode\n    string, on Python 2 it will be a native string.  In both cases the\n    return value is usually restricted to ascii as the vast majority of\n    values are properly escaped, but that is no guarantee.  If a unicode\n    string is returned it's tunneled through latin1 as required by\n    PEP 3333.\n\n    The return value is not ASCII safe if the key contains unicode\n    characters.  This is technically against the specification but\n    happens in the wild.  It's strongly recommended to not use\n    non-ASCII values for the keys.\n\n    :param max_age: should be a number of seconds, or `None` (default) if\n                    the cookie should last only as long as the client's\n                    browser session.  Additionally `timedelta` objects\n                    are accepted, too.\n    :param expires: should be a `datetime` object or unix timestamp.\n    :param path: limits the cookie to a given path, per default it will\n                 span the whole domain.\n    :param domain: Use this if you want to set a cross-domain cookie. For\n                   example, ``domain=\".example.com\"`` will set a cookie\n                   that is readable by the domain ``www.example.com``,\n                   ``foo.example.com`` etc. Otherwise, a cookie will only\n                   be readable by the domain that set it.\n    :param secure: The cookie will only be available via HTTPS\n    :param httponly: disallow JavaScript to access the cookie.  This is an\n                     extension to the cookie standard and probably not\n                     supported by all browsers.\n    :param charset: the encoding for unicode values.\n    :param sync_expires: automatically set expires if max_age is defined\n                         but expires not.\n    \"\"\"\n    key = to_bytes(key, charset)\n    value = to_bytes(value, charset)\n\n    if path is not None:\n        path = iri_to_uri(path, charset)\n    domain = _make_cookie_domain(domain)\n    if isinstance(max_age, timedelta):\n        max_age = (max_age.days * 60 * 60 * 24) + max_age.seconds\n    if expires is not None:\n        if not isinstance(expires, string_types):\n            expires = cookie_date(expires)\n    elif max_age is not None and sync_expires:\n        expires = to_bytes(cookie_date(time() + max_age))\n\n    buf = [key + b'=' + _cookie_quote(value)]\n\n    # XXX: In theory all of these parameters that are not marked with `None`\n    # should be quoted.  Because stdlib did not quote it before I did not\n    # want to introduce quoting there now.\n    for k, v, q in ((b'Domain', domain, True),\n                    (b'Expires', expires, False,),\n                    (b'Max-Age', max_age, False),\n                    (b'Secure', secure, None),\n                    (b'HttpOnly', httponly, None),\n                    (b'Path', path, False)):\n        if q is None:\n            if v:\n                buf.append(k)\n            continue\n\n        if v is None:\n            continue\n\n        tmp = bytearray(k)\n        if not isinstance(v, (bytes, bytearray)):\n            v = to_bytes(text_type(v), charset)\n        if q:\n            v = _cookie_quote(v)\n        tmp += b'=' + v\n        buf.append(bytes(tmp))\n\n    # The return value will be an incorrectly encoded latin1 header on\n    # Python 3 for consistency with the headers object and a bytestring\n    # on Python 2 because that's how the API makes more sense.\n    rv = b'; '.join(buf)\n    if not PY2:\n        rv = rv.decode('latin1')\n    return rv"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating a CSS minifier for the even Taxonomy.", "response": "def _make_cssmin(python_only=False):\n    \"\"\"\n    Generate CSS minifier.\n\n    :Parameters:\n      `python_only` : ``bool``\n        Use only the python variant. If true, the c extension is not even\n        tried to be loaded.\n\n    :Return: Minifier\n    :Rtype: ``callable``\n    \"\"\"\n    # pylint: disable = W0612\n    # (\"unused\" variables)\n\n    # pylint: disable = R0911, R0912, R0914, R0915\n    # (too many anything)\n\n    if not python_only:\n        try:\n            import _rcssmin\n        except ImportError:\n            pass\n        else:\n            return _rcssmin.cssmin\n\n    nl = r'(?:[\\n\\f]|\\r\\n?)' # pylint: disable = C0103\n    spacechar = r'[\\r\\n\\f\\040\\t]'\n\n    unicoded = r'[0-9a-fA-F]{1,6}(?:[\\040\\n\\t\\f]|\\r\\n?)?'\n    escaped = r'[^\\n\\r\\f0-9a-fA-F]'\n    escape = r'(?:\\\\(?:%(unicoded)s|%(escaped)s))' % locals()\n\n    nmchar = r'[^\\000-\\054\\056\\057\\072-\\100\\133-\\136\\140\\173-\\177]'\n    #nmstart = r'[^\\000-\\100\\133-\\136\\140\\173-\\177]'\n    #ident = (r'(?:'\n    #    r'-?(?:%(nmstart)s|%(escape)s)%(nmchar)s*(?:%(escape)s%(nmchar)s*)*'\n    #r')') % locals()\n\n    comment = r'(?:/\\*[^*]*\\*+(?:[^/*][^*]*\\*+)*/)'\n\n    # only for specific purposes. The bang is grouped:\n    _bang_comment = r'(?:/\\*(!?)[^*]*\\*+(?:[^/*][^*]*\\*+)*/)'\n\n    string1 = \\\n        r'(?:\\047[^\\047\\\\\\r\\n\\f]*(?:\\\\[^\\r\\n\\f][^\\047\\\\\\r\\n\\f]*)*\\047)'\n    string2 = r'(?:\"[^\"\\\\\\r\\n\\f]*(?:\\\\[^\\r\\n\\f][^\"\\\\\\r\\n\\f]*)*\")'\n    strings = r'(?:%s|%s)' % (string1, string2)\n\n    nl_string1 = \\\n        r'(?:\\047[^\\047\\\\\\r\\n\\f]*(?:\\\\(?:[^\\r]|\\r\\n?)[^\\047\\\\\\r\\n\\f]*)*\\047)'\n    nl_string2 = r'(?:\"[^\"\\\\\\r\\n\\f]*(?:\\\\(?:[^\\r]|\\r\\n?)[^\"\\\\\\r\\n\\f]*)*\")'\n    nl_strings = r'(?:%s|%s)' % (nl_string1, nl_string2)\n\n    uri_nl_string1 = r'(?:\\047[^\\047\\\\]*(?:\\\\(?:[^\\r]|\\r\\n?)[^\\047\\\\]*)*\\047)'\n    uri_nl_string2 = r'(?:\"[^\"\\\\]*(?:\\\\(?:[^\\r]|\\r\\n?)[^\"\\\\]*)*\")'\n    uri_nl_strings = r'(?:%s|%s)' % (uri_nl_string1, uri_nl_string2)\n\n    nl_escaped = r'(?:\\\\%(nl)s)' % locals()\n\n    space = r'(?:%(spacechar)s|%(comment)s)' % locals()\n\n    ie7hack = r'(?:>/\\*\\*/)'\n\n    uri = (r'(?:'\n        r'(?:[^\\000-\\040\"\\047()\\\\\\177]*'\n            r'(?:%(escape)s[^\\000-\\040\"\\047()\\\\\\177]*)*)'\n        r'(?:'\n            r'(?:%(spacechar)s+|%(nl_escaped)s+)'\n            r'(?:'\n                r'(?:[^\\000-\\040\"\\047()\\\\\\177]|%(escape)s|%(nl_escaped)s)'\n                r'[^\\000-\\040\"\\047()\\\\\\177]*'\n                r'(?:%(escape)s[^\\000-\\040\"\\047()\\\\\\177]*)*'\n            r')+'\n        r')*'\n    r')') % locals()\n\n    nl_unesc_sub = _re.compile(nl_escaped).sub\n\n    uri_space_sub = _re.compile((\n        r'(%(escape)s+)|%(spacechar)s+|%(nl_escaped)s+'\n    ) % locals()).sub\n    uri_space_subber = lambda m: m.groups()[0] or ''\n\n    space_sub_simple = _re.compile((\n        r'[\\r\\n\\f\\040\\t;]+|(%(comment)s+)'\n    ) % locals()).sub\n    space_sub_banged = _re.compile((\n        r'[\\r\\n\\f\\040\\t;]+|(%(_bang_comment)s+)'\n    ) % locals()).sub\n\n    post_esc_sub = _re.compile(r'[\\r\\n\\f\\t]+').sub\n\n    main_sub = _re.compile((\n        r'([^\\\\\"\\047u>@\\r\\n\\f\\040\\t/;:{}]+)'\n        r'|(?<=[{}(=:>+[,!])(%(space)s+)'\n        r'|^(%(space)s+)'\n        r'|(%(space)s+)(?=(([:{});=>+\\],!])|$)?)'\n        r'|;(%(space)s*(?:;%(space)s*)*)(?=(\\})?)'\n        r'|(\\{)'\n        r'|(\\})'\n        r'|(%(strings)s)'\n        r'|(?<!%(nmchar)s)url\\(%(spacechar)s*('\n                r'%(uri_nl_strings)s'\n                r'|%(uri)s'\n            r')%(spacechar)s*\\)'\n        r'|(@[mM][eE][dD][iI][aA])(?!%(nmchar)s)'\n        r'|(%(ie7hack)s)(%(space)s*)'\n        r'|(:[fF][iI][rR][sS][tT]-[lL]'\n            r'(?:[iI][nN][eE]|[eE][tT][tT][eE][rR]))'\n            r'(%(space)s*)(?=[{,])'\n        r'|(%(nl_strings)s)'\n        r'|(%(escape)s[^\\\\\"\\047u>@\\r\\n\\f\\040\\t/;:{}]*)'\n    ) % locals()).sub\n\n    #print main_sub.__self__.pattern\n\n    def main_subber(keep_bang_comments, base_dir=''):\n        \"\"\" Make main subber \"\"\"\n        in_macie5, in_rule, at_media = [0], [0], [0]\n\n        if keep_bang_comments:\n            space_sub = space_sub_banged\n            def space_subber(match):\n                \"\"\" Space|Comment subber \"\"\"\n                if match.lastindex:\n                    group1, group2 = match.group(1, 2)\n                    if group2:\n                        if group1.endswith(r'\\*/'):\n                            in_macie5[0] = 1\n                        else:\n                            in_macie5[0] = 0\n                        return group1\n                    elif group1:\n                        if group1.endswith(r'\\*/'):\n                            if in_macie5[0]:\n                                return ''\n                            in_macie5[0] = 1\n                            return r'/*\\*/'\n                        elif in_macie5[0]:\n                            in_macie5[0] = 0\n                            return '/**/'\n                return ''\n        else:\n            space_sub = space_sub_simple\n            def space_subber(match):\n                \"\"\" Space|Comment subber \"\"\"\n                if match.lastindex:\n                    if match.group(1).endswith(r'\\*/'):\n                        if in_macie5[0]:\n                            return ''\n                        in_macie5[0] = 1\n                        return r'/*\\*/'\n                    elif in_macie5[0]:\n                        in_macie5[0] = 0\n                        return '/**/'\n                return ''\n\n        def fn_space_post(group):\n            \"\"\" space with token after \"\"\"\n            if group(5) is None or (\n                    group(6) == ':' and not in_rule[0] and not at_media[0]):\n                return ' ' + space_sub(space_subber, group(4))\n            return space_sub(space_subber, group(4))\n\n        def fn_semicolon(group):\n            \"\"\" ; handler \"\"\"\n            return ';' + space_sub(space_subber, group(7))\n\n        def fn_semicolon2(group):\n            \"\"\" ; handler \"\"\"\n            if in_rule[0]:\n                return space_sub(space_subber, group(7))\n            return ';' + space_sub(space_subber, group(7))\n\n        def fn_open(group):\n            \"\"\" { handler \"\"\"\n            # pylint: disable = W0613\n            if at_media[0]:\n                at_media[0] -= 1\n            else:\n                in_rule[0] = 1\n            return '{'\n\n        def fn_close(group):\n            \"\"\" } handler \"\"\"\n            # pylint: disable = W0613\n            in_rule[0] = 0\n            return '}'\n\n        def fn_media(group):\n            \"\"\" @media handler \"\"\"\n            at_media[0] += 1\n            return group(13)\n\n        def fn_ie7hack(group):\n            \"\"\" IE7 Hack handler \"\"\"\n            if not in_rule[0] and not at_media[0]:\n                in_macie5[0] = 0\n                return group(14) + space_sub(space_subber, group(15))\n            return '>' + space_sub(space_subber, group(15))\n\n        def fn_url(group):\n            \"\"\"\n            Process url\n            \"\"\"\n            import os\n            \n            url = uri_space_sub(uri_space_subber, group(12))\n            if base_dir:\n                if (url.startswith('\"') and url.endswith('\"')) or (url.startswith(\"'\") and url.endswith(\"'\")):\n                    url = url[0] + os.path.join(base_dir, url[1:-1]).replace('\\\\', '/') + url[0]\n                else:\n                    url = os.path.join(base_dir, url).replace('\\\\', '/')\n            return 'url(%s)' % url\n        \n        table = (\n            None,\n            None,\n            None,\n            None,\n            fn_space_post,                      # space with token after\n            fn_space_post,                      # space with token after\n            fn_space_post,                      # space with token after\n            fn_semicolon,                       # semicolon\n            fn_semicolon2,                      # semicolon\n            fn_open,                            # {\n            fn_close,                           # }\n            lambda g: g(11),                    # string\n#            lambda g: 'url(%s)' % uri_space_sub(uri_space_subber, g(12)),\n            fn_url,\n                                                # url(...)\n            fn_media,                           # @media\n            None,\n            fn_ie7hack,                         # ie7hack\n            None,\n            lambda g: g(16) + ' ' + space_sub(space_subber, g(17)),\n                                                # :first-line|letter followed\n                                                # by [{,] (apparently space\n                                                # needed for IE6)\n            lambda g: nl_unesc_sub('', g(18)),  # nl_string\n            lambda g: post_esc_sub(' ', g(19)), # escape\n        )\n\n        def func(match):\n            \"\"\" Main subber \"\"\"\n            idx, group = match.lastindex, match.group\n            if idx > 3:\n                return table[idx](group)\n\n            # shortcuts for frequent operations below:\n            elif idx == 1:     # not interesting\n                return group(1)\n            #else: # space with token before or at the beginning\n            return space_sub(space_subber, group(idx))\n\n        return func\n\n    def cssmin(style, keep_bang_comments=False, base_dir=''): # pylint: disable = W0621\n        \"\"\"\n        Minify CSS.\n\n        :Parameters:\n          `style` : ``str``\n            CSS to minify\n\n          `keep_bang_comments` : ``bool``\n            Keep comments starting with an exclamation mark? (``/*!...*/``)\n\n        :Return: Minified style\n        :Rtype: ``str``\n        \"\"\"\n        return main_sub(main_subber(keep_bang_comments, base_dir), style)\n\n    return cssmin"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking whether a process id exists in the current process table.", "response": "def pid_exists(pid):\n    \"\"\"Check whether pid exists in the current process table.\"\"\"\n    if pid < 0:\n        return False\n    try:\n        os.kill(pid, 0)\n    except OSError as e:\n        return e.errno == errno.EPERM\n    else:\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwaiting for a process with pid to terminate and return its exit status code as an integer.", "response": "def wait_pid(pid, timeout=None, callback=None):\n    \"\"\"Wait for process with pid 'pid' to terminate and return its\n    exit status code as an integer.\n\n    If pid is not a children of os.getpid() (current process) just\n    waits until the process disappears and return None.\n\n    If pid does not exist at all return None immediately.\n\n    Raise TimeoutExpired on timeout expired (if specified).\n    \"\"\"\n    def check_timeout(delay):\n        if timeout is not None:\n            if time.time() >= stop_at:\n                if callback:\n                    callback(pid)\n                else:\n                    raise TimeoutExpired\n        time.sleep(delay)\n        return min(delay * 2, 0.04)\n\n    if timeout is not None:\n        waitcall = lambda: os.waitpid(pid, os.WNOHANG)\n        stop_at = time.time() + timeout\n    else:\n        waitcall = lambda: os.waitpid(pid, 0)\n\n    delay = 0.0001\n    while 1:\n        try:\n            retpid, status = waitcall()\n        except OSError as err:\n            if err.errno == errno.EINTR:\n                delay = check_timeout(delay)\n                continue\n            elif err.errno == errno.ECHILD:\n                # This has two meanings:\n                # - pid is not a child of os.getpid() in which case\n                #   we keep polling until it's gone\n                # - pid never existed in the first place\n                # In both cases we'll eventually return None as we\n                # can't determine its exit status code.\n                while 1:\n                    if pid_exists(pid):\n                        delay = check_timeout(delay)\n                    else:\n                        return\n            else:\n                raise\n        else:\n            if retpid == 0:\n                # WNOHANG was used, pid is still running\n                delay = check_timeout(delay)\n                continue\n            # process exited due to a signal; return the integer of\n            # that signal\n            if os.WIFSIGNALED(status):\n                return os.WTERMSIG(status)\n            # process exited using exit(2) system call; return the\n            # integer exit(2) system call has been called with\n            elif os.WIFEXITED(status):\n                return os.WEXITSTATUS(status)\n            else:\n                # should never happen\n                raise RuntimeError(\"unknown process exit status\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the filename according to self. to_path and return the filesystem encoded filename if filesystem is False otherwise return unicode filename if filesystem is True otherwise return filesystem encoded filename if convert is True", "response": "def get_filename(self, filename, filesystem=False, convert=False, subpath=''):\r\n        \"\"\"\r\n        Get the filename according to self.to_path, and if filesystem is False\r\n        then return unicode filename, otherwise return filesystem encoded filename\r\n    \r\n        @param filename: relative filename, it'll be combine with self.to_path\r\n        @param filesystem: if True, then encoding the filename to filesystem\r\n        @param convert: if True, then convert filename with FilenameConverter class\r\n        @param subpath: sub folder in to_path\r\n        \"\"\"\r\n        from uliweb.utils.common import safe_unicode\r\n        \r\n        #make sure the filename is unicode\r\n        s = settings.GLOBAL\r\n        if convert:\r\n            _p, _f = os.path.split(filename)\r\n            _filename = os.path.join(_p, self.filename_convert(_f))\r\n        else:\r\n            _filename = filename\r\n        nfile = safe_unicode(_filename, s.HTMLPAGE_ENCODING)\r\n\r\n        if subpath:\r\n            paths = [application_path(self.to_path), subpath, nfile]\r\n        else:\r\n            paths = [application_path(self.to_path), nfile]\r\n        f = os.path.normpath(os.path.join(*paths)).replace('\\\\', '/')\r\n    \r\n        if filesystem:\r\n            return files.encode_filename(f, to_encoding=s.FILESYSTEM_ENCODING)\r\n        return f"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef download(self, filename, action='download', x_filename='', x_sendfile=None, real_filename=''):\r\n        from uliweb import request\r\n        from uliweb.utils.common import safe_str\r\n        from uliweb.utils.filedown import filedown\r\n        \r\n        s = settings.GLOBAL\r\n\r\n        action = request.GET.get('action', action)\r\n        \r\n        if not real_filename:\r\n            real_filename = self.get_filename(filename, True, convert=False)\r\n        else:\r\n            real_filename = files.encode_filename(real_filename, to_encoding=s.FILESYSTEM_ENCODING)\r\n\r\n        if not x_filename:\r\n            x_filename = safe_str(filename, s.FILESYSTEM_ENCODING)\r\n        if self.x_file_prefix:\r\n            x_filename = os.path.normpath(os.path.join(self.x_file_prefix, x_filename)).replace('\\\\', '/')\r\n        \r\n        xsend_flag = bool(self.x_sendfile) if x_sendfile is None else x_sendfile\r\n        return filedown(request.environ, filename, action=action, \r\n            x_sendfile=xsend_flag, x_header_name=self.x_header_name, \r\n            x_filename=x_filename, real_filename=real_filename)", "response": "Download the next entry in the list of items in the file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the url of a tag with the given title and text.", "response": "def get_url(self, filename, query_para=None, **url_args):\r\n        \"\"\"\r\n        Return <a href=\"filename\" title=\"filename\"> tag\r\n        You should pass title and text to url_args, if not pass, then using filename\r\n        \"\"\"\r\n        from uliweb.core.html import Tag\r\n        \r\n        title = url_args.pop('title', filename)\r\n        text = url_args.pop('text', title)\r\n        query_para = query_para or {}\r\n        return str(Tag('a', title, href=self.get_href(filename, **query_para), **url_args))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a sequence of timesince messages for a given date.", "response": "def timesince(d, now=None, pos=True, flag=False):\n    \"\"\"\n    pos means calculate which direction, pos = True, now - d, pos = False, d - now\n    flag means return value type, True will return since, message and Flase return message\n    >>> d = datetime.datetime(2009, 10, 1, 12, 23, 19)\n    >>> timesince(d, d, True)\n    >>> now = datetime.datetime(2009, 10, 1, 12, 24, 19)\n    >>> timesince(d, now, True)\n    u'1 minute ago'\n    >>> now = datetime.datetime(2009, 10, 1, 12, 24, 30)\n    >>> timesince(d, now, True)\n    u'1 minute ago'\n    >>> now = datetime.datetime(2009, 9, 28, 12, 24, 30)\n    >>> timesince(d, now, True)\n    u'2 days, 23 hours later'\n    >>> now = datetime.datetime(2009, 10, 3, 12, 24, 30)\n    >>> timesince(d, now, True)\n    u'2 days ago'\n    \"\"\"\n    if not d:\n        if flag:\n            return 0, ''\n        else:\n            return ''\n    chunks = (\n      (60 * 60 * 24 * 365, lambda n: ungettext('year', 'years', n)),\n      (60 * 60 * 24 * 30, lambda n: ungettext('month', 'months', n)),\n      (60 * 60 * 24 * 7, lambda n : ungettext('week', 'weeks', n)),\n      (60 * 60 * 24, lambda n : ungettext('day', 'days', n)),\n      (60 * 60, lambda n: ungettext('hour', 'hours', n)),\n      (60, lambda n: ungettext('minute', 'minutes', n))\n    )\n\n    if not now:\n        now = date.now()\n    else:\n        now = date.to_datetime(now)\n    d = date.to_datetime(d)\n    \n    delta = now - (d - datetime.timedelta(0, 0, d.microsecond))\n    oldsince = since = delta.days * 24 * 60 * 60 + delta.seconds\n    \n    suffix = ''\n    if pos:\n        if since >= 0:\n            suffix = ugettext(' ago')\n        elif since < 0:\n            suffix = ugettext(' later')\n            since *= -1\n    \n    for i, (seconds, name) in enumerate(chunks):\n        count = since // seconds\n        if count != 0:\n            break\n    s = ('%(number)d %(type)s') % {'number': count, 'type': name(count)}\n    if i + 1 < len(chunks):\n        # Now get the second item\n        seconds2, name2 = chunks[i + 1]\n        count2 = (since - (seconds * count)) // seconds2\n        if count2 != 0:\n            s += (', %(number)d %(type)s') % {'number': count2, 'type': name2(count2)}\n    #if flag==True, then return twe elements (since, message) \n    if flag:\n        return oldsince, s + suffix\n    else:\n        return s + suffix"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_hexdigest(algorithm, salt, raw_password):\r\n    if isinstance(salt, unicode):\r\n        salt = salt.encode('utf8')\r\n    if algorithm == 'crypt':\r\n        try:\r\n            import crypt\r\n        except ImportError:\r\n            raise ValueError('\"crypt\" password algorithm not supported in this environment')\r\n        return crypt.crypt(raw_password, salt)\r\n    # The rest of the supported algorithms are supported by hashlib, but\r\n    # hashlib is only available in Python 2.5.\r\n    try:\r\n        import hashlib\r\n    except ImportError:\r\n        if algorithm == 'md5':\r\n            import md5\r\n            return md5.new(salt + raw_password).hexdigest()\r\n        elif algorithm == 'sha1':\r\n            import sha\r\n            return sha.new(salt + raw_password).hexdigest()\r\n    else:\r\n        if algorithm == 'md5':\r\n            return hashlib.md5(salt + raw_password).hexdigest()\r\n        elif algorithm == 'sha1':\r\n            return hashlib.sha1(salt + raw_password).hexdigest()\r\n    raise ValueError(\"Got unknown password algorithm type in password.\")", "response": "Returns a string of the hexdigest of the given plaintext password and salt using the given algorithm."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check_password(raw_password, enc_password):\r\n    l = enc_password.split('$')\r\n    #only password of built-in user can split to 3\r\n    if len(l)==3:\r\n        algo, salt, hsh = l\r\n        return hsh == get_hexdigest(algo, salt, raw_password)\r\n    else:\r\n        return False", "response": "Checks if the raw_password is correct for the given encryption format."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfinds auth user object", "response": "def get_auth_user():\r\n    \"\"\"\r\n    Find auth user\r\n    :return: user object could be a model instance or a dict\r\n    \"\"\"\r\n    from uliweb import request\r\n    from uliweb import settings\r\n\r\n    user_fieldname = settings.get_var('AUTH/GET_AUTH_USER_FIELDNAME', 'id')\r\n\r\n    session_key = _get_auth_key()\r\n    user_id = request.session.get(session_key)\r\n    if user_id:\r\n        if isinstance(user_id, dict):\r\n            return user_id\r\n        else:\r\n            User = get_model('user')\r\n            user = User.get(User.c[user_fieldname]==user_id)\r\n            return user"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset user session :param user: user object chould be model instance or dict :return:", "response": "def set_user_session(user):\r\n    \"\"\"\r\n    Set user session\r\n    :param user: user object chould be model instance or dict\r\n    :return:\r\n    \"\"\"\r\n    from uliweb import settings, request\r\n\r\n    user_fieldname = settings.get_var('AUTH/GET_AUTH_USER_FIELDNAME', 'id')\r\n    share_session = settings.get_var('AUTH/AUTH_SHARE_USER_SESSION', False)\r\n    if isinstance(user, dict):\r\n        user_id = user[user_fieldname]\r\n    else:\r\n        user_id = getattr(user, user_fieldname)\r\n\r\n    if share_session:\r\n        cache = functions.get_cache()\r\n        key = get_user_session_key(user_id)\r\n        session_id = cache.get(key, None)\r\n        log.debug('Auth: user session user_id={}, session_id={}, key={}'.format(user_id, session_id, key))\r\n        if not session_id:\r\n            request.session.save()\r\n            log.debug('Auth: set user session mapping userid={}, '\r\n                      'session_id={}, expiry time={}'.format(user_id,\r\n                                                             request.session.key,\r\n                                                             request.session.expiry_time))\r\n            cache.set(key, request.session.key, expire=request.session.expiry_time)\r\n        elif session_id != request.session.key:\r\n            log.debug('Auth: load oldkey={}, key={}'.format(request.session.key, session_id))\r\n            request.session.delete()\r\n            request.session.load(session_id)\r\n    if isinstance(user, dict):\r\n        request.session[_get_auth_key()] = user\r\n    else:\r\n        request.session[_get_auth_key()] = user_id\r\n    request.user = user"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a new user in the tree", "response": "def create_user(username, password, **kwargs):\r\n    \"\"\"\r\n    return flag, result(result can be an User object or just True, {} for errors)\r\n    \"\"\"\r\n    try:\r\n        User = get_model('user')\r\n        user = User.get(User.c.username==username)\r\n        if user:\r\n            return False, {'username':\"Username is already existed!\"}\r\n        user = User(username=username, password=password, **kwargs)\r\n        user.set_password(password)\r\n        user.save()\r\n        return True, user\r\n    except Exception as e:\r\n        log.exception(e)\r\n        return False, {'_': \"Creating user failed!\"}"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nlogs-in to the user", "response": "def login(username):\r\n    \"\"\"\r\n    return user\r\n    \"\"\"\r\n    from uliweb.utils.date import now\r\n    from uliweb import request\r\n\r\n    User = get_model('user')\r\n\r\n    if isinstance(username, (str, unicode)):\r\n        user = User.get(User.c.username==username)\r\n    else:\r\n        user = username\r\n    user.last_login = now()\r\n    user.save()\r\n    set_user_session(user)\r\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef logout():\r\n    from uliweb import request\r\n\r\n    delete_user_session()\r\n    request.session.delete()\r\n    request.user = None\r\n    return True", "response": "Logout the authenticated user from the request."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get(self, key):\n        if isinstance(key, unicode):\n            key = key.encode('utf-8')\n        v = self.client.get(key)\n        if v is None:\n            raise KeyError(\"Cache key [%s] not found\" % key)\n        else:\n            return v", "response": "get a key from memcached"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_answer(message, answers='Yn', default='Y', quit=''):\r\n    if quit and quit not in answers:\r\n        answers = answers + quit\r\n        \r\n    message = message + '(' + '/'.join(answers) + ')[' + default + ']:'\r\n    ans = raw_input(message).strip().upper()\r\n    if default and not ans:\r\n        ans = default.upper()\r\n    while ans not in answers.upper():\r\n        ans = raw_input(message).strip().upper()\r\n    if quit and ans == quit.upper():\r\n        print \"Command be cancelled!\"\r\n        sys.exit(0)\r\n    return ans", "response": "Get an answer from stdin"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_commands(mod):\r\n    import inspect\r\n    import types\r\n\r\n    commands = {}\r\n\r\n    def check(c):\r\n        return (inspect.isclass(c) and\r\n            issubclass(c, Command) and c is not Command and not issubclass(c, CommandManager))\r\n\r\n    for name in dir(mod):\r\n        c = getattr(mod, name)\r\n        if check(c):\r\n            commands[c.name] = c\r\n\r\n    return commands", "response": "Find commands from a module\r\nentric sequence."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_input(prompt, default=None, choices=None, option_value=None):\r\n    if option_value is not None:\r\n        return option_value\r\n    \r\n    choices = choices or []\r\n    while 1:\r\n        r = raw_input(prompt+' ').strip()\r\n        if not r and default is not None:\r\n            return default\r\n        if choices:\r\n            if r not in choices:\r\n                r = None\r\n            else:\r\n                break\r\n        else:\r\n            break\r\n    return r", "response": "Get the input from the user."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates and return an OptionParser instance which will be used to parse the arguments to this command.", "response": "def create_parser(self, prog_name, subcommand):\r\n        \"\"\"\r\n        Create and return the ``OptionParser`` which will be used to\r\n        parse the arguments to this command.\r\n    \r\n        \"\"\"\r\n        return OptionParser(prog=prog_name,\r\n                            usage=self.usage(subcommand),\r\n                            version='',\r\n                            add_help_option = False,\r\n                            option_list=self.option_list)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a brief description of how to use this command by default from the attribute self. help.", "response": "def usage(self, subcommand):\r\n        \"\"\"\r\n        Return a brief description of how to use this command, by\r\n        default from the attribute ``self.help``.\r\n    \r\n        \"\"\"\r\n        if len(self.option_list) > 0:\r\n            usage = '%%prog %s [options] %s' % (subcommand, self.args)\r\n        else:\r\n            usage = '%%prog %s %s' % (subcommand, self.args)\r\n        if self.help:\r\n            return '%s\\n\\n%s' % (usage, self.help)\r\n        else:\r\n            return usage"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef print_help(self, prog_name, subcommand):\r\n        parser = self.create_parser(prog_name, subcommand)\r\n        parser.print_help()", "response": "Print the help message for this command."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrun a command from the given command line arguments.", "response": "def run_from_argv(self, prog, subcommand, global_options, argv):\r\n        \"\"\"\r\n        Set up any environment changes requested, then run this command.\r\n    \r\n        \"\"\"\r\n        self.prog_name = prog\r\n        if self.skip_options:\r\n            parser = NewOptionParser(prog=self.prog_name,\r\n                     usage=self.usage(subcommand),\r\n                    version='',\r\n                    formatter = NewFormatter(),\r\n                     add_help_option = False,\r\n                     option_list=self.option_list)\r\n        else:\r\n            parser = self.create_parser(prog, subcommand)\r\n        options, args = parser.parse_args(argv)\r\n        self.execute(args, options, global_options)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntries to fetch the given subcommand from the command line and returns the appropriate command class.", "response": "def fetch_command(self, global_options, subcommand):\r\n        \"\"\"\r\n        Tries to fetch the given subcommand, printing a message with the\r\n        appropriate command called from the command line (usually\r\n        \"uliweb\") if it can't be found.\r\n        \"\"\"\r\n        commands = self.get_commands(global_options)\r\n        try:\r\n            klass = commands[subcommand]\r\n        except KeyError:\r\n            sys.stderr.write(\"Unknown command: %r\\nType '%s help' for usage.\\nMany commands will only run at project directory, maybe the directory is not right.\\n\" % \\\r\n                (subcommand, self.prog_name))\r\n            sys.exit(1)\r\n        return klass"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngives the command-line arguments, this figures out which subcommand is being run, creates a parser appropriate to that command, and runs it.", "response": "def execute(self, callback=None):\r\n        \"\"\"\r\n        Given the command-line arguments, this figures out which subcommand is\r\n        being run, creates a parser appropriate to that command, and runs it.\r\n        \"\"\"\r\n        # Preprocess options to extract --settings and --pythonpath.\r\n        # These options could affect the commands that are available, so they\r\n        # must be processed early.\r\n        self.parser = parser = NewOptionParser(prog=self.prog_name,\r\n                             usage=self.usage_info,\r\n#                             version=self.get_version(),\r\n                             formatter = NewFormatter(),\r\n                             add_help_option = False,\r\n                             option_list=self.option_list)\r\n        \r\n        if not self.global_options:\r\n            global_options, args = parser.parse_args(self.argv)\r\n            global_options.apps_dir = os.path.normpath(os.path.join(global_options.project, 'apps'))\r\n            handle_default_options(global_options)\r\n            args = args[1:]\r\n        else:\r\n            global_options = self.global_options\r\n            args = self.argv\r\n\r\n        if global_options.envs:\r\n            for x in global_options.envs:\r\n                if '=' in x:\r\n                    k, v = x.split('=')\r\n                    os.environ[k.strip()] = v.strip()\r\n                else:\r\n                    print ('Error: environment variable definition (%s) format is not right, '\r\n                           'shoule be -Ek=v or -Ek=\"a b\"' % x)\r\n\r\n        global_options.settings = global_options.settings or os.environ.get('SETTINGS', 'settings.ini')\r\n        global_options.local_settings = global_options.local_settings or os.environ.get('LOCAL_SETTINGS', 'local_settings.ini')\r\n        \r\n        if callback:\r\n            callback(global_options)\r\n            \r\n        if len(args) == 0:\r\n            if global_options.version:\r\n                print self.get_version()\r\n                sys.exit(0)\r\n            else:\r\n                self.print_help(global_options)\r\n                sys.ext(1)\r\n\r\n        self.do_command(args, global_options)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the url route rules.", "response": "def set_urlroute_rules(rules=None):\n    \"\"\"\n    rules should be (pattern, replace)\n\n    e.g.: ('/admin', '/demo')\n    \"\"\"\n    global __url_route_rules__\n    __url_route_rules__ = []\n    for k, v in (rules or {}).values():\n        __url_route_rules__.append((re.compile(k), v))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef encode_filename(filename, from_encoding='utf-8', to_encoding=None):\n    import sys\n    to_encoding = to_encoding or sys.getfilesystemencoding()\n    from_encoding = from_encoding or sys.getfilesystemencoding()\n    if not isinstance(filename, unicode):\n        try:\n            f = unicode(filename, from_encoding)\n        except UnicodeDecodeError:\n            try:\n                f = unicode(filename, 'utf-8')\n            except UnicodeDecodeError:\n                raise Exception, \"Unknown encoding of the filename %s\" % filename\n        filename = f\n    if to_encoding:\n        return filename.encode(to_encoding)\n    else:\n        return filename", "response": "Encode a file in the language of the user."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a string representation of the given size in bytes.", "response": "def str_filesize(size):\n    \"\"\"\n    >>> print str_filesize(0)\n    0\n    >>> print str_filesize(1023) \n    1023\n    >>> print str_filesize(1024)\n    1K\n    >>> print str_filesize(1024*2)\n    2K\n    >>> print str_filesize(1024**2-1)\n    1023K\n    >>> print str_filesize(1024**2)\n    1M\n    \"\"\"\n    import bisect\n    \n    d = [(1024-1,'K'), (1024**2-1,'M'), (1024**3-1,'G'), (1024**4-1,'T')]\n    s = [x[0] for x in d]\n    \n    index = bisect.bisect_left(s, size) - 1\n    if index == -1:\n        return str(size)\n    else:\n        b, u = d[index]\n    return str(size / (b+1)) + u"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef show_table(name, table, i, total):\n    return '[%d/%d, %s] %s' % (i+1, total, table.__appname__, name)", "response": "Display table info,\n    name is tablename\n    table is table object\n    i is current Index\n    total is total of tables"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef handle(self, options, global_options, *args):\n\n        from uliweb.orm.graph import generate_dot, generate_file\n        from uliweb.utils.common import get_tempfilename, get_tempfilename2\n\n        engine = get_engine(options, global_options)\n\n        if args:\n            apps = args\n        else:\n            apps = self.get_apps(global_options)\n\n        if not options.tables:\n            _apps = apps\n        else:\n            _apps = None\n        tables = get_tables(global_options.apps_dir, _apps, tables=options.tables,\n            engine_name=options.engine,\n            settings_file=global_options.settings,\n            local_settings_file=global_options.local_settings)\n\n        if options.format=='txt' or not options.format:\n            result = generate_dot(tables, apps, engine_name=options.engine, fontname=options.font)\n            if options.outputfile:\n                with open(options.outputfile, 'w') as f:\n                    f.write(result)\n            else:\n                print result\n        else:\n            result = generate_file(tables, apps, options.outputfile, options.format,\n                                   engine_name=options.engine, fontname=options.font)\n            if result:\n                print result", "response": "Handles the command line options."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _make_jsmin(python_only=False):\n    # pylint: disable = R0912, R0914, W0612\n    if not python_only:\n        try:\n            import _rjsmin\n        except ImportError:\n            pass\n        else:\n            return _rjsmin.jsmin\n    try:\n        xrange\n    except NameError:\n        xrange = range # pylint: disable = W0622\n\n    space_chars = r'[\\000-\\011\\013\\014\\016-\\040]'\n\n    line_comment = r'(?://[^\\r\\n]*)'\n    space_comment = r'(?:/\\*[^*]*\\*+(?:[^/*][^*]*\\*+)*/)'\n    string1 = \\\n        r'(?:\\047[^\\047\\\\\\r\\n]*(?:\\\\(?:[^\\r\\n]|\\r?\\n|\\r)[^\\047\\\\\\r\\n]*)*\\047)'\n    string2 = r'(?:\"[^\"\\\\\\r\\n]*(?:\\\\(?:[^\\r\\n]|\\r?\\n|\\r)[^\"\\\\\\r\\n]*)*\")'\n    strings = r'(?:%s|%s)' % (string1, string2)\n\n    charclass = r'(?:\\[[^\\\\\\]\\r\\n]*(?:\\\\[^\\r\\n][^\\\\\\]\\r\\n]*)*\\])'\n    nospecial = r'[^/\\\\\\[\\r\\n]'\n    regex = r'(?:/(?![\\r\\n/*])%s*(?:(?:\\\\[^\\r\\n]|%s)%s*)*/)' % (\n        nospecial, charclass, nospecial\n    )\n    space = r'(?:%s|%s)' % (space_chars, space_comment)\n    newline = r'(?:%s?[\\r\\n])' % line_comment\n\n    def fix_charclass(result):\n        \"\"\" Fixup string of chars to fit into a regex char class \"\"\"\n        pos = result.find('-')\n        if pos >= 0:\n            result = r'%s%s-' % (result[:pos], result[pos + 1:])\n\n        def sequentize(string):\n            \"\"\"\n            Notate consecutive characters as sequence\n\n            (1-4 instead of 1234)\n            \"\"\"\n            first, last, result = None, None, []\n            for char in map(ord, string):\n                if last is None:\n                    first = last = char\n                elif last + 1 == char:\n                    last = char\n                else:\n                    result.append((first, last))\n                    first = last = char\n            if last is not None:\n                result.append((first, last))\n            return ''.join(['%s%s%s' % (\n                chr(first),\n                last > first + 1 and '-' or '',\n                last != first and chr(last) or ''\n            ) for first, last in result])\n\n        return _re.sub(r'([\\000-\\040\\047])', # for better portability\n            lambda m: '\\\\%03o' % ord(m.group(1)), (sequentize(result)\n                .replace('\\\\', '\\\\\\\\')\n                .replace('[', '\\\\[')\n                .replace(']', '\\\\]')\n            )\n        )\n\n    def id_literal_(what):\n        \"\"\" Make id_literal like char class \"\"\"\n        match = _re.compile(what).match\n        result = ''.join([\n            chr(c) for c in xrange(127) if not match(chr(c))\n        ])\n        return '[^%s]' % fix_charclass(result)\n\n    def not_id_literal_(keep):\n        \"\"\" Make negated id_literal like char class \"\"\"\n        match = _re.compile(id_literal_(keep)).match\n        result = ''.join([\n            chr(c) for c in xrange(127) if not match(chr(c))\n        ])\n        return r'[%s]' % fix_charclass(result)\n\n    not_id_literal = not_id_literal_(r'[a-zA-Z0-9_$]')\n    preregex1 = r'[(,=:\\[!&|?{};\\r\\n]'\n    preregex2 = r'%(not_id_literal)sreturn' % locals()\n\n    id_literal = id_literal_(r'[a-zA-Z0-9_$]')\n    id_literal_open = id_literal_(r'[a-zA-Z0-9_${\\[(+-]')\n    id_literal_close = id_literal_(r'[a-zA-Z0-9_$}\\])\"\\047+-]')\n\n    space_sub = _re.compile((\n        r'([^\\047\"/\\000-\\040]+)'\n        r'|(%(strings)s[^\\047\"/\\000-\\040]*)'\n        r'|(?:(?<=%(preregex1)s)%(space)s*(%(regex)s[^\\047\"/\\000-\\040]*))'\n        r'|(?:(?<=%(preregex2)s)%(space)s*(%(regex)s[^\\047\"/\\000-\\040]*))'\n        r'|(?<=%(id_literal_close)s)'\n            r'%(space)s*(?:(%(newline)s)%(space)s*)+'\n            r'(?=%(id_literal_open)s)'\n        r'|(?<=%(id_literal)s)(%(space)s)+(?=%(id_literal)s)'\n        r'|(?<=\\+)(%(space)s)+(?=\\+\\+)'\n        r'|(?<=-)(%(space)s)+(?=--)'\n        r'|%(space)s+'\n        r'|(?:%(newline)s%(space)s*)+'\n    ) % locals()).sub\n    #print space_sub.__self__.pattern\n\n    def space_subber(match):\n        \"\"\" Substitution callback \"\"\"\n        # pylint: disable = C0321, R0911\n        groups = match.groups()\n        if groups[0]: return groups[0]\n        elif groups[1]: return groups[1]\n        elif groups[2]: return groups[2]\n        elif groups[3]: return groups[3]\n        elif groups[4]: return '\\n'\n        elif groups[5] or groups[6] or groups[7]: return ' '\n        else: return ''\n\n    def jsmin(script): # pylint: disable = W0621\n        r\"\"\"\n        Minify javascript based on `jsmin.c by Douglas Crockford`_\\.\n\n        Instead of parsing the stream char by char, it uses a regular\n        expression approach which minifies the whole script with one big\n        substitution regex.\n\n        .. _jsmin.c by Douglas Crockford:\n           http://www.crockford.com/javascript/jsmin.c\n\n        :Parameters:\n          `script` : ``str``\n            Script to minify\n\n        :Return: Minified script\n        :Rtype: ``str``\n        \"\"\"\n        return space_sub(space_subber, '\\n%s\\n' % script).strip()\n\n    return jsmin", "response": "Generate a jsmin version of the even\nTaxonomy."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_template(self, name):\n        filename = path.join(self.search_path, *[p for p in name.split('/')\n                                                 if p and p[0] != '.'])\n        if not path.exists(filename):\n            raise TemplateNotFound(name)\n        return Template.from_file(filename, self.encoding)", "response": "Get a template from a given name."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef render_to_string(self, *args, **kwargs):\n        try:\n            template_name, args = args[0], args[1:]\n        except IndexError:\n            raise TypeError('name of template required')\n        return self.get_template(template_name).render(*args, **kwargs)", "response": "Load and render a template into a unicode string."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_template(self, template_name):\n        try:\n            return self.loader.load(template_name, encoding=self.encoding)\n        except self.not_found_exception, e:\n            # catch the exception raised by Genshi, convert it into a werkzeug\n            # exception (for the sake of consistency)\n            raise TemplateNotFound(template_name)", "response": "Get the template which is at the given name"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload and render a template into an unicode string", "response": "def render_to_string(self, template_name, context=None):\n        \"\"\"Load and render a template into an unicode string\"\"\"\n        # create an empty context if no context was specified\n        context = context or {}\n        tmpl = self.get_template(template_name)\n        # render the template into a unicode string (None means unicode)\n        return tmpl. \\\n            generate(**context). \\\n            render(self.output_type, encoding=None)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate a JavaScript function that can be used to generate a map.", "response": "def generate_map(map, name='url_map'):\n    \"\"\"\n    Generates a JavaScript function containing the rules defined in\n    this map, to be used with a MapAdapter's generate_javascript\n    method.  If you don't pass a name the returned JavaScript code is\n    an expression that returns a function.  Otherwise it's a standalone\n    script that assigns the function with that name.  Dotted names are\n    resolved (so you an use a name like 'obj.url_for')\n\n    In order to use JavaScript generation, simplejson must be installed.\n\n    Note that using this feature will expose the rules\n    defined in your map to users. If your rules contain sensitive\n    information, don't use JavaScript generation!\n    \"\"\"\n    from warnings import warn\n    warn(DeprecationWarning('This module is deprecated'))\n    map.update()\n    rules = []\n    converters = []\n    for rule in map.iter_rules():\n        trace = [{\n            'is_dynamic':   is_dynamic,\n            'data':         data\n        } for is_dynamic, data in rule._trace]\n        rule_converters = {}\n        for key, converter in iteritems(rule._converters):\n            js_func = js_to_url_function(converter)\n            try:\n                index = converters.index(js_func)\n            except ValueError:\n                converters.append(js_func)\n                index = len(converters) - 1\n            rule_converters[key] = index\n        rules.append({\n            u'endpoint':    rule.endpoint,\n            u'arguments':   list(rule.arguments),\n            u'converters':  rule_converters,\n            u'trace':       trace,\n            u'defaults':    rule.defaults\n        })\n\n    return render_template(name_parts=name and name.split('.') or [],\n                           rules=dumps(rules),\n                           converters=converters)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef generate_adapter(adapter, name='url_for', map_name='url_map'):\n    values = {\n        u'server_name':     dumps(adapter.server_name),\n        u'script_name':     dumps(adapter.script_name),\n        u'subdomain':       dumps(adapter.subdomain),\n        u'url_scheme':      dumps(adapter.url_scheme),\n        u'name':            name,\n        u'map_name':        map_name\n    }\n    return u'''\\\nvar %(name)s = %(map_name)s(\n    %(server_name)s,\n    %(script_name)s,\n    %(subdomain)s,\n    %(url_scheme)s\n);''' % values", "response": "Generates the url building function for a map."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef js_to_url_function(converter):\n    if hasattr(converter, 'js_to_url_function'):\n        data = converter.js_to_url_function()\n    else:\n        for cls in getmro(type(converter)):\n            if cls in js_to_url_functions:\n                data = js_to_url_functions[cls](converter)\n                break\n        else:\n            return 'encodeURIComponent'\n    return '(function(value) { %s })' % data", "response": "Get the JavaScript converter function from a rule."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _warn_if_string(iterable):\n    if isinstance(iterable, string_types):\n        from warnings import warn\n        warn(Warning('response iterable was set to a string.  This appears '\n                     'to work but means that the server will send the '\n                     'data to the client char, by char.  This is almost '\n                     'never intended behavior, use response.data to assign '\n                     'strings to the response object.'), stacklevel=2)", "response": "Helper for the response objects to check if the iterable returned\n    to the WSGI server is not a string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a new request object based on the values provided.", "response": "def from_values(cls, *args, **kwargs):\n        \"\"\"Create a new request object based on the values provided.  If\n        environ is given missing values are filled from there.  This method is\n        useful for small scripts when you need to simulate a request from an URL.\n        Do not use this method for unittesting, there is a full featured client\n        object (:class:`Client`) that allows to create multipart requests,\n        support for cookies etc.\n\n        This accepts the same options as the\n        :class:`~werkzeug.test.EnvironBuilder`.\n\n        .. versionchanged:: 0.5\n           This method now accepts the same arguments as\n           :class:`~werkzeug.test.EnvironBuilder`.  Because of this the\n           `environ` parameter is now called `environ_overrides`.\n\n        :return: request object\n        \"\"\"\n        from werkzeug.test import EnvironBuilder\n        charset = kwargs.pop('charset', cls.charset)\n        builder = EnvironBuilder(*args, **kwargs)\n        try:\n            return builder.get_request(cls)\n        finally:\n            builder.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndecorates a function as responder that accepts the request as first argument.", "response": "def application(cls, f):\n        \"\"\"Decorate a function as responder that accepts the request as first\n        argument.  This works like the :func:`responder` decorator but the\n        function is passed the request object as first argument and the\n        request object will be closed automatically::\n\n            @Request.application\n            def my_wsgi_app(request):\n                return Response('Hello World!')\n\n        :param f: the WSGI callable to decorate\n        :return: a new WSGI callable\n        \"\"\"\n        #: return a callable that wraps the -2nd argument with the request\n        #: and calls the function with all the arguments up to that one and\n        #: the request.  The return value is then called with the latest\n        #: two arguments.  This makes it possible to use this decorator for\n        #: both methods and standalone WSGI functions.\n        def application(*args):\n            request = cls(args[-2])\n            with request:\n                return f(*args[:-2] + (request,))(*args[-2:])\n        return update_wrapper(application, f)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a file - like object for the file upload.", "response": "def _get_file_stream(self, total_content_length, content_type, filename=None,\n                        content_length=None):\n        \"\"\"Called to get a stream for the file upload.\n\n        This must provide a file-like class with `read()`, `readline()`\n        and `seek()` methods that is both writeable and readable.\n\n        The default implementation returns a temporary file if the total\n        content length is higher than 500KB.  Because many browsers do not\n        provide a content length for the files only the total content\n        length matters.\n\n        :param total_content_length: the total content length of all the\n                                     data in the request combined.  This value\n                                     is guaranteed to be there.\n        :param content_type: the mimetype of the uploaded file.\n        :param filename: the filename of the uploaded file.  May be `None`.\n        :param content_length: the length of this file.  This value is usually\n                               not provided because webbrowsers do not provide\n                               this value.\n        \"\"\"\n        return default_stream_factory(total_content_length, content_type,\n                                      filename, content_length)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates the form data parser class.", "response": "def make_form_data_parser(self):\n        \"\"\"Creates the form data parser.  Instanciates the\n        :attr:`form_data_parser_class` with some parameters.\n\n        .. versionadded:: 0.8\n        \"\"\"\n        return self.form_data_parser_class(self._get_file_stream,\n                                           self.charset,\n                                           self.encoding_errors,\n                                           self.max_form_memory_size,\n                                           self.max_content_length,\n                                           self.parameter_storage_class)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef close(self):\n        files = self.__dict__.get('files')\n        for key, value in iter_multi_items(files or ()):\n            value.close()", "response": "Closes all associated resources of this request object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef args(self):\n        return url_decode(wsgi_get_bytes(self.environ.get('QUERY_STRING', '')),\n                          self.url_charset, errors=self.encoding_errors,\n                          cls=self.parameter_storage_class)", "response": "The parsed URL parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the full path to the resource.", "response": "def full_path(self):\n        \"\"\"Requested path as unicode, including the query string.\"\"\"\n        return self.path + u'?' + to_unicode(self.query_string, self.url_charset)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_data(self, value):\n        # if an unicode string is set, it's encoded directly so that we\n        # can set the content length\n        if isinstance(value, text_type):\n            value = value.encode(self.charset)\n        else:\n            value = bytes(value)\n        self.response = [value]\n        if self.automatically_set_content_length:\n            self.headers['Content-Length'] = str(len(value))", "response": "Sets a new string as response."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef calculate_content_length(self):\n        try:\n            self._ensure_sequence()\n        except RuntimeError:\n            return None\n        return sum(len(x) for x in self.response)", "response": "Returns the content length if available or None otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _ensure_sequence(self, mutable=False):\n        if self.is_sequence:\n            # if we need a mutable object, we ensure it's a list.\n            if mutable and not isinstance(self.response, list):\n                self.response = list(self.response)\n            return\n        if self.direct_passthrough:\n            raise RuntimeError('Attempted implicit sequence conversion '\n                               'but the response object is in direct '\n                               'passthrough mode.')\n        if not self.implicit_sequence_conversion:\n            raise RuntimeError('The response object required the iterable '\n                               'to be a sequence, but the implicit '\n                               'conversion was disabled.  Call '\n                               'make_sequence() yourself.')\n        self.make_sequence()", "response": "Ensures that the response object is a sequence."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\niterate the response encoded with the encoding of the response.", "response": "def iter_encoded(self):\n        \"\"\"Iter the response encoded with the encoding of the response.\n        If the response object is invoked as WSGI application the return\n        value of this method is used as application iterator unless\n        :attr:`direct_passthrough` was activated.\n        \"\"\"\n        charset = self.charset\n        if __debug__:\n            _warn_if_string(self.response)\n        # Encode in a separate function so that self.response is fetched\n        # early.  This allows us to wrap the response with the return\n        # value from get_app_iter or iter_encoded.\n        return _iter_encoded(self.response, self.charset)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_cookie(self, key, value='', max_age=None, expires=None,\n                   path='/', domain=None, secure=None, httponly=False):\n        \"\"\"Sets a cookie. The parameters are the same as in the cookie `Morsel`\n        object in the Python standard library but it accepts unicode data, too.\n\n        :param key: the key (name) of the cookie to be set.\n        :param value: the value of the cookie.\n        :param max_age: should be a number of seconds, or `None` (default) if\n                        the cookie should last only as long as the client's\n                        browser session.\n        :param expires: should be a `datetime` object or UNIX timestamp.\n        :param domain: if you want to set a cross-domain cookie.  For example,\n                       ``domain=\".example.com\"`` will set a cookie that is\n                       readable by the domain ``www.example.com``,\n                       ``foo.example.com`` etc.  Otherwise, a cookie will only\n                       be readable by the domain that set it.\n        :param path: limits the cookie to a given path, per default it will\n                     span the whole domain.\n        \"\"\"\n        self.headers.add('Set-Cookie', dump_cookie(key, value, max_age,\n                         expires, path, domain, secure, httponly,\n                         self.charset))", "response": "Sets a cookie. The parameters are the same as in the cookie `Morsel`\n        object in the Python standard library but it accepts unicode data, too.\n\n        :param key: the key (name) of the cookie to be set.\n        :param value: the value of the cookie.\n        :param max_age: should be a number of seconds, or `None` (default) if\n                        the cookie should last only as long as the client's\n                        browser session.\n        :param expires: should be a `datetime` object or UNIX timestamp.\n        :param domain: if you want to set a cross-domain cookie.  For example,\n                       ``domain=\".example.com\"`` will set a cookie that is\n                       readable by the domain ``www.example.com``,\n                       ``foo.example.com`` etc.  Otherwise, a cookie will only\n                       be readable by the domain that set it.\n        :param path: limits the cookie to a given path, per default it will\n                     span the whole domain."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndeletes a cookie. Fails silently if key doesn t exist.", "response": "def delete_cookie(self, key, path='/', domain=None):\n        \"\"\"Delete a cookie.  Fails silently if key doesn't exist.\n\n        :param key: the key (name) of the cookie to be deleted.\n        :param path: if the cookie that should be deleted was limited to a\n                     path, the path has to be defined here.\n        :param domain: if the cookie that should be deleted was limited to a\n                       domain, that domain has to be defined here.\n        \"\"\"\n        self.set_cookie(key, expires=0, max_age=0, path=path, domain=domain)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef freeze(self):\n        # we explicitly set the length to a list of the *encoded* response\n        # iterator.  Even if the implicit sequence conversion is disabled.\n        self.response = list(self.iter_encoded())\n        self.headers['Content-Length'] = str(sum(map(len, self.response)))", "response": "This method is called when you want to make your response object ready for pickled for a specific locale."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_app_iter(self, environ):\n        status = self.status_code\n        if environ['REQUEST_METHOD'] == 'HEAD' or \\\n           100 <= status < 200 or status in (204, 304):\n            iterable = ()\n        elif self.direct_passthrough:\n            if __debug__:\n                _warn_if_string(self.response)\n            return self.response\n        else:\n            iterable = self.iter_encoded()\n        return ClosingIterator(iterable, self.close)", "response": "Returns the application iterator for the given WSGI environment."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cache_control(self):\n        def on_update(cache_control):\n            if not cache_control and 'cache-control' in self.headers:\n                del self.headers['cache-control']\n            elif cache_control:\n                self.headers['Cache-Control'] = cache_control.to_header()\n        return parse_cache_control_header(self.headers.get('cache-control'),\n                                          on_update,\n                                          ResponseCacheControl)", "response": "The Cache - Control general - header field is used to specify all caching mechanisms along the request and response chain."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef make_conditional(self, request_or_environ):\n        environ = _get_environ(request_or_environ)\n        if environ['REQUEST_METHOD'] in ('GET', 'HEAD'):\n            # if the date is not in the headers, add it now.  We however\n            # will not override an already existing header.  Unfortunately\n            # this header will be overriden by many WSGI servers including\n            # wsgiref.\n            if 'date' not in self.headers:\n                self.headers['Date'] = http_date()\n            if 'content-length' not in self.headers:\n                length = self.calculate_content_length()\n                if length is not None:\n                    self.headers['Content-Length'] = length\n            if not is_resource_modified(environ, self.headers.get('etag'), None,\n                                        self.headers.get('last-modified')):\n                self.status_code = 304\n        return self", "response": "This method will make the response conditional to the request."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef match(self, path_info=None, method=None, return_rule=False,\n              query_args=None):\n        \"\"\"The usage is simple: you just pass the match method the current\n        path info as well as the method (which defaults to `GET`).  The\n        following things can then happen:\n\n        - you receive a `NotFound` exception that indicates that no URL is\n          matching.  A `NotFound` exception is also a WSGI application you\n          can call to get a default page not found page (happens to be the\n          same object as `werkzeug.exceptions.NotFound`)\n\n        - you receive a `MethodNotAllowed` exception that indicates that there\n          is a match for this URL but not for the current request method.\n          This is useful for RESTful applications.\n\n        - you receive a `RequestRedirect` exception with a `new_url`\n          attribute.  This exception is used to notify you about a request\n          Werkzeug requests from your WSGI application.  This is for example the\n          case if you request ``/foo`` although the correct URL is ``/foo/``\n          You can use the `RequestRedirect` instance as response-like object\n          similar to all other subclasses of `HTTPException`.\n\n        - you get a tuple in the form ``(endpoint, arguments)`` if there is\n          a match (unless `return_rule` is True, in which case you get a tuple\n          in the form ``(rule, arguments)``)\n\n        If the path info is not passed to the match method the default path\n        info of the map is used (defaults to the root URL if not defined\n        explicitly).\n\n        All of the exceptions raised are subclasses of `HTTPException` so they\n        can be used as WSGI responses.  The will all render generic error or\n        redirect pages.\n\n        Here is a small example for matching:\n\n        >>> m = Map([\n        ...     Rule('/', endpoint='index'),\n        ...     Rule('/downloads/', endpoint='downloads/index'),\n        ...     Rule('/downloads/<int:id>', endpoint='downloads/show')\n        ... ])\n        >>> urls = m.bind(\"example.com\", \"/\")\n        >>> urls.match(\"/\", \"GET\")\n        ('index', {})\n        >>> urls.match(\"/downloads/42\")\n        ('downloads/show', {'id': 42})\n\n        And here is what happens on redirect and missing URLs:\n\n        >>> urls.match(\"/downloads\")\n        Traceback (most recent call last):\n          ...\n        RequestRedirect: http://example.com/downloads/\n        >>> urls.match(\"/missing\")\n        Traceback (most recent call last):\n          ...\n        NotFound: 404 Not Found\n\n        :param path_info: the path info to use for matching.  Overrides the\n                          path info specified on binding.\n        :param method: the HTTP method used for matching.  Overrides the\n                       method specified on binding.\n        :param return_rule: return the rule that matched instead of just the\n                            endpoint (defaults to `False`).\n        :param query_args: optional query arguments that are used for\n                           automatic redirects as string or dictionary.  It's\n                           currently not possible to use the query arguments\n                           for URL matching.\n\n        .. versionadded:: 0.6\n           `return_rule` was added.\n\n        .. versionadded:: 0.7\n           `query_args` was added.\n\n        .. versionchanged:: 0.8\n           `query_args` can now also be a string.\n        \"\"\"\n        self.map.update()\n        if path_info is None:\n            path_info = self.path_info\n        else:\n            path_info = to_unicode(path_info, self.map.charset)\n        if query_args is None:\n            query_args = self.query_args\n        method = (method or self.default_method).upper()\n\n        path = u'%s|/%s' % (self.map.host_matching and self.server_name or\n                            self.subdomain, path_info.lstrip('/'))\n\n        have_match_for = set()\n        for rule in self.map._rules:\n            try:\n                rv = rule.match(path)\n            except RequestSlash:\n                raise RequestRedirect(self.make_redirect_url(\n                    path_info + '/', query_args))\n            except RequestAliasRedirect as e:\n                raise RequestRedirect(self.make_alias_redirect_url(\n                    path, rule.endpoint, e.matched_values, method, query_args))\n            if rv is None:\n                continue\n            if rule.methods is not None and method not in rule.methods:\n                have_match_for.update(rule.methods)\n                continue\n\n            if self.map.redirect_defaults:\n                redirect_url = self.get_default_redirect(rule, method, rv,\n                                                         query_args)\n                if redirect_url is not None:\n                    raise RequestRedirect(redirect_url)\n\n            if rule.redirect_to is not None:\n                if isinstance(rule.redirect_to, string_types):\n                    def _handle_match(match):\n                        value = rv[match.group(1)]\n                        return rule._converters[match.group(1)].to_url(value)\n                    redirect_url = _simple_rule_re.sub(_handle_match,\n                                                       rule.redirect_to)\n                else:\n                    redirect_url = rule.redirect_to(self, **rv)\n                raise RequestRedirect(str(urljoin('%s://%s%s%s' % (\n                    self.url_scheme,\n                    self.subdomain and self.subdomain + '.' or '',\n                    self.server_name,\n                    self.script_name\n                ), redirect_url)))\n\n            if return_rule:\n                return rule, rv\n            else:\n                return rule.endpoint, rv\n\n        if have_match_for:\n            raise MethodNotAllowed(valid_methods=list(have_match_for))\n        raise NotFound()", "response": "This method is used to match a URL with a given method."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_host(self, domain_part):\n        if self.map.host_matching:\n            if domain_part is None:\n                return self.server_name\n            return to_unicode(domain_part, 'ascii')\n        subdomain = domain_part\n        if subdomain is None:\n            subdomain = self.subdomain\n        else:\n            subdomain = to_unicode(subdomain, 'ascii')\n        return (subdomain and subdomain + u'.' or u'') + self.server_name", "response": "Figures out the full host name for the given domain part."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef make_redirect_url(self, path_info, query_args=None, domain_part=None):\n        suffix = ''\n        if query_args:\n            suffix = '?' + self.encode_query_args(query_args)\n        return str('%s://%s/%s%s' % (\n            self.url_scheme,\n            self.get_host(domain_part),\n            posixpath.join(self.script_name[:-1].lstrip('/'),\n                           url_quote(path_info.lstrip('/'), self.map.charset,\n                                     safe='/:|+')),\n            suffix\n        ))", "response": "Creates a redirect URL."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef make_alias_redirect_url(self, path, endpoint, values, method, query_args):\n        url = self.build(endpoint, values, method, append_unknown=False,\n                         force_external=True)\n        if query_args:\n            url += '?' + self.encode_query_args(query_args)\n        assert url != path, 'detected invalid alias setting.  No canonical ' \\\n               'URL found'\n        return url", "response": "Internally called to make an alias redirect URL."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbuild a URL from the given endpoint and values and method.", "response": "def build(self, endpoint, values=None, method=None, force_external=False,\n              append_unknown=True):\n        \"\"\"Building URLs works pretty much the other way round.  Instead of\n        `match` you call `build` and pass it the endpoint and a dict of\n        arguments for the placeholders.\n\n        The `build` function also accepts an argument called `force_external`\n        which, if you set it to `True` will force external URLs. Per default\n        external URLs (include the server name) will only be used if the\n        target URL is on a different subdomain.\n\n        >>> m = Map([\n        ...     Rule('/', endpoint='index'),\n        ...     Rule('/downloads/', endpoint='downloads/index'),\n        ...     Rule('/downloads/<int:id>', endpoint='downloads/show')\n        ... ])\n        >>> urls = m.bind(\"example.com\", \"/\")\n        >>> urls.build(\"index\", {})\n        '/'\n        >>> urls.build(\"downloads/show\", {'id': 42})\n        '/downloads/42'\n        >>> urls.build(\"downloads/show\", {'id': 42}, force_external=True)\n        'http://example.com/downloads/42'\n\n        Because URLs cannot contain non ASCII data you will always get\n        bytestrings back.  Non ASCII characters are urlencoded with the\n        charset defined on the map instance.\n\n        Additional values are converted to unicode and appended to the URL as\n        URL querystring parameters:\n\n        >>> urls.build(\"index\", {'q': 'My Searchstring'})\n        '/?q=My+Searchstring'\n\n        If a rule does not exist when building a `BuildError` exception is\n        raised.\n\n        The build method accepts an argument called `method` which allows you\n        to specify the method you want to have an URL built for if you have\n        different methods for the same endpoint specified.\n\n        .. versionadded:: 0.6\n           the `append_unknown` parameter was added.\n\n        :param endpoint: the endpoint of the URL to build.\n        :param values: the values for the URL to build.  Unhandled values are\n                       appended to the URL as query parameters.\n        :param method: the HTTP method for the rule if there are different\n                       URLs for different methods on the same endpoint.\n        :param force_external: enforce full canonical external URLs.\n        :param append_unknown: unknown parameters are appended to the generated\n                               URL as query string argument.  Disable this\n                               if you want the builder to ignore those.\n        \"\"\"\n        self.map.update()\n        if values:\n            if isinstance(values, MultiDict):\n                valueiter = values.iteritems(multi=True)\n            else:\n                valueiter = iteritems(values)\n            values = dict((k, v) for k, v in valueiter if v is not None)\n        else:\n            values = {}\n\n        rv = self._partial_build(endpoint, values, method, append_unknown)\n        if rv is None:\n            raise BuildError(endpoint, values, method)\n        domain_part, path = rv\n\n        host = self.get_host(domain_part)\n\n        # shortcut this.\n        if not force_external and (\n            (self.map.host_matching and host == self.server_name) or\n            (not self.map.host_matching and domain_part == self.subdomain)):\n            return str(urljoin(self.script_name, './' + path.lstrip('/')))\n        return str('%s://%s%s/%s' % (\n            self.url_scheme,\n            host,\n            self.script_name[:-1],\n            path.lstrip('/')\n        ))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_file(cls, file, charset='utf-8', errors='strict',\n                  unicode_mode=True):\n        \"\"\"Load a template from a file.\n\n        .. versionchanged:: 0.5\n            The encoding parameter was renamed to charset.\n\n        :param file: a filename or file object to load the template from.\n        :param charset: the charset of the template to load.\n        :param errors: the error behavior of the charset decoding.\n        :param unicode_mode: set to `False` to disable unicode mode.\n        :return: a template\n        \"\"\"\n        close = False\n        f = file\n        if isinstance(file, basestring):\n            f = open(file, 'r')\n            close = True\n        try:\n            data = _decode_unicode(f.read(), charset, errors)\n        finally:\n            if close:\n                f.close()\n        return cls(data, getattr(f, 'name', '<template>'), charset,\n                   errors, unicode_mode)", "response": "Load a template from a file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef render(self, *args, **kwargs):\n        ns = self.default_context.copy()\n        if len(args) == 1 and isinstance(args[0], MultiDict):\n            ns.update(args[0].to_dict(flat=True))\n        else:\n            ns.update(dict(*args))\n        if kwargs:\n            ns.update(kwargs)\n        context = Context(ns, self.charset, self.errors)\n        exec self.code in context.runtime, context\n        return context.get_value(self.unicode_mode)", "response": "This function renders the template with the specified context."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef pbkdf2_hex(data, salt, iterations=DEFAULT_PBKDF2_ITERATIONS,\n               keylen=None, hashfunc=None):\n    \"\"\"Like :func:`pbkdf2_bin` but returns a hex encoded string.\n\n    .. versionadded:: 0.9\n\n    :param data: the data to derive.\n    :param salt: the salt for the derivation.\n    :param iterations: the number of iterations.\n    :param keylen: the length of the resulting key.  If not provided\n                   the digest size will be used.\n    :param hashfunc: the hash function to use.  This can either be the\n                     string name of a known hash function or a function\n                     from the hashlib module.  Defaults to sha1.\n    \"\"\"\n    rv = pbkdf2_bin(data, salt, iterations, keylen, hashfunc)\n    return to_native(codecs.encode(rv, 'hex_codec'))", "response": "Like pbkdf2_bin but returns a hex encoded string."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pbkdf2_bin(data, salt, iterations=DEFAULT_PBKDF2_ITERATIONS,\n               keylen=None, hashfunc=None):\n    \"\"\"Returns a binary digest for the PBKDF2 hash algorithm of `data`\n    with the given `salt`. It iterates `iterations` time and produces a\n    key of `keylen` bytes. By default SHA-1 is used as hash function,\n    a different hashlib `hashfunc` can be provided.\n\n    .. versionadded:: 0.9\n\n    :param data: the data to derive.\n    :param salt: the salt for the derivation.\n    :param iterations: the number of iterations.\n    :param keylen: the length of the resulting key.  If not provided\n                   the digest size will be used.\n    :param hashfunc: the hash function to use.  This can either be the\n                     string name of a known hash function or a function\n                     from the hashlib module.  Defaults to sha1.\n    \"\"\"\n    if isinstance(hashfunc, string_types):\n        hashfunc = _hash_funcs[hashfunc]\n    elif not hashfunc:\n        hashfunc = hashlib.sha1\n    salt = to_bytes(salt)\n    mac = hmac.HMAC(to_bytes(data), None, hashfunc)\n    if not keylen:\n        keylen = mac.digest_size\n    def _pseudorandom(x, mac=mac):\n        h = mac.copy()\n        h.update(x)\n        return bytearray(h.digest())\n    buf = bytearray()\n    for block in range_type(1, -(-keylen // mac.digest_size) + 1):\n        rv = u = _pseudorandom(salt + _pack_int(block))\n        for i in range_type(iterations - 1):\n            u = _pseudorandom(bytes(u))\n            rv = bytearray(starmap(xor, izip(rv, u)))\n        buf.extend(rv)\n    return bytes(buf[:keylen])", "response": "Returns a binary digest of the PBKDF2 hash algorithm of data with the given salt."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef safe_str_cmp(a, b):\n    if _builtin_safe_str_cmp is not None:\n        return _builtin_safe_str_cmp(a, b)\n    if len(a) != len(b):\n        return False\n    rv = 0\n    if isinstance(a, bytes) and isinstance(b, bytes) and not PY2:\n        for x, y in izip(a, b):\n            rv |= x ^ y\n    else:\n        for x, y in izip(a, b):\n            rv |= ord(x) ^ ord(y)\n    return rv == 0", "response": "This function compares two strings in somewhat constant time."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate a random string of specified length.", "response": "def gen_salt(length):\n    \"\"\"Generate a random string of SALT_CHARS with specified ``length``.\"\"\"\n    if length <= 0:\n        raise ValueError('requested salt of length <= 0')\n    return ''.join(_sys_rng.choice(SALT_CHARS) for _ in range_type(length))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate a hashed password with the given method and salt with the given length.", "response": "def generate_password_hash(password, method='pbkdf2:sha1', salt_length=8):\n    \"\"\"Hash a password with the given method and salt with with a string of\n    the given length.  The format of the string returned includes the method\n    that was used so that :func:`check_password_hash` can check the hash.\n\n    The format for the hashed string looks like this::\n\n        method$salt$hash\n\n    This method can **not** generate unsalted passwords but it is possible\n    to set the method to plain to enforce plaintext passwords.  If a salt\n    is used, hmac is used internally to salt the password.\n\n    If PBKDF2 is wanted it can be enabled by setting the method to\n    ``pbkdf2:method:iterations`` where iterations is optional::\n\n        pbkdf2:sha1:2000$salt$hash\n        pbkdf2:sha1$salt$hash\n\n    :param password: the password to hash\n    :param method: the hash method to use (one that hashlib supports), can\n                   optionally be in the format ``pbpdf2:<method>[:iterations]``\n                   to enable PBKDF2.\n    :param salt_length: the length of the salt in letters\n    \"\"\"\n    salt = method != 'plain' and gen_salt(salt_length) or ''\n    h, actual_method = _hash_internal(method, salt, password)\n    return '%s$%s$%s' % (actual_method, salt, h)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef make_form(fields=None, layout=None, layout_class=None, base_class=None,\n              get_form_field=None, name=None, rules=None, **kwargs):\n    \"\"\"\n    Make a from according dict data:\n\n    {'fields':[\n            {'name':'name', 'type':'str', 'label':'label,\n                'rules':{\n                        'required':\n                        'email'\n                        'required:back|front' #back means server side, front means front side\n                    }\n            ...},\n            ...\n        ],\n    #layout_class should be defined in settings.ini, just like\n    #[FORM_LAYOUT_CLASSES]\n    #bs3 = '#{appname}.form_help.Bootstrap3Layout'\n    #is also can be a Layout Class\n    #default is BootstrapLayout\n    'layout_class':'bs3',\n    'layout':{\n        'rows':[\n            '-- legend title --',\n            'field_name',\n            ['group_fieldname', 'group_fieldname']\n            {'name':'name', 'colspan':3}\n        ],\n    }\n    'base_class':'form class if not existed, then use Form'\n    }\n\n    get_form_field is a callback function, used to defined customized field class\n    if has name then it'll be cached\n    \"\"\"\n    from uliweb.utils.sorteddict import SortedDict\n\n    get_form_field = get_form_field or (lambda name, f:None)\n\n    #make fields\n    props = SortedDict({})\n    for f in fields or []:\n        if isinstance(f, BaseField):\n            props[f.name] = get_form_field(f.name, f) or f\n        else:\n            props[f['name']] = get_form_field(f['name'], f) or make_field(**f)\n\n    #set other props\n    if layout:\n        props['layout'] = layout\n    if layout_class:\n        props['layout_class'] = layout_class\n    if rules:\n        props['rules'] = rules\n    layout_class_args = kwargs.pop('layout_class_args', None)\n    if layout_class_args:\n        props['layout_class_args'] = layout_class_args\n\n    cls = type(name or 'MakeForm_', (base_class or Form,), props)\n    return cls", "response": "Make a form from according dict data."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef to_python(self, data):\n        if data is None:\n            return data\n        if self.datatype:\n            return self.datatype(data)\n        else:\n            return data", "response": "Convert a data to python format."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts the object to html value format.", "response": "def html(self, data='', py=True):\n        \"\"\"\n        Convert data to html value format.\n        \"\"\"\n\n        if py:\n            value = self.to_html(data)\n        else:\n            value = data\n        if self.static:\n            return str('<span class=\"value\">%s</span>' % safe_str(value))\n        else:\n            if self.hidden:\n                build = Hidden\n            else:\n                build = self.build\n            self._get_http_attrs()\n            return str(build(name=self.name, value=value, id=self.id, **self.html_attrs))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nvalidating the data for the current object.", "response": "def validate(self, data, all_data=None):\n        \"\"\"\n        if 'rule' in kwargs, then validate extra rules\n\n        e.g.:\n            rule= {'required':True, 'minlength':6}\n        \"\"\"\n\n        all_data = all_data or {}\n\n        if hasattr(data, 'stream'):\n            data.file = data.stream\n\n        if hasattr(data, 'file'):\n            if data.file:\n                v = data.filename\n            else:\n                raise Exception, 'Unsupport type %s' % type(data)\n        else:\n            v = data\n#        if v is None:\n\n        msg = TEST_NOT_EMPTY()(v)\n        if self.required:\n            if msg:\n                return False, msg\n        else:\n            if msg:\n                return True, self.default\n\n        try:\n            if isinstance(data, list):\n                v = []\n                for i in data:\n                    v.append(self.to_python(i))\n                data = v\n            else:\n                data = self.to_python(data)\n        except:\n            return False, unicode(ERR_CONVERT) % (data, self.__class__.__name__)\n        for v in self.get_validators():\n            msg = v(data, all_data)\n            if msg:\n                return False, msg\n        return True, data"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts a data to python format.", "response": "def to_python(self, data):\n        \"\"\"\n        Convert a data to python format.\n        \"\"\"\n        if data is None:\n            return ''\n        if isinstance(data, unicode):\n            data = data.encode(DEFAULT_ENCODING)\n        else:\n            data = str(data)\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_python(self, data):\n        if data is None:\n            return u''\n        if isinstance(data, unicode):\n            return data\n        else:\n            return unicode(data, DEFAULT_ENCODING)", "response": "Convert a data to python format."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts a data to python format.", "response": "def to_python(self, data):\n        \"\"\"\n        Convert a data to python format.\n        \"\"\"\n        if data is None:\n            return ''\n        if isinstance(data, self.datatype):\n            return data\n        if self.datatype is unicode:\n            return unicode(data, DEFAULT_ENCODING)\n        else:\n            return data.encode(DEFAULT_ENCODING)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cache_property(key, empty, type):\n    return property(lambda x: x._get_cache_value(key, empty, type),\n                    lambda x, v: x._set_cache_value(key, v, type),\n                    lambda x: x._del_cache_value(key),\n                    'accessor for %r' % key)", "response": "Return a property object for a cache header. Useful if you want to add support for a cache extension in a subclass."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a new file to the dict.", "response": "def add_file(self, name, file, filename=None, content_type=None):\n        \"\"\"Adds a new file to the dict.  `file` can be a file name or\n        a :class:`file`-like or a :class:`FileStorage` object.\n\n        :param name: the name of the field.\n        :param file: a filename or :class:`file`-like object\n        :param filename: an optional filename\n        :param content_type: an optional content type\n        \"\"\"\n        if isinstance(file, FileStorage):\n            value = file\n        else:\n            if isinstance(file, string_types):\n                if filename is None:\n                    filename = file\n                file = open(file, 'rb')\n            if filename and content_type is None:\n                content_type = mimetypes.guess_type(filename)[0] or \\\n                               'application/octet-stream'\n            value = FileStorage(file, filename, name, content_type)\n\n        self.add(name, value)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set(self, start, stop, length=None, units='bytes'):\n        assert is_byte_range_valid(start, stop, length), \\\n            'Bad range provided'\n        self._units = units\n        self._start = start\n        self._stop = stop\n        self._length = length\n        if self.on_update is not None:\n            self.on_update(self)", "response": "Simple method to update the ranges."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef qop(self):\n        def on_update(header_set):\n            if not header_set and 'qop' in self:\n                del self['qop']\n            elif header_set:\n                self['qop'] = header_set.to_header()\n        return parse_set_header(self.get('qop'), on_update)", "response": "Indicates what quality of protection the client has applied to\n        the message for HTTP digest auth auth."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the auth info and enable basic auth.", "response": "def set_basic(self, realm='authentication required'):\n        \"\"\"Clear the auth info and enable basic auth.\"\"\"\n        dict.clear(self)\n        dict.update(self, {'__auth_type__': 'basic', 'realm': realm})\n        if self.on_update:\n            self.on_update(self)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_connection(connection='', engine_name=None, connection_type='long', **args):\n    engine_name = engine_name or __default_engine__\n    if '://' in connection:\n        d = {\n            'connection_string':connection,\n            'connection_args':args,\n            'connection_type':connection_type,\n            }\n        \n        return engine_manager.add(engine_name, d).engine\n    else:\n        connection = connection or __default_engine__\n        if connection in engine_manager:\n            return engine_manager[connection].engine\n        else:\n            raise Error(\"Can't find engine %s\" % connection)", "response": "Returns an instance of a named engine or just return existed engine"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_metadata(engine_name=None):\n    dispatch.get(None, 'load_models')\n\n    engine = engine_manager[engine_name]\n    \n    for tablename, m in engine.models.items():\n        get_model(tablename, engine_name, signal=False)\n        if hasattr(m, '__dynamic__') and getattr(m, '__dynamic__'):\n            m.table.__mapping_only__ = True\n    return engine.metadata", "response": "get metadata according used for alembic\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_session(ec=None, create=True):\n    \n    ec = ec or __default_engine__\n    if isinstance(ec, (str, unicode)):\n        session = engine_manager[ec].session(create=True)\n    elif isinstance(ec, Session):\n        session = ec\n    else:\n        raise Error(\"Connection %r should be existed engine name or Session object\" % ec)\n    return session", "response": "get a session object from the specified engine or default engine"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a raw SQL query string", "response": "def rawsql(query, ec=None):\n    \"\"\"\n    ec could be engine name or engine instance\n    \"\"\"\n    if isinstance(query, Result):\n        query = query.get_query()\n\n    ec = ec or __default_engine__\n    if isinstance(ec, (str, unicode)):\n        engine = engine_manager[ec]\n        dialect = engine.engine.dialect\n    else:\n        dialect = ec.dialect\n    if isinstance(query, (str, unicode)):\n        return query\n    # comp = query.compile(dialect=dialect)\n\n    compiler = query._compiler(dialect)\n    class LiteralCompiler(compiler.__class__):\n        def visit_bindparam(\n                self, bindparam, within_columns_clause=False,\n                literal_binds=False, **kwargs\n        ):\n            return super(LiteralCompiler, self).render_literal_bindparam(\n                    bindparam, within_columns_clause=within_columns_clause,\n                    literal_binds=literal_binds, **kwargs\n            )\n        def render_literal_value(self, value, type_):\n            \"\"\"Render the value of a bind parameter as a quoted literal.\n\n            This is used for statement sections that do not accept bind paramters\n            on the target driver/database.\n\n            This should be implemented by subclasses using the quoting services\n            of the DBAPI.\n\n            \"\"\"\n            return repr_value(value)\n\n    compiler = LiteralCompiler(dialect, query)\n    return str(compiler.process(query)).replace('\\n', '')"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the name of an engine or session", "response": "def get_engine_name(ec=None):\n    \"\"\"\n    Get the name of a engine or session\n    \"\"\"\n    ec = ec or __default_engine__\n    if isinstance(ec, (str, unicode)):\n        return ec\n    elif isinstance(ec, Session):\n        return ec.engine_name\n    else:\n        raise Error(\"Parameter ec should be an engine_name or Session object, but %r found\" % ec)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef do_(query, ec=None, args=None):\n    from time import time\n    from uliweb.utils.common import get_caller\n\n    conn = get_session(ec)\n    b = time()\n    result = conn.execute(query, *(args or ()))\n    t = time() - b\n    dispatch.call(ec, 'post_do', query, conn, t)\n    \n    flag = False\n    sql = ''\n    if hasattr(Local, 'echo') and Local.echo:\n        if hasattr(Local, 'echo_args'):\n            _ec = Local.echo_args.get('session')\n        else:\n            _ec = None\n        engine_name = get_engine_name(ec)\n        _e = get_engine_name(_ec)\n        \n        if not _ec or _ec and _ec == _e:\n            if hasattr(Local, 'echo_args') and Local.echo_args['time']:\n                if t >= Local.echo_args['time']:\n                    sql = rawsql(query)\n                    \n                    flag = True\n            else:\n                sql = rawsql(query)\n                flag = True\n        \n        if flag:\n            print '\\n===>>>>> [%s]' % engine_name,\n            if hasattr(Local, 'echo_args') and Local.echo_args['caller']:\n                v = get_caller(skip=__file__)\n                print '(%s:%d:%s)' % v\n            else:\n                print\n            print sql+';'\n            if hasattr(Local, 'echo_args') and Local.echo_args['explain'] and sql:\n                r = conn.execute('explain '+sql).fetchone()\n                print '\\n----\\nExplain: %s' % ''.join([\"%s=%r, \" % (k, v) for k, v in r.items()])\n            print '===<<<<< time used %fs\\n' % t\n                \n    return result", "response": "Execute a query and print the result"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsave query result to a CSV file", "response": "def save_file(result, filename, encoding='utf8', headers=None,\n              convertors=None, visitor=None, writer=None, **kwargs):\n    \"\"\"\n    save query result to a csv file\n    visitor can used to convert values, all value should be convert to string\n    visitor function should be defined as:\n        def visitor(keys, values, encoding):\n            #return new values []\n    \n    convertors is used to convert single column value, for example:\n        \n        convertors = {'field1':convert_func1, 'fields2':convert_func2}\n        \n        def convert_func1(value, data):\n            value is value of field1\n            data is the record\n            \n    if visitor and convertors all provided, only visitor is available.\n    \n    headers used to convert column to a provided value\n    \"\"\"\n    import os\n    from uliweb.utils.common import simple_value\n    \n    convertors = convertors or {}\n    headers = headers or []\n    writer_class = Writer\n    if isinstance(filename, (str, unicode)):\n        ext = os.path.splitext(filename)[1]\n        if ext == '.csv':\n            writer_class = Writer\n        elif ext == '.dict':\n            writer_class = DictWriter\n        elif ext == '.xlsx':\n            writer_class = XlsxWriter\n\n    def convert(k, v, data):\n        f = convertors.get(k)\n        if f:\n            v = f(v, data)\n        return v\n\n    if isinstance(result, (str, unicode)):\n        result = text(safe_unicode(result))\n\n    if isinstance(result, (Select, TextClause)):\n        result = do_(result)\n    _header = []\n    for k in result.keys():\n        flag = False\n        for x in headers:\n            if x['name'] == k:\n                _header.append(x)\n                flag = True\n                break\n        if not flag:\n            _header.append({'name':k, 'title':k})\n\n    def _data():\n        for row in result:\n            if visitor and callable(visitor):\n                _row = visitor(result.keys(), row.values(), encoding)\n            else:\n                _row = [convert(k, v, row) for k, v in zip(result.keys(), row.values())]\n            yield _row\n\n    writer = writer_class(filename, header=_header, data=_data(), **kwargs)\n    writer.save()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef CommitAll(close=None):\n    if close:\n        warnings.simplefilter('default')\n        warnings.warn(\"close parameter will not need at all.\", DeprecationWarning)\n    \n    for k, v in engine_manager.items():\n        session = v.session(create=False)\n        if session:\n            session.commit()", "response": "Commits all transactions according Local. conn\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nregister a model and tablename to a global variable.", "response": "def set_model(model, tablename=None, created=None, appname=None, model_path=None):\n    \"\"\"\n    Register an model and tablename to a global variable.\n    model could be a string format, i.e., 'uliweb.contrib.auth.models.User'\n\n    :param appname: if no appname, then archive according to model\n\n    item structure\n        created\n        model\n        model_path\n        appname\n\n    For dynamic model you should pass model_path with '' value\n    \"\"\"\n    if isinstance(model, type) and issubclass(model, Model):\n        #use alias first\n        tablename = model._alias or model.tablename\n    tablename = tablename.lower()\n    #set global __models__\n    d = __models__.setdefault(tablename, {})\n    engines = d.get('config', {}).pop('engines', ['default'])\n    if isinstance(engines, (str, unicode)):\n        engines = [engines]\n    d['engines'] = engines\n    \n    item = {}\n    if created is not None:\n        item['created'] = created\n    else:\n        item['created'] = None\n    if isinstance(model, (str, unicode)):\n        if model_path is None:\n            model_path = model\n        else:\n            model_path = model_path\n        if not appname:\n            appname = model.rsplit('.', 2)[0]\n        #for example 'uliweb.contrib.auth.models.User'\n        model = None\n    else:\n        appname = model.__module__.rsplit('.', 1)[0]\n        if model_path is None:\n            model_path = model.__module__ + '.' + model.__name__\n        else:\n            model_path = ''\n        #for example 'uliweb.contrib.auth.models'\n        model.__engines__ = engines\n        \n    item['model'] = model\n    item['model_path'] = model_path\n    item['appname'] = appname\n    d['model_path'] = model_path\n    d['appname'] = appname\n\n    for name in engines:\n        if not isinstance(name, (str, unicode)):\n            raise BadValueError('Engine name should be string type, but %r found' % name)\n    \n        engine_manager[name].models[tablename] = item.copy()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a new model dynamically based on the given fields and indexes.", "response": "def create_model(modelname, fields, indexes=None, basemodel=None, **props):\n    \"\"\"\n    Create model dynamically\n\n    :param fields: Just format like [\n                        {'name':name, 'type':type, ...},\n                        ...\n                        ]\n                    type should be a string, eg. 'str', 'int', etc\n                    kwargs will be passed to Property.__init__() according field type,\n                    it'll be a dict\n    :param props: Model attributes, such as '__mapping_only__', '__replace__'\n    :param indexes: Multiple fields index, single index can be set directly using `index=True`\n                    to a field, the value format should be:\n\n                    [\n                        {'name':name, 'fields':[...], ...},\n                    ]\n\n                    e.g. [\n                        {'name':'audit_idx', 'fields':['table_id', 'obj_id']}\n                    ]\n\n                    for kwargs can be ommited.\n\n    :param basemodel: Will be the new Model base class, so new Model can inherited\n                    parent methods, it can be a string or a real class object\n    \"\"\"\n    assert not props or isinstance(props, dict)\n    assert not indexes or isinstance(indexes, list)\n\n    props = SortedDict(props or {})\n    props['__dynamic__'] = True\n    props['__config__'] = False\n\n    for p in fields:\n        kwargs = p.copy()\n        name = kwargs.pop('name')\n        _type = kwargs.pop('type')\n\n        #if the key is start with '_', then remove it\n        for k in kwargs.keys():\n            if k.startswith('_'):\n                kwargs.pop(k, None)\n\n        field_type = get_field_type(_type)\n        prop = field_type(**kwargs)\n        props[name] = prop\n\n    if basemodel:\n        model = import_attr(basemodel)\n        # model.clear_relation()\n    else:\n        model = Model\n    # try:\n    #     old = get_model(modelname, signal=False)\n    #     old.clear_relation()\n    # except ModelNotFound as e:\n    #     pass\n\n    cls = type(str(modelname.title()), (model,), props)\n\n    tablename = props.get('__tablename__', modelname)\n    set_model(cls, tablename, appname=__name__, model_path='')\n    get_model(modelname, signal=False, reload=True)\n\n    indexes = indexes or []\n    for x in indexes:\n        kwargs = x.copy()\n        name = kwargs.pop('name')\n        fields = kwargs.pop('fields')\n\n        #if the key is start with '_', then remove it\n        for k in kwargs.keys():\n            if k.startswith('_'):\n                kwargs.pop(k, None)\n\n        if not isinstance(fields, (list, tuple)):\n            raise ValueError(\"Index value format is not right, the value is %r\" % indexes)\n\n        props = []\n        for y in fields:\n            props.append(cls.c[y])\n\n        Index(name, *props, **kwargs)\n\n    return cls"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_metadata(model):\n    engine_name = model.get_engine_name()\n    engine = engine_manager[engine_name]\n    return engine.metadata", "response": "Find the metadata for the given model instance."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_model(model, engine_name=None, signal=True, reload=False):\n    if isinstance(model, type) and issubclass(model, Model):\n        return model\n    if not isinstance(model, (str, unicode)):\n        raise Error(\"Model %r should be string or unicode type\" % model)\n    \n    #make model name is lower case\n    model = model.lower()\n    model_item = __models__.get(model)\n    if not model_item:\n        model_item = dispatch.get(None, 'find_model', model_name=model)\n    if model_item:\n        if not engine_name:\n            #search according model_item, and it should has only one engine defined\n            engines = model_item['engines']\n            if len(engines) > 1:\n                engine_name = __default_engine__\n            else:\n                engine_name = engines[0]\n        engine = engine_manager[engine_name]\n\n        item = engine._models.get(model)\n        #process duplication\n        if not item and engine.options.duplication:\n            _item = engine.models.get(model)\n            if _item:\n                item = _item.copy()\n                item['model'] = None\n                engine._models[model] = item\n        if item:\n            loaded = False #True, model is already loaded, so consider if it needs be cached\n            m = item['model']\n            m_config = __models__[model].get('config', {})\n            if isinstance(m, type) and issubclass(m, Model):\n                loaded = True\n                if reload:\n                    loaded = False\n                #add get_model previous hook\n                if signal:\n                    model_inst = dispatch.get(None, 'get_model', model_name=model, model_inst=m,\n                                      model_info=item, model_config=m_config) or m\n                    if m is not model_inst:\n                        loaded = False\n                else:\n                    model_inst = m\n            else:\n                #add get_model previous hook\n                if signal:\n                    model_inst = dispatch.get(None, 'get_model', model_name=model, model_inst=None,\n                                      model_info=item, model_config=m_config)\n                else:\n                    model_inst = None\n                if not model_inst:\n                    if item['model_path']:\n                        mod_path, name = item['model_path'].rsplit('.', 1)\n                        mod = __import__(mod_path, fromlist=['*'])\n                        model_inst = getattr(mod, name)\n                    #empty model_path means dynamic model\n                if not model_inst:\n                    raise ModelNotFound(\"Can't found the model %s in engine %s\" % (model, engine_name))\n\n            if not loaded:\n                if model_inst._bound_classname == model and not reload:\n                    model_inst = model_inst._use(engine_name)\n                    item['model'] = model_inst\n                else:\n                    config = __models__[model].get('config', {})\n                    if config:\n                        for k, v in config.items():\n                            setattr(model_inst, k, v)\n                    item['model'] = model_inst\n                    model_inst._alias = model\n                    model_inst._engine_name = engine_name\n\n                    if __lazy_model_init__:\n                        for k, v in model_inst.properties.items():\n                            v.__property_config__(model_inst, k)\n\n                #add bind process\n                if reload:\n                    reset = True\n                else:\n                    reset = False\n                model_inst.bind(engine.metadata, reset=reset)\n\n            #post get_model\n            if signal:\n                dispatch.call(None, 'post_get_model', model_name=model, model_inst=model_inst,\n                                      model_info=item, model_config=m_config)\n            return model_inst\n            \n    raise ModelNotFound(\"Can't found the model %s in engine %s\" % (model, engine_name))", "response": "Return a real model object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_object(table, id=None, condition=None, cache=False, fields=None, use_local=False,\n               engine_name=None, session=None):\n    \"\"\"\n    Get obj in Local.object_caches first and also use get(cache=True) function if \n    not found in object_caches\n    \"\"\"\n    from uliweb import functions, settings\n    \n    model = get_model(table, engine_name)\n        \n    #if id is an object of Model, so get the real id value\n    if isinstance(id, Model):\n        return id\n      \n    if cache:\n        if use_local:\n            s = get_session(session)\n            key = get_object_id(s.engine_name, model.tablename, id)\n            value = s.get_local_cache(key)\n            if value:\n                return value\n        obj = model.get(id, condition=condition, fields=fields, cache=True)\n        if use_local:\n            value = s.get_local_cache(key, obj)\n    else:\n        obj = model.get(id, condition=condition, fields=fields)\n    \n    return obj", "response": "Get object from database."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef reflect_table_data(table, mapping=None, engine_name='default'):\n    table = reflect_table(table, engine_name)\n    mapping = mapping or {}\n\n    from uliweb.utils.sorteddict import SortedDict\n\n    field_type_map = {'VARCHAR':'str', 'VARCHAR2':'str', 'INTEGER':'int', 'FLOAT':'float'}\n\n    meta = {}\n    columns = SortedDict()\n    #write columns\n    _primary_key = None\n    for k, v in table.columns.items():\n        column_type = v.type\n        type_name = column_type.__class__.__name__.lower()\n        kwargs = SortedDict()\n        field_type = type_name.upper()\n\n        if type_name in ('char', 'varchar'):\n            kwargs['max_length'] = column_type.length\n        elif type_name in ('text', 'blob', 'integer', 'float', 'bigint'):\n            pass\n        elif type_name == 'long':\n            field_type = 'bigint'\n        elif type_name in ('clob',):\n            field_type = 'TEXT'\n        elif type_name in ('decimal', 'float'):\n            kwargs['precision'] = v.type.precision\n            kwargs['scale'] = v.type.scale\n        elif type_name == 'raw': #oracle\n            field_type = 'binary'\n            kwargs['max_length'] = column_type.length\n        elif type_name == 'number':\n            if v.type.scale:\n                kwargs['precision'] = v.type.precision\n                kwargs['scale'] = v.type.scale\n                field_type = 'DECIMAL'\n            else:\n                field_type = 'int'\n        elif type_name == 'numeric':\n            field_type = 'DECIMAL'\n            kwargs['precision'] = v.type.precision\n            kwargs['scale'] = v.type.scale\n        elif type_name in ('timestamp',):\n            field_type = 'TIMESTAMP'\n        elif type_name in ('datetime', 'date', 'time'):\n            pass\n        #for tinyint will be treated as bool\n        elif type_name in ('tinyint', 'boolean'):\n            field_type = 'bool'\n        else:\n            raise ValueError(\"Don't support column [{0}] for type [{1}] when parsing {2}\".format(k, type_name, table.name))\n\n        if v.primary_key:\n            kwargs['primary_key'] = True\n            _primary_key = k\n            if v.autoincrement:\n                kwargs['autoincrement'] = True\n        if not v.nullable:\n            kwargs['nullable'] = False\n        if v.server_default:\n            server_default = v.server_default.arg\n            kwargs['server_default'] = server_default\n        if v.index:\n            kwargs['index'] = True\n        if v.unique:\n            kwargs['unique'] = True\n        if sa_version >= '1.2' and v.comment:\n            kwargs['verbose_name'] = v.comment\n\n        #convert field_type to common python data type\n        field_type = field_type_map.get(field_type, field_type)\n        columns[k] = field_type, kwargs\n\n    meta['columns'] = columns\n\n    indexes = []\n    indexes_names = []\n    for index in table.indexes:\n        cols = list(index.columns)\n        _len = len(cols)\n        #if only one column it'll be set to Property\n        if _len == 1:\n            column_name = cols[0].name\n            d = {'index':True}\n            if index.unique:\n                d['unique'] = index.unique\n            columns[column_name][1].update(d)\n        else:\n            if not index.name in indexes_names:\n                indexes.append({'name':index.name, 'columns':[x.name for x in index.columns],\n                            'unique':index.unique})\n                indexes_names.append(index.name)\n\n    meta['indexes'] = indexes\n    return meta", "response": "Reflect a table to a Model dict"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef reflect_table_model(table, mapping=None, without_id=False, engine_name='default'):\n    table = reflect_table(table, engine_name)\n    mapping = mapping or {}\n    meta = reflect_table_data(table)\n\n    code = ['class {}(Model):'.format(table.name.title())]\n    code.append('''    \"\"\"\n    Description:\n    \"\"\"\n\n    __tablename__ = '{}\\''''.format(table.name))\n\n    if sa_version >= '1.2' and table.comment:\n        code.append('    __verbose_name__ = {}\\n'.format(dumps(table.comment, bool_int=False)))\n\n    #process id\n    if 'id' not in meta['columns'] and without_id:\n        code.append('    __without_id__ = True\\n')\n    # if _primary_key:\n    #     code.append('    _primary_field = {}'.format(_primary_key))\n\n    #output columns text\n    for k, v in meta['columns'].items():\n        kw = v[1].items()\n        x_v = mapping.get(v[0])\n        kwargs = ', '.join([v[0]] + ['{0}={1}'.format(x, dumps(y, bool_int=False)) for x, y in kw])\n        if x_v:\n            type_class = ' ,type_class={}'.format(x_v)\n        else:\n            type_class = ''\n        txt = \" \"*4 + \"{0} = Field({1}{2})\".format(k, kwargs, type_class)\n        code.append(txt)\n\n    #output index text\n    if meta['indexes']:\n        code.append(\"\"\"\n    @classmethod\n    def OnInit(cls):\"\"\")\n    for index in meta['indexes']:\n        buf = []\n        buf.append(index['name'])\n        for c in index['columns']:\n            buf.append('cls.c.{}'.format(c))\n        if index['unique']:\n            buf.append('unique=True')\n        code.append(' '*8 + 'Index({})'.format(', '.join(buf)))\n\n    return '\\n'.join(code)", "response": "Write table to model class"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef migrate_tables(tables, engine_name=None):\n    from alembic.migration import MigrationContext\n    engine = engine_manager[engine_name]\n    mc = MigrationContext.configure(engine.session().connection)\n    script =  get_migrate_script(mc, tables, engine.metadata)\n    run_migrate_script(mc, script)", "response": "Used to migrate dynamic tables to database"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef SelfReferenceProperty(label=None, collection_name=None, **attrs):\n    if 'reference_class' in attrs:\n        raise ConfigurationError(\n                'Do not provide reference_class to self-reference.')\n    return ReferenceProperty(_SELF_REFERENCE, label, collection_name, **attrs)", "response": "Create a self - reference property."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef session(self, create=True):\n        if hasattr(self.local, 'session'):\n            return self.local.session\n        else:\n            if create:\n                s = Session(self.name)\n                self.local.session = s\n                return s", "response": "Used to create default session"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting parameters and it ll used for Model. relationship clone", "response": "def get_parameters(self):\n        \"\"\"\n        Get common attributes and it'll used for Model.relationship clone process\n        \"\"\"\n        d = {}\n        for k in ['label', 'verbose_name', 'required', 'hint', 'placeholder', 'choices',\n            'default', 'validators', 'max_length']:\n            d[k] = getattr(self, k)\n        return d"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef validate(self, value):\n        if value == '':\n            if self.kwargs.get('nullable', __nullable__):\n                value = None\n            else:\n                value = 0\n            \n        if not isinstance(value, Model):\n            return super(ReferenceProperty, self).validate(value)\n\n        if not value.is_saved():\n            raise BadValueError(\n                    '%s instance must be saved before it can be stored as a '\n                    'reference' % self.reference_class.__class__.__name__)\n        if not isinstance(value, self.reference_class):\n            raise KindError('Property %s must be an instance of %s' %\n                    (self.name, self.reference_class.__class__.__name__))\n\n        return value", "response": "Validate reference.\n\n        Returns:\n            A valid value.\n\n        Raises:\n            BadValueError for the following reasons:\n                - Value is not saved.\n                - Object not of correct model type for reference."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the list of fields according self. columns", "response": "def get_fields(self):\n        \"\"\"\n        get property instance according self.columns\n        \"\"\"\n        columns = self.columns\n        model = self.model\n        fields = []\n        for col in columns:\n            if isinstance(col, (str, unicode)):\n                v = col.split('.')\n                if len(v) > 1:\n                    field = get_model(v[0], engine_name=self.model.get_engine_name(),\n                                      signal=False).properties(v[1])\n                else:\n                    field = model.properties[col]\n            elif isinstance(col, Column):\n                field = get_model(col.table.name, engine_name=self.model.get_engine_name(),\n                                  signal=False).properties[col.name]\n            else:\n                field = col\n            \n            fields.append(field)\n        \n        return fields"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef count(self):\n        if self._group_by or self._join or self.distinct_field:\n            return self.do_(self.get_query().limit(None).order_by(None).offset(None).alias().count()).scalar()\n        else:\n            return self.do_(self.get_query().with_only_columns([func.count()]).limit(None).order_by(None).offset(None)).scalar()", "response": "Returns the count of the items in the database."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfilter the list of related resources based on a condition.", "response": "def filter(self, *condition):\n        \"\"\"\n        If there are multple condition, then treats them *and* relastion.\n        \"\"\"\n        if not condition:\n            return self\n        cond = true()\n        for c in condition:\n            if c is not None:\n                if isinstance(c, (str, unicode)):\n                    c = text(c)\n                cond = and_(c, cond)\n        if self.condition is not None:\n            self.condition = and_(cond, self.condition)\n        else:\n            self.condition = cond\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef distinct(self, field=None):\n        if field is None:\n            self.funcs.append(('distinct', (), {}))\n        else:\n            self.distinct_field = field\n        return self", "response": "Add a distinct function to the query"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nexecutes update table set field = field + 1 like statement", "response": "def update(self, **kwargs):\n        \"\"\"\n        Execute update table set field = field+1 like statement\n        \"\"\"\n        if self.condition is not None:\n            self.result = self.do_(self.model.table.update().where(self.condition).values(**kwargs))\n        else:\n            self.result = self.do_(self.model.table.update().values(**kwargs))\n        return self.result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef save_file(self, filename, encoding='utf8', headers=None,\n                  convertors=None, display=True, **kwargs):\n        \"\"\"\n        save result to a csv file.\n        display = True will convert value according choices value\n        \"\"\"\n        global save_file\n        \n        convertors = convertors or {}\n        headers = headers or []\n        \n        fields = self.get_fields()\n        _header = []\n        for i, column in enumerate(fields):\n            if column.name not in convertors:\n                if display:\n                    def f(value, data):\n                        return column.get_display_value(value)\n\n                    convertors[column.name] = f\n            flag = False\n            for j in headers:\n                if not isinstance(j, dict):\n                    raise ValueError(\"Header should be a list of dict, but {} type found\".format(type(j)))\n                if j['name'] == column.name:\n                    _header.append(j)\n                    flag = True\n                    break\n            if not flag:\n                d = {'name':column.name}\n                if display:\n                    d['title'] = column.verbose_name or column.name\n                else:\n                    d['title'] = column.name\n                _header.append(d)\n\n        return save_file(self.run(), filename, encoding=encoding,\n                         headers=_header, convertors=convertors, **kwargs)", "response": "save result to a csv file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nclear the third relationship table but not the ModelA or ModelB.", "response": "def clear(self, *objs):\n        \"\"\"\n        Clear the third relationship table, but not the ModelA or ModelB\n        \"\"\"\n        if objs:\n            keys = get_objs_columns(objs)\n            self.do_(self.model.table.delete(self.condition & self.model.table.c[self.model._primary_field].in_(keys)))\n        else:\n            self.do_(self.model.table.delete(self.condition))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets all objects in the database", "response": "def all(self, cache=False):\n        \"\"\"\n        can use cache to return objects\n        \"\"\"\n        if cache:\n            return [get_object(self.modelb, obj_id, cache=True, use_local=True) for obj_id in self.keys(True)]\n        else:\n            return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef update(self, *objs):\n        keys = self.keys()\n        new_keys = get_objs_columns(objs, self.realfieldb)\n\n        modified = False\n        for v in new_keys:\n            if v in keys:    #the id has been existed, so don't insert new record\n                keys.remove(v)\n            else:\n                d = {self.fielda:self.valuea, self.fieldb:v}\n                if self.before_save:\n                    self.before_save(d)\n                if self.through_model:\n                    obj = self.through_model(**d)\n                    obj.save()\n                else:\n                    self.do_(self.table.insert().values(**d))\n                modified = True\n                \n        if keys: #if there are still keys, so delete them\n            self.clear(*keys)\n            modified = True\n        \n        #cache [] to _STORED_attr_name\n        setattr(self.instance, self.store_key, new_keys)\n        \n        return modified", "response": "Update the third relationship table but not the ModelA or ModelB."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nclearing the third relationship table but not the ModelA or ModelB.", "response": "def clear(self, *objs):\n        \"\"\"\n        Clear the third relationship table, but not the ModelA or ModelB\n        \"\"\"\n        if objs:\n            keys = get_objs_columns(objs, self.realfieldb)\n            self.do_(self.table.delete(self.get_default_condition() & (self.table.c[self.fieldb].in_(keys))))\n        else:\n            self.do_(self.table.delete(self.get_default_condition()))\n        #cache [] to _STORED_attr_name\n        setattr(self.instance, self.store_key, Lazy)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef with_relation(self, relation_name=None):\n        if not relation_name:\n            relation_name = 'relation'\n        if hasattr(self.modelb, relation_name):\n            raise Error(\"The attribute name %s has already existed in Model %s!\" % (relation_name, self.modelb.__name__))\n        if not self.through_model:\n            raise Error(\"Only with through style in ManyToMany supports with_relation function of Model %s!\" % self.modelb.__name__)\n        self.with_relation_name = relation_name\n        return self", "response": "Set the relation name for this manytomany object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_value_for_datastore(self, model_instance, cached=False):\n        value = getattr(model_instance, self._attr_name(), None)\n        if not cached:\n            value = getattr(model_instance, self.property_name).keys()\n            setattr(model_instance, self._attr_name(), value)\n        return value", "response": "Get the value of the attribute from the datastore."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef in_(self, *objs):\n        if not objs:\n            return self.table.c[self.fielda]!=self.table.c[self.fielda]\n        else:\n            keys = get_objs_columns(objs, self.reference_fieldname)\n            sub_query = select([self.table.c[self.fielda]], (self.table.c[self.fieldb] == self.reference_class.c[self.reference_fieldname]) & (self.table.c[self.fieldb].in_(keys)))\n            condition = self.model_class.c[self.reversed_fieldname].in_(sub_query)\n            return condition", "response": "Create a condition that returns True if the related object is in the specified list of objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef join_in(self, *objs):\n        if not objs:\n            return self.table.c[self.fielda]!=self.table.c[self.fielda]\n        else:\n            keys = get_objs_columns(objs, self.reference_fieldname)\n            return (self.table.c[self.fielda] == self.model_class.c[self.reversed_fieldname]) & (self.table.c[self.fieldb].in_(keys))", "response": "Create a join condition connect A and C\n           "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef join_right_in(self, *objs):\n        if not objs:\n            return self.table.c[self.fielda]!=self.table.c[self.fielda]\n        else:\n            keys = get_objs_columns(objs, self.reference_fieldname)\n            return (self.table.c[self.fieldb] == self.reference_class.c[self.reference_fieldname]) & (self.table.c[self.fielda].in_(keys))", "response": "Create a join condition connect B and C\n           "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef to_display(self, fields=None, manytomany=False, dicttype=dict, prefix=''):\n        d = dicttype()\n\n        fields = fields or self.properties.keys()\n        for k in fields:\n            v = self.properties.get(k)\n            if not v: continue\n            if prefix:\n                field_name = prefix+k\n            else:\n                field_name = k\n            if not isinstance(v, ManyToMany):\n                field_value = getattr(self, k)\n                d[k] = v.get_value_for_datastore(self)\n                d[field_name] = self.get_display_value(k, field_value)\n            else:\n                if manytomany:\n                    field_value = getattr(self, v._lazy_value(), [])\n                    d[k] = getattr(self, v._lazy_value(), [])\n                    d[field_name] = u', '.join([unicode(x) in field_value])\n        return d", "response": "Create display dict for instance of Model\n           "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_data(self, fields=None, compare=True):\n        fields = fields or []\n        if self._key is None or self._key == '' or self._key == 0:\n            d = {}\n            for k, v in self.properties.items():\n                #test fields\n                if fields and k not in fields:\n                    continue\n#                if not isinstance(v, ManyToMany):\n                if v.property_type == 'compound':\n                    continue\n                if v.sequence:\n                    continue\n                if not isinstance(v, ManyToMany):\n                    x = v.get_value_for_datastore(self)\n                    if isinstance(x, Model):\n                        x = x._key\n                    elif x is None or (k==self._primary_field and not x):\n                        if isinstance(v, DateTimeProperty) and v.auto_now_add:\n                            x = v.now()\n                        elif (v.auto_add or (not v.auto and not v.auto_add)):\n                            x = v.default_value()\n                else:\n                    x = v.get_value_for_datastore(self, cached=True)\n                if x is not None and not x is Lazy:\n                    d[k] = x\n        else:\n            d = {}\n            d[self._primary_field] = self._key\n            for k, v in self.properties.items():\n                if fields and k not in fields:\n                    continue\n                if v.property_type == 'compound':\n                    continue\n                t = self._old_values.get(k, None)\n                if not isinstance(v, ManyToMany):\n                    x = v.get_value_for_datastore(self)\n                    if isinstance(x, Model):\n                        x = x._key\n                else:\n                    x = v.get_value_for_datastore(self, cached=True)\n                if not x is Lazy:\n                    if (compare and t != self.field_str(x)) or not compare:\n                        d[k] = x\n        \n        return d", "response": "Get the data for the object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsaving the object to the database.", "response": "def save(self, insert=False, changed=None, saved=None,\n            send_dispatch=True, version=False, version_fieldname=None, \n            version_exception=True):\n        \"\"\"\n        If insert=True, then it'll use insert() indead of update()\n        \n        changed will be callback function, only when the non manytomany properties\n        are saved, the signature is:\n            \n            def changed(obj, created, old_data, diff_data):\n                if flag is true, then it means the record is changed\n                you can change new_data, and the new_data will be saved to database\n                \n        version = Optimistic Concurrency Control\n        version_fieldname default is 'version'\n        if check_many, it'll auto check if manytomany value need to save, \n        only available in UPDATE\n        \"\"\"\n        _saved = False\n        created = False\n        version_fieldname = version_fieldname or 'version'\n        d = self._get_data()\n        #fix when d is empty, orm will not insert record bug 2013/04/07\n        if d or not self._saved or insert:\n            _id = d.get(self._primary_field, None)\n            if insert or not self._saved or not _id:\n                created = True\n                old = d.copy()\n                \n                if get_dispatch_send() and self.__dispatch_enabled__:\n                    dispatch.call(self.__class__, 'pre_save', instance=self, created=True, data=d, old_data=self._old_values, signal=self.tablename)\n                \n                #process auto_now_add\n                _manytomany = {}\n                for k, v in self.properties.items():\n                    if v.property_type == 'compound':\n                        continue\n                    if not isinstance(v, ManyToMany):\n                        if isinstance(v, DateTimeProperty) and v.auto_now_add and k not in d:\n                            d[k] = v.now()\n                        elif (not k in d) and v.auto_add:\n                            d[k] = v.default_value()\n                    else:\n                        if k in d:\n                            _manytomany[k] = d.pop(k)\n                if d:\n                    if callable(changed):\n                        changed(self, created, self._old_values, d)\n                        old.update(d)\n                    obj = do_(self.table.insert().values(**d), self.get_session())\n                    _saved = True\n\n                    if obj.inserted_primary_key and self._primary_field:\n                        setattr(self, self._primary_field, obj.inserted_primary_key[0])\n\n                if _manytomany:\n                    for k, v in _manytomany.items():\n                        if v:\n                            _saved = getattr(self, k).update(v) or _saved\n                \n            else:\n                _id = d.pop(self._primary_field)\n                if d:\n                    old = d.copy()\n                    if get_dispatch_send() and self.__dispatch_enabled__:\n                        dispatch.call(self.__class__, 'pre_save', instance=self, created=False, data=d, old_data=self._old_values, signal=self.tablename)\n\n                    #process auto_now\n                    _manytomany = {}\n                    for k, v in self.properties.items():\n                        if v.property_type == 'compound' or k == self._primary_field:\n                            continue\n                        if not isinstance(v, ManyToMany):\n                            if isinstance(v, DateTimeProperty) and v.auto_now and k not in d:\n                                d[k] = v.now()\n                            elif (not k in d) and v.auto:\n                                d[k] = v.default_value()\n                        else:\n                            if k in d:\n                                _manytomany[k] = d.pop(k)\n                    if d:\n                        _cond = self.table.c[self._primary_field] == self._key\n                        if version:\n                            version_field = self.table.c.get(version_fieldname)\n                            if version_field is None:\n                                raise KindError(\"version_fieldname %s is not existed in Model %s\" % (version_fieldname, self.__class__.__name__))\n                            _version_value = getattr(self, version_fieldname, 0)\n                            # setattr(self, version_fieldname, _version_value+1)\n                            d[version_fieldname] = _version_value+1\n                            _cond = (version_field == _version_value) & _cond\n                            \n                        if callable(changed):\n                            changed(self, created, self._old_values, d)\n                            old.update(d)\n                        result = do_(self.table.update(_cond).values(**d), self.get_session())\n                        _saved = True\n                        if version:\n                            if result.rowcount == 0:\n                                _saved = False\n                                if version_exception:\n                                    raise SaveError(\"The record {0}:{1} has been saved by others, current version is {2}\".format(\n                                        self.tablename, self._key, _version_value))\n                            else:\n                                setattr(self, version_fieldname, d[version_fieldname])\n                        elif result.rowcount == 0:\n                            _saved = False\n                            # raise NotFound(\"The record can't be found!\", self.tablename, self._key)\n                      \n                    if _manytomany:\n                        for k, v in _manytomany.items():\n                            if v is not None:\n                                _saved = getattr(self, k).update(v) or _saved\n\n                # else:\n                #     #check if the field is primary_key, true will raise Exception\n                #     #2015/11/20 limodou\n                #     raise ValueError(\"You can't only change primary key '{}'.\".format(self._primary_field))\n\n\n            if _saved:\n                for k, v in d.items():\n                    x = self.properties[k].get_value_for_datastore(self)\n                    if self.field_str(x) != self.field_str(v):\n                        setattr(self, k, v)\n                if send_dispatch and get_dispatch_send() and self.__dispatch_enabled__:\n                    dispatch.call(self.__class__, 'post_save', instance=self, created=created, data=old, old_data=self._old_values, signal=self.tablename)\n                self.set_saved()\n                \n                if callable(saved):\n                    saved(self, created, self._old_values, old)\n\n        return _saved"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndeletes the current object from the database.", "response": "def delete(self, manytomany=True, delete_fieldname=None, send_dispatch=True,\n               onetoone=True, **kwargs):\n        \"\"\"\n        Delete current obj\n        :param manytomany: if also delete all manytomany relationships\n        :param delete_fieldname: if True then it'll use 'deleted', others will \n        be the property name\n        \"\"\"\n        if get_dispatch_send() and self.__dispatch_enabled__:\n            dispatch.call(self.__class__, 'pre_delete', instance=self, signal=self.tablename)\n        if manytomany:\n            for k, v in self._manytomany.items():\n                getattr(self, k).clear()\n        if onetoone:\n            for k, v in self._onetoone.items():\n                row = getattr(self, k)\n                if row:\n                    row.delete()\n        if delete_fieldname:\n            if delete_fieldname is True:\n                delete_fieldname = 'deleted'\n            if not hasattr(self, delete_fieldname):\n                raise KeyError(\"There is no %s property exists\" % delete_fieldname)\n            setattr(self, delete_fieldname, True)\n            self.save()\n        else:\n            do_(self.table.delete(self.table.c[self._primary_field]==self._key), self.get_session())\n            self._key = None\n            self._old_values = {}\n        if send_dispatch and get_dispatch_send() and self.__dispatch_enabled__:\n            dispatch.call(self.__class__, 'post_delete', instance=self, signal=self.tablename)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates sql statement for manytomany record.", "response": "def create_sql(self, insert=False, version=False, version_fieldname=None,\n                   fields=None, ec=None, compare=False):\n        \"\"\"\n        Create sql statement, do not process manytomany\n        \"\"\"\n        version_fieldname = version_fieldname or 'version'\n        #fix when d is empty, orm will not insert record bug 2013/04/07\n        if not self._key or insert:\n            d = self._get_data(fields, compare=compare)\n            if d:\n                return rawsql(self.table.insert().values(**d),\n                              ec or self.get_engine_name()) + ';'\n\n        else:\n            d = self._get_data(fields, compare=compare)\n            _key = d.pop(self._primary_field)\n            if d:\n                _cond = self.table.c[self._primary_field] == self._key\n                if version:\n                    version_field = self.table.c.get(version_fieldname)\n                    if version_field is None:\n                        raise KindError(\"version_fieldname %s is not existed in Model %s\" % (version_fieldname, self.__class__.__name__))\n                    _version_value = getattr(self, version_fieldname, 0)\n                    # setattr(self, version_fieldname, _version_value+1)\n                    d[version_fieldname] = _version_value+1\n                    _cond = (version_field == _version_value) & _cond\n\n                return rawsql(self.table.update(_cond).values(**d),\n                              ec or self.get_engine_name()) + ';'\n        return ''"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets reference collection_name, if the collection_name is None then make sure the collection_name is not conflict, but if the collection_name is not None, then check if the collection_name is already exists, if existed then raise Exception.", "response": "def get_collection_name(cls, from_class_name, collection_name=None, prefix=None):\n        \"\"\"\n        Get reference collection_name, if the collection_name is None\n        then make sure the collection_name is not conflict, but\n        if the collection_name is not None, then check if the collection_name\n        is already exists, if existed then raise Exception.\n        \"\"\"\n        if not collection_name:\n            collection_name = prefix + '_set'\n            if hasattr(cls, collection_name):\n                #if the xxx_set is already existed, then automatically\n                #create unique collection_set id\n                collection_name = prefix + '_set_' + str(cls._collection_set_id)\n                cls._collection_set_id += 1\n        else:\n            if collection_name in cls._collection_names:\n                if cls._collection_names.get(collection_name) != from_class_name:\n                    raise DuplicatePropertyError(\"Model %s already has collection property %s\" % (cls.__name__, collection_name))\n            #add property check\n            if collection_name in cls.properties:\n                raise DuplicatePropertyError(\"Model %s already has property %s\" % (cls.__name__, collection_name))\n        return collection_name"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a new ConnectModel underly implement of use .", "response": "def _use(cls, ec):\n        \"\"\"\n        underly implement of use\n        \"\"\"\n        # class ConnectModel(cls):\n        #     pass\n        ConnectModel = type(cls.__name__, (cls,), {})\n\n        ConnectModel.tablename = cls.tablename\n        ConnectModel._base_class = cls\n        if isinstance(ec, (str, unicode)):\n            ConnectModel._engine_name = ec\n        elif isinstance(ec, Session):\n            ConnectModel._engine_name = ec.engine_name\n            ConnectModel._connection = ec\n        return ConnectModel"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nuses will duplicate a new Model class and bind ec ec is Engine name or Sesstion object", "response": "def use(cls, ec):\n        \"\"\"\n        use will duplicate a new Model class and bind ec\n        \n        ec is Engine name or Sesstion object\n        \"\"\"\n        \n        if isinstance(ec, (str, unicode)):\n            m = get_model(cls._alias, ec, signal=False)\n        else:\n            m = cls._use(ec)\n        return m"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget object from Model.", "response": "def get(cls, id=None, condition=None, fields=None, cache=False, engine_name=None, **kwargs):\n        \"\"\"\n        Get object from Model, if given fields, then only fields will be loaded\n        into object, other properties will be Lazy\n        \n        if cache is True or defined __cacheable__=True in Model class, it'll use cache first\n        \"\"\"\n        \n        if id is None and condition is None:\n            return None\n        \n        can_cacheable = (cache or getattr(cls, '__cacheable__', None)) and \\\n            isinstance(id, (int, long, str, unicode))\n        if can_cacheable:\n            #send 'get_object' topic to get cached object\n            obj = dispatch.get(cls, 'get_object', id)\n            if obj:\n                return obj\n\n        if condition is not None:\n            _cond = condition\n        else:\n            if is_condition(id):\n                _cond = id\n            else:\n                _cond = cls.c[cls._primary_field] == id\n            #todo\n            # if isinstance(id, (int, long)):\n            #     _cond = cls.c.id==id\n            # elif isinstance(id, (str, unicode)) and id.isdigit():\n            #     _cond = cls.c.id==int(id)\n            # else:\n            #     _cond = id\n\n        #if there is no cached object, then just fetch from database\n        obj = cls.filter(_cond, **kwargs).fields(*(fields or [])).one()\n        \n        if obj and cache or getattr(cls, '__cacheable__', None):\n            dispatch.call(cls, 'set_object', instance=obj)\n\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget all the records in the tree.", "response": "def get_tree(cls, *condition, **kwargs):\n        \"\"\"\n        parent is root parent value, default is None\n        current is current value\n        condition is extra condition for select root records\n        mode is search method, value is 'wide' or 'deep'\n        \"\"\"\n        parent_field = kwargs.pop('parent_field', 'parent')\n        parent = kwargs.pop('parent', None)\n        parent_order_by = kwargs.pop('parent_order_by', None)\n        current = kwargs.pop('current', None)\n        order_by = kwargs.pop('order_by', None)\n        id_field = kwargs.pop('id_field', 'id')\n        mode = kwargs.pop('mode', 'wide')\n        if mode not in ('wide', 'deep'):\n            raise Exception(\"mode parameter should be 'wide' or 'deep', but '{}' found.\".format(mode))\n\n        def _f(parent):\n            query = cls.filter(cls.c[parent_field]==parent, *condition)\n            if order_by is not None:\n                query.order_by(order_by)\n            for row in query:\n                if mode == 'wide':\n                    yield row\n                for _row in _f(getattr(row, id_field)):\n                    yield _row\n                if mode == 'deep':\n                    yield row\n\n        if current:\n            query = cls.filter(cls.c[id_field]==current)\n        else:\n            if is_condition(parent):\n                query = cls.filter(parent)\n            else:\n                query = cls.filter(cls.c[parent_field]==parent)\n        if parent_order_by is not None:\n            query.order_by(parent_order_by)\n        for row in query:\n            if mode == 'wide':\n                yield row\n            for r in _f(getattr(row, id_field)):\n                yield r\n            if mode == 'deep':\n                yield row"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_choices(cls, condition=None, order_by=None, query=None, value_field=None, text_field=None):\n        result = []\n        if query is None:\n            query = cls.filter(condition).order_by(order_by)\n        for row in query:\n            if not value_field:\n                value = row._key\n            else:\n                value = getattr(row, value_field)\n            if not text_field:\n                text = unicode(row)\n            else:\n                text = getattr(row, text_field)\n            result.append((value, text))\n        return result", "response": "Get list of items from the database."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef refresh(self, fields=None, **kwargs):\n        cond = self.c[self._primary_field]==self._key\n        query = self.filter(cond, **kwargs)\n        if not fields:\n            fields = list(self.table.c)\n        \n        v = query.values_one(*fields)\n        if not v:\n            raise NotFound('Instance <{0}:{1}> can not be found'.format(self.tablename, self._key))\n        \n        d = self._data_prepare(v.items())\n        self.update(**d)\n        self.set_saved()", "response": "Refresh the instance of the current id"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dump(self, fields=None, exclude=None):\n        exclude = exclude or []\n        d = {}\n        if fields and self._primary_field not in fields:\n            fields = list(fields)\n            fields.append(self._primary_field)\n        for k, v in self.properties.items():\n            if ((not fields) or (k in fields)) and (not exclude or (k not in exclude)):\n                if not isinstance(v, ManyToMany):\n                    t = v.get_value_for_datastore(self)\n                    if t is Lazy:\n                        self.refresh()\n                        t = v.get_value_for_datastore(self)\n                    if isinstance(t, Model):\n                        t = t._key\n                    d[k] = v.to_str(t)\n                else:\n                    if fields:\n                       d[k] = ','.join([str(x) for x in getattr(self, v._lazy_value(), [])])\n        if self._primary_field and d and self._primary_field not in d:\n            d[self._primary_field] = str(self._key)\n        return d", "response": "Dump current object to dict."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef clear_relation(cls):\n        for k, v in cls.properties.items():\n            if isinstance(v, ReferenceProperty):\n                if hasattr(v, 'collection_name') and hasattr(v.reference_class, v.collection_name):\n                    delattr(v.reference_class, v.collection_name)\n\n                    if isinstance(v, OneToOne):\n                        #append to reference_class._onetoone\n                        del v.reference_class._onetoone[v.collection_name]", "response": "Clear relation properties for a class such as OneToOne Reference ManyToMany"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef put(self, _name, **values):\n        try:\n            sql = self.sqles[_name]\n            data = sql['data']\n            if sql['positional']:\n                d = [values[k] for k, v in sql['fields'].items()]\n            else:\n                d = {v:values[k] for k, v in sql['fields'].items()}\n            data.append(d)\n            if self.size and len(data) >= self.size:\n                do_(sql['raw_sql'], args=data)\n                sql['data'] = []\n        except:\n            if self.transcation:\n                Rollback(self.engine)\n            raise", "response": "Put data to cach if reached size value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nlike the name says. But who knows how it works?", "response": "def _easteregg(app=None):\n    \"\"\"Like the name says.  But who knows how it works?\"\"\"\n    def bzzzzzzz(gyver):\n        import base64\n        import zlib\n        return zlib.decompress(base64.b64decode(gyver)).decode('ascii')\n    gyver = u'\\n'.join([x + (77 - len(x)) * u' ' for x in bzzzzzzz(b'''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\n7f2zLkGNv8b191cD/3vs9Q833z8t''').splitlines()])\n    def easteregged(environ, start_response):\n        def injecting_start_response(status, headers, exc_info=None):\n            headers.append(('X-Powered-By', 'Werkzeug'))\n            return start_response(status, headers, exc_info)\n        if app is not None and environ.get('QUERY_STRING') != 'macgybarchakku':\n            return app(environ, injecting_start_response)\n        injecting_start_response('200 OK', [('Content-Type', 'text/html')])\n        return [(u'''\n<!DOCTYPE html>\n<html>\n<head>\n<title>About Werkzeug</title>\n<style type=\"text/css\">\n  body { font: 15px Georgia, serif; text-align: center; }\n  a { color: #333; text-decoration: none; }\n  h1 { font-size: 30px; margin: 20px 0 10px 0; }\n  p { margin: 0 0 30px 0; }\n  pre { font: 11px 'Consolas', 'Monaco', monospace; line-height: 0.95; }\n</style>\n</head>\n<body>\n<h1><a href=\"http://werkzeug.pocoo.org/\">Werkzeug</a></h1>\n<p>the Swiss Army knife of Python web development.</p>\n<pre>%s\\n\\n\\n</pre>\n</body>\n</html>''' % gyver).encode('latin1')]\n    return easteregged"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_cipher(key=None, keyfile=None):\n    des_func = import_attr(settings.SECRETKEY.CIPHER_CLS)\n    kwargs = settings.SECRETKEY.CIPHER_ARGS\n    \n    if not key:\n        key = functions.get_cipher_key(keyfile)\n    cipher = des_func(key, **kwargs)\n    return cipher", "response": "Get cipher object and then invoke it"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_key(keyfile=None):\n    keyfile = keyfile or application_path(settings.SECRETKEY.SECRET_FILE)\n    with file(keyfile, 'rb') as f:\n        return f.read()", "response": "Read the key content from secret_file"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_cipher_key(keyfile=None):\n    _key = get_key(keyfile)\n    _k = md5(_key).hexdigest()\n    key = xor(_k[:8], _k[8:16], _k[16:24], _k[24:])\n    return key", "response": "Create key which will be used in des"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwrapping for efficient iteration over mappings represented by dicts or sequences. items", "response": "def _items(mappingorseq):\n    \"\"\"Wrapper for efficient iteration over mappings represented by dicts\n    or sequences::\n\n        >>> for k, v in _items((i, i*i) for i in xrange(5)):\n        ...    assert k*k == v\n\n        >>> for k, v in _items(dict((i, i*i) for i in xrange(5))):\n        ...    assert k*k == v\n\n    \"\"\"\n    if hasattr(mappingorseq, \"iteritems\"):\n        return mappingorseq.iteritems()\n    elif hasattr(mappingorseq, \"items\"):\n        return mappingorseq.items()\n    return mappingorseq"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset multiple keys and values from a mapping.", "response": "def set_many(self, mapping, timeout=None):\n        \"\"\"Sets multiple keys and values from a mapping.\n\n        :param mapping: a mapping with the keys/values to set.\n        :param timeout: the cache timeout for the key (if not specified,\n                        it uses the default timeout).\n        \"\"\"\n        for key, value in _items(mapping):\n            self.set(key, value, timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dec(self, key, delta=1):\n        self.set(key, (self.get(key) or 0) - delta)", "response": "Decrements the value of a key by delta."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn an initialized memcache client. Used by the constructor.", "response": "def import_preferred_memcache_lib(self, servers):\n        \"\"\"Returns an initialized memcache client.  Used by the constructor.\"\"\"\n        try:\n            import pylibmc\n        except ImportError:\n            pass\n        else:\n            return pylibmc.Client(servers)\n\n        try:\n            from google.appengine.api import memcache\n        except ImportError:\n            pass\n        else:\n            return memcache.Client()\n\n        try:\n            import memcache\n        except ImportError:\n            pass\n        else:\n            return memcache.Client(servers)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dump_object(self, value):\n        t = type(value)\n        if t is int or t is long:\n            return str(value)\n        return '!' + pickle.dumps(value)", "response": "Dumps an object into a string for redis."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a list of fully qualified cache filenames", "response": "def _list_dir(self):\n        \"\"\"return a list of (fully qualified) cache filenames\n        \"\"\"\n        return [os.path.join(self._path, fn) for fn in os.listdir(self._path)\n                if not fn.endswith(self._fs_transaction_suffix)]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef import_mod_attr(path):\n    import inspect\n\n    if isinstance(path, (str, unicode)):\n        v = path.split(':')\n        if len(v) == 1:\n            module, func = path.rsplit('.', 1)\n        else:\n            module, func = v\n        mod = __import__(module, fromlist=['*'])\n        f = mod\n        for x in func.split('.'):\n            try:\n                f = getattr(f, x)\n            except:\n                raise AttributeError(\"Get %s attribute according %s error\" % (x, path))\n    else:\n        f = path\n        mod = inspect.getmodule(path)\n    return mod, f", "response": "Import string format module e. g. uuliweb. orm or object\n    return module object and object\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef extract_dirs(mod, path, dst, verbose=False, exclude=None, exclude_ext=None, recursion=True, replace=True):\n    default_exclude = ['.svn', '_svn', '.git']\n    default_exclude_ext = ['.pyc', '.pyo', '.bak', '.tmp']\n    exclude = exclude or []\n    exclude_ext = exclude_ext or []\n#    log = logging.getLogger('uliweb')\n    if not os.path.exists(dst):\n        os.makedirs(dst)\n        if verbose:\n            print 'Make directory %s' % dst\n    for r in pkg.resource_listdir(mod, path):\n        if r in exclude or r in default_exclude:\n            continue\n        fpath = os.path.join(path, r)\n        if pkg.resource_isdir(mod, fpath):\n            if recursion:\n                extract_dirs(mod, fpath, os.path.join(dst, r), verbose, exclude, exclude_ext, recursion, replace)\n        else:\n            ext = os.path.splitext(fpath)[1]\n            if ext in exclude_ext or ext in default_exclude_ext:\n                continue\n            extract_file(mod, fpath, dst, verbose, replace)", "response": "Extract all sub modules of mod from path to dst."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef walk_dirs(path, include=None, include_ext=None, exclude=None,\n        exclude_ext=None, recursion=True, file_only=False,\n        use_default_pattern=True, patterns=None):\n    \"\"\"\n    path directory path\n    resursion True will extract all sub module of mod\n    \"\"\"\n    default_exclude = ['.svn', '_svn', '.git']\n    default_exclude_ext = ['.pyc', '.pyo', '.bak', '.tmp']\n    exclude = exclude or []\n    exclude_ext = exclude_ext or []\n    include_ext = include_ext or []\n    include = include or []\n\n    if not os.path.exists(path):\n        raise StopIteration\n\n    for r in os.listdir(path):\n        if match(r, exclude) or (use_default_pattern and r in default_exclude):\n            continue\n        if include and r not in include:\n            continue\n        fpath = os.path.join(path, r)\n        if os.path.isdir(fpath):\n            if not file_only:\n                if patterns and match(r, patterns):\n                    yield os.path.normpath(fpath).replace('\\\\', '/')\n            if recursion:\n                for f in walk_dirs(fpath, include, include_ext, exclude,\n                    exclude_ext, recursion, file_only, use_default_pattern, patterns):\n                    yield os.path.normpath(f).replace('\\\\', '/')\n        else:\n            ext = os.path.splitext(fpath)[1]\n            if ext in exclude_ext or (use_default_pattern and ext in default_exclude_ext):\n                continue\n            if include_ext and ext not in include_ext:\n                continue\n            if patterns:\n                if not match(r, patterns):\n                    continue\n            yield os.path.normpath(fpath).replace('\\\\', '/')", "response": "Generator function that walks a directory tree and returns all the modules in the tree."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nexpanding path string to a list of files and directories", "response": "def expand_path(path):\n    \"\"\"\n    Auto search some variables defined in path string, such as:\n        $[PROJECT]/files\n        $[app_name]/files\n    for $[PROJECT] will be replaced with uliweb application apps_dir directory\n    and others will be treated as a normal python package, so uliweb will\n    use pkg_resources to get the path of the package\n    \n    update: 0.2.5 changed from ${} to $[]\n\n    Also apply with os.path.expandvars(os.path.expanduser(path))\n    \"\"\"\n    from uliweb import application\n    \n    def replace(m):\n        txt = m.groups()[0]\n        if txt == 'PROJECT':\n            return application.apps_dir\n        else:\n            return pkg.resource_filename(txt, '')\n    p = re.sub(r_expand_path, replace, path)\n    return os.path.expandvars(os.path.expanduser(path))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert CamelCase to snake_case", "response": "def camel_to_(s):\n    \"\"\"\n    Convert CamelCase to camel_case\n    \"\"\"\n    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', s)\n    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef application_path(path):\n    from uliweb import application\n    return os.path.join(application.project_dir, path)", "response": "Join application project_dir and path"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\noutput pretty formatted dict for example", "response": "def pretty_dict(d, leading=' ', newline='\\n', indent=0, tabstop=4, process=None):\n    \"\"\"\n    Output pretty formatted dict, for example:\n        \n        d = {\"a\":\"b\",\n            \"c\":{\n                \"d\":\"e\",\n                \"f\":\"g\",\n                }\n            }\n        \n    will output:\n        \n        a : 'b'\n        c : \n            d : 'e'\n            f : 'g'\n        \n    \"\"\"\n    for k, v in d.items():\n        if process:\n            k, v = process(k, v)\n        if isinstance(v, dict):\n            yield '%s%s : %s' % (indent*tabstop*leading, simple_value(k), newline)\n            for x in pretty_dict(v, leading=leading, newline=newline, indent=indent+1, tabstop=tabstop):\n                yield x\n            continue\n        yield '%s%s : %s%s' % (indent*tabstop*leading, simple_value(k), simple_value(v), newline)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting full url of a request", "response": "def request_url(req=None):\n    \"\"\"\n    Get full url of a request\n    \"\"\"\n    from uliweb import request\n\n    r = req or request\n    if request:\n        if r.query_string:\n            return r.path + '?' + r.query_string\n        else:\n            return r.path\n    else:\n        return ''"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef flat_list(*alist):\n    a = []\n    for x in alist:\n        if x is None:\n            continue\n        if isinstance(x, (tuple, list)):\n            a.extend([i for i in x if i is not None])\n        else:\n            a.append(x)\n    return a", "response": "Return a list of all the items in alist in a flat list."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompare differencs from two dicts", "response": "def compare_dict(da, db):\n    \"\"\"\n    Compare differencs from two dicts\n    \"\"\"\n    sa = set(da.items())\n    sb = set(db.items())\n    \n    diff = sa & sb\n    return dict(sa - diff), dict(sb - diff)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the caller information it ll return the module filename line number and function name", "response": "def get_caller(skip=None):\n    \"\"\"\n    Get the caller information, it'll return: module, filename, line_no\n    \"\"\"\n    import inspect\n    from fnmatch import fnmatch\n\n    try:\n        stack = inspect.stack()\n    except:\n        stack = [None, inspect.currentframe()]\n    if len(stack) > 1:\n        stack.pop(0)\n        if skip and not isinstance(skip, (list, tuple)):\n            skip = [skip]\n        else:\n            skip = []\n        ptn = [os.path.splitext(s.replace('\\\\', '/'))[0] for s in skip]\n        for frame in stack:\n            #see doc: inspect\n            #the frame object, the filename, the line number of the current line, \n            #the function name, a list of lines of context from the source code, \n            #and the index of the current line within that list\n            if isinstance(frame, tuple):\n                filename, funcname, lineno = frame[1], frame[3], frame[2]\n            else:\n                filename, funcname, lineno = frame.f_code.co_filename, frame.f_code.co_name, frame.f_lineno\n            del frame\n            found = False\n            for k in ptn:\n                filename = os.path.splitext(filename.replace('\\\\', '/'))[0]\n                if fnmatch(filename, k):\n                    found = True\n                    break\n            if not found:\n                return filename, lineno, funcname"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntrimming path to specified length", "response": "def trim_path(path, length=30):\n    \"\"\"\n    trim path to specified length, for example:\n    >>> a = '/project/apps/default/settings.ini'\n    >>> trim_path(a)\n    '.../apps/default/settings.ini'\n    \n    The real length will be length-4, it'll left '.../' for output.\n    \"\"\"\n    s = path.replace('\\\\', '/').split('/')\n    t = -1\n    for i in range(len(s)-1, -1, -1):\n        t = len(s[i]) + t + 1\n        if t > length-4:\n            break\n    return '.../' + '/'.join(s[i+1:])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget object from config file.", "response": "def get_configrable_object(key, section, cls=None):\n    \"\"\"\n    if obj is a class, then check if the class is subclass of cls\n    or it should be object path, and it'll be imported by import_attr\n    \"\"\"\n    from uliweb import UliwebError, settings\n    import inspect\n\n    if inspect.isclass(key) and cls and issubclass(key, cls):\n        return key\n    elif isinstance(key, (str, unicode)):\n        path = settings[section].get(key)\n        if path:\n            _cls = import_attr(path)\n            return _cls\n        else:\n            raise UliwebError(\"Can't find section name %s in settings\" % section)\n    else:\n        raise UliwebError(\"Key %r should be subclass of %r object or string path format!\" % (key, cls))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts size to XB XKB XMB XGB XB XGB XB XKB XMB XGB XB XGB XB XKB XGB XB XMB XGB XB XGB XB XKB XMB XGB XB XGB XB XGB XB XGB XB XGB XB XGB XB XGB XB X", "response": "def format_size(size):\n    \"\"\"\n    Convert size to XB, XKB, XMB, XGB\n    :param size: length value\n    :return: string value with size unit\n    \"\"\"\n    units = ['B', 'KB', 'MB', 'GB']\n    unit = ''\n    n = size\n    old_n = n\n    value = size\n    for i in units:\n        old_n = n\n        x, y = divmod(n, 1024)\n        if x == 0:\n            unit = i\n            value = y\n            break\n\n        n = x\n        unit = i\n        value = old_n\n    return str(value)+unit"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting a size number to K M or Z", "response": "def convert_bytes(n):\n    \"\"\"\n    Convert a size number to 'K', 'M', .etc\n    \"\"\"\n    symbols = ('K', 'M', 'G', 'T', 'P', 'E', 'Z', 'Y')\n    prefix = {}\n    for i, s in enumerate(symbols):\n        prefix[s] = 1 << (i + 1) * 10\n    for s in reversed(symbols):\n        if n >= prefix[s]:\n            value = float(n) / prefix[s]\n            return '%.1f%s' % (value, s)\n    return \"%sB\" % n"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprocess static combine create md5 key according each static filename", "response": "def init_static_combine():\r\n    \"\"\"\r\n    Process static combine, create md5 key according each static filename\r\n    \"\"\"\r\n    from uliweb import settings\r\n    from hashlib import md5\r\n    import os\r\n    \r\n    d = {}\r\n    if settings.get_var('STATIC_COMBINE_CONFIG/enable', False):\r\n        for k, v in settings.get('STATIC_COMBINE', {}).items():\r\n            key = '_cmb_'+md5(''.join(v)).hexdigest()+os.path.splitext(v[0])[1]\r\n            d[key] = v\r\n        \r\n    return d"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets object from redis", "response": "def get_object(model, cid, engine_name=None, connection=None):\n    \"\"\"\n    Get cached object from redis\n    \n    if id is None then return None:\n    \"\"\"\n    from uliweb import settings\n    \n    if not id:\n        return \n    \n    if not check_enable():\n        return\n    \n    redis = get_redis()\n    if not redis: return\n\n    tablename = model._alias or model.tablename\n    \n    info = settings.get_var('OBJCACHE_TABLES/%s' % tablename, {})\n    if info is None:\n        return\n    \n    _id = get_id(engine_name or model.get_engine_name(), tablename, cid)\n    try:\n        log.debug(\"Try to find objcache:get:table=%s:id=[%s]\" % (tablename, _id))\n        if redis.exists(_id):\n            v = redis.hgetall(_id)\n            o = model.load(v, from_='dump')\n            log.debug(\"Found!\")\n            return o\n        else:\n            log.debug(\"Not Found!\")\n    except Exception as e:\n        log.exception(e)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_object(model, instance, fields=None, engine_name=None):\n    from uliweb import settings\n    \n    if not check_enable():\n        return\n    \n    redis = get_redis()\n    if not redis: return\n    \n    tablename = model._alias or model.tablename\n    exclude = []\n    if not fields:\n        fields, exclude = get_fields(tablename)\n    \n    v = instance.dump(fields, exclude=exclude)\n    info = settings.get_var('OBJCACHE_TABLES/%s' % tablename, {})\n    \n    if info is None:\n        return\n    \n    expire = settings.get_var('OBJCACHE/timeout', 0)\n    key = 'id'\n    if info and isinstance(info, dict):\n        expire = info.get('expire', expire)\n        key = info.get('key', key)\n    \n    if '.' in key or key not in model.properties:\n        _key = import_attr(key)(instance)\n    else:\n        _key = getattr(instance, key)\n    _id = get_id(engine_name or model.get_engine_name(), tablename, _key)\n    try:\n        pipe = redis.pipeline()\n        p = pipe.delete(_id).hmset(_id, v)\n        expire_msg = ''\n        if expire:\n            p = p.expire(_id, expire)\n            expire_msg = ':expire=%d' % expire\n        r = p.execute()\n        log.debug(\"Saving to cache objcache:set:table=%s:id=[%s]:%s\" % (tablename, _id, expire_msg))\n    except Exception as e:\n        log.exception(e)", "response": "Set an object in the cache."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget csrf token or create new one", "response": "def csrf_token():\n    \"\"\"\n    Get csrf token or create new one\n    \"\"\"\n    from uliweb import request, settings\n    from uliweb.utils.common import safe_str\n    \n    v = {}\n    token_name = settings.CSRF.cookie_token_name\n    if not request.session.deleted and request.session.get(token_name):\n        v = request.session[token_name]\n        if time.time() >= v['created_time'] + v['expiry_time']:\n            v = {}\n        else:\n            v['created_time'] = time.time()\n    if not v:\n        token = request.cookies.get(token_name)\n        if not token:\n            token = uuid.uuid4().get_hex()\n        \n        v = {'token':token, 'expiry_time':settings.CSRF.timeout, 'created_time':time.time()}\n\n    if not request.session.deleted:\n        request.session[token_name] = v\n    return safe_str(v['token'])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef json(self):\n        if 'json' not in self.environ.get('CONTENT_TYPE', ''):\n            raise BadRequest('Not a JSON request')\n        try:\n            return loads(self.data)\n        except Exception:\n            raise BadRequest('Unable to read JSON request')", "response": "Get the result of simplejson. loads if possible."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_protobuf(self, proto_type):\n        if 'protobuf' not in self.environ.get('CONTENT_TYPE', ''):\n            raise BadRequest('Not a Protobuf request')\n\n        obj = proto_type()\n        try:\n            obj.ParseFromString(self.data)\n        except Exception:\n            raise BadRequest(\"Unable to parse Protobuf request\")\n\n        # Fail if not all required fields are set\n        if self.protobuf_check_initialization and not obj.IsInitialized():\n            raise BadRequest(\"Partial Protobuf request\")\n\n        return obj", "response": "Parse the data into an instance of proto_type."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef charset(self):\n        header = self.environ.get('CONTENT_TYPE')\n        if header:\n            ct, options = parse_options_header(header)\n            charset = options.get('charset')\n            if charset:\n                if is_known_charset(charset):\n                    return charset\n                return self.unknown_charset(charset)\n        return self.default_charset", "response": "The charset from the content type."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef utf8(value):\r\n    if isinstance(value, _UTF8_TYPES):\r\n        return value\r\n    elif isinstance(value, unicode_type):\r\n        return value.encode(\"utf-8\")\r\n    else:\r\n        return str(value)", "response": "Converts a string argument to a byte string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_basestring(value):\r\n    if value is None:\r\n        return 'None'\r\n    if isinstance(value, _BASESTRING_TYPES):\r\n        return value\r\n    elif isinstance(value, unicode_type):\r\n        return value.decode(\"utf-8\")\r\n    else:\r\n        return str(value)", "response": "Converts a string argument to a subclass of basestring."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting a plain text into a link tag with optional extra parameters.", "response": "def linkify(text, shorten=False, extra_params=\"\",\r\n            require_protocol=False, permitted_protocols=[\"http\", \"https\"]):\r\n    \"\"\"Converts plain text into HTML with links.\r\n\r\n    For example: ``linkify(\"Hello http://tornadoweb.org!\")`` would return\r\n    ``Hello <a href=\"http://tornadoweb.org\">http://tornadoweb.org</a>!``\r\n\r\n    Parameters:\r\n\r\n    * ``shorten``: Long urls will be shortened for display.\r\n\r\n    * ``extra_params``: Extra text to include in the link tag, or a callable\r\n        taking the link as an argument and returning the extra text\r\n        e.g. ``linkify(text, extra_params='rel=\"nofollow\" class=\"external\"')``,\r\n        or::\r\n\r\n            def extra_params_cb(url):\r\n                if url.startswith(\"http://example.com\"):\r\n                    return 'class=\"internal\"'\r\n                else:\r\n                    return 'class=\"external\" rel=\"nofollow\"'\r\n            linkify(text, extra_params=extra_params_cb)\r\n\r\n    * ``require_protocol``: Only linkify urls which include a protocol. If\r\n        this is False, urls such as www.facebook.com will also be linkified.\r\n\r\n    * ``permitted_protocols``: List (or set) of protocols which should be\r\n        linkified, e.g. ``linkify(text, permitted_protocols=[\"http\", \"ftp\",\r\n        \"mailto\"])``. It is very unsafe to include protocols such as\r\n        ``javascript``.\r\n    \"\"\"\r\n    if extra_params and not callable(extra_params):\r\n        extra_params = \" \" + extra_params.strip()\r\n\r\n    def make_link(m):\r\n        url = m.group(1)\r\n        proto = m.group(2)\r\n        if require_protocol and not proto:\r\n            return url  # not protocol, no linkify\r\n\r\n        if proto and proto not in permitted_protocols:\r\n            return url  # bad protocol, no linkify\r\n\r\n        href = m.group(1)\r\n        if not proto:\r\n            href = \"http://\" + href   # no proto specified, use http\r\n\r\n        if callable(extra_params):\r\n            params = \" \" + extra_params(href).strip()\r\n        else:\r\n            params = extra_params\r\n\r\n        # clip long urls. max_len is just an approximation\r\n        max_len = 30\r\n        if shorten and len(url) > max_len:\r\n            before_clip = url\r\n            if proto:\r\n                proto_len = len(proto) + 1 + len(m.group(3) or \"\")  # +1 for :\r\n            else:\r\n                proto_len = 0\r\n\r\n            parts = url[proto_len:].split(\"/\")\r\n            if len(parts) > 1:\r\n                # Grab the whole host part plus the first bit of the path\r\n                # The path is usually not that interesting once shortened\r\n                # (no more slug, etc), so it really just provides a little\r\n                # extra indication of shortening.\r\n                url = url[:proto_len] + parts[0] + \"/\" + \\\r\n                    parts[1][:8].split('?')[0].split('.')[0]\r\n\r\n            if len(url) > max_len * 1.5:  # still too long\r\n                url = url[:max_len]\r\n\r\n            if url != before_clip:\r\n                amp = url.rfind('&')\r\n                # avoid splitting html char entities\r\n                if amp > max_len - 5:\r\n                    url = url[:amp]\r\n                url += \"...\"\r\n\r\n                if len(url) >= len(before_clip):\r\n                    url = before_clip\r\n                else:\r\n                    # full url is visible on mouse-over (for those who don't\r\n                    # have a status bar, such as Safari by default)\r\n                    params += ' title=\"%s\"' % href\r\n\r\n        return u('<a href=\"%s\"%s>%s</a>') % (href, params, url)\r\n\r\n    # First HTML-escape so that our strings are all safe.\r\n    # The regex is modified to avoid character entites other than &amp; so\r\n    # that we won't pick up &quot;, etc.\r\n    text = _unicode(xhtml_escape(text))\r\n    return _URL_RE.sub(make_link, text)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef generate(self, vars=None, env=None):\r\n        def defined(v, default=None):\r\n            _v = default\r\n            if v in vars:\r\n                _v = vars[v]\r\n            elif v in env:\r\n                _v = env[v]\r\n            return _v\r\n\r\n        namespace = {\r\n            # __name__ and __loader__ allow the traceback mechanism to find\r\n            # the generated source code.\r\n            \"defined\": defined,\r\n            #fix RuntimeWarning: Parent module 'a/b/c' not found while handling absolute import warning\r\n            \"__name__\": self.name.replace('.', '_'),\r\n            # \"__name__\": self.name,\r\n            \"__loader__\": ObjectDict(get_source=lambda name: self.code),\r\n        }\r\n        namespace.update(default_namespace)\r\n        namespace.update(env or {})\r\n        namespace.update(self.namespace)\r\n        namespace.update(vars or {})\r\n        namespace['_vars'] = vars\r\n        exec_in(self.compiled, namespace)\r\n        execute = namespace[\"_tt_execute\"]\r\n        # Clear the traceback module's cache of source data now that\r\n        # we've generated a new template (mainly for this module's\r\n        # unittests, where different tests reuse the same name).\r\n        linecache.clearcache()\r\n        return execute()", "response": "Generate this template with the given arguments."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn True if the key is in the cache False otherwise.", "response": "def has(self, key, mtime=None):\r\n        \"\"\"\r\n        This method should almost NEVER be used. The reason is that between the time\r\n        has_key is called, and the key is accessed, the key might vanish.\r\n\r\n        \"\"\"\r\n        v = self.__values.get(key, None)\r\n        if not v:\r\n            return False\r\n        if self.check_modified_time:\r\n            mtime = self._get_mtime(key, mtime)\r\n            if mtime != self.__modified_times[key]:\r\n                del self[key]\r\n                return False\r\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nresets the cache of compiled templates.", "response": "def reset(self):\r\n        \"\"\"Resets the cache of compiled templates.\"\"\"\r\n        with self.lock:\r\n            if self.cache:\r\n                if self.use_tmp:\r\n                    shutil.rmtree(self.tmp_dir, ignore_errors=True)\r\n                else:\r\n                    self.templates = {}"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads a template from disk.", "response": "def load(self, name, skip='', skip_original='', default_template=None, layout=None):\r\n        \"\"\"Loads a template.\"\"\"\r\n        filename = self.resolve_path(name, skip=skip, skip_original=skip_original,\r\n                                     default_template=default_template)\r\n\r\n        if not filename:\r\n            raise ParseError(\"Can't find template %s.\" % name)\r\n\r\n        with self.lock:\r\n            if layout:\r\n                _filename = filename + '$$' + layout\r\n                mtime = os.path.getmtime(filename)\r\n            else:\r\n                mtime = None\r\n                _filename = filename\r\n\r\n            if self.cache:\r\n                if not self.use_tmp:\r\n                    #check current template file expiration\r\n                    t = self.templates.get(_filename, mtime=mtime)\r\n                    if t:\r\n                        #check depends tempaltes expiration\r\n                        check = self.check_expiration(t)\r\n                        if not check:\r\n                            return t\r\n                else:\r\n                    #get cached file from disk\r\n                    pass\r\n            t = self._create_template(name, filename, layout=layout)\r\n            if self.cache:\r\n                if not self.use_tmp:\r\n                    self.templates.set(_filename, t, mtime)\r\n                else:\r\n                    #save cached file to disk\r\n                    pass\r\n\r\n            return t"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef resolve_path(self, filename, skip='', skip_original='',\r\n                     default_template=None):\r\n        \"\"\"\r\n        Fetch the template filename according dirs\r\n        :para skip: if the searched filename equals skip, then using the one before.\r\n        \"\"\"\r\n        def _file(filename, dirs):\r\n            for d in dirs:\r\n                _f = os.path.normpath(os.path.join(d, filename))\r\n                if os.path.exists(_f):\r\n                    yield _f\r\n            raise StopIteration\r\n\r\n        filename = os.path.normpath(filename)\r\n        skip = os.path.normpath(skip)\r\n        skip_original = os.path.normpath(skip_original)\r\n\r\n        if os.path.exists(filename):\r\n            return filename\r\n\r\n        if filename and self.dirs:\r\n            _files = _file(filename, self.dirs)\r\n            if skip_original == filename:\r\n                for f in _files:\r\n                    if f == skip:\r\n                        break\r\n\r\n            for f in _files:\r\n                if f != skip:\r\n                    return f\r\n                else:\r\n                    continue\r\n\r\n        if default_template:\r\n            if not isinstance(default_template, (tuple, list)):\r\n                default_template = [default_template]\r\n            for x in default_template:\r\n                filename = self.resolve_path(x)\r\n                if filename:\r\n                    return filename", "response": "Resolve the path to a new file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef has_role(user, *roles, **kwargs):\r\n    Role = get_model('role')\r\n    if isinstance(user, (unicode, str)):\r\n        User = get_model('user')\r\n        user = User.get(User.c.username==user)\r\n        \r\n    for role in roles:\r\n        if isinstance(role, (str, unicode)):\r\n            role = Role.get(Role.c.name==role)\r\n            if not role:\r\n                continue\r\n        name = role.name\r\n        \r\n        func = __role_funcs__.get(name, None)\r\n        if func:\r\n            if isinstance(func, (unicode, str)):\r\n                func = import_attr(func)\r\n                \r\n            assert callable(func)\r\n            \r\n            para = kwargs.copy()\r\n            para['user'] = user\r\n            flag = call_func(func, para)\r\n            if flag:\r\n                return role\r\n        flag = role.users.has(user)\r\n        if flag:\r\n            return role\r\n        \r\n        flag = role.usergroups_has_user(user)\r\n        if flag:\r\n            return role\r\n    return False", "response": "Returns True if the user belongs to the role and False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning True if a user has any of the given permissions.", "response": "def has_permission(user, *permissions, **role_kwargs):\r\n    \"\"\"\r\n    Judge if an user has permission, and if it does return role object, and if it doesn't\r\n    return False. role_kwargs will be passed to role functions.\r\n    With role object, you can use role.relation to get Role_Perm_Rel object.\r\n    \"\"\"\r\n    Role = get_model('role')\r\n    Perm = get_model('permission')\r\n    if isinstance(user, (unicode, str)):\r\n        User = get_model('user')\r\n        user = User.get(User.c.username==user)\r\n        \r\n    for name in permissions:\r\n        perm = Perm.get(Perm.c.name==name)\r\n        if not perm:\r\n            continue\r\n        \r\n        flag = has_role(user, *list(perm.perm_roles.with_relation().all()), **role_kwargs)\r\n        if flag:\r\n            return flag\r\n        \r\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check_role(*roles, **args_map):\r\n    def f1(func, roles=roles):\r\n        @wraps(func)\r\n        def f2(*args, **kwargs):\r\n            from uliweb import request, error\r\n            \r\n            arguments = {}\r\n            for k, v in args_map.items():\r\n                if v in kwargs:\r\n                    arguments[k] = kwargs[v]\r\n            if not has_role(request.user, *roles, **arguments):\r\n                error(_(\"You have no roles to visit this page.\"))\r\n            return func(*args, **kwargs)\r\n        return f2\r\n    return f1", "response": "Decorator to check if a user has one of the specified roles."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef generate_adhoc_ssl_context():\n    from OpenSSL import SSL\n    cert, pkey = generate_adhoc_ssl_pair()\n    ctx = SSL.Context(SSL.SSLv23_METHOD)\n    ctx.use_privatekey(pkey)\n    ctx.use_certificate(cert)\n    return ctx", "response": "Generates an adhoc SSL context for the development server."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load_ssl_context(cert_file, pkey_file):\n    from OpenSSL import SSL\n    ctx = SSL.Context(SSL.SSLv23_METHOD)\n    ctx.use_certificate_file(cert_file)\n    ctx.use_privatekey_file(pkey_file)\n    return ctx", "response": "Loads an SSL context from a certificate and private key file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_ssl_error(error=None):\n    if error is None:\n        error = sys.exc_info()[1]\n    from OpenSSL import SSL\n    return isinstance(error, SSL.Error)", "response": "Checks if the given error is an SSL error."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef select_ip_version(host, port):\n    # disabled due to problems with current ipv6 implementations\n    # and various operating systems.  Probably this code also is\n    # not supposed to work, but I can't come up with any other\n    # ways to implement this.\n    ##try:\n    ##    info = socket.getaddrinfo(host, port, socket.AF_UNSPEC,\n    ##                              socket.SOCK_STREAM, 0,\n    ##                              socket.AI_PASSIVE)\n    ##    if info:\n    ##        return info[0][0]\n    ##except socket.gaierror:\n    ##    pass\n    if ':' in host and hasattr(socket, 'AF_INET6'):\n        return socket.AF_INET6\n    return socket.AF_INET", "response": "Returns AF_INET4 or AF_INET6 depending on where to connect to."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a new server instance that is either threaded or forks .", "response": "def make_server(host, port, app=None, threaded=False, processes=1,\n                request_handler=None, passthrough_errors=False,\n                ssl_context=None):\n    \"\"\"Create a new server instance that is either threaded, or forks\n    or just processes one request after another.\n    \"\"\"\n    if threaded and processes > 1:\n        raise ValueError(\"cannot have a multithreaded and \"\n                         \"multi process server.\")\n    elif threaded:\n        return ThreadedWSGIServer(host, port, app, request_handler,\n                                  passthrough_errors, ssl_context)\n    elif processes > 1:\n        return ForkingWSGIServer(host, port, app, processes, request_handler,\n                                 passthrough_errors, ssl_context)\n    else:\n        return BaseWSGIServer(host, port, app, request_handler,\n                              passthrough_errors, ssl_context)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _reloader_stat_loop(extra_files=None, interval=1):\n    from itertools import chain\n    mtimes = {}\n    while 1:\n        for filename in chain(_iter_module_files(), extra_files or ()):\n            try:\n                mtime = os.stat(filename).st_mtime\n            except OSError:\n                continue\n\n            old_time = mtimes.get(filename)\n            if old_time is None:\n                mtimes[filename] = mtime\n                continue\n            elif mtime > old_time:\n                _log('info', ' * Detected change in %r, reloading' % filename)\n                sys.exit(3)\n        time.sleep(interval)", "response": "This function is run from the main thread and will reload other modules when any modules currently loaded change."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run_with_reloader(main_func, extra_files=None, interval=1):\n    import signal\n    signal.signal(signal.SIGTERM, lambda *args: sys.exit(0))\n    if os.environ.get('WERKZEUG_RUN_MAIN') == 'true':\n        thread.start_new_thread(main_func, ())\n        try:\n            reloader_loop(extra_files, interval)\n        except KeyboardInterrupt:\n            return\n    try:\n        sys.exit(restart_with_reloader())\n    except KeyboardInterrupt:\n        pass", "response": "Run the given function in an independent python interpreter."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nstart a simple WSGI application.", "response": "def run_simple(hostname, port, application, use_reloader=False,\n               use_debugger=False, use_evalex=True,\n               extra_files=None, reloader_interval=1, threaded=False,\n               processes=1, request_handler=None, static_files=None,\n               passthrough_errors=False, ssl_context=None):\n    \"\"\"Start an application using wsgiref and with an optional reloader.  This\n    wraps `wsgiref` to fix the wrong default reporting of the multithreaded\n    WSGI variable and adds optional multithreading and fork support.\n\n    This function has a command-line interface too::\n\n        python -m werkzeug.serving --help\n\n    .. versionadded:: 0.5\n       `static_files` was added to simplify serving of static files as well\n       as `passthrough_errors`.\n\n    .. versionadded:: 0.6\n       support for SSL was added.\n\n    .. versionadded:: 0.8\n       Added support for automatically loading a SSL context from certificate\n       file and private key.\n\n    .. versionadded:: 0.9\n       Added command-line interface.\n\n    :param hostname: The host for the application.  eg: ``'localhost'``\n    :param port: The port for the server.  eg: ``8080``\n    :param application: the WSGI application to execute\n    :param use_reloader: should the server automatically restart the python\n                         process if modules were changed?\n    :param use_debugger: should the werkzeug debugging system be used?\n    :param use_evalex: should the exception evaluation feature be enabled?\n    :param extra_files: a list of files the reloader should watch\n                        additionally to the modules.  For example configuration\n                        files.\n    :param reloader_interval: the interval for the reloader in seconds.\n    :param threaded: should the process handle each request in a separate\n                     thread?\n    :param processes: if greater than 1 then handle each request in a new process\n                      up to this maximum number of concurrent processes.\n    :param request_handler: optional parameter that can be used to replace\n                            the default one.  You can use this to replace it\n                            with a different\n                            :class:`~BaseHTTPServer.BaseHTTPRequestHandler`\n                            subclass.\n    :param static_files: a dict of paths for static files.  This works exactly\n                         like :class:`SharedDataMiddleware`, it's actually\n                         just wrapping the application in that middleware before\n                         serving.\n    :param passthrough_errors: set this to `True` to disable the error catching.\n                               This means that the server will die on errors but\n                               it can be useful to hook debuggers in (pdb etc.)\n    :param ssl_context: an SSL context for the connection. Either an OpenSSL\n                        context, a tuple in the form ``(cert_file, pkey_file)``,\n                        the string ``'adhoc'`` if the server should\n                        automatically create one, or `None` to disable SSL\n                        (which is the default).\n    \"\"\"\n    if use_debugger:\n        from werkzeug.debug import DebuggedApplication\n        application = DebuggedApplication(application, use_evalex)\n    if static_files:\n        from werkzeug.wsgi import SharedDataMiddleware\n        application = SharedDataMiddleware(application, static_files)\n\n    def inner():\n        make_server(hostname, port, application, threaded,\n                    processes, request_handler,\n                    passthrough_errors, ssl_context).serve_forever()\n\n    if os.environ.get('WERKZEUG_RUN_MAIN') != 'true':\n        display_hostname = hostname != '*' and hostname or 'localhost'\n        if ':' in display_hostname:\n            display_hostname = '[%s]' % display_hostname\n        _log('info', ' * Running on %s://%s:%d/', ssl_context is None\n             and 'http' or 'https', display_hostname, port)\n    if use_reloader:\n        # Create and destroy a socket so that any exceptions are raised before\n        # we spawn a separate Python interpreter and lose this ability.\n        address_family = select_ip_version(hostname, port)\n        test_socket = socket.socket(address_family, socket.SOCK_STREAM)\n        test_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        test_socket.bind((hostname, port))\n        test_socket.close()\n        run_with_reloader(inner, extra_files, reloader_interval)\n    else:\n        inner()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_attrs(args, nocreate_if_none=['id', 'for', 'class']):\n    if not args:\n        return ''\n    s = ['']\n    for k, v in sorted(args.items()):\n        k = u_str(k)\n        v = u_str(v)\n        if k.startswith('_'):\n            k = k[1:]\n        if v is None:\n            if k not in nocreate_if_none:\n                s.append(k)\n        else:\n            if k.lower() in __noescape_attrs__:\n                t = u_str(v)\n            else:\n                t = cgi.escape(u_str(v))\n            t = '\"%s\"' % t.replace('\"', '&quot;')\n            s.append('%s=%s' % (k, t))\n    return ' '.join(s)", "response": "Make python dict to k = v format"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _visit_pyfiles(list, dirname, names):\n    # get extension for python source files\n    if not globals().has_key('_py_ext'):\n        global _py_ext\n#        _py_ext = [triple[0] for triple in imp.get_suffixes()\n#                   if triple[2] == imp.PY_SOURCE][0]\n        _py_ext = [triple[0] for triple in imp.get_suffixes()\n                   if triple[2] == imp.PY_SOURCE]\n        \n    # don't recurse into CVS directories\n    if 'CVS' in names:\n        names.remove('CVS')\n\n    if '.svn' in names:\n        names.remove('.svn')\n\n    if '.git' in names:\n        names.remove('.git')\n\n    if 'static' in names:\n        names.remove('static')\n    \n    # add all *.py files to list\n    list.extend(\n        [os.path.join(dirname, file) for file in names\n         if os.path.splitext(file)[1] in _py_ext]\n        )", "response": "Helper for getFilesForName method."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_modpkg_path(dotted_name, pathlist=None):\n    # split off top-most name\n    parts = dotted_name.split('.', 1)\n\n    if len(parts) > 1:\n        # we have a dotted path, import top-level package\n        try:\n            file, pathname, description = imp.find_module(parts[0], pathlist)\n            if file: file.close()\n        except ImportError:\n            return None\n\n        # check if it's indeed a package\n        if description[2] == imp.PKG_DIRECTORY:\n            # recursively handle the remaining name parts\n            pathname = _get_modpkg_path(parts[1], [pathname])\n        else:\n            pathname = None\n    else:\n        # plain name\n        try:\n            file, pathname, description = imp.find_module(\n                dotted_name, pathlist)\n            if file:\n                file.close()\n            if description[2] not in [imp.PY_SOURCE, imp.PKG_DIRECTORY]:\n                pathname = None\n        except ImportError:\n            pathname = None\n\n    return pathname", "response": "Get the filesystem path for a module or a package."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget a list of module files for a filename a module or package name or a directory.", "response": "def getFilesForName(name):\n    \"\"\"Get a list of module files for a filename, a module or package name,\n    or a directory.\n    \"\"\"\n    if not os.path.exists(name):\n        # check for glob chars\n        if containsAny(name, \"*?[]\"):\n            files = glob.glob(name)\n            list = []\n            for file in files:\n                list.extend(getFilesForName(file))\n            return list\n\n        # try to find module or package\n        name = _get_modpkg_path(name)\n        if not name:\n            return []\n\n    if os.path.isdir(name):\n        # find all python files in directory\n        list = []\n        os.path.walk(name, _visit_pyfiles, list)\n        return list\n    elif os.path.exists(name):\n        # a single file\n        return [name]\n\n    return []"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef escape(s, quote=None):\n    if s is None:\n        return ''\n    elif hasattr(s, '__html__'):\n        return text_type(s.__html__())\n    elif not isinstance(s, string_types):\n        s = text_type(s)\n    if quote is not None:\n        from warnings import warn\n        warn(DeprecationWarning('quote parameter is implicit now'), stacklevel=2)\n    s = s.replace('&', '&amp;').replace('<', '&lt;') \\\n        .replace('>', '&gt;').replace('\"', \"&quot;\")\n    return s", "response": "Escape special characters in a string."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a WSGI application that redirects the client to the specified location.", "response": "def redirect(location, code=302):\n    \"\"\"Return a response object (a WSGI application) that, if called,\n    redirects the client to the target location.  Supported codes are 301,\n    302, 303, 305, and 307.  300 is not supported because it's not a real\n    redirect and 304 because it's the answer for a request with a request\n    with defined If-Modified-Since headers.\n\n    .. versionadded:: 0.6\n       The location can now be a unicode string that is encoded using\n       the :func:`iri_to_uri` function.\n\n    :param location: the location the response should redirect to.\n    :param code: the redirect status code. defaults to 302.\n    \"\"\"\n    from werkzeug.wrappers import Response\n    display_location = escape(location)\n    if isinstance(location, text_type):\n        from werkzeug.urls import iri_to_uri\n        location = iri_to_uri(location)\n    response = Response(\n        '<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2 Final//EN\">\\n'\n        '<title>Redirecting...</title>\\n'\n        '<h1>Redirecting...</h1>\\n'\n        '<p>You should be redirected automatically to target URL: '\n        '<a href=\"%s\">%s</a>.  If not click the link.' %\n        (escape(location), display_location), code, mimetype='text/html')\n    response.headers['Location'] = location\n    return response"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef append_slash_redirect(environ, code=301):\n    new_path = environ['PATH_INFO'].strip('/') + '/'\n    query_string = environ.get('QUERY_STRING')\n    if query_string:\n        new_path += '?' + query_string\n    return redirect(new_path, code)", "response": "Redirect to the same URL but with a slash appended."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef import_string(import_name, silent=False):\n    #XXX: py3 review needed\n    assert isinstance(import_name, string_types)\n    # force the import name to automatically convert to strings\n    import_name = str(import_name)\n    try:\n        if ':' in import_name:\n            module, obj = import_name.split(':', 1)\n        elif '.' in import_name:\n            module, obj = import_name.rsplit('.', 1)\n        else:\n            return __import__(import_name)\n        # __import__ is not able to handle unicode strings in the fromlist\n        # if the module is a package\n        if PY2 and isinstance(obj, unicode):\n            obj = obj.encode('utf-8')\n        try:\n            return getattr(__import__(module, None, None, [obj]), obj)\n        except (ImportError, AttributeError):\n            # support importing modules not yet set up by the parent module\n            # (or package for that matter)\n            modname = module + '.' + obj\n            __import__(modname)\n            return sys.modules[modname]\n    except ImportError as e:\n        if not silent:\n            reraise(\n                ImportStringError,\n                ImportStringError(import_name, e),\n                sys.exc_info()[2])", "response": "Imports an object based on a dotted name."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread a single translation entry from the file F and return a tuple with the comments msgid and msgstr.", "response": "def parse_translation(f, lineno):\r\n    \"\"\"Read a single translation entry from the file F and return a\r\n    tuple with the comments, msgid and msgstr.  The comments is returned\r\n    as a list of lines which do not end in new-lines.  The msgid and\r\n    msgstr are strings, possibly with embedded newlines\"\"\"\r\n    line = f.readline()\r\n    \r\n    def get_line(f, line, need_keys, lineno, default='\"\"'):\r\n        line = line.rstrip()\r\n        if not line:\r\n            return lineno, need_keys[0], default, line\r\n        key, value = line.split(' ', 1)\r\n        # Parse msgid\r\n        if key not in need_keys:\r\n            print 'Error Line, need %r: %d, line=' % (need_keys, lineno, line)\r\n            raise RuntimeError(\"parse error\")\r\n        v = value\r\n        while 1:\r\n            line = f.readline()\r\n            line = line.rstrip()\r\n            lineno += 1\r\n            if not line or line[0] != '\"':\r\n                break\r\n            v += '\\n' + line[:]\r\n        return lineno, key, v, line\r\n    \r\n    # Parse comments\r\n    comments = []\r\n    while 1:\r\n        if not line:\r\n            return lineno, None, None, None\r\n        if line.strip() == '':\r\n            return lineno, comments, None, None\r\n        elif line[0] == '#':\r\n            comments.append(line[:-1])\r\n        else:\r\n            break\r\n        line = f.readline()\r\n        lineno += 1\r\n\r\n    lineno, key, msgid, line = get_line(f, line, ['msgid'], lineno)\r\n    lineno, key, value, line = get_line(f, line, ['msgid_plural', 'msgstr'], lineno)\r\n    if key == 'msgid_plural':\r\n        msgid = (msgid, value)\r\n        lineno, key, v1, line = get_line(f, line, ['msgstr[0]'], lineno)\r\n        lineno, key, v2, line = get_line(f, line, ['msgstr[1]'], lineno)\r\n        msgstr = (v1, v2)\r\n    else:\r\n        msgstr = value\r\n        \r\n    if line != '':\r\n        print 'File: %s Error Line: %s' % (f.name, line)\r\n        raise RuntimeError(\"parse error\")\r\n\r\n    return lineno, comments, msgid, msgstr"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef split_comments(comments):\r\n    flags = []\r\n    other = []\r\n    for c in comments:\r\n        if len(c) > 1 and c[1] == ',':\r\n            flags.append(c)\r\n        else:\r\n            other.append(c)\r\n    return flags, other", "response": "Split COMMENTS into flag comments and other comments."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets form class according form class path or form class object", "response": "def get_form(formcls):\n    \"\"\"\n    get form class according form class path or form class object\n    \"\"\"\n    from uliweb.form import Form\n    import inspect\n    \n    if inspect.isclass(formcls) and issubclass(formcls, Form):\n        return formcls\n    elif isinstance(formcls, (str, unicode)):\n        path = settings.FORMS.get(formcls)\n        if path:\n            _cls = import_attr(path)\n            return _cls\n        else:\n            raise UliwebError(\"Can't find formcls name %s in settings.FORMS\" % formcls)\n    else:\n        raise UliwebError(\"formcls should be Form class object or string path format, but %r found!\" % formcls)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _read(self, ddfile):\n        import csv\n\n        d = {}\n        with open(ddfile) as f:\n            reader = csv.DictReader(f)\n            for row in reader:\n                key = row['name']\n                if key.startswith('#'):\n                    key = key[1:]\n                    deprecated = True\n                else:\n                    deprecated = False\n                value = d.setdefault(key, {})\n                value[self._get_value(row)] = deprecated\n        return d", "response": "Read the data from the log file and return it as a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse the properties of the nacite tables.", "response": "def _parse_prop(self, dd, row):\n        \"\"\"\n        :param dd: datadict\n        :param _row: (tablename, row)\n        :return:\n        \"\"\"\n        key = row['name']\n        if key.startswith('#'):\n            deprecated = True\n        else:\n            deprecated = False\n        v = dd.get(key)\n        _value = self._get_value(row)\n        if not v:\n            v = dd.setdefault(key, {})\n            v[_value] = deprecated\n        else:\n            if not _value in v:\n                v[_value] = deprecated"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nruns the script. Participating actions are looked up in the caller's namespace if no namespace is given, otherwise in the dict provided. Only items that start with action_prefix are processed as actions. If you want to use all items in the namespace provided as actions set action_prefix to an empty string. :param namespace: An optional dict where the functions are looked up in. By default the local namespace of the caller is used. :param action_prefix: The prefix for the functions. Everything else is ignored. :param args: the arguments for the function. If not specified :data:`sys.argv` without the first argument is used.", "response": "def run(namespace=None, action_prefix='action_', args=None):\n    \"\"\"Run the script.  Participating actions are looked up in the caller's\n    namespace if no namespace is given, otherwise in the dict provided.\n    Only items that start with action_prefix are processed as actions.  If\n    you want to use all items in the namespace provided as actions set\n    action_prefix to an empty string.\n\n    :param namespace: An optional dict where the functions are looked up in.\n                      By default the local namespace of the caller is used.\n    :param action_prefix: The prefix for the functions.  Everything else\n                          is ignored.\n    :param args: the arguments for the function.  If not specified\n                 :data:`sys.argv` without the first argument is used.\n    \"\"\"\n    if namespace is None:\n        namespace = sys._getframe(1).f_locals\n    actions = find_actions(namespace, action_prefix)\n\n    if args is None:\n        args = sys.argv[1:]\n    if not args or args[0] in ('-h', '--help'):\n        return print_usage(actions)\n    elif args[0] not in actions:\n        fail('Unknown action \\'%s\\'' % args[0])\n\n    arguments = {}\n    types = {}\n    key_to_arg = {}\n    long_options = []\n    formatstring = ''\n    func, doc, arg_def = actions[args.pop(0)]\n    for idx, (arg, shortcut, default, option_type) in enumerate(arg_def):\n        real_arg = arg.replace('-', '_')\n        if shortcut:\n            formatstring += shortcut\n            if not isinstance(default, bool):\n                formatstring += ':'\n            key_to_arg['-' + shortcut] = real_arg\n        long_options.append(isinstance(default, bool) and arg or arg + '=')\n        key_to_arg['--' + arg] = real_arg\n        key_to_arg[idx] = real_arg\n        types[real_arg] = option_type\n        arguments[real_arg] = default\n\n    try:\n        optlist, posargs = getopt.gnu_getopt(args, formatstring, long_options)\n    except getopt.GetoptError as e:\n        fail(str(e))\n\n    specified_arguments = set()\n    for key, value in enumerate(posargs):\n        try:\n            arg = key_to_arg[key]\n        except IndexError:\n            fail('Too many parameters')\n        specified_arguments.add(arg)\n        try:\n            arguments[arg] = converters[types[arg]](value)\n        except ValueError:\n            fail('Invalid value for argument %s (%s): %s' % (key, arg, value))\n\n    for key, value in optlist:\n        arg = key_to_arg[key]\n        if arg in specified_arguments:\n            fail('Argument \\'%s\\' is specified twice' % arg)\n        if types[arg] == 'boolean':\n            if arg.startswith('no_'):\n                value = 'no'\n            else:\n                value = 'yes'\n        try:\n            arguments[arg] = converters[types[arg]](value)\n        except ValueError:\n            fail('Invalid value for \\'%s\\': %s' % (key, value))\n\n    newargs = {}\n    for k, v in iteritems(arguments):\n        newargs[k.startswith('no_') and k[3:] or k] = v\n    arguments = newargs\n    return func(**arguments)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfailing with an error.", "response": "def fail(message, code=-1):\n    \"\"\"Fail with an error.\"\"\"\n    print('Error: %s' % message, file=sys.stderr)\n    sys.exit(code)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef find_actions(namespace, action_prefix):\n    actions = {}\n    for key, value in iteritems(namespace):\n        if key.startswith(action_prefix):\n            actions[key[len(action_prefix):]] = analyse_action(value)\n    return actions", "response": "Find all the actions in the namespace."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprint the usage information.", "response": "def print_usage(actions):\n    \"\"\"Print the usage information.  (Help screen)\"\"\"\n    actions = actions.items()\n    actions.sort()\n    print('usage: %s <action> [<options>]' % basename(sys.argv[0]))\n    print('       %s --help' % basename(sys.argv[0]))\n    print()\n    print('actions:')\n    for name, (func, doc, arguments) in actions:\n        print('  %s:' % name)\n        for line in doc.splitlines():\n            print('    %s' % line)\n        if arguments:\n            print()\n        for arg, shortcut, default, argtype in arguments:\n            if isinstance(default, bool):\n                print('    %s' % (\n                    (shortcut and '-%s, ' % shortcut or '') + '--' + arg\n                ))\n            else:\n                print('    %-30s%-10s%s' % (\n                    (shortcut and '-%s, ' % shortcut or '') + '--' + arg,\n                    argtype, default\n                ))\n        print()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef make_shell(init_func=None, banner=None, use_ipython=True):\n    if banner is None:\n        banner = 'Interactive Werkzeug Shell'\n    if init_func is None:\n        init_func = dict\n    def action(ipython=use_ipython):\n        \"\"\"Start a new interactive python session.\"\"\"\n        namespace = init_func()\n        if ipython:\n            try:\n                try:\n                    from IPython.frontend.terminal.embed import InteractiveShellEmbed\n                    sh = InteractiveShellEmbed(banner1=banner)\n                except ImportError:\n                    from IPython.Shell import IPShellEmbed\n                    sh = IPShellEmbed(banner=banner)\n            except ImportError:\n                pass\n            else:\n                sh(global_ns={}, local_ns=namespace)\n                return\n        from code import interact\n        interact(banner, local=namespace)\n    return action", "response": "Returns an action callback that spawns a new interactive Werkzeug shell."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a new WSGI application that spawns a new development server.", "response": "def make_runserver(app_factory, hostname='localhost', port=5000,\n                   use_reloader=False, use_debugger=False, use_evalex=True,\n                   threaded=False, processes=1, static_files=None,\n                   extra_files=None, ssl_context=None):\n    \"\"\"Returns an action callback that spawns a new development server.\n\n    .. versionadded:: 0.5\n       `static_files` and `extra_files` was added.\n\n    ..versionadded:: 0.6.1\n       `ssl_context` was added.\n\n    :param app_factory: a function that returns a new WSGI application.\n    :param hostname: the default hostname the server should listen on.\n    :param port: the default port of the server.\n    :param use_reloader: the default setting for the reloader.\n    :param use_evalex: the default setting for the evalex flag of the debugger.\n    :param threaded: the default threading setting.\n    :param processes: the default number of processes to start.\n    :param static_files: optional dict of static files.\n    :param extra_files: optional list of extra files to track for reloading.\n    :param ssl_context: optional SSL context for running server in HTTPS mode.\n    \"\"\"\n    def action(hostname=('h', hostname), port=('p', port),\n               reloader=use_reloader, debugger=use_debugger,\n               evalex=use_evalex, threaded=threaded, processes=processes):\n        \"\"\"Start a new development server.\"\"\"\n        from werkzeug.serving import run_simple\n        app = app_factory()\n        run_simple(hostname, port, app, reloader, debugger, evalex,\n                   extra_files, 1, threaded, processes,\n                   static_files=static_files, ssl_context=ssl_context)\n    return action"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_form_data(environ, stream_factory=None, charset='utf-8',\n                    errors='replace', max_form_memory_size=None,\n                    max_content_length=None, cls=None,\n                    silent=True):\n    \"\"\"Parse the form data in the environ and return it as tuple in the form\n    ``(stream, form, files)``.  You should only call this method if the\n    transport method is `POST`, `PUT`, or `PATCH`.\n\n    If the mimetype of the data transmitted is `multipart/form-data` the\n    files multidict will be filled with `FileStorage` objects.  If the\n    mimetype is unknown the input stream is wrapped and returned as first\n    argument, else the stream is empty.\n\n    This is a shortcut for the common usage of :class:`FormDataParser`.\n\n    Have a look at :ref:`dealing-with-request-data` for more details.\n\n    .. versionadded:: 0.5\n       The `max_form_memory_size`, `max_content_length` and\n       `cls` parameters were added.\n\n    .. versionadded:: 0.5.1\n       The optional `silent` flag was added.\n\n    :param environ: the WSGI environment to be used for parsing.\n    :param stream_factory: An optional callable that returns a new read and\n                           writeable file descriptor.  This callable works\n                           the same as :meth:`~BaseResponse._get_file_stream`.\n    :param charset: The character set for URL and url encoded form data.\n    :param errors: The encoding error behavior.\n    :param max_form_memory_size: the maximum number of bytes to be accepted for\n                           in-memory stored form data.  If the data\n                           exceeds the value specified an\n                           :exc:`~exceptions.RequestEntityTooLarge`\n                           exception is raised.\n    :param max_content_length: If this is provided and the transmitted data\n                               is longer than this value an\n                               :exc:`~exceptions.RequestEntityTooLarge`\n                               exception is raised.\n    :param cls: an optional dict class to use.  If this is not specified\n                       or `None` the default :class:`MultiDict` is used.\n    :param silent: If set to False parsing errors will not be caught.\n    :return: A tuple in the form ``(stream, form, files)``.\n    \"\"\"\n    return FormDataParser(stream_factory, charset, errors,\n                          max_form_memory_size, max_content_length,\n                          cls, silent).parse_from_environ(environ)", "response": "Parse the form data in the WSGI environment and return it as a tuple in the form\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves line ending characters and returns a tuple of the stripped_line and is_terminated.", "response": "def _line_parse(line):\n    \"\"\"Removes line ending characters and returns a tuple (`stripped_line`,\n    `is_terminated`).\n    \"\"\"\n    if line[-2:] in ['\\r\\n', b'\\r\\n']:\n        return line[:-2], True\n    elif line[-1:] in ['\\r', '\\n', b'\\r', b'\\n']:\n        return line[:-1], True\n    return line, False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse a list of headers from an iterable that yields lines.", "response": "def parse_multipart_headers(iterable):\n    \"\"\"Parses multipart headers from an iterable that yields lines (including\n    the trailing newline symbol).  The iterable has to be newline terminated.\n\n    The iterable will stop at the line where the headers ended so it can be\n    further consumed.\n\n    :param iterable: iterable of strings that are newline terminated\n    \"\"\"\n    result = []\n    for line in iterable:\n        line = to_native(line)\n        line, line_terminated = _line_parse(line)\n        if not line_terminated:\n            raise ValueError('unexpected end of line in multipart header')\n        if not line:\n            break\n        elif line[0] in ' \\t' and result:\n            key, value = result[-1]\n            result[-1] = (key, value + '\\n ' + line[1:])\n        else:\n            parts = line.split(':', 1)\n            if len(parts) == 2:\n                result.append((parts[0].strip(), parts[1].strip()))\n\n    # we link the list to the headers, no need to create a copy, the\n    # list was not shared anyways.\n    return Headers(result)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfinding the next line of the terminator.", "response": "def _find_terminator(self, iterator):\n        \"\"\"The terminator might have some additional newlines before it.\n        There is at least one application that sends additional newlines\n        before headers (the python setuptools package).\n        \"\"\"\n        for line in iterator:\n            if not line:\n                break\n            line = line.strip()\n            if line:\n                return line\n        return b''"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_lines(self, file, boundary, content_length):\n        next_part = b'--' + boundary\n        last_part = next_part + b'--'\n\n        iterator = chain(make_line_iter(file, limit=content_length,\n                                        buffer_size=self.buffer_size),\n                         _empty_string_iter)\n\n        terminator = self._find_terminator(iterator)\n\n        if terminator == last_part:\n            return\n        elif terminator != next_part:\n            self.fail('Expected boundary at start of multipart data')\n\n        while terminator != last_part:\n            headers = parse_multipart_headers(iterator)\n\n            disposition = headers.get('content-disposition')\n            if disposition is None:\n                self.fail('Missing Content-Disposition header')\n            disposition, extra = parse_options_header(disposition)\n            transfer_encoding = self.get_part_encoding(headers)\n            name = extra.get('name')\n            filename = extra.get('filename')\n\n            # if no content type is given we stream into memory.  A list is\n            # used as a temporary container.\n            if filename is None:\n                yield _begin_form, (headers, name)\n\n            # otherwise we parse the rest of the headers and ask the stream\n            # factory for something we can write in.\n            else:\n                yield _begin_file, (headers, name, filename)\n\n            buf = b''\n            for line in iterator:\n                if not line:\n                    self.fail('unexpected end of stream')\n\n                if line[:2] == b'--':\n                    terminator = line.rstrip()\n                    if terminator in (next_part, last_part):\n                        break\n\n                if transfer_encoding is not None:\n                    if transfer_encoding == 'base64':\n                        transfer_encoding = 'base64_codec'\n                    try:\n                        line = codecs.decode(line, transfer_encoding)\n                    except Exception:\n                        self.fail('could not decode transfer encoded chunk')\n\n                # we have something in the buffer from the last iteration.\n                # this is usually a newline delimiter.\n                if buf:\n                    yield _cont, buf\n                    buf = b''\n\n                # If the line ends with windows CRLF we write everything except\n                # the last two bytes.  In all other cases however we write\n                # everything except the last byte.  If it was a newline, that's\n                # fine, otherwise it does not matter because we will write it\n                # the next iteration.  this ensures we do not write the\n                # final newline into the stream.  That way we do not have to\n                # truncate the stream.  However we do have to make sure that\n                # if something else than a newline is in there we write it\n                # out.\n                if line[-2:] == b'\\r\\n':\n                    buf = b'\\r\\n'\n                    cutoff = -2\n                else:\n                    buf = line[-1:]\n                    cutoff = -1\n                yield _cont, line[:cutoff]\n\n            else: # pragma: no cover\n                raise ValueError('unexpected end of part')\n\n            # if we have a leftover in the buffer that is not a newline\n            # character we have to flush it, otherwise we will chop of\n            # certain values.\n            if buf not in (b'', b'\\r', b'\\n', b'\\r\\n'):\n                yield _cont, buf\n\n            yield _end, None", "response": "Parse the lines of a multipart file into a list of tuples."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse the file and form parts.", "response": "def parse_parts(self, file, boundary, content_length):\n        \"\"\"Generate ``('file', (name, val))`` and\n        ``('form', (name, val))`` parts.\n        \"\"\"\n        in_memory = 0\n\n        for ellt, ell in self.parse_lines(file, boundary, content_length):\n            if ellt == _begin_file:\n                headers, name, filename = ell\n                is_file = True\n                guard_memory = False\n                filename, container = self.start_file_streaming(\n                    filename, headers, content_length)\n                _write = container.write\n\n            elif ellt == _begin_form:\n                headers, name = ell\n                is_file = False\n                container = []\n                _write = container.append\n                guard_memory = self.max_form_memory_size is not None\n\n            elif ellt == _cont:\n                _write(ell)\n                # if we write into memory and there is a memory size limit we\n                # count the number of bytes in memory and raise an exception if\n                # there is too much data in memory.\n                if guard_memory:\n                    in_memory += len(ell)\n                    if in_memory > self.max_form_memory_size:\n                        self.in_memory_threshold_reached(in_memory)\n\n            elif ellt == _end:\n                if is_file:\n                    container.seek(0)\n                    yield ('file',\n                           (name, FileStorage(container, filename, name,\n                                              headers=headers)))\n                else:\n                    part_charset = self.get_part_charset(headers)\n                    yield ('form',\n                           (name, b''.join(container).decode(\n                                part_charset, self.errors)))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nremoves receiver function WorkItem", "response": "def unbind(topic, func):\r\n    \"\"\"\r\n    Remove receiver function\r\n    \"\"\"\r\n    if topic in _receivers:\r\n        receivers = _receivers[topic]\r\n        for i in range(len(receivers)-1, -1, -1):\r\n            nice, f = receivers[i]\r\n            if (callable(func) and f['func'] == func) or (f['func_name'] == func):\r\n                del receivers[i]\r\n                return"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef call(sender, topic, *args, **kwargs):\r\n    if not topic in _receivers:\r\n        return\r\n    items = _receivers[topic]\r\n    def _cmp(x, y):\r\n        return cmp(x[0], y[0])\r\n    \r\n    items.sort(_cmp)\r\n    i = 0\r\n    while i<len(items):\r\n        nice, f = items[i]\r\n        i = i + 1\r\n        _f = f['func']\r\n        if not _f:\r\n            try:\r\n                _f = import_attr(f['func_name'])\r\n            except (ImportError, AttributeError) as e:\r\n                logging.error(\"Can't import function %s\" % f['func_name'])\r\n                raise\r\n            f['func'] = _f\r\n        if callable(_f):\r\n            kw = kwargs.copy()\r\n            if not _test(kw, f):\r\n                continue\r\n            try:\r\n                _f(sender, *args, **kw)\r\n            except:\r\n                func = _f.__module__ + '.' + _f.__name__\r\n                logging.exception('Calling dispatch point [%s] %s(%r, %r) error!' % (topic, func, args, kw))\r\n                raise\r\n        else:\r\n            raise Exception, \"Dispatch point [%s] %r can't been invoked\" % (topic, _f)", "response": "Invoke receiver functions according topic and return a value."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the first value in the list of items in the given topic.", "response": "def get(sender, topic, *args, **kwargs):\r\n    \"\"\"\r\n    Invoke receiver functions according topic, it'll invoke receiver functions one by one,\r\n    and if one receiver function return non-None value, it'll return it and break\r\n    the loop.\r\n    \"\"\"\r\n    if not topic in _receivers:\r\n        return\r\n    items = _receivers[topic]\r\n    def _cmp(x, y):\r\n        return cmp(x[0], y[0])\r\n    \r\n    items.sort(_cmp)\r\n    for i in range(len(items)):\r\n        nice, f = items[i]\r\n        _f = f['func']\r\n        if not _f:\r\n            try:\r\n                _f = import_attr(f['func_name'])\r\n            except ImportError:\r\n                logging.error(\"Can't import function %s\" % f['func_name'])\r\n                raise\r\n            f['func'] = _f\r\n        if callable(_f):\r\n            if not _test(kwargs, f):\r\n                continue\r\n            try:\r\n                v = _f(sender, *args, **kwargs)\r\n            except:\r\n                func = _f.__module__ + '.' + _f.__name__\r\n                logging.exception('Calling dispatch point [%s] %s(%r,%r) error!' % (topic, func, args, kwargs))\r\n                raise\r\n            if v is not None:\r\n                return v\r\n        else:\r\n            raise \"Dispatch point [%s] %r can't been invoked\" % (topic, _f)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _pre_var(self, value):\r\n\r\n        def sub_(m):\r\n            return self._pre_variables.get(m.group()[2:-1].strip(), '')\r\n\r\n        return r_pre_var.sub(sub_, value)", "response": "replace predefined variables with the value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef __read_line(self, f):\r\n        g = tokenize.generate_tokens(f.readline)\r\n        \r\n        buf = []\r\n        time = 0\r\n        iden_existed = False\r\n        while 1:\r\n            v = g.next()\r\n            tokentype, t, start, end, line = v\r\n            if tokentype == 54:\r\n                continue\r\n            if tokentype in (token.INDENT, token.DEDENT, tokenize.COMMENT):\r\n                continue\r\n            if tokentype == token.NAME:\r\n                iden_existed = True\r\n            if tokentype == token.NEWLINE:\r\n                return ''.join(buf), iden_existed\r\n            else:\r\n                if t == '=' and time == 0:\r\n                    time += 1\r\n                    continue\r\n                buf.append(t)", "response": "Read the logic line according to the syntax not the physical line\r\n            It ll return the line text and bool\r\n           "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef freeze(self):\r\n        self._lazy = False\r\n        for k, v in self.items():\r\n            if k in self._env:\r\n                continue\r\n            for _k, _v in v.items():\r\n                if isinstance(_v, Lazy):\r\n                    if self.writable:\r\n                        _v.get()\r\n                    else:\r\n                        try:\r\n                            v.__setitem__(_k, _v.get(), replace=True)\r\n                        except:\r\n                            print \"Error ini key:\", _k\r\n                            raise\r\n                        del _v\r\n        self._globals = SortedDict()", "response": "Freeze all EvalValue to real value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nquote the value for the cookie.", "response": "def quote(cls, value):\n        \"\"\"Quote the value for the cookie.  This can be any object supported\n        by :attr:`serialization_method`.\n\n        :param value: the value to quote.\n        \"\"\"\n        if cls.serialization_method is not None:\n            value = cls.serialization_method.dumps(value)\n        if cls.quote_base64:\n            value = b''.join(base64.b64encode(value).splitlines()).strip()\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef unquote(cls, value):\n        try:\n            if cls.quote_base64:\n                value = base64.b64decode(value)\n            if cls.serialization_method is not None:\n                value = cls.serialization_method.loads(value)\n            return value\n        except Exception:\n            # unfortunately pickle and other serialization modules can\n            # cause pretty every error here.  if we get one we catch it\n            # and convert it into an UnquoteError\n            raise UnquoteError()", "response": "Unquote the value for the cookie."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nserializing the secure cookie into a string.", "response": "def serialize(self, expires=None):\n        \"\"\"Serialize the secure cookie into a string.\n\n        If expires is provided, the session will be automatically invalidated\n        after expiration when you unseralize it. This provides better\n        protection against session cookie theft.\n\n        :param expires: an optional expiration date for the cookie (a\n                        :class:`datetime.datetime` object)\n        \"\"\"\n        if self.secret_key is None:\n            raise RuntimeError('no secret key defined')\n        if expires:\n            self['_expires'] = _date_to_unix(expires)\n        result = []\n        mac = hmac(self.secret_key, None, self.hash_method)\n        for key, value in sorted(self.items()):\n            result.append(('%s=%s' % (\n                url_quote_plus(key),\n                self.quote(value).decode('ascii')\n            )).encode('ascii'))\n            mac.update(b'|' + result[-1])\n        return b'?'.join([\n            base64.b64encode(mac.digest()).strip(),\n            b'&'.join(result)\n        ])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nloading the secure cookie from a serialized string.", "response": "def unserialize(cls, string, secret_key):\n        \"\"\"Load the secure cookie from a serialized string.\n\n        :param string: the cookie value to unserialize.\n        :param secret_key: the secret key used to serialize the cookie.\n        :return: a new :class:`SecureCookie`.\n        \"\"\"\n        if isinstance(string, text_type):\n            string = string.encode('utf-8', 'replace')\n        if isinstance(secret_key, text_type):\n            secret_key = secret_key.encode('utf-8', 'replace')\n        try:\n            base64_hash, data = string.split(b'?', 1)\n        except (ValueError, IndexError):\n            items = ()\n        else:\n            items = {}\n            mac = hmac(secret_key, None, cls.hash_method)\n            for item in data.split(b'&'):\n                mac.update(b'|' + item)\n                if not b'=' in item:\n                    items = None\n                    break\n                key, value = item.split(b'=', 1)\n                # try to make the key a string\n                key = url_unquote_plus(key.decode('ascii'))\n                try:\n                    key = to_native(key)\n                except UnicodeError:\n                    pass\n                items[key] = value\n\n            # no parsing error and the mac looks okay, we can now\n            # sercurely unpickle our cookie.\n            try:\n                client_hash = base64.b64decode(base64_hash)\n            except TypeError:\n                items = client_hash = None\n            if items is not None and safe_str_cmp(client_hash, mac.digest()):\n                try:\n                    for key, value in iteritems(items):\n                        items[key] = cls.unquote(value)\n                except UnquoteError:\n                    items = ()\n                else:\n                    if '_expires' in items:\n                        if time() > items['_expires']:\n                            items = ()\n                        else:\n                            del items['_expires']\n            else:\n                items = ()\n        return cls(items, secret_key, False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load_cookie(cls, request, key='session', secret_key=None):\n        data = request.cookies.get(key)\n        if not data:\n            return cls(secret_key=secret_key)\n        return cls.unserialize(data, secret_key)", "response": "Loads a : class : SecureCookie from a request."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsave the SecureCookie in a response object.", "response": "def save_cookie(self, response, key='session', expires=None,\n                    session_expires=None, max_age=None, path='/', domain=None,\n                    secure=None, httponly=False, force=False):\n        \"\"\"Saves the SecureCookie in a cookie on response object.  All\n        parameters that are not described here are forwarded directly\n        to :meth:`~BaseResponse.set_cookie`.\n\n        :param response: a response object that has a\n                         :meth:`~BaseResponse.set_cookie` method.\n        :param key: the name of the cookie.\n        :param session_expires: the expiration date of the secure cookie\n                                stored information.  If this is not provided\n                                the cookie `expires` date is used instead.\n        \"\"\"\n        if force or self.should_save:\n            data = self.serialize(session_expires or expires)\n            response.set_cookie(key, data, expires=expires, max_age=max_age,\n                                path=path, domain=domain, secure=secure,\n                                httponly=httponly)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef find_model(sender, model_name):\n    MC = get_mc()\n    model = MC.get((MC.c.model_name==model_name) & (MC.c.uuid!=''))\n    if model:\n        model_inst = model.get_instance()\n        orm.set_model(model_name, model_inst.table_name, appname=__name__, model_path='')\n        return orm.__models__.get(model_name)", "response": "Find a model by name"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets a new model from the database", "response": "def get_model(sender, model_name, model_inst, model_info, model_config):\n    \"\"\"\n    #todo Add objcache support\n    \"\"\"\n    MC = get_mc()\n    if MC:\n        model = MC.get((MC.c.model_name==model_name) & (MC.c.uuid!=''))\n        if model:\n            cached_inst = __cache__.get(model_name)\n            if not cached_inst or (cached_inst and cached_inst[1]!=model.uuid):\n                model_inst = model.get_instance()\n                M = orm.create_model(model_name, fields=eval(model_inst.fields or '[]'),\n                                     indexes=eval(model_inst.indexes or '[]'),\n                                     basemodel=model_inst.basemodel,\n                                     __replace__=True)\n                __cache__[model_name] = (M, model.uuid)\n\n                #process extension model\n                if model_inst.has_extension:\n                    ext_model_name = model_name + '_extension'\n                    fields = eval(model_inst.extension_fields or '[]')\n                    fields.insert(0, {'name':'_parent', 'type':'OneToOne', 'reference_class':model_name, 'collection_name':'ext'})\n                    ME = orm.create_model(ext_model_name, fields=fields,\n                                          indexes=eval(model_inst.extension_indexes or '[]'),\n                                          basemodel=model_inst.extension_model,\n                                          __replace__=True)\n\n            else:\n                M = cached_inst[0]\n\n            return M"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list of dicts that are required by the model.", "response": "def get_model_fields(model, add_reserver_flag=True):\n    \"\"\"\n    Creating fields suit for model_config , id will be skipped.\n    \"\"\"\n    import uliweb.orm as orm\n\n    fields = []\n    m = {'type':'type_name', 'hint':'hint',\n         'default':'default', 'required':'required'}\n    m1 = {'index':'index', 'unique':'unique'}\n    for name, prop in model.properties.items():\n        if name == 'id':\n            continue\n        d = {}\n        for k, v in m.items():\n            d[k] = getattr(prop, v)\n        for k, v in m1.items():\n            d[k] = bool(prop.kwargs.get(v))\n        d['name'] = prop.fieldname or name\n        d['verbose_name'] = unicode(prop.verbose_name)\n        d['nullable'] = bool(prop.kwargs.get('nullable', orm.__nullable__))\n        if d['type'] in ('VARCHAR', 'CHAR', 'BINARY', 'VARBINARY'):\n            d['max_length'] = prop.max_length\n        if d['type'] in ('Reference', 'OneToOne', 'ManyToMany'):\n            d['reference_class'] = prop.reference_class\n            #collection_name will be _collection_name, it the original value\n            d['collection_name'] = prop._collection_name\n        d['server_default'] = prop.kwargs.get('server_default')\n        d['_reserved'] = True\n        fields.append(d)\n\n    return fields"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_model_indexes(model, add_reserver_flag=True):\n    import uliweb.orm as orm\n    from sqlalchemy.engine.reflection import Inspector\n\n    indexes = []\n    engine = model.get_engine().engine\n    insp = Inspector.from_engine(engine)\n    for index in insp.get_indexes(model.tablename):\n        d = {}\n        d['name'] = index['name']\n        d['unique'] = index['unique']\n        d['fields'] = index['column_names']\n        if add_reserver_flag:\n            d['_reserved'] = True\n        indexes.append(d)\n\n    return indexes", "response": "Returns a list of dicts that can be used to create the model s indexes."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if a host is trusted against a list of trusted hosts.", "response": "def host_is_trusted(hostname, trusted_list):\n    \"\"\"Checks if a host is trusted against a list.  This also takes care\n    of port normalization.\n\n    .. versionadded:: 0.9\n\n    :param hostname: the hostname to check\n    :param trusted_list: a list of hostnames to check against.  If a\n                         hostname starts with a dot it will match against\n                         all subdomains as well.\n    \"\"\"\n    if not hostname:\n        return False\n\n    if isinstance(trusted_list, string_types):\n        trusted_list = [trusted_list]\n\n    def _normalize(hostname):\n        if ':' in hostname:\n            hostname = hostname.rsplit(':', 1)[0]\n        return _encode_idna(hostname)\n\n    hostname = _normalize(hostname)\n    for ref in trusted_list:\n        if ref.startswith('.'):\n            ref = ref[1:]\n            suffix_match = True\n        else:\n            suffix_match = False\n        ref = _normalize(ref)\n        if ref == hostname:\n            return True\n        if suffix_match and hostname.endswith('.' + ref):\n            return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the content length from the WSGI environment as integer. If it s not available None is returned.", "response": "def get_content_length(environ):\n    \"\"\"Returns the content length from the WSGI environment as\n    integer.  If it's not available `None` is returned.\n\n    .. versionadded:: 0.9\n\n    :param environ: the WSGI environ to fetch the content length from.\n    \"\"\"\n    content_length = environ.get('CONTENT_LENGTH')\n    if content_length is not None:\n        try:\n            return max(0, int(content_length))\n        except (ValueError, TypeError):\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_input_stream(environ, safe_fallback=True):\n    stream = environ['wsgi.input']\n    content_length = get_content_length(environ)\n\n    # A wsgi extension that tells us if the input is terminated.  In\n    # that case we return the stream unchanged as we know we can savely\n    # read it until the end.\n    if environ.get('wsgi.input_terminated'):\n        return stream\n\n    # If we don't have a content length we fall back to an empty stream\n    # in case of a safe fallback, otherwise we return the stream unchanged.\n    # The non-safe fallback is not recommended but might be useful in\n    # some situations.\n    if content_length is None:\n        return safe_fallback and _empty_stream or stream\n\n    # Otherwise limit the stream to the content length\n    return LimitedStream(stream, content_length)", "response": "Returns the input stream from the WSGI environment and wraps it in a LimitedStream if it can t be read."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_path_info(environ, charset='utf-8', errors='replace'):\n    path = wsgi_get_bytes(environ.get('PATH_INFO', ''))\n    return to_unicode(path, charset, errors, allow_none_charset=True)", "response": "Returns the PATH_INFO from the WSGI environment and properly\n    decodes it."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving and returns the next segment of PATH_INFO pushed onto the SCRIPT_NAME. Returns None if there is nothing left on PATH_INFO.", "response": "def pop_path_info(environ, charset='utf-8', errors='replace'):\n    \"\"\"Removes and returns the next segment of `PATH_INFO`, pushing it onto\n    `SCRIPT_NAME`.  Returns `None` if there is nothing left on `PATH_INFO`.\n\n    If the `charset` is set to `None` a bytestring is returned.\n\n    If there are empty segments (``'/foo//bar``) these are ignored but\n    properly pushed to the `SCRIPT_NAME`:\n\n    >>> env = {'SCRIPT_NAME': '/foo', 'PATH_INFO': '/a/b'}\n    >>> pop_path_info(env)\n    'a'\n    >>> env['SCRIPT_NAME']\n    '/foo/a'\n    >>> pop_path_info(env)\n    'b'\n    >>> env['SCRIPT_NAME']\n    '/foo/a/b'\n\n    .. versionadded:: 0.5\n\n    .. versionchanged:: 0.9\n       The path is now decoded and a charset and encoding\n       parameter can be provided.\n\n    :param environ: the WSGI environment that is modified.\n    \"\"\"\n    path = environ.get('PATH_INFO')\n    if not path:\n        return None\n\n    script_name = environ.get('SCRIPT_NAME', '')\n\n    # shift multiple leading slashes over\n    old_path = path\n    path = path.lstrip('/')\n    if path != old_path:\n        script_name += '/' * (len(old_path) - len(path))\n\n    if '/' not in path:\n        environ['PATH_INFO'] = ''\n        environ['SCRIPT_NAME'] = script_name + path\n        rv = wsgi_get_bytes(path)\n    else:\n        segment, path = path.split('/', 1)\n        environ['PATH_INFO'] = '/' + path\n        environ['SCRIPT_NAME'] = script_name + segment\n        rv = wsgi_get_bytes(segment)\n\n    return to_unicode(rv, charset, errors, allow_none_charset=True)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the next segment of the PATH_INFO environment variable.", "response": "def peek_path_info(environ, charset='utf-8', errors='replace'):\n    \"\"\"Returns the next segment on the `PATH_INFO` or `None` if there\n    is none.  Works like :func:`pop_path_info` without modifying the\n    environment:\n\n    >>> env = {'SCRIPT_NAME': '/foo', 'PATH_INFO': '/a/b'}\n    >>> peek_path_info(env)\n    'a'\n    >>> peek_path_info(env)\n    'a'\n\n    If the `charset` is set to `None` a bytestring is returned.\n\n    .. versionadded:: 0.5\n\n    .. versionchanged:: 0.9\n       The path is now decoded and a charset and encoding\n       parameter can be provided.\n\n    :param environ: the WSGI environment that is checked.\n    \"\"\"\n    segments = environ.get('PATH_INFO', '').lstrip('/').split('/', 1)\n    if segments:\n        return to_unicode(wsgi_get_bytes(segments[0]),\n                          charset, errors, allow_none_charset=True)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nextracting the path info from the given URL or WSGI environment dict and the given path.", "response": "def extract_path_info(environ_or_baseurl, path_or_url, charset='utf-8',\n                      errors='replace', collapse_http_schemes=True):\n    \"\"\"Extracts the path info from the given URL (or WSGI environment) and\n    path.  The path info returned is a unicode string, not a bytestring\n    suitable for a WSGI environment.  The URLs might also be IRIs.\n\n    If the path info could not be determined, `None` is returned.\n\n    Some examples:\n\n    >>> extract_path_info('http://example.com/app', '/app/hello')\n    u'/hello'\n    >>> extract_path_info('http://example.com/app',\n    ...                   'https://example.com/app/hello')\n    u'/hello'\n    >>> extract_path_info('http://example.com/app',\n    ...                   'https://example.com/app/hello',\n    ...                   collapse_http_schemes=False) is None\n    True\n\n    Instead of providing a base URL you can also pass a WSGI environment.\n\n    .. versionadded:: 0.6\n\n    :param environ_or_baseurl: a WSGI environment dict, a base URL or\n                               base IRI.  This is the root of the\n                               application.\n    :param path_or_url: an absolute path from the server root, a\n                        relative path (in which case it's the path info)\n                        or a full URL.  Also accepts IRIs and unicode\n                        parameters.\n    :param charset: the charset for byte data in URLs\n    :param errors: the error handling on decode\n    :param collapse_http_schemes: if set to `False` the algorithm does\n                                  not assume that http and https on the\n                                  same server point to the same\n                                  resource.\n    \"\"\"\n    def _normalize_netloc(scheme, netloc):\n        parts = netloc.split(u'@', 1)[-1].split(u':', 1)\n        if len(parts) == 2:\n            netloc, port = parts\n            if (scheme == u'http' and port == u'80') or \\\n               (scheme == u'https' and port == u'443'):\n                port = None\n        else:\n            netloc = parts[0]\n            port = None\n        if port is not None:\n            netloc += u':' + port\n        return netloc\n\n    # make sure whatever we are working on is a IRI and parse it\n    path = uri_to_iri(path_or_url, charset, errors)\n    if isinstance(environ_or_baseurl, dict):\n        environ_or_baseurl = get_current_url(environ_or_baseurl,\n                                             root_only=True)\n    base_iri = uri_to_iri(environ_or_baseurl, charset, errors)\n    base_scheme, base_netloc, base_path = url_parse(base_iri)[:3]\n    cur_scheme, cur_netloc, cur_path, = \\\n        url_parse(url_join(base_iri, path))[:3]\n\n    # normalize the network location\n    base_netloc = _normalize_netloc(base_scheme, base_netloc)\n    cur_netloc = _normalize_netloc(cur_scheme, cur_netloc)\n\n    # is that IRI even on a known HTTP scheme?\n    if collapse_http_schemes:\n        for scheme in base_scheme, cur_scheme:\n            if scheme not in (u'http', u'https'):\n                return None\n    else:\n        if not (base_scheme in (u'http', u'https') and \\\n                base_scheme == cur_scheme):\n            return None\n\n    # are the netlocs compatible?\n    if base_netloc != cur_netloc:\n        return None\n\n    # are we below the application path?\n    base_path = base_path.rstrip(u'/')\n    if not cur_path.startswith(base_path):\n        return None\n\n    return u'/' + cur_path[len(base_path):].lstrip(u'/')"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns an iterator over the content of a node - level tree tree", "response": "def make_chunk_iter(stream, separator, limit=None, buffer_size=10 * 1024):\n    \"\"\"Works like :func:`make_line_iter` but accepts a separator\n    which divides chunks.  If you want newline based processing\n    you should use :func:`make_line_iter` instead as it\n    supports arbitrary newline markers.\n\n    .. versionadded:: 0.8\n\n    .. versionadded:: 0.9\n       added support for iterators as input stream.\n\n    :param stream: the stream or iterate to iterate over.\n    :param separator: the separator that divides chunks.\n    :param limit: the limit in bytes for the stream.  (Usually\n                  content length.  Not necessary if the `stream`\n                  is otherwise already limited).\n    :param buffer_size: The optional buffer size.\n    \"\"\"\n    _iter = _make_chunk_iter(stream, limit, buffer_size)\n\n    first_item = next(_iter, '')\n    if not first_item:\n        return\n\n    _iter = chain((first_item,), _iter)\n    if isinstance(first_item, text_type):\n        separator = to_unicode(separator)\n        _split = re.compile(r'(%s)' % re.escape(separator)).split\n        _join = u''.join\n    else:\n        separator = to_bytes(separator)\n        _split = re.compile(b'(' + re.escape(separator) + b')').split\n        _join = b''.join\n\n    buffer = []\n    while 1:\n        new_data = next(_iter, '')\n        if not new_data:\n            break\n        chunks = _split(new_data)\n        new_buf = []\n        for item in chain(buffer, chunks):\n            if item == separator:\n                yield _join(new_buf)\n                new_buf = []\n            else:\n                new_buf.append(item)\n        buffer = new_buf\n    if buffer:\n        yield _join(buffer)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsoaps supports multiple SOAP function collections it ll save functions to target dict", "response": "def soap(func=None, name=None, returns=None, args=None, doc=None, target='SOAP'):\n    \"\"\"\n    soap supports multiple SOAP function collections, it'll save functions to \n    target dict, and you can give other target, but it should be keep up with\n    SoapView.target definition.\n    \"\"\"\n    global __soap_functions__\n    \n    returns = _fix_soap_kwargs(returns)\n    args = _fix_soap_kwargs(args)\n    \n    if isinstance(func, str) and not name:\n        return partial(soap, name=func, returns=returns, args=args, doc=doc, target=target)\n    \n    if not func:\n        return partial(soap, name=name, returns=returns, args=args, doc=doc, target=target)\n\n    target_functions = __soap_functions__.setdefault(target, {})\n    if inspect.isfunction(func):\n        f_name = func.__name__\n        if name:\n            f_name = name\n        target_functions[f_name] = endpoint = ('.'.join([func.__module__, func.__name__]), returns, args, doc)\n        func.soap_endpoint = (f_name, endpoint)\n    elif inspect.isclass(func):\n        if not name:\n            name = func.__name__\n        for _name in dir(func):\n            f = getattr(func, _name)\n            if (inspect.ismethod(f) or inspect.isfunction(f)) and not _name.startswith('_'):\n                f_name = name + '.' + f.__name__\n                endpoint = ('.'.join([func.__module__, func.__name__, _name]), returns, args, doc)\n                if hasattr(f, 'soap_endpoint'):\n                    #the method has already been decorate by soap \n                    _n, _e = f.soap_endpoint\n                    target_functions[name + '.' + _n] = endpoint\n                    del target_functions[_n]\n                else:\n                    target_functions[f_name] = endpoint\n    else:\n        raise Exception(\"Can't support this type [%r]\" % func)\n    return func"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nturning the DataFrame returned by getData into a dictionary", "response": "def getJsonData(self, params):\n        \"\"\"turns the DataFrame returned by getData into a dictionary\n\n        arguments:\n        the params passed used for table or d3 outputs are forwarded on to getData\n        \"\"\"\n        try:\n            return eval(\"self.\" + str(params['output_id']) + \"(params)\")\n        except AttributeError:\n            df = self.getData(params)\n            if df is None:\n                return None\n            return df.to_dict(orient='records')"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\noverrides this function arguments", "response": "def getDownload(self, params):\n        \"\"\"Override this function\n\n        arguments: params (dict)\n        returns: path to file or buffer to be downloaded (string or buffer)\n        \"\"\"\n        df = self.getData(params)\n        buffer = io.StringIO()\n        df.to_csv(buffer, index=False, encoding='utf-8')\n        filepath = buffer\n        return filepath"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getPlot(self, params):\n        try:\n            return eval(\"self.\" + str(params['output_id']) + \"(params)\")\n        except AttributeError:\n            df = self.getData(params)\n            if df is None:\n                return None\n            return df.plot()", "response": "Override this function\n            returns matplotlib. pyplot figure\n       "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef launch_in_notebook(self, port=9095, width=900, height=600):\n        from IPython.lib import backgroundjobs as bg\n        from IPython.display import HTML\n\n        jobs = bg.BackgroundJobManager()\n        jobs.new(self.launch, kw=dict(port=port))\n        frame = HTML(\n            '<iframe src=http://localhost:{} width={} height={}></iframe>'\n            .format(port, width, height)\n        )\n        return frame", "response": "launch the app within an iframe in ipython notebook"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef launch(self, host=\"local\", port=8080):\n\n        # Need to add in the appbar if many apps\n        self.root.templateVars['app_bar'] = self.site_app_bar\n        for fullRoute, _ in self.site_app_bar[1:]:\n            parent, route = self.get_route(fullRoute)\n            parent.__dict__[route].templateVars['app_bar'] = self.site_app_bar\n\n        if host != \"local\":\n            cherrypy.server.socket_host = '0.0.0.0'\n        cherrypy.server.socket_port = port\n        cherrypy.quickstart(self.root)", "response": "This method is called by the Launch method of a CherryPy object. It will serve the top\n            node of the CherryPy object tree."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef exclude_args(parser, args, excluded_args, target):\n    msg = '\"%s\" option invalid for %s'\n    for argname in excluded_args:\n        if argname not in args:\n            continue\n        if args[argname]:\n            optname = \"--%s\" % argname.replace(\"_\", \"-\")\n            parser.error(msg % (optname, target))\n        del args[argname]", "response": "Delete options that are not appropriate for a particular code path."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef build_master_ufos(\n        self,\n        glyphs_path,\n        designspace_path=None,\n        master_dir=None,\n        instance_dir=None,\n        family_name=None,\n        mti_source=None,\n    ):\n        \"\"\"Build UFOs and MutatorMath designspace from Glyphs source.\"\"\"\n        import glyphsLib\n\n        if master_dir is None:\n            master_dir = self._output_dir(\"ufo\")\n        if not os.path.isdir(master_dir):\n            os.mkdir(master_dir)\n        if instance_dir is None:\n            instance_dir = self._output_dir(\"ufo\", is_instance=True)\n        if not os.path.isdir(instance_dir):\n            os.mkdir(instance_dir)\n\n        font = glyphsLib.GSFont(glyphs_path)\n\n        if designspace_path is not None:\n            designspace_dir = os.path.dirname(designspace_path)\n        else:\n            designspace_dir = master_dir\n        # glyphsLib.to_designspace expects instance_dir to be relative\n        instance_dir = os.path.relpath(instance_dir, designspace_dir)\n\n        designspace = glyphsLib.to_designspace(\n            font, family_name=family_name, instance_dir=instance_dir\n        )\n\n        masters = {}\n        # multiple sources can have the same font/filename (but different layer),\n        # we want to save a font only once\n        for source in designspace.sources:\n            if source.filename in masters:\n                assert source.font is masters[source.filename]\n                continue\n            ufo_path = os.path.join(master_dir, source.filename)\n            # no need to also set the relative 'filename' attribute as that\n            # will be auto-updated on writing the designspace document\n            source.path = ufo_path\n            source.font.save(ufo_path)\n            masters[source.filename] = source.font\n\n        if designspace_path is None:\n            designspace_path = os.path.join(master_dir, designspace.filename)\n        designspace.write(designspace_path)\n        if mti_source:\n            self.add_mti_features_to_master_ufos(mti_source, masters.values())\n        return designspace_path", "response": "Build UFOs and MutatorMath designspace from Glyphs source."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nremoving overlaps in UFOs glyphs contours.", "response": "def remove_overlaps(self, ufos, glyph_filter=lambda g: len(g)):\n        \"\"\"Remove overlaps in UFOs' glyphs' contours.\"\"\"\n        from booleanOperations import union, BooleanOperationsError\n\n        for ufo in ufos:\n            font_name = self._font_name(ufo)\n            logger.info(\"Removing overlaps for \" + font_name)\n            for glyph in ufo:\n                if not glyph_filter(glyph):\n                    continue\n                contours = list(glyph)\n                glyph.clearContours()\n                try:\n                    union(contours, glyph.getPointPen())\n                except BooleanOperationsError:\n                    logger.error(\n                        \"Failed to remove overlaps for %s: %r\", font_name, glyph.name\n                    )\n                    raise"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmove components of UFOs glyphs to their outlines.", "response": "def decompose_glyphs(self, ufos, glyph_filter=lambda g: True):\n        \"\"\"Move components of UFOs' glyphs to their outlines.\"\"\"\n\n        for ufo in ufos:\n            logger.info(\"Decomposing glyphs for \" + self._font_name(ufo))\n            for glyph in ufo:\n                if not glyph.components or not glyph_filter(glyph):\n                    continue\n                self._deep_copy_contours(ufo, glyph, glyph, Transform())\n                glyph.clearComponents()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _deep_copy_contours(self, ufo, parent, component, transformation):\n\n        for nested in component.components:\n            self._deep_copy_contours(\n                ufo,\n                parent,\n                ufo[nested.baseGlyph],\n                transformation.transform(nested.transformation),\n            )\n\n        if component != parent:\n            pen = TransformPen(parent.getPen(), transformation)\n\n            # if the transformation has a negative determinant, it will reverse\n            # the contour direction of the component\n            xx, xy, yx, yy = transformation[:4]\n            if xx * yy - xy * yx < 0:\n                pen = ReverseContourPen(pen)\n\n            component.draw(pen)", "response": "Copy contours from component to parent including nested components."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef build_ttfs(self, ufos, **kwargs):\n        self.save_otfs(ufos, ttf=True, **kwargs)", "response": "Build OpenType binaries with TrueType outlines."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbuilding OpenType binaries with interpolatable TrueType outlines from DesignSpaceDocument object.", "response": "def build_interpolatable_ttfs(self, designspace, **kwargs):\n        \"\"\"Build OpenType binaries with interpolatable TrueType outlines\n        from DesignSpaceDocument object.\n        \"\"\"\n        return self._build_interpolatable_masters(designspace, ttf=True, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef build_interpolatable_otfs(self, designspace, **kwargs):\n        return self._build_interpolatable_masters(designspace, ttf=False, **kwargs)", "response": "Build OpenType binaries with interpolatable TrueType outlines\n        from DesignSpaceDocument object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbuilds OpenType variable font from masters in a designspace.", "response": "def build_variable_font(\n        self,\n        designspace,\n        output_path=None,\n        output_dir=None,\n        master_bin_dir=None,\n        ttf=True,\n    ):\n        \"\"\"Build OpenType variable font from masters in a designspace.\"\"\"\n        assert not (output_path and output_dir), \"mutually exclusive args\"\n\n        ext = \"ttf\" if ttf else \"otf\"\n\n        if hasattr(designspace, \"__fspath__\"):\n            designspace = designspace.__fspath__()\n        if isinstance(designspace, basestring):\n            designspace = designspaceLib.DesignSpaceDocument.fromfile(designspace)\n            if master_bin_dir is None:\n                master_bin_dir = self._output_dir(ext, interpolatable=True)\n            finder = partial(_varLib_finder, directory=master_bin_dir)\n        else:\n            assert all(isinstance(s.font, TTFont) for s in designspace.sources)\n            finder = lambda s: s  # noqa: E731\n\n        if output_path is None:\n            output_path = (\n                os.path.splitext(os.path.basename(designspace.path))[0] + \"-VF\"\n            )\n            output_path = self._output_path(\n                output_path, ext, is_variable=True, output_dir=output_dir\n            )\n\n        logger.info(\"Building variable font \" + output_path)\n\n        font, _, _ = varLib.build(designspace, finder)\n\n        font.save(output_path)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef save_otfs(\n        self,\n        ufos,\n        ttf=False,\n        is_instance=False,\n        interpolatable=False,\n        use_afdko=False,\n        autohint=None,\n        subset=None,\n        use_production_names=None,\n        subroutinize=None,  # deprecated\n        optimize_cff=CFFOptimization.NONE,\n        cff_round_tolerance=None,\n        remove_overlaps=True,\n        overlaps_backend=None,\n        reverse_direction=True,\n        conversion_error=None,\n        feature_writers=None,\n        interpolate_layout_from=None,\n        interpolate_layout_dir=None,\n        output_path=None,\n        output_dir=None,\n        inplace=True,\n    ):\n        \"\"\"Build OpenType binaries from UFOs.\n\n        Args:\n            ufos: Font objects to compile.\n            ttf: If True, build fonts with TrueType outlines and .ttf extension.\n            is_instance: If output fonts are instances, for generating paths.\n            interpolatable: If output is interpolatable, for generating paths.\n            use_afdko: If True, use AFDKO to compile feature source.\n            autohint: Parameters to provide to ttfautohint. If not provided, the\n                autohinting step is skipped.\n            subset: Whether to subset the output according to data in the UFOs.\n                If not provided, also determined by flags in the UFOs.\n            use_production_names: Whether to use production glyph names in the\n                output. If not provided, determined by flags in the UFOs.\n            subroutinize: If True, subroutinize CFF outlines in output.\n            cff_round_tolerance (float): controls the rounding of point\n                coordinates in CFF table. It is defined as the maximum absolute\n                difference between the original float and the rounded integer\n                value. By default, all floats are rounded to integer (tolerance\n                0.5); a value of 0 completely disables rounding; values in\n                between only round floats which are close to their integral\n                part within the tolerated range. Ignored if ttf=True.\n            remove_overlaps: If True, remove overlaps in glyph shapes.\n            overlaps_backend: name of the library to remove overlaps. Can be\n                either \"booleanOperations\" (default) or \"pathops\".\n            reverse_direction: If True, reverse contour directions when\n                compiling TrueType outlines.\n            conversion_error: Error to allow when converting cubic CFF contours\n                to quadratic TrueType contours.\n            feature_writers: list of ufo2ft-compatible feature writer classes\n                or pre-initialized objects that are passed on to ufo2ft\n                feature compiler to generate automatic feature code. The\n                default value (None) means that ufo2ft will use its built-in\n                default feature writers (for kern, mark, mkmk, etc.). An empty\n                list ([]) will skip any automatic feature generation.\n            interpolate_layout_from: A DesignSpaceDocument object to give varLib\n                for interpolating layout tables to use in output.\n            interpolate_layout_dir: Directory containing the compiled master\n                fonts to use for interpolating binary layout tables.\n            output_path: output font file path. Only works when the input\n                'ufos' list contains a single font.\n            output_dir: directory where to save output files. Mutually\n                exclusive with 'output_path' argument.\n        \"\"\"\n        assert not (output_path and output_dir), \"mutually exclusive args\"\n\n        if output_path is not None and len(ufos) > 1:\n            raise ValueError(\"output_path requires a single input\")\n\n        if subroutinize is not None:\n            import warnings\n\n            warnings.warn(\n                \"the 'subroutinize' argument is deprecated, use 'optimize_cff'\",\n                UserWarning,\n            )\n            if subroutinize:\n                optimize_cff = CFFOptimization.SUBROUTINIZE\n            else:\n                # for b/w compatibility, we still run the charstring specializer\n                # even when --no-subroutinize is used. Use the new --optimize-cff\n                # option to disable both specilization and subroutinization\n                optimize_cff = CFFOptimization.SPECIALIZE\n\n        ext = \"ttf\" if ttf else \"otf\"\n\n        if interpolate_layout_from is not None:\n            if interpolate_layout_dir is None:\n                interpolate_layout_dir = self._output_dir(\n                    ext, is_instance=False, interpolatable=interpolatable\n                )\n            finder = partial(_varLib_finder, directory=interpolate_layout_dir, ext=ext)\n            # no need to generate automatic features in ufo2ft, since here we\n            # are interpolating precompiled GPOS table with fontTools.varLib.\n            # An empty 'featureWriters' list tells ufo2ft to not generate any\n            # automatic features.\n            # TODO: Add an argument to ufo2ft.compileOTF/compileTTF to\n            # completely skip compiling features into OTL tables\n            feature_writers = []\n\n        compiler_options = dict(\n            useProductionNames=use_production_names,\n            reverseDirection=reverse_direction,\n            cubicConversionError=conversion_error,\n            featureWriters=feature_writers,\n            inplace=True,  # avoid extra copy\n        )\n        if use_afdko:\n            compiler_options[\"featureCompilerClass\"] = FDKFeatureCompiler\n\n        if interpolatable:\n            if not ttf:\n                raise NotImplementedError(\"interpolatable CFF not supported yet\")\n\n            logger.info(\"Building interpolation-compatible TTFs\")\n\n            fonts = ufo2ft.compileInterpolatableTTFs(ufos, **compiler_options)\n        else:\n            fonts = self._iter_compile(\n                ufos,\n                ttf,\n                removeOverlaps=remove_overlaps,\n                overlapsBackend=overlaps_backend,\n                optimizeCFF=optimize_cff,\n                roundTolerance=cff_round_tolerance,\n                **compiler_options\n            )\n\n        do_autohint = ttf and autohint is not None\n\n        for font, ufo in zip(fonts, ufos):\n\n            if interpolate_layout_from is not None:\n                master_locations, instance_locations = self._designspace_locations(\n                    interpolate_layout_from\n                )\n                loc = instance_locations[_normpath(ufo.path)]\n                gpos_src = interpolate_layout(\n                    interpolate_layout_from, loc, finder, mapped=True\n                )\n                font[\"GPOS\"] = gpos_src[\"GPOS\"]\n                gsub_src = TTFont(finder(self._closest_location(master_locations, loc)))\n                if \"GDEF\" in gsub_src:\n                    font[\"GDEF\"] = gsub_src[\"GDEF\"]\n                if \"GSUB\" in gsub_src:\n                    font[\"GSUB\"] = gsub_src[\"GSUB\"]\n\n            if do_autohint:\n                # if we are autohinting, we save the unhinted font to a\n                # temporary path, and the hinted one to the final destination\n                fd, otf_path = tempfile.mkstemp(\".\" + ext)\n                os.close(fd)\n            elif output_path is None:\n                otf_path = self._output_path(\n                    ufo, ext, is_instance, interpolatable, output_dir=output_dir\n                )\n            else:\n                otf_path = output_path\n\n            logger.info(\"Saving %s\", otf_path)\n            font.save(otf_path)\n\n            # 'subset' is an Optional[bool], can be None, True or False.\n            # When False, we never subset; when True, we always do; when\n            # None (default), we check the presence of custom parameters\n            if subset is False:\n                pass\n            elif subset is True or (\n                (KEEP_GLYPHS_OLD_KEY in ufo.lib or KEEP_GLYPHS_NEW_KEY in ufo.lib)\n                or any(glyph.lib.get(GLYPH_EXPORT_KEY, True) is False for glyph in ufo)\n            ):\n                self.subset_otf_from_ufo(otf_path, ufo)\n\n            if not do_autohint:\n                continue\n\n            if output_path is not None:\n                hinted_otf_path = output_path\n            else:\n                hinted_otf_path = self._output_path(\n                    ufo,\n                    ext,\n                    is_instance,\n                    interpolatable,\n                    autohinted=True,\n                    output_dir=output_dir,\n                )\n            try:\n                ttfautohint(otf_path, hinted_otf_path, args=autohint)\n            except TTFAError:\n                # copy unhinted font to destination before re-raising error\n                shutil.copyfile(otf_path, hinted_otf_path)\n                raise\n            finally:\n                # must clean up temp file\n                os.remove(otf_path)", "response": "Save OpenType binaries from UFOs."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef subset_otf_from_ufo(self, otf_path, ufo):\n        from fontTools import subset\n\n        # ufo2ft always inserts a \".notdef\" glyph as the first glyph\n        ufo_order = makeOfficialGlyphOrder(ufo)\n        if \".notdef\" not in ufo_order:\n            ufo_order.insert(0, \".notdef\")\n        ot_order = TTFont(otf_path).getGlyphOrder()\n        assert ot_order[0] == \".notdef\"\n        assert len(ufo_order) == len(ot_order)\n\n        for key in (KEEP_GLYPHS_NEW_KEY, KEEP_GLYPHS_OLD_KEY):\n            keep_glyphs_list = ufo.lib.get(key)\n            if keep_glyphs_list is not None:\n                keep_glyphs = set(keep_glyphs_list)\n                break\n        else:\n            keep_glyphs = None\n\n        include = []\n        for source_name, binary_name in zip(ufo_order, ot_order):\n            if keep_glyphs and source_name not in keep_glyphs:\n                continue\n\n            if source_name in ufo:\n                exported = ufo[source_name].lib.get(GLYPH_EXPORT_KEY, True)\n                if not exported:\n                    continue\n\n            include.append(binary_name)\n\n        # copied from nototools.subset\n        opt = subset.Options()\n        opt.name_IDs = [\"*\"]\n        opt.name_legacy = True\n        opt.name_languages = [\"*\"]\n        opt.layout_features = [\"*\"]\n        opt.notdef_outline = True\n        opt.recalc_bounds = True\n        opt.recalc_timestamp = True\n        opt.canonical_order = True\n\n        opt.glyph_names = True\n\n        font = subset.load_font(otf_path, opt, lazy=False)\n        subsetter = subset.Subsetter(options=opt)\n        subsetter.populate(glyphs=include)\n        subsetter.subset(font)\n        subset.save_font(font, otf_path, opt)", "response": "Subset a font using export flags set by glyphsLib."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run_from_glyphs(\n        self,\n        glyphs_path,\n        designspace_path=None,\n        master_dir=None,\n        instance_dir=None,\n        family_name=None,\n        mti_source=None,\n        **kwargs\n    ):\n        \"\"\"Run toolchain from Glyphs source.\n\n        Args:\n            glyphs_path: Path to source file.\n            designspace_path: Output path of generated designspace document.\n                By default it's \"<family_name>[-<base_style>].designspace\".\n            master_dir: Directory where to save UFO masters (default:\n                \"master_ufo\").\n            instance_dir: Directory where to save UFO instances (default:\n                \"instance_ufo\").\n            family_name: If provided, uses this family name in the output.\n            mti_source: Path to property list file containing a dictionary\n                mapping UFO masters to dictionaries mapping layout table\n                tags to MTI source paths which should be compiled into\n                those tables.\n            kwargs: Arguments passed along to run_from_designspace.\n        \"\"\"\n\n        logger.info(\"Building master UFOs and designspace from Glyphs source\")\n        designspace_path = self.build_master_ufos(\n            glyphs_path,\n            designspace_path=designspace_path,\n            master_dir=master_dir,\n            instance_dir=instance_dir,\n            family_name=family_name,\n            mti_source=mti_source,\n        )\n        self.run_from_designspace(designspace_path, **kwargs)", "response": "This function will run toolchain from Glyphs source."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef interpolate_instance_ufos(\n        self,\n        designspace,\n        include=None,\n        round_instances=False,\n        expand_features_to_instances=False,\n    ):\n        \"\"\"Interpolate master UFOs with MutatorMath and return instance UFOs.\n\n        Args:\n            designspace: a DesignSpaceDocument object containing sources and\n                instances.\n            include (str): optional regular expression pattern to match the\n                DS instance 'name' attribute and only interpolate the matching\n                instances.\n            round_instances (bool): round instances' coordinates to integer.\n            expand_features_to_instances: parses the master feature file, expands all\n                include()s and writes the resulting full feature file to all instance\n                UFOs. Use this if you share feature files among masters in external\n                files. Otherwise, the relative include paths can break as instances\n                may end up elsewhere. Only done on interpolation.\n        Returns:\n            list of defcon.Font objects corresponding to the UFO instances.\n        Raises:\n            FontmakeError: if any of the sources defines a custom 'layer', for\n                this is not supported by MutatorMath.\n            ValueError: \"expand_features_to_instances\" is True but no source in the\n                designspace document is designated with '<features copy=\"1\"/>'.\n        \"\"\"\n        from glyphsLib.interpolation import apply_instance_data\n        from mutatorMath.ufo.document import DesignSpaceDocumentReader\n\n        if any(source.layerName is not None for source in designspace.sources):\n            raise FontmakeError(\n                \"MutatorMath doesn't support DesignSpace sources with 'layer' \"\n                \"attribute\"\n            )\n\n        # TODO: replace mutatorMath with ufoProcessor?\n        builder = DesignSpaceDocumentReader(\n            designspace.path, ufoVersion=3, roundGeometry=round_instances, verbose=True\n        )\n        logger.info(\"Interpolating master UFOs from designspace\")\n        if include is not None:\n            instances = self._search_instances(designspace, pattern=include)\n            for instance_name in instances:\n                builder.readInstance((\"name\", instance_name))\n            filenames = set(instances.values())\n        else:\n            builder.readInstances()\n            filenames = None  # will include all instances\n        logger.info(\"Applying instance data from designspace\")\n        instance_ufos = apply_instance_data(designspace, include_filenames=filenames)\n\n        if expand_features_to_instances:\n            logger.debug(\"Expanding features to instance UFOs\")\n            master_source = next(\n                (s for s in designspace.sources if s.copyFeatures), None\n            )\n            if not master_source:\n                raise ValueError(\"No source is designated as the master for features.\")\n            else:\n                master_source_font = builder.sources[master_source.name][0]\n                master_source_features = parseLayoutFeatures(master_source_font).asFea()\n                for instance_ufo in instance_ufos:\n                    instance_ufo.features.text = master_source_features\n                    instance_ufo.save()\n\n        return instance_ufos", "response": "Interpolate master UFOs with MutatorMath and return instance UFOs."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrun toolchain from a DesignSpace document.", "response": "def run_from_designspace(\n        self,\n        designspace_path,\n        output=(),\n        interpolate=False,\n        masters_as_instances=False,\n        interpolate_binary_layout=False,\n        round_instances=False,\n        feature_writers=None,\n        expand_features_to_instances=False,\n        **kwargs\n    ):\n        \"\"\"Run toolchain from a DesignSpace document to produce either static\n        instance fonts (ttf or otf), interpolatable or variable fonts.\n\n        Args:\n            designspace_path: Path to designspace document.\n            interpolate: If True output all instance fonts, otherwise just\n                masters. If the value is a string, only build instance(s) that\n                match given name. The string is compiled into a regular\n                expression and matched against the \"name\" attribute of\n                designspace instances using `re.fullmatch`.\n            masters_as_instances: If True, output master fonts as instances.\n            interpolate_binary_layout: Interpolate layout tables from compiled\n                master binaries.\n            round_instances: apply integer rounding when interpolating with\n                MutatorMath.\n            kwargs: Arguments passed along to run_from_ufos.\n\n        Raises:\n            TypeError: \"variable\" or \"interpolatable\" outputs are incompatible\n                with arguments \"interpolate\", \"masters_as_instances\", and\n                \"interpolate_binary_layout\".\n        \"\"\"\n\n        interp_outputs = INTERPOLATABLE_OUTPUTS.intersection(output)\n        static_outputs = STATIC_OUTPUTS.intersection(output)\n        if interp_outputs:\n            for argname in (\n                \"interpolate\",\n                \"masters_as_instances\",\n                \"interpolate_binary_layout\",\n            ):\n                if locals()[argname]:\n                    raise TypeError(\n                        '\"%s\" argument incompatible with output %r'\n                        % (argname, \", \".join(sorted(interp_outputs)))\n                    )\n\n        designspace = designspaceLib.DesignSpaceDocument.fromfile(designspace_path)\n\n        # if no --feature-writers option was passed, check in the designspace's\n        # <lib> element if user supplied a custom featureWriters configuration;\n        # if so, use that for all the UFOs built from this designspace\n        if feature_writers is None and FEATURE_WRITERS_KEY in designspace.lib:\n            feature_writers = loadFeatureWriters(designspace)\n\n        if static_outputs:\n            self._run_from_designspace_static(\n                designspace,\n                outputs=static_outputs,\n                interpolate=interpolate,\n                masters_as_instances=masters_as_instances,\n                interpolate_binary_layout=interpolate_binary_layout,\n                round_instances=round_instances,\n                feature_writers=feature_writers,\n                expand_features_to_instances=expand_features_to_instances,\n                **kwargs\n            )\n        if interp_outputs:\n            self._run_from_designspace_interpolatable(\n                designspace,\n                outputs=interp_outputs,\n                feature_writers=feature_writers,\n                **kwargs\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrun toolchain from UFOs.", "response": "def run_from_ufos(self, ufos, output=(), **kwargs):\n        \"\"\"Run toolchain from UFO sources.\n\n        Args:\n            ufos: List of UFO sources, as either paths or opened objects.\n            output: List of output formats to generate.\n            kwargs: Arguments passed along to save_otfs.\n        \"\"\"\n\n        if set(output) == {\"ufo\"}:\n            return\n\n        # the `ufos` parameter can be a list of UFO objects\n        # or it can be a path (string) with a glob syntax\n        ufo_paths = []\n        if isinstance(ufos, basestring):\n            ufo_paths = glob.glob(ufos)\n            ufos = [Font(x) for x in ufo_paths]\n        elif isinstance(ufos, list):\n            # ufos can be either paths or open Font objects, so normalize them\n            ufos = [Font(x) if isinstance(x, basestring) else x for x in ufos]\n            ufo_paths = [x.path for x in ufos]\n        else:\n            raise FontmakeError(\n                \"UFOs parameter is neither a defcon.Font object, a path or a glob, \"\n                \"nor a list of any of these.\",\n                ufos,\n            )\n\n        need_reload = False\n        if \"otf\" in output:\n            self.build_otfs(ufos, **kwargs)\n            need_reload = True\n\n        if \"ttf\" in output:\n            if need_reload:\n                ufos = [Font(path) for path in ufo_paths]\n            self.build_ttfs(ufos, **kwargs)\n            need_reload = True"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate a postscript - style font name.", "response": "def _font_name(self, ufo):\n        \"\"\"Generate a postscript-style font name.\"\"\"\n        family_name = (\n            ufo.info.familyName.replace(\" \", \"\")\n            if ufo.info.familyName is not None\n            else \"None\"\n        )\n        style_name = (\n            ufo.info.styleName.replace(\" \", \"\")\n            if ufo.info.styleName is not None\n            else \"None\"\n        )\n        return \"{}-{}\".format(family_name, style_name)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _output_dir(\n        self,\n        ext,\n        is_instance=False,\n        interpolatable=False,\n        autohinted=False,\n        is_variable=False,\n    ):\n        \"\"\"Generate an output directory.\n\n            Args:\n                ext: extension string.\n                is_instance: The output is instance font or not.\n                interpolatable: The output is interpolatable or not.\n                autohinted: The output is autohinted or not.\n                is_variable: The output is variable font or not.\n            Return:\n                output directory string.\n        \"\"\"\n\n        assert not (is_variable and any([is_instance, interpolatable]))\n        # FIXME? Use user configurable destination folders.\n        if is_variable:\n            dir_prefix = \"variable_\"\n        elif is_instance:\n            dir_prefix = \"instance_\"\n        else:\n            dir_prefix = \"master_\"\n        dir_suffix = \"_interpolatable\" if interpolatable else \"\"\n        output_dir = dir_prefix + ext + dir_suffix\n        if autohinted:\n            output_dir = os.path.join(\"autohinted\", output_dir)\n        return output_dir", "response": "Generate an output directory string."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _output_path(\n        self,\n        ufo_or_font_name,\n        ext,\n        is_instance=False,\n        interpolatable=False,\n        autohinted=False,\n        is_variable=False,\n        output_dir=None,\n        suffix=None,\n    ):\n        \"\"\"Generate output path for a font file with given extension.\"\"\"\n\n        if isinstance(ufo_or_font_name, basestring):\n            font_name = ufo_or_font_name\n        elif ufo_or_font_name.path:\n            font_name = os.path.splitext(\n                os.path.basename(os.path.normpath(ufo_or_font_name.path))\n            )[0]\n        else:\n            font_name = self._font_name(ufo_or_font_name)\n\n        if output_dir is None:\n            output_dir = self._output_dir(\n                ext, is_instance, interpolatable, autohinted, is_variable\n            )\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n\n        if suffix:\n            return os.path.join(output_dir, \"{}-{}.{}\".format(font_name, suffix, ext))\n        else:\n            return os.path.join(output_dir, \"{}.{}\".format(font_name, ext))", "response": "Generate output path for a font file with given extension."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmaps font filenames to their locations in a designspace.", "response": "def _designspace_locations(self, designspace):\n        \"\"\"Map font filenames to their locations in a designspace.\"\"\"\n\n        maps = []\n        for elements in (designspace.sources, designspace.instances):\n            location_map = {}\n            for element in elements:\n                path = _normpath(element.path)\n                location_map[path] = element.location\n            maps.append(location_map)\n        return maps"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _closest_location(self, location_map, target):\n\n        def dist(a, b):\n            return math.sqrt(sum((a[k] - b[k]) ** 2 for k in a.keys()))\n\n        paths = iter(location_map.keys())\n        closest = next(paths)\n        closest_dist = dist(target, location_map[closest])\n        for path in paths:\n            cur_dist = dist(target, location_map[path])\n            if cur_dist < closest_dist:\n                closest = path\n                closest_dist = cur_dist\n        return closest", "response": "Return path of font whose location is closest to target."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nthin wrapper around the ttfautohint command line tool. Can take in command line arguments directly as a string, or spelled out as Python keyword arguments.", "response": "def ttfautohint(in_file, out_file, args=None, **kwargs):\n    \"\"\"Thin wrapper around the ttfautohint command line tool.\n\n    Can take in command line arguments directly as a string, or spelled out as\n    Python keyword arguments.\n    \"\"\"\n\n    arg_list = [\"ttfautohint\"]\n    file_args = [in_file, out_file]\n\n    if args is not None:\n        if kwargs:\n            raise TypeError(\"Should not provide both cmd args and kwargs.\")\n        rv = subprocess.call(arg_list + args.split() + file_args)\n        if rv != 0:\n            raise TTFAError(rv)\n        return\n\n    boolean_options = (\n        \"debug\",\n        \"composites\",\n        \"dehint\",\n        \"help\",\n        \"ignore_restrictions\",\n        \"detailed_info\",\n        \"no_info\",\n        \"adjust_subglyphs\",\n        \"symbol\",\n        \"ttfa_table\",\n        \"verbose\",\n        \"version\",\n        \"windows_compatibility\",\n    )\n    other_options = (\n        \"default_script\",\n        \"fallback_script\",\n        \"family_suffix\",\n        \"hinting_limit\",\n        \"fallback_stem_width\",\n        \"hinting_range_min\",\n        \"control_file\",\n        \"hinting_range_max\",\n        \"strong_stem_width\",\n        \"increase_x_height\",\n        \"x_height_snapping_exceptions\",\n    )\n\n    for option in boolean_options:\n        if kwargs.pop(option, False):\n            arg_list.append(\"--\" + option.replace(\"_\", \"-\"))\n\n    for option in other_options:\n        arg = kwargs.pop(option, None)\n        if arg is not None:\n            arg_list.append(\"--{}={}\".format(option.replace(\"_\", \"-\"), arg))\n\n    if kwargs:\n        raise TypeError(\"Unexpected argument(s): \" + \", \".join(kwargs.keys()))\n\n    rv = subprocess.call(arg_list + file_args)\n    if rv != 0:\n        raise TTFAError(rv)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _default_json_default(obj):\n    if isinstance(obj, (datetime.datetime, datetime.date, datetime.time)):\n        return obj.isoformat()\n    else:\n        return str(obj)", "response": "Default json serializer for the base class."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nformat a log record to JSON", "response": "def format(self, record):\n        \"\"\"\n        Format a log record to JSON, if the message is a dict\n        assume an empty message and use the dict as additional\n        fields.\n        \"\"\"\n\n        fields = record.__dict__.copy()\n\n        if isinstance(record.msg, dict):\n            fields.update(record.msg)\n            fields.pop('msg')\n            msg = \"\"\n        else:\n            msg = record.getMessage()\n\n        try:\n            msg = msg.format(**fields)\n        except (KeyError, IndexError, ValueError):\n            pass\n        except:\n            # in case we can not format the msg properly we log it as is instead of crashing\n            msg = msg\n\n        if 'msg' in fields:\n            fields.pop('msg')\n\n        if 'exc_info' in fields:\n            if fields['exc_info']:\n                formatted = tb.format_exception(*fields['exc_info'])\n                fields['exception'] = formatted\n            fields.pop('exc_info')\n\n        if 'exc_text' in fields and not fields['exc_text']:\n            fields.pop('exc_text')\n\n        logr = self.defaults.copy()\n\n        logr.update({'@message': msg,\n                     '@timestamp': datetime.datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%S.%fZ'),\n                     '@source_host': self.source_host,\n                     '@fields': self._build_fields(logr, fields)})\n\n        return json.dumps(logr, default=self.json_default, cls=self.json_cls)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _build_fields(self, defaults, fields):\n        return dict(list(defaults.get('@fields', {}).items()) + list(fields.items()))", "response": "Build the fields from the defaults and fields."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _authenticate(self):\n        self.cleanup_headers()\n        url = LOGIN_ENDPOINT\n        data = self.query(\n            url,\n            method='POST',\n            extra_params={\n                'email': self.__username,\n                'password': self.__password\n            })\n\n        if isinstance(data, dict) and data.get('success'):\n            data = data.get('data')\n            self.authenticated = data.get('authenticated')\n            self.country_code = data.get('countryCode')\n            self.date_created = data.get('dateCreated')\n            self.__token = data.get('token')\n            self.userid = data.get('userId')\n\n            # update header with the generated token\n            self.__headers['Authorization'] = self.__token", "response": "Authenticate user and generate token."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nresetting the headers and params.", "response": "def cleanup_headers(self):\n        \"\"\"Reset the headers and params.\"\"\"\n        headers = {'Content-Type': 'application/json'}\n        headers['Authorization'] = self.__token\n        self.__headers = headers\n        self.__params = {}"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a JSON object or raw session. :param url: Arlo API URL :param method: Specify the method GET, POST or PUT. Default is GET. :param extra_params: Dictionary to be appended on request.body :param extra_headers: Dictionary to be apppended on request.headers :param retry: Attempts to retry a query. Default is 3. :param raw: Boolean if query() will return request object instead JSON. :param stream: Boolean if query() will return a stream object.", "response": "def query(self,\n              url,\n              method='GET',\n              extra_params=None,\n              extra_headers=None,\n              retry=3,\n              raw=False,\n              stream=False):\n        \"\"\"\n        Return a JSON object or raw session.\n\n        :param url:  Arlo API URL\n        :param method: Specify the method GET, POST or PUT. Default is GET.\n        :param extra_params: Dictionary to be appended on request.body\n        :param extra_headers: Dictionary to be apppended on request.headers\n        :param retry: Attempts to retry a query. Default is 3.\n        :param raw: Boolean if query() will return request object instead JSON.\n        :param stream: Boolean if query() will return a stream object.\n        \"\"\"\n        response = None\n        loop = 0\n\n        # always make sure the headers and params are clean\n        self.cleanup_headers()\n\n        while loop <= retry:\n\n            # override request.body or request.headers dictionary\n            if extra_params:\n                params = self.__params\n                params.update(extra_params)\n            else:\n                params = self.__params\n            _LOGGER.debug(\"Params: %s\", params)\n\n            if extra_headers:\n                headers = self.__headers\n                headers.update(extra_headers)\n            else:\n                headers = self.__headers\n            _LOGGER.debug(\"Headers: %s\", headers)\n\n            _LOGGER.debug(\"Querying %s on attempt: %s/%s\", url, loop, retry)\n            loop += 1\n\n            # define connection method\n            req = None\n\n            if method == 'GET':\n                req = self.session.get(url, headers=headers, stream=stream)\n            elif method == 'PUT':\n                req = self.session.put(url, json=params, headers=headers)\n            elif method == 'POST':\n                req = self.session.post(url, json=params, headers=headers)\n\n            if req and (req.status_code == 200):\n                if raw:\n                    _LOGGER.debug(\"Required raw object.\")\n                    response = req\n                else:\n                    response = req.json()\n\n                # leave if everything worked fine\n                break\n\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef devices(self):\n        if self._all_devices:\n            return self._all_devices\n\n        self._all_devices = {}\n        self._all_devices['cameras'] = []\n        self._all_devices['base_station'] = []\n\n        url = DEVICES_ENDPOINT\n        data = self.query(url)\n\n        for device in data.get('data'):\n            name = device.get('deviceName')\n            if ((device.get('deviceType') == 'camera' or\n                 device.get('deviceType') == 'arloq' or\n                 device.get('deviceType') == 'arloqs') and\n                    device.get('state') == 'provisioned'):\n                camera = ArloCamera(name, device, self)\n                self._all_devices['cameras'].append(camera)\n\n            if (device.get('state') == 'provisioned' and\n                    (device.get('deviceType') == 'basestation' or\n                     device.get('modelId') == 'ABC1000')):\n                base = ArloBaseStation(name, device, self.__token, self)\n                self._all_devices['base_station'].append(base)\n\n        return self._all_devices", "response": "Return all devices on Arlo account."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn camera object by device_id.", "response": "def lookup_camera_by_id(self, device_id):\n        \"\"\"Return camera object by device_id.\"\"\"\n        camera = list(filter(\n            lambda cam: cam.device_id == device_id, self.cameras))[0]\n        if camera:\n            return camera\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrefreshing attributes from a given Arlo object.", "response": "def refresh_attributes(self, name):\n        \"\"\"Refresh attributes from a given Arlo object.\"\"\"\n        url = DEVICES_ENDPOINT\n        response = self.query(url)\n\n        if not response or not isinstance(response, dict):\n            return None\n\n        for device in response.get('data'):\n            if device.get('deviceName') == name:\n                return device\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nloading Arlo videos from the given criteria.", "response": "def load(self, days=PRELOAD_DAYS, only_cameras=None,\n             date_from=None, date_to=None, limit=None):\n        \"\"\"Load  Arlo videos from the given criteria\n\n        :param days: number of days to retrieve\n        :param only_cameras: retrieve only <ArloCamera> on that list\n        :param date_from: refine from initial date\n        :param date_to: refine final date\n        :param limit: define number of objects to return\n        \"\"\"\n        videos = []\n        url = LIBRARY_ENDPOINT\n        if not (date_from and date_to):\n            now = datetime.today()\n            date_from = (now - timedelta(days=days)).strftime('%Y%m%d')\n            date_to = now.strftime('%Y%m%d')\n\n        params = {'dateFrom': date_from, 'dateTo': date_to}\n        data = self._session.query(url,\n                                   method='POST',\n                                   extra_params=params).get('data')\n\n        # get all cameras to append to create ArloVideo object\n        all_cameras = self._session.cameras\n\n        for video in data:\n            # pylint: disable=cell-var-from-loop\n            srccam = \\\n                list(filter(\n                    lambda cam: cam.device_id == video.get('deviceId'),\n                    all_cameras)\n                    )[0]\n\n            # make sure only_cameras is a list\n            if only_cameras and \\\n               not isinstance(only_cameras, list):\n                only_cameras = [(only_cameras)]\n\n            # filter by camera only\n            if only_cameras:\n                if list(filter(lambda cam: cam.device_id == srccam.device_id,\n                               list(only_cameras))):\n                    videos.append(ArloVideo(video, srccam, self._session))\n            else:\n                videos.append(ArloVideo(video, srccam, self._session))\n\n        if limit:\n            return videos[:limit]\n\n        return videos"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef created_today(self):\n        if self.datetime.date() == datetime.today().date():\n            return True\n        return False", "response": "Return True if created today."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns datetime object from timestamp.", "response": "def to_datetime(timestamp):\n    \"\"\"Return datetime object from timestamp.\"\"\"\n    return dt.fromtimestamp(time.mktime(\n        time.localtime(int(str(timestamp)[:10]))))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating stream for a given record video.", "response": "def http_stream(url, chunk=4096):\n    \"\"\"Generate stream for a given record video.\n\n    :param chunk: chunk bytes to read per time\n    :returns generator object\n    \"\"\"\n    ret = requests.get(url, stream=True)\n    ret.raise_for_status()\n    for data in ret.iter_content(chunk):\n        yield data"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef unseen_videos_reset(self):\n        url = RESET_CAM_ENDPOINT.format(self.unique_id)\n        ret = self._session.query(url).get('success')\n        return ret", "response": "Reset the unseen videos counter."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef last_video(self):\n        if self._cached_videos is None:\n            self.make_video_cache()\n\n        if self._cached_videos:\n            return self._cached_videos[0]\n        return None", "response": "Return the last <ArloVideo object from camera."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef make_video_cache(self, days=None):\n        if days is None:\n            days = self._min_days_vdo_cache\n        self._cached_videos = self.videos(days)", "response": "Save videos on _cache_videos."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef videos(self, days=None):\n        if days is None:\n            days = self._min_days_vdo_cache\n        library = ArloMediaLibrary(self._session, preload=False)\n        try:\n            return library.load(only_cameras=[self], days=days)\n        except (AttributeError, IndexError):\n            # make sure we are returning an empty list istead of None\n            # returning an empty list, cache will be forced only when calling\n            # the update method. Changing this can impact badly\n            # in the Home Assistant performance\n            return []", "response": "Retrieve all videos from the camera given days range\n           "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning list of <ArloVideo object captured today.", "response": "def captured_today(self):\n        \"\"\"Return list of <ArloVideo> object captured today.\"\"\"\n        if self._cached_videos is None:\n            self.make_video_cache()\n\n        return [vdo for vdo in self._cached_videos if vdo.created_today]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the base station assigned for the given camera.", "response": "def base_station(self):\n        \"\"\"Return the base_station assigned for the given camera.\"\"\"\n        try:\n            return list(filter(lambda x: x.device_id == self.parent_id,\n                               self._session.base_stations))[0]\n        except (IndexError, AttributeError):\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets a camera s triggers.", "response": "def triggers(self):\n        \"\"\"Get a camera's triggers.\"\"\"\n        capabilities = self.capabilities\n        if not capabilities:\n            return None\n\n        for capability in capabilities:\n            if not isinstance(capability, dict):\n                continue\n\n            triggers = capability.get(\"Triggers\")\n            if triggers:\n                return triggers\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef motion_detection_sensitivity(self):\n        if not self.triggers:\n            return None\n\n        for trigger in self.triggers:\n            if trigger.get(\"type\") != \"pirMotionActive\":\n                continue\n\n            sensitivity = trigger.get(\"sensitivity\")\n            if sensitivity:\n                return sensitivity.get(\"default\")\n\n        return None", "response": "Sensitivity level of Camera motion detection."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef audio_detection_sensitivity(self):\n        if not self.triggers:\n            return None\n\n        for trigger in self.triggers:\n            if trigger.get(\"type\") != \"audioAmplitude\":\n                continue\n\n            sensitivity = trigger.get(\"sensitivity\")\n            if sensitivity:\n                return sensitivity.get(\"default\")\n\n        return None", "response": "Sensitivity level of Camera audio detection."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef live_streaming(self):\n        url = STREAM_ENDPOINT\n\n        # override params\n        params = STREAMING_BODY\n        params['from'] = \"{0}_web\".format(self.user_id)\n        params['to'] = self.device_id\n        params['resource'] = \"cameras/{0}\".format(self.device_id)\n        params['transId'] = \"web!{0}\".format(self.xcloud_id)\n\n        # override headers\n        headers = {'xCloudId': self.xcloud_id}\n\n        _LOGGER.debug(\"Streaming device %s\", self.name)\n        _LOGGER.debug(\"Device params %s\", params)\n        _LOGGER.debug(\"Device headers %s\", headers)\n\n        ret = self._session.query(url,\n                                  method='POST',\n                                  extra_params=params,\n                                  extra_headers=headers)\n\n        _LOGGER.debug(\"Streaming results %s\", ret)\n        if ret.get('success'):\n            return ret.get('data').get('url')\n        return ret.get('data')", "response": "Return live streaming generator."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef schedule_snapshot(self):\n        # Notes:\n        #  - Snapshots are not immediate.\n        #  - Snapshots will be cached for predefined amount\n        #    of time.\n        #  - Snapshots are not balanced. To get a better\n        #    image, it must be taken from the stream, a few\n        #    seconds after stream start.\n        url = SNAPSHOTS_ENDPOINT\n        params = SNAPSHOTS_BODY\n        params['from'] = \"{0}_web\".format(self.user_id)\n        params['to'] = self.device_id\n        params['resource'] = \"cameras/{0}\".format(self.device_id)\n        params['transId'] = \"web!{0}\".format(self.xcloud_id)\n\n        # override headers\n        headers = {'xCloudId': self.xcloud_id}\n\n        _LOGGER.debug(\"Snapshot device %s\", self.name)\n        _LOGGER.debug(\"Device params %s\", params)\n        _LOGGER.debug(\"Device headers %s\", headers)\n\n        ret = self._session.query(url,\n                                  method='POST',\n                                  extra_params=params,\n                                  extra_headers=headers)\n\n        _LOGGER.debug(\"Snapshot results %s\", ret)\n\n        return ret is not None and ret.get('success')", "response": "Trigger snapshot to be uploaded to AWS. Return success state."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_event_stream(self):\n        self.__event_handle = threading.Event()\n        event_thread = threading.Thread(target=self.thread_function)\n        event_thread.start()", "response": "Spawn a thread and monitor the Arlo Event Stream."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _close_event_stream(self):\n        self.__subscribed = False\n        del self.__events[:]\n        self.__event_handle.clear()", "response": "Stop the Event stream thread."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef publish_and_get_event(self, resource):\n        l_subscribed = False\n        this_event = None\n\n        if not self.__subscribed:\n            self._get_event_stream()\n            self._subscribe_myself()\n            l_subscribed = True\n\n        status = self.publish(\n            action='get',\n            resource=resource,\n            mode=None,\n            publish_response=False)\n\n        if status == 'success':\n            i = 0\n            while not this_event and i < 2:\n                self.__event_handle.wait(5.0)\n                self.__event_handle.clear()\n                _LOGGER.debug(\"Instance %s resource: %s\", str(i), resource)\n                for event in self.__events:\n                    if event['resource'] == resource:\n                        this_event = event\n                        self.__events.remove(event)\n                        break\n                i = i + 1\n\n        if l_subscribed:\n            self._unsubscribe_myself()\n            self._close_event_stream()\n            l_subscribed = False\n\n        return this_event", "response": "Publish and get the event from base station."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\npublish a new object to the arlo.", "response": "def publish(\n            self,\n            action='get',\n            resource=None,\n            camera_id=None,\n            mode=None,\n            publish_response=None,\n            properties=None):\n        \"\"\"Run action.\n\n        :param method: Specify the method GET, POST or PUT. Default is GET.\n        :param resource: Specify one of the resources to fetch from arlo.\n        :param camera_id: Specify the camera ID involved with this action\n        :param mode: Specify the mode to set, else None for GET operations\n        :param publish_response: Set to True for SETs. Default False\n        \"\"\"\n        url = NOTIFY_ENDPOINT.format(self.device_id)\n\n        body = ACTION_BODY.copy()\n\n        if properties is None:\n            properties = {}\n\n        if resource:\n            body['resource'] = resource\n\n        if action == 'get':\n            body['properties'] = None\n        else:\n            # consider moving this logic up a layer\n            if resource == 'schedule':\n                properties.update({'active': True})\n            elif resource == 'subscribe':\n                body['resource'] = \"subscriptions/\" + \\\n                        \"{0}_web\".format(self.user_id)\n                dev = []\n                dev.append(self.device_id)\n                properties.update({'devices': dev})\n            elif resource == 'modes':\n                available_modes = self.available_modes_with_ids\n                properties.update({'active': available_modes.get(mode)})\n            elif resource == 'privacy':\n                properties.update({'privacyActive': not mode})\n                body['resource'] = \"cameras/{0}\".format(camera_id)\n\n        body['action'] = action\n        body['properties'] = properties\n        body['publishResponse'] = publish_response\n\n        body['from'] = \"{0}_web\".format(self.user_id)\n        body['to'] = self.device_id\n        body['transId'] = \"web!{0}\".format(self.xcloud_id)\n\n        _LOGGER.debug(\"Action body: %s\", body)\n\n        ret = \\\n            self._session.query(url, method='POST', extra_params=body,\n                                extra_headers={\"xCloudId\": self.xcloud_id})\n\n        if ret and ret.get('success'):\n            return 'success'\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\noverrides the refresh_rate attribute.", "response": "def refresh_rate(self, value):\n        \"\"\"Override the refresh_rate attribute.\"\"\"\n        if isinstance(value, (int, float)):\n            self._refresh_rate = value"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns list of available mode names.", "response": "def available_modes(self):\n        \"\"\"Return list of available mode names.\"\"\"\n        if not self._available_modes:\n            modes = self.available_modes_with_ids\n            if not modes:\n                return None\n            self._available_modes = list(modes.keys())\n        return self._available_modes"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef available_modes_with_ids(self):\n        if not self._available_mode_ids:\n            all_modes = FIXED_MODES.copy()\n            self._available_mode_ids = all_modes\n            modes = self.get_available_modes()\n            try:\n                if modes:\n                    # pylint: disable=consider-using-dict-comprehension\n                    simple_modes = dict(\n                        [(m.get(\"type\", m.get(\"name\")), m.get(\"id\"))\n                         for m in modes]\n                    )\n                    all_modes.update(simple_modes)\n                    self._available_mode_ids = all_modes\n            except TypeError:\n                _LOGGER.debug(\"Did not receive a valid response. Passing..\")\n        return self._available_mode_ids", "response": "Return list of objects containing available mode name and id."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn current mode key.", "response": "def mode(self):\n        \"\"\"Return current mode key.\"\"\"\n\n        if self.is_in_schedule_mode:\n            return \"schedule\"\n\n        resource = \"modes\"\n        mode_event = self.publish_and_get_event(resource)\n        if mode_event:\n            properties = mode_event.get('properties')\n            active_mode = properties.get('active')\n            modes = properties.get('modes')\n            if not modes:\n                return None\n\n            for mode in modes:\n                if mode.get('id') == active_mode:\n                    return mode.get('type') \\\n                        if mode.get('type') is not None else mode.get('name')\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_in_schedule_mode(self):\n        resource = \"schedule\"\n        mode_event = self.publish_and_get_event(resource)\n        if mode_event and mode_event.get(\"resource\", None) == \"schedule\":\n            properties = mode_event.get('properties')\n            return properties.get(\"active\", False)\n        return False", "response": "Returns True if base station is currently on a scheduled mode."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a list of available mode objects for an Arlo user.", "response": "def get_available_modes(self):\n        \"\"\"Return a list of available mode objects for an Arlo user.\"\"\"\n        resource = \"modes\"\n        resource_event = self.publish_and_get_event(resource)\n        if resource_event:\n            properties = resource_event.get(\"properties\")\n            return properties.get(\"modes\")\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of battery levels of all cameras.", "response": "def get_cameras_battery_level(self):\n        \"\"\"Return a list of battery levels of all cameras.\"\"\"\n        battery_levels = {}\n        if not self.camera_properties:\n            return None\n\n        for camera in self.camera_properties:\n            serialnum = camera.get('serialNumber')\n            cam_battery = camera.get('batteryLevel')\n            battery_levels[serialnum] = cam_battery\n        return battery_levels"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of signal strength of all cameras.", "response": "def get_cameras_signal_strength(self):\n        \"\"\"Return a list of signal strength of all cameras.\"\"\"\n        signal_strength = {}\n        if not self.camera_properties:\n            return None\n\n        for camera in self.camera_properties:\n            serialnum = camera.get('serialNumber')\n            cam_strength = camera.get('signalStrength')\n            signal_strength[serialnum] = cam_strength\n        return signal_strength"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_camera_extended_properties(self):\n        resource = 'cameras/{}'.format(self.device_id)\n        resource_event = self.publish_and_get_event(resource)\n\n        if resource_event is None:\n            return None\n\n        self._camera_extended_properties = resource_event.get('properties')\n        return self._camera_extended_properties", "response": "Return camera extended properties."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_speaker_muted(self):\n        if not self.camera_extended_properties:\n            return None\n\n        speaker = self.camera_extended_properties.get('speaker')\n        if not speaker:\n            return None\n\n        return speaker.get('mute')", "response": "Return whether or not the speaker is muted."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the volume setting of the speaker.", "response": "def get_speaker_volume(self):\n        \"\"\"Return the volume setting of the speaker.\"\"\"\n        if not self.camera_extended_properties:\n            return None\n\n        speaker = self.camera_extended_properties.get('speaker')\n        if not speaker:\n            return None\n\n        return speaker.get('volume')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_night_light_state(self):\n        if not self.camera_extended_properties:\n            return None\n\n        night_light = self.camera_extended_properties.get('nightLight')\n        if not night_light:\n            return None\n\n        if night_light.get('enabled'):\n            return 'on'\n\n        return 'off'", "response": "Return the state of the night light."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_night_light_brightness(self):\n        if not self.camera_extended_properties:\n            return None\n\n        night_light = self.camera_extended_properties.get('nightLight')\n        if not night_light:\n            return None\n\n        return night_light.get('brightness')", "response": "Return the brightness of the night light."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the base station info.", "response": "def properties(self):\n        \"\"\"Return the base station info.\"\"\"\n        resource = \"basestation\"\n        basestn_event = self.publish_and_get_event(resource)\n        if basestn_event:\n            return basestn_event.get('properties')\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the camera rules.", "response": "def get_cameras_rules(self):\n        \"\"\"Return the camera rules.\"\"\"\n        resource = \"rules\"\n        rules_event = self.publish_and_get_event(resource)\n        if rules_event:\n            return rules_event.get('properties')\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the schedule set for cameras.", "response": "def get_cameras_schedule(self):\n        \"\"\"Return the schedule set for cameras.\"\"\"\n        resource = \"schedule\"\n        schedule_event = self.publish_and_get_event(resource)\n        if schedule_event:\n            return schedule_event.get('properties')\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_ambient_sensor_data(self):\n        resource = 'cameras/{}/ambientSensors/history'.format(self.device_id)\n        history_event = self.publish_and_get_event(resource)\n\n        if history_event is None:\n            return None\n\n        properties = history_event.get('properties')\n\n        self._ambient_sensor_data = \\\n            ArloBaseStation._decode_sensor_data(properties)\n\n        return self._ambient_sensor_data", "response": "Refresh the ambient sensor history"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndecoding decompress and parse the data from the history API", "response": "def _decode_sensor_data(properties):\n        \"\"\"Decode, decompress, and parse the data from the history API\"\"\"\n        b64_input = \"\"\n        for s in properties.get('payload'):\n            # pylint: disable=consider-using-join\n            b64_input += s\n\n        decoded = base64.b64decode(b64_input)\n        data = zlib.decompress(decoded)\n        points = []\n        i = 0\n\n        while i < len(data):\n            points.append({\n                'timestamp': int(1e3 * ArloBaseStation._parse_statistic(\n                    data[i:(i + 4)], 0)),\n                'temperature': ArloBaseStation._parse_statistic(\n                    data[(i + 8):(i + 10)], 1),\n                'humidity': ArloBaseStation._parse_statistic(\n                    data[(i + 14):(i + 16)], 1),\n                'airQuality': ArloBaseStation._parse_statistic(\n                    data[(i + 20):(i + 22)], 1)\n            })\n            i += 22\n\n        return points"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _parse_statistic(data, scale):\n        i = 0\n        for byte in bytearray(data):\n            i = (i << 8) + byte\n\n        if i == 32768:\n            return None\n\n        if scale == 0:\n            return i\n\n        return float(i) / (scale * 10)", "response": "Parse binary statistics returned from the history API."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef play_track(self, track_id=DEFAULT_TRACK_ID, position=0):\n        self.publish(\n            action='playTrack',\n            resource='audioPlayback/player',\n            publish_response=False,\n            properties={'trackId': track_id, 'position': position}\n        )", "response": "Plays a track at the given position."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset playback to shuffle.", "response": "def set_shuffle_on(self):\n        \"\"\"Sets playback to shuffle.\"\"\"\n        self.publish(\n            action='set',\n            resource='audioPlayback/config',\n            publish_response=False,\n            properties={'config': {'shuffleActive': True}}\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_shuffle_off(self):\n        self.publish(\n            action='set',\n            resource='audioPlayback/config',\n            publish_response=False,\n            properties={'config': {'shuffleActive': False}}\n        )", "response": "Sets playback to sequential."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the music volume.", "response": "def set_volume(self, mute=False, volume=50):\n        \"\"\"Sets the music volume (0-100)\"\"\"\n        self.publish(\n            action='set',\n            resource='cameras/{}'.format(self.device_id),\n            publish_response=False,\n            properties={'speaker': {'mute': mute, 'volume': volume}}\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_night_light_on(self):\n        self.publish(\n            action='set',\n            resource='cameras/{}'.format(self.device_id),\n            publish_response=False,\n            properties={'nightLight': {'enabled': True}}\n        )", "response": "Turns on the night light."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_night_light_off(self):\n        self.publish(\n            action='set',\n            resource='cameras/{}'.format(self.device_id),\n            publish_response=False,\n            properties={'nightLight': {'enabled': False}}\n        )", "response": "Turns off the night light."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the brightness of the night light.", "response": "def set_night_light_brightness(self, brightness=200):\n        \"\"\"Sets the brightness of the night light (0-255).\"\"\"\n        self.publish(\n            action='set',\n            resource='cameras/{}'.format(self.device_id),\n            publish_response=False,\n            properties={'nightLight': {'brightness': brightness}}\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets Arlo camera mode.", "response": "def mode(self, mode):\n        \"\"\"Set Arlo camera mode.\n\n        :param mode: arm, disarm\n        \"\"\"\n        modes = self.available_modes\n        if (not modes) or (mode not in modes):\n            return\n        self.publish(\n            action='set',\n            resource='modes' if mode != 'schedule' else 'schedule',\n            mode=mode,\n            publish_response=True)\n        self.update()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nturns Arlo camera On or Off.", "response": "def set_camera_enabled(self, camera_id, is_enabled):\n        \"\"\"Turn Arlo camera On/Off.\n\n        :param mode: True, False\n        \"\"\"\n        self.publish(\n            action='set',\n            resource='privacy',\n            camera_id=camera_id,\n            mode=is_enabled,\n            publish_response=True)\n        self.update()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nremoves any indentation that is common to all of the given lines.", "response": "def unindent(self, lines):\n        \"\"\"Removes any indentation that is common to all of the given lines.\"\"\"\n        indent = min(\n            len(self.re.match(r'^ *', line).group()) for line in lines)\n        return [line[indent:].rstrip() for line in lines]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the argument expressions from the source of a function call.", "response": "def get_call_exprs(self, line):\n        \"\"\"Gets the argument expressions from the source of a function call.\"\"\"\n        line = line.lstrip()\n        try:\n            tree = self.ast.parse(line)\n        except SyntaxError:\n            return None\n        for node in self.ast.walk(tree):\n            if isinstance(node, self.ast.Call):\n                offsets = []\n                for arg in node.args:\n                    # In Python 3.4 the col_offset is calculated wrong. See\n                    # https://bugs.python.org/issue21295\n                    if isinstance(arg, self.ast.Attribute) and (\n                            (3, 4, 0) <= self.sys.version_info <= (3, 4, 3)):\n                        offsets.append(arg.col_offset - len(arg.value.id) - 1)\n                    else:\n                        offsets.append(arg.col_offset)\n                if node.keywords:\n                    line = line[:node.keywords[0].value.col_offset]\n                    line = self.re.sub(r'\\w+\\s*=\\s*$', '', line)\n                else:\n                    line = self.re.sub(r'\\s*\\)\\s*$', '', line)\n                offsets.append(len(line))\n                args = []\n                for i in range(len(node.args)):\n                    args.append(line[offsets[i]:offsets[i + 1]].rstrip(', '))\n                return args"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprints out nice representations of the given values.", "response": "def show(self, func_name, values, labels=None):\n        \"\"\"Prints out nice representations of the given values.\"\"\"\n        s = self.Stanza(self.indent)\n        if func_name == '<module>' and self.in_console:\n            func_name = '<console>'\n        s.add([func_name + ': '])\n        reprs = map(self.safe_repr, values)\n        if labels:\n            sep = ''\n            for label, repr in zip(labels, reprs):\n                s.add([label + '=', self.CYAN, repr, self.NORMAL], sep)\n                sep = ', '\n        else:\n            sep = ''\n            for repr in reprs:\n                s.add([self.CYAN, repr, self.NORMAL], sep)\n                sep = ', '\n        self.writer.write(s.chunks)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef trace(self, func):\n\n        def wrapper(*args, **kwargs):\n            # Print out the call to the function with its arguments.\n            s = self.Stanza(self.indent)\n            s.add([self.GREEN, func.__name__, self.NORMAL, '('])\n            s.indent += 4\n            sep = ''\n            for arg in args:\n                s.add([self.CYAN, self.safe_repr(arg), self.NORMAL], sep)\n                sep = ', '\n            for name, value in sorted(kwargs.items()):\n                s.add([name + '=', self.CYAN, self.safe_repr(value),\n                       self.NORMAL], sep)\n                sep = ', '\n            s.add(')', wrap=False)\n            self.writer.write(s.chunks)\n\n            # Call the function.\n            self.indent += 2\n            try:\n                result = func(*args, **kwargs)\n            except:\n                # Display an exception.\n                self.indent -= 2\n                etype, evalue, etb = self.sys.exc_info()\n                info = self.inspect.getframeinfo(etb.tb_next, context=3)\n                s = self.Stanza(self.indent)\n                s.add([self.RED, '!> ', self.safe_repr(evalue), self.NORMAL])\n                s.add(['at ', info.filename, ':', info.lineno], ' ')\n                lines = self.unindent(info.code_context)\n                firstlineno = info.lineno - info.index\n                fmt = '%' + str(len(str(firstlineno + len(lines)))) + 'd'\n                for i, line in enumerate(lines):\n                    s.newline()\n                    s.add([\n                        i == info.index and self.MAGENTA or '',\n                        fmt % (i + firstlineno),\n                        i == info.index and '> ' or ': ', line, self.NORMAL])\n                self.writer.write(s.chunks)\n                raise\n\n            # Display the return value.\n            self.indent -= 2\n            s = self.Stanza(self.indent)\n            s.add([self.GREEN, '-> ', self.CYAN, self.safe_repr(result),\n                   self.NORMAL])\n            self.writer.write(s.chunks)\n            return result\n        return self.functools.update_wrapper(wrapper, func)", "response": "Decorator to print out a function s arguments and return value."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef d(self, depth=1):\n        info = self.inspect.getframeinfo(self.sys._getframe(1))\n        s = self.Stanza(self.indent)\n        s.add([info.function + ': '])\n        s.add([self.MAGENTA, 'Interactive console opened', self.NORMAL])\n        self.writer.write(s.chunks)\n\n        frame = self.sys._getframe(depth)\n        env = frame.f_globals.copy()\n        env.update(frame.f_locals)\n        self.indent += 2\n        self.in_console = True\n        self.code.interact(\n            'Python console opened by q.d() in ' + info.function, local=env)\n        self.in_console = False\n        self.indent -= 2\n\n        s = self.Stanza(self.indent)\n        s.add([info.function + ': '])\n        s.add([self.MAGENTA, 'Interactive console closed', self.NORMAL])\n        self.writer.write(s.chunks)", "response": "Launches an interactive console at the point where it s called."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef swagger(self):\n        # prepare Swagger.host & Swagger.basePath\n        if not self.__swagger:\n            return None\n\n        common_path = os.path.commonprefix(list(self.__swagger.paths))\n        # remove tailing slash,\n        # because all paths in Paths Object would prefixed with slah.\n        common_path = common_path[:-1] if common_path[-1] == '/' else common_path\n\n        if len(common_path) > 0:\n            p = six.moves.urllib.parse.urlparse(common_path)\n            self.__swagger.update_field('host', p.netloc)\n\n            new_common_path = six.moves.urllib.parse.urlunparse((\n                p.scheme, p.netloc, '', '', '', ''))\n            new_path = {}\n            for k in self.__swagger.paths.keys():\n                new_path[k[len(new_common_path):]] = self.__swagger.paths[k]\n            self.__swagger.update_field('paths', new_path)\n\n        return self.__swagger", "response": "some preparation before returning Swagger object\n           "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef scope_compose(scope, name, sep=private.SCOPE_SEPARATOR):\n\n    if name == None:\n        new_scope = scope\n    else:\n        new_scope = scope if scope else name\n\n    if scope and name:\n        new_scope = scope + sep + name\n\n    return new_scope", "response": "compose a new scope"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert ISO - 8601 string to datetime object.", "response": "def from_iso8601(s):\n    \"\"\" convert iso8601 string to datetime object.\n    refer to http://xml2rfc.ietf.org/public/rfc/html/rfc3339.html#anchor14\n    for details.\n\n    :param str s: time in ISO-8601\n    :rtype: datetime.datetime\n    \"\"\"\n    m = _iso8601_fmt.match(s)\n    if not m:\n        m = _iso8601_fmt_date.match(s)\n\n    if not m:\n        raise ValueError('not a valid iso 8601 format string:[{0}]'.format(s))\n\n    g = m.groupdict()\n\n    def _default_zero(key):\n        v = g.get(key, None)\n        return int(v) if v else 0\n\n    def _default_none(key):\n        v = g.get(key, None)\n        return int(v) if v else None\n\n    year = _default_zero('year')\n    month = _default_zero('month')\n    day = _default_zero('day')\n    hour = _default_none('hour')\n    minute = _default_none('minute')\n    second = _default_none('second')\n    microsecond = g.get('microsecond', None)\n    if microsecond is not None:\n        # append zero when not matching 6 digits\n        microsecond = int(microsecond + '0' * (6 - len(microsecond)))\n    tz_s = g.get('tz')\n\n    if not (year and month and day):\n        raise ValueError('missing y-m-d: [{0}]'.format(s))\n\n    # only date part, time part is none\n    if hour == None and minute == None and second == None:\n        return datetime.datetime(year, month, day)\n\n    # prepare tz info\n    tz = None\n    if tz_s:\n        if hour is None and minute is None:\n            raise ValueError('missing h:m when tzinfo is provided: [{0}]'.format(s))\n\n        negtive = hh = mm = 0\n\n        if tz_s != 'Z':\n            negtive = -1 if tz_s[0] == '-' else 1\n            hh = int(tz_s[1:3])\n            mm = int(tz_s[4:6]) if len(tz_s) > 5 else 0\n\n        tz = FixedTZ(h=hh*negtive, m=mm*negtive)\n\n    return datetime.datetime(\n        year=year,\n        month=month,\n        day=day,\n        hour=hour or 0,\n        minute=minute or 0,\n        second=second or 0,\n        microsecond=microsecond or 0,\n        tzinfo=tz\n    )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef import_string(name):\n    mod = fp = None\n\n    # code below, please refer to\n    #   https://docs.python.org/2/library/imp.html\n    # for details\n    try:\n        return sys.modules[name]\n    except KeyError:\n        pass\n\n    try:\n        fp, pathname, desc = imp.find_module(name)\n        mod = imp.load_module(name, fp, pathname, desc)\n    except ImportError:\n        mod = None\n    finally:\n        # Since we may exit via an exception, close fp explicitly.\n        if fp:\n            fp.close()\n\n    return mod", "response": "Import a single object from a string."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef jp_compose(s, base=None):\n    if s == None:\n        return base\n\n    ss = [s] if isinstance(s, six.string_types) else s\n    ss = [s.replace('~', '~0').replace('/', '~1') for s in ss]\n    if base:\n        ss.insert(0, base)\n    return '/'.join(ss)", "response": "append a string to json - pointer\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef jp_split(s):\n    if s == '' or s == None:\n        return []\n\n    def _decode(s):\n        s = s.replace('~1', '/')\n        return s.replace('~0', '~')\n\n    return [_decode(ss) for ss in s.split('/')]", "response": "split a string from json - pointer"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef jr_split(s):\n    p = six.moves.urllib.parse.urlparse(s)\n    return (\n        normalize_url(six.moves.urllib.parse.urlunparse(p[:5]+('',))),\n        '#'+p.fragment if p.fragment else '#'\n    )", "response": "split a json - reference into url json - pointer"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef deref(obj, guard=None):\n    cur, guard = obj, guard or CycleGuard()\n    guard.update(cur)\n    while cur and getattr(cur, 'ref_obj', None) != None:\n        cur = cur.ref_obj\n        guard.update(cur)\n    return cur", "response": "dereference a node from the tree."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreplace a tuple in a tuple list l with v", "response": "def nv_tuple_list_replace(l, v):\n    \"\"\" replace a tuple in a tuple list\n    \"\"\"\n    _found = False\n    for i, x in enumerate(l):\n        if x[0] == v[0]:\n            l[i] = v\n            _found = True\n\n    if not _found:\n        l.append(v)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns file:// URL from a filename.", "response": "def path2url(p):\n    \"\"\" Return file:// URL from a filename.\n    \"\"\"\n    # Python 3 is a bit different and does a better job.\n    if sys.version_info.major >= 3 and sys.version_info.minor >= 4:\n        import pathlib\n        return pathlib.Path(p).as_uri()\n    else:\n        return six.moves.urllib.parse.urljoin(\n            'file:', six.moves.urllib.request.pathname2url(p)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef normalize_url(url):\n    if not url:\n        return url\n\n    matched = _windows_path_prefix.match(url)\n    if matched:\n        return path2url(url)\n\n    p = six.moves.urllib.parse.urlparse(url)\n    if p.scheme == '':\n        if p.netloc == '' and p.path != '':\n            # it should be a file path\n            url = path2url(os.path.abspath(url))\n        else:\n            raise ValueError('url should be a http-url or file path -- ' + url)\n\n    return url", "response": "Normalize url to be a file path or http - url."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the folder containing the. json file", "response": "def url_dirname(url):\n    \"\"\" Return the folder containing the '.json' file\n    \"\"\"\n    p = six.moves.urllib.parse.urlparse(url)\n    for e in [private.FILE_EXT_JSON, private.FILE_EXT_YAML]:\n        if p.path.endswith(e):\n            return six.moves.urllib.parse.urlunparse(\n                p[:2]+\n                (os.path.dirname(p.path),)+\n                p[3:]\n            )\n    return url"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef normalize_jr(jr, url=None):\n\n    if jr == None:\n        return jr\n\n    idx = jr.find('#')\n    path, jp = (jr[:idx], jr[idx+1:]) if idx != -1 else (jr, None)\n\n    if len(path) > 0:\n        p = six.moves.urllib.parse.urlparse(path)\n        if p.scheme == '' and url:\n            p = six.moves.urllib.parse.urlparse(url)\n            # it's the path of relative file\n            path = six.moves.urllib.parse.urlunparse(p[:2]+('/'.join([os.path.dirname(p.path), path]),)+p[3:])\n            path = derelativise_url(path)\n    else:\n        path = url\n\n    if path:\n        return ''.join([path, '#', jp]) if jp else path\n    else:\n        return '#' + jp", "response": "normalize JSON reference, also fix\n    implicit reference of JSON pointer.\n    input:\n    - #/definitions/User\n    - http://test.com/swagger.json#/definitions/User\n    output:\n    - http://test.com/swagger.json#/definitions/User\n\n    input:\n    - some_folder/User.json\n    output:\n    - http://test.com/some_folder/User.json"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef derelativise_url(url):\n    '''\n    Normalizes URLs, gets rid of .. and .\n    '''\n    parsed = six.moves.urllib.parse.urlparse(url)\n    newpath=[]\n    for chunk in parsed.path[1:].split('/'):\n        if chunk == '.':\n            continue\n        elif chunk == '..':\n            # parent dir.\n            newpath=newpath[:-1]\n            continue\n        # TODO: Verify this behaviour.\n        elif _fullmatch(r'\\.{3,}', chunk) is not None:\n            # parent dir.\n            newpath=newpath[:-1]\n            continue\n        newpath += [chunk]\n    return six.moves.urllib.parse.urlunparse(parsed[:2]+('/'+('/'.join(newpath)),)+parsed[3:])", "response": "Takes a URL and returns a new URL with the original path."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets swagger version from loaded json", "response": "def get_swagger_version(obj):\n    \"\"\" get swagger version from loaded json \"\"\"\n\n    if isinstance(obj, dict):\n        if 'swaggerVersion' in obj:\n            return obj['swaggerVersion']\n        elif 'swagger' in obj:\n            return obj['swagger']\n        return None\n    else:\n        # should be an instance of BaseObj\n        return obj.swaggerVersion if hasattr(obj, 'swaggerVersion') else obj.swagger"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef walk(start, ofn, cyc=None):\n    ctx, stk = {}, [start]\n    cyc = [] if cyc == None else cyc\n\n    while len(stk):\n        top = stk[-1]\n\n        if top not in ctx:\n            ctx.update({top:list(ofn(top))})\n\n        if len(ctx[top]):\n            n = ctx[top][0]\n            if n in stk:\n                # cycles found,\n                # normalize the representation of cycles,\n                # start from the smallest vertex, ex.\n                # 4 -> 5 -> 2 -> 7 -> 9 would produce\n                # (2, 7, 9, 4, 5)\n                nc = stk[stk.index(n):]\n                ni = nc.index(min(nc))\n                nc = nc[ni:] + nc[:ni] + [min(nc)]\n                if nc not in cyc:\n                    cyc.append(nc)\n\n                ctx[top].pop(0)\n            else:\n                stk.append(n)\n        else:\n            ctx.pop(top)\n            stk.pop()\n            if len(stk):\n                ctx[stk[-1]].remove(top)\n\n    return cyc", "response": "Non recursive DFS to detect cycles in a list of nodes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomparing 2 dict and list return a list containing what s different and what s diff exactly.", "response": "def _diff_(src, dst, ret=None, jp=None, exclude=[], include=[]):\n    \"\"\" compare 2 dict/list, return a list containing\n    json-pointer indicating what's different, and what's diff exactly.\n\n    - list length diff: (jp, length of src, length of dst)\n    - dict key diff: (jp, None, None)\n    - when src is dict or list, and dst is not: (jp, type(src), type(dst))\n    - other: (jp, src, dst)\n    \"\"\"\n\n    def _dict_(src, dst, ret, jp):\n        ss, sd = set(src.keys()), set(dst.keys())\n        # what's include is prior to what's exclude\n        si, se = set(include or []), set(exclude or [])\n        ss, sd = (ss & si, sd & si) if si else (ss, sd)\n        ss, sd = (ss - se, sd - se) if se else (ss, sd)\n\n        # added keys\n        for k in sd - ss:\n            ret.append((jp_compose(k, base=jp), None, None,))\n\n        # removed keys\n        for k in ss - sd:\n            ret.append((jp_compose(k, base=jp), None, None,))\n\n        # same key\n        for k in ss & sd:\n            _diff_(src[k], dst[k], ret, jp_compose(k, base=jp), exclude, include)\n\n    def _list_(src, dst, ret, jp):\n        if len(src) < len(dst):\n            ret.append((jp, len(src), len(dst),))\n        elif len(src) > len(dst):\n            ret.append((jp, len(src), len(dst),))\n        else:\n            if len(src) == 0:\n                return\n\n            # make sure every element in list is the same\n            def r(x, y):\n                if type(y) != type(x):\n                    raise ValueError('different type: {0}, {1}'.format(type(y).__name__, type(x).__name__))\n                return x\n            ts = type(functools.reduce(r, src))\n            td = type(functools.reduce(r, dst))\n\n            # when type is different\n            while True:\n                if issubclass(ts, six.string_types) and issubclass(td, six.string_types):\n                    break\n                if issubclass(ts, six.integer_types) and issubclass(td, six.integer_types):\n                    break\n                if ts == td:\n                    break\n                ret.append((jp, str(ts), str(td),))\n                return\n\n            if ts != dict:\n                ss, sd = sorted(src), sorted(dst)\n            else:\n                # process dict without sorting\n                # TODO: find a way to sort list of dict, (ooch)\n                ss, sd = src, dst\n\n            for idx, (s, d) in enumerate(zip(src, dst)):\n                _diff_(s, d, ret, jp_compose(str(idx), base=jp), exclude, include)\n\n    ret = [] if ret == None else ret\n    jp = '' if jp == None else jp\n\n    if isinstance(src, dict):\n        if not isinstance(dst, dict):\n            ret.append((jp, type(src).__name__, type(dst).__name__,))\n        else:\n            _dict_(src, dst, ret, jp)\n    elif isinstance(src, list):\n        if not isinstance(dst, list):\n            ret.append((jp, type(src).__name__, type(dst).__name__,))\n        else:\n            _list_(src, dst, ret, jp)\n    elif src != dst:\n        ret.append((jp, src, dst,))\n\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _validate_data(self, path, obj, _):\n        errs = []\n\n        if obj.type == None and getattr(obj, '$ref') == None:\n            errs.append('type or $ref should be existed.')\n\n        # make sure there is 'item' along with 'array'\n        if not (obj.type == 'array') == (obj.items != None):\n            errs.append('array should be existed along with items')\n        if obj.uniqueItems != None and obj.type != 'array':\n            errs.append('uniqueItems is only used for array type.')\n\n        return path, obj.__class__.__name__, errs", "response": "validate data for the key - value store"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _validate_param(self, path, obj, _):\n        errs = []\n\n        if obj.allowMultiple:\n            if not obj.paramType in ('path', 'query', 'header'):\n                errs.append('allowMultiple should be applied on path, header, or query parameters')\n            if obj.type == 'array':\n                errs.append('array Type with allowMultiple is not supported.')\n\n        if obj.paramType == 'body' and obj.name not in ('', 'body'):\n            errs.append('body parameter with invalid name: {0}'.format(obj.name))\n\n        if obj.type == 'File':\n            if obj.paramType != 'form':\n                errs.append('File parameter should be form type: {0}'.format(obj.name))\n            if 'multipart/form-data' not in obj._parent_.consumes:\n                errs.append('File parameter should consume multipart/form-data: {0}'.format(obj.name))\n\n        if obj.type == 'void':\n            errs.append('void is only allowed in Operation object.')\n\n        return path, obj.__class__.__name__, errs", "response": "validate option combination of Parameter object"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nvalidates option combination of Property object", "response": "def _validate_items(self, path, obj, _):\n        \"\"\" validate option combination of Property object \"\"\"\n        errs = []\n\n        if obj.type == 'void':\n            errs.append('void is only allowed in Operation object.')\n\n        return path, obj.__class__.__name__, errs"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nvalidate that apiKey and oauth2 are set", "response": "def _validate_auth(self, path, obj, _):\n        \"\"\" validate that apiKey and oauth2 requirements \"\"\"\n        errs = []\n\n        if obj.type == 'apiKey':\n            if not obj.passAs:\n                errs.append('need \"passAs\" for apiKey')\n            if not obj.keyname:\n                errs.append('need \"keyname\" for apiKey')\n\n        elif obj.type == 'oauth2':\n            if not obj.grantTypes:\n                errs.append('need \"grantTypes\" for oauth2')\n\n        return path, obj.__class__.__name__, errs"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmake sure either implicit or authorization_code is defined", "response": "def _validate_granttype(self, path, obj, _):\n        \"\"\" make sure either implicit or authorization_code is defined \"\"\"\n        errs = []\n\n        if not obj.implicit and not obj.authorization_code:\n            errs.append('Either implicit or authorization_code should be defined.')\n\n        return path, obj.__class__.__name__, errs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _validate_auths(self, path, obj, app):\n        errs = []\n\n        for k, v in six.iteritems(obj.authorizations or {}):\n            if k not in app.raw.authorizations:\n                errs.append('auth {0} not found in resource list'.format(k))\n\n            if app.raw.authorizations[k].type in ('basicAuth', 'apiKey') and v != []:\n                errs.append('auth {0} should be an empty list'.format(k))\n\n        return path, obj.__class__.__name__, errs", "response": "validate that apiKey and basicAuth are empty list in Operation object"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nresolves the referenced object and inject it to self.", "response": "def _merge(obj, app, creator, parser):\n    \"\"\" resolve $ref, and inject/merge referenced object to self.\n    This operation should be carried in a cascade manner.\n    \"\"\"\n    result = creator(NullContext())\n    result.merge(obj, parser)\n\n    guard = CycleGuard()\n    guard.update(obj)\n\n    r = getattr(obj, '$ref')\n    while r and len(r) > 0:\n        ro = app.resolve(r, parser)\n        if ro.__class__ != obj.__class__:\n            raise TypeError('Referenced Type mismatch: {0}'.format(r))\n        try:\n            guard.update(ro)\n        except CycleDetectionError:\n            # avoid infinite loop,\n            # cycle detection has a dedicated scanner.\n            break\n\n        result.merge(ro, parser)\n        r = getattr(ro, '$ref')\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning default options for a specific entry in a dictionary", "response": "def default():\n        \"\"\" return default options, available options:\n        - max_name_length: maximum length of name for additionalProperties\n        - max_prop_count: maximum count of properties (count of fixed properties + additional properties)\n        - max_str_length: maximum length of string type\n        - max_byte_length: maximum length of byte type\n        - max_array_length: maximum length of array\n        - max_file_length: maximum length of file, in byte\n        - minimal_property: only generate 'required' properties\n        - minimal_parameter: only generate 'required' parameter\n        - files: registered file object: refer to pyswagger.primitives.File for details\n        - object_template: dict of default values assigned for properties when 'name' matched\n        - parameter_template: dict of default values assigned for parameters when 'name matched\n        - max_property: all properties are generated, ignore 'required'\n        - max_parameter: all parameters are generated, ignore 'required'\n\n        :return: options\n        :rtype: dict\n        \"\"\"\n        return dict(\n            max_name_length=64,\n            max_prop_count=32,\n            max_str_length=100,\n            max_byte_length=100,\n            max_array_length=100,\n            max_file_length=200,\n            minimal_property=False,\n            minimal_parameter=False,\n            files=[],\n            object_template={},\n            parameter_template={},\n            max_property=False,\n            max_parameter=False,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrender a Schema / Parameter", "response": "def render(self, obj, opt=None):\n        \"\"\" render a Schema/Parameter\n\n        :param obj Schema/Parameter: the swagger spec object\n        :param opt dict: render option\n        :return: values that can be passed to Operation.__call__\n        :rtype: depends on type of 'obj'\n        \"\"\"\n        opt = self.default() if opt == None else opt\n        if not isinstance(opt, dict):\n            raise ValueError('Not a dict: {0}'.format(opt))\n\n        if isinstance(obj, Parameter):\n            if getattr(obj, 'in', None) == 'body':\n                return self._generate(obj.schema, opt)\n            return self._generate(obj, opt=opt)\n        elif isinstance(obj, Schema):\n            return self._generate(obj, opt)\n        else:\n            raise ValueError('Not a Schema/Parameter: {0}'.format(obj))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef render_all(self, op, exclude=[], opt=None):\n        opt = self.default() if opt == None else opt\n        if not isinstance(op, Operation):\n            raise ValueError('Not a Operation: {0}'.format(op))\n        if not isinstance(opt, dict):\n            raise ValueError('Not a dict: {0}'.format(opt))\n\n        template = opt['parameter_template']\n        max_p = opt['max_parameter']\n        out = {}\n        for p in op.parameters:\n            if p.name in exclude:\n                continue\n            if p.name in template:\n                out.update({p.name: template[p.name]})\n                continue\n            if not max_p and not p.required:\n                if random.randint(0, 1) == 0 or opt['minimal_parameter']:\n                    continue\n            out.update({p.name: self.render(p, opt=opt)})\n        return out", "response": "render a set of parameters for an Operation"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _prepare_forms(self):\n        content_type = 'application/x-www-form-urlencoded'\n        if self.__op.consumes and content_type not in self.__op.consumes:\n            raise errs.SchemaError('content type {0} does not present in {1}'.format(content_type, self.__op.consumes))\n\n        return content_type, six.moves.urllib.parse.urlencode(self.__p['formData'])", "response": "private function to prepare content for paramType = form\n       "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef prepare(self, scheme='http', handle_files=True, encoding='utf-8'):\n\n        if isinstance(scheme, list):\n            if self.__scheme is None:\n                scheme = scheme.pop()\n            else:\n                if self.__scheme in scheme:\n                    scheme = self.__scheme\n                else:\n                    raise Exception('preferred scheme:{} is not supported by the client or spec:{}'.format(self.__scheme, scheme))\n        elif not isinstance(scheme, six.string_types):\n            raise ValueError('\"scheme\" should be a list or string')\n\n        # combine path parameters into path\n        # TODO: 'dot' is allowed in swagger, but it won't work in python-format\n        path_params = {}\n        for k, v in six.iteritems(self.__p['path']):\n            path_params[k] = six.moves.urllib.parse.quote_plus(v)\n\n        self.__path = self.__path.format(**path_params)\n\n        # combine path parameters into url\n        self.__url = ''.join([scheme, ':', self.__url.format(**path_params)])\n\n        # header parameters\n        self.__header.update(self.__p['header'])\n\n        # update data parameter\n        content_type = None\n        if self.__p['file']:\n            if handle_files:\n                content_type, self.__data = self._prepare_files(encoding)\n            else:\n                # client that can encode multipart/form-data, should\n                # access form-data via data property and file from file\n                # property.\n\n                # only form data can be carried along with files,\n                self.__data = self.__p['formData']\n\n        elif self.__p['formData']:\n            content_type, self.__data = self._prepare_forms()\n        elif self.__p['body']:\n            content_type, self.__data = self._prepare_body()\n        else:\n            self.__data = None\n\n        if content_type:\n            self.__header.update({'Content-Type': content_type})\n\n        accept = self.__produce\n        if not accept and self.__op.produces:\n            accept = self.__op.produces[0]\n        if accept:\n            if self.__op.produces and accept not in self.__op.produces:\n                raise errs.SchemaError('accept {0} does not present in {1}'.format(accept, self.__op.produces))\n            self.__header.update({'Accept': accept})\n\n        return self", "response": "make this request ready for Clients\n           "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupdates header status code raw datum header section", "response": "def apply_with(self, status=None, raw=None, header=None):\n        \"\"\" update header, status code, raw datum, ...etc\n\n        :param int status: status code\n        :param str raw: body content\n        :param dict header: header section\n        :return: return self for chaining\n        :rtype: Response\n        \"\"\"\n\n        if status != None:\n            self.__status = status\n\n        r = (final(self.__op.responses.get(str(self.__status), None)) or\n             final(self.__op.responses.get('default', None)))\n\n        if header != None:\n            if isinstance(header, (collections.Mapping, collections.MutableMapping)):\n                for k, v in six.iteritems(header):\n                    self._convert_header(r, k, v)\n            else:\n                for k, v in header:\n                    self._convert_header(r, k, v)\n\n        if raw != None:\n            # update 'raw'\n            self.__raw = raw\n\n            if self.__status == None:\n                raise Exception('Update status code before assigning raw data')\n\n            if r and r.schema and not self.__raw_body_only:\n                # update data from Opeartion if succeed else from responseMessage.responseModel\n                content_type = 'application/json'\n                for k, v in six.iteritems(self.header):\n                    if k.lower() == 'content-type':\n                        content_type = v[0].lower()\n                        break\n                schema = deref(r.schema)\n                _type = schema.type\n                _format = schema.format\n                name = schema.name\n                data = self.__op._mime_codec.unmarshal(content_type, self.raw, _type=_type, _format=_format, name=name)\n                self.__data = r.schema._prim_(data, self.__op._prim_factory, ctx=dict(read=True))\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmajoring part do parsing.", "response": "def parse(self, obj=None):\n        \"\"\" major part do parsing.\n\n        :param dict obj: json object to be parsed.\n        :raises ValueError: if obj is not a dict type.\n        \"\"\"\n        if obj == None:\n            return\n\n        if not isinstance(obj, dict):\n            raise ValueError('invalid obj passed: ' + str(type(obj)))\n\n        def _apply(x, kk, ct, v):\n            if key not in self._obj:\n                self._obj[kk] = {} if ct == None else []\n            with x(self._obj, kk) as ctx:\n                ctx.parse(obj=v)\n\n        def _apply_dict(x, kk, ct, v, k):\n            if k not in self._obj[kk]:\n                self._obj[kk][k] = {} if ct == ContainerType.dict_ else []\n            with x(self._obj[kk], k) as ctx:\n                ctx.parse(obj=v)\n\n        def _apply_dict_before_list(kk, ct, v, k):\n            self._obj[kk][k] = []\n\n        if hasattr(self, '__swagger_child__'):\n            # to nested objects\n            for key, (ct, ctx_kls) in six.iteritems(self.__swagger_child__):\n                items = obj.get(key, None)\n\n                # create an empty child, even it's None in input.\n                # this makes other logic easier.\n                if ct == ContainerType.list_:\n                    self._obj[key] = []\n                elif ct:\n                    self._obj[key] = {}\n\n                if items == None:\n                    continue\n\n                container_apply(ct, items,\n                    functools.partial(_apply, ctx_kls, key),\n                    functools.partial(_apply_dict, ctx_kls, key),\n                    functools.partial(_apply_dict_before_list, key)\n                )\n\n        # update _obj with obj\n        if self._obj != None:\n            for key in (set(obj.keys()) - set(self._obj.keys())):\n                self._obj[key] = obj[key]\n        else:\n            self._obj = obj"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nassigning parent to the current object", "response": "def _assign_parent(self, ctx):\n        \"\"\" parent assignment, internal usage only\n        \"\"\"\n        def _assign(cls, _, obj):\n            if obj == None:\n                return\n\n            if cls.is_produced(obj):\n                if isinstance(obj, BaseObj):\n                    obj._parent__ = self\n            else:\n                raise ValueError('Object is not instance of {0} but {1}'.format(cls.__swagger_ref_object__.__name__, obj.__class__.__name__))\n\n        # set self as childrent's parent\n        for name, (ct, ctx) in six.iteritems(ctx.__swagger_child__):\n            obj = getattr(self, name)\n            if obj == None:\n                continue\n\n            container_apply(ct, obj, functools.partial(_assign, ctx))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget private protected name of an attribute", "response": "def get_private_name(self, f):\n        \"\"\" get private protected name of an attribute\n\n        :param str f: name of the private attribute to be accessed.\n        \"\"\"\n        f = self.__swagger_rename__[f] if f in self.__swagger_rename__.keys() else f\n        return '_' + self.__class__.__name__ + '__' + f"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nupdates a field in a specific object", "response": "def update_field(self, f, obj):\n        \"\"\" update a field\n\n        :param str f: name of field to be updated.\n        :param obj: value of field to be updated.\n        \"\"\"\n        n = self.get_private_name(f)\n        if not hasattr(self, n):\n            raise AttributeError('{0} is not in {1}'.format(n, self.__class__.__name__))\n\n        setattr(self, n, obj)\n        self.__origin_keys.add(f)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef resolve(self, ts):\n        if isinstance(ts, six.string_types):\n            ts = [ts]\n\n        obj = self\n        while len(ts) > 0:\n            t = ts.pop(0)\n\n            if issubclass(obj.__class__, BaseObj):\n                obj = getattr(obj, t)\n            elif isinstance(obj, list):\n                obj = obj[int(t)]\n            elif isinstance(obj, dict):\n                obj = obj[t]\n\n        return obj", "response": "resolve a list of tokens to an object"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmerge properties from other object to self object", "response": "def merge(self, other, ctx, exclude=[]):\n        \"\"\" merge properties from other object,\n        - for child object, only merge from 'not-default' to 'default'.\n        - for others, merge (list, dict, primitives)\n\n        :param BaseObj other: the source object to be merged from.\n        :param Context ctx: the parsing context\n        \"\"\"\n        def _produce_new_obj(x, ct, v):\n            return x(None, None).produce().merge(v, x)\n\n        for name, default in itertools.chain(\n                six.iteritems(self.__swagger_fields__),\n                six.iteritems(self.__internal_fields__)):\n            if name in exclude:\n                continue\n\n            v = getattr(other, name)\n            if v == default:\n                continue\n\n            childs = [(n, ct, cctx) for n, (ct, cctx) in six.iteritems(ctx.__swagger_child__) if n == name]\n            if len(childs) == 0:\n                # we don't need to make a copy,\n                # since everything under App should be\n                # readonly.\n                if isinstance(v, list):\n                    try:\n                        self.update_field(name, list(set((getattr(self, name) or []) + v)))\n                    except TypeError:\n                        self.update_field(name, list((getattr(self, name) or []) + v))\n                elif isinstance(v, dict):\n                    d = copy.copy(v)\n                    d.update(getattr(self, name) or {})\n                    self.update_field(name, d)\n                elif getattr(self, name) == default:\n                    self.update_field(name, v)\n                continue\n            else:\n                # for child, stop when src object has something\n                if getattr(self, name) != default:\n                    continue\n\n            ct, cctx = childs[0][1], childs[0][2]\n            self.update_field(name,\n                container_apply(\n                    ct, v, \n                    functools.partial(_produce_new_obj, cctx)\n            ))\n\n        # make sure parent is correctly assigned.\n        self._assign_parent(ctx)\n\n        # allow cascade calling\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef compare(self, other, base=None):\n        if self.__class__ != other.__class__:\n            return False, ''\n\n        names = self._field_names_\n\n        def cmp_func(name, s, o):\n            # special case for string types\n            if isinstance(s, six.string_types) and isinstance(o, six.string_types):\n                return s == o, name\n\n            if s.__class__ != o.__class__:\n                return False, name\n\n            if isinstance(s, BaseObj):\n                if not isinstance(s, weakref.ProxyTypes):\n                    return s.compare(o, name)\n            elif isinstance(s, list):\n                for i, v in zip(range(len(s)), s):\n                    same, n = cmp_func(jp_compose(str(i), name), v, o[i])\n                    if not same:\n                        return same, n\n            elif isinstance(s, dict):\n                # compare if any key diff\n                diff = list(set(s.keys()) - set(o.keys()))\n                if diff:\n                    return False, jp_compose(str(diff[0]), name)\n                diff = list(set(o.keys()) - set(s.keys()))\n                if diff:\n                    return False, jp_compose(str(diff[0]), name)\n\n                for k, v in six.iteritems(s):\n                    same, n = cmp_func(jp_compose(k, name), v, o[k])\n                    if not same:\n                        return same, n\n            else:\n                return s == o, name\n\n            return True, name\n\n        for n in names:\n            same, n = cmp_func(jp_compose(n, base), getattr(self, n), getattr(other, n))\n            if not same:\n                return same, n\n\n        return True, ''", "response": "compare two weakref. ProxyTypes objects return True if the first difference"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dump(self):\n        r = {}\n        def _dump_(obj):\n            if isinstance(obj, dict):\n                ret = {}\n                for k, v in six.iteritems(obj):\n                    ret[k] = _dump_(v)\n                return None if ret == {} else ret\n            elif isinstance(obj, list):\n                ret = []\n                for v in obj:\n                    ret.append(_dump_(v))\n                return None if ret == [] else ret\n            elif isinstance(obj, BaseObj):\n                return obj.dump()\n            elif isinstance(obj, (six.string_types, six.integer_types)):\n                return obj\n            elif isinstance(obj, float):\n                return obj\n            else:\n                raise ValueError('Unknown object to dump: {0}'.format(obj.__class__.__name__))\n\n        for name, default in six.iteritems(self.__swagger_fields__):\n            # only dump a field when its value is not equal to default value\n            v = getattr(self, name)\n            if v != default:\n                d = _dump_(v)\n                if d != None:\n                    r[name] = d\n\n        return None if r == {} else r", "response": "dump the current object in dict which can be\n        convert to JSON"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _field_names_(self):\n        ret = []\n        for n in six.iterkeys(self.__swagger_fields__):\n            new_n = self.__swagger_rename__.get(n, None)\n            ret.append(new_n) if new_n else ret.append(n)\n        \n        return ret", "response": "get list of field names defined in Swagger spec\n\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _children_(self):\n        ret = {}\n        names = self._field_names_\n\n        def down(name, obj):\n            if isinstance(obj, BaseObj):\n                if not isinstance(obj, weakref.ProxyTypes):\n                    ret[name] = obj\n            elif isinstance(obj, list):\n                for i, v in zip(range(len(obj)), obj):\n                    down(jp_compose(str(i), name), v)\n            elif isinstance(obj, dict):\n                for k, v in six.iteritems(obj):\n                    down(jp_compose(k, name), v)\n\n        for n in names:\n            down(jp_compose(n), getattr(self, n))\n\n        return ret", "response": "get children objects\n\n        :rtype: a dict of children {child_name: child_object}"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load_obj(self, jref, getter=None, parser=None):\n        obj = self.__resolver.resolve(jref, getter)\n\n        # get root document to check its swagger version.\n        tmp = {'_tmp_': {}}\n        version = utils.get_swagger_version(obj)\n        if version == '1.2':\n            # swagger 1.2\n            with ResourceListContext(tmp, '_tmp_') as ctx:\n                ctx.parse(obj, jref, self.__resolver, getter)\n        elif version == '2.0':\n            # swagger 2.0\n            with SwaggerContext(tmp, '_tmp_') as ctx:\n                ctx.parse(obj)\n        elif version == None and parser:\n            with parser(tmp, '_tmp_') as ctx:\n                ctx.parse(obj)\n\n            version = tmp['_tmp_'].__swagger_version__ if hasattr(tmp['_tmp_'], '__swagger_version__') else version\n        else:\n            raise NotImplementedError('Unsupported Swagger Version: {0} from {1}'.format(version, jref))\n\n        if not tmp['_tmp_']:\n            raise Exception('Unable to parse object from {0}'.format(jref))\n\n        logger.info('version: {0}'.format(version))\n\n        return tmp['_tmp_'], version", "response": "load a single object from a JSON reference."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\npreparing an object for use in the schema", "response": "def prepare_obj(self, obj, jref):\n        \"\"\" basic preparation of an object(those in sepc._version_.objects),\n        and cache the 'prepared' object.\n        \"\"\"\n        if not obj:\n            raise Exception('unexpected, passing {0}:{1} to prepare'.format(obj, jref))\n\n        s = Scanner(self)\n        if self.version == '1.2':\n            # upgrade from 1.2 to 2.0\n            converter = Upgrade(self.__sep)\n            s.scan(root=obj, route=[converter])\n            obj = converter.swagger\n\n            if not obj:\n                raise Exception('unable to upgrade from 1.2: {0}'.format(jref))\n\n            s.scan(root=obj, route=[AssignParent()])\n\n        # fix for yaml that treat response code as number\n        s.scan(root=obj, route=[YamlFixer()], leaves=[Operation])\n        # normalize $ref\n        url, jp = utils.jr_split(jref)\n        s.scan(root=obj, route=[NormalizeRef(url)])\n        # cache this object\n        if url not in self.__objs:\n            if jp == '#':\n                self.__objs[url] = obj\n            else:\n                self.__objs[url] = {jp: obj}\n        else:\n            if not isinstance(self.__objs[url], dict):\n                raise Exception('it should be able to resolve with BaseObj')\n            self.__objs[url].update({jp: obj})\n\n        # pre resolve Schema Object\n        # note: make sure this object is cached before using 'Resolve' scanner\n        s.scan(root=obj, route=[Resolve()])\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if this Swagger API is valid or not", "response": "def _validate(self):\n        \"\"\" check if this Swagger API valid or not.\n\n        :param bool strict: when in strict mode, exception would be raised if not valid.\n        :return: validation errors\n        :rtype: list of tuple(where, type, msg).\n        \"\"\"\n\n        v_mod = utils.import_string('.'.join([\n            'pyswagger',\n            'scanner',\n            'v' + self.version.replace('.', '_'),\n            'validate'\n        ]))\n\n        if not v_mod:\n            # there is no validation module\n            # for this version of spec\n            return []\n\n        s = Scanner(self)\n        v = v_mod.Validate()\n\n        s.scan(route=[v], root=self.__raw)\n        return v.errs"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nloading a json from a url and returns the App object.", "response": "def load(kls, url, getter=None, parser=None, url_load_hook=None, sep=consts.private.SCOPE_SEPARATOR, prim=None, mime_codec=None, resolver=None):\n        \"\"\" load json as a raw App\n\n        :param str url: url of path of Swagger API definition\n        :param getter: customized Getter\n        :type getter: sub class/instance of Getter\n        :param parser: the parser to parse the loaded json.\n        :type parser: pyswagger.base.Context\n        :param dict app_cache: the cache shared by related App\n        :param func url_load_hook: hook to patch the url to load json\n        :param str sep: scope-separater used in this App\n        :param prim pyswager.primitives.Primitive: factory for primitives in Swagger\n        :param mime_codec pyswagger.primitives.MimeCodec: MIME codec\n        :param resolver: pyswagger.resolve.Resolver: customized resolver used as default when none is provided when resolving\n        :return: the created App object\n        :rtype: App\n        :raises ValueError: if url is wrong\n        :raises NotImplementedError: the swagger version is not supported.\n        \"\"\"\n\n        logger.info('load with [{0}]'.format(url))\n\n        url = utils.normalize_url(url)\n        app = kls(url, url_load_hook=url_load_hook, sep=sep, prim=prim, mime_codec=mime_codec, resolver=resolver)\n        app.__raw, app.__version = app.load_obj(url, getter=getter, parser=parser)\n        if app.__version not in ['1.2', '2.0']:\n            raise NotImplementedError('Unsupported Version: {0}'.format(self.__version))\n\n        # update scheme if any\n        p = six.moves.urllib.parse.urlparse(url)\n        if p.scheme:\n            app.schemes.append(p.scheme)\n\n        return app"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef validate(self, strict=True):\n\n        result = self._validate()\n        if strict and len(result):\n            for r in result:\n                logger.error(r)\n            raise errs.ValidationError('this Swagger App contains error: {0}.'.format(len(result)))\n\n        return result", "response": "check if this Swagger API is valid or not."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef prepare(self, strict=True):\n\n        self.__root = self.prepare_obj(self.raw, self.__url)\n        self.validate(strict=strict)\n\n        if hasattr(self.__root, 'schemes') and self.__root.schemes:\n            if len(self.__root.schemes) > 0:\n                self.__schemes = self.__root.schemes\n            else:\n                # extract schemes from the url to load spec\n                self.__schemes = [six.moves.urlparse(self.__url).schemes]\n\n        s = Scanner(self)\n        s.scan(root=self.__root, route=[Merge()])\n        s.scan(root=self.__root, route=[PatchObject()])\n        s.scan(root=self.__root, route=[Aggregate()])\n\n        # reducer for Operation\n        tr = TypeReduce(self.__sep)\n        cy = CycleDetector()\n        s.scan(root=self.__root, route=[tr, cy])\n\n        # 'op' -- shortcut for Operation with tag and operaionId\n        self.__op = utils.ScopeDict(tr.op)\n        # 'm' -- shortcut for model in Swagger 1.2\n        if hasattr(self.__root, 'definitions') and self.__root.definitions != None:\n            self.__m = utils.ScopeDict(self.__root.definitions)\n        else:\n            self.__m = utils.ScopeDict({})\n        # update scope-separater\n        self.__m.sep = self.__sep\n        self.__op.sep = self.__sep\n\n        # cycle detection\n        if len(cy.cycles['schema']) > 0 and strict:\n            raise errs.CycleDetectionError('Cycles detected in Schema Object: {0}'.format(cy.cycles['schema']))", "response": "prepare for loaded json\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nresolves a JSON reference to a weakref. ProxyType object.", "response": "def resolve(self, jref, parser=None):\n        \"\"\" JSON reference resolver\n\n        :param str jref: a JSON Reference, refer to http://tools.ietf.org/html/draft-pbryan-zyp-json-ref-03 for details.\n        :param parser: the parser corresponding to target object.\n        :type parser: pyswagger.base.Context\n        :return: the referenced object, wrapped by weakref.ProxyType\n        :rtype: weakref.ProxyType\n        :raises ValueError: if path is not valid\n        \"\"\"\n\n        logger.info('resolving: [{0}]'.format(jref))\n\n        if jref == None or len(jref) == 0:\n            raise ValueError('Empty Path is not allowed')\n\n        obj = None\n        url, jp = utils.jr_split(jref)\n\n        # check cacahed object against json reference by\n        # comparing url first, and find those object prefixed with\n        # the JSON pointer.\n        o = self.__objs.get(url, None)\n        if o:\n            if isinstance(o, BaseObj):\n                obj = o.resolve(utils.jp_split(jp)[1:])\n            elif isinstance(o, dict):\n                for k, v in six.iteritems(o):\n                    if jp.startswith(k):\n                        obj = v.resolve(utils.jp_split(jp[len(k):])[1:])\n                        break\n            else:\n                raise Exception('Unknown Cached Object: {0}'.format(str(type(o))))\n\n        # this object is not found in cache\n        if obj == None:\n            if url:\n                obj, _ = self.load_obj(jref, parser=parser)\n                if obj:\n                    obj = self.prepare_obj(obj, jref)\n            else:\n                # a local reference, 'jref' is just a json-pointer\n                if not jp.startswith('#'):\n                    raise ValueError('Invalid Path, root element should be \\'#\\', but [{0}]'.format(jref))\n\n                obj = self.root.resolve(utils.jp_split(jp)[1:]) # heading element is #, mapping to self.root\n\n        if obj == None:\n            raise ValueError('Unable to resolve path, [{0}]'.format(jref))\n\n        if isinstance(obj, (six.string_types, six.integer_types, list, dict)):\n            return obj\n        return weakref.proxy(obj)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninsert or clear the authorizations", "response": "def update_with(self, name, security_info):\n        \"\"\" insert/clear authorizations\n\n        :param str name: name of the security info to be updated\n        :param security_info: the real security data, token, ...etc.\n        :type security_info: **(username, password)** for *basicAuth*, **token** in str for *oauth2*, *apiKey*.\n\n        :raises ValueError: unsupported types of authorizations\n        \"\"\"\n        s = self.__app.root.securityDefinitions.get(name, None)\n        if s == None:\n            raise ValueError('Unknown security name: [{0}]'.format(name))\n\n        cred = security_info\n        header = True\n        if s.type == 'basic':\n            cred = 'Basic ' + base64.standard_b64encode(six.b('{0}:{1}'.format(*security_info))).decode('utf-8')\n            key = 'Authorization'\n        elif s.type == 'apiKey':\n            key = s.name\n            header = getattr(s, 'in') == 'header'\n        elif s.type == 'oauth2':\n            key = 'access_token'\n        else:\n            raise ValueError('Unsupported Authorization type: [{0}, {1}]'.format(name, s.type))\n\n        self.__info.update({name: (header, {key: cred})})"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmakes sure this client support schemes required by current request", "response": "def prepare_schemes(self, req):\n        \"\"\" make sure this client support schemes required by current request\n\n        :param pyswagger.io.Request req: current request object\n        \"\"\"\n\n        # fix test bug when in python3 scheme, more details in commint msg\n        ret = sorted(self.__schemes__ & set(req.schemes), reverse=True)\n\n        if len(ret) == 0:\n            raise ValueError('No schemes available: {0}'.format(req.schemes))\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef compose_headers(self, req, headers=None, opt=None, as_dict=False):\n        if headers is None:\n            return list(req.header.items()) if not as_dict else req.header\n\n        opt = opt or {}\n        join_headers = opt.pop(BaseClient.join_headers, None)\n        if as_dict and not join_headers:\n            # pick the most efficient way for special case\n            headers = dict(headers) if isinstance(headers, list) else headers\n            headers.update(req.header)\n            return headers\n\n        # include Request.headers\n        aggregated_headers = list(req.header.items())\n\n        # include customized headers\n        if isinstance(headers, list):\n            aggregated_headers.extend(headers)\n        elif isinstance(headers, dict):\n            aggregated_headers.extend(headers.items())\n        else:\n            raise Exception('unknown type as header: {}'.format(str(type(headers))))\n\n        if join_headers:\n            joined = {}\n            for h in aggregated_headers:\n                key = h[0]\n                if key in joined:\n                    joined[key] = ','.join([joined[key], h[1]])\n                else:\n                    joined[key] = h[1]\n            aggregated_headers = list(joined.items())\n\n        return dict(aggregated_headers) if as_dict else aggregated_headers", "response": "a utility to compose headers from pyswagger. io. Request and customized headers\n\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\npatches request and response with additional information", "response": "def request(self, req_and_resp, opt=None, headers=None):\n        \"\"\" preprocess before performing a request, usually some patching.\n        authorization also applied here.\n\n        :param req_and_resp: tuple of Request and Response\n        :type req_and_resp: (Request, Response)\n        :param opt: customized options\n        :type opt: dict\n        :param headers: customized headers\n        :type headers: dict of 'string', or list of tuple: (string, string) for multiple values for one key\n        :return: patched request and response\n        :rtype: Request, Response\n        \"\"\"\n        req, resp = req_and_resp\n\n        # dump info for debugging\n        logger.info('request.url: {0}'.format(req.url))\n        logger.info('request.header: {0}'.format(req.header))\n        logger.info('request.query: {0}'.format(req.query))\n        logger.info('request.file: {0}'.format(req.files))\n        logger.info('request.schemes: {0}'.format(req.schemes))\n\n        # apply authorizations\n        if self.__security:\n            self.__security(req)\n\n        return req, resp"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nvalidate that the key val is a dict containing the relevant information.", "response": "def apply_with(self, _, val, ctx):\n        \"\"\" constructor\n\n        example val:\n        {\n            # header values used in multipart/form-data according to RFC2388\n            'header': {\n                'Content-Type': 'text/plain',\n                \n                # according to RFC2388, available values are '7bit', '8bit', 'binary'\n                'Content-Transfer-Encoding': 'binary'\n            },\n            filename: 'a.txt',\n            data: None (or any file-like object)\n        }\n\n        :param val: dict containing file info.\n        \"\"\"\n        self.header = val.get('header', {})\n        self.data = val.get('data', None)\n        self.filename = val.get('filename', '')\n        if self.data == None and self.filename == '':\n            raise ValidationError('should have file name or file object, not: {0}, {1}'.format(\n                self.data, self.filename\n            ))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef to_url(self):\n        if self.__collection_format == 'multi':\n            return [str(s) for s in self]\n        else:\n            return [str(self)]", "response": "converts the resource to a list of url"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef apply_with(self, obj, val, ctx):\n        for k, v in six.iteritems(val):\n            if k in obj.properties:\n                pobj = obj.properties.get(k)\n                if pobj.readOnly == True and ctx['read'] == False:\n                    raise Exception('read-only property is set in write context.')\n\n                self[k] = ctx['factory'].produce(pobj, v)\n\n            # TODO: patternProperties here\n            # TODO: fix bug, everything would not go into additionalProperties, instead of recursive\n            elif obj.additionalProperties == True:\n                ctx['addp'] = True\n            elif obj.additionalProperties not in (None, False):\n                ctx['addp_schema'] = obj\n\n        in_obj = set(six.iterkeys(obj.properties))\n        in_self = set(six.iterkeys(self))\n\n        other_prop = in_obj - in_self\n        for k in other_prop:\n            p = obj.properties[k]\n            if p.is_set(\"default\"):\n                self[k] = ctx['factory'].produce(p, p.default)\n\n        not_found = set(obj.required) - set(six.iterkeys(self))\n        if len(not_found):\n            raise ValueError('Model missing required key(s): {0}'.format(', '.join(not_found)))\n\n        # remove assigned properties to avoid duplicated\n        # primitive creation\n        _val = {}\n        for k in set(six.iterkeys(val)) - in_obj:\n            _val[k] = val[k]\n\n        if obj.discriminator:\n            self[obj.discriminator] = ctx['name']\n\n        return _val", "response": "recursively apply schema object\n            to self and return the new dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfulfills name field for objects under", "response": "def _schema(self, path, obj, app):\n        \"\"\" fulfill 'name' field for objects under\n        '#/definitions' and with 'properties'\n        \"\"\"\n        if path.startswith('#/definitions'):\n            last_token = jp_split(path)[-1]\n            if app.version == '1.2':\n                obj.update_field('name', scope_split(last_token)[-1])\n            else:\n                obj.update_field('name', last_token)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert status code in Responses from int to string", "response": "def _op(self, _, obj, app):\n        \"\"\" convert status code in Responses from int to string\n        \"\"\"\n        if obj.responses == None: return \n\n        tmp = {}\n        for k, v in six.iteritems(obj.responses):\n            if isinstance(k, six.integer_types):\n                tmp[str(k)] = v\n            else:\n                tmp[k] = v\n        obj.update_field('responses', tmp)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nregisters a type and format handler when producing primitives", "response": "def register(self, _type, _format, creater, _2nd_pass=None):\n        \"\"\" register a type/format handler when producing primitives\n\n        example function to create a byte primitive:\n        ```python\n        def create_byte(obj, val, ctx):\n            # val is the value used to create this primitive, for example, a\n            # dict would be used to create a Model and list would be used to\n            # create an Array\n\n            # obj in the spec used to create primitives, they are\n            # Header, Items, Schema, Parameter in Swagger 2.0.\n\n            # ctx is parsing context when producing primitives. Some primitves needs\n            # multiple passes to produce(ex. Model), when we need to keep some globals\n            # between passes, we should place them in ctx\n            return base64.urlsafe_b64encode(val)\n        ```\n\n        example function of 2nd pass:\n        ```python\n        def validate_int(obj, ret, val, ctx):\n            # val, obj, ctx are the same as those in creater\n\n            # ret is the object returned by creater\n\n            # do some stuff\n            check_min_max(obj, val)\n\n            # remember to return val, the 'outer' val would be overwritten\n            # by the one you return, if you didn't return, it would be None.\n            return val\n        ```\n\n        pseudo function of 2nd pass in Model:\n        ```python\n        def gen_mode(obj, ret, val, ctx):\n            # - go through obj.properties to create properties of this model, and add\n            #   them to 'ret'.\n            # - remove those values used in this pass in 'val'\n            # - return val\n        ```\n\n        :param _type str: type in json-schema\n        :param _format str: format in json-schema\n        :param creater function: a function to create a primitive.\n        :param _2nd_pass function: a function used in 2nd pass when producing primitive.\n        \"\"\"\n        if _type not in self._map:\n            self._map[_type] = {}\n        self._map[_type][_format] = (creater, _2nd_pass)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef produce(self, obj, val, ctx=None):\n        val = obj.default if val == None else val\n        if val == None:\n            return None\n\n        obj = deref(obj)\n        ctx = {} if ctx == None else ctx\n        if 'name' not in ctx and hasattr(obj, 'name'):\n            ctx['name'] = obj.name\n        if 'guard' not in ctx:\n            ctx['guard'] = CycleGuard()\n        if 'addp_schema' not in ctx:\n            # Schema Object of additionalProperties\n            ctx['addp_schema'] = None\n        if 'addp' not in ctx:\n            # additionalProperties\n            ctx['addp'] = False\n        if '2nd_pass' not in ctx:\n            # 2nd pass processing function\n            ctx['2nd_pass'] = None\n        if 'factory' not in ctx:\n            # primitive factory\n            ctx['factory'] = self\n        if 'read' not in ctx:\n            # default is in 'read' context\n            ctx['read'] = True\n\n        # cycle guard\n        ctx['guard'].update(obj)\n\n        ret = None\n        if obj.type:\n            creater, _2nd = self.get(_type=obj.type, _format=obj.format)\n            if not creater:\n                raise ValueError('Can\\'t resolve type from:(' + str(obj.type) + ', ' + str(obj.format) + ')')\n\n            ret = creater(obj, val, ctx)\n            if _2nd:\n                val = _2nd(obj, ret, val, ctx)\n                ctx['2nd_pass'] = _2nd\n        elif len(obj.properties) or obj.additionalProperties:\n            ret = Model()\n            val = ret.apply_with(obj, val, ctx)\n\n        if isinstance(ret, (Date, Datetime, Byte, File)):\n            # it's meanless to handle allOf for these types.\n            return ret\n\n        def _apply(o, r, v, c):\n            if hasattr(ret, 'apply_with'):\n                v = r.apply_with(o, v, c)\n            else:\n                _2nd = c['2nd_pass']\n                if _2nd == None:\n                    _, _2nd = self.get(_type=o.type, _format=o.format)\n                if _2nd:\n                    _2nd(o, r, v, c)\n                    # update it back to context\n                    c['2nd_pass'] = _2nd\n            return v\n\n        # handle allOf for Schema Object\n        allOf = getattr(obj, 'allOf', None)\n        if allOf:\n            not_applied = []\n            for a in allOf:\n                a = deref(a)\n                if not ret:\n                    # try to find right type for this primitive.\n                    ret = self.produce(a, val, ctx)\n                    is_member = hasattr(ret, 'apply_with')\n                else:\n                    val = _apply(a, ret, val, ctx)\n\n                if not ret:\n                    # if we still can't determine the type,\n                    # keep this Schema object for later use.\n                    not_applied.append(a)\n            if ret:\n                for a in not_applied:\n                    val = _apply(a, ret, val, ctx)\n\n        if ret != None and hasattr(ret, 'cleanup'):\n            val = ret.cleanup(val, ctx)\n\n        return ret", "response": "produce - creates a new object of the appropriate type based on the spec and value"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses an indexing expression and returns a set of all used identifiers and a list of composite axes.", "response": "def parse_expression(expression: str) -> Tuple[Set[str], List[CompositeAxis]]:\n    \"\"\"\n    Parses an indexing expression (for a single tensor).\n    Checks uniqueness of names, checks usage of '...' (allowed only once)\n    Returns set of all used identifiers and a list of axis groups\n    \"\"\"\n    identifiers = set()\n    composite_axes = []\n    if '.' in expression:\n        if '...' not in expression:\n            raise EinopsError('Expression may contain dots only inside ellipsis (...)')\n        if str.count(expression, '...') != 1 or str.count(expression, '.') != 3:\n            raise EinopsError('Expression may contain dots only inside ellipsis (...); only one ellipsis for tensor ')\n        expression = expression.replace('...', _ellipsis)\n\n    bracket_group = None\n\n    def add_axis_name(x):\n        if x is not None:\n            if x in identifiers:\n                raise ValueError('Indexing expression contains duplicate dimension \"{}\"'.format(x))\n            identifiers.add(x)\n            if bracket_group is None:\n                composite_axes.append([x])\n            else:\n                bracket_group.append(x)\n\n    current_identifier = None\n    for char in expression:\n        if char in '() ' + _ellipsis:\n            add_axis_name(current_identifier)\n            current_identifier = None\n            if char == _ellipsis:\n                if bracket_group is not None:\n                    raise EinopsError(\"Ellipsis can't be used inside the composite axis (inside brackets)\")\n                composite_axes.append(_ellipsis)\n                identifiers.add(_ellipsis)\n            elif char == '(':\n                if bracket_group is not None:\n                    raise EinopsError(\"Axis composition is one-level (brackets inside brackets not allowed)\")\n                bracket_group = []\n            elif char == ')':\n                if bracket_group is None:\n                    raise EinopsError('Brackets are not balanced')\n                composite_axes.append(bracket_group)\n                bracket_group = None\n        elif '0' <= char <= '9':\n            if current_identifier is None:\n                raise EinopsError(\"Axis name can't start with a digit\")\n            current_identifier += char\n        elif 'a' <= char <= 'z':\n            if current_identifier is None:\n                current_identifier = char\n            else:\n                current_identifier += char\n        else:\n            if 'A' <= char <= 'Z':\n                raise EinopsError(\"Only lower-case latin letters allowed in names, not '{}'\".format(char))\n            raise EinopsError(\"Unknown character '{}'\".format(char))\n\n    if bracket_group is not None:\n        raise EinopsError('Imbalanced parentheses in expression: \"{}\"'.format(expression))\n    add_axis_name(current_identifier)\n    return identifiers, composite_axes"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if the given elementary axis name is valid.", "response": "def _check_elementary_axis_name(name: str) -> bool:\n    \"\"\"\n    Valid elementary axes contain only lower latin letters and digits and start with a letter.\n    \"\"\"\n    if len(name) == 0:\n        return False\n    if not 'a' <= name[0] <= 'z':\n        return False\n    for letter in name:\n        if (not letter.isdigit()) and not ('a' <= letter <= 'z'):\n            return False\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprepares the transformation recipe for the given pattern and reduction.", "response": "def _prepare_transformation_recipe(pattern: str, reduction: str, axes_lengths: Tuple) -> TransformRecipe:\n    \"\"\" Perform initial parsing of pattern and provided supplementary info\n    axes_lengths is a tuple of tuples (axis_name, axis_length)\n    \"\"\"\n    left, right = pattern.split('->')\n    identifiers_left, composite_axes_left = parse_expression(left)\n    identifiers_rght, composite_axes_rght = parse_expression(right)\n\n    # checking that both have similar letters\n    if reduction == 'rearrange':\n        difference = set.symmetric_difference(identifiers_left, identifiers_rght)\n        if len(difference) > 0:\n            raise EinopsError('Identifiers only on one side of expression (should be on both): {}'.format(difference))\n    elif reduction in _reductions:\n        difference = set.difference(identifiers_rght, identifiers_left)\n        if len(difference) > 0:\n            raise EinopsError('Unexpected identifiers on the right side of expression: {}'.format(difference))\n    else:\n        raise EinopsError('Unknown reduction {}'.format(reduction))\n\n    # parsing all dimensions to find out lengths\n    known_lengths = OrderedDict()\n    position_lookup = {}\n    position_lookup_after_reduction = {}\n    reduced_axes = []\n    for composite_axis in composite_axes_left:\n        for axis in composite_axis:\n            position_lookup[axis] = len(position_lookup)\n            if axis in identifiers_rght:\n                position_lookup_after_reduction[axis] = len(position_lookup_after_reduction)\n            else:\n                reduced_axes.append(len(known_lengths))\n            known_lengths[axis] = None\n\n    def update_axis_length(axis_name, axis_length):\n        if known_lengths[axis_name] is not None:\n            # check is not performed for symbols\n            if isinstance(axis_length, int) and isinstance(known_lengths[axis_name], int):\n                if axis_length != known_lengths[axis_name]:\n                    raise RuntimeError('Inferred length for {} is {} not {}'.format(\n                        axis_name, axis_length, known_lengths[axis_name]))\n        else:\n            known_lengths[axis_name] = axis_length\n\n    for elementary_axis, axis_length in axes_lengths:\n        if not _check_elementary_axis_name(elementary_axis):\n            raise EinopsError('Invalid name for an axis', elementary_axis)\n        if elementary_axis not in known_lengths:\n            raise EinopsError('Axis {} is not used in transform'.format(elementary_axis))\n        update_axis_length(elementary_axis, axis_length)\n\n    input_axes_known_unknown = []\n    # inferring rest of sizes from arguments\n    for composite_axis in composite_axes_left:\n        known = {axis for axis in composite_axis if known_lengths[axis] is not None}\n        unknown = {axis for axis in composite_axis if known_lengths[axis] is None}\n        lookup = dict(zip(list(known_lengths), range(len(known_lengths))))\n        if len(unknown) > 1:\n            raise EinopsError('Could not infer sizes for {}'.format(unknown))\n        assert len(unknown) + len(known) == len(composite_axis)\n        input_axes_known_unknown.append(([lookup[axis] for axis in known], [lookup[axis] for axis in unknown]))\n\n    result_axes_grouping = [[position_lookup_after_reduction[axis] for axis in composite_axis]\n                            for composite_axis in composite_axes_rght]\n\n    ellipsis_left = math.inf if _ellipsis not in composite_axes_left else composite_axes_left.index(_ellipsis)\n    ellipsis_rght = math.inf if _ellipsis not in composite_axes_rght else composite_axes_rght.index(_ellipsis)\n\n    return TransformRecipe(elementary_axes_lengths=list(known_lengths.values()),\n                           input_composite_axes=input_axes_known_unknown,\n                           output_composite_axes=result_axes_grouping,\n                           reduction_type=reduction,\n                           reduced_elementary_axes=tuple(reduced_axes),\n                           ellipsis_positions=(ellipsis_left, ellipsis_rght)\n                           )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef reduce(tensor, pattern: str, reduction: str, **axes_lengths: int):\n    try:\n        hashable_axes_lengths = tuple(sorted(axes_lengths.items()))\n        recipe = _prepare_transformation_recipe(pattern, reduction, axes_lengths=hashable_axes_lengths)\n        return recipe.apply(tensor)\n    except EinopsError as e:\n        message = ' Error while processing {}-reduction pattern \"{}\".'.format(reduction, pattern)\n        if not isinstance(tensor, list):\n            message += '\\n Input tensor shape: {}. '.format(get_backend(tensor).shape(tensor))\n        else:\n            message += '\\n Input is list. '\n        message += 'Additional info: {}.'.format(axes_lengths)\n        raise EinopsError(message + '\\n {}'.format(e))", "response": "Einstein reduce operation for a single graph."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef rearrange(tensor, pattern, **axes_lengths):\n    if isinstance(tensor, list):\n        if len(tensor) == 0:\n            raise TypeError(\"Rearrange can't be applied to an empty list\")\n        tensor = get_backend(tensor[0]).stack_on_zeroth_dimension(tensor)\n    return reduce(tensor, pattern, reduction='rearrange', **axes_lengths)", "response": "Rearrange a single - dimensional tensor into a single - dimensional alphabetical alphabet"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses a tensor shape to dictionary mapping axes names to their lengths.", "response": "def parse_shape(x, pattern: str):\n    \"\"\"\n    Parse a tensor shape to dictionary mapping axes names to their lengths.\n    Use underscore to skip the dimension in parsing.\n    >>> x = np.zeros([2, 3, 5, 7])\n    >>> parse_shape(x, 'batch _ h w')\n    {'batch': 2, 'h': 5, 'w': 7}\n\n    parse_shape output can be used to specify axes_lengths for other operations\n    >>> y = np.zeros([700])\n    >>> rearrange(y, '(b c h w) -> b c h w', **parse_shape(x, 'b _ h w')).shape\n    (2, 10, 5, 7)\n\n    For symbolic frameworks may return symbols, not integers.\n    :param x: tensor of any of supported frameworks\n    :param pattern: str, space separated names for axes, underscore means skip axis\n    :return: dict, maps axes names to their lengths\n    \"\"\"\n    names = [elementary_axis for elementary_axis in pattern.split(' ') if len(elementary_axis) > 0]\n    shape = get_backend(x).shape(x)\n    if len(shape) != len(names):\n        raise RuntimeError(\"Can't parse shape with different number of dimensions: {pattern} {shape}\".format(\n            pattern=pattern, shape=shape))\n    result = {}\n    for axis_name, axis_length in zip(names, shape):\n        if axis_name != '_':\n            result[axis_name] = axis_length\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nenumerate the n - dimensional tensors of a n - dimensional tensor and return a list of tensors that are connected to each axis.", "response": "def _enumerate_directions(x):\n    \"\"\"\n    For an n-dimensional tensor, returns tensors to enumerate each axis.\n    >>> x = np.zeros([2, 3, 4]) # or any other tensor\n    >>> i, j, k = _enumerate_directions(x)\n    >>> result = i + 2 * j + 3 * k\n\n    result[i, j, k] = i + 2 * j + 3 * k, and also has the same shape as result\n    Works very similarly to numpy.ogrid (open indexing grid)\n    \"\"\"\n    backend = get_backend(x)\n    shape = backend.shape(x)\n    result = []\n    for axis_id, axis_length in enumerate(shape):\n        shape = [1] * len(shape)\n        shape[axis_id] = axis_length\n        result.append(backend.reshape(backend.arange(0, axis_length), shape))\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef reconstruct_from_shape(self, shape, optimize=False):\n        axes_lengths = list(self.elementary_axes_lengths)\n        if self.ellipsis_positions != (math.inf, math.inf):\n            if len(shape) < len(self.input_composite_axes) - 1:\n                raise EinopsError('Expected at least {} dimensions, got {}'.format(\n                    len(self.input_composite_axes) - 1, len(shape)))\n        else:\n            if len(shape) != len(self.input_composite_axes):\n                raise EinopsError('Expected {} dimensions, got {}'.format(len(self.input_composite_axes), len(shape)))\n        for input_axis, (known_axes, unknown_axes) in enumerate(self.input_composite_axes):\n            before_ellipsis = input_axis\n            after_ellipsis = input_axis + len(shape) - len(self.input_composite_axes)\n            if input_axis == self.ellipsis_positions[0]:\n                assert len(known_axes) == 0 and len(unknown_axes) == 1\n                unknown_axis, = unknown_axes\n                ellipsis_shape = shape[before_ellipsis:after_ellipsis + 1]\n                if any(d is None for d in ellipsis_shape):\n                    raise EinopsError(\"Couldn't infer shape for one or more axes represented by ellipsis\")\n                axes_lengths[unknown_axis] = _product(ellipsis_shape)\n            else:\n                if input_axis < self.ellipsis_positions[0]:\n                    length = shape[before_ellipsis]\n                else:\n                    length = shape[after_ellipsis]\n                known_product = 1\n                for axis in known_axes:\n                    known_product *= axes_lengths[axis]\n\n                if len(unknown_axes) == 0:\n                    if isinstance(length, int) and isinstance(known_product, int) and length != known_product:\n                        raise EinopsError('Shape mismatch, {} != {}'.format(length, known_product))\n                else:\n                    if isinstance(length, int) and isinstance(known_product, int) and length % known_product != 0:\n                        raise EinopsError(\"Shape mismatch, can't divide axis of length {} in chunks of {}\".format(\n                            length, known_product))\n\n                    unknown_axis, = unknown_axes\n                    axes_lengths[unknown_axis] = length // known_product\n\n        init_shapes = axes_lengths\n        reduced_axes_lengths = [dim for i, dim in enumerate(axes_lengths) if i not in self.reduced_elementary_axes]\n        final_shapes = []\n        for output_axis, grouping in enumerate(self.output_composite_axes):\n            if output_axis == self.ellipsis_positions[1]:\n                final_shapes.extend(ellipsis_shape)\n            else:\n                lengths = [reduced_axes_lengths[elementary_axis] for elementary_axis in grouping]\n                if any(l is None for l in lengths):\n                    final_shapes.append(None)\n                else:\n                    final_shapes.append(_product(lengths))\n        reduced_axes = self.reduced_elementary_axes\n        axes_reordering = self.final_axes_grouping_flat\n        if optimize:\n            return _optimize_transformation(init_shapes, reduced_axes, axes_reordering, final_shapes)\n        else:\n            return init_shapes, reduced_axes, axes_reordering, final_shapes", "response": "Reconstructs the internal state of the object from a shape."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a correct backend for a given tensor.", "response": "def get_backend(tensor):\n    \"\"\"\n    Takes a correct backend (e.g. numpy backend if tensor is numpy.ndarray) for a tensor.\n    If needed, imports package and creates backend\n    \"\"\"\n    for framework_name, backend in _backends.items():\n        if backend.is_appropriate_type(tensor):\n            return backend\n\n    for BackendSubclass in AbstractBackend.__subclasses__():\n        if _debug_importing:\n            print('Testing for subclass of ', BackendSubclass)\n        if BackendSubclass.framework_name not in _backends:\n            # check that module was already imported. Otherwise it can't be imported\n            if BackendSubclass.framework_name in sys.modules:\n                if _debug_importing:\n                    print('Imported backend for ', BackendSubclass.framework_name)\n                backend = BackendSubclass()\n                _backends[backend.framework_name] = backend\n                if backend.is_appropriate_type(tensor):\n                    return backend\n\n    raise RuntimeError('Tensor type unknown to einops {}'.format(type(tensor)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef xml2text(xml):\n    text = u''\n    root = ET.fromstring(xml)\n    for child in root.iter():\n        if child.tag == qn('w:t'):\n            t_text = child.text\n            text += t_text if t_text is not None else ''\n        elif child.tag == qn('w:tab'):\n            text += '\\t'\n        elif child.tag in (qn('w:br'), qn('w:cr')):\n            text += '\\n'\n        elif child.tag == qn(\"w:p\"):\n            text += '\\n\\n'\n    return text", "response": "Returns a string representing the textual content of this run with content\n    child elements translated to their Python\n    equivalent."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nshow an array of pixels on the window.", "response": "def show(self, frame):\n        \"\"\"\n        Show an array of pixels on the window.\n\n        Args:\n            frame (numpy.ndarray): the frame to show on the window\n\n        Returns:\n            None\n        \"\"\"\n        # check that the frame has the correct dimensions\n        if len(frame.shape) != 3:\n            raise ValueError('frame should have shape with only 3 dimensions')\n        # open the window if it isn't open already\n        if not self.is_open:\n            self.open()\n        # prepare the window for the next frame\n        self._window.clear()\n        self._window.switch_to()\n        self._window.dispatch_events()\n        # create an image data object\n        image = ImageData(\n            frame.shape[1],\n            frame.shape[0],\n            'RGB',\n            frame.tobytes(),\n            pitch=frame.shape[1]*-3\n        )\n        # send the image to the window\n        image.blit(0, 0, width=self._window.width, height=self._window.height)\n        self._window.flip()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef display_arr(screen, arr, video_size, transpose):\n    # take the transpose if necessary\n    if transpose:\n        pyg_img = pygame.surfarray.make_surface(arr.swapaxes(0, 1))\n    else:\n        pyg_img = arr\n    # resize the image according to given image size\n    pyg_img = pygame.transform.scale(pyg_img, video_size)\n    # blit the image to the surface\n    screen.blit(pyg_img, (0, 0))", "response": "Display an image to the pygame screen."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nplays the game using the keyboard as a human.", "response": "def play(env, transpose=True, fps=30, nop_=0):\n    \"\"\"Play the game using the keyboard as a human.\n\n    Args:\n        env (gym.Env): the environment to use for playing\n        transpose (bool): whether to transpose frame before viewing them\n        fps (int): number of steps of the environment to execute every second\n        nop_ (any): the object to use as a null op action for the environment\n\n    Returns:\n        None\n\n    \"\"\"\n    # ensure the observation space is a box of pixels\n    assert isinstance(env.observation_space, gym.spaces.box.Box)\n    # ensure the observation space is either B&W pixels or RGB Pixels\n    obs_s = env.observation_space\n    is_bw = len(obs_s.shape) == 2\n    is_rgb = len(obs_s.shape) == 3 and obs_s.shape[2] in [1, 3]\n    assert is_bw or is_rgb\n    # get the mapping of keyboard keys to actions in the environment\n    if hasattr(env, 'get_keys_to_action'):\n        keys_to_action = env.get_keys_to_action()\n    # get the mapping of keyboard keys to actions in the unwrapped environment\n    elif hasattr(env.unwrapped, 'get_keys_to_action'):\n        keys_to_action = env.unwrapped.get_keys_to_action()\n    else:\n        raise ValueError('env has no get_keys_to_action method')\n    relevant_keys = set(sum(map(list, keys_to_action.keys()), []))\n    # determine the size of the video in pixels\n    video_size = env.observation_space.shape[0], env.observation_space.shape[1]\n    if transpose:\n        video_size = tuple(reversed(video_size))\n    # generate variables to determine the running state of the game\n    pressed_keys = []\n    running = True\n    env_done = True\n    # setup the screen using pygame\n    flags = pygame.RESIZABLE | pygame.HWSURFACE | pygame.DOUBLEBUF\n    screen = pygame.display.set_mode(video_size, flags)\n    pygame.event.set_blocked(pygame.MOUSEMOTION)\n    # set the caption for the pygame window. if the env has a spec use its id\n    if env.spec is not None:\n        pygame.display.set_caption(env.spec.id)\n    # otherwise just use the default nes-py caption\n    else:\n        pygame.display.set_caption('nes-py')\n    # start a clock for limiting the frame rate to the given FPS\n    clock = pygame.time.Clock()\n    # start the main game loop\n    while running:\n        # reset if the environment is done\n        if env_done:\n            env_done = False\n            obs = env.reset()\n        # otherwise take a normal step\n        else:\n            # unwrap the action based on pressed relevant keys\n            action = keys_to_action.get(tuple(sorted(pressed_keys)), nop_)\n            obs, rew, env_done, info = env.step(action)\n        # make sure the observation exists\n        if obs is not None:\n            # if the observation is just height and width (B&W)\n            if len(obs.shape) == 2:\n                # add a dummy channel for pygame display\n                obs = obs[:, :, None]\n            # if the observation is single channel (B&W)\n            if obs.shape[2] == 1:\n                # repeat the single channel 3 times for RGB encoding of B&W\n                obs = obs.repeat(3, axis=2)\n            # display the observation on the pygame screen\n            display_arr(screen, obs, video_size, transpose)\n\n        # process keyboard events\n        for event in pygame.event.get():\n            # handle a key being pressed\n            if event.type == pygame.KEYDOWN:\n                # make sure the key is in the relevant key list\n                if event.key in relevant_keys:\n                    # add the key to pressed keys\n                    pressed_keys.append(event.key)\n                # ASCII code 27 is the \"escape\" key\n                elif event.key == 27:\n                    running = False\n                # handle the backup and reset functions\n                elif event.key == ord('e'):\n                    env.unwrapped._backup()\n                elif event.key == ord('r'):\n                    env.unwrapped._restore()\n            # handle a key being released\n            elif event.type == pygame.KEYUP:\n                # make sure the key is in the relevant key list\n                if event.key in relevant_keys:\n                    # remove the key from the pressed keys\n                    pressed_keys.remove(event.key)\n            # if the event is quit, set running to False\n            elif event.type == pygame.QUIT:\n                running = False\n\n        # flip the pygame screen\n        pygame.display.flip()\n        # throttle to maintain the framerate\n        clock.tick(fps)\n    # quite the pygame setup\n    pygame.quit()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef play_human(env):\n    # play the game and catch a potential keyboard interrupt\n    try:\n        play(env, fps=env.metadata['video.frames_per_second'])\n    except KeyboardInterrupt:\n        pass\n    # reset and close the environment\n    env.close()", "response": "Play the environment using keyboard as a human."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_keys_to_action(self):\n        # get the old mapping of keys to actions\n        old_keys_to_action = self.env.unwrapped.get_keys_to_action()\n        # invert the keys to action mapping to lookup key combos by action\n        action_to_keys = {v: k for k, v in old_keys_to_action.items()}\n        # create a new mapping of keys to actions\n        keys_to_action = {}\n        # iterate over the actions and their byte values in this mapper\n        for action, byte in self._action_map.items():\n            # get the keys to press for the action\n            keys = action_to_keys[byte]\n            # set the keys value in the dictionary to the current discrete act\n            keys_to_action[keys] = action\n\n        return keys_to_action", "response": "Return the dictionary of keyboard keys to actions."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_action_meanings(self):\n        actions = sorted(self._action_meanings.keys())\n        return [self._action_meanings[action] for action in actions]", "response": "Return a list of actions meanings."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef prg_rom(self):\n        try:\n            return self.raw_data[self.prg_rom_start:self.prg_rom_stop]\n        except IndexError:\n            raise ValueError('failed to read PRG-ROM on ROM.')", "response": "Return the PRG - ROM of the ROM file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef chr_rom(self):\n        try:\n            return self.raw_data[self.chr_rom_start:self.chr_rom_stop]\n        except IndexError:\n            raise ValueError('failed to read CHR-ROM on ROM.')", "response": "Return the CHR - ROM of the ROM file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _ram_buffer(self):\n        # get the address of the RAM\n        address = _LIB.Memory(self._env)\n        # create a buffer from the contents of the address location\n        buffer_ = ctypes.cast(address, ctypes.POINTER(RAM_VECTOR)).contents\n        # create a NumPy array from the buffer\n        return np.frombuffer(buffer_, dtype='uint8')", "response": "Setup the RAM buffer from the C ++ code."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfind the pointer to a controller and setup a NumPy buffer.", "response": "def _controller_buffer(self, port):\n        \"\"\"\n        Find the pointer to a controller and setup a NumPy buffer.\n\n        Args:\n            port: the port of the controller to setup\n\n        Returns:\n            a NumPy buffer with the controller's binary data\n\n        \"\"\"\n        # get the address of the controller\n        address = _LIB.Controller(self._env, port)\n        # create a memory buffer using the ctypes pointer for this vector\n        buffer_ = ctypes.cast(address, ctypes.POINTER(CONTROLLER_VECTOR)).contents\n        # create a NumPy buffer from the binary data and return it\n        return np.frombuffer(buffer_, dtype='uint8')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _frame_advance(self, action):\n        # set the action on the controller\n        self.controllers[0][:] = action\n        # perform a step on the emulator\n        _LIB.Step(self._env)", "response": "Advance a frame in the emulator with an action."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nresets the state of the environment and returns an initial observation.", "response": "def reset(self):\n        \"\"\"\n        Reset the state of the environment and returns an initial observation.\n\n        Returns:\n            state (np.ndarray): next frame as a result of the given action\n\n        \"\"\"\n        # call the before reset callback\n        self._will_reset()\n        # reset the emulator\n        if self._has_backup:\n            self._restore()\n        else:\n            _LIB.Reset(self._env)\n        # call the after reset callback\n        self._did_reset()\n        # set the done flag to false\n        self.done = False\n        # return the screen from the emulator\n        return self.screen"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrunning one frame of the NES and return the relevant observation data.", "response": "def step(self, action):\n        \"\"\"\n        Run one frame of the NES and return the relevant observation data.\n\n        Args:\n            action (byte): the bitmap determining which buttons to press\n\n        Returns:\n            a tuple of:\n            - state (np.ndarray): next frame as a result of the given action\n            - reward (float) : amount of reward returned after given action\n            - done (boolean): whether the episode has ended\n            - info (dict): contains auxiliary diagnostic information\n\n        \"\"\"\n        # if the environment is done, raise an error\n        if self.done:\n            raise ValueError('cannot step in a done environment! call `reset`')\n        # set the action on the controller\n        self.controllers[0][:] = action\n        # pass the action to the emulator as an unsigned byte\n        _LIB.Step(self._env)\n        # get the reward for this step\n        reward = self._get_reward()\n        # get the done flag for this step\n        self.done = self._get_done()\n        # get the info for this step\n        info = self._get_info()\n        # call the after step callback\n        self._did_step(self.done)\n        # bound the reward in [min, max]\n        if reward < self.reward_range[0]:\n            reward = self.reward_range[0]\n        elif reward > self.reward_range[1]:\n            reward = self.reward_range[1]\n        # return the screen from the emulator and other relevant data\n        return self.screen, reward, self.done, info"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrender the environment. Args: mode (str): the mode to render with: - human: render to the current display - rgb_array: Return an numpy.ndarray with shape (x, y, 3), representing RGB values for an x-by-y pixel image Returns: a numpy array if mode is 'rgb_array', None otherwise", "response": "def render(self, mode='human'):\n        \"\"\"\n        Render the environment.\n\n        Args:\n            mode (str): the mode to render with:\n            - human: render to the current display\n            - rgb_array: Return an numpy.ndarray with shape (x, y, 3),\n              representing RGB values for an x-by-y pixel image\n\n        Returns:\n            a numpy array if mode is 'rgb_array', None otherwise\n\n        \"\"\"\n        if mode == 'human':\n            # if the viewer isn't setup, import it and create one\n            if self.viewer is None:\n                from ._image_viewer import ImageViewer\n                # get the caption for the ImageViewer\n                if self.spec is None:\n                    # if there is no spec, just use the .nes filename\n                    caption = self._rom_path.split('/')[-1]\n                else:\n                    # set the caption to the OpenAI Gym id\n                    caption = self.spec.id\n                # create the ImageViewer to display frames\n                self.viewer = ImageViewer(\n                    caption=caption,\n                    height=SCREEN_HEIGHT,\n                    width=SCREEN_WIDTH,\n                )\n            # show the screen on the image viewer\n            self.viewer.show(self.screen)\n        elif mode == 'rgb_array':\n            return self.screen\n        else:\n            # unpack the modes as comma delineated strings ('a', 'b', ...)\n            render_modes = [repr(x) for x in self.metadata['render.modes']]\n            msg = 'valid render modes are: {}'.format(', '.join(render_modes))\n            raise NotImplementedError(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_keys_to_action(self):\n        # keyboard keys in an array ordered by their byte order in the bitmap\n        # i.e. right = 7, left = 6, ..., B = 1, A = 0\n        buttons = np.array([\n            ord('d'),  # right\n            ord('a'),  # left\n            ord('s'),  # down\n            ord('w'),  # up\n            ord('\\r'), # start\n            ord(' '),  # select\n            ord('p'),  # B\n            ord('o'),  # A\n        ])\n        # the dictionary of key presses to controller codes\n        keys_to_action = {}\n        # the combination map of values for the controller\n        values = 8 * [[0, 1]]\n        # iterate over all the combinations\n        for combination in itertools.product(*values):\n            # unpack the tuple of bits into an integer\n            byte = int(''.join(map(str, combination)), 2)\n            # unwrap the pressed buttons based on the bitmap\n            pressed = buttons[list(map(bool, combination))]\n            # assign the pressed buttons to the output byte\n            keys_to_action[tuple(sorted(pressed))] = byte\n\n        return keys_to_action", "response": "Return the dictionary of keyboard keys to actions."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_args():\n    parser = argparse.ArgumentParser(description=__doc__)\n    # add the argument for the Super Mario Bros environment to run\n    parser.add_argument('--rom', '-r',\n        type=str,\n        help='The path to the ROM to play.',\n        required=True,\n    )\n    # add the argument for the mode of execution as either human or random\n    parser.add_argument('--mode', '-m',\n        type=str,\n        default='human',\n        choices=['human', 'random'],\n        help='The execution mode for the emulation.',\n    )\n    # add the argument for the number of steps to take in random mode\n    parser.add_argument('--steps', '-s',\n        type=int,\n        default=500,\n        help='The number of random steps to take.',\n    )\n    return parser.parse_args()", "response": "Parse arguments from the command line and return them."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef main():\n    # get arguments from the command line\n    args = _get_args()\n    # create the environment\n    env = NESEnv(args.rom)\n    # play the environment with the given mode\n    if args.mode == 'human':\n        play_human(env)\n    else:\n        play_random(env, args.steps)", "response": "The main entry point for the command line interface."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nplays the environment making uniformly random decisions.", "response": "def play_random(env, steps):\n    \"\"\"\n    Play the environment making uniformly random decisions.\n\n    Args:\n        env (gym.Env): the initialized gym environment to play\n        steps (int): the number of random steps to take\n\n    Returns:\n        None\n\n    \"\"\"\n    try:\n        done = True\n        progress = tqdm(range(steps))\n        for _ in progress:\n            if done:\n                _ = env.reset()\n            action = env.action_space.sample()\n            _, reward, done, info = env.step(action)\n            progress.set_postfix(reward=reward, info=info)\n            env.render()\n    except KeyboardInterrupt:\n        pass\n    # close the environment\n    env.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef markowitz_portfolio(cov_mat, exp_rets, target_ret,\n                        allow_short=False, market_neutral=False):\n    \"\"\"\n    Computes a Markowitz portfolio.\n\n    Parameters\n    ----------\n    cov_mat: pandas.DataFrame\n        Covariance matrix of asset returns.\n    exp_rets: pandas.Series\n        Expected asset returns (often historical returns).\n    target_ret: float\n        Target return of portfolio.\n    allow_short: bool, optional\n        If 'False' construct a long-only portfolio.\n        If 'True' allow shorting, i.e. negative weights.\n    market_neutral: bool, optional\n        If 'False' sum of weights equals one.\n        If 'True' sum of weights equal zero, i.e. create a\n            market neutral portfolio (implies allow_short=True).\n            \n    Returns\n    -------\n    weights: pandas.Series\n        Optimal asset weights.\n    \"\"\"\n    if not isinstance(cov_mat, pd.DataFrame):\n        raise ValueError(\"Covariance matrix is not a DataFrame\")\n\n    if not isinstance(exp_rets, pd.Series):\n        raise ValueError(\"Expected returns is not a Series\")\n\n    if not isinstance(target_ret, float):\n        raise ValueError(\"Target return is not a float\")\n\n    if not cov_mat.index.equals(exp_rets.index):\n        raise ValueError(\"Indices do not match\")\n\n    if market_neutral and not allow_short:\n        warnings.warn(\"A market neutral portfolio implies shorting\")\n        allow_short=True\n\n    n = len(cov_mat)\n\n    P = opt.matrix(cov_mat.values)\n    q = opt.matrix(0.0, (n, 1))\n\n    # Constraints Gx <= h\n    if not allow_short:\n        # exp_rets*x >= target_ret and x >= 0\n        G = opt.matrix(np.vstack((-exp_rets.values,\n                                  -np.identity(n))))\n        h = opt.matrix(np.vstack((-target_ret,\n                                  +np.zeros((n, 1)))))\n    else:\n        # exp_rets*x >= target_ret\n        G = opt.matrix(-exp_rets.values).T\n        h = opt.matrix(-target_ret)\n\n    # Constraints Ax = b\n    # sum(x) = 1\n    A = opt.matrix(1.0, (1, n))\n\n    if not market_neutral:\n        b = opt.matrix(1.0)\n    else:\n        b = opt.matrix(0.0)\n\n    # Solve\n    optsolvers.options['show_progress'] = False\n    sol = optsolvers.qp(P, q, G, h, A, b)\n\n    if sol['status'] != 'optimal':\n        warnings.warn(\"Convergence problem\")\n\n    # Put weights into a labeled series\n    weights = pd.Series(sol['x'], index=cov_mat.index)\n    return weights", "response": "Returns a Markowitz portfolio."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef min_var_portfolio(cov_mat, allow_short=False):\n    if not isinstance(cov_mat, pd.DataFrame):\n        raise ValueError(\"Covariance matrix is not a DataFrame\")\n\n    n = len(cov_mat)\n\n    P = opt.matrix(cov_mat.values)\n    q = opt.matrix(0.0, (n, 1))\n\n    # Constraints Gx <= h\n    if not allow_short:\n        # x >= 0\n        G = opt.matrix(-np.identity(n))\n        h = opt.matrix(0.0, (n, 1))\n    else:\n        G = None\n        h = None\n\n    # Constraints Ax = b\n    # sum(x) = 1\n    A = opt.matrix(1.0, (1, n))\n    b = opt.matrix(1.0)\n\n    # Solve\n    optsolvers.options['show_progress'] = False\n    sol = optsolvers.qp(P, q, G, h, A, b)\n\n    if sol['status'] != 'optimal':\n        warnings.warn(\"Convergence problem\")\n\n    # Put weights into a labeled series\n    weights = pd.Series(sol['x'], index=cov_mat.index)\n    return weights", "response": "Computes the minimum variance portfolio."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef tangency_portfolio(cov_mat, exp_rets, allow_short=False):\n    if not isinstance(cov_mat, pd.DataFrame):\n        raise ValueError(\"Covariance matrix is not a DataFrame\")\n\n    if not isinstance(exp_rets, pd.Series):\n        raise ValueError(\"Expected returns is not a Series\")\n\n    if not cov_mat.index.equals(exp_rets.index):\n        raise ValueError(\"Indices do not match\")\n\n    n = len(cov_mat)\n\n    P = opt.matrix(cov_mat.values)\n    q = opt.matrix(0.0, (n, 1))\n\n    # Constraints Gx <= h\n    if not allow_short:\n        # exp_rets*x >= 1 and x >= 0\n        G = opt.matrix(np.vstack((-exp_rets.values,\n                                  -np.identity(n))))\n        h = opt.matrix(np.vstack((-1.0,\n                                  np.zeros((n, 1)))))\n    else:\n        # exp_rets*x >= 1\n        G = opt.matrix(-exp_rets.values).T\n        h = opt.matrix(-1.0)\n\n    # Solve\n    optsolvers.options['show_progress'] = False\n    sol = optsolvers.qp(P, q, G, h)\n\n    if sol['status'] != 'optimal':\n        warnings.warn(\"Convergence problem\")\n\n    # Put weights into a labeled series\n    weights = pd.Series(sol['x'], index=cov_mat.index)\n\n    # Rescale weights, so that sum(weights) = 1\n    weights /= weights.sum()\n    return weights", "response": "Returns a tangency portfolio for the given covariance matrix and expected asset returns."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef max_ret_portfolio(exp_rets):\n    if not isinstance(exp_rets, pd.Series):\n        raise ValueError(\"Expected returns is not a Series\")\n\n    weights = exp_rets[:]\n    weights[weights == weights.max()] = 1.0\n    weights[weights != weights.max()] = 0.0\n    weights /= weights.sum()\n\n    return weights", "response": "Computes a long - only maximum return portfolio."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ntruncates small weight vectors i. e. sets weights below a treshold to zero.", "response": "def truncate_weights(weights, min_weight=0.01, rescale=True):\n    \"\"\"\n    Truncates small weight vectors, i.e. sets weights below a treshold to zero.\n    This can be helpful to remove portfolio weights, which are negligibly small.\n    \n    Parameters\n    ----------\n    weights: pandas.Series\n        Optimal asset weights.\n    min_weight: float, optional\n        All weights, for which the absolute value is smaller\n        than this parameter will be set to zero.\n    rescale: boolean, optional\n        If 'True', rescale weights so that weights.sum() == 1.\n        If 'False', do not rescale.\n\n    Returns\n    -------\n    adj_weights: pandas.Series\n        Adjusted weights.\n    \"\"\"\n    if not isinstance(weights, pd.Series):\n        raise ValueError(\"Weight vector is not a Series\")\n\n    adj_weights = weights[:]\n    adj_weights[adj_weights.abs() < min_weight] = 0.0\n\n    if rescale:\n        if not adj_weights.sum():\n            raise ValueError(\"Cannot rescale weight vector as sum is not finite\")\n        \n        adj_weights /= adj_weights.sum()\n\n    return adj_weights"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nprinting information on expected portfolio performance.", "response": "def print_portfolio_info(returns, avg_rets, weights):\n    \"\"\"\n    Print information on expected portfolio performance.\n    \"\"\"\n    ret = (weights * avg_rets).sum()\n    std = (weights * returns).sum(1).std()\n    sharpe = ret / std\n    print(\"Optimal weights:\\n{}\\n\".format(weights))\n    print(\"Expected return:   {}\".format(ret))\n    print(\"Expected variance: {}\".format(std**2))\n    print(\"Expected Sharpe:   {}\".format(sharpe))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the default method in this module.", "response": "def main(self):\n        \"\"\"\n        Return the default method in this module.\n\n        :return: the default method in this module\n        :rtype: ``boa.code.method.Method``\n        \"\"\"\n\n        for m in self.methods:\n\n            if m.name in ['Main', 'main']:\n                return m\n\n        if len(self.methods):\n            return self.methods[0]\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef orderered_methods(self):\n\n        oms = []\n\n        self.methods.reverse()\n\n        if self.main:\n            oms = [self.main]\n\n        for m in self.methods:\n\n            if m == self.main:\n                continue\n            oms.append(m)\n\n        return oms", "response": "Returns a list of ordered methods in this module."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nlook up a method by its name from the module methods list.", "response": "def method_by_name(self, method_name):\n        \"\"\"\n        Look up a method by its name from the module ``methods`` list.\n        :param method_name: the name of the method to look up\n        :type method_name: str\n\n        :return: the method ( if it is found)\n        :rtype: ``boa.code.method.Method``\n        \"\"\"\n\n        #        print(\"METHODS FOR MODULE: %s \" % self.path)\n        for m in self.methods:\n            if m.full_name == method_name or m.name == method_name:\n                return m\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwrites all methods in the current module to a byte string.", "response": "def write_methods(self):\n        \"\"\"\n        Write all methods in the current module to a byte string.\n\n        :return: A bytestring of all current methods in this module\n        :rtype: bytes\n        \"\"\"\n\n        b_array = bytearray()\n        for key, vm_token in self.all_vm_tokens.items():\n\n            b_array.append(vm_token.out_op)\n\n            if vm_token.data is not None and vm_token.vm_op != VMOp.NOP:\n                b_array = b_array + vm_token.data\n\n#        self.to_s()\n        return b_array"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef link_methods(self):\n\n        from ..compiler import Compiler\n\n        for method in self.methods:\n            method.prepare()\n\n        self.all_vm_tokens = OrderedDict()\n\n        address = 0\n\n        for method in self.orderered_methods:\n\n            if not method.is_interop:\n                #                print(\"ADDING METHOD %s \" % method.full_name)\n                method.address = address\n\n                for key, vmtoken in method.vm_tokens.items():\n                    self.all_vm_tokens[address] = vmtoken\n                    address += 1\n\n                    if vmtoken.data is not None and vmtoken.vm_op != VMOp.NOP:\n                        address += len(vmtoken.data)\n\n                    vmtoken.addr = vmtoken.addr + method.address\n\n        for key, vmtoken in self.all_vm_tokens.items():\n            if vmtoken.src_method is not None:\n\n                target_method = self.method_by_name(vmtoken.target_method)\n                if target_method:\n                    jump_len = target_method.address - vmtoken.addr\n\n                    param_ret_counts = bytearray()\n                    if Compiler.instance().nep8:\n                        param_ret_counts = vmtoken.data[0:2]\n                        jump_len -= 2\n\n                    if jump_len > -32767 and jump_len < 32767:\n                        vmtoken.data = param_ret_counts + jump_len.to_bytes(2, 'little', signed=True)\n                    else:\n                        vmtoken.data = param_ret_counts + jump_len.to_bytes(4, 'little', signed=True)\n                else:\n                    raise Exception(\"Target method %s not found\" % vmtoken.target_method)", "response": "Link all methods in the order of their addresses."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_s(self):\n        # Initialize if needed\n        if self.all_vm_tokens is None:\n            self.link_methods()\n\n        lineno = 0\n        output = []\n        pstart = True\n\n        for i, (key, value) in enumerate(self.all_vm_tokens.items()):\n            if value.pytoken:\n                pt = value.pytoken\n                do_print_line_no = False\n                to_label = None\n                from_label = '    '\n\n                if pt.lineno != lineno:\n                    output.append(\"\\n\")\n                    lineno = pt.lineno\n                    do_print_line_no = True\n\n                ds = ''\n                if value.data is not None:\n                    try:\n                        ds = int.from_bytes(value.data, 'little', signed=True)\n                    except Exception as e:\n                        pass\n                    if type(ds) is not int and len(ds) < 1:\n                        try:\n                            ds = value.data.decode('utf-8')\n                        except Exception as e:\n                            pass\n\n                lno = \"{:<10}\".format(\n                    pt.lineno if do_print_line_no or pstart else '')\n                addr = \"{:<5}\".format(key)\n                op = \"{:<20}\".format(pt.instruction.name)\n\n                # If this is a number, it is likely a custom python opcode, get the name\n                if str(pt.pyop).isnumeric():\n                    opname = pyop.to_name(int(str(pt.pyop))).replace('HAVE_ARGUMENT', 'STORE_NAME').replace('YIELD_VALUE', 'REVERSE')\n                    if opname is not None:\n                        op = \"{:<20}\".format(opname)\n\n                arg = \"{:<50}\".format(pt.arg_str)\n                data = \"[data] {:<20}\".format(ds)\n                output.append(\"%s%s%s%s%s%s\" % (lno, from_label, addr, op, arg, data))\n\n            pstart = False\n\n        return \"\\n\".join(output)", "response": "This method writes the output of the executable in a readable tokenized format."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef export_debug(self, output_path):\n\n        file_hash = hashlib.md5(open(output_path, 'rb').read()).hexdigest()\n        avm_name = os.path.splitext(os.path.basename(output_path))[0]\n\n        json_data = self.generate_debug_json(avm_name, file_hash)\n\n        mapfilename = output_path.replace('.avm', '.debug.json')\n        with open(mapfilename, 'w+') as out_file:\n            out_file.write(json_data)", "response": "This method generates a debug map for NEO debugger"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading a. avm file and save it to a file.", "response": "def load_and_save(path, output_path=None, use_nep8=True):\n        \"\"\"\n        Call `load_and_save` to load a Python file to be compiled to the .avm format and save the result.\n        By default, the resultant .avm file is saved along side the source file.\n\n        :param path: The path of the Python file to compile\n        :param output_path: Optional path to save the compiled `.avm` file\n        :return: the instance of the compiler\n\n        The following returns the compiler object for inspection\n\n        .. code-block:: python\n\n            from boa.compiler import Compiler\n\n            Compiler.load_and_save('path/to/your/file.py')\n        \"\"\"\n\n        compiler = Compiler.load(os.path.abspath(path), use_nep8=use_nep8)\n        data = compiler.write()\n        if output_path is None:\n            fullpath = os.path.realpath(path)\n            path, filename = os.path.split(fullpath)\n            newfilename = filename.replace('.py', '.avm')\n            output_path = '%s/%s' % (path, newfilename)\n\n        Compiler.write_file(data, output_path)\n        compiler.entry_module.export_debug(output_path)\n\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load(path, use_nep8=True):\n\n        Compiler.__instance = None\n\n        compiler = Compiler.instance()\n        compiler.nep8 = use_nep8\n        compiler.entry_module = Module(path)\n\n        return compiler", "response": "Load a Python file to be compiled and not written to. avm\n\n           "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a list of integers from start to stop.", "response": "def range(start, stop):\n    \"\"\"\n    range(start, stop) -> list object\n\n    Return an list that is a a sequence of integers from start (inclusive)\n    to stop (exclusive).  range(i, j) produces i, i+1, i+2, ..., j-1.\n    \"\"\"\n\n    length = stop - start\n\n    out = list(length=length)\n\n    index = 0\n\n    orig_start = start\n\n    while start < stop:\n        val = index + orig_start\n        out[index] = val\n        index = index + 1\n        start = orig_start + index\n\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nbulking operation to directly add record references without making any additional requests", "response": "def add_record_references(self, app_id, record_id, field_id, target_record_ids):\n        \"\"\"Bulk operation to directly add record references without making any additional requests\n\n        Warnings:\n            Does not perform any app, record, or target app/record validation\n\n        Args:\n            app_id (str): Full App ID string\n            record_id (str): Full parent Record ID string\n            field_id (str): Full field ID to target reference field on parent Record string\n            target_record_ids (List(str)): List of full target reference Record ID strings\n        \"\"\"\n\n        self._swimlane.request(\n            'post',\n            'app/{0}/record/{1}/add-references'.format(app_id, record_id),\n            json={\n                'fieldId': field_id,\n                'targetRecordIds': target_record_ids\n            }\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _evaluate(self):\n\n        retrieved_records = SortedDict()\n\n        for record_id, record in six.iteritems(self._elements):\n            if record is self._field._unset:\n                # Record has not yet been retrieved, get it\n                try:\n                    record = self.target_app.records.get(id=record_id)\n                except SwimlaneHTTP400Error:\n                    # Record appears to be orphaned, don't include in set of elements\n                    logger.debug(\"Received 400 response retrieving record '{}', ignoring assumed orphaned record\")\n                    continue\n\n            retrieved_records[record_id] = record\n\n        self._elements = retrieved_records\n\n        return self._elements.values()", "response": "Scan for orphaned records and retrieve any records that have not already been grabbed"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add(self, record):\n        self._field.validate_value(record)\n        self._elements[record.id] = record\n        self._sync_field()", "response": "Add a reference to the provided record"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef remove(self, record):\n        self._field.validate_value(record)\n        del self._elements[record.id]\n        self._sync_field()", "response": "Removes a reference to the provided record"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef target_app(self):\n        if self.__target_app is None:\n            self.__target_app = self._swimlane.apps.get(id=self.__target_app_id)\n\n        return self.__target_app", "response": "Defer target app retrieval until requested"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef validate_value(self, value):\n        if value not in (None, self._unset):\n\n            super(ReferenceField, self).validate_value(value)\n\n            if value.app != self.target_app:\n                raise ValidationError(\n                    self.record,\n                    \"Reference field '{}' has target app '{}', cannot reference record '{}' from app '{}'\".format(\n                        self.name,\n                        self.target_app,\n                        value,\n                        value.app\n                    )\n                )", "response": "Validate that the provided record is a part of the appropriate target app for the field."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nstores record ids in separate location but ignore initial value", "response": "def set_swimlane(self, value):\n        \"\"\"Store record ids in separate location for later use, but ignore initial value\"\"\"\n\n        # Move single record into list to be handled the same by cursor class\n        if not self.multiselect:\n            if value and not isinstance(value, list):\n                value = [value]\n\n        # Values come in as a list of record ids or None\n        value = value or []\n\n        records = SortedDict()\n\n        for record_id in value:\n            records[record_id] = self._unset\n\n        return super(ReferenceField, self).set_swimlane(records)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nexpecting list of record instances convert to a SortedDict for internal representation", "response": "def set_python(self, value):\n        \"\"\"Expect list of record instances, convert to a SortedDict for internal representation\"\"\"\n        if not self.multiselect:\n            if value and not isinstance(value, list):\n                value = [value]\n\n        value = value or []\n\n        records = SortedDict()\n\n        for record in value:\n            self.validate_value(record)\n            records[record.id] = record\n\n        return_value = self._set(records)\n        self.record._raw['values'][self.id] = self.get_swimlane()\n\n        return return_value"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns list of record ids", "response": "def get_swimlane(self):\n        \"\"\"Return list of record ids\"\"\"\n        value = super(ReferenceField, self).get_swimlane()\n        if value:\n            ids = list(value.keys())\n\n            if self.multiselect:\n                return ids\n\n            return ids[0]\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_python(self):\n        cursor = super(ReferenceField, self).get_python()\n\n        if self.multiselect:\n            return cursor\n\n        else:\n            try:\n                return cursor[0]\n            except IndexError:\n                return None", "response": "Return cursor if multi - select direct value if single - select"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets single app by one of id or name.", "response": "def get(self, key, value):\n        \"\"\"Get single app by one of id or name\n\n        Supports resource cache\n\n        Keyword Args:\n            id (str): Full app id\n            name (str): App name\n\n        Returns:\n            App: Corresponding App resource instance\n\n        Raises:\n            TypeError: No or multiple keyword arguments provided\n            ValueError: No matching app found on server\n        \"\"\"\n        if key == 'id':\n            # Server returns 204 instead of 404 for a non-existent app id\n            response = self._swimlane.request('get', 'app/{}'.format(value))\n            if response.status_code == 204:\n                raise ValueError('No app with id \"{}\"'.format(value))\n\n            return App(\n                self._swimlane,\n                response.json()\n            )\n        else:\n            # Workaround for lack of support for get by name\n            # Holdover from previous driver support, to be fixed as part of 3.x\n            for app in self.list():\n                if value and value == app.name:\n                    return app\n\n            # No matching app found\n            raise ValueError('No app with name \"{}\"'.format(value))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving list of all apps", "response": "def list(self):\n        \"\"\"Retrieve list of all apps\n\n        Returns:\n            :class:`list` of :class:`~swimlane.core.resources.app.App`: List of all retrieved apps\n        \"\"\"\n        response = self._swimlane.request('get', 'app')\n        return [App(self._swimlane, item) for item in response.json()]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef users(self):\n        if self.__users is None:\n            self.__users = GroupUsersCursor(swimlane=self._swimlane, user_ids=self.__user_ids)\n        return self.__users", "response": "Returns a GroupUsersCursor with list of User instances for this Group\n       .. versionadded:: 2. 16. 2\n       "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _evaluate(self):\n        if self._elements:\n            for element in self._elements:\n                yield element\n        else:\n            for user_id in self.__user_ids:\n                element = self._swimlane.users.get(id=user_id)\n                self._elements.append(element)\n                yield element", "response": "Lazily retrieve and build User instances from returned data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a User instance with appropriate raw data parsed from login response content.", "response": "def _user_raw_from_login_content(login_content):\n    \"\"\"Returns a User instance with appropriate raw data parsed from login response content\"\"\"\n    matching_keys = [\n        'displayName',\n        'lastLogin',\n        'active',\n        'name',\n        'isMe',\n        'lastPasswordChangedDate',\n        'passwordResetRequired',\n        'groups',\n        'roles',\n        'email',\n        'isAdmin',\n        'createdDate',\n        'modifiedDate',\n        'createdByUser',\n        'modifiedByUser',\n        'userName',\n        'id',\n        'disabled'\n    ]\n\n    raw_data = {\n        '$type': User._type,\n    }\n\n    for key in matching_keys:\n        if key in login_content:\n            raw_data[key] = login_content[key]\n\n    return raw_data"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef __verify_auth_params(username, password, access_token):\n\n        if all(v is not None for v in [username, password, access_token]):\n            raise ValueError('Cannot supply a username/password and a access token')\n\n        if (username is None or password is None) and access_token is None:\n            raise ValueError('Must supply a username/password or access token')", "response": "Verify that the username password and access token are passed to the constructor."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nverify that the server version is lower than the package major version.", "response": "def __verify_server_version(self):\n        \"\"\"Verify connected to supported server product version\n\n        Notes:\n            Logs warning if connecting to a newer minor server version\n\n        Raises:\n            swimlane.exceptions.InvalidServerVersion: If server major version is higher than package major version\n        \"\"\"\n        if compare_versions('.'.join([_lib_major_version, _lib_minor_version]), self.product_version) > 0:\n            logger.warning('Client version {} connecting to server with newer minor release {}.'.format(\n                _lib_full_version,\n                self.product_version\n            ))\n\n        if compare_versions(_lib_major_version, self.product_version) != 0:\n            raise InvalidSwimlaneProductVersion(\n                self,\n                '{}.0'.format(_lib_major_version),\n                '{}.0'.format(str(int(_lib_major_version) + 1))\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef request(self, method, api_endpoint, **kwargs):\n        while api_endpoint.startswith('/'):\n            api_endpoint = api_endpoint[1:]\n\n        # Ensure a timeout is set\n        kwargs.setdefault('timeout', self._default_timeout)\n\n        # Manually grab and dump json data to have full control over serialization\n        # Emulate default requests behavior\n        json_data = kwargs.pop('json', None)\n        if json_data is not None:\n            headers = CaseInsensitiveDict(kwargs.get('headers', {}))\n            headers.setdefault('Content-Type', 'application/json')\n            kwargs['headers'] = headers\n\n            kwargs['data'] = json.dumps(json_data, sort_keys=True, separators=(',', ':'))\n\n        response = self._session.request(method, urljoin(str(self.host) + self._api_root, api_endpoint), **kwargs)\n\n        # Roll 400 errors up into SwimlaneHTTP400Errors with specific Swimlane error code support\n        try:\n            response.raise_for_status()\n        except requests.HTTPError as error:\n            if error.response.status_code == 400:\n                raise SwimlaneHTTP400Error(error)\n            else:\n                raise error\n\n        return response", "response": "Wrapper for requests. Session. request"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef settings(self):\n        if not self.__settings:\n            self.__settings = self.request('get', 'settings').json()\n        return self.__settings", "response": "Retrieve and cache settings from server"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsend login request and update User instance login headers and token expiration", "response": "def authenticate(self):\n        \"\"\"Send login request and update User instance, login headers, and token expiration\"\"\"\n\n        # Temporarily remove auth from Swimlane session for auth request to avoid recursive loop during login request\n        self._swimlane._session.auth = None\n        resp = self._swimlane.request(\n            'post',\n            'user/login',\n            json={\n                'userName': self._username,\n                'password': self._password\n            },\n        )\n        self._swimlane._session.auth = self\n\n        # Get JWT from response content\n        json_content = resp.json()\n        token = json_content.pop('token', None)\n\n        # Grab token expiration\n        token_data = jwt.decode(token, verify=False)\n        token_expiration = pendulum.from_timestamp(token_data['exp'])\n\n        headers = {\n            'Authorization': 'Bearer {}'.format(token)\n        }\n\n        # Create User instance for authenticating user from login response data\n        user = User(self._swimlane, _user_raw_from_login_content(json_content))\n\n        self._login_headers = headers\n        self.user = user\n        self._token_expiration = token_expiration"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _evaluate(self):\n        if self._elements:\n            for element in self._elements:\n                yield element\n        else:\n            for page in itertools.count():\n                raw_elements = self._retrieve_raw_elements(page)\n\n                for raw_element in raw_elements:\n                    element = self._parse_raw_element(raw_element)\n                    self._elements.append(element)\n                    yield element\n                    if self.__limit and len(self._elements) >= self.__limit:\n                        break\n\n                if any([\n                    len(raw_elements) < self.page_size,\n                    (self.__limit and len(self._elements) >= self.__limit)\n                ]):\n                    break", "response": "Lazily retrieve and paginate report results and build Record instances from returned data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nvalidating a new user or group value against any User or Group restrictions.", "response": "def validate_value(self, value):\n        \"\"\"Validate new user/group value against any User/Group restrictions\n\n        Attempts to resolve generic UserGroup instances if necessary to respect special \"Everyone\" group, and\n        \"All Users\" + \"All Groups\" options\n        \"\"\"\n        super(UserGroupField, self).validate_value(value)\n\n        if value is not None:\n            # Ignore validation if all users + groups are allowed\n            if self._show_all_groups and self._show_all_users:\n                return\n\n            # Try to directly check allowed ids against user/group id first to avoid having to resolve generic\n            # UserGroup with an additional request\n            if value.id in self._allowed_user_ids | self._allowed_group_ids:\n                return\n\n            # Resolve to check Users vs Groups separately\n            value = value.resolve()\n\n            if isinstance(value, User):\n                self._validate_user(value)\n            else:\n                self._validate_group(value)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _validate_user(self, user):\n        # All users allowed\n        if self._show_all_users:\n            return\n\n        # User specifically allowed\n        if user.id in self._allowed_user_ids:\n            return\n\n        # User allowed by group membership\n        user_member_group_ids = set([g['id'] for g in user._raw['groups']])\n        if user_member_group_ids & self._allowed_member_ids:\n            return\n\n        raise ValidationError(\n            self.record,\n            'User `{}` is not a valid selection for field `{}`'.format(\n                user,\n                self.name\n            )\n        )", "response": "Validate a User instance against allowed user IDs or membership in a group"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _validate_group(self, group):\n        # All groups allowed\n        if self._show_all_groups:\n            return\n\n        # Group specifically allowed\n        if group.id in self._allowed_group_ids:\n            return\n\n        # Group allowed by subgroup membership\n        for parent_group_id in self._allowed_subgroup_ids:\n            # Get each group, and check subgroup ids\n            parent_group = self._swimlane.groups.get(id=parent_group_id)\n            parent_group_child_ids = set([g['id'] for g in parent_group._raw['groups']])\n            if group.id in parent_group_child_ids:\n                return\n\n        raise ValidationError(\n            self.record,\n            'Group `{}` is not a valid selection for field `{}`'.format(\n                group,\n                self.name\n            )\n        )", "response": "Validate a Group instance against allowed group IDs or subgroup of a parent group."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cast_to_python(self, value):\n        # v2.x does not provide a distinction between users and groups at the field selection level, can only return\n        # UserGroup instances instead of specific User or Group instances\n        if value is not None:\n            value = UserGroup(self._swimlane, value)\n\n        return value", "response": "Convert JSON definition to UserGroup object"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncaching and return cursor_class instance", "response": "def cursor(self):\n        \"\"\"Cache and return cursor_class instance\"\"\"\n        if self._cursor is None:\n            # pylint: disable=not-callable\n            self._cursor = self.cursor_class(self, self.get_initial_elements())\n\n        return self._cursor"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef for_json(self):\n        cursor = super(CursorField, self).for_json()\n        if cursor is not None:\n            # Lambda called immediately, false-positive from pylint on closure scope warning\n            # pylint: disable=cell-var-from-loop\n            return [getattr(item, 'for_json', lambda: item)() for item in cursor]", "response": "Return list of all cursor items calling. for_json if available for best representations"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding new comment to record comment field", "response": "def comment(self, message):\n        \"\"\"Add new comment to record comment field\"\"\"\n        message = str(message)\n\n        sw_repr = {\n            '$type': 'Core.Models.Record.Comments, Core',\n            'createdByUser': self._record._swimlane.user.as_usergroup_selection(),\n            'createdDate': pendulum.now().to_rfc3339_string(),\n            'message': message\n        }\n\n        comment = Comment(self._swimlane, sw_repr)\n        self._elements.append(comment)\n\n        self._record._raw['comments'].setdefault(self._field.id, [])\n        self._record._raw['comments'][self._field.id].append(comment._raw)\n\n        return comment"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_recursive_subclasses(cls):\n    return cls.__subclasses__() + [g for s in cls.__subclasses__() for g in get_recursive_subclasses(s)]", "response": "Return list of all subclasses of a class including subclasses of direct subclasses"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef import_submodules(package):\n\n    if isinstance(package, str):\n        package = importlib.import_module(package)\n\n    results = {}\n\n    for _, full_name, is_pkg in pkgutil.walk_packages(package.__path__, package.__name__ + '.'):\n        results[full_name] = importlib.import_module(full_name)\n\n        if is_pkg:\n            results.update(import_submodules(full_name))\n\n    return results", "response": "Return list of imported module instances from beneath root_package"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_python(self):\n        value = super(DatetimeField, self).get_python()\n\n        if value is not None:\n            # Handle subtypes with matching Pendulum types\n            if self.input_type == self._type_time:\n                value = value.time()\n            if self.input_type == self._type_date:\n                value = value.date()\n\n        return value", "response": "Coerce to best date type representation for the field subtype"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncasts a value to a swimlane format", "response": "def cast_to_swimlane(self, value):\n        \"\"\"Return datetimes formatted as expected by API and timespans as millisecond epochs\"\"\"\n        if value is None:\n            return value\n\n        if self.input_type == self._type_interval:\n            return value.in_seconds() * 1000\n\n        return self.format_datetime(value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef for_json(self):\n        value = super(DatetimeField, self).for_json()\n\n        # Order of instance checks matters for proper inheritance checks\n        if isinstance(value, pendulum.Interval):\n            return value.in_seconds() * 1000\n        if isinstance(value, datetime):\n            return self.format_datetime(value)\n        if isinstance(value, pendulum.Time):\n            return str(value)\n        if isinstance(value, pendulum.Date):\n            return value.to_date_string()", "response": "Return ISO8601 string formats for datetime date and time values milliseconds for intervals"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a Report instance for the given app and report_name.", "response": "def report_factory(app, report_name, **kwargs):\n    \"\"\"Report instance factory populating boilerplate raw data\n\n    Args:\n        app (App): Swimlane App instance\n        report_name (str): Generated Report name\n\n    Keyword Args\n        **kwargs: Kwargs to pass to the Report class\n    \"\"\"\n    # pylint: disable=protected-access\n    created = pendulum.now().to_rfc3339_string()\n    user_model = app._swimlane.user.as_usergroup_selection()\n\n    return Report(\n        app,\n        {\n            \"$type\": Report._type,\n            \"groupBys\": [],\n            \"aggregates\": [],\n            \"applicationIds\": [app.id],\n            \"columns\": [],\n            \"sorts\": {\n                \"$type\": \"System.Collections.Generic.Dictionary`2\"\n                         \"[[System.String, mscorlib],\"\n                         \"[Core.Models.Search.SortTypes, Core]], mscorlib\",\n            },\n            \"filters\": [],\n            \"defaultSearchReport\": False,\n            \"allowed\": [],\n            \"permissions\": {\n                \"$type\": \"Core.Models.Security.PermissionMatrix, Core\"\n            },\n            \"createdDate\": created,\n            \"modifiedDate\": created,\n            \"createdByUser\": user_model,\n            \"modifiedByUser\": user_model,\n            \"id\": None,\n            \"name\": report_name,\n            \"disabled\": False,\n            \"keywords\": \"\"\n        },\n        **kwargs\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a filter to the report", "response": "def filter(self, field_name, operand, value):\n        \"\"\"Adds a filter to report\n\n        Notes:\n            All filters are currently AND'ed together\n\n        Args:\n            field_name (str): Target field name to filter on\n            operand (str): Operand used in comparison. See `swimlane.core.search` for options\n            value: Target value used in comparision\n        \"\"\"\n        if operand not in self._FILTER_OPERANDS:\n            raise ValueError('Operand must be one of {}'.format(', '.join(self._FILTER_OPERANDS)))\n\n        # Use temp Record instance for target app to translate values into expected API format\n        record_stub = record_factory(self._app)\n        field = record_stub.get_field(field_name)\n\n        self._raw['filters'].append({\n            \"fieldId\": field.id,\n            \"filterType\": operand,\n            \"value\": field.get_report(value)\n        })"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef list(self):\n        raw_reports = self._swimlane.request('get', \"reports?appId={}\".format(self._app.id)).json()\n        # Ignore StatsReports for now\n        return [Report(self._app, raw_report) for raw_report in raw_reports if raw_report['$type'] == Report._type]", "response": "Retrieve all reports for parent app\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get(self, report_id):\n        return Report(\n            self._app,\n            self._swimlane.request('get', \"reports/{0}\".format(report_id)).json()\n        )", "response": "Retrieve a specific report by ID."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get(self, key, value):\n        if key == 'id':\n            response = self._swimlane.request('get', 'groups/{}'.format(value))\n            return Group(self._swimlane, response.json())\n\n        else:\n            response = self._swimlane.request('get', 'groups/lookup?name={}'.format(value))\n            matched_groups = response.json()\n\n            for group_data in matched_groups:\n                if group_data.get('name') == value:\n                    return Group(self._swimlane, group_data)\n\n            raise ValueError('Unable to find group with name \"{}\"'.format(value))", "response": "Retrieve single group record by id or name"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretrieves single user record by id or display name.", "response": "def get(self, arg, value):\n        \"\"\"Retrieve single user record by id or username\n\n        Warnings:\n            User display names are not unique. If using `display_name`, method will fail if multiple Users are returned\n            with the same display name\n\n        Keyword Args:\n            id (str): Full User ID\n            display_name (str): User display name\n\n        Returns:\n            User: User instance matching provided inputs\n\n        Raises:\n            TypeError: Unexpected or more than one keyword argument provided\n            ValueError: No matching user found based on provided inputs, or multiple Users with same display name\n        \"\"\"\n        if arg == 'id':\n            response = self._swimlane.request('get', 'user/{}'.format(value))\n\n            try:\n                user_data = response.json()\n            except ValueError:\n                raise ValueError('Unable to find user with ID \"{}\"'.format(value))\n\n            return User(self._swimlane, user_data)\n\n        else:\n            response = self._swimlane.request('get', 'user/search?query={}'.format(quote_plus(value)))\n            matched_users = response.json()\n\n            # Display name not unique, fail if multiple users share the same target display name\n            target_matches = []\n\n            for user_data in matched_users:\n                user_display_name = user_data.get('displayName')\n                if user_display_name == value:\n                    target_matches.append(user_data)\n\n            # No matches\n            if not target_matches:\n                raise ValueError('Unable to find user with display name \"{}\"'.format(value))\n\n            # Multiple matches\n            if len(target_matches) > 1:\n                raise ValueError('Multiple users returned with display name \"{}\". Matching user IDs: {}'.format(\n                    value,\n                    ', '.join(['\"{}\"'.format(r['id']) for r in target_matches])\n                ))\n\n            return User(self._swimlane, target_matches[0])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndownloading the contents of the current file into a BytesIO stream", "response": "def download(self, chunk_size=1024):\n        \"\"\"Download attachment\n\n        Args:\n            chunk_size (int): Byte-size of chunked download request stream\n\n        Returns:\n            BytesIO: Stream ready for reading containing the attachment file contents\n        \"\"\"\n        stream = BytesIO()\n\n        response = self._swimlane.request(\n            'get',\n            'attachment/download/{}'.format(self.file_id),\n            stream=True\n        )\n\n        for chunk in response.iter_content(chunk_size):\n            stream.write(chunk)\n\n        stream.seek(0)\n\n        return stream"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving and populate Revision instances from history API endpoint", "response": "def _retrieve_revisions(self):\n        \"\"\"Retrieve and populate Revision instances from history API endpoint\"\"\"\n        response = self._swimlane.request(\n            'get',\n            'history',\n            params={\n                'type': 'Records',\n                'id': self._record.id\n            }\n        )\n\n        raw_revisions = response.json()\n\n        return [Revision(self._record, raw) for raw in raw_revisions]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nvalidates provided value is one of the valid options", "response": "def validate_value(self, value):\n        \"\"\"Validate provided value is one of the valid options\"\"\"\n        super(ValuesListField, self).validate_value(value)\n\n        if value is not None:\n            if value not in self.selection_to_id_map:\n                raise ValidationError(\n                    self.record,\n                    'Field \"{}\" invalid value \"{}\". Valid options: {}'.format(\n                        self.name,\n                        value,\n                        ', '.join(self.selection_to_id_map.keys())\n                    )\n                )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cast_to_report(self, value):\n        value = super(ValuesListField, self).cast_to_report(value)\n\n        if value:\n            return value['id']", "response": "Report format uses only the value s id"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef validate_filters_or_records(filters_or_records):\n    # If filters_or_records is empty, fail\n    if not filters_or_records:\n        raise ValueError('Must provide at least one filter tuples or Records')\n    # If filters_or_records is not list of Record or tuple, fail\n    if not isinstance(filters_or_records[0], (Record, tuple)):\n        raise ValueError('Cannot provide both filter tuples and Records')\n    # If filters_or_records is not list of either Record or only tuple, fail\n    _type = type(filters_or_records[0])\n    for item in filters_or_records:\n        if not isinstance(item, _type):\n            raise ValueError(\"Expected filter tuple or Record, received {0}\".format(item))\n\n    return _type", "response": "Validate that filters_or_records is a list of tuples or Records."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting a single record by id", "response": "def get(self, key, value):\n        \"\"\"Get a single record by id\n\n        Supports resource cache\n\n        .. versionchanged:: 2.17.0 \n            Added option to retrieve record by tracking_id\n\n        Keyword Args:\n            id (str): Full record ID\n            tracking_id (str): Record Tracking ID\n\n        Returns:\n            Record: Matching Record instance returned from API\n\n        Raises:\n            TypeError: No id argument provided\n        \"\"\"\n        if key == 'id':\n            response = self._swimlane.request('get', \"app/{0}/record/{1}\".format(self._app.id, value))\n            return Record(self._app, response.json())\n        if key == 'tracking_id':\n            response = self._swimlane.request('get', \"app/{0}/record/tracking/{1}\".format(self._app.id, value))\n            return Record(self._app, response.json())"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef search(self, *filters, **kwargs):\n        report = self._app.reports.build(\n            'search-' + random_string(8),\n            keywords=kwargs.pop('keywords', []),\n            limit=kwargs.pop('limit', Report.default_limit)\n        )\n\n        for filter_tuples in filters:\n            report.filter(*filter_tuples)\n\n        return list(report)", "response": "Shortcut to generate a new temporary search report using provided filters and return the resulting Records"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create(self, **fields):\n        new_record = record_factory(self._app, fields)\n\n        new_record.save()\n\n        return new_record", "response": "Create and return a new record in associated app and return the newly created Record instance"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate and validate multiple records in associated app", "response": "def bulk_create(self, *records):\n        \"\"\"Create and validate multiple records in associated app\n\n        Args:\n            *records (dict): One or more dicts of new record field names and values\n\n        Notes:\n            Requires Swimlane 2.15+\n\n            Validates like :meth:`create`, but only sends a single request to create all provided fields, and does not\n            return the newly created records\n\n            Any validation failures on any of the records will abort the batch creation, not creating any new records\n\n            Does not return the newly created records\n\n        Examples:\n            Create 3 new records with single request\n\n            ::\n\n                app.records.bulk_create(\n                    {'Field 1': 'value 1', ...},\n                    {'Field 1': 'value 2', ...},\n                    {'Field 1': 'value 3', ...}\n                )\n\n        Raises:\n            swimlane.exceptions.UnknownField: If any field in any new record cannot be found\n            swimlane.exceptions.ValidationError: If any field in any new record fails validation\n            TypeError: If no dict of fields was provided, or any provided argument is not a dict\n        \"\"\"\n\n        if not records:\n            raise TypeError('Must provide at least one record')\n\n        if any(not isinstance(r, dict) for r in records):\n            raise TypeError('New records must be provided as dicts')\n\n        # Create local records from factory for initial full validation\n        new_records = []\n\n        for record_data in records:\n            record = record_factory(self._app, record_data)\n            record.validate()\n\n            new_records.append(record)\n\n        self._swimlane.request(\n            'post',\n            'app/{}/record/batch'.format(self._app.id),\n            json=[r._raw for r in new_records]\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef bulk_modify(self, *filters_or_records, **kwargs):\n        values = kwargs.pop('values', None)\n\n        if kwargs:\n            raise ValueError('Unexpected arguments: {}'.format(kwargs))\n\n        if not values:\n            raise ValueError('Must provide \"values\" as keyword argument')\n\n        if not isinstance(values, dict):\n            raise ValueError(\"values parameter must be dict of {'field_name': 'update_value'} pairs\")\n\n        _type = validate_filters_or_records(filters_or_records)\n\n        request_payload = {}\n        record_stub = record_factory(self._app)\n\n        # build record_id list\n        if _type is Record:\n            request_payload['recordIds'] = [record.id for record in filters_or_records]\n\n        # build filters\n        else:\n            filters = []\n            for filter_tuples in filters_or_records:\n                field_name = record_stub.get_field(filter_tuples[0])\n                filters.append({\n                    \"fieldId\": field_name.id,\n                    \"filterType\": filter_tuples[1],\n                    \"value\": field_name.get_report(filter_tuples[2])\n                })\n            request_payload['filters'] = filters\n\n        # Ensure all values are wrapped in a bulk modification operation, defaulting to Replace if not provided for\n        # backwards compatibility\n        for field_name in list(values.keys()):\n            modification_operation = values[field_name]\n            if not isinstance(modification_operation, _BulkModificationOperation):\n                values[field_name] = Replace(modification_operation)\n\n        # build modifications\n        modifications = []\n        for field_name, modification_operation in values.items():\n            # Lookup target field\n            modification_field = record_stub.get_field(field_name)\n            if not modification_field.bulk_modify_support:\n                raise ValueError(\"Field '{}' of Type '{}', is not supported for bulk modify\".format(\n                    field_name,\n                    modification_field.__class__.__name__\n                ))\n\n            modifications.append({\n                \"fieldId\": {\n                    \"value\": modification_field.id,\n                    \"type\": \"id\"\n                },\n                \"value\": modification_field.get_bulk_modify(modification_operation.value),\n                \"type\": modification_operation.type\n            })\n        request_payload['modifications'] = modifications\n        response = self._swimlane.request('put', \"app/{0}/record/batch\".format(self._app.id), json=request_payload)\n\n        # Update records if instances were used to submit bulk modify request after request was successful\n        if _type is Record:\n            for record in filters_or_records:\n                for field_name, modification_operation in six.iteritems(values):\n                    record[field_name] = modification_operation.value\n\n        return response.text", "response": "Shortcut to bulk modify records by filter or records."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _validate_list(self, target):\n        # Check list length restrictions\n        min_items = self._field.field_definition.get('minItems')\n        max_items = self._field.field_definition.get('maxItems')\n\n        if min_items is not None:\n            if len(target) < min_items:\n                raise ValidationError(\n                    self._record,\n                    \"Field '{}' must have a minimum of {} item(s)\".format(self._field.name, min_items)\n                )\n\n        if max_items is not None:\n            if len(target) > max_items:\n                raise ValidationError(\n                    self._record,\n                    \"Field '{}' can only have a maximum of {} item(s)\".format(self._field.name, max_items)\n                )\n\n        # Individual item validation\n        for item in target:\n            self._validate_item(item)", "response": "Validate a list against field validation rules"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _validate_item(self, item):\n        if not isinstance(item, six.string_types):\n            raise ValidationError(\n                self._record,\n                \"Text list field items must be strings, not '{}'\".format(item.__class__)\n            )\n\n        words = item.split(' ')\n\n        item_length_type = self._field.field_definition.get('itemLengthType')\n        item_max_length = self._field.field_definition.get('itemMaxLength')\n        item_min_length = self._field.field_definition.get('itemMinLength')\n        if item_length_type is not None:\n            # Min/max word count\n            if item_length_type == 'words':\n                if item_max_length is not None:\n                    if len(words) > item_max_length:\n                        raise ValidationError(\n                            self._record,\n                            \"Field '{}' items cannot contain more than {} words\".format(\n                                self._field.name,\n                                item_max_length\n                            )\n                        )\n                if item_min_length is not None:\n                    if len(words) < item_min_length:\n                        raise ValidationError(\n                            self._record,\n                            \"Field '{}' items must contain at least {} words\".format(\n                                self._field.name,\n                                item_min_length\n                            )\n                        )\n\n            # Min/max char count of full item\n            else:\n                if item_max_length is not None:\n                    if len(item) > item_max_length:\n                        raise ValidationError(\n                            self._record,\n                            \"Field '{}' items cannot contain more than {} characters\".format(\n                                self._field.name,\n                                item_max_length\n                            )\n                        )\n                if item_min_length is not None:\n                    if len(item) < item_min_length:\n                        raise ValidationError(\n                            self._record,\n                            \"Field '{}' items must contain at least {} characters\".format(\n                                self._field.name,\n                                item_min_length\n                            )\n                        )", "response": "Validate the items in the text list field."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert from list of dicts with values to list of values Cache list items with their ID pairs to restore existing IDs to unmodified values to prevent workflow evaluating on each save", "response": "def set_swimlane(self, value):\n        \"\"\"Convert from list of dicts with values to list of values\n\n        Cache list items with their ID pairs to restore existing IDs to unmodified values to prevent workflow\n        evaluating on each save for any already existing values\n        \"\"\"\n        value = value or []\n\n        self._initial_value_to_ids_map = defaultdict(list)\n        for item in value:\n            self._initial_value_to_ids_map[item['value']].append(item['id'])\n\n        return super(ListField, self).set_swimlane([d['value'] for d in value])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nvalidating using cursor for consistency between direct set of values vs modification of cursor values", "response": "def set_python(self, value):\n        \"\"\"Validate using cursor for consistency between direct set of values vs modification of cursor values\"\"\"\n        if not isinstance(value, (list, type(None))):\n            raise ValidationError(\n                self.record,\n                \"Field '{}' must be set to a list, not '{}'\".format(\n                    self.name,\n                    value.__class__\n                )\n            )\n        value = value or []\n        self.cursor._validate_list(value)\n        return super(ListField, self).set_python(value)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrestoring swimlane format attempting to keep initial IDs for any previously existing values", "response": "def cast_to_swimlane(self, value):\n        \"\"\"Restore swimlane format, attempting to keep initial IDs for any previously existing values\"\"\"\n        value = super(ListField, self).cast_to_swimlane(value)\n\n        if not value:\n            return None\n\n        # Copy initial values to pop IDs out as each value is hydrated back to server format, without modifying initial\n        # cache of value -> list(ids) map\n        value_ids = deepcopy(self._initial_value_to_ids_map)\n\n        return [self._build_list_item(item, value_ids[item].pop(0) if value_ids[item] else None) for item in value]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbuild a dict with ID and type for API representation of item_value", "response": "def _build_list_item(self, item_value, id_=None):\n        \"\"\"Return a dict with ID and $type for API representation of value\n\n        Uses id_ param if provided, defaults to new random ID\n        \"\"\"\n        return {\n            '$type': self._type_map[self.input_type]['list_item_type'],\n            'id': id_ or SID.generate(),\n            'value': item_value\n        }"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds an element to the set of selected elementsProxy to internal set. add and sync field", "response": "def select(self, element):\n        \"\"\"Add an element to the set of selected elements\n        \n        Proxy to internal set.add and sync field\n        \"\"\"\n        self._field.validate_value(element)\n        self._elements.add(element)\n        self._sync_field()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_python(self):\n        if self.multiselect:\n            return super(MultiSelectField, self).get_python()\n\n        return self._get()", "response": "Only return cursor instance if configured for multiselect"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nhandle multi - select and single - select modes", "response": "def get_swimlane(self):\n        \"\"\"Handle multi-select and single-select modes\"\"\"\n        if self.multiselect:\n            value = self._get()\n            children = []\n            if value:\n                for child in value:\n                    children.append(self.cast_to_swimlane(child))\n\n                return children\n            return None\n        return super(MultiSelectField, self).get_swimlane()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_python(self, value):\n        if self.multiselect:\n            value = value or []\n            elements = []\n\n            for element in value:\n                self.validate_value(element)\n                elements.append(element)\n\n            value = elements\n        else:\n            self.validate_value(value)\n\n        self._set(value)", "response": "Override to remove key from raw data when empty to work with server 2. 16 + validation"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_swimlane(self, value):\n        if self.multiselect:\n            value = value or []\n            children = []\n\n            for child in value:\n                children.append(self.cast_to_python(child))\n\n            return self._set(children)\n\n        return super(MultiSelectField, self).set_swimlane(value)", "response": "Cast all multi - select elements to correct internal type like single - select mode"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nhandle multi - select vs single - select", "response": "def for_json(self):\n        \"\"\"Handle multi-select vs single-select\"\"\"\n\n        if self.multiselect:\n            return super(MultiSelectField, self).for_json()\n\n        value = self.get_python()\n        if hasattr(value, 'for_json'):\n            return value.for_json()\n\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef record_factory(app, fields=None):\n    # pylint: disable=line-too-long\n    record = Record(app, {\n        '$type': Record._type,\n        'isNew': True,\n        'applicationId': app.id,\n        'comments': {\n            '$type': 'System.Collections.Generic.Dictionary`2[[System.String, mscorlib],[System.Collections.Generic.List`1[[Core.Models.Record.Comments, Core]], mscorlib]], mscorlib'\n        },\n        'values': {\n            '$type': 'System.Collections.Generic.Dictionary`2[[System.String, mscorlib],[System.Object, mscorlib]], mscorlib'\n        }\n    })\n\n    fields = fields or {}\n\n    for name, value in six.iteritems(fields):\n        record[name] = value\n\n    # Pop off fields with None value to allow for saving empty fields\n    copy_raw = copy.copy(record._raw)\n    values_dict = {}\n    for key, value in six.iteritems(copy_raw['values']):\n        if value is not None:\n            values_dict[key] = value\n    record._raw['values'] = values_dict\n\n    return record", "response": "Returns a temporary Record instance for the given App and optional dict of fields and values."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_python(self, value):\n        \n        # hook exists to stringify before validation\n\n        # set to string if not string or unicode\n        if value is not None and not isinstance(value, self.supported_types) or isinstance(value, int):\n            value = str(value)\n        return super(TextField, self).set_python(value)", "response": "Set field internal value from the python representation of field value"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns direction of version relative to provided version sections", "response": "def compare_versions(version_a, version_b, zerofill=False):\n    \"\"\"Return direction of version relative to provided version sections\n\n    Args:\n        version_a (str): First version to compare\n        version_b (str): Second version to compare\n        zerofill (bool): If True, treat any missing version sections as 0, otherwise ignore section. Defaults to False\n\n    Returns:\n        int: 0 if equal, -1 if a > b, 1 if a < b\n\n    Examples:\n\n        If a is equal to b, return 0\n        If a is greater than b, return -1\n        If a is less than b, return 1\n\n        >>> compare_versions('2', '2') == 0\n        >>> compare_versions('2', '1') == -1\n        >>> compare_versions('2', '3') == 1\n\n        If zerofill is False (default), sections not included in both versions are ignored during comparison\n\n        >>> compare_versions('2.13.2', '2.13') == 0\n        >>> compare_versions('2.13.2-1234', '3') == 1\n\n        If zerofill is True, any sections in one version not included in other version are set to 0\n\n        >>> compare_versions('2.13.2', '2.13', True) == -1\n        >>> compare_versions('2.13.2-1234', '2.13.2', True) == -1\n        >>> compare_versions('2.13.2', '2.13.2', True) == 0\n    \"\"\"\n    a_sections = list((int(match) for match in re.findall(r'\\d+', version_a)))\n    b_sections = list((int(match) for match in re.findall(r'\\d+', version_b)))\n\n    if zerofill:\n        max_sections = max([len(a_sections), len(b_sections)])\n\n        a_sections += [0 for _ in range(max(max_sections - len(a_sections), 0))]\n        b_sections += [0 for _ in range(max(max_sections - len(b_sections), 0))]\n\n    else:\n        min_sections = min([len(a_sections), len(b_sections)])\n\n        a_sections = a_sections[:min_sections]\n        b_sections = b_sections[:min_sections]\n\n    return (b_sections > a_sections) - (b_sections < a_sections)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef requires_swimlane_version(min_version=None, max_version=None):\n\n    if min_version is None and max_version is None:\n        raise ValueError('Must provide either min_version, max_version, or both')\n\n    if min_version and max_version and compare_versions(min_version, max_version) < 0:\n        raise ValueError('min_version must be <= max_version ({}, {})'.format(min_version, max_version))\n\n    def decorator(func):\n\n        @functools.wraps(func)\n        def wrapper(self, *args, **kwargs):\n            swimlane = self._swimlane\n\n            if min_version and compare_versions(min_version, swimlane.build_version, True) < 0:\n                raise InvalidSwimlaneBuildVersion(swimlane, min_version, max_version)\n\n            if max_version and compare_versions(swimlane.build_version, max_version, True) < 0:\n                raise InvalidSwimlaneBuildVersion(swimlane, min_version, max_version)\n\n            return func(self, *args, **kwargs)\n\n        return wrapper\n\n    return decorator", "response": "Decorator for SwimlaneResolver methods verifying Swimlane server build version is within a given inclusive range"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_report(self, value):\n        if self.multiselect:\n            value = value or []\n            children = []\n\n            for child in value:\n                children.append(self.cast_to_report(child))\n\n            return children\n\n        return self.cast_to_report(value)", "response": "Return provided field Python value formatted for use in report filter"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn value in format for bulk modify", "response": "def get_bulk_modify(self, value):\n        \"\"\"Return value in format for bulk modify\"\"\"\n        if self.multiselect:\n            value = value or []\n            return [self.cast_to_bulk_modify(child) for child in value]\n\n        return self.cast_to_bulk_modify(value)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef validate_value(self, value):\n        if self.readonly:\n            raise ValidationError(self.record, \"Cannot set readonly field '{}'\".format(self.name))\n        if value not in (None, self._unset):\n            if self.supported_types and not isinstance(value, tuple(self.supported_types)):\n                raise ValidationError(self.record, \"Field '{}' expects one of {}, got '{}' instead\".format(\n                    self.name,\n                    ', '.join([repr(t.__name__) for t in self.supported_types]),\n                    type(value).__name__)\n                )", "response": "Validate value is an acceptable type during set_python operation"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _set(self, value):\n        self._value = value\n        self.record._raw['values'][self.id] = self.get_swimlane()", "response": "Default setter used for both representations unless overridden"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _build_field_type_map(base_class):\n    mapping = {}\n\n    for cls in _get_recursive_subclasses(base_class):\n        if cls.field_type:\n            if isinstance(cls.field_type, tuple):\n                for field_type in cls.field_type:\n                    mapping[field_type] = cls\n            elif isinstance(cls.field_type, _string_types):\n                mapping[cls.field_type] = cls\n            else:\n                raise ValueError('Field type must be str or tuple, cannot understand type \"{}\" on class \"{}\"'.format(\n                    type(cls.field_type),\n                    cls\n                ))\n\n    return mapping", "response": "Create mapping from all $type values to their respective Field classes"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef resolve_field_class(field_definition):\n    try:\n        return _FIELD_TYPE_MAP[field_definition['$type']]\n    except KeyError as error:\n        error.message = 'No field available to handle Swimlane $type \"{}\"'.format(field_definition)\n        raise", "response": "Return the most fitting of the given Swimlane field definition"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a usable cache lookup key for an already initialized resource", "response": "def get_cache_index_key(resource):\n    \"\"\"Return a usable cache lookup key for an already initialized resource\n\n    Args:\n        resource (APIResource|tuple): APIResource instance or 3-length tuple key returned from this function\n\n    Raises:\n        TypeError: If resource is not an APIResource instance or acceptable 3-length tuple cache key\n    \"\"\"\n    if isinstance(resource, APIResource):\n        attr, attr_value = list(resource.get_cache_index_keys().items())[0]\n        key = (type(resource), attr, attr_value)\n    else:\n        key = tuple(resource)\n\n    if len(key) != 3:\n        raise TypeError('Cache key must be tuple of (class, key, value), got `{!r}` instead'.format(key))\n\n    if not issubclass(key[0], APIResource):\n        raise TypeError('First value of cache key must be a subclass of APIResource, got `{!r}` instead'.format(key[0]))\n\n    return key"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cache(self, resource):\n        if not isinstance(resource, APIResource):\n            raise TypeError('Cannot cache `{!r}`, can only cache APIResource instances'.format(resource))\n\n        # Disable inserts to cache when disabled\n        if self.__cache_max_size == 0:\n            return\n\n        try:\n            cache_internal_key = resource.get_cache_internal_key()\n            cache_index_keys = resource.get_cache_index_keys().items()\n        except NotImplementedError:\n            logger.warning(\n                'Not caching `{!r}`, resource did not provide all necessary cache details'.format(resource)\n            )\n        else:\n            resource_type = type(resource)\n\n            for key, value in cache_index_keys:\n                self.__cache_index_key_map[(resource_type, key, value)] = cache_internal_key\n\n            self.__caches[resource_type][cache_internal_key] = resource\n\n            logger.debug('Cached `{!r}`'.format(resource))", "response": "Insert a resource instance into appropriate resource cache"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nclear cache for each provided APIResource class or all resources", "response": "def clear(self, *resource_types):\n        \"\"\"Clear cache for each provided APIResource class, or all resources if no classes are provided\"\"\"\n        resource_types = resource_types or tuple(self.__caches.keys())\n\n        for cls in resource_types:\n            # Clear and delete cache instances to guarantee no lingering references\n            self.__caches[cls].clear()\n            del self.__caches[cls]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupload a new attachment and add it to the current fields", "response": "def add(self, filename, stream, content_type=None):\n        \"\"\"Upload a new attachment, and add it to current fields raw data to be persisted on save\n        \n        Can optionally manually set the content_type, will be guessed by provided filename extension and default to \n        application/octet-stream if it cannot be guessed\n        \"\"\"\n        # Guess file Content-Type or default\n        content_type = content_type or mimetypes.guess_type(filename)[0] or 'application/octet-stream'\n\n        response = self._record._swimlane.request(\n            'post',\n            'attachment',\n            files={\n                'file': (filename, stream, content_type)\n            },\n        )\n\n        # Returns raw attachment data as list with single element\n        raw_attachment_data = response.json()[0]\n\n        attachment = Attachment(self._record._swimlane, raw_attachment_data)\n        self._elements.append(attachment)\n\n        self._sync_field()\n\n        return attachment"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\noverrides setter allow clearing cursor", "response": "def _set(self, value):\n        \"\"\"Override setter, allow clearing cursor\"\"\"\n        super(AttachmentsField, self)._set(value)\n        self._cursor = None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef verify_otp(request):\n    ctx = {}\n\n    if request.method == \"POST\":\n        verification_code = request.POST.get('verification_code')\n\n        if verification_code is None:\n            ctx['error_message'] = \"Missing verification code.\"\n        else:\n            otp_ = UserOTP.objects.get(user=request.user)\n            totp_ = totp.TOTP(otp_.secret_key)\n\n            is_verified = totp_.verify(verification_code)\n\n            if is_verified:\n                request.session['verfied_otp'] = True\n                response = redirect(request.POST.get(\"next\", settings.LOGIN_REDIRECT_URL))\n                return update_rmb_cookie(request, response)\n            ctx['error_message'] = \"Your code is expired or invalid.\"\n\n    ctx['next'] = request.GET.get('next', settings.LOGIN_REDIRECT_URL)\n    return render(request, 'django_mfa/login_verify.html', ctx, status=400)", "response": "Verify a OTP request"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef at(self, for_time, counter_offset=0):\n        if not isinstance(for_time, datetime.datetime):\n            for_time = datetime.datetime.fromtimestamp(int(for_time))\n        return self.generate_otp(self.timecode(for_time) + counter_offset)", "response": "Generate an OTP for a specific time."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nverify the OTP passed in against the current time OTP", "response": "def verify(self, otp, for_time=None, valid_window=0):\n        \"\"\"\n        Verifies the OTP passed in against the current time OTP\n        @param [String/Integer] otp the OTP to check against\n        @param [Integer] valid_window extends the validity to this many counter ticks before and after the current one\n        \"\"\"\n        if for_time is None:\n            for_time = datetime.datetime.now()\n\n        if valid_window:\n            for i in range(-valid_window, valid_window + 1):\n                if utils.strings_equal(str(otp), str(self.at(for_time, i))):\n                    return True\n            return False\n\n        return utils.strings_equal(str(otp), str(self.at(for_time)))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef provisioning_uri(self, name, issuer_name=None):\n        return utils.build_uri(self.secret, name, issuer_name=issuer_name)", "response": "Returns the provisioning URI for the OTP\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef build_uri(secret, name, initial_count=None, issuer_name=None):\n    # initial_count may be 0 as a valid param\n    is_initial_count_present = (initial_count is not None)\n\n    otp_type = 'hotp' if is_initial_count_present else 'totp'\n    base = 'otpauth://%s/' % otp_type\n\n    if issuer_name:\n        issuer_name = quote(issuer_name)\n        base += '%s:' % issuer_name\n\n    uri = '%(base)s%(name)s?secret=%(secret)s' % {\n        'name': quote(name, safe='@'),\n        'secret': secret,\n        'base': base,\n    }\n\n    if is_initial_count_present:\n        uri += '&counter=%s' % initial_count\n\n    if issuer_name:\n        uri += '&issuer=%s' % issuer_name\n\n    return uri", "response": "Builds the provisioning URI for the OTP."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn True if strings s1 and s2 are equal.", "response": "def strings_equal(s1, s2):\n    \"\"\"\n    Timing-attack resistant string comparison.\n\n    Normal comparison using == will short-circuit on the first mismatching\n    character. This avoids that by scanning the whole string, though we\n    still reveal to a timing attack whether the strings are the same\n    length.\n    \"\"\"\n    try:\n        s1 = unicodedata.normalize('NFKC', str(s1))\n        s2 = unicodedata.normalize('NFKC', str(s2))\n    except:\n        s1 = unicodedata.normalize('NFKC', unicode(s1))\n        s2 = unicodedata.normalize('NFKC', unicode(s2))\n    return compare_digest(s1, s2)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving the Python library directory path.", "response": "def GetPythonLibraryDirectoryPath():\n  \"\"\"Retrieves the Python library directory path.\"\"\"\n  path = sysconfig.get_python_lib(True)\n  _, _, path = path.rpartition(sysconfig.PREFIX)\n\n  if path.startswith(os.sep):\n    path = path[1:]\n\n  return path"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrun the build extension.", "response": "def run(self):\n    \"\"\"Runs the build extension.\"\"\"\n    compiler = new_compiler(compiler=self.compiler)\n    if compiler.compiler_type == \"msvc\":\n      self.define = [\n          (\"UNICODE\", \"\"),\n      ]\n\n    else:\n      command = \"sh configure --disable-shared-libs\"\n      output = self._RunCommand(command)\n\n      print_line = False\n      for line in output.split(\"\\n\"):\n        line = line.rstrip()\n        if line == \"configure:\":\n          print_line = True\n\n        if print_line:\n          print(line)\n\n      self.define = [\n          (\"HAVE_CONFIG_H\", \"\"),\n          (\"LOCALEDIR\", \"\\\"/usr/share/locale\\\"\"),\n      ]\n\n    build_ext.run(self)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nbuilding a source distribution ( sdist ) package.", "response": "def run(self):\n    \"\"\"Builds a source distribution (sdist) package.\"\"\"\n    if self.formats != [\"gztar\"]:\n      print(\"'setup.py sdist' unsupported format.\")\n      sys.exit(1)\n\n    if glob.glob(\"*.tar.gz\"):\n      print(\"'setup.py sdist' remove existing *.tar.gz files from \"\n            \"source directory.\")\n      sys.exit(1)\n\n    command = \"make dist\"\n    exit_code = subprocess.call(command, shell=True)\n    if exit_code != 0:\n      raise RuntimeError(\"Running: {0:s} failed.\".format(command))\n\n    if not os.path.exists(\"dist\"):\n      os.mkdir(\"dist\")\n\n    source_package_file = glob.glob(\"*.tar.gz\")[0]\n    source_package_prefix, _, source_package_suffix = (\n        source_package_file.partition(\"-\"))\n    sdist_package_file = \"{0:s}-python-{1:s}\".format(\n        source_package_prefix, source_package_suffix)\n    sdist_package_file = os.path.join(\"dist\", sdist_package_file)\n    os.rename(source_package_file, sdist_package_file)\n\n    # Create and add the PKG-INFO file to the source package.\n    with gzip.open(sdist_package_file, 'rb') as input_file:\n      with open(sdist_package_file[:-3], 'wb') as output_file:\n        shutil.copyfileobj(input_file, output_file)\n    os.remove(sdist_package_file)\n\n    self.distribution.metadata.write_pkg_info(\".\")\n    pkg_info_path = \"{0:s}-{1:s}/PKG-INFO\".format(\n        source_package_prefix, source_package_suffix[:-7])\n    with tarfile.open(sdist_package_file[:-3], \"a:\") as tar_file:\n      tar_file.add(\"PKG-INFO\", arcname=pkg_info_path)\n    os.remove(\"PKG-INFO\")\n\n    with open(sdist_package_file[:-3], 'rb') as input_file:\n      with gzip.open(sdist_package_file, 'wb') as output_file:\n        shutil.copyfileobj(input_file, output_file)\n    os.remove(sdist_package_file[:-3])\n\n    # Inform distutils what files were created.\n    dist_files = getattr(self.distribution, \"dist_files\", [])\n    dist_files.append((\"sdist\", \"\", sdist_package_file))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _ReadConfigureAc(self):\n    file_object = open(\"configure.ac\", \"rb\")\n    if not file_object:\n      raise IOError(\"Unable to open: configure.ac\")\n\n    found_ac_init = False\n    found_library_name = False\n    for line in file_object.readlines():\n      line = line.strip()\n      if found_library_name:\n        library_version = line[1:-2]\n        if sys.version_info[0] >= 3:\n          library_version = library_version.decode(\"ascii\")\n        self.library_version = library_version\n        break\n\n      elif found_ac_init:\n        library_name = line[1:-2]\n        if sys.version_info[0] >= 3:\n          library_name = library_name.decode(\"ascii\")\n        self.library_name = library_name\n        found_library_name = True\n\n      elif line.startswith(b\"AC_INIT\"):\n        found_ac_init = True\n\n    file_object.close()\n\n    if not self.library_name or not self.library_version:\n      raise RuntimeError(\n          \"Unable to find library name and version in: configure.ac\")", "response": "Reads the configure. ac file to initialize the project information."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _ReadMakefileAm(self):\n    if not self.library_name:\n      raise RuntimeError(\"Missing library name\")\n\n    file_object = open(\"Makefile.am\", \"rb\")\n    if not file_object:\n      raise IOError(\"Unable to open: Makefile.am\")\n\n    found_subdirs = False\n    for line in file_object.readlines():\n      line = line.strip()\n      if found_subdirs:\n        library_name, _, _ = line.partition(b\" \")\n        if sys.version_info[0] >= 3:\n          library_name = library_name.decode(\"ascii\")\n\n        self.include_directories.append(library_name)\n\n        if library_name.startswith(\"lib\"):\n          self.library_names.append(library_name)\n\n        if library_name == self.library_name:\n          break\n\n      elif line.startswith(b\"SUBDIRS\"):\n        found_subdirs = True\n\n    file_object.close()\n\n    if not self.include_directories or not self.library_names:\n      raise RuntimeError(\n          \"Unable to find include directories and library names in: \"\n          \"Makefile.am\")", "response": "Reads Makefile. am to initialize the project information."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompile the given source from ES6 to ES5 using Babeljs", "response": "def babel_compile(source, **kwargs):\n    \"\"\"Compiles the given ``source`` from ES6 to ES5 using Babeljs\"\"\"\n    presets = kwargs.get('presets')\n    if not presets:\n        kwargs['presets'] = [\"es2015\"]\n    with open(BABEL_COMPILER, 'rb') as babel_js:\n        return evaljs(\n            (babel_js.read().decode('utf-8'),\n             'var bres, res;'\n             'bres = Babel.transform(dukpy.es6code, dukpy.babel_options);',\n             'res = {map: bres.map, code: bres.code};'),\n            es6code=source,\n            babel_options=kwargs\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef coffee_compile(source):\n    with open(COFFEE_COMPILER, 'rb') as coffeescript_js:\n        return evaljs(\n            (coffeescript_js.read().decode('utf-8'),\n             'CoffeeScript.compile(dukpy.coffeecode)'),\n            coffeecode=source\n        )", "response": "Compiles the given source from CoffeeScript to JavaScript"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nregister a directory where to look for modules.", "response": "def register_path(self, path):\n        \"\"\"Registers a directory where to look for modules.\n\n        By default only modules relative to current path are found.\n        \"\"\"\n        self._paths.insert(0, os.path.abspath(path))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef lookup(self, module_name):\n        for search_path in self._paths:\n            module_path = os.path.join(search_path, module_name)\n            new_module_name, module_file = self._lookup(module_path, module_name)\n            if module_file:\n                return new_module_name, module_file\n        return None, None", "response": "Searches for a file providing given module."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the source code and normalized module id of the given module.", "response": "def load(self, module_name):\n        \"\"\"Returns source code and normalized module id of the given module.\n\n        Only supports source code files encoded as UTF-8\n        \"\"\"\n        module_name, path = self.lookup(module_name)\n        if path:\n            with open(path, 'rb') as f:\n                return module_name, f.read().decode('utf-8')\n        return None, None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef less_compile(source, options=None):\n    options = options or {}\n    res = NodeLikeInterpreter().evaljs(\n        ('var result = null;'\n         'var less = require(\"less/less-node\");',\n         'less.render(dukpy.lesscode, dukpy.lessoptions, function(error, output) {'\n         '  result = {\"error\": error, \"output\": output};'\n         '});'\n         'result;'),\n        lesscode=source,\n        lessoptions=options\n    )\n    if not res:\n        raise RuntimeError('Results or errors unavailable')\n\n    if res.get('error'):\n        raise LessCompilerError(res['error']['message'])\n\n    return res['output']['css']", "response": "Compiles the given source from LESS to CSS"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninstall a JavaScript package downloaded from npmjs. org.", "response": "def install_jspackage(package_name, version, modulesdir):\n    \"\"\"Installs a JavaScript package downloaded from npmjs.org.\n\n    For example to install React::\n\n        install_jspackage('react', '0.14.8', './node_modules')\n\n    To install last version provide `None` as the version.\n    \"\"\"\n    if not version:\n        version = ''\n\n    requirements = _resolve_dependencies(package_name, version)\n    print('Packages going to be installed: {0}'.format(', '.join(\n        '{0}->{1}'.format(*i) for i in requirements\n    )))\n\n    downloads = {}\n    for dependency_name, _, version_info in requirements:\n        try:\n            downloads[dependency_name] = version_info['dist']['tarball']\n        except KeyError:\n            raise JSPackageInstallError('Unable to detect a supported download url for package',\n                                        error_code=3)\n\n    for dependency_name, download_url in downloads.items():\n        tarball = BytesIO()\n        print('Fetching {0}'.format(download_url), end='')\n        with closing(urlopen(download_url)) as data:\n            chunk = data.read(1024)\n            while chunk:\n                print('.', end='')\n                tarball.write(chunk)\n                chunk = data.read(1024)\n        print('')\n\n        tarball.seek(0)\n        with closing(tarfile.open(fileobj=tarball)) as tb:\n            dest = os.path.join(modulesdir, dependency_name)\n            tmpdir = tempfile.mkdtemp()\n            try:\n                tb.extractall(tmpdir)\n                shutil.rmtree(os.path.abspath(dest), ignore_errors=True)\n                shutil.move(os.path.join(tmpdir, 'package'),\n                            os.path.abspath(dest))\n            finally:\n                shutil.rmtree(tmpdir)\n\n    print('Installing {0} in {1} Done!'.format(package_name, modulesdir))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nruns JavaScript code in the context of the interpreter.", "response": "def evaljs(self, code, **kwargs):\n        \"\"\"Runs JavaScript code in the context of the interpreter.\n\n        All arguments will be converted to plain javascript objects\n        through the JSON encoder and will be available in `dukpy`\n        global object.\n\n        Returns the last object on javascript stack.\n        \"\"\"\n        jsvars = json.dumps(kwargs)\n        jscode = self._adapt_code(code)\n\n        if not isinstance(jscode, bytes):\n            jscode = jscode.encode('utf-8')\n\n        if not isinstance(jsvars, bytes):\n            jsvars = jsvars.encode('utf-8')\n\n        res = _dukpy.eval_string(self, jscode, jsvars)\n        if res is None:\n            return None\n\n        return json.loads(res.decode('utf-8'))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncompiling the given source from TypeScript to ES5 using TypescriptServices. js", "response": "def typescript_compile(source):\n    \"\"\"Compiles the given ``source`` from TypeScript to ES5 using TypescriptServices.js\"\"\"\n    with open(TS_COMPILER, 'r') as tsservices_js:\n        return evaljs(\n            (tsservices_js.read(),\n             'ts.transpile(dukpy.tscode, {options});'.format(options=TSC_OPTIONS)),\n            tscode=source\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_private_file(self):\n        return PrivateFile(\n            request=self.request,\n            storage=self.get_storage(),\n            relative_name=self.get_path()\n        )", "response": "Returns all relevant data in a single object so this is easy to extend\n            and server implementations can pick what they need."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nhandling incoming GET requests", "response": "def get(self, request, *args, **kwargs):\n        \"\"\"\n        Handle incoming GET requests\n        \"\"\"\n        private_file = self.get_private_file()\n\n        if not self.can_access_file(private_file):\n            return HttpResponseForbidden('Private storage access denied')\n\n        if not private_file.exists():\n            return self.serve_file_not_found(private_file)\n        else:\n            return self.serve_file(private_file)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef serve_file(self, private_file):\n        response = self.server_class().serve(private_file)\n\n        if self.content_disposition:\n            # Join syntax works in all Python versions. Python 3 doesn't support b'..'.format(),\n            # and % formatting was added for bytes in 3.5: https://bugs.python.org/issue3982\n            filename = self.get_content_disposition_filename(private_file)\n            response['Content-Disposition'] = b'; '.join([\n                self.content_disposition.encode(), self._encode_filename_header(filename)\n            ])\n\n        return response", "response": "Serve the file that was retrieved from the storage."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_content_disposition_filename(self, private_file):\n        return self.content_disposition_filename or os.path.basename(private_file.relative_name)", "response": "Return the filename in the download header."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _encode_filename_header(self, filename):\n        # Based on https://www.djangosnippets.org/snippets/1710/\n        user_agent = self.request.META.get('HTTP_USER_AGENT', None)\n        if 'WebKit' in user_agent:\n            # Support available for UTF-8 encoded strings.\n            # This also matches Edgee.\n            return u'filename={}'.format(filename).encode(\"utf-8\")\n        elif 'MSIE' in user_agent:\n            # IE does not support RFC2231 for internationalized headers, but somehow\n            # percent-decodes it so this can be used instead. Note that using the word\n            # \"attachment\" anywhere in the filename overrides an inline content-disposition.\n            url_encoded = quote(filename.encode(\"utf-8\")).replace('attachment', \"a%74tachment\")\n            return \"filename={}\".format(url_encoded).encode(\"utf-8\")\n        else:\n            # For others like Firefox, we follow RFC2231 (encoding extension in HTTP headers).\n            rfc2231_filename = quote(filename.encode(\"utf-8\"))\n            return \"filename*=UTF-8''{}\".format(rfc2231_filename).encode(\"utf-8\")", "response": "Encode the filename to use in a Content - Disposition header."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef open(self, mode='rb'):\n        file = self.storage.open(self.relative_name, mode=mode)  # type: File\n        return file", "response": "Open the file for reading."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_no_cache_headers(func):\n\n    @wraps(func)\n    def _dec(*args, **kwargs):\n        response = func(*args, **kwargs)\n        response['Expires'] = 'Thu, 01 Jan 1970 00:00:00 GMT'  # HTTP 1.0 proxies\n        response['Cache-Control'] = 'max-age=0, no-cache, must-revalidate, proxy-revalidate'  # HTTP 1.1\n        return response\n\n    return _dec", "response": "Decorator that adds no cache headers to the response."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef readTrainingData(file_locations, GROUP_LABEL):\n    '''\n    Used in downstream tests\n    '''\n    class Mock(object):\n        pass\n    mock_module = Mock()\n    mock_module.PARENT_LABEL = GROUP_LABEL\n    for location in file_locations:\n        with open(location) as f:\n            tree = etree.parse(f)\n\n        xml = tree.getroot()\n        for each in data_prep_utils.TrainingData(xml, mock_module):\n            yield each", "response": "Reads training data from XML files."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef f1_with_flattening(estimator, X, y):\n    predicted = estimator.predict(X)\n    flat_pred, flat_gold = [], []\n    for a, b in zip(predicted, y):\n        if len(a) == len(b):\n            flat_pred.extend(a)\n            flat_gold.extend(b)\n    return f1_score(flat_gold, flat_pred)", "response": "Calculate F1 score by flattening predictions of the estimator across all sequences."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_data_sklearn_format(train_file_list, module):\n    data = list(readTrainingData(train_file_list, module.GROUP_LABEL))\n    random.shuffle(data)\n\n    x, y = [], []\n    for raw_string, components in data:\n        tokens, labels = zip(*components)\n        x.append(raw_string)\n        y.append(labels)\n    return x, y", "response": "Parses the specified data files and returns it in sklearn format."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncompares two models. Given a list of strings prints out tokens & tags whenever the two models parse a string differently.", "response": "def compareTaggers(model1, model2, string_list, module_name):\n    \"\"\"\n    Compare two models. Given a list of strings, prints out tokens & tags\n    whenever the two taggers parse a string differently. This is for spot-checking models\n    :param tagger1: a .crfsuite filename\n    :param tagger2: another .crfsuite filename\n    :param string_list: a list of strings to be checked\n    :param module_name: name of a parser module\n    \"\"\"\n    module = __import__(module_name)\n\n    tagger1 = pycrfsuite.Tagger()\n    tagger1.open(module_name+'/'+model1)\n    tagger2 = pycrfsuite.Tagger()\n    tagger2.open(module_name+'/'+model2)\n\n    count_discrepancies = 0\n\n    for string in string_list:\n        tokens = module.tokenize(string)\n        if tokens:\n            features = module.tokens2features(tokens)\n\n            tags1 = tagger1.tag(features)\n            tags2 = tagger2.tag(features)\n\n            if tags1 != tags2:\n                count_discrepancies += 1\n                print('\\n')\n                print(\"%s. %s\" %(count_discrepancies, string))\n                \n                print('-'*75)\n                print_spaced('token', model1, model2)\n                print('-'*75)\n                for token in zip(tokens, tags1, tags2):\n                    print_spaced(token[0], token[1], token[2])\n    print(\"\\n\\n%s of %s strings were labeled differently\"%(count_discrepancies, len(string_list)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsaves the current session data to the database.", "response": "def save(self, must_create=False):\n        \"\"\"\n        Saves the current session data to the database. If 'must_create' is\n        True, a database error will be raised if the saving operation doesn't\n        create a *new* entry (as opposed to possibly updating an existing\n        entry).\n        \"\"\"\n        obj = Session(\n            session_key=self._get_or_create_session_key(),\n            session_data=self.encode(self._get_session(no_load=must_create)),\n            expire_date=self.get_expiry_date(),\n            user_agent=self.user_agent,\n            user_id=self.user_id,\n            ip=self.ip,\n        )\n        using = router.db_for_write(Session, instance=obj)\n        try:\n            with transaction.atomic(using):\n                obj.save(force_insert=must_create, using=using)\n        except IntegrityError as e:\n            if must_create and 'session_key' in str(e):\n                raise CreateError\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntransform a User Agent into human readable text.", "response": "def device(value):\n    \"\"\"\n    Transform a User Agent into human readable text.\n\n    Example output:\n\n    * Safari on iPhone\n    * Chrome on Windows 8.1\n    * Safari on OS X\n    * Firefox\n    * Linux\n    * None\n    \"\"\"\n\n    browser = None\n    for regex, name in BROWSERS:\n        if regex.search(value):\n            browser = name\n            break\n\n    device = None\n    for regex, name in DEVICES:\n        if regex.search(value):\n            device = name\n            break\n\n    if browser and device:\n        return _('%(browser)s on %(device)s') % {\n            'browser': browser,\n            'device': device\n        }\n\n    if browser:\n        return browser\n\n    if device:\n        return device\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef location(value):\n    try:\n        location = geoip() and geoip().city(value)\n    except Exception:\n        try:\n            location = geoip() and geoip().country(value)\n        except Exception as e:\n            warnings.warn(str(e))\n            location = None\n    if location and location['country_name']:\n        if 'city' in location and location['city']:\n            return '{}, {}'.format(location['city'], location['country_name'])\n        return location['country_name']\n    return None", "response": "Transform an IP address into an approximate location."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrunning a function before the handler is invoked, is passed the event & context and must return an event & context too. Usage:: >>> # to create a reusable decorator >>> @before ... def print_request_id(event, context): ... print(context.aws_request_id) ... return event, context >>> @print_request_id ... def handler(event, context): ... pass >>> class Context: ... aws_request_id = 'ID!' >>> handler({}, Context()) ID! >>> # or a one off >>> @before(lambda e, c: (e['body'], c)) ... def handler(body, context): ... return body >>> handler({'body': 'BOOODYY'}, object()) 'BOOODYY'", "response": "def before(func):\n    \"\"\"\n    Run a function before the handler is invoked, is passed the event & context\n    and must return an event & context too.\n\n    Usage::\n\n        >>> # to create a reusable decorator\n        >>> @before\n        ... def print_request_id(event, context):\n        ...     print(context.aws_request_id)\n        ...     return event, context\n        >>> @print_request_id\n        ... def handler(event, context):\n        ...     pass\n        >>> class Context:\n        ...     aws_request_id = 'ID!'\n        >>> handler({}, Context())\n        ID!\n        >>> # or a one off\n        >>> @before(lambda e, c: (e['body'], c))\n        ... def handler(body, context):\n        ...     return body\n        >>> handler({'body': 'BOOODYY'}, object())\n        'BOOODYY'\n    \"\"\"\n    class BeforeDecorator(LambdaDecorator):\n        def before(self, event, context):\n            return func(event, context)\n\n    return BeforeDecorator"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrunning a function after the handler is invoked, is passed the response and must return an response too. Usage:: >>> # to create a reusable decorator >>> @after ... def gnu_terry_pratchett(retval): ... retval.setdefault('Headers', {})['X-Clacks-Overhead'] = 'GNU Terry Pratchett' ... return retval >>> @gnu_terry_pratchett ... def handler(event, context): ... return {'body': ''} >>> handler({}, object()) {'body': '', 'Headers': {'X-Clacks-Overhead': 'GNU Terry Pratchett'}}", "response": "def after(func):\n    \"\"\"\n    Run a function after the handler is invoked, is passed the response\n    and must return an response too.\n\n    Usage::\n\n        >>> # to create a reusable decorator\n        >>> @after\n        ... def gnu_terry_pratchett(retval):\n        ...     retval.setdefault('Headers', {})['X-Clacks-Overhead'] = 'GNU Terry Pratchett'\n        ...     return retval\n        >>> @gnu_terry_pratchett\n        ... def handler(event, context):\n        ...     return {'body': ''}\n        >>> handler({}, object())\n        {'body': '', 'Headers': {'X-Clacks-Overhead': 'GNU Terry Pratchett'}}\n    \"\"\"\n    class AfterDecorator(LambdaDecorator):\n        def after(self, retval):\n            return func(retval)\n\n    return AfterDecorator"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef on_exception(func):\n    class OnExceptionDecorator(LambdaDecorator):\n        def on_exception(self, exception):\n            return func(exception)\n\n    return OnExceptionDecorator", "response": "Decorator to wrap a function when a handler thows an exception. It s return value is\n    returned to AWS."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef async_handler(handler):\n    @wraps(handler)\n    def wrapper(event, context):\n        context.loop = asyncio.get_event_loop()\n        return context.loop.run_until_complete(handler(event, context))\n\n    return wrapper", "response": "A decorator that allows for use of async handlers by automatically running them in an event loop."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cors_headers(handler_or_origin=None, origin=None, credentials=False):\n    if isinstance(handler_or_origin, str) and origin is not None:\n        raise TypeError('You cannot include any positonal arguments when using'\n                        ' the `origin` keyword argument')\n    if isinstance(handler_or_origin, str) or origin is not None:\n        def wrapper_wrapper(handler):\n            @wraps(handler)\n            def wrapper(event, context):\n                response = handler(event, context)\n                headers = response.setdefault('headers', {})\n                if origin is not None:\n                    headers['Access-Control-Allow-Origin'] = origin\n                else:\n                    headers['Access-Control-Allow-Origin'] = handler_or_origin\n                if credentials:\n                    headers['Access-Control-Allow-Credentials'] = True\n                return response\n\n            return wrapper\n\n        return wrapper_wrapper\n    elif handler_or_origin is None:\n        return cors_headers('*', credentials=credentials)\n    else:\n        return cors_headers('*')(handler_or_origin)", "response": "Decorator for adding CORS headers to http\nresponses."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef dump_json_body(handler):\n    @wraps(handler)\n    def wrapper(event, context):\n        response = handler(event, context)\n        if 'body' in response:\n            try:\n                response['body'] = json.dumps(response['body'])\n            except Exception as exception:\n                return {'statusCode': 500, 'body': str(exception)}\n        return response\n    return wrapper", "response": "Decorator that dumps the response body with json. dumps."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_json_body(handler):\n    @wraps(handler)\n    def wrapper(event, context):\n        if isinstance(event.get('body'), str):\n            try:\n                event['body'] = json.loads(event['body'])\n            except:\n                return {'statusCode': 400, 'body': 'BAD REQUEST'}\n        return handler(event, context)\n\n    return wrapper", "response": "Decorator to load event bodies with json. loads."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef json_schema_validator(request_schema=None, response_schema=None):\n    def wrapper_wrapper(handler):\n        @wraps(handler)\n        def wrapper(event, context):\n            if request_schema is not None:\n                if jsonschema is None:\n                    logger.error('jsonschema is not installed, skipping request validation')\n                else:\n                    try:\n                        jsonschema.validate(event, request_schema)\n                    except jsonschema.ValidationError as exception:\n                        return {'statusCode': 400,\n                                'body': 'RequestValidationError: {}'.format(\n                                    exception.message)}\n            response = handler(event, context)\n            if response_schema is not None:\n                if jsonschema is None:\n                    logger.error('jsonschema is not installed, skipping response validation')\n                else:\n                    try:\n                        jsonschema.validate(response, response_schema)\n                    except jsonschema.ValidationError as exception:\n                        return {'statusCode': 500,\n                                'body': 'ResponseValidationError: {}'.format(\n                                    exception.message)}\n            return response\n\n        return wrapper\n\n    return wrapper_wrapper", "response": "Validate your request & response payloads against a JSONSchema."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef no_retry_on_failure(handler):\n    seen_request_ids = set()\n\n    @wraps(handler)\n    def wrapper(event, context):\n        if context.aws_request_id in seen_request_ids:\n            logger.critical('Retry attempt on request id %s detected.',\n                            context.aws_request_id)\n            return {'statusCode': 200}\n        seen_request_ids.add(context.aws_request_id)\n        return handler(event, context)\n\n    return wrapper", "response": "Decorator that does not retry on failure."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ssm_parameter_store(*parameters):\n    # support old list as 1st argument invocation style\n    if len(parameters) == 1 and not isinstance(parameters[0], basestring):\n        parameters = parameters[0]\n\n    def wrapper_wrapper(handler):\n        @wraps(handler)\n        def wrapper(event, context):\n            ssm = boto3.client('ssm')\n            if not hasattr(context, 'parameters'):\n                context.parameters = {}\n            for parameter in ssm.get_parameters(Names=parameters, WithDecryption=True)['Parameters']:\n                context.parameters[parameter['Name']] = parameter['Value']\n\n            return handler(event, context)\n\n        return wrapper\n\n    return wrapper_wrapper", "response": "Decorator to add parameters to the SSM Parameter Store."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef secrets_manager(*secret_names):\n    def wrapper_wrapper(handler):\n        @wraps(handler)\n        def wrapper(event, context):\n            if not hasattr(context, 'secrets'):\n                context.secrets = {}\n            for secret_name in secret_names:\n                secret_value = boto3.client(service_name='secretsmanager').get_secret_value(SecretId=secret_name)\n                if 'SecretString' in secret_value:\n                    context.secrets[secret_name] = json.loads(secret_value['SecretString'])\n                else:\n                    context.secrets[secret_name] = secret_value['SecretBinary']\n\n            return handler(event, context)\n\n        return wrapper\n\n    return wrapper_wrapper", "response": "Decorator for secrets_manager decorator."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmimicing praw. Submission and praw. Comment API", "response": "def _wrap_thing(self, thing, kind):\n        \"\"\"Mimic praw.Submission and praw.Comment API\"\"\"\n        thing['created'] = self._epoch_utc_to_local(thing['created_utc'])\n        thing['d_'] = copy.deepcopy(thing)\n        ThingType = namedtuple(kind, thing.keys())\n        thing = ThingType(**thing)\n        return thing"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding limit and created_utc arguments to the payload as necessary.", "response": "def _add_nec_args(self, payload):\n        \"\"\"Adds 'limit' and 'created_utc' arguments to the payload as necessary.\"\"\"\n        if self._limited(payload):\n            # Do nothing I guess? Not sure how paging works on this endpoint...\n            return\n        if 'limit' not in payload:\n            payload['limit'] = self.max_results_per_request\n        if 'sort' not in payload:\n            # Getting weird results if this is not made explicit. Unclear why.\n            payload['sort'] = 'desc'\n        if 'filter' in payload: #and payload.get('created_utc', None) is None:\n            if not isinstance(payload['filter'], list):\n                if isinstance(payload['filter'], str):\n                    payload['filter'] = [payload['filter']]\n                else:\n                    payload['filter'] = list(payload['filter'])\n            if 'created_utc' not in payload['filter']:\n                payload['filter'].append('created_utc')"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprettifies path for logging.", "response": "def pretty_path(path):\n    \"\"\" Prettify path for logging.\n    \"\"\"\n    path = fmt.to_utf8(path)\n    home_dir = os.path.expanduser(\"~\")\n    if path.startswith(home_dir):\n        path = \"~\" + path[len(home_dir):]\n    return '\"%s\"' % (path,)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_options(self):\n        super(RtorrentMove, self).add_options()\n\n        # basic options\n        self.add_bool_option(\"-n\", \"--dry-run\",\n            help=\"don't move data, just tell what would happen\")\n        self.add_bool_option(\"-F\", \"--force-incomplete\",\n            help=\"force a move of incomplete data\")", "response": "Add command line options for the RtorrentMove command."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef resolve_slashed(self, path):\n        if path.endswith(os.sep):\n            path = path.rstrip(os.sep)\n            if os.path.islink(path):\n                real = os.path.realpath(path)\n                self.LOG.debug('Resolved \"%s/\" to \"%s\"' % (path, real))\n                path = real\n\n        return path", "response": "Resolve symlinked directories if they end in a slash."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef guarded(self, call, *args):\n        self.LOG.debug('%s(%s)' % (\n            call.__name__, ', '.join([pretty_path(i) for i in args]),\n        ))\n        if not self.options.dry_run:\n            try:\n                call(*args)\n            except (EnvironmentError, UnicodeError) as exc:\n                self.fatal('%s(%s) failed [%s]' % (\n                    call.__name__, ', '.join([pretty_path(i) for i in args]), exc,\n                ))", "response": "Catch exceptions thrown by filesystem calls and don t really\n            execute them in dry - run mode."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nperforming some error - checked XMLRPC calls.", "response": "def _make_it_so(self, command, calls, *args, **kwargs):\n        \"\"\" Perform some error-checked XMLRPC calls.\n        \"\"\"\n        observer = kwargs.pop('observer', False)\n        args = (self._fields[\"hash\"],) + args\n        try:\n            for call in calls:\n                self._engine.LOG.debug(\"%s%s torrent #%s (%s)\" % (\n                    command[0].upper(), command[1:], self._fields[\"hash\"], call))\n                if call.startswith(':') or call[:2].endswith('.'):\n                    namespace = self._engine._rpc\n                else:\n                    namespace = self._engine._rpc.d\n                result = getattr(namespace, call.lstrip(':'))(*args)\n                if observer:\n                    observer(result)\n        except xmlrpc.ERRORS as exc:\n            raise error.EngineError(\"While %s torrent #%s: %s\" % (command, self._fields[\"hash\"], exc))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_files(self, attrs=None):\n        try:\n            # Get info for all files\n            f_multicall = self._engine._rpc.f.multicall\n            f_params = [self._fields[\"hash\"], 0,\n                \"f.path=\", \"f.size_bytes=\", \"f.last_touched=\",\n                \"f.priority=\", \"f.is_created=\", \"f.is_open=\",\n            ]\n            for attr in (attrs or []):\n                f_params.append(\"f.%s=\" % attr)\n            rpc_result = f_multicall(*tuple(f_params))\n        except xmlrpc.ERRORS as exc:\n            raise error.EngineError(\"While %s torrent #%s: %s\" % (\n                \"getting files for\", self._fields[\"hash\"], exc))\n        else:\n            #self._engine.LOG.debug(\"files result: %r\" % rpc_result)\n\n            # Return results\n            result = [Bunch(\n                path=i[0], size=i[1], mtime=i[2] / 1000000.0,\n                prio=i[3], created=i[4], opened=i[5],\n            ) for i in rpc_result]\n\n            if attrs:\n                for idx, attr in enumerate(attrs):\n                    if attr.startswith(\"get_\"):\n                        attr = attr[4:]\n                    for item, rpc_item in zip(result, rpc_result):\n                        item[attr] = rpc_item[6+idx]\n\n            return result", "response": "Get a list of all files in this download ; each entry has the\n            attributes C { path size mtime and prio."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncache a stable expensive-to-get item value for later (optimized) retrieval.", "response": "def _memoize(self, name, getter, *args, **kwargs):\n        \"\"\" Cache a stable expensive-to-get item value for later (optimized) retrieval.\n        \"\"\"\n        field = \"custom_m_\" + name\n        cached = self.fetch(field)\n        if cached:\n            value = cached\n        else:\n            value = getter(*args, **kwargs)\n            self._make_it_so(\"caching %s=%r for\" % (name, value,), [\"custom.set\"], field[7:], value)\n            self._fields[field] = value\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_kind(self, limit):\n        histo = self.fetch(\"custom_kind\")\n\n        if histo:\n            # Parse histogram from cached field\n            histo = [i.split(\"%_\") for i in str(histo).split()]\n            histo = [(int(val, 10), ext) for val, ext in histo]\n            ##self._engine.LOG.debug(\"~~~~~~~~~~ cached histo = %r\" % histo)\n        else:\n            # Get filetypes\n            histo = traits.get_filetypes(self.fetch(\"files\"),\n                path=operator.attrgetter(\"path\"), size=operator.attrgetter(\"size\"))\n\n            # Set custom cache field with value formatted like \"80%_flac 20%_jpg\" (sorted by percentage)\n            histo_str = ' '.join((\"%d%%_%s\" % i).replace(' ', '_') for i in histo)\n            self._make_it_so(\"setting kind cache %r on\" % (histo_str,), [\"custom.set\"], \"kind\", histo_str)\n            self._fields[\"custom_kind\"] = histo_str\n\n        # Return all non-empty extensions that make up at least <limit>% of total size\n        return set(ext for val, ext in histo if ext and val >= limit)", "response": "Get a set of dominant file types."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfetches a single on - demand field on demand.", "response": "def fetch(self, name, engine_name=None):\n        \"\"\" Get a field on demand.\n        \"\"\"\n        # TODO: Get each on-demand field in a multicall for all other items, since\n        # we likely need it anyway; another (more easy) way would be to pre-fetch dynamically\n        # with the list of fields from filters and output formats\n        try:\n            return self._fields[name]\n        except KeyError:\n            if isinstance(name, (int, long)):\n                name = \"custom_%d\" % name\n\n            if name == \"done\":\n                val = float(self.fetch(\"completed_chunks\")) / self.fetch(\"size_chunks\")\n            elif name == \"files\":\n                val = self._get_files()\n            elif name.startswith(\"kind_\") and name[5:].isdigit():\n                val = self._get_kind(int(name[5:], 10))\n            elif name.startswith(\"custom_\"):\n                key = name[7:]\n                try:\n                    if len(key) == 1 and key in \"12345\":\n                        val = getattr(self._engine._rpc.d, \"custom\"+key)(self._fields[\"hash\"])\n                    else:\n                        val = self._engine._rpc.d.custom(self._fields[\"hash\"], key)\n                except xmlrpc.ERRORS as exc:\n                    raise error.EngineError(\"While accessing field %r: %s\" % (name, exc))\n            else:\n                getter_name = engine_name if engine_name else RtorrentEngine.PYRO2RT_MAPPING.get(name, name)\n                if getter_name[0] == '=':\n                    getter_name = getter_name[1:]\n                else:\n                    getter_name = \"get_\" + getter_name\n                getter = getattr(self._engine._rpc.d, getter_name)\n\n                try:\n                    val = getter(self._fields[\"hash\"])\n                except xmlrpc.ERRORS as exc:\n                    raise error.EngineError(\"While accessing field %r: %s\" % (name, exc))\n\n            # TODO: Currently, NOT caching makes no sense; in a demon, it does!\n            #if isinstance(FieldDefinition.FIELDS.get(name), engine.ConstantField):\n            self._fields[name] = val\n\n            return val"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef datapath(self):\n        path = self._fields['path']\n        if not path:  # stopped item with no base_dir?\n            path = self.fetch('directory')\n            if path and not self._fields['is_multi_file']:\n                path = os.path.join(path, self._fields['name'])\n        return os.path.expanduser(fmt.to_unicode(path))", "response": "Get an item s data path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef announce_urls(self, default=[]):  # pylint: disable=dangerous-default-value\n        try:\n            response = self._engine._rpc.t.multicall(self._fields[\"hash\"], 0, \"t.url=\", \"t.is_enabled=\")\n        except xmlrpc.ERRORS as exc:\n            raise error.EngineError(\"While getting announce URLs for #%s: %s\" % (self._fields[\"hash\"], exc))\n\n        if response:\n            return [i[0] for i in response if i[1]]\n        else:\n            return default", "response": "Get a list of all announce URLs."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding or remove tags.", "response": "def tag(self, tags):\n        \"\"\" Add or remove tags.\n        \"\"\"\n        # Get tag list and add/remove given tags\n        tags = tags.lower()\n        previous = self.tagged\n        tagset = previous.copy()\n        for tag in tags.replace(',', ' ').split():\n            if tag.startswith('-'):\n                tagset.discard(tag[1:])\n            elif tag.startswith('+'):\n                tagset.add(tag[1:])\n            else:\n                tagset.add(tag)\n\n        # Write back new tagset, if changed\n        tagset.discard('')\n        if tagset != previous:\n            tagset = ' '.join(sorted(tagset))\n            self._make_it_so(\"setting tags %r on\" % (tagset,), [\"custom.set\"], \"tags\", tagset)\n            self._fields[\"custom_tags\"] = tagset"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nassign to throttle group.", "response": "def set_throttle(self, name):\n        \"\"\" Assign to throttle group.\n        \"\"\"\n        if name.lower() == \"null\":\n            name = \"NULL\"\n        if name.lower() == \"none\":\n            name = ''\n\n        if name not in self._engine.known_throttle_names:\n            if self._engine._rpc.throttle.up.max(xmlrpc.NOHASH, name) == -1:\n                if self._engine._rpc.throttle.down.max(xmlrpc.NOHASH, name) == -1:\n                    raise error.UserError(\"Unknown throttle name '{}'\".format(name))\n            self._engine.known_throttle_names.add(name)\n\n        if (name or \"NONE\") == self.throttle:\n            self._engine.LOG.debug(\"Keeping throttle %r on torrent #%s\" % (self.throttle, self._fields[\"hash\"]))\n            return\n\n        active = self.is_active\n        if active:\n            self._engine.LOG.debug(\"Torrent #%s stopped for throttling\" % (self._fields[\"hash\"],))\n            self.stop()\n        self._make_it_so(\"setting throttle %r on\" % (name,), [\"throttle_name.set\"], name)\n        if active:\n            self._engine.LOG.debug(\"Torrent #%s restarted after throttling\" % (self._fields[\"hash\"],))\n            self.start()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_custom(self, key, value=None):\n        # Split combined key/value\n        if value is None:\n            try:\n                key, value = key.split('=', 1)\n            except (ValueError, TypeError) as exc:\n                raise error.UserError(\"Bad custom field assignment %r, probably missing a '=' (%s)\" % (key, exc))\n\n        # Check identifier rules\n        if not key:\n            raise error.UserError(\"Custom field name cannot be empty!\")\n        elif len(key) == 1 and key in \"12345\":\n            method, args = \"custom\"+key+\".set\", (value,)\n        elif not (key[0].isalpha() and key.replace(\"_\", \"\").isalnum()):\n            raise error.UserError(\"Bad custom field name %r (must only contain a-z, A-Z, 0-9 and _)\" % (key,))\n        else:\n            method, args = \"custom.set\", (key, value)\n\n        # Make the assignment\n        self._make_it_so(\"setting custom_%s = %r on\" % (key, value), [method], *args)\n        self._fields[\"custom_\"+key] = value", "response": "Set a custom value."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nexecutes XMLRPC commands on the current object.", "response": "def execute(self, commands):\n        \"\"\" Execute XMLRPC command(s).\n        \"\"\"\n        try:\n            commands = [i.strip() for i in commands.split(' ; ')]\n        except (TypeError, AttributeError):\n            pass # assume an iterable\n\n        for command in commands:\n            try:\n                method, args = command.split('=', 1)\n                args = tuple(CommaLexer(args))\n            except (ValueError, TypeError) as exc:\n                raise error.UserError(\"Bad command %r, probably missing a '=' (%s)\" % (command, exc))\n\n            def print_result(data):\n                \"Helper to print XMLRPC call results\"\n                args_list = ''\n                if args:\n                    args_list = '\"' + '\",\"'.join(args) + '\"'\n                print('%s\\t%s\\t%s=%s' % (self._fields[\"hash\"], data, method.lstrip(':'), args_list))\n\n            observer = print_result if method.startswith('>') else None\n            method = method.lstrip('>')\n            if not (method.startswith(':') or method[:2].endswith('.')):\n                method = 'd.' + method\n            self._make_it_so(\"executing command on\", [method], *args, observer=observer)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndeletes PARTIAL data files and remove torrent from client.", "response": "def purge(self):\n        \"\"\" Delete PARTIAL data files and remove torrent from client.\n        \"\"\"\n        def partial_file(item):\n            \"Filter out partial files\"\n            #print \"???\", repr(item)\n            return item.completed_chunks < item.size_chunks\n\n        self.cull(file_filter=partial_file, attrs=[\"get_completed_chunks\", \"get_size_chunks\"])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndeleting all files and directories and return a list of attributes.", "response": "def cull(self, file_filter=None, attrs=None):\n        \"\"\" Delete ALL data files and remove torrent from client.\n\n            @param file_filter: Optional callable for selecting a subset of all files.\n                The callable gets a file item as described for RtorrentItem._get_files\n                and must return True for items eligible for deletion.\n            @param attrs: Optional list of additional attributes to fetch for a filter.\n        \"\"\"\n        dry_run = 0 # set to 1 for testing\n\n        def remove_with_links(path):\n            \"Remove a path including any symlink chains leading to it.\"\n            rm_paths = []\n            while os.path.islink(path):\n                target = os.readlink(path)\n                rm_paths.append(path)\n                path = target\n\n            if os.path.exists(path):\n                rm_paths.append(path)\n            else:\n                self._engine.LOG.debug(\"Real path '%s' doesn't exist,\"\n                    \" but %d symlink(s) leading to it will be deleted...\" % (path, len(rm_paths)))\n\n            # Remove the link chain, starting at the real path\n            # (this prevents losing the chain when there's permission problems)\n            for rm_path in reversed(rm_paths):\n                is_dir = os.path.isdir(rm_path) and not os.path.islink(rm_path)\n                self._engine.LOG.debug(\"Deleting '%s%s'\" % (rm_path, '/' if is_dir else ''))\n                if not dry_run:\n                    try:\n                        (os.rmdir if is_dir else os.remove)(rm_path)\n                    except OSError as exc:\n                        if exc.errno == errno.ENOENT:\n                            # Seems this disappeared somehow inbetween (race condition)\n                            self._engine.LOG.info(\"Path '%s%s' disappeared before it could be deleted\"\n                                                  % (rm_path, '/' if is_dir else ''))\n                        else:\n                            raise\n\n            return rm_paths\n\n        # Assemble doomed files and directories\n        files, dirs = set(), set()\n        base_path = os.path.expanduser(self.directory)\n        item_files = list(self._get_files(attrs=attrs))\n\n        if not self.directory:\n            raise error.EngineError(\"Directory for item #%s is empty,\"\n                \" you might want to add a filter 'directory=!'\" % (self._fields[\"hash\"],))\n        if not os.path.isabs(base_path):\n            raise error.EngineError(\"Directory '%s' for item #%s is not absolute, which is a bad idea;\"\n                \" fix your .rtorrent.rc, and use 'directory.default.set = /...'\"\n                % (self.directory, self._fields[\"hash\"],))\n        if self.fetch(\"=is_multi_file\") and os.path.isdir(self.directory):\n            dirs.add(self.directory)\n\n        for item_file in item_files:\n            if file_filter and not file_filter(item_file):\n                continue\n            #print repr(item_file)\n            path = os.path.join(base_path, item_file.path)\n            files.add(path)\n            if '/' in item_file.path:\n                dirs.add(os.path.dirname(path))\n\n        # Delete selected files\n        if not dry_run:\n            self.stop()\n        for path in sorted(files):\n            ##self._engine.LOG.debug(\"Deleting file '%s'\" % (path,))\n            remove_with_links(path)\n\n        # Prune empty directories (longer paths first)\n        doomed = files | dirs\n        for path in sorted(dirs, reverse=True):\n            residue = set(os.listdir(path) if os.path.exists(path) else [])\n            ignorable = set(i for i in residue\n                if any(fnmatch.fnmatch(i, pat) for pat in config.waif_pattern_list)\n                #or os.path.join(path, i) in doomed\n            )\n            ##print \"---\", residue - ignorable\n            if residue and residue != ignorable:\n                self._engine.LOG.info(\"Keeping non-empty directory '%s' with %d %s%s!\" % (\n                    path, len(residue),\n                    \"entry\" if len(residue) == 1 else \"entries\",\n                    (\" (%d ignorable)\" % len(ignorable)) if ignorable else \"\",\n                ))\n            else:\n                ##print \"---\", ignorable\n                for waif in ignorable:# - doomed:\n                    waif = os.path.join(path, waif)\n                    self._engine.LOG.debug(\"Deleting waif '%s'\" % (waif,))\n                    if not dry_run:\n                        try:\n                            os.remove(waif)\n                        except EnvironmentError as exc:\n                            self._engine.LOG.warn(\"Problem deleting waif '%s' (%s)\" % (waif, exc))\n\n                ##self._engine.LOG.debug(\"Deleting empty directory '%s'\" % (path,))\n                doomed.update(remove_with_links(path))\n\n        # Delete item from engine\n        if not dry_run:\n            self.delete()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nloading the configuration file and return the rTorrent s config object.", "response": "def load_config(self, namespace=None, rcfile=None):\n        \"\"\" Load file given in \"rcfile\".\n        \"\"\"\n        if namespace is None:\n            namespace = config\n        if namespace.scgi_url:\n            return  # already have the connection to rTorrent\n\n        # Get and check config file name\n        if not rcfile:\n            rcfile = getattr(config, \"rtorrent_rc\", None)\n        if not rcfile:\n            raise error.UserError(\"No 'rtorrent_rc' path defined in configuration!\")\n        if not os.path.isfile(rcfile):\n            raise error.UserError(\"Config file %r doesn't exist!\" % (rcfile,))\n\n        # Parse the file\n        self.LOG.debug(\"Loading rtorrent config from %r\" % (rcfile,))\n        rc_vals = Bunch(scgi_local='', scgi_port = '')\n        with open(rcfile) as handle:\n            continued = False\n            for line in handle.readlines():\n                # Skip comments, continuations, and empty lines\n                line = line.strip()\n                continued, was_continued = line.endswith('\\\\'), continued\n                if not line or was_continued or line.startswith(\"#\"):\n                    continue\n\n                # Be lenient about errors, after all it's not our own config file\n                try:\n                    key, val = line.split(\"=\", 1)\n                except ValueError:\n                    self.LOG.warning(\"Ignored invalid line %r in %r!\" % (line, rcfile))\n                    continue\n                key, val = key.strip(), val.strip()\n                key = self.RTORRENT_RC_ALIASES.get(key, key).replace('.', '_')\n\n                # Copy values we're interested in\n                if key in self.RTORRENT_RC_KEYS:\n                    self.LOG.debug(\"rtorrent.rc: %s = %s\" % (key, val))\n                    rc_vals[key] = val\n\n        # Validate fields\n        if rc_vals.scgi_local:\n            rc_vals.scgi_local = os.path.expanduser(rc_vals.scgi_local)\n            if rc_vals.scgi_local.startswith('/'):\n                rc_vals.scgi_local = \"scgi://\" + rc_vals.scgi_local\n        if rc_vals.scgi_port and not rc_vals.scgi_port.startswith(\"scgi://\"):\n            rc_vals.scgi_port = \"scgi://\" + rc_vals.scgi_port\n\n        # Prefer UNIX domain sockets over TCP sockets\n        namespace.scgi_url = rc_vals.scgi_local or rc_vals.scgi_port"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _resolve_viewname(self, viewname):\n        if viewname == \"-\":\n            try:\n                # Only works with rTorrent-PS at this time!\n                viewname = self.open().ui.current_view()\n            except xmlrpc.ERRORS as exc:\n                raise error.EngineError(\"Can't get name of current view: %s\" % (exc))\n\n        return viewname", "response": "Resolve the viewname of the current rTorrent."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nquerying the given fields of items in the given view.", "response": "def multicall(self, viewname, fields):\n        \"\"\" Query the given fields of items in the given view.\n\n            The result list contains named tuples,\n            so you can access the fields directly by their name.\n        \"\"\"\n        commands = tuple('d.{}='.format(x) for x in fields)\n        result_type = namedtuple('DownloadItem', [x.replace('.', '_') for x in fields])\n        items = self.open().d.multicall(viewname, *commands)\n        return [result_type(*x) for x in items]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef item(self, infohash, prefetch=None, cache=False):\n        return next(self.items(infohash, prefetch, cache))", "response": "Fetch a single item by its info hash."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef items(self, view=None, prefetch=None, cache=True):\n        # TODO: Cache should be by hash.\n        # Then get the initial data when cache is empty,\n        # else get a list of hashes from the view, make a diff\n        # to what's in the cache, fetch the rest. Getting the\n        # fields for one hash might be done by a special view\n        # (filter: $d.hash == hashvalue)\n\n        if view is None:\n            view = engine.TorrentView(self, \"default\")\n        elif isinstance(view, basestring):\n            view = engine.TorrentView(self, self._resolve_viewname(view))\n        else:\n            view.viewname = self._resolve_viewname(view.viewname)\n\n        if not cache or view.viewname not in self._item_cache:\n            # Map pyroscope names to rTorrent ones\n            if prefetch:\n                prefetch = self.CORE_FIELDS | set((self.PYRO2RT_MAPPING.get(i, i) for i in prefetch))\n            else:\n                prefetch = self.PREFETCH_FIELDS\n\n            # Fetch items\n            items = []\n            try:\n                # Prepare multi-call arguments\n                args = [\"d.%s%s\" % (\"\" if field.startswith(\"is_\") else \"get_\", field)\n                    for field in prefetch\n                ]\n\n                infohash = view._check_hash_view()\n                if infohash:\n                    multi_call = self.open().system.multicall\n                    args = [dict(methodName=field.rsplit('=', 1)[0],\n                                 params=[infohash] + (field.rsplit('=', 1)[1].split(',') if '=' in field else []))\n                            for field in args]\n                    raw_items = [[i[0] for i in multi_call(args)]]\n                else:\n                    multi_call = self.open().d.multicall\n                    args = [view.viewname] + [field if '=' in field else field + '=' for field in args]\n                    if view.matcher and int(config.fast_query):\n                        pre_filter = matching.unquote_pre_filter(view.matcher.pre_filter())\n                        self.LOG.info(\"!!! pre-filter: {}\".format(pre_filter or 'N/A'))\n                        if pre_filter:\n                            multi_call = self.open().d.multicall.filtered\n                            args.insert(1, pre_filter)\n                    raw_items = multi_call(*tuple(args))\n\n                ##self.LOG.debug(\"multicall %r\" % (args,))\n                ##import pprint; self.LOG.debug(pprint.pformat(raw_items))\n                self.LOG.debug(\"Got %d items with %d attributes from %r [%s]\" % (\n                    len(raw_items), len(prefetch), self.engine_id, multi_call))\n\n                for item in raw_items:\n                    items.append(RtorrentItem(self, zip(\n                        [self.RT2PYRO_MAPPING.get(i, i) for i in prefetch], item\n                    )))\n                    yield items[-1]\n            except xmlrpc.ERRORS as exc:\n                raise error.EngineError(\"While getting download items from %r: %s\" % (self, exc))\n\n            # Everything yielded, store for next iteration\n            if cache:\n                self._item_cache[view.viewname] = items\n        else:\n            # Yield prefetched results\n            for item in self._item_cache[view.viewname]:\n                yield item", "response": "Get list of download items."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nvisualizing a set of items and return the view name.", "response": "def show(self, items, view=None, append=False, disjoin=False):\n        \"\"\" Visualize a set of items (search result), and return the view name.\n        \"\"\"\n        proxy = self.open()\n        view = self._resolve_viewname(view or \"rtcontrol\")\n\n        if append and disjoin:\n            raise error.EngineError(\"Cannot BOTH append to / disjoin from view '{}'\".format(view))\n\n        # Add view if needed\n        if view not in proxy.view.list():\n            proxy.view.add(xmlrpc.NOHASH, view)\n\n        # Clear view and show it\n        if not append and not disjoin:\n            proxy.view.filter(xmlrpc.NOHASH, view, \"false=\")\n            proxy.d.multicall2(xmlrpc.NOHASH, 'default', 'd.views.remove=' + view)\n        proxy.ui.current_view.set(view)\n\n        # Add items\n        # TODO: should be a \"system.multicall\"\n        for item in items:\n            if disjoin:\n                proxy.d.views.remove(item.hash, view)\n                proxy.view.set_not_visible(item.hash, view)\n            else:\n                proxy.d.views.push_back_unique(item.hash, view)\n                proxy.view.set_visible(item.hash, view)\n\n        return view"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nload rule definitions from config.", "response": "def _load_rules(self):\n        \"\"\"Load rule definitions from config.\"\"\"\n        for ruleset in self.active_rulesets:\n            section_name = 'sweep_rules_' + ruleset.lower()\n            try:\n                ruledefs = getattr(self.config, section_name)\n            except AttributeError:\n                raise error.UserError(\"There is no [{}] section in your configuration\"\n                                      .format(section_name.upper()))\n            for ruledef, filtercond in ruledefs.items():\n                if ruledef.endswith('.filter'):\n                    rulename = ruledef.rsplit('.', 1)[0]\n                    rule = SweepRule(ruleset, rulename,\n                                     int(ruledefs.get(rulename + '.prio', '999')),\n                                     ruledefs.get(rulename + '.order', self.default_order),\n                                     parse_cond(filtercond))\n                    self.rules.append(rule)\n\n        self.rules.sort(key=lambda x: (x.prio, x.name))\n\n        return self.rules"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing a job schedule.", "response": "def _parse_schedule(self, schedule):\n        \"\"\" Parse a job schedule.\n        \"\"\"\n        result = {}\n\n        for param in shlex.split(str(schedule)): # do not feed unicode to shlex\n            try:\n                key, val = param.split('=', 1)\n            except (TypeError, ValueError):\n                self.fatal(\"Bad param '%s' in job schedule '%s'\" % (param, schedule))\n            else:\n                result[key] = val\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nhandling and check configuration.", "response": "def _validate_config(self):\n        \"\"\" Handle and check configuration.\n        \"\"\"\n        groups = dict(\n            job=defaultdict(Bunch),\n            httpd=defaultdict(Bunch),\n        )\n\n        for key, val in config.torque.items():\n            # Auto-convert numbers and bools\n            if val.isdigit():\n                config.torque[key] = val = int(val)\n            elif val.lower() in (matching.TRUE | matching.FALSE):\n                val = matching.truth(str(val), key)\n\n            # Assemble grouped parameters\n            stem = key.split('.', 1)[0]\n            if key == \"httpd.active\":\n                groups[stem][\"active\"] = val\n            elif stem in groups:\n                try:\n                    stem, name, param = key.split('.', 2)\n                except (TypeError, ValueError):\n                    self.fatal(\"Bad %s configuration key %r (expecting %s.NAME.PARAM)\" % (stem, key, stem))\n                else:\n                    groups[stem][name][param] = val\n\n        for key, val in groups.iteritems():\n            setattr(self, key.replace(\"job\", \"jobs\"), Bunch(val))\n\n        # Validate httpd config\n        if self.httpd.active:\n            if self.httpd.waitress.url_scheme not in (\"http\", \"https\"):\n                self.fatal(\"HTTP URL scheme must be either 'http' or 'https'\")\n            if not isinstance(self.httpd.waitress.port, int) or not(1024 <= self.httpd.waitress.port < 65536):\n                self.fatal(\"HTTP port must be a 16 bit number >= 1024\")\n\n        # Validate jobs\n        for name, params in self.jobs.items():\n            for key in (\"handler\", \"schedule\"):\n                if key not in params:\n                    self.fatal(\"Job '%s' is missing the required 'job.%s.%s' parameter\" % (name, name, key))\n\n            bool_param = lambda k, default, p=params: matching.truth(p.get(k, default), \"job.%s.%s\" % (name, k))\n\n            params.job_name = name\n            params.dry_run = bool_param(\"dry_run\", False) or self.options.dry_run\n            params.active = bool_param(\"active\", True)\n            params.schedule = self._parse_schedule(params.schedule)\n\n            if params.active:\n                try:\n                    params.handler = pymagic.import_name(params.handler)\n                except ImportError as exc:\n                    self.fatal(\"Bad handler name '%s' for job '%s':\\n    %s\" % (params.handler, name, exc))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ninitializing WSGI HTTP server.", "response": "def _init_wsgi_server(self):\n        \"\"\" Set up WSGI HTTP server.\n        \"\"\"\n        self.wsgi_server = None\n\n        if self.httpd.active:\n            # Only import dependencies when server is active\n            from waitress.server import WSGIServer\n            from pyrocore.daemon import webapp\n\n            # Set up WSGI stack\n            wsgi_app = webapp.make_app(self.httpd)\n#            try:\n#                import wsgilog\n#            except ImportError:\n#                self.LOG.info(\"'wsgilog' middleware not installed\")\n#            else:\n#                wsgi_app = wsgilog.WsgiLog(wsgi_app, **self.httpd.wsgilog)\n\n            ##logging.getLogger('waitress').setLevel(logging.DEBUG)\n            self.LOG.debug(\"Waitress config: %r\" % self.httpd.waitress)\n            self.wsgi_server = WSGIServer(wsgi_app, **self.httpd.waitress)\n            self.LOG.info(\"Started web server at %s://%s:%d/\" % (\n                self.httpd.waitress.url_scheme,\n                self.wsgi_server.get_server_name(self.wsgi_server.effective_host),\n                int(self.wsgi_server.effective_port),\n            ))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _run_forever(self):\n        while True:\n            try:\n                tick = time.time()\n\n                asyncore.loop(timeout=self.POLL_TIMEOUT, use_poll=True)\n\n                # Sleep for remaining poll cycle time\n                tick += self.POLL_TIMEOUT - time.time()\n                if tick > 0:\n                    # wait POLL_TIMEOUT at most (robust against time shifts)\n                    time.sleep(min(tick, self.POLL_TIMEOUT))\n            except KeyboardInterrupt as exc:\n                self.LOG.info(\"Termination request received (%s)\" % exc)\n                break\n            except SystemExit as exc:\n                self.return_code = exc.code or 0\n                self.LOG.info(\"System exit (RC=%r)\" % self.return_code)\n                break\n            else:\n                # Idle work\n                #self.LOG.warn(\"IDLE %s %r\" % (self.options.guard_file, os.path.exists(self.options.guard_file)))\n                if self.options.guard_file and not os.path.exists(self.options.guard_file):\n                    self.LOG.warn(\"Guard file '%s' disappeared, exiting!\" % self.options.guard_file)\n                    break", "response": "Run configured jobs until termination request."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read_blob(arg):\n    result = None\n    if arg == '@-':\n        result = sys.stdin.read()\n    elif any(arg.startswith('@{}://'.format(x)) for x in {'http', 'https', 'ftp', 'file'}):\n        if not requests:\n            raise error.UserError(\"You must 'pip install requests' to support @URL arguments.\")\n        try:\n            response = requests.get(arg[1:])\n            response.raise_for_status()\n            result = response.content\n        except requests.RequestException as exc:\n            raise error.UserError(str(exc))\n    else:\n        with open(os.path.expanduser(arg[1:]), 'rb') as handle:\n            result = handle.read()\n\n    return result", "response": "Read a BLOB from given arg."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding command line options.", "response": "def add_options(self):\n        \"\"\" Add program options.\n        \"\"\"\n        super(RtorrentXmlRpc, self).add_options()\n\n        # basic options\n        self.add_bool_option(\"-r\", \"--repr\", help=\"show Python pretty-printed response\")\n        self.add_bool_option(\"-x\", \"--xml\", help=\"show XML response\")\n        self.add_bool_option(\"-i\", \"--as-import\",\n            help=\"execute each argument as a private command using 'import'\")\n        self.add_bool_option(\"--session\", \"--restore\",\n            help=\"restore session state from .rtorrent session file(s)\")\n\n        # TODO: Tempita with \"result\" object in namespace\n        #self.add_value_option(\"-o\", \"--output-format\", \"FORMAT\",\n        #    help=\"pass result to a template for formatting\")\n\n        self.proxy = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nopens connection and return proxy.", "response": "def open(self):\n        \"\"\"Open connection and return proxy.\"\"\"\n        if not self.proxy:\n            if not config.scgi_url:\n                config.engine.load_config()\n            if not config.scgi_url:\n                self.LOG.error(\"You need to configure a XMLRPC connection, read\"\n                    \" https://pyrocore.readthedocs.io/en/latest/setup.html\")\n            self.proxy = xmlrpc.RTorrentProxy(config.scgi_url)\n            self.proxy._set_mappings()\n        return self.proxy"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn interpreted or typed list of args.", "response": "def cooked(self, raw_args):\n        \"\"\"Return interpreted / typed list of args.\"\"\"\n        args = []\n        for arg in raw_args:\n            # TODO: use the xmlrpc-c type indicators instead / additionally\n            if arg and arg[0] in \"+-\":\n                try:\n                    arg = int(arg, 10)\n                except (ValueError, TypeError), exc:\n                    self.LOG.warn(\"Not a valid number: %r (%s)\" % (arg, exc))\n            elif arg.startswith('[['):  # escaping, not a list\n                arg = arg[1:]\n            elif arg == '[]':\n                arg = []\n            elif arg.startswith('['):\n                arg = arg[1:].split(',')\n                if all(i.isdigit() for i in arg):\n                    arg = [int(i, 10) for i in arg]\n            elif arg.startswith('@'):\n                arg = xmlrpclib.Binary(read_blob(arg))\n            args.append(arg)\n\n        return args"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef execute(self, proxy, method, args):\n        try:\n            result = getattr(proxy, method)(raw_xml=self.options.xml, *tuple(args))\n        except xmlrpc.ERRORS as exc:\n            self.LOG.error(\"While calling %s(%s): %s\" % (method, \", \".join(repr(i) for i in args), exc))\n            self.return_code = error.EX_NOINPUT if \"not find\" in getattr(exc, \"faultString\", \"\") else error.EX_DATAERR\n        else:\n            if not self.options.quiet:\n                if self.options.repr:\n                    # Pretty-print if requested, or it's a collection and not a scalar\n                    result = pformat(result)\n                elif hasattr(result, \"__iter__\"):\n                    result = '\\n'.join(i if isinstance(i, basestring) else pformat(i) for i in result)\n                print(fmt.to_console(result))", "response": "Execute given XMLRPC call."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef do_repl(self):\n        from prompt_toolkit import prompt\n        from prompt_toolkit.history import FileHistory\n        from prompt_toolkit.auto_suggest import AutoSuggestFromHistory\n        from prompt_toolkit.contrib.completers import WordCompleter\n\n        self.options.quiet = False\n        proxy = self.open()\n        ps1 = proxy.session.name() + u'> '\n        words = ['help', 'stats', 'exit']\n        words += [x + '=' for x in proxy.system.listMethods()]\n        history_file = os.path.join(config.config_dir, '.rtxmlrpc_history')\n\n        while True:\n            try:\n                try:\n                    cmd = prompt(ps1, completer=WordCompleter(words),\n                                 auto_suggest=AutoSuggestFromHistory(),\n                                 history=FileHistory(history_file))\n                except KeyboardInterrupt:\n                    cmd = ''\n                if not cmd:\n                    print(\"Enter '?' or 'help' for usage information, 'Ctrl-D' to exit.\")\n\n                if cmd in {'?', 'help'}:\n                    self.repl_usage()\n                    continue\n                elif cmd in {'', 'stats'}:\n                    print(repr(proxy).split(None, 1)[1])\n                    continue\n                elif cmd in {'exit'}:\n                    raise EOFError()\n\n                try:\n                    method, raw_args = cmd.split('=', 1)\n                except ValueError:\n                    print(\"ERROR: '=' not found\")\n                    continue\n\n                raw_args = raw_args.split(',')\n                args = self.cooked(raw_args)\n                self.execute(proxy, method, args)\n            except EOFError:\n                print('Bye from {!r}'.format(proxy))\n                break", "response": "REPL for rTorrent XMLRPC commands."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nhandle import files or streams passed with - i.", "response": "def do_import(self):\n        \"\"\"Handle import files or streams passed with '-i'.\"\"\"\n        tmp_import = None\n        try:\n            if self.args[0].startswith('@') and self.args[0] != '@-':\n                import_file = os.path.expanduser(self.args[0][1:])\n                if not os.path.isfile(import_file):\n                    self.parser.error(\"File not found (or not a file): {}\".format(import_file))\n                args = (xmlrpc.NOHASH, os.path.abspath(import_file))\n            else:\n                script_text = '\\n'.join(self.args + [''])\n                if script_text == '@-\\n':\n                    script_text = sys.stdin.read()\n\n                with tempfile.NamedTemporaryFile(suffix='.rc', prefix='rtxmlrpc-', delete=False) as handle:\n                    handle.write(script_text)\n                    tmp_import = handle.name\n                args = (xmlrpc.NOHASH, tmp_import)\n\n            self.execute(self.open(), 'import', args)\n        finally:\n            if tmp_import and os.path.exists(tmp_import):\n                os.remove(tmp_import)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef do_command(self):\n        method = self.args[0]\n\n        raw_args = self.args[1:]\n        if '=' in method:\n            if raw_args:\n                self.parser.error(\"Please don't mix rTorrent and shell argument styles!\")\n            method, raw_args = method.split('=', 1)\n            raw_args = raw_args.split(',')\n\n        self.execute(self.open(), method, self.cooked(raw_args))", "response": "Call a single command with arguments."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nrestores state from session files.", "response": "def do_session(self):\n        \"\"\"Restore state from session files.\"\"\"\n        def filenames():\n            'Helper'\n            for arg in self.args:\n                if os.path.isdir(arg):\n                    for name in glob.glob(os.path.join(arg, '*.torrent.rtorrent')):\n                        yield name\n                elif arg == '@-':\n                    for line in sys.stdin.read().splitlines():\n                        if line.strip():\n                            yield line.strip()\n                elif arg.startswith('@'):\n                    if not os.path.isfile(arg[1:]):\n                        self.parser.error(\"File not found (or not a file): {}\".format(arg[1:]))\n                    with io.open(arg[1:], encoding='utf-8') as handle:\n                        for line in handle:\n                            if line.strip():\n                                yield line.strip()\n                else:\n                    yield arg\n\n        proxy = self.open()\n        for filename in filenames():\n            # Check filename and extract infohash\n            self.LOG.debug(\"Reading '%s'...\", filename)\n            match = re.match(r'(?:.+?[-._])?([a-fA-F0-9]{40})(?:[-._].+?)?\\.torrent\\.rtorrent',\n                             os.path.basename(filename))\n            if not match:\n                self.LOG.warn(\"Skipping badly named session file '%s'...\", filename)\n                continue\n            infohash = match.group(1)\n\n            # Read bencoded data\n            try:\n                with open(filename, 'rb') as handle:\n                    raw_data = handle.read()\n                data = Bunch(bencode.bdecode(raw_data))\n            except EnvironmentError as exc:\n                self.LOG.warn(\"Can't read '%s' (%s)\" % (\n                    filename, str(exc).replace(\": '%s'\" % filename, \"\"),\n                ))\n                continue\n\n            ##print(infohash, '=', repr(data))\n            if 'state_changed' not in data:\n                self.LOG.warn(\"Skipping invalid session file '%s'...\", filename)\n                continue\n\n            # Restore metadata\n            was_active = proxy.d.is_active(infohash)\n            proxy.d.ignore_commands.set(infohash, data.ignore_commands)\n            proxy.d.priority.set(infohash, data.priority)\n\n            if proxy.d.throttle_name(infohash) != data.throttle_name:\n                proxy.d.pause(infohash)\n                proxy.d.throttle_name.set(infohash, data.throttle_name)\n\n            if proxy.d.directory(infohash) != data.directory:\n                proxy.d.stop(infohash)\n                proxy.d.directory_base.set(infohash, data.directory)\n\n            for i in range(5):\n                key = 'custom%d' % (i + 1)\n                getattr(proxy.d, key).set(infohash, data[key])\n\n            for key, val in data.custom.items():\n                proxy.d.custom.set(infohash, key, val)\n\n            for name in data.views:\n                try:\n                    proxy.view.set_visible(infohash, name)\n                except xmlrpclib.Fault as exc:\n                    if 'Could not find view' not in str(exc):\n                        raise\n\n            if was_active and not proxy.d.is_active(infohash):\n                (proxy.d.resume if proxy.d.is_open(infohash) else proxy.d.start)(infohash)\n            proxy.d.save_full_session(infohash)\n\n            \"\"\" TODO:\n                NO public \"set\" command! 'timestamp.finished': 1503012786,\n                NO public \"set\" command! 'timestamp.started': 1503012784,\n                NO public \"set\" command! 'total_uploaded': 0,\n            \"\"\""}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef mainloop(self):\n        self.check_for_connection()\n\n        # Enter REPL if no args\n        if len(self.args) < 1:\n            return self.do_repl()\n\n        # Check for bad options\n        if self.options.repr and self.options.xml:\n            self.parser.error(\"You cannot combine --repr and --xml!\")\n        if sum([self.options.as_import, self.options.session]) > 1:\n            self.parser.error(\"You cannot combine -i and --session!\")\n\n        # Dispatch to handlers\n        if self.options.as_import:\n            self.do_import()\n        elif self.options.session:\n            self.do_session()\n        else:\n            self.do_command()\n\n        # XMLRPC stats\n        self.LOG.debug(\"XMLRPC stats: %s\" % self.open())", "response": "The main loop of the XMLRPC command."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd command line options for the RtorrentSweep class.", "response": "def add_options(self):\n        \"\"\" Add program options.\n        \"\"\"\n        super(RtorrentSweep, self).add_options()\n\n        # basic options\n        self.add_bool_option(\"-n\", \"--dry-run\",\n            help=\"do not remove anything, just tell what would happen\")\n        self.add_value_option(\"-p\", \"--path\", \"PATH\",\n            help=\"path into the filesystem to sweep (else the default download location)\")\n        self.add_value_option(\"-r\", \"--rules\", \"RULESET [-r ...]\",\n            action=\"append\", default=[],\n            help=\"name the ruleset(s) to use, instead of the default ones\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_options(self):\n        super(AdminTool, self).add_options()\n\n        self.add_bool_option(\"--create-config\",\n            help=\"create default configuration\")\n        self.add_bool_option(\"--remove-all-rc-files\",\n            help=\"write new versions of BOTH .rc and .rc.default files, and remove stale ones\")\n        self.add_bool_option(\"--dump-config\",\n            help=\"pretty-print configuration including all defaults\")\n        self.add_value_option(\"--create-import\", \"GLOB-PATTERN\",\n            action=\"append\", default=[],\n            help=\"create import file for a '.d' directory\")\n        self.add_bool_option(\"--dump-rc\",\n            help=\"pretty-print dynamic commands defined in 'rtorrent.rc'\")\n        self.add_value_option(\"-o\", \"--output\", \"KEY,KEY1.KEY2=DEFAULT,...\",\n            action=\"append\", default=[],\n            help=\"select fields to print, output is separated by TABs;\"\n                 \" default values can be provided after the key\")\n        self.add_bool_option(\"--reveal\",\n            help=\"show config internals and full announce URL including keys\")\n        self.add_bool_option(\"--screenlet\",\n            help=\"create screenlet stub\")", "response": "Add command line options for the admin tool."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef download_resource(self, download_url, target, guard):\n        download_url = download_url.strip()\n        if not os.path.isabs(target):\n            target = os.path.join(config.config_dir, target)\n\n        if os.path.exists(os.path.join(target, guard)):\n            self.LOG.info(\"Already have '%s' in '%s'...\" % (download_url, target))\n            return\n\n        if not os.path.isdir(target):\n            os.makedirs(target)\n\n        self.LOG.info(\"Downloading '%s' to '%s'...\" % (download_url, target))\n        with closing(urllib2.urlopen(download_url)) as url_handle:\n            if download_url.endswith(\".zip\"):\n                with closing(ZipFile(StringIO(url_handle.read()))) as zip_handle:  # pylint: disable=no-member\n                    zip_handle.extractall(target)  # pylint: disable=no-member\n            else:\n                with open(os.path.join(target, guard), \"wb\") as file_handle:\n                    shutil.copyfileobj(url_handle, file_handle)", "response": "Download and install external resources."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nhandle theme selection changes, or rotate through selection.", "response": "def mainloop(self):\n        \"\"\" Handle theme selection changes, or rotate through selection.\n        \"\"\"\n        themes_dir = os.path.join(self.config_dir, 'color-schemes')\n        selected_file = os.path.join(themes_dir, '.selected')\n        current_file = os.path.join(themes_dir, '.current')\n\n        # Read persisted state\n        selected_themes = []\n        if os.path.exists(selected_file):\n            with open(selected_file, 'rt') as handle:\n                selected_themes = [x.strip() for x in handle]\n\n        current_theme = None\n        if os.path.exists(current_file):\n            with open(current_file, 'rt') as handle:\n                current_theme = handle.readline().strip()\n\n        # Scan config for available themes\n        themes = {}\n        for ext in ('.rc.default', '.rc'):\n            for filepath in glob.glob(themes_dir + '/*' + ext):\n                name = os.path.basename(filepath).split('.')[0]\n                if name:\n                    # This prefers '.rc.default' files, because those are checked first\n                    themes.setdefault(name, filepath)\n\n        # Use available selected themes in given order, if there are any, else all themes\n        if selected_themes and not set(selected_themes).isdisjoint(set(themes)):\n            theme_list = [x for x in selected_themes if x in themes]\n        else:\n            theme_list = list(sorted(themes))\n\n        # Check options\n        if self.options.list or self.options.all or self.options.toggle:\n            if self.options.all:\n                if os.path.exists(selected_file):\n                    os.remove(selected_file)\n                selected_themes = []\n\n            for name in (self.options.toggle or '').replace(',', ' ').split():\n                if name not in themes:\n                    self.parser.error(\"Unknown theme {0!r}, use '--list' to show them\".format(name))\n                elif name in selected_themes:\n                    selected_themes = [x for x in selected_themes if x != name]\n                else:\n                    selected_themes.append(name)\n\n                with open(selected_file, 'wt') as handle:\n                    handle.write('\\n'.join(selected_themes + ['']))\n\n            if self.options.list:\n                for name in sorted(themes):\n                    print(\"{} {} {}\".format(\n                        '*' if name == current_theme else ' ',\n                        '{:2d}'.format(selected_themes.index(name) + 1)\n                            if name in selected_themes else '  ',\n                        name,\n                    ))\n\n        elif self.options.current or self.options.next:\n            # Determine current theme, or rotate to next one\n            new_theme = theme_list[0]\n            if self.options.current and current_theme in theme_list:\n                new_theme = current_theme\n            elif current_theme in theme_list:\n                new_theme = (theme_list * 2)[theme_list.index(current_theme) + 1]\n\n            # Persist new theme\n            if new_theme != current_theme:\n                with open(current_file, 'wt') as handle:\n                    handle.write(new_theme + '\\n')\n\n            # Return result\n            sys.stdout.write(themes[new_theme])\n            sys.stdout.flush()\n\n        else:\n            self.LOG.info(\"Current color theme is '{}'.\".format(\n                current_theme if current_theme in theme_list else theme_list[0]))\n            self.LOG.info(\"Use '--help' to get usage information.\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fmt_duration(secs):\n    return ' '.join(fmt.human_duration(secs, 0, precision=2, short=True).strip().split())", "response": "Format a duration in seconds."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the free bytes on partition holding path.", "response": "def disk_free(path):\n    \"\"\"Return free bytes on partition holding `path`.\"\"\"\n    stats = os.statvfs(path)\n    return stats.f_bavail * stats.f_frsize"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting truth value in val to a boolean.", "response": "def truth(val, context):\n    \"\"\" Convert truth value in \"val\" to a boolean.\n    \"\"\"\n    try:\n        0 + val\n    except TypeError:\n        lower_val = val.lower()\n\n        if lower_val in TRUE:\n            return True\n        elif lower_val in FALSE:\n            return False\n        else:\n            raise FilterError(\"Bad boolean value %r in %r (expected one of '%s', or '%s')\" % (\n                val, context, \"' '\".join(TRUE), \"' '\".join(FALSE)\n            ))\n    else:\n        return bool(val)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _time_ym_delta(timestamp, delta, months):\n    timestamp = list(time.localtime(timestamp))\n    timestamp[int(months)] += delta\n    return time.mktime(timestamp)", "response": "Helper to add a year or month delta to a timestamp."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef unquote_pre_filter(pre_filter, _regex=re.compile(r'[\\\\]+')):\n    if pre_filter.startswith('\"') and pre_filter.endswith('\"'):\n        # Unquote outer level\n        pre_filter = pre_filter[1:-1]\n        pre_filter = _regex.sub(lambda x: x.group(0)[:len(x.group(0)) // 2], pre_filter)\n\n    return pre_filter", "response": "Unquote a pre - filter condition."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning rTorrent condition to speed up data transfer.", "response": "def pre_filter(self):\n        \"\"\" Return rTorrent condition to speed up data transfer.\n        \"\"\"\n        if len(self) == 1:\n            return self[0].pre_filter()\n        else:\n            result = [x.pre_filter() for x in self if not isinstance(x, CompoundFilterBase)]\n            result = [x for x in result if x]\n            if result:\n                if int(config.fast_query) == 1:\n                    return result[0]  # using just one simple expression is safer\n                else:\n                    # TODO: make this purely value-based (is.nz=\u2026)\n                    return 'and={%s}' % ','.join(result)\n        return ''"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn rTorrent condition to speed up data transfer.", "response": "def pre_filter(self):\n        \"\"\" Return rTorrent condition to speed up data transfer.\n        \"\"\"\n        inner = self._inner.pre_filter()\n        if inner:\n            if inner.startswith('\"not=$') and inner.endswith('\"') and '\\\\' not in inner:\n                return inner[6:-1]  # double negation, return inner command\n            elif inner.startswith('\"'):\n                inner = '\"$' + inner[1:]\n            else:\n                inner = '$' + inner\n            return 'not=' + inner\n        else:\n            return ''"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn True if filter matches item.", "response": "def match(self, item):\n        \"\"\" Return True if filter matches item.\n        \"\"\"\n        result = self._value == getattr(item, self._name)\n        #log.debug(\"%r for %r = %r, name %r, item %r\" % (\n        #    result, getattr(item, self._name), self._value, self._name, item))\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef validate(self):\n        from pyrocore.torrent import formatting\n\n        super(PatternFilter, self).validate()\n        self._value = self._value.lower()\n        self._template = None\n        self._is_regex = self._value.startswith('/') and self._value.endswith('/')\n        if self._is_regex:\n            self._matcher = re.compile(self._value[1:-1]).search\n        elif self._value.startswith('{{') or self._value.endswith('}}'):\n            def _template_globber(val, item):\n                \"\"\"Helper.\"\"\"\n                pattern = formatting.format_item(self._template, item).replace('[', '[[]')\n                ##print('!!!', val, '~~~', pattern, '???')\n                return fnmatch.fnmatchcase(val, pattern.lower())\n\n            self._template = formatting.preparse(self._value)\n            self._matcher = _template_globber\n        else:\n            self._matcher = lambda val, _: fnmatch.fnmatchcase(val, self._value)", "response": "Validate filter condition (template method)."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn rTorrent condition to speed up data transfer.", "response": "def pre_filter(self):\n        \"\"\" Return rTorrent condition to speed up data transfer.\n        \"\"\"\n        if self._name not in self.PRE_FILTER_FIELDS or self._template:\n            return ''\n        if not self._value:\n            return '\"equal={},cat=\"'.format(self.PRE_FILTER_FIELDS[self._name])\n\n        if self._is_regex:\n            needle = self._value[1:-1]\n            needle = self.CLEAN_PRE_VAL_RE.sub(' ', needle)\n            needle = self.SPLIT_PRE_VAL_RE.split(needle)\n        else:\n            needle = self.CLEAN_PRE_VAL_RE.sub(' ', self._value)\n            needle = self.SPLIT_PRE_GLOB_RE.split(needle)\n        needle = list(sorted(needle, key=len))[-1]\n\n        if needle:\n            try:\n                needle.encode('ascii')\n            except UnicodeEncodeError:\n                return ''\n            else:\n                return r'\"string.contains_i=${},\\\"{}\\\"\"'.format(\n                       self.PRE_FILTER_FIELDS[self._name], needle.replace('\"', r'\\\\\\\"'))\n\n        return ''"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning True if filter matches item.", "response": "def match(self, item):\n        \"\"\" Return True if filter matches item.\n        \"\"\"\n        val = (getattr(item, self._name) or '').lower()\n        result = self._matcher(val) if self._is_regex else self._matcher(val, item)\n        if 0:\n            log.debug(\"%r for %r ~ %r, name %r, item %r\" % (\n                result, val, self._value, self._name, item))\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn True if filter matches item.", "response": "def match(self, item):\n        \"\"\" Return True if filter matches item.\n        \"\"\"\n        val = getattr(item, self._name)\n        if val is not None:\n            for fileinfo in val:\n                if fnmatch.fnmatchcase(fileinfo.path.lower(), self._value):\n                    return True\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef pre_filter(self):\n        if self._name in self.PRE_FILTER_FIELDS:\n            if not self._value:\n                return '\"not=${}\"'.format(self.PRE_FILTER_FIELDS[self._name])\n            else:\n                val = self._value\n                if self._exact:\n                    val = val.copy().pop()\n                return r'\"string.contains_i=${},\\\"{}\\\"\"'.format(\n                       self.PRE_FILTER_FIELDS[self._name], val.replace('\"', r'\\\\\\\"'))\n        return ''", "response": "Return rTorrent condition to speed up data transfer."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef validate(self):\n        super(TaggedAsFilter, self).validate()\n        self._value = self._value.lower()\n\n        # If the tag starts with '=', test on equality (just this tag, no others)\n        if self._value.startswith('='):\n            self._exact = True\n            self._value = self._value[1:]\n        else:\n            self._exact = not self._value\n\n        # For exact matches, value is the set to compare to\n        if self._exact:\n            # Empty tag means empty set, not set of one empty string\n            self._value = set((self._value,)) if self._value else set()", "response": "Validate the filter condition."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef match(self, item):\n        tags = getattr(item, self._name) or []\n        if self._exact:\n            # Equality check\n            return self._value == set(tags)\n        else:\n            # Is given tag in list?\n            return self._value in tags", "response": "Return True if filter matches item.\n           "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef validate(self):\n        super(BoolFilter, self).validate()\n\n        self._value = truth(str(self._value), self._condition)\n        self._condition = \"yes\" if self._value else \"no\"", "response": "Validate filter condition (template method)."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns True if filter matches item.", "response": "def match(self, item):\n        \"\"\" Return True if filter matches item.\n        \"\"\"\n        val = getattr(item, self._name) or False\n        return bool(val) is self._value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef validate(self):\n        super(NumericFilterBase, self).validate()\n\n        self.not_null = False\n\n        if self._value.startswith('+'):\n            self._cmp = operator.gt\n            self._rt_cmp = 'greater'\n            self._value = self._value[1:]\n        elif self._value.startswith('-'):\n            self._cmp = operator.lt\n            self._rt_cmp = 'less'\n            self._value = self._value[1:]\n        else:\n            self._cmp = operator.eq\n            self._rt_cmp = 'equal'", "response": "Validate the filter condition."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning True if filter matches item.", "response": "def match(self, item):\n        \"\"\" Return True if filter matches item.\n        \"\"\"\n        if 0 and getattr(item, self._name):\n            print \"%r %r %r %r %r %r\" % (self._cmp(float(getattr(item, self._name)), self._value),\n                  self._name, self._condition, item.name, getattr(item, self._name), self._value)\n        val = getattr(item, self._name) or 0\n        if self.not_null and self._value and not val:\n            return False\n        else:\n            return self._cmp(float(val), self._value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns rTorrent condition to speed up data transfer.", "response": "def pre_filter(self):\n        \"\"\" Return rTorrent condition to speed up data transfer.\n        \"\"\"\n        if self._name in self.PRE_FILTER_FIELDS:\n            val = int(self._value * self.FIELD_SCALE.get(self._name, 1))\n            return '\"{}=value=${},value={}\"'.format(\n                   self._rt_cmp, self.PRE_FILTER_FIELDS[self._name], val)\n        return ''"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nvalidate the filter condition.", "response": "def validate(self):\n        \"\"\" Validate filter condition (template method).\n        \"\"\"\n        super(FloatFilter, self).validate()\n\n        try:\n            self._value = float(self._value)\n        except (ValueError, TypeError), exc:\n            raise FilterError(\"Bad numerical value %r in %r (%s)\" % (self._value, self._condition, exc))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nvalidate the timestamp of the current time.", "response": "def validate_time(self, duration=False):\n        \"\"\" Validate filter condition (template method) for timestamps and durations.\n        \"\"\"\n        super(TimeFilter, self).validate()\n        timestamp = now = time.time()\n\n        if str(self._value).isdigit():\n            # Literal UNIX timestamp\n            try:\n                timestamp = float(self._value)\n            except (ValueError, TypeError), exc:\n                raise FilterError(\"Bad timestamp value %r in %r (%s)\" % (self._value, self._condition, exc))\n        else:\n            # Something human readable\n            delta = self.TIMEDELTA_RE.match(self._value)\n            ##print self.TIMEDELTA_RE.pattern\n            if delta:\n                # Time delta\n                for unit, val in delta.groupdict().items():\n                    if val:\n                        timestamp = self.TIMEDELTA_UNITS[unit](timestamp, int(val, 10))\n\n                if duration:\n                    timestamp = now - timestamp\n                else:\n                    # Invert logic for time deltas (+ = older; - = within the delta range)\n                    if self._cmp == operator.lt:\n                        self._cmp = operator.gt\n                        self._rt_cmp = 'greater'\n                    elif self._cmp == operator.gt:\n                        self._cmp = operator.lt\n                        self._rt_cmp = 'less'\n            else:\n                # Assume it's an absolute date\n                if '/' in self._value:\n                    # U.S.\n                    dtfmt = \"%m/%d/%Y\"\n                elif '.' in self._value:\n                    # European\n                    dtfmt = \"%d.%m.%Y\"\n                else:\n                    # Fall back to ISO\n                    dtfmt = \"%Y-%m-%d\"\n\n                val = str(self._value).upper().replace(' ', 'T')\n                if 'T' in val:\n                    # Time also given\n                    dtfmt += \"T%H:%M:%S\"[:3+3*val.count(':')]\n\n                try:\n                    timestamp = time.mktime(time.strptime(val, dtfmt))\n                except (ValueError), exc:\n                    raise FilterError(\"Bad timestamp value %r in %r (%s)\" % (self._value, self._condition, exc))\n\n                if duration:\n                    timestamp -= now\n\n        self._value = timestamp"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef match(self, item):\n        if getattr(item, self._name) is None:\n            # Never match \"N/A\" items, except when \"-0\" was specified\n            return False if self._value else self._cmp(-1, 0)\n        else:\n            return super(DurationFilter, self).match(item)", "response": "Return True if filter matches item."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns rTorrent condition to speed up data transfer.", "response": "def pre_filter(self):\n        \"\"\" Return rTorrent condition to speed up data transfer.\n        \"\"\"\n        if self._name in self.PRE_FILTER_FIELDS:\n            return '\"{}={},value={}\"'.format(\n                   self._rt_cmp, self.PRE_FILTER_FIELDS[self._name], int(self._value))\n        return ''"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nvalidate the filter condition.", "response": "def validate(self):\n        \"\"\" Validate filter condition (template method).\n        \"\"\"\n        super(ByteSizeFilter, self).validate()\n\n        # Get scale\n        lower_val = str(self._value).lower()\n        if any(lower_val.endswith(i) for i in self.UNITS):\n            scale = self.UNITS[lower_val[-1]]\n            self._value = self._value[:-1]\n        else:\n            scale = 1\n\n        # Get float value\n        try:\n            self._value = float(self._value)\n        except (ValueError, TypeError), exc:\n            raise FilterError(\"Bad numerical value %r in %r (%s)\" % (self._value, self._condition, exc))\n\n        # Scale to bytes\n        self._value = self._value * scale"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef validate(self):\n        val = self._condition.lower()\n        if val and val[0] in \"+-\":\n            val = val[1:]\n\n        matcher = PatternFilter\n        if not val or val.startswith('/') and val.endswith('/'):\n            pass\n        elif val.replace('.', '0').isdigit():\n            matcher = FloatFilter\n        elif self._condition in (TRUE | FALSE):\n            matcher = BoolFilter\n        elif any(val.endswith(i) for i in ByteSizeFilter.UNITS) and val[:-1].isdigit():\n            matcher = ByteSizeFilter\n        elif TimeFilter.TIMEDELTA_RE.match(val):\n            matcher = TimeFilter\n\n        self._inner = matcher(self._name, self._condition)", "response": "Validate the filter condition."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _create_filter(self, condition):\n        # \"Normal\" comparison operators?\n        comparison = re.match(r\"^(%s)(<[>=]?|>=?|!=|~)(.*)$\" % self.ident_re, condition)\n        if comparison:\n            name, comparison, values = comparison.groups()\n            if values and values[0] in \"+-\":\n                raise FilterError(\"Comparison operator cannot be followed by '%s' in '%s'\" % (values[0], condition))\n            values = self.COMPARISON_OPS[comparison] % values\n        else:\n            # Split name from value(s)\n            try:\n                name, values = condition.split('=', 1)\n            except ValueError:\n                if self.default_field:\n                    name, values = self.default_field, condition\n                else:\n                    raise FilterError(\"Field name missing in '%s' (expected '=')\" % condition)\n\n        # Try to find field definition\n        field = self.lookup(name)\n        if not field:\n            raise FilterError(\"Unknown field %r in %r\" % (name, condition))\n        if field.get(\"matcher\") is None:\n            raise FilterError(\"Field %r cannot be used as a filter\" % (name,))\n\n        # Make filters from values (split on commas outside of /\u2026/)\n        filters = []\n        split_values = re.findall(r'(!?/[^/]*/|[^,]+)(?:,|$)', values) if values else ['']\n        if not split_values:\n            raise FilterError(\"Internal Error: Cannot split %r into match values\" % (values,))\n\n        for value in split_values:\n            wrapper = None\n            if value.startswith('!'):\n                wrapper = NegateFilter\n                value = value[1:]\n            field_matcher = field[\"matcher\"](name, value)\n            filters.append(wrapper(field_matcher) if wrapper else field_matcher)\n\n        # Return filters\n        return CompoundFilterAny(filters) if len(filters) > 1 else filters[0]", "response": "Create a filter object from a textual condition."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts a parsed condition tree back to a string.", "response": "def _tree2str(self, tree, root=True):\n        \"\"\" Convert parsed condition tree back to a (printable) string.\n        \"\"\"\n        try:\n            # Keep strings as they are\n            return '' + tree\n        except (TypeError, ValueError):\n            flat = ' '.join(self._tree2str(i, root=False) for i in tree)\n            return flat if root else \"[ %s ]\" % flat"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse(self, conditions):\n        conditions_text = conditions\n        try:\n            conditions = shlex.split(fmt.to_utf8(conditions))\n        except AttributeError:\n            # Not a string, assume parsed tree\n            conditions_text = self._tree2str(conditions)\n\n        # Empty list?\n        if not conditions:\n            raise FilterError(\"No conditions given at all!\")\n\n        # NOT *must* appear at the start of a group\n        negate = conditions[:1] == [\"NOT\"]\n        if negate:\n            conditions = conditions[1:]\n            if not conditions:\n                raise FilterError(\"NOT must be followed by some conditions!\")\n\n        # Handle grouping\n        if '[' in conditions:\n            tree = [[]]\n            for term in conditions:\n                if term == '[':\n                    tree.append([]) # new grouping\n                elif term == ']':\n                    subtree = tree.pop()\n                    if not tree:\n                        raise FilterError(\"Unbalanced brackets, too many closing ']' in condition %r\" % (conditions_text,))\n                    tree[-1].append(subtree) # append finished group to containing level\n                else:\n                    tree[-1].append(term) # append to current level\n\n            if len(tree) > 1:\n                raise FilterError(\"Unbalanced brackets, too many open '[' in condition %r\" % (conditions_text,))\n            conditions = tree[0]\n\n        # Prepare root matcher\n        conditions = list(conditions)\n        matcher = CompoundFilterAll()\n        if \"OR\" in conditions:\n            root = CompoundFilterAny()\n            root.append(matcher)\n        else:\n            root = matcher\n\n        # Go through conditions and parse them\n        for condition in conditions:\n            if condition == \"OR\":\n                # Leading OR, or OR OR in sequence?\n                if not matcher:\n                    raise FilterError(\"Left-hand side of OR missing in %r!\" % (conditions_text,))\n\n                # Start next run of AND conditions\n                matcher = CompoundFilterAll()\n                root.append(matcher)\n            elif isinstance(condition, list):\n                matcher.append(self.parse(condition))\n            else:\n                matcher.append(self._create_filter(condition))\n\n        # Trailing OR?\n        if not matcher:\n            raise FilterError(\"Right-hand side of OR missing in %r!\" % (conditions_text,))\n\n        return NegateFilter(root) if negate else root", "response": "Parse the filter conditions."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _flux_engine_data(engine):\n    data = stats.engine_data(engine)\n\n    # Make it flat\n    data[\"up_rate\"] = data[\"upload\"][0]\n    data[\"up_limit\"] = data[\"upload\"][1]\n    data[\"down_rate\"] = data[\"download\"][0]\n    data[\"down_limit\"] = data[\"download\"][1]\n    data[\"version\"] = data[\"versions\"][0]\n    views = data[\"views\"]\n\n    del data[\"upload\"]\n    del data[\"download\"]\n    del data[\"versions\"]\n    del data[\"views\"]\n\n    return data, views", "response": "Return rTorrent data set for pushing to InfluxDB."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run(self):\n        try:\n            proxy = config_ini.engine.open()\n            self.LOG.info(\"Stats for %s - up %s, %s\" % (\n                config_ini.engine.engine_id,\n                fmt.human_duration(proxy.system.time() - config_ini.engine.startup, 0, 2, True).strip(),\n                proxy\n            ))\n        except (error.LoggableError, xmlrpc.ERRORS), exc:\n            self.LOG.warn(str(exc))", "response": "Statistics logger job callback."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning REST API URL to access time series.", "response": "def _influxdb_url(self):\n        \"\"\" Return REST API URL to access time series.\n        \"\"\"\n        url = \"{0}/db/{1}/series\".format(self.influxdb.url.rstrip('/'), self.config.dbname)\n\n        if self.influxdb.user and self.influxdb.password:\n            url += \"?u={0}&p={1}\".format(self.influxdb.user, self.influxdb.password)\n\n        return url"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _push_data(self):\n        if not (self.config.series or self.config.series_host):\n            self.LOG.info(\"Misconfigured InfluxDB job, neither 'series' nor 'series_host' is set!\")\n            return\n\n        # Assemble data\n        fluxdata = []\n\n        if self.config.series:\n            try:\n                config_ini.engine.open()\n                data, views = _flux_engine_data(config_ini.engine)\n                fluxdata.append(dict(\n                    name=self.config.series,\n                    columns=data.keys(),\n                    points=[data.values()]\n                ))\n                fluxdata.append(dict(\n                    name=self.config.series + '_views',\n                    columns=views.keys(),\n                    points=[views.values()]\n                ))\n            except (error.LoggableError, xmlrpc.ERRORS), exc:\n                self.LOG.warn(\"InfluxDB stats: {0}\".format(exc))\n\n#        if self.config.series_host:\n#            fluxdata.append(dict(\n#                name = self.config.series_host,\n#                columns = .keys(),\n#                points = [.values()]\n#            ))\n\n        if not fluxdata:\n            self.LOG.debug(\"InfluxDB stats: no data (previous errors?)\")\n            return\n\n        # Encode into InfluxDB data packet\n        fluxurl = self._influxdb_url()\n        fluxjson = json.dumps(fluxdata)\n        self.LOG.debug(\"POST to {0} with {1}\".format(fluxurl.split('?')[0], fluxjson))\n\n        # Push it!\n        try:\n            # TODO: Use a session\n            requests.post(fluxurl, data=fluxjson, timeout=self.influxdb.timeout)\n        except RequestException, exc:\n            self.LOG.info(\"InfluxDB POST error: {0}\".format(exc))", "response": "Push stats data to InfluxDB."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd command line options for the current language.", "response": "def add_options(self):\n        \"\"\" Add program options.\n        \"\"\"\n        self.add_bool_option(\"--reveal\",\n            help=\"show full announce URL including keys\")\n        self.add_bool_option(\"--raw\",\n            help=\"print the metafile's raw content in all detail\")\n        self.add_bool_option(\"-V\", \"--skip-validation\",\n            help=\"show broken metafiles with an invalid structure\")\n        self.add_value_option(\"-o\", \"--output\", \"KEY,KEY1.KEY2,...\",\n            action=\"append\", default=[],\n            help=\"select fields to print, output is separated by TABs;\"\n                 \" note that __file__ is the path to the metafile,\"\n                 \" __hash__ is the info hash,\"\n                 \" and __size__ is the data size in bytes\"\n         )"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreplace patterns in fields.", "response": "def replace_fields(meta, patterns):\n    \"\"\" Replace patterns in fields.\n    \"\"\"\n    for pattern in patterns:\n        try:\n            field, regex, subst, _ = pattern.split(pattern[-1])\n\n            # TODO: Allow numerical indices, and \"+\" for append\n            namespace = meta\n            keypath = [i.replace('\\0', '.') for i in field.replace('..', '\\0').split('.')]\n            for key in keypath[:-1]:\n                namespace = namespace[key]\n\n            namespace[keypath[-1]] = re.sub(regex, subst, namespace[keypath[-1]])\n        except (KeyError, IndexError, TypeError, ValueError) as exc:\n            raise error.UserError(\"Bad substitution '%s' (%s)!\" % (pattern, exc))\n\n    return meta"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds command line options for the current instance of the class MetafileChanger.", "response": "def add_options(self):\n        \"\"\" Add program options.\n        \"\"\"\n        super(MetafileChanger, self).add_options()\n\n        self.add_bool_option(\"-n\", \"--dry-run\",\n            help=\"don't write changes to disk, just tell what would happen\")\n        self.add_bool_option(\"-V\", \"--no-skip\",\n            help=\"do not skip broken metafiles that fail the integrity check\")\n        self.add_value_option(\"-o\", \"--output-directory\", \"PATH\",\n            help=\"optional output directory for the modified metafile(s)\")\n        self.add_bool_option(\"-p\", \"--make-private\",\n            help=\"make torrent private (DHT/PEX disabled)\")\n        self.add_bool_option(\"-P\", \"--make-public\",\n            help=\"make torrent public (DHT/PEX enabled)\")\n        self.add_value_option(\"-s\", \"--set\", \"KEY=VAL [-s ...]\",\n            action=\"append\", default=[],\n            help=\"set a specific key to the given value; omit the '=' to delete a key\")\n        self.add_value_option(\"-r\", \"--regex\", \"KEYcREGEXcSUBSTc [-r ...]\",\n            action=\"append\", default=[],\n            help=\"replace pattern in a specific key by the given substitution\")\n        self.add_bool_option(\"-C\", \"--clean\",\n            help=\"remove all non-standard data from metafile outside the info dict\")\n        self.add_bool_option(\"-A\", \"--clean-all\",\n            help=\"remove all non-standard data from metafile including inside the info dict\")\n        self.add_bool_option(\"-X\", \"--clean-xseed\",\n            help=\"like --clean-all, but keep libtorrent resume information\")\n        self.add_bool_option(\"-R\", \"--clean-rtorrent\",\n            help=\"remove all rTorrent session data from metafile\")\n        self.add_value_option(\"-H\", \"--hashed\", \"--fast-resume\", \"DATAPATH\",\n            help=\"add libtorrent fast-resume information (use {} in place of the torrent's name in DATAPATH)\")\n        # TODO: chtor --tracker\n        ##self.add_value_option(\"-T\", \"--tracker\", \"DOMAIN\",\n        ##    help=\"filter given torrents for a tracker domain\")\n        self.add_value_option(\"-a\", \"--reannounce\", \"URL\",\n            help=\"set a new announce URL, but only if the old announce URL matches the new one\")\n        self.add_value_option(\"--reannounce-all\", \"URL\",\n            help=\"set a new announce URL on ALL given metafiles\")\n        self.add_bool_option(\"--no-ssl\",\n            help=\"force announce URL to 'http'\")\n        self.add_bool_option(\"--no-cross-seed\",\n            help=\"when using --reannounce-all, do not add a non-standard field to the info dict ensuring unique info hashes\")\n        self.add_value_option(\"--comment\", \"TEXT\",\n            help=\"set a new comment (an empty value deletes it)\")\n        self.add_bool_option(\"--bump-date\",\n            help=\"set the creation date to right now\")\n        self.add_bool_option(\"--no-date\",\n            help=\"remove the 'creation date' field\")"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninitializing everything for interactive use.", "response": "def connect(config_dir=None, optional_config_files=None, cron_cfg=\"cron\"):\n    \"\"\" Initialize everything for interactive use.\n\n        Returns a ready-to-use RtorrentEngine object.\n    \"\"\"\n    from pyrocore.scripts.base import ScriptBase\n    from pyrocore.util import load_config\n\n    ScriptBase.setup(cron_cfg=cron_cfg)\n    load_config.ConfigLoader(config_dir).load(optional_config_files or [])\n\n    from pyrocore import config\n    config.engine.open()\n    return config.engine"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef view(viewname='default', matcher=None,\n         config_dir=None, optional_config_files=None, cron_cfg=\"cron\"):\n    \"\"\"Helper for interactive / high-level API use.\"\"\"\n    return connect(config_dir, optional_config_files, cron_cfg).view(viewname, matcher)", "response": "Helper for interactive API use."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef setup(cls, cron_cfg=\"cron\"):\n        random.seed()\n        logging_cfg = cls.LOGGING_CFG\n        if \"%s\" in logging_cfg:\n            logging_cfg = logging_cfg % (cron_cfg if \"--cron\" in sys.argv[1:] else \"scripts\",)\n        logging_cfg = os.path.expanduser(logging_cfg)\n\n        if os.path.exists(logging_cfg):\n            logging.HERE = os.path.dirname(logging_cfg)\n            logging.config.fileConfig(logging_cfg)\n        else:\n            logging.basicConfig(level=logging.INFO)\n\n        logging.getLogger().debug(\"Logging config read from '%s'\" % logging_cfg)", "response": "Setup the logging environment."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntry to find package metadata.", "response": "def _get_pkg_meta(self):\n        \"\"\" Try to find package metadata.\n        \"\"\"\n        logger = logging.getLogger('pyrocore.scripts.base.version_info')\n        pkg_info = None\n        warnings = []\n\n        for info_ext, info_name in (('.dist-info', 'METADATA'), ('.egg-info', 'PKG-INFO')):\n            try:\n                # Development setup\n                pkg_path = os.path.join(\n                    __file__.split(__name__.replace('.', os.sep))[0],  # containing path\n                    __name__.split(\".\")[0]  # package name\n                )\n                if os.path.exists(pkg_path + info_ext):\n                    pkg_path += info_ext\n                else:\n                    globbed_paths = glob.glob(pkg_path + \"-*-py%d.%d\" % sys.version_info[:2] + info_ext)\n                    if len(globbed_paths) == 1:\n                        pkg_path = globbed_paths[0]\n                    elif globbed_paths:\n                        warnings.append(\"Found {} release-specific candidate versions in *{}\"\n                                        .format(len(globbed_paths), info_ext))\n                        pkg_path = None\n                    else:\n                        globbed_paths = glob.glob(pkg_path + \"-*\" + info_ext)\n                        if len(globbed_paths) == 1:\n                            pkg_path = globbed_paths[0]\n                        else:\n                            warnings.append(\"Found {} candidate versions in *{}\"\n                                            .format(len(globbed_paths), info_ext))\n                            pkg_path = None\n                if pkg_path:\n                    with open(os.path.join(pkg_path, info_name)) as handle:\n                        pkg_info = handle.read()\n                    break\n            except IOError:\n                continue\n\n        if not pkg_info:\n            logger.warn(\"Software version cannot be determined! ({})\".format(', '.join(warnings)))\n\n        return pkg_info or \"Version: 0.0.0\\n\""}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_bool_option(self, *args, **kwargs):\n        dest = [o for o in args if o.startswith(\"--\")][0].replace(\"--\", \"\").replace(\"-\", \"_\")\n        self.parser.add_option(dest=dest, action=\"store_true\", default=False,\n            help=kwargs['help'], *args)", "response": "Add a boolean option."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_value_option(self, *args, **kwargs):\n        kwargs['metavar'] = args[-1]\n        if 'dest' not in kwargs:\n            kwargs['dest'] = [o for o in args if o.startswith(\"--\")][0].replace(\"--\", \"\").replace(\"-\", \"_\")\n        if 'default' in kwargs and kwargs['default']:\n            kwargs['help'] += \" [%s]\" % kwargs['default']\n        self.parser.add_option(*args[:-1], **kwargs)", "response": "Add a value option."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef handle_completion(self):\n        # We don't want these in the help, so handle them explicitely\n        if len(sys.argv) > 1 and sys.argv[1].startswith(\"--help-completion-\"):\n            handler = getattr(self, sys.argv[1][2:].replace('-', '_'), None)\n            if handler:\n                print '\\n'.join(sorted(handler()))\n                self.STD_LOG_LEVEL = logging.DEBUG\n                sys.exit(error.EX_OK)", "response": "Handle shell completion stuff."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a generator of all options that can be used to complete the command.", "response": "def help_completion_options(self):\n        \"\"\" Return options of this command.\n        \"\"\"\n        for opt in self.parser.option_list:\n            for lopt in opt._long_opts:\n                yield lopt"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fatal(self, msg, exc=None):\n        if exc is not None:\n            self.LOG.fatal(\"%s (%s)\" % (msg, exc))\n            if self.options.debug:\n                return # let the caller re-raise it\n        else:\n            self.LOG.fatal(msg)\n        sys.exit(error.EX_SOFTWARE)", "response": "Exit on a fatal error."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run(self):\n        log_total = True\n\n        try:\n            try:\n                # Preparation steps\n                self.get_options()\n\n                # Template method with the tool's main loop\n                self.mainloop()\n            except error.LoggableError, exc:\n                if self.options.debug:\n                    raise\n\n                # Log errors caused by invalid user input\n                try:\n                    msg = str(exc)\n                except UnicodeError:\n                    msg = unicode(exc, \"UTF-8\")\n                self.LOG.error(msg)\n                sys.exit(error.EX_SOFTWARE)\n            except KeyboardInterrupt, exc:\n                if self.options.debug:\n                    raise\n\n                sys.stderr.write(\"\\n\\nAborted by CTRL-C!\\n\")\n                sys.stderr.flush()\n                sys.exit(error.EX_TEMPFAIL)\n            except IOError, exc:\n                # [Errno 32] Broken pipe?\n                if exc.errno == errno.EPIPE:\n                    sys.stderr.write(\"\\n%s, exiting!\\n\" % exc)\n                    sys.stderr.flush()\n\n                    # Monkey patch to prevent an exception during logging shutdown\n                    try:\n                        handlers = logging._handlerList\n                    except AttributeError:\n                        pass\n                    else:\n                        for handler in handlers:\n                            try:\n                                handler.flush = lambda *_: None\n                            except AttributeError:\n                                pass # skip special handlers\n\n                    log_total = False\n                    sys.exit(error.EX_IOERR)\n                else:\n                    raise\n        finally:\n            # Shut down\n            if log_total and self.options:  ## No time logging on --version and such\n                running_time = time.time() - self.startup\n                self.LOG.log(self.STD_LOG_LEVEL, \"Total time: %.3f seconds.\" % running_time)\n            logging.shutdown()\n\n        # Special exit code?\n        if self.return_code:\n            sys.exit(self.return_code)", "response": "Main entry point for the skeleton."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nscan arguments for a connection.", "response": "def check_for_connection(self):\n        \"\"\" Scan arguments for a `@name` one.\n        \"\"\"\n        for idx, arg in enumerate(self.args):\n            if arg.startswith('@'):\n                if arg[1:] not in config.connections:\n                    self.parser.error(\"Undefined connection '{}'!\".format(arg[1:]))\n                config.scgi_url = config.connections[arg[1:]]\n                self.LOG.debug(\"Switched to connection %s (%s)\", arg[1:], config.scgi_url)\n                del self.args[idx]\n                break"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds program options to the containing script.", "response": "def add_options(self):\n        \"\"\" Add program options, must be called in script's addOptions().\n        \"\"\"\n        # These options need to be conflict-free to the containing\n        # script, i.e. avoid short options if possible.\n        self.script.add_bool_option(\"-i\", \"--interactive\",\n            help=\"interactive mode (prompt before changing things)\")\n        self.script.add_bool_option(\"--yes\",\n            help=\"positively answer all prompts (e.g. --delete --yes)\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ask_bool(self, question, default=True):\n        if self.script.options.yes:\n            return True\n        elif self.script.options.dry_run or not self.script.options.interactive:\n            return default\n        else:\n            # Let the user decide\n            choice = '*'\n            while choice not in \"YNAQ\":\n                choice = raw_input(\"%s? [%s)es, %s)o, a)ll yes, q)uit]: \" % (\n                    fmt.to_console(question), \"yY\"[int(default)], \"Nn\"[int(default)],\n                ))\n                choice = choice[:1].upper() or \"NY\"[int(default)]\n\n            if choice == 'Q':\n                self.quit()\n            if choice == 'A':\n                self.script.options.yes = True\n                choice = 'Y'\n\n            return choice == 'Y'", "response": "Ask the user for Yes Nn or Q."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nexit the program due to user s choices.", "response": "def quit(self):\n        \"\"\" Exit the program due to user's choices.\n        \"\"\"\n        self.script.LOG.warn(\"Abort due to user choice!\")\n        sys.exit(self.QUIT_RC)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef redirect(req, _log=pymagic.get_lazy_logger(\"redirect\")):\n    log = req.environ.get(\"wsgilog.logger\", _log)\n    target = req.relative_url(req.urlvars.to)\n    log.info(\"Redirecting '%s' to '%s'\" % (req.url, target))\n    return exc.HTTPMovedPermanently(location=target)", "response": "Redirect controller to emit a HTTP 301."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef guarded(self, func, *args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except (EnvironmentError, error.LoggableError, xmlrpc.ERRORS) as g_exc:\n            if func.__name__ not in self.ERRORS_LOGGED:\n                self.LOG.warn(\"While calling '%s': %s\" % (func.__name__, g_exc))\n                self.ERRORS_LOGGED.add(func.__name__)\n            return None", "response": "Call a function and return None on errors."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef json_engine(self, req): # pylint: disable=R0201,W0613\n        try:\n            return stats.engine_data(config.engine)\n        except (error.LoggableError, xmlrpc.ERRORS) as torrent_exc:\n            raise exc.HTTPInternalServerError(str(torrent_exc))", "response": "Return the dictionary of the current engine data."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_route(cls, template):\n        regex = ''\n        last_pos = 0\n\n        for match in cls.ROUTES_RE.finditer(template):\n            regex += re.escape(template[last_pos:match.start()])\n            var_name = match.group(1)\n            expr = match.group(2) or '[^/]+'\n            expr = '(?P<%s>%s)' % (var_name, expr)\n            regex += expr\n            last_pos = match.end()\n\n        regex += re.escape(template[last_pos:])\n        regex = '^%s$' % regex\n\n        return re.compile(regex)", "response": "Parse a route definition and return the compiled regex that matches it."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_route(self, template, controller, **kwargs):\n        if isinstance(controller, basestring):\n            controller = pymagic.import_name(controller)\n\n        self.routes.append((self.parse_route(template), controller, kwargs))\n\n        return self", "response": "Add a route definition to the internal list of routes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsplit an interval into a series of event type and timestamp tuples.", "response": "def _interval_split(interval, only=None, context=None, event_re=re.compile(\"[A-Z][0-9]+\")):\n    \"\"\" Split C{interval} into a series of event type and timestamp tuples.\n        An exaple of the input is \"R1283008245P1283008268\".\n        Returns events in reversed order (latest first).\n    \"\"\"\n    def split_event(event):\n        \"Helper to parse events.\"\n        kind, val = event[:1], event[1:]\n        try:\n            return kind, float(val)\n        except (TypeError, ValueError):\n            return None, 0\n\n    if hasattr(interval, \"fetch\"):\n        interval = interval.fetch(\"custom_activations\")\n    ##import sys; print >>sys.stderr, \"!!!!!isplit\", interval, event_re.findall(interval)\n\n    return list(reversed([split_event(i) for i in event_re.findall(interval) if not only or i.startswith(only)]))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the sum of intervals between start and end.", "response": "def _interval_sum(interval, start=None, end=None, context=None):\n    \"\"\" Return sum of intervals between \"R\"esume and \"P\"aused events\n        in C{interval}, optionally limited by a time window defined\n        by C{start} and C{end}. Return ``None`` if there's no sensible\n        information.\n\n        C{interval} is a series of event types and timestamps,\n        e.g. \"R1283008245P1283008268\".\n    \"\"\"\n    end = float(end) if end else time.time()\n    events = _interval_split(interval, context=context)\n    result = []\n    ##import sys; print >>sys.stderr, \"!!!!!isum\", interval.fetch(\"custom_activations\"), events, start, end\n\n    while events:\n        event, resumed = events.pop()\n        ##print \"~~~~~~~~~~\", context, event, resumed\n\n        if event != \"R\":\n            # Ignore other events\n            continue\n        resumed = max(resumed, start or resumed)\n\n        if events: # Further events?\n            if not events[-1][0] == \"P\":\n                continue # If not followed by \"P\", it's not a valid interval\n            _, paused = events.pop()\n            paused = min(paused, end)\n        else:\n            # Currently active, ends at time window\n            paused = end\n\n        ##print \"~~~~~~~~~~ R: %r, P: %r\" % (resumed, paused)\n        ##print \"~~~~~~~~~~ I: %r\" % (paused - resumed)\n        if resumed >= paused:\n            # Ignore empty intervals\n            continue\n\n        result.append(paused - resumed)\n\n    return sum(result) if result else None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nproduce a file listing.", "response": "def _fmt_files(filelist):\n    \"\"\" Produce a file listing.\n    \"\"\"\n    depth = max(i.path.count('/') for i in filelist)\n    pad = ['\\uFFFE'] * depth\n\n    base_indent = ' ' * 38\n    indent = 0\n    result = []\n    prev_path = pad\n    sorted_files = sorted((i.path.split('/')[:-1]+pad, i.path.rsplit('/', 1)[-1], i) for i in filelist)\n\n    for path, name, fileinfo in sorted_files:\n        path = path[:depth]\n        if path != prev_path:\n            common = min([depth] + [idx\n                for idx, (dirname, prev_name) in enumerate(zip(path, prev_path))\n                if dirname != prev_name\n            ])\n            #result.append(\"!!%r %r\" % (indent, common))\n            #result.append(\"!!%r\" % (prev_path,))\n            #result.append(\"!!%r\" % (path,))\n\n            while indent > common:\n                indent -= 1\n                result.append(\"%s%s/\" % (base_indent, ' ' * indent))\n\n            for dirname in path[common:]:\n                if dirname == '\\uFFFE':\n                    break\n                result.append(\"%s%s\\\\ %s\" % (base_indent, ' ' * indent, dirname))\n                indent += 1\n\n        ##result.append(\"!!%r %r\" % (path, name))\n        result.append(\"  %s %s %s %s| %s\" % (\n            {0: \"off \", 1: \"    \", 2: \"high\"}.get(fileinfo.prio, \"????\"),\n            fmt.iso_datetime(fileinfo.mtime),\n            fmt.human_size(fileinfo.size),\n            ' ' * indent, name,\n        ))\n\n        prev_path = path\n\n    while indent > 0:\n        indent -= 1\n        result.append(\"%s%s/\" % (base_indent, ' ' * indent))\n    result.append(\"%s= %d file(s)\" % (base_indent, len(filelist)))\n\n    return '\\n'.join(result)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef detect_traits(item):\n    return traits.detect_traits(\n        name=item.name, alias=item.alias,\n        filetype=(list(item.fetch(\"kind_51\")) or [None]).pop(),\n    )", "response": "Build a list of traits from attributes of the passed item."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntry to find field C { name}. and return its description.", "response": "def lookup(cls, name):\n        \"\"\" Try to find field C{name}.\n\n            @return: Field descriptions, see C{matching.ConditionParser} for details.\n        \"\"\"\n        try:\n            field = cls.FIELDS[name]\n        except KeyError:\n            # Is it a custom attribute?\n            field = TorrentProxy.add_manifold_attribute(name)\n\n        return {\"matcher\": field._matcher} if field else None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nregistering a manifold engine attribute.", "response": "def add_manifold_attribute(cls, name):\n        \"\"\" Register a manifold engine attribute.\n\n            @return: field definition object, or None if \"name\" isn't a manifold attribute.\n        \"\"\"\n        if name.startswith(\"custom_\"):\n            try:\n                return FieldDefinition.FIELDS[name]\n            except KeyError:\n                field = OnDemandField(fmt.to_unicode, name, \"custom attribute %r\" % name.split('_', 1)[1],\n                    matcher=matching.PatternFilter)\n                setattr(cls, name, field) # add field to all proxy objects\n\n                return field\n        elif name.startswith(\"kind_\") and name[5:].isdigit():\n            try:\n                return FieldDefinition.FIELDS[name]\n            except KeyError:\n                limit = int(name[5:].lstrip('0') or '0', 10)\n                if limit > 100:\n                    raise error.UserError(\"kind_N: N > 100 in %r\" % name)\n                field = OnDemandField(set, name,\n                    \"kinds of files that make up more than %d%% of this item's size\" % limit,\n                    matcher=matching.TaggedAsFilter, formatter=_fmt_tags,\n                    engine_name=\"kind_%d\" % limit)\n                setattr(cls, name, field)\n\n                return field"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_custom_fields(cls, *args, **kw):\n        for factory in config.custom_field_factories:\n            for field in factory():\n                setattr(cls, field.name, field)", "response": "Add any custom fields defined in the configuration."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _check_hash_view(self):\n        infohash = None\n        if self.viewname.startswith('#'):\n            infohash = self.viewname[1:]\n        elif len(self.viewname) == 40:\n            try:\n                int(self.viewname, 16)\n            except (TypeError, ValueError):\n                pass\n            else:\n                infohash = self.viewname\n        return infohash", "response": "Check if the view name refers to a single item."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the total size of the view.", "response": "def size(self):\n        \"\"\" Total unfiltered size of view.\n        \"\"\"\n        #return len(self._fetch_items())\n        if self._check_hash_view():\n            return 1\n        else:\n            return self.engine.open().view.size(xmlrpc.NOHASH, self.viewname)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget list of download items.", "response": "def items(self):\n        \"\"\" Get list of download items.\n        \"\"\"\n        if self.matcher:\n            for item in self._fetch_items():\n                if self.matcher.match(item):\n                    yield item\n        else:\n            for item in self._fetch_items():\n                yield item"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef group_by(self, fields, items=None):\n        result = defaultdict(list)\n        if items is None:\n            items = self.items()\n\n        try:\n            key = operator.attrgetter(fields + '')\n        except TypeError:\n            def key(obj, names=tuple(fields)):\n                'Helper to return group key tuple'\n                return tuple(getattr(obj, x) for x in names)\n\n        for item in items:\n            result[key(item)].append(item)\n\n        return result", "response": "Returns a dict of lists of items grouped by the given fields."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting command mappings according to rTorrent version.", "response": "def _set_mappings(self):\n        \"\"\" Set command mappings according to rTorrent version.\n        \"\"\"\n        try:\n            self._versions = (self.system.client_version(), self.system.library_version(),)\n            self._version_info = tuple(int(i) for i in self._versions[0].split('.'))\n            self._use_deprecated = self._version_info < (0, 8, 7)\n\n            # Merge mappings for this version\n            self._mapping = self._mapping.copy()\n            for key, val in sorted(i for i in vars(config).items() if i[0].startswith(\"xmlrpc_\")):\n                map_version = tuple(int(i) for i in key.split('_')[1:])\n                if map_version <= self._version_info:\n                    if config.debug:\n                        self.LOG.debug(\"MAPPING for %r added: %r\" % (map_version, val))\n                    self._mapping.update(val)\n            self._fix_mappings()\n        except ERRORS as exc:\n            raise error.LoggableError(\"Can't connect to %s (%s)\" % (self._url, exc))\n\n        return self._versions, self._version_info"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _fix_mappings(self):\n        self._mapping.update((key+'=', val+'=') for key, val in self._mapping.items() if not key.endswith('='))\n\n        if config.debug:\n            self.LOG.debug(\"CMD MAPPINGS ARE: %r\" % (self._mapping,))", "response": "Add computed stuff to mappings.\n           "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmap old command names to new command names.", "response": "def _map_call(self, cmd):\n        \"\"\" Map old to new command names.\n        \"\"\"\n        if config.debug and cmd != self._mapping.get(cmd, cmd):\n            self.LOG.debug(\"MAP %s ==> %s\" % (cmd, self._mapping[cmd]))\n        cmd = self._mapping.get(cmd, cmd)\n\n        # These we do by code, to avoid lengthy lists in the config\n        if not self._use_deprecated and any(cmd.startswith(i) for i in (\"d.get_\", \"f.get_\", \"p.get_\", \"t.get_\")):\n            cmd = cmd[:2] + cmd[6:]\n\n        return cmd"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse the metafile and check pre - conditions.", "response": "def parse(self):\n        \"\"\" Parse metafile and check pre-conditions.\n        \"\"\"\n        try:\n            if not os.path.getsize(self.ns.pathname):\n                # Ignore 0-byte dummy files (Firefox creates these while downloading)\n                self.job.LOG.warn(\"Ignoring 0-byte metafile '%s'\" % (self.ns.pathname,))\n                return\n            self.metadata = metafile.checked_open(self.ns.pathname)\n        except EnvironmentError as exc:\n            self.job.LOG.error(\"Can't read metafile '%s' (%s)\" % (\n                self.ns.pathname, str(exc).replace(\": '%s'\" % self.ns.pathname, \"\"),\n            ))\n            return\n        except ValueError as exc:\n            self.job.LOG.error(\"Invalid metafile '%s': %s\" % (self.ns.pathname, exc))\n            return\n\n        self.ns.info_hash = metafile.info_hash(self.metadata)\n        self.ns.info_name = self.metadata[\"info\"][\"name\"]\n        self.job.LOG.info(\"Loaded '%s' from metafile '%s'\" % (self.ns.info_name, self.ns.pathname))\n\n        # Check whether item is already loaded\n        try:\n            name = self.job.proxy.d.name(self.ns.info_hash, fail_silently=True)\n        except xmlrpc.HashNotFound:\n            pass\n        except xmlrpc.ERRORS as exc:\n            if exc.faultString != \"Could not find info-hash.\":\n                self.job.LOG.error(\"While checking for #%s: %s\" % (self.ns.info_hash, exc))\n                return\n        else:\n            self.job.LOG.warn(\"Item #%s '%s' already added to client\" % (self.ns.info_hash, name))\n            return\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef addinfo(self):\n        # Basic values\n        self.ns.watch_path = self.job.config.path\n        self.ns.relpath = None\n        for watch in self.job.config.path:\n            if self.ns.pathname.startswith(watch.rstrip('/') + '/'):\n                self.ns.relpath = os.path.dirname(self.ns.pathname)[len(watch.rstrip('/'))+1:]\n                break\n\n        # Build indicator flags for target state from filename\n        flags = self.ns.pathname.split(os.sep)\n        flags.extend(flags[-1].split('.'))\n        self.ns.flags = set(i for i in flags if i)\n\n        # Metafile stuff\n        announce = self.metadata.get(\"announce\", None)\n        if announce:\n            self.ns.tracker_alias = configuration.map_announce2alias(announce)\n\n        main_file = self.ns.info_name\n        if \"files\" in self.metadata[\"info\"]:\n            main_file = list(sorted((i[\"length\"], i[\"path\"][-1])\n                for i in self.metadata[\"info\"][\"files\"]))[-1][1]\n        self.ns.filetype = os.path.splitext(main_file)[1]\n\n        # Add name traits\n        kind, info = traits.name_trait(self.ns.info_name, add_info=True)\n        self.ns.traits = Bunch(info)\n        self.ns.traits.kind = kind\n        self.ns.label = '/'.join(traits.detect_traits(\n            name=self.ns.info_name, alias=self.ns.tracker_alias, filetype=self.ns.filetype)).strip('/')\n\n        # Finally, expand commands from templates\n        self.ns.commands = []\n        for key, cmd in sorted(self.job.custom_cmds.items()):\n            try:\n                self.ns.commands.append(formatting.expand_template(cmd, self.ns))\n            except error.LoggableError as exc:\n                self.job.LOG.error(\"While expanding '%s' custom command: %s\" % (key, exc))", "response": "Add known facts to templating namespace."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load(self):\n        if not self.ns.info_hash and not self.parse():\n            return\n\n        self.addinfo()\n\n        # TODO: dry_run\n        try:\n            # TODO: Scrub metafile if requested\n\n            # Determine target state\n            start_it = self.job.config.load_mode.lower() in (\"start\", \"started\")\n            queue_it = self.job.config.queued\n\n            if \"start\" in self.ns.flags:\n                start_it = True\n            elif \"load\" in self.ns.flags:\n                start_it = False\n\n            if \"queue\" in self.ns.flags:\n                queue_it = True\n\n            # Load metafile into client\n            load_cmd = self.job.proxy.load.verbose\n            if queue_it:\n                if not start_it:\n                    self.ns.commands.append(\"d.priority.set=0\")\n            elif start_it:\n                load_cmd = self.job.proxy.load.start_verbose\n\n            self.job.LOG.debug(\"Templating values are:\\n    %s\" % \"\\n    \".join(\"%s=%s\" % (key, repr(val))\n                for key, val in sorted(self.ns.items())\n            ))\n\n            load_cmd(xmlrpc.NOHASH, self.ns.pathname, *tuple(self.ns.commands))\n            time.sleep(.05) # let things settle\n\n            # Announce new item\n            if not self.job.config.quiet:\n                msg = \"%s: Loaded '%s' from '%s/'%s%s\" % (\n                    self.job.__class__.__name__,\n                    fmt.to_utf8(self.job.proxy.d.name(self.ns.info_hash, fail_silently=True)),\n                    os.path.dirname(self.ns.pathname).rstrip(os.sep),\n                    \" [queued]\" if queue_it else \"\",\n                    (\" [startable]\"  if queue_it else \" [started]\") if start_it else \" [normal]\",\n                )\n                self.job.proxy.log(xmlrpc.NOHASH, msg)\n\n            # TODO: Evaluate fields and set client values\n            # TODO: Add metadata to tied file if requested\n\n            # TODO: Execute commands AFTER adding the item, with full templating\n            # Example: Labeling - add items to a persistent view, i.e. \"postcmd = view.set_visible={{label}}\"\n            #   could also be done automatically from the path, see above under \"flags\" (autolabel = True)\n            #   and add traits to the flags, too, in that case\n\n        except xmlrpc.ERRORS as exc:\n            self.job.LOG.error(\"While loading #%s: %s\" % (self.ns.info_hash, exc))", "response": "Load the item into the client."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nhandling a path - related event.", "response": "def handle_path(self, event):\n        \"\"\" Handle a path-related event.\n        \"\"\"\n        self.job.LOG.debug(\"Notification %r\" % event)\n        if event.dir:\n            return\n\n        if any(event.pathname.endswith(i) for i in self.METAFILE_EXT):\n            MetafileHandler(self.job, event.pathname).handle()\n        elif os.path.basename(event.pathname) == \"watch.ini\":\n            self.job.LOG.info(\"NOT YET Reloading watch config for '%s'\" % event.path)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprocess the default event.", "response": "def process_default(self, event):\n        \"\"\" Fallback.\n        \"\"\"\n        if self.job.LOG.isEnabledFor(logging.DEBUG):\n            # On debug level, we subscribe to ALL events, so they're expected in that case ;)\n            if self.job.config.trace_inotify:\n                self.job.LOG.debug(\"Ignored inotify event:\\n    %r\" % event)\n        else:\n            self.job.LOG.warning(\"Unexpected inotify event %r\" % event)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef setup(self):\n        if not pyinotify.WatchManager:\n            raise error.UserError(\"You need to install 'pyinotify' to use %s (%s)!\" % (\n                self.__class__.__name__, pyinotify._import_error)) # pylint: disable=E1101, W0212\n\n        self.manager = pyinotify.WatchManager()\n        self.handler = TreeWatchHandler(job=self)\n        self.notifier = pyinotify.AsyncNotifier(self.manager, self.handler)\n\n        if self.LOG.isEnabledFor(logging.DEBUG):\n            mask = pyinotify.ALL_EVENTS\n        else:\n            mask = pyinotify.IN_CLOSE_WRITE | pyinotify.IN_MOVED_TO # bogus pylint: disable=E1101\n\n        # Add all configured base dirs\n        for path in self.config.path:\n            self.manager.add_watch(path.strip(), mask, rec=True, auto_add=True)", "response": "Setup the inotify object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef mainloop(self):\n        # Print usage if not enough args or bad options\n        if len(self.args) < 1:\n            self.parser.error(\"You have to provide the root directory of your watch tree, or a metafile path!\")\n\n        configuration.engine.load_config()\n\n        pathname = os.path.abspath(self.args[0])\n        if os.path.isdir(pathname):\n            watch = TreeWatch(Bunch(path=pathname, job_name=\"watch\", active=True, dry_run=True, load_mode=None))\n            asyncore.loop(timeout=~0, use_poll=True)\n        else:\n            config = Bunch()\n            config.update(dict((key.split('.', 2)[-1], val)\n                for key, val in configuration.torque.items()\n                if key.startswith(\"job.treewatch.\")\n            ))\n            config.update(dict(path=os.path.dirname(os.path.dirname(pathname)), job_name=\"treewatch\", active=False, dry_run=True))\n            watch = TreeWatch(config)\n            handler = MetafileHandler(watch, pathname)\n\n            ok = handler.parse()\n            self.LOG.debug(\"Metafile '%s' would've %sbeen loaded\" % (pathname, \"\" if ok else \"NOT \"))\n\n            if ok:\n                handler.addinfo()\n                post_process = str if self.options.verbose else logutil.shorten\n                self.LOG.info(\"Templating values are:\\n    %s\" % \"\\n    \".join(\"%s=%s\" % (key, post_process(repr(val)))\n                    for key, val in sorted(handler.ns.items())\n                ))", "response": "The main loop for the main loop."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_filetypes(filelist, path=None, size=os.path.getsize):\n    path = path or (lambda _: _)\n\n    # Get total size for each file extension\n    histo = defaultdict(int)\n    for entry in filelist:\n        ext = os.path.splitext(path(entry))[1].lstrip('.').lower()\n        if ext and ext[0] == 'r' and ext[1:].isdigit():\n            ext = \"rar\"\n        elif ext == \"jpeg\":\n            ext = \"jpg\"\n        elif ext == \"mpeg\":\n            ext = \"mpg\"\n        histo[ext] += size(entry)\n\n    # Normalize values to integer percent\n    total = sum(histo.values())\n    if total:\n        for ext, val in histo.items():\n            histo[ext] = int(val * 100.0 / total + .499)\n\n    return sorted(zip(histo.values(), histo.keys()), reverse=True)", "response": "Get a sorted list of file types and their weight in descending order\n   "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef name_trait(name, add_info=False):\n    kind, info = None, {}\n\n    # Anything to check against?\n    if name and not name.startswith(\"VTS_\"):\n        lower_name = name.lower()\n        trait_patterns = ((\"tv\", TV_PATTERNS, \"show\"), (\"movie\", MOVIE_PATTERNS, \"title\"))\n\n        # TV check\n        if any(i in lower_name for i in _DEFINITELY_TV):\n            kind = \"tv\"\n            trait_patterns = trait_patterns[:1]\n\n        # Regex checks\n        re_name = '.'.join([i.lstrip('[(').rstrip(')]') for i in name.split(' .')])\n        for trait, patterns, title_group in trait_patterns:\n            matched, patname = None, None\n\n            for patname, pattern in patterns:\n                matched = pattern.match(re_name)\n                ##print matched, patname, re_name; print \"   \", pattern.pattern\n                if matched and not any(i in matched.groupdict()[title_group].lower() for i in BAD_TITLE_WORDS):\n                    kind, info = trait, matched.groupdict()\n                    break\n\n            if matched:\n                info[\"pattern\"] = patname\n\n                # Fold auxiliary groups into main one\n                for key, val in list(info.items()):\n                    if key[-1].isdigit():\n                        del info[key]\n                        if val:\n                            key = re.sub(\"[0-9]+$\", \"\", key)\n                            info[key] = (\"%s %s\" % (info.get(key) or \"\", val)).strip()\n                break\n\n        # TODO: Split by \"dvdrip\", year, etc. to get to the title and then\n        # do a imdb / tvdb lookup; cache results, hits for longer, misses\n        # for a day at max.\n\n    # Return requested result\n    return (kind, info) if add_info else kind", "response": "Determine content type from name."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a progress indicator for consoles if stdout is a tty.", "response": "def console_progress():\n    \"\"\" Return a progress indicator for consoles if\n        stdout is a tty.\n    \"\"\"\n    def progress(totalhashed, totalsize):\n        \"Helper\"\n        msg = \" \" * 30\n        if totalhashed < totalsize:\n            msg = \"%5.1f%% complete\" % (totalhashed * 100.0 / totalsize)\n        sys.stdout.write(msg + \" \\r\")\n        sys.stdout.flush()\n\n    try:\n        return progress if sys.stdout.isatty() else None\n    except AttributeError:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmasks any passkeys in an announce URL.", "response": "def mask_keys(announce_url):\n    \"\"\" Mask any passkeys (hex sequences) in an announce URL.\n    \"\"\"\n    return PASSKEY_RE.sub(\n        lambda m: m.group() if m.group() in PASSKEY_OK else \"*\" * len(m.group()),\n        announce_url)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nvalidating the info dict. Raise ValueError if validation fails.", "response": "def check_info(info):\n    \"\"\" Validate info dict.\n\n        Raise ValueError if validation fails.\n    \"\"\"\n    if not isinstance(info, dict):\n        raise ValueError(\"bad metainfo - not a dictionary\")\n\n    pieces = info.get(\"pieces\")\n    if not isinstance(pieces, basestring) or len(pieces) % 20 != 0:\n        raise ValueError(\"bad metainfo - bad pieces key\")\n\n    piece_size = info.get(\"piece length\")\n    if not isinstance(piece_size, (int, long)) or piece_size <= 0:\n        raise ValueError(\"bad metainfo - illegal piece length\")\n\n    name = info.get(\"name\")\n    if not isinstance(name, basestring):\n        raise ValueError(\"bad metainfo - bad name (type is %r)\" % type(name).__name__)\n    if not ALLOWED_ROOT_NAME.match(name):\n        raise ValueError(\"name %s disallowed for security reasons\" % name)\n\n    if (\"files\" in info) == (\"length\" in info):\n        raise ValueError(\"single/multiple file mix\")\n\n    if \"length\" in info:\n        length = info.get(\"length\")\n        if not isinstance(length, (int, long)) or length < 0:\n            raise ValueError(\"bad metainfo - bad length\")\n    else:\n        files = info.get(\"files\")\n        if not isinstance(files, (list, tuple)):\n            raise ValueError(\"bad metainfo - bad file list\")\n\n        for item in files:\n            if not isinstance(item, dict):\n                raise ValueError(\"bad metainfo - bad file value\")\n\n            length = item.get(\"length\")\n            if not isinstance(length, (int, long)) or length < 0:\n                raise ValueError(\"bad metainfo - bad length\")\n\n            path = item.get(\"path\")\n            if not isinstance(path, (list, tuple)) or not path:\n                raise ValueError(\"bad metainfo - bad path\")\n\n            for part in path:\n                if not isinstance(part, basestring):\n                    raise ValueError(\"bad metainfo - bad path dir\")\n                part = fmt.to_unicode(part)\n                if part == '..':\n                    raise ValueError(\"relative path in %s disallowed for security reasons\" % '/'.join(path))\n                if part and not ALLOWED_PATH_NAME.match(part):\n                    raise ValueError(\"path %s disallowed for security reasons\" % part)\n\n        file_paths = [os.sep.join(item[\"path\"]) for item in files]\n        if len(set(file_paths)) != len(file_paths):\n            raise ValueError(\"bad metainfo - duplicate path\")\n\n    return info"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_meta(meta):\n    if not isinstance(meta, dict):\n        raise ValueError(\"bad metadata - not a dictionary\")\n    if not isinstance(meta.get(\"announce\"), basestring):\n        raise ValueError(\"bad announce URL - not a string\")\n    check_info(meta.get(\"info\"))\n\n    return meta", "response": "Validate meta dict. Raise ValueError if validation fails."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nclean the given meta dictionary. Optionally log changes using the given logger.", "response": "def clean_meta(meta, including_info=False, logger=None):\n    \"\"\" Clean meta dict. Optionally log changes using the given logger.\n\n        @param logger: If given, a callable accepting a string message.\n        @return: Set of keys removed from C{meta}.\n    \"\"\"\n    modified = set()\n\n    for key in meta.keys():\n        if [key] not in METAFILE_STD_KEYS:\n            if logger:\n                logger(\"Removing key %r...\" % (key,))\n            del meta[key]\n            modified.add(key)\n\n    if including_info:\n        for key in meta[\"info\"].keys():\n            if [\"info\", key] not in METAFILE_STD_KEYS:\n                if logger:\n                    logger(\"Removing key %r...\" % (\"info.\" + key,))\n                del meta[\"info\"][key]\n                modified.add(\"info.\" + key)\n\n        for idx, entry in enumerate(meta[\"info\"].get(\"files\", [])):\n            for key in entry.keys():\n                if [\"info\", \"files\", key] not in METAFILE_STD_KEYS:\n                    if logger:\n                        logger(\"Removing key %r from file #%d...\" % (key, idx + 1))\n                    del entry[key]\n                    modified.add(\"info.files.\" + key)\n\n            # Remove crap that certain PHP software puts in paths\n            entry[\"path\"] = [i for i in entry[\"path\"] if i]\n\n    return modified"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sanitize(meta, diagnostics=False):\n    bad_encodings, bad_fields = set(), set()\n\n    def sane_encoding(field, text):\n        \"Transcoding helper.\"\n        for encoding in ('utf-8', meta.get('encoding', None), 'cp1252'):\n            if encoding:\n                try:\n                    u8_text = text.decode(encoding).encode(\"utf-8\")\n                    if encoding != 'utf-8':\n                        bad_encodings.add(encoding)\n                        bad_fields.add(field)\n                    return u8_text\n                except UnicodeError:\n                    continue\n        else:\n            # Broken beyond anything reasonable\n            bad_encodings.add('UNKNOWN/EXOTIC')\n            bad_fields.add(field)\n            return str(text, 'utf-8', 'replace').replace('\\ufffd', '_').encode(\"utf-8\")\n\n    # Go through all string fields and check them\n    for field in (\"comment\", \"created by\"):\n        if field in meta:\n            meta[field] = sane_encoding(field, meta[field])\n\n    meta[\"info\"][\"name\"] = sane_encoding('info name', meta[\"info\"][\"name\"])\n\n    for entry in meta[\"info\"].get(\"files\", []):\n        entry[\"path\"] = [sane_encoding('file path', i) for i in entry[\"path\"]]\n\n    return (meta, bad_encodings, bad_fields) if diagnostics else meta", "response": "Sanitize the given meta dictionary to ensure that it is valid."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef assign_fields(meta, assignments):\n    for assignment in assignments:\n        assignment = fmt.to_unicode(assignment)\n        try:\n            if '=' in assignment:\n                field, val = assignment.split('=', 1)\n            else:\n                field, val = assignment, None\n\n            if val and val[0] in \"+-\" and val[1:].isdigit():\n                val = int(val, 10)\n\n            # TODO: Allow numerical indices, and \"+\" for append\n            namespace = meta\n            keypath = [i.replace('\\0', '.') for i in field.replace('..', '\\0').split('.')]\n            for key in keypath[:-1]:\n                # Create missing dicts as we go...\n                namespace = namespace.setdefault(fmt.to_utf8(key), {})\n        except (KeyError, IndexError, TypeError, ValueError) as exc:\n            if self.options.debug:\n                raise\n            raise error.UserError(\"Bad assignment %r (%s)!\" % (assignment, exc))\n        else:\n            if val is None:\n                del namespace[fmt.to_utf8(keypath[-1])]\n            else:\n                namespace[fmt.to_utf8(keypath[-1])] = fmt.to_utf8(val)\n\n    return meta", "response": "Assigns the fields in assignments to the given metafile."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd fast resume data to a metafile dict.", "response": "def add_fast_resume(meta, datapath):\n    \"\"\" Add fast resume data to a metafile dict.\n    \"\"\"\n    # Get list of files\n    files = meta[\"info\"].get(\"files\", None)\n    single = files is None\n    if single:\n        if os.path.isdir(datapath):\n            datapath = os.path.join(datapath, meta[\"info\"][\"name\"])\n        files = [Bunch(\n            path=[os.path.abspath(datapath)],\n            length=meta[\"info\"][\"length\"],\n        )]\n\n    # Prepare resume data\n    resume = meta.setdefault(\"libtorrent_resume\", {})\n    resume[\"bitfield\"] = len(meta[\"info\"][\"pieces\"]) // 20\n    resume[\"files\"] = []\n    piece_length = meta[\"info\"][\"piece length\"]\n    offset = 0\n\n    for fileinfo in files:\n        # Get the path into the filesystem\n        filepath = os.sep.join(fileinfo[\"path\"])\n        if not single:\n            filepath = os.path.join(datapath, filepath.strip(os.sep))\n\n        # Check file size\n        if os.path.getsize(filepath) != fileinfo[\"length\"]:\n            raise OSError(errno.EINVAL, \"File size mismatch for %r [is %d, expected %d]\" % (\n                filepath, os.path.getsize(filepath), fileinfo[\"length\"],\n            ))\n\n        # Add resume data for this file\n        resume[\"files\"].append(dict(\n            priority=1,\n            mtime=int(os.path.getmtime(filepath)),\n            completed=(offset+fileinfo[\"length\"]+piece_length-1) // piece_length\n                     - offset // piece_length,\n        ))\n        offset += fileinfo[\"length\"]\n\n    return meta"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating the size of a torrent based on parsed metadata.", "response": "def data_size(metadata):\n    \"\"\" Calculate the size of a torrent based on parsed metadata.\n    \"\"\"\n    info = metadata['info']\n\n    if 'length' in info:\n        # Single file\n        total_size = info['length']\n    else:\n        # Directory structure\n        total_size = sum([f['length'] for f in info['files']])\n\n    return total_size"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef checked_open(filename, log=None, quiet=False):\n    with open(filename, \"rb\") as handle:\n        raw_data = handle.read()\n    data = bencode.bdecode(raw_data)\n\n    try:\n        check_meta(data)\n        if raw_data != bencode.bencode(data):\n            raise ValueError(\"Bad bencoded data - dict keys out of order?\")\n    except ValueError as exc:\n        if log:\n            # Warn about it, unless it's a quiet value query\n            if not quiet:\n                log.warn(\"%s: %s\" % (filename, exc))\n        else:\n            raise\n\n    return data", "response": "Open and validate the given metafile."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nformats the object using the pretty printer.", "response": "def format(self, obj, context, maxlevels, level):  # pylint: disable=arguments-differ\n        \"\"\" Mask obj if it looks like an URL, then pass it to the super class.\n        \"\"\"\n        if isinstance(obj, basestring) and \"://\" in fmt.to_unicode(obj):\n            obj = mask_keys(obj)\n        return pprint.PrettyPrinter.format(self, obj, context, maxlevels, level)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_datapath(self):\n        if self._datapath is None:\n            raise OSError(errno.ENOENT, \"You didn't provide any datapath for %r\" % self.filename)\n\n        return self._datapath", "response": "Get a valid datapath."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset a datapath s datapath s datapath s fifo flag.", "response": "def _set_datapath(self, datapath):\n        \"\"\" Set a datapath.\n        \"\"\"\n        if datapath:\n            self._datapath = datapath.rstrip(os.sep)\n            self._fifo = int(stat.S_ISFIFO(os.stat(self.datapath).st_mode))\n        else:\n            self._datapath = None\n            self._fifo = False"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef walk(self):\n        # FIFO?\n        if self._fifo:\n            if self._fifo > 1:\n                raise RuntimeError(\"INTERNAL ERROR: FIFO read twice!\")\n            self._fifo += 1\n\n            # Read paths relative to directory containing the FIFO\n            with open(self.datapath, \"r\") as fifo:\n                while True:\n                    relpath = fifo.readline().rstrip('\\n')\n                    if not relpath: # EOF?\n                        break\n                    self.LOG.debug(\"Read relative path %r from FIFO...\" % (relpath,))\n                    yield os.path.join(os.path.dirname(self.datapath), relpath)\n\n            self.LOG.debug(\"FIFO %r closed!\" % (self.datapath,))\n\n        # Directory?\n        elif os.path.isdir(self.datapath):\n            # Walk the directory tree\n            for dirpath, dirnames, filenames in os.walk(self.datapath): #, followlinks=True):\n                # Don't scan blacklisted directories\n                for bad in dirnames[:]:\n                    if any(fnmatch.fnmatch(bad, pattern) for pattern in self.ignore):\n                        dirnames.remove(bad)\n\n                # Yield all filenames that aren't blacklisted\n                for filename in filenames:\n                    if not any(fnmatch.fnmatch(filename, pattern) for pattern in self.ignore):\n                        #yield os.path.join(dirpath[len(self.datapath)+1:], filename)\n                        yield os.path.join(dirpath, filename)\n\n        # Single file\n        else:\n            # Yield the filename\n            yield self.datapath", "response": "Yields all paths in self. datapath."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _calc_size(self):\n        return sum(os.path.getsize(filename)\n            for filename in self.walk()\n        )", "response": "Calculate the size of the datapath."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate info dict for a file and a piece.", "response": "def _make_info(self, piece_size, progress, walker, piece_callback=None):\n        \"\"\" Create info dict.\n        \"\"\"\n        # These collect the file descriptions and piece hashes\n        file_list = []\n        pieces = []\n\n        # Initialize progress state\n        hashing_secs = time.time()\n        totalsize = -1 if self._fifo else self._calc_size()\n        totalhashed = 0\n\n        # Start a new piece\n        sha1sum = hashlib.sha1()\n        done = 0\n        filename = None\n\n        # Hash all files\n        for filename in walker:\n            # Assemble file info\n            filesize = os.path.getsize(filename)\n            filepath = filename[len(os.path.dirname(self.datapath) if self._fifo else self.datapath):].lstrip(os.sep)\n            file_list.append({\n                \"length\": filesize,\n                \"path\": [fmt.to_utf8(x) for x in fmt.to_unicode(filepath).replace(os.sep, '/').split('/')],\n            })\n            self.LOG.debug(\"Hashing %r, size %d...\" % (filename, filesize))\n\n            # Open file and hash it\n            fileoffset = 0\n            handle = open(filename, \"rb\")\n            try:\n                while fileoffset < filesize:\n                    # Read rest of piece or file, whatever is smaller\n                    chunk = handle.read(min(filesize - fileoffset, piece_size - done))\n                    sha1sum.update(chunk) # bogus pylint: disable=E1101\n                    done += len(chunk)\n                    fileoffset += len(chunk)\n                    totalhashed += len(chunk)\n\n                    # Piece is done\n                    if done == piece_size:\n                        pieces.append(sha1sum.digest()) # bogus pylint: disable=E1101\n                        if piece_callback:\n                            piece_callback(filename, pieces[-1])\n\n                        # Start a new piece\n                        sha1sum = hashlib.sha1()\n                        done = 0\n\n                    # Report progress\n                    if progress:\n                        progress(totalhashed, totalsize)\n            finally:\n                handle.close()\n\n        # Add hash of partial last piece\n        if done > 0:\n            pieces.append(sha1sum.digest()) # bogus pylint: disable=E1103\n            if piece_callback:\n                piece_callback(filename, pieces[-1])\n\n        # Build the meta dict\n        metainfo = {\n            \"pieces\": b\"\".join(pieces),\n            \"piece length\": piece_size,\n            \"name\": os.path.basename(self.datapath),\n        }\n\n        # Handle directory/FIFO vs. single file\n        if self._fifo or os.path.isdir(self.datapath):\n            metainfo[\"files\"] = file_list\n        else:\n            metainfo[\"length\"] = totalhashed\n\n        hashing_secs = time.time() - hashing_secs\n        self.LOG.info(\"Hashing of %s took %.1f secs (%s/s)\" % (\n            fmt.human_size(totalhashed).strip(), hashing_secs, fmt.human_size(totalhashed / hashing_secs).strip(),\n        ))\n\n        # Return validated info dict\n        return check_info(metainfo), totalhashed"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _make_meta(self, tracker_url, root_name, private, progress):\n        # Calculate piece size\n        if self._fifo:\n            # TODO we need to add a (command line) param, probably for total data size\n            # for now, always 1MB\n            piece_size_exp = 20\n        else:\n            total_size = self._calc_size()\n            if total_size:\n                piece_size_exp = int(math.log(total_size) / math.log(2)) - 9\n            else:\n                piece_size_exp = 0\n\n        piece_size_exp = min(max(15, piece_size_exp), 24)\n        piece_size = 2 ** piece_size_exp\n\n        # Build info hash\n        info, totalhashed = self._make_info(piece_size, progress, self.walk() if self._fifo else sorted(self.walk()))\n\n        # Enforce unique hash per tracker\n        info[\"x_cross_seed\"] = hashlib.md5(tracker_url).hexdigest()\n\n        # Set private flag\n        if private:\n            info[\"private\"] = 1\n\n        # Freely chosen root name (default is basename of the data path)\n        if root_name:\n            info[\"name\"] = root_name\n\n        # Torrent metadata\n        meta = {\n            \"info\": info,\n            \"announce\": tracker_url.strip(),\n        }\n\n        #XXX meta[\"encoding\"] = \"UTF-8\"\n\n        # Return validated meta dict\n        return check_meta(meta), totalhashed", "response": "Create a dictionary of torrent metadata."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a new metafile with the given path.", "response": "def create(self, datapath, tracker_urls, comment=None, root_name=None,\n                     created_by=None, private=False, no_date=False, progress=None,\n                     callback=None):\n        \"\"\" Create a metafile with the path given on object creation.\n            Returns the last metafile dict that was written (as an object, not bencoded).\n        \"\"\"\n        if datapath:\n            self.datapath = datapath\n\n        try:\n            tracker_urls = ['' + tracker_urls]\n        except TypeError:\n            tracker_urls = list(tracker_urls)\n        multi_mode = len(tracker_urls) > 1\n\n        # TODO add optimization so the hashing happens only once for multiple URLs!\n        for tracker_url in tracker_urls:\n            # Lookup announce URLs from config file\n            try:\n                if urlparse.urlparse(tracker_url).scheme:\n                    tracker_alias = urlparse.urlparse(tracker_url).netloc.split(':')[0].split('.')\n                    tracker_alias = tracker_alias[-2 if len(tracker_alias) > 1 else 0]\n                else:\n                    tracker_alias, tracker_url = config.lookup_announce_alias(tracker_url)\n                    tracker_url = tracker_url[0]\n            except (KeyError, IndexError):\n                raise error.UserError(\"Bad tracker URL %r, or unknown alias!\" % (tracker_url,))\n\n            # Determine metafile name\n            output_name = self.filename\n            if multi_mode:\n                # Add 2nd level of announce URL domain to metafile name\n                output_name = list(os.path.splitext(output_name))\n                try:\n                    output_name[1:1] = '-' + tracker_alias\n                except (IndexError,):\n                    self.LOG.error(\"Malformed announce URL %r, skipping!\" % (tracker_url,))\n                    continue\n                output_name = ''.join(output_name)\n\n            # Hash the data\n            self.LOG.info(\"Creating %r for %s %r...\" % (\n                output_name, \"filenames read from\" if self._fifo else \"data in\", self.datapath,\n            ))\n            meta, _ = self._make_meta(tracker_url, root_name, private, progress)\n\n            # Add optional fields\n            if comment:\n                meta[\"comment\"] = comment\n            if created_by:\n                meta[\"created by\"] = created_by\n            if not no_date:\n                meta[\"creation date\"] = int(time.time())\n            if callback:\n                callback(meta)\n\n            # Write metafile to disk\n            self.LOG.debug(\"Writing %r...\" % (output_name,))\n            bencode.bwrite(output_name, meta)\n\n        return meta"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck piece hashes of a metafile against the given datapath.", "response": "def check(self, metainfo, datapath, progress=None):\n        \"\"\" Check piece hashes of a metafile against the given datapath.\n        \"\"\"\n        if datapath:\n            self.datapath = datapath\n\n        def check_piece(filename, piece):\n            \"Callback for new piece\"\n            if piece != metainfo[\"info\"][\"pieces\"][check_piece.piece_index:check_piece.piece_index+20]:\n                self.LOG.warn(\"Piece #%d: Hashes differ in file %r\" % (check_piece.piece_index//20, filename))\n            check_piece.piece_index += 20\n        check_piece.piece_index = 0\n\n        datameta, _ = self._make_info(int(metainfo[\"info\"][\"piece length\"]), progress,\n            [datapath] if \"length\" in metainfo[\"info\"] else\n            (os.path.join(*([datapath] + i[\"path\"])) for i in metainfo[\"info\"][\"files\"]),\n            piece_callback=check_piece\n        )\n        return datameta[\"pieces\"] == metainfo[\"info\"][\"pieces\"]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef listing(self, masked=True):\n        # Assemble data\n        metainfo, bad_encodings, bad_fields = sanitize(bencode.bread(self.filename), diagnostics=True)\n        announce = metainfo['announce']\n        info = metainfo['info']\n        infohash = hashlib.sha1(bencode.bencode(info))\n\n        total_size = data_size(metainfo)\n        piece_length = info['piece length']\n        piece_number, last_piece_length = divmod(total_size, piece_length)\n\n        # Build result\n        result = [\n            \"NAME %s\" % (os.path.basename(fmt.to_unicode(self.filename))),\n            \"SIZE %s (%i * %s + %s)\" % (\n                fmt.human_size(total_size).strip(),\n                piece_number, fmt.human_size(piece_length).strip(),\n                fmt.human_size(last_piece_length).strip(),\n            ),\n            \"META %s (pieces %s %.1f%%)\" % (\n                fmt.human_size(os.path.getsize(self.filename)).strip(),\n                fmt.human_size(len(info[\"pieces\"])).strip(),\n                100.0 * len(info[\"pieces\"]) / os.path.getsize(self.filename),\n            ),\n            \"HASH %s\" % (infohash.hexdigest().upper()),\n            \"URL  %s\" % (mask_keys if masked else str)(announce),\n            \"PRV  %s\" % (\"YES (DHT/PEX disabled)\" if info.get(\"private\") else \"NO (DHT/PEX enabled)\"),\n            \"TIME %s\" % (\"N/A\" if \"creation date\" not in metainfo else\n                time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(metainfo[\"creation date\"]))\n            ),\n        ]\n\n        for label, key in ((\"BY  \", \"created by\"), (\"REM \", \"comment\")):\n            if key in metainfo:\n                result.append(\"%s %s\" % (label, metainfo.get(key, \"N/A\")))\n\n        result.extend([\n            \"\",\n            \"FILE LISTING%s\" % (\"\" if 'length' in info else \" [%d file(s)]\" % len(info['files']),),\n        ])\n        if 'length' in info:\n            # Single file\n            result.append(\"%-69s%9s\" % (\n                    fmt.to_unicode(info['name']),\n                    fmt.human_size(total_size),\n            ))\n        else:\n            # Directory structure\n            result.append(\"%s/\" % fmt.to_unicode(info['name']))\n            oldpaths = [None] * 99\n            for entry in info['files']:\n                # Remove crap that certain PHP software puts in paths\n                entry_path = [fmt.to_unicode(i) for i in entry[\"path\"] if i]\n\n                for idx, item in enumerate(entry_path[:-1]):\n                    if item != oldpaths[idx]:\n                        result.append(\"%s%s/\" % (' ' * (4*(idx+1)), item))\n                        oldpaths[idx] = item\n                result.append(\"%-69s%9s\" % (\n                    ' ' * (4*len(entry_path)) + entry_path[-1],\n                    fmt.human_size(entry['length']),\n                ))\n\n        if bad_encodings:\n            result.extend([\n                \"\",\n                \"WARNING: Bad encoding(s) {} in these fields: {}\"\n                .format(', '.join(sorted(bad_encodings)), ', '.join(sorted(bad_fields))),\n                \"Use the --raw option to inspect these encoding issues.\",\n            ])\n\n        return result", "response": "Return a list of formatted lines for a torrent."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _start(self, items):\n        # TODO: Filter by a custom date field, for scheduled downloads starting at a certain time, or after a given delay\n\n        # TODO: Don't start anything more if download BW is used >= config threshold in %\n\n        # Check if anything more is ready to start downloading\n        startable = [i for i in items if self.config.startable.match(i)]\n        if not startable:\n            self.LOG.debug(\"Checked %d item(s), none startable according to [ %s ]\",\n                           len(items), self.config.startable)\n            return\n\n        # Check intermission delay\n        now = time.time()\n        if now < self.last_start:\n            # compensate for summer time and other oddities\n            self.last_start = now\n        delayed = int(self.last_start + self.config.intermission - now)\n        if delayed > 0:\n            self.LOG.debug(\"Delaying start of {:d} item(s),\"\n                           \" due to {:d}s intermission with {:d}s left\"\n                           .format(len(startable), self.config.intermission, delayed))\n            return\n\n        # TODO: sort by priority, then loaded time\n\n        # Stick to \"start_at_once\" parameter, unless \"downloading_min\" is violated\n        downloading = [i for i in items if self.config.downloading.match(i)]\n        start_now = max(self.config.start_at_once, self.config.downloading_min - len(downloading))\n        start_now = min(start_now, len(startable))\n\n        #down_traffic = sum(i.down for i in downloading)\n        ##self.LOG.info(\"%d downloading, down %d\" % (len(downloading), down_traffic))\n\n        # Start eligible items\n        for idx, item in enumerate(startable):\n            # Check if we reached 'start_now' in this run\n            if idx >= start_now:\n                self.LOG.debug(\"Only starting %d item(s) in this run, %d more could be downloading\" % (\n                    start_now, len(startable)-idx,))\n                break\n\n            # TODO: Prevent start of more torrents that can fit on the drive (taking \"off\" files into account)\n            # (restarts items that were stopped due to the \"low_diskspace\" schedule, and also avoids triggering it at all)\n\n            # Only check the other conditions when we have `downloading_min` covered\n            if len(downloading) < self.config.downloading_min:\n                self.LOG.debug(\"Catching up from %d to a minimum of %d downloading item(s)\" % (\n                    len(downloading), self.config.downloading_min))\n            else:\n                # Limit to the given maximum of downloading items\n                if len(downloading) >= self.config.downloading_max:\n                    self.LOG.debug(\"Already downloading %d item(s) out of %d max, %d more could be downloading\" % (\n                        len(downloading), self.config.downloading_max, len(startable)-idx,))\n                    break\n\n            # If we made it here, start it!\n            self.last_start = now\n            downloading.append(item)\n            self.LOG.info(\"%s '%s' [%s, #%s]\" % (\n                \"WOULD start\" if self.config.dry_run else \"Starting\",\n                fmt.to_utf8(item.name), item.alias, item.hash))\n            if not self.config.dry_run:\n                item.start()\n                if not self.config.quiet:\n                    self.proxy.log(xmlrpc.NOHASH, \"%s: Started '%s' {%s}\" % (\n                        self.__class__.__name__, fmt.to_utf8(item.name), item.alias,\n                    ))", "response": "Start some items if conditions are met."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run(self):\n        try:\n            self.proxy = config_ini.engine.open()\n\n            # Get items from 'pyrotorque' view\n            items = list(config_ini.engine.items(self.VIEWNAME, cache=False))\n\n            if self.sort_key:\n                items.sort(key=self.sort_key)\n                #self.LOG.debug(\"Sorted: %r\" % [i.name for i in items])\n\n            # Handle found items\n            self._start(items)\n            self.LOG.debug(\"%s - %s\" % (config_ini.engine.engine_id, self.proxy))\n        except (error.LoggableError, xmlrpc.ERRORS) as exc:\n            # only debug, let the statistics logger do its job\n            self.LOG.debug(str(exc))", "response": "This method is called by the queue manager job callback."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef print_help_fields():\n    # Mock entries, so they fulfill the expectations towards a field definition\n    def custom_manifold():\n        \"named rTorrent custom attribute, e.g. 'custom_completion_target'\"\n        return (\"custom_KEY\", custom_manifold)\n    def kind_manifold():\n        \"file types that contribute at least N% to the item's total size\"\n        return (\"kind_N\", kind_manifold)\n\n    print('')\n    print(\"Fields are:\")\n    print(\"\\n\".join([\"  %-21s %s\" % (name, field.__doc__)\n        for name, field in sorted(engine.FieldDefinition.FIELDS.items() + [\n            custom_manifold(), kind_manifold(),\n        ])\n    ]))\n\n    print('')\n    print(\"Format specifiers are:\")\n    print(\"\\n\".join([\"  %-21s %s\" % (name, doc)\n        for name, doc in sorted(formatting.OutputMapping.formatter_help())\n    ]))\n    print('')\n    print(\"Append format specifiers using a '.' to field names in '-o' lists,\\n\"\n          \"e.g. 'size.sz' or 'completed.raw.delta'.\")", "response": "Print help about fields and field formatters."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_options(self):\n        super(RtorrentControl, self).add_options()\n\n        # basic options\n        self.add_bool_option(\"--help-fields\",\n            help=\"show available fields and their description\")\n        self.add_bool_option(\"-n\", \"--dry-run\",\n            help=\"don't commit changes, just tell what would happen\")\n        self.add_bool_option(\"--detach\",\n            help=\"run the process in the background\")\n        self.prompt.add_options()\n\n        # output control\n        self.add_bool_option(\"-S\", \"--shell\",\n            help=\"escape output following shell rules\")\n        self.add_bool_option(\"-0\", \"--nul\", \"--print0\",\n            help=\"use a NUL character instead of a linebreak after items\")\n        self.add_bool_option(\"-c\", \"--column-headers\",\n            help=\"print column headers\")\n        self.add_bool_option(\"-+\", \"--stats\",\n            help=\"add sum / avg / median of numerical fields\")\n        self.add_bool_option(\"--summary\",\n            help=\"print only statistical summary, without the items\")\n        #self.add_bool_option(\"-f\", \"--full\",\n        #    help=\"print full torrent details\")\n        self.add_bool_option(\"--json\",\n            help=\"dump all items as JSON (use '-o f1,f2,...' to specify fields)\")\n        self.add_value_option(\"-o\", \"--output-format\", \"FORMAT\",\n            help=\"specify display format (use '-o-' to disable item display)\")\n        self.add_value_option(\"-O\", \"--output-template\", \"FILE\",\n            help=\"pass control of output formatting to the specified template\")\n        self.add_value_option(\"-s\", \"--sort-fields\", \"[-]FIELD[,...] [-s...]\",\n            action='append', default=[],\n            help=\"fields used for sorting, descending if prefixed with a '-'; '-s*' uses output field list\")\n        self.add_bool_option(\"-r\", \"--reverse-sort\",\n            help=\"reverse the sort order\")\n        self.add_value_option(\"-A\", \"--anneal\", \"MODE [-A...]\",\n            type='choice', action='append', default=[],\n            choices=('dupes+', 'dupes-', 'dupes=', 'invert', 'unique'),\n            help=\"modify result set using some pre-defined methods\")\n        self.add_value_option(\"-/\", \"--select\", \"[N-]M\",\n            help=\"select result subset by item position (counting from 1)\")\n        self.add_bool_option(\"-V\", \"--view-only\",\n            help=\"show search result only in default ncurses view\")\n        self.add_value_option(\"--to-view\", \"--to\", \"NAME\",\n            help=\"show search result only in named ncurses view\")\n        self.add_bool_option(\"--append-view\", \"--append\",\n            help=\"DEPRECATED: use '--alter append' instead\")\n        self.add_value_option(\"--alter-view\", \"--alter\", \"MODE\",\n            type='choice', default=None, choices=self.ALTER_MODES,\n            help=\"alter view according to mode: {} (modifies -V and --to behaviour)\"\n                 .format(', '.join(self.ALTER_MODES)))\n        self.add_bool_option(\"--tee-view\", \"--tee\",\n            help=\"ADDITIONALLY show search results in ncurses view (modifies -V and --to behaviour)\")\n        self.add_value_option(\"--from-view\", \"--from\", \"NAME\",\n            help=\"select only items that are on view NAME (NAME can be an info hash to quickly select a single item)\")\n        self.add_value_option(\"-M\", \"--modify-view\", \"NAME\",\n            help=\"get items from given view and write result back to it (short-cut to combine --from-view and --to-view)\")\n        self.add_value_option(\"-Q\", \"--fast-query\", \"LEVEL\",\n            type='choice', default='=', choices=('=', '0', '1', '2'),\n            help=\"enable query optimization (=: use config; 0: off; 1: safe; 2: danger seeker)\")\n        self.add_value_option(\"--call\", \"CMD\",\n            help=\"call an OS command pattern in the shell\")\n        self.add_value_option(\"--spawn\", \"CMD [--spawn ...]\",\n            action=\"append\", default=[],\n            help=\"execute OS command pattern(s) directly\")\n# TODO: implement -S\n#        self.add_bool_option(\"-S\", \"--summary\",\n#            help=\"print statistics\")\n\n        # torrent state change (actions)\n        for action in self.ACTION_MODES:\n            action.setdefault(\"label\", action.name.upper())\n            action.setdefault(\"method\", action.name)\n            action.setdefault(\"interactive\", False)\n            action.setdefault(\"argshelp\", \"\")\n            action.setdefault(\"args\", ())\n            if action.argshelp:\n                self.add_value_option(*action.options + (action.argshelp,),\n                    **{\"help\": action.help + (\" (implies -i)\" if action.interactive else \"\")})\n            else:\n                self.add_bool_option(*action.options,\n                    **{\"help\": action.help + (\" (implies -i)\" if action.interactive else \"\")})\n        self.add_value_option(\"--ignore\", \"|\".join(self.IGNORE_OPTIONS),\n            type=\"choice\", choices=self.IGNORE_OPTIONS,\n            help=\"set 'ignore commands' status on torrent\")\n        self.add_value_option(\"--prio\", \"|\".join(self.PRIO_OPTIONS),\n            type=\"choice\", choices=self.PRIO_OPTIONS,\n            help=\"set priority of torrent\")\n        self.add_bool_option(\"-F\", \"--flush\", help=\"flush changes immediately (save session data)\")", "response": "Add options specific to the current instance of RtorrentControl."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn valid field names.", "response": "def help_completion_fields(self):\n        \"\"\" Return valid field names.\n        \"\"\"\n        for name, field in sorted(engine.FieldDefinition.FIELDS.items()):\n            if issubclass(field._matcher, matching.BoolFilter):\n                yield \"%s=no\" % (name,)\n                yield \"%s=yes\" % (name,)\n                continue\n            elif issubclass(field._matcher, matching.PatternFilter):\n                yield \"%s=\" % (name,)\n                yield \"%s=/\" % (name,)\n                yield \"%s=?\" % (name,)\n                yield \"%s=\\\"'*'\\\"\" % (name,)\n                continue\n            elif issubclass(field._matcher, matching.NumericFilterBase):\n                for i in range(10):\n                    yield \"%s=%d\" % (name, i)\n            else:\n                yield \"%s=\" % (name,)\n\n            yield r\"%s=+\" % (name,)\n            yield r\"%s=-\" % (name,)\n\n        yield \"custom_\"\n        yield \"kind_\""}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef emit(self, item, defaults=None, stencil=None, to_log=False, item_formatter=None):\n        item_text = self.format_item(item, defaults, stencil)\n\n        # Post-process line?\n        if item_formatter:\n            item_text = item_formatter(item_text)\n\n        # For a header, use configured escape codes on a terminal\n        if item is None and os.isatty(sys.stdout.fileno()):\n            item_text = ''.join((config.output_header_ecma48, item_text, \"\\x1B[0m\"))\n\n        # Dump to selected target\n        if to_log:\n            if callable(to_log):\n                to_log(item_text)\n            else:\n                self.LOG.info(item_text)\n        elif self.options.nul:\n            sys.stdout.write(item_text + '\\0')\n            sys.stdout.flush()\n        else:\n            print(item_text)\n\n        return item_text.count('\\n') + 1", "response": "Print an item to stdout or to the log on INFO level."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nvalidating the output format for the current locale.", "response": "def validate_output_format(self, default_format):\n        \"\"\" Prepare output format for later use.\n        \"\"\"\n        output_format = self.options.output_format\n\n        # Use default format if none is given\n        if output_format is None:\n            output_format = default_format\n\n        # Check if it's a custom output format from configuration\n        # (they take precedence over field names, so name them wisely)\n        output_format = config.formats.get(output_format, output_format)\n\n        # Expand plain field list to usable form\n        if re.match(r\"^[,._0-9a-zA-Z]+$\", output_format):\n            self.plain_output_format = True\n            output_format = \"%%(%s)s\" % \")s\\t%(\".join(formatting.validate_field_list(output_format, allow_fmt_specs=True))\n\n        # Replace some escape sequences\n        output_format = (output_format\n            .replace(r\"\\\\\", \"\\\\\")\n            .replace(r\"\\n\", \"\\n\")\n            .replace(r\"\\t\", \"\\t\")\n            .replace(r\"\\$\", \"\\0\") # the next 3 allow using $() instead of %()\n            .replace(\"$(\", \"%(\")\n            .replace(\"\\0\", \"$\")\n            .replace(r\"\\ \", \" \") # to prevent stripping in config file\n            #.replace(r\"\\\", \"\\\")\n        )\n\n        self.options.output_format = formatting.preparse(output_format)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_output_fields(self):\n        # Re-engineer list from output format\n        # XXX TODO: Would be better to use a FieldRecorder class to catch the full field names\n        emit_fields = list(i.lower() for i in re.sub(r\"[^_A-Z]+\", ' ', self.format_item(None)).split())\n\n        # Validate result\n        result = []\n        for name in emit_fields[:]:\n            if name not in engine.FieldDefinition.FIELDS:\n                self.LOG.warn(\"Omitted unknown name '%s' from statistics and output format sorting\" % name)\n            else:\n                result.append(name)\n\n        return result", "response": "Get field names from output template."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef validate_sort_fields(self):\n        sort_fields = ','.join(self.options.sort_fields)\n        if sort_fields == '*':\n            sort_fields = self.get_output_fields()\n\n        return formatting.validate_sort_fields(sort_fields or config.sort_fields)", "response": "Take care of sorting."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nshow search result in ncurses view.", "response": "def show_in_view(self, sourceview, matches, targetname=None):\n        \"\"\" Show search result in ncurses view.\n        \"\"\"\n        append = self.options.append_view or self.options.alter_view == 'append'\n        remove = self.options.alter_view == 'remove'\n        action_name = ', appending to' if append else ', removing from' if remove else ' into'\n        targetname = config.engine.show(matches,\n            targetname or self.options.to_view or \"rtcontrol\",\n            append=append, disjoin=remove)\n        msg = \"Filtered %d out of %d torrents using [ %s ]\" % (\n            len(matches), sourceview.size(), sourceview.matcher)\n        self.LOG.info(\"%s%s rTorrent view %r.\" % (msg, action_name, targetname))\n        config.engine.log(msg)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nperforms annealing of the items in the matches list.", "response": "def anneal(self, mode, matches, orig_matches):\n        \"\"\" Perform post-processing.\n\n            Return True when any changes were applied.\n        \"\"\"\n        changed = False\n\n        def dupes_in_matches():\n            \"\"\"Generator for index of matches that are dupes.\"\"\"\n            items_by_path = config.engine.group_by('realpath')\n            hashes = set([x.hash for x in matches])\n            for idx, item in enumerate(matches):\n                same_path_but_not_in_matches = any(\n                    x.hash not in hashes\n                    for x in items_by_path.get(item.realpath, [])\n                )\n                if item.realpath and same_path_but_not_in_matches:\n                    yield idx\n\n        if mode == 'dupes+':\n            items_by_path = config.engine.group_by('realpath')\n            hashes = set([x.hash for x in matches])\n            dupes = []\n            for item in matches:\n                if item.realpath:\n                    # Add all items with the same path that are missing\n                    for dupe in items_by_path.get(item.realpath, []):\n                        if dupe.hash not in hashes:\n                            changed = True\n                            dupes.append(dupe)\n                            hashes.add(dupe.hash)\n            matches.extend(dupes)\n        elif mode == 'dupes-':\n            for idx in reversed(list(dupes_in_matches())):\n                changed = True\n                del matches[idx]\n        elif mode == 'dupes=':\n            items_by_path = config.engine.group_by('realpath')\n            dupes = list(i for i in matches if i.realpath and len(items_by_path.get(i.realpath, [])) > 1)\n            if len(dupes) != len(matches):\n                changed = True\n                matches[:] = dupes\n        elif mode == 'invert':\n            hashes = set([x.hash for x in matches])\n            changed = True\n            matches[:] = list(i for i in orig_matches if i.hash not in hashes)\n        elif mode == 'unique':\n            seen, dupes = set(), []\n            for i, item in enumerate(matches):\n                if item.name in seen:\n                    changed = True\n                    dupes.append(i)\n                seen.add(item.name)\n            for i in reversed(dupes):\n                del matches[i]\n        else:\n            raise RuntimeError('Internal Error: Unknown anneal mode ' + mode)\n\n        return changed"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef mainloop(self):\n        # Print field definitions?\n        if self.options.help_fields:\n            self.parser.print_help()\n            print_help_fields()\n            sys.exit(1)\n\n        # Print usage if no conditions are provided\n        if not self.args:\n            self.parser.error(\"No filter conditions given!\")\n        self.check_for_connection()\n\n        # Check special action options\n        actions = []\n        if self.options.ignore:\n            actions.append(Bunch(name=\"ignore\", method=\"ignore\", label=\"IGNORE\" if int(self.options.ignore) else \"HEED\",\n                help=\"commands on torrent\", interactive=False, args=(self.options.ignore,)))\n        if self.options.prio:\n            actions.append(Bunch(name=\"prio\", method=\"set_prio\", label=\"PRIO\" + str(self.options.prio),\n                help=\"for torrent\", interactive=False, args=(self.options.prio,)))\n\n        # Check standard action options\n        # TODO: Allow certain combinations of actions (like --tag foo --stop etc.)\n        #       How do we get a sensible order of execution?\n        for action_mode in self.ACTION_MODES:\n            if getattr(self.options, action_mode.name):\n                if actions:\n                    self.parser.error(\"Options --%s and --%s are mutually exclusive\" % (\n                        \", --\".join(i.name.replace('_', '-') for i in actions),\n                        action_mode.name.replace('_', '-'),\n                    ))\n                if action_mode.argshelp:\n                    action_mode.args = (getattr(self.options, action_mode.name),)\n                actions.append(action_mode)\n        if not actions and self.options.flush:\n            actions.append(Bunch(name=\"flush\", method=\"flush\", label=\"FLUSH\",\n                help=\"flush session data\", interactive=False, args=()))\n            self.options.flush = False # No need to flush twice\n        if any(i.interactive for i in actions):\n            self.options.interactive = True\n\n        # Reduce results according to index range\n        selection = None\n        if self.options.select:\n            try:\n                if '-' in self.options.select:\n                    selection = tuple(int(i or default, 10) for i, default in\n                        zip(self.options.select.split('-', 1), (\"1\", \"-1\")))\n                else:\n                    selection = 1, int(self.options.select, 10)\n            except (ValueError, TypeError), exc:\n                self.fatal(\"Bad selection '%s' (%s)\" % (self.options.select, exc))\n\n#        print repr(config.engine)\n#        config.engine.open()\n#        print repr(config.engine)\n\n        # Preparation steps\n        if self.options.fast_query != '=':\n            config.fast_query = int(self.options.fast_query)\n        raw_output_format = self.options.output_format\n        default_output_format = \"default\"\n        if actions:\n            default_output_format = \"action_cron\" if self.options.cron else \"action\"\n        self.validate_output_format(default_output_format)\n        sort_key = self.validate_sort_fields()\n        matcher = matching.ConditionParser(engine.FieldDefinition.lookup, \"name\").parse(self.args)\n        self.LOG.debug(\"Matcher is: %s\" % matcher)\n\n        # Detach to background?\n        # This MUST happen before the next step, when we connect to the torrent client\n        if self.options.detach:\n            config.engine.load_config()\n            daemon_log = os.path.join(config.config_dir, \"log\", \"rtcontrol.log\")\n            osmagic.daemonize(logfile=daemon_log if os.path.exists(os.path.dirname(daemon_log)) else None)\n            time.sleep(.05) # let things settle a little\n\n        # View handling\n        if self.options.append_view and self.options.alter_view:\n            self.fatal(\"You cannot combine --append-view with --alter-view\")\n\n        if self.options.modify_view:\n            if self.options.from_view or self.options.to_view:\n                self.fatal(\"You cannot combine --modify-view with --from-view or --to-view\")\n            self.options.from_view = self.options.to_view = self.options.modify_view\n\n        # Find matching torrents\n        view = config.engine.view(self.options.from_view, matcher)\n        matches = list(view.items())\n        orig_matches = matches[:]\n        matches.sort(key=sort_key, reverse=self.options.reverse_sort)\n\n        if self.options.anneal:\n            if not self.options.quiet and set(self.options.anneal).difference(\n                                          set(['invert', 'unique'])):\n                if self.options.from_view not in (None, 'default'):\n                    self.LOG.warn(\"Mixing --anneal with a view other than 'default' might yield unexpected results!\")\n                if int(config.fast_query):\n                    self.LOG.warn(\"Using --anneal together with the query optimizer might yield unexpected results!\")\n            for mode in self.options.anneal:\n                if self.anneal(mode, matches, orig_matches):\n                    matches.sort(key=sort_key, reverse=self.options.reverse_sort)\n\n        if selection:\n            matches = matches[selection[0]-1:selection[1]]\n\n        if not matches:\n            # Think \"404 NOT FOUND\", but then exit codes should be < 256\n            self.return_code = 44\n\n        # Build header stencil\n        stencil = None\n        if self.options.column_headers and self.plain_output_format and matches:\n            stencil = fmt.to_console(formatting.format_item(\n                self.options.output_format, matches[0], self.FORMATTER_DEFAULTS)).split('\\t')\n\n        # Tee to ncurses view, if requested\n        if self.options.tee_view and (self.options.to_view or self.options.view_only):\n            self.show_in_view(view, matches)\n\n        # Generate summary?\n        summary = FieldStatistics(len(matches))\n        if self.options.stats or self.options.summary:\n            for field in self.get_output_fields():\n                try:\n                    0 + getattr(matches[0], field)\n                except (TypeError, ValueError, IndexError):\n                    summary.total[field] = ''\n                else:\n                    for item in matches:\n                        summary.add(field, getattr(item, field))\n\n        def output_formatter(templ, namespace=None):\n            \"Output formatting helper\"\n            full_ns = dict(\n                version=self.version,\n                proxy=config.engine.open(),\n                view=view,\n                query=matcher,\n                matches=matches,\n                summary=summary\n            )\n            full_ns.update(namespace or {})\n            return formatting.expand_template(templ, full_ns)\n\n        # Execute action?\n        if actions:\n            action = actions[0] # TODO: loop over it\n            self.LOG.log(logging.DEBUG if self.options.cron else logging.INFO, \"%s %s %d out of %d torrents.\" % (\n                \"Would\" if self.options.dry_run else \"About to\", action.label, len(matches), view.size(),\n            ))\n            defaults = {\"action\": action.label}\n            defaults.update(self.FORMATTER_DEFAULTS)\n\n            if self.options.column_headers and matches:\n                self.emit(None, stencil=stencil)\n\n            # Perform chosen action on matches\n            template_args = [formatting.preparse(\"{{#tempita}}\" + i if \"{{\" in i else i) for i in action.args]\n            for item in matches:\n                if not self.prompt.ask_bool(\"%s item %s\" % (action.label, item.name)):\n                    continue\n                if (self.options.output_format and not self.options.view_only\n                        and str(self.options.output_format) != \"-\"):\n                    self.emit(item, defaults, to_log=self.options.cron)\n\n                args = tuple([output_formatter(i, namespace=dict(item=item)) for i in template_args])\n\n                if self.options.dry_run:\n                    if self.options.debug:\n                        self.LOG.debug(\"Would call action %s(*%r)\" % (action.method, args))\n                else:\n                    getattr(item, action.method)(*args)\n                    if self.options.flush:\n                        item.flush()\n                    if self.options.view_only:\n                        show_in_client = lambda x: config.engine.open().log(xmlrpc.NOHASH, x)\n                        self.emit(item, defaults, to_log=show_in_client)\n\n        # Show in ncurses UI?\n        elif not self.options.tee_view and (self.options.to_view or self.options.view_only):\n            self.show_in_view(view, matches)\n\n        # Execute OS commands?\n        elif self.options.call or self.options.spawn:\n            if self.options.call and self.options.spawn:\n                self.fatal(\"You cannot mix --call and --spawn\")\n\n            template_cmds = []\n            if self.options.call:\n                template_cmds.append([formatting.preparse(\"{{#tempita}}\" + self.options.call)])\n            else:\n                for cmd in self.options.spawn:\n                    template_cmds.append([formatting.preparse(\"{{#tempita}}\" + i if \"{{\" in i else i)\n                                          for i in shlex.split(str(cmd))])\n\n            for item in matches:\n                cmds = [[output_formatter(i, namespace=dict(item=item)) for i in k] for k in template_cmds]\n                cmds = [[i.encode('utf-8') if isinstance(i, unicode) else i for i in k] for k in cmds]\n\n                if self.options.dry_run:\n                    self.LOG.info(\"Would call command(s) %r\" % (cmds,))\n                else:\n                    for cmd in cmds:\n                        if self.options.call:\n                            logged_cmd = cmd[0]\n                        else:\n                            logged_cmd = '\"%s\"' % ('\" \"'.join(cmd),)\n                        if self.options.verbose:\n                            self.LOG.info(\"Calling: %s\" % (logged_cmd,))\n                        try:\n                            if self.options.call:\n                                subprocess.check_call(cmd[0], shell=True)\n                            else:\n                                subprocess.check_call(cmd)\n                        except subprocess.CalledProcessError, exc:\n                            raise error.UserError(\"Command failed: %s\" % (exc,))\n                        except OSError, exc:\n                            raise error.UserError(\"Command failed (%s): %s\" % (logged_cmd, exc,))\n\n        # Dump as JSON array?\n        elif self.options.json:\n            json_data = matches\n            if raw_output_format:\n                json_fields = raw_output_format.split(',')\n                json_data = [dict([(name, getattr(i, name)) for name in json_fields])\n                             for i in matches]\n            json.dump(json_data, sys.stdout, indent=2, separators=(',', ': '),\n                      sort_keys=True, cls=pymagic.JSONEncoder)\n            sys.stdout.write('\\n')\n            sys.stdout.flush()\n\n        # Show via template?\n        elif self.options.output_template:\n            output_template = self.options.output_template\n            if not output_template.startswith(\"file:\"):\n                output_template = \"file:\" + output_template\n\n            sys.stdout.write(output_formatter(output_template))\n            sys.stdout.flush()\n\n        # Show on console?\n        elif self.options.output_format and str(self.options.output_format) != \"-\":\n            if not self.options.summary:\n                line_count = 0\n                for item in matches:\n                    # Emit a header line every 'output_header_frequency' lines\n                    if self.options.column_headers and line_count % config.output_header_frequency == 0:\n                        self.emit(None, stencil=stencil)\n\n                    # Print matching item\n                    line_count += self.emit(item, self.FORMATTER_DEFAULTS)\n\n            # Print summary?\n            if matches and summary:\n                self.emit(None, stencil=stencil)\n                self.emit(summary.total, item_formatter=lambda i: i.rstrip() + \" [SUM of %d item(s)]\" % len(matches))\n                self.emit(summary.min, item_formatter=lambda i: i.rstrip() + \" [MIN of %d item(s)]\" % len(matches))\n                self.emit(summary.average, item_formatter=lambda i: i.rstrip() + \" [AVG of %d item(s)]\" % len(matches))\n                self.emit(summary.max, item_formatter=lambda i: i.rstrip() + \" [MAX of %d item(s)]\" % len(matches))\n\n            self.LOG.info(\"Dumped %d out of %d torrents.\" % (len(matches), view.size(),))\n        else:\n            self.LOG.info(\"Filtered %d out of %d torrents.\" % (len(matches), view.size(),))\n\n        if self.options.debug and 0:\n            print '\\n' + repr(matches[0])\n            print '\\n' + repr(matches[0].files)\n\n        # XMLRPC stats\n        self.LOG.debug(\"XMLRPC stats: %s\" % config.engine._rpc)", "response": "Main loop for the main application."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef heatmap(self, df, imagefile):\n        import seaborn as sns\n        import matplotlib.ticker as tkr\n        import matplotlib.pyplot as plt\n        from  matplotlib.colors import LinearSegmentedColormap\n\n        sns.set()\n        with sns.axes_style('whitegrid'):\n            fig, ax = plt.subplots(figsize=(5, 11))  # inches\n\n            cmax = max(df[self.args[2]].max(), self.CMAP_MIN_MAX)\n            csteps = {\n                0.0: 'darkred', 0.3/cmax: 'red', 0.6/cmax: 'orangered', 0.9/cmax: 'coral',\n                1.0/cmax: 'skyblue', 1.5/cmax: 'blue', 1.9/cmax: 'darkblue',\n                2.0/cmax: 'darkgreen', 3.0/cmax: 'green',\n                (self.CMAP_MIN_MAX - .1)/cmax: 'palegreen', 1.0: 'yellow'}\n            cmap = LinearSegmentedColormap.from_list('RdGrYl', sorted(csteps.items()), N=256)\n\n            dataset = df.pivot(*self.args)\n\n            sns.heatmap(dataset, mask=dataset.isnull(), annot=False, linewidths=.5, square=True, ax=ax, cmap=cmap,\n                        annot_kws=dict(stretch='condensed'))\n            ax.tick_params(axis='y', labelrotation=30, labelsize=8)\n            # ax.get_yaxis().set_major_formatter(tkr.FuncFormatter(lambda x, p: x))\n            plt.savefig(imagefile)", "response": "Create the heat map."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _custom_fields():\n    # Import some commonly needed modules\n    import os\n    from pyrocore.torrent import engine, matching\n    from pyrocore.util import fmt\n\n    # PUT CUSTOM FIELD CODE HERE\n\n    # Disk space check (as an example)\n    # see https://pyrocore.readthedocs.io/en/latest/custom.html#has-room\n    def has_room(obj):\n        \"Check disk space.\"\n        pathname = obj.path\n        if pathname and not os.path.exists(pathname):\n            pathname = os.path.dirname(pathname)\n        if pathname and os.path.exists(pathname):\n            stats = os.statvfs(pathname)\n            return (stats.f_bavail * stats.f_frsize - int(diskspace_threshold_mb) * 1024**2\n                > obj.size * (1.0 - obj.done / 100.0))\n        else:\n            return None\n\n    yield engine.DynamicField(engine.untyped, \"has_room\",\n        \"check whether the download will fit on its target device\",\n        matcher=matching.BoolFilter, accessor=has_room,\n        formatter=lambda val: \"OK\" if val else \"??\" if val is None else \"NO\")\n    globals().setdefault(\"diskspace_threshold_mb\", \"500\")", "response": "Yields custom field definitions."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting important performance data and metadata from rTorrent.", "response": "def engine_data(engine):\n    \"\"\" Get important performance data and metadata from rTorrent.\n    \"\"\"\n    views = (\"default\", \"main\", \"started\", \"stopped\", \"complete\",\n             \"incomplete\", \"seeding\", \"leeching\", \"active\", \"messages\")\n    methods = [\n        \"throttle.global_up.rate\", \"throttle.global_up.max_rate\",\n        \"throttle.global_down.rate\", \"throttle.global_down.max_rate\",\n    ]\n\n    # Get data via multicall\n    proxy = engine.open()\n    calls = [dict(methodName=method, params=[]) for method in methods] \\\n          + [dict(methodName=\"view.size\", params=['', view]) for view in views]\n    result = proxy.system.multicall(calls, flatten=True)\n\n    # Build result object\n    data = dict(\n        now         = time.time(),\n        engine_id   = engine.engine_id,\n        versions    = engine.versions,\n        uptime      = engine.uptime,\n        upload      = [result[0], result[1]],\n        download    = [result[2], result[3]],\n        views       = dict([(name, result[4+i])\n            for i, name in enumerate(views)\n        ]),\n    )\n\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding command line options for the StuckTrackers command.", "response": "def add_options(self):\n        \"\"\" Add program options.\n        \"\"\"\n        super(StuckTrackers, self).add_options()\n\n        # basic options\n        self.add_bool_option(\"-a\", \"--all\",\n            help=\"list ALL items, not just stuck ones\")\n        self.add_bool_option(\"-s\", \"--stuck-only\",\n            help=\"list just stuck items / skip 'no enabled trackers' check\")\n        self.add_bool_option(\"-t\", \"--to-tagged\",\n            help=\"add stuck items to 'tagged' view\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwrites file with current process ID.", "response": "def _write_pidfile(pidfile):\n    \"\"\" Write file with current process ID.\n    \"\"\"\n    pid = str(os.getpid())\n    handle = open(pidfile, 'w')\n    try:\n        handle.write(\"%s\\n\" % pid)\n    finally:\n        handle.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads pid file and check process status.", "response": "def check_process(pidfile):\n    \"\"\" Read pid file and check process status.\n        Return (running, pid).\n    \"\"\"\n    # Check pid file\n    try:\n        handle = open(pidfile, 'r')\n    except IOError as exc:\n        if exc.errno == errno.ENOENT:\n            # pid file disappeared\n            return False, 0\n        raise\n\n    try:\n        pid = int(handle.read().strip(), 10)\n    except (TypeError, ValueError) as exc:\n        raise EnvironmentError(\"Invalid PID file '%s' (%s), won't start!\" % (pidfile, exc))\n    finally:\n        handle.close()\n\n    # Check process\n    try:\n        os.kill(pid, 0)\n    except EnvironmentError as exc:\n        return False, pid\n    else:\n        return True, pid"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks if a process is still running and if it is not raise an EnvironmentError when the process is still active.", "response": "def guard(pidfile, guardfile=None):\n    \"\"\" Raise an EnvironmentError when the \"guardfile\" doesn't exist, or\n        the process with the ID found in \"pidfile\" is still active.\n    \"\"\"\n    # Check guard\n    if guardfile and not os.path.exists(guardfile):\n        raise EnvironmentError(\"Guard file '%s' not found, won't start!\" % guardfile)\n\n    if os.path.exists(pidfile):\n        running, pid = check_process(pidfile)\n        if running:\n            raise EnvironmentError(\"Daemon process #%d still running, won't start!\" % pid)\n        else:\n            logging.getLogger(\"daemonize\").info(\"Process #%d disappeared, continuing...\" % pid)\n\n    # Keep race condition window small, by immediately writing launcher process ID\n    _write_pidfile(pidfile)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfork the process into the background.", "response": "def daemonize(pidfile=None, logfile=None, sync=True):\n    \"\"\" Fork the process into the background.\n\n        @param pidfile: Optional PID file path.\n        @param sync: Wait for parent process to disappear?\n        @param logfile: Optional name of stdin/stderr log file or stream.\n    \"\"\"\n    log = logging.getLogger(\"daemonize\")\n    ppid = os.getpid()\n\n    try:\n        pid = os.fork()\n        if pid > 0:\n            log.debug(\"Parent exiting (PID %d, CHILD %d)\" % (ppid, pid))\n            sys.exit(0)\n    except OSError as exc:\n        log.critical(\"fork #1 failed (PID %d): (%d) %s\\n\" % (os.getpid(), exc.errno, exc.strerror))\n        sys.exit(1)\n\n    ##os.chdir(\"/\")\n    ##os.umask(0022)\n    os.setsid()\n\n    try:\n        pid = os.fork()\n        if pid > 0:\n            log.debug(\"Session leader exiting (PID %d, PPID %d, DEMON %d)\" % (os.getpid(), ppid, pid))\n            sys.exit(0)\n    except OSError as exc:\n        log.critical(\"fork #2 failed (PID %d): (%d) %s\\n\" % (os.getpid(), exc.errno, exc.strerror))\n        sys.exit(1)\n\n    if pidfile:\n        _write_pidfile(pidfile)\n\n    def sig_term(*dummy):\n        \"Handler for SIGTERM.\"\n        sys.exit(0)\n\n    stdin = open(\"/dev/null\", \"r\")\n    os.dup2(stdin.fileno(), sys.stdin.fileno())\n    signal.signal(signal.SIGTERM, sig_term)\n\n    if logfile:\n        try:\n            logfile + \"\"\n        except TypeError:\n            if logfile.fileno() != sys.stdout.fileno():\n                os.dup2(logfile.fileno(), sys.stdout.fileno())\n            if logfile.fileno() != sys.stderr.fileno():\n                os.dup2(logfile.fileno(), sys.stderr.fileno())\n        else:\n            log.debug(\"Redirecting stdout / stderr to %r\" % logfile)\n            loghandle = open(logfile, \"a+\")\n            os.dup2(loghandle.fileno(), sys.stdout.fileno())\n            os.dup2(loghandle.fileno(), sys.stderr.fileno())\n            loghandle.close()\n\n    if sync:\n        # Wait for 5 seconds at most, in 10ms steps\n        polling = 5, .01\n        for _ in range(int(polling[0] * 1 / polling[1])):\n            try:\n                os.kill(ppid, 0)\n            except OSError:\n                break\n            else:\n                time.sleep(polling[1])\n\n    log.debug(\"Process detached (PID %d)\" % os.getpid())"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef flatten(nested, containers=(list, tuple)):\n    flat = list(nested) # handle iterators / generators\n    i = 0\n    while i < len(flat):\n        while isinstance(flat[i], containers):\n            if not flat[i]:\n                # kill empty list\n                flat.pop(i)\n\n                # inspect new 'i'th element in outer loop\n                i -= 1\n                break\n            else:\n                flat[i:i + 1] = (flat[i])\n\n        # 'i'th element is scalar, proceed\n        i += 1\n\n    return flat", "response": "Flatten a nested list in - place and return it."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates some doc pages automatically", "response": "def gendocs():\n    \"create some doc pages automatically\"\n    helppage = path(\"docs/references-cli-usage.rst\")\n    content = [\n        \".. automatically generated using 'paver gendocs'.\",\n        \"\",\n        \".. contents::\",\n        \"    :local:\",\n        \"\",\n        \".. note::\",\n        \"\",\n        \"    The help output presented here applies to version ``%s`` of the tools.\"\n            % sh(\"pyroadmin --version\", capture=True).split()[1],\n        \"\",\n    ]\n\n    for tool in sorted(project.entry_points[\"console_scripts\"]):\n        tool, _ = tool.split(None, 1)\n        content.extend([\n            \".. _cli-usage-%s:\" % tool,\n            \"\",\n            tool,\n            '^' * len(tool),\n            \"\",\n            \"::\",\n            \"\",\n        ])\n        help_opt = \"--help-fields --config-dir /tmp\" if tool == \"rtcontrol\" else \"--help\"\n        help_txt = sh(\"%s -q %s\" % (tool, help_opt), capture=True, ignore_error=True).splitlines()\n        content.extend('    ' + i for i in help_txt\n                       if ' on Python ' not in i and 'Copyright (c) 200' not in i\n                       and 'see the full documentation' not in i\n                       and '    https://pyrocore.readthedocs.io/' not in i)\n        content.extend([\n            \"\",\n        ])\n\n    content = [line.rstrip() for line in content if all(\n        i not in line for i in (\", Copyright (c) \", \"Total time: \", \"Configuration file '/tmp/\")\n    )]\n    content = [line for line, succ in zip(content, content[1:] + ['']) if line or succ] # filter twin empty lines\n    helppage.write_lines(content)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dist_docs():\n    \"create a documentation bundle\"\n    dist_dir = path(\"dist\")\n    html_dir = path(\"docs/_build/html\")\n    docs_package = path(\"%s/%s-%s-docs.zip\" % (dist_dir.abspath(), options.setup.name, options.setup.version))\n\n    if not html_dir.exists():\n        error(\"\\n*** ERROR: Please build the HTML docs!\")\n        sys.exit(1)\n\n    dist_dir.exists() or dist_dir.makedirs()\n    docs_package.exists() and docs_package.remove()\n\n    sh(r'cd %s && find . -type f \\! \\( -path \"*/.svn*\" -o -name \"*~\" \\) | sort'\n       ' | zip -qr -@ %s' % (html_dir, docs_package,))\n\n    print\n    print \"Upload @ http://pypi.python.org/pypi?:action=pkg_edit&name=%s\" % ( options.setup.name,)\n    print docs_package", "response": "create a documentation bundle"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets watchdog PID via netstat.", "response": "def watchdog_pid():\n    \"\"\"Get watchdog PID via ``netstat``.\"\"\"\n    result = sh('netstat -tulpn 2>/dev/null | grep 127.0.0.1:{:d}'\n                .format(SPHINX_AUTOBUILD_PORT), capture=True, ignore_error=True)\n    pid = result.strip()\n    pid = pid.split()[-1] if pid else None\n    pid = pid.split('/', 1)[0] if pid and pid != '-' else None\n\n    return pid"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef autodocs():\n    \"create Sphinx docs locally, and start a watchdog\"\n    build_dir = path('docs/_build')\n    index_html = build_dir / 'html/index.html'\n    if build_dir.exists():\n        build_dir.rmtree()\n\n    with pushd(\"docs\"):\n        print \"\\n*** Generating API doc ***\\n\"\n        sh(\"sphinx-apidoc -o apidoc -f -T -M ../src/pyrocore\")\n        sh(\"sphinx-apidoc -o apidoc -f -T -M $(dirname $(python -c 'import tempita; print(tempita.__file__)'))\")\n        print \"\\n*** Generating HTML doc ***\\n\"\n        sh('nohup %s/Makefile SPHINXBUILD=\"sphinx-autobuild -p %d'\n           ' -i \\'.*\\' -i \\'*.log\\' -i \\'*.png\\' -i \\'*.txt\\'\" html >autobuild.log 2>&1 &'\n           % (os.getcwd(), SPHINX_AUTOBUILD_PORT))\n\n    for i in range(25):\n        time.sleep(2.5)\n        pid = watchdog_pid()\n        if pid:\n            sh(\"touch docs/index.rst\")\n            sh('ps {}'.format(pid))\n            url = 'http://localhost:{port:d}/'.format(port=SPHINX_AUTOBUILD_PORT)\n            print(\"\\n*** Open '{}' in your browser...\".format(url))\n            break", "response": "create Sphinx docs locally and start a watchdog"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates coverage report and show in browser", "response": "def coverage():\n    \"generate coverage report and show in browser\"\n    coverage_index = path(\"build/coverage/index.html\")\n    coverage_index.remove()\n    sh(\"paver test\")\n    coverage_index.exists() and webbrowser.open(coverage_index)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking release before upload to PyPI", "response": "def release():\n    \"check release before upload to PyPI\"\n    sh(\"paver bdist_wheel\")\n    wheels = path(\"dist\").files(\"*.whl\")\n    if not wheels:\n        error(\"\\n*** ERROR: No release wheel was built!\")\n        sys.exit(1)\n    if any(\".dev\" in i for i in wheels):\n        error(\"\\n*** ERROR: You're still using a 'dev' version!\")\n        sys.exit(1)\n\n    # Check that source distribution can be built and is complete\n    print\n    print \"~~~ TESTING SOURCE BUILD\".ljust(78, '~')\n    sh( \"{ command cd dist/ && unzip -q %s-%s.zip && command cd %s-%s/\"\n        \"  && /usr/bin/python setup.py sdist >/dev/null\"\n        \"  && if { unzip -ql ../%s-%s.zip; unzip -ql dist/%s-%s.zip; }\"\n        \"        | cut -b26- | sort | uniq -c| egrep -v '^ +2 +' ; then\"\n        \"       echo '^^^ Difference in file lists! ^^^'; false;\"\n        \"    else true; fi; } 2>&1\"\n        % tuple([project[\"name\"], version] * 4)\n    )\n    path(\"dist/%s-%s\" % (project[\"name\"], version)).rmtree()\n    print \"~\" * 78\n\n    print\n    print \"~~~ sdist vs. git \".ljust(78, '~')\n    subprocess.call(\n        \"unzip -v dist/pyrocore-*.zip | egrep '^ .+/' | cut -f2- -d/ | sort >./build/ls-sdist.txt\"\n        \" && git ls-files | sort >./build/ls-git.txt\"\n        \" && $(which colordiff || echo diff) -U0 ./build/ls-sdist.txt ./build/ls-git.txt || true\", shell=True)\n    print \"~\" * 78\n\n    print\n    print \"Created\", \" \".join([str(i) for i in path(\"dist\").listdir()])\n    print \"Use 'paver sdist bdist_wheel' to build the release and\"\n    print \"    'twine upload dist/*.{zip,whl}' to upload to PyPI\"\n    print \"Use 'paver dist_docs' to prepare an API documentation upload\""}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlook up an announce alias name and its URL list.", "response": "def lookup_announce_alias(name):\n    \"\"\" Get canonical alias name and announce URL list for the given alias.\n    \"\"\"\n    for alias, urls in announce.items():\n        if alias.lower() == name.lower():\n            return alias, urls\n\n    raise KeyError(\"Unknown alias %s\" % (name,))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef map_announce2alias(url):\n    import urlparse\n\n    # Try to find an exact alias URL match and return its label\n    for alias, urls in announce.items():\n        if any(i == url for i in urls):\n            return alias\n\n    # Try to find an alias URL prefix and return its label\n    parts = urlparse.urlparse(url)\n    server = urlparse.urlunparse((parts.scheme, parts.netloc, \"/\", None, None, None))\n\n    for alias, urls in announce.items():\n        if any(i.startswith(server) for i in urls):\n            return alias\n\n    # Return 2nd level domain name if no alias found\n    try:\n        return '.'.join(parts.netloc.split(':')[0].split('.')[-2:])\n    except IndexError:\n        return parts.netloc", "response": "Map an announce URL to an alias."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nvalidate a configuration value.", "response": "def validate(key, val):\n    \"\"\" Validate a configuration value.\n    \"\"\"\n    if val and val.startswith(\"~/\"):\n        return os.path.expanduser(val)\n    if key == \"output_header_frequency\":\n        return int(val, 10)\n    if key.endswith(\"_ecma48\"):\n        return eval(\"'%s'\" % val.replace(\"'\", r\"\\'\"))  # pylint: disable=eval-used\n\n    return val"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef walk_resources(package_or_requirement, resource_name, recurse=True, base=''):\n    base = base.rstrip('/') + '/'\n    resource_base = (resource_name.rstrip('/') + '/' + base.strip('/')).rstrip('/')\n\n    # Create default configuration files\n    for filename in pymagic.resource_listdir(package_or_requirement, resource_base):\n        # Skip hidden and other trashy names\n        if filename.startswith('.') or any(filename.endswith(i) for i in (\".pyc\", \".pyo\", \"~\")):\n            continue\n\n        # Handle subdirectories\n        if pymagic.resource_isdir(package_or_requirement, resource_base + '/' + filename):\n            if recurse:\n                for i in walk_resources(package_or_requirement, resource_name, recurse, base=base + filename):\n                    yield i\n        else:\n            yield base + filename", "response": "Yields all files in the given resource directory recursively"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _update_config(self, namespace):  # pylint: disable=no-self-use\n        for key, val in namespace.items():\n            setattr(config, key, val)", "response": "Update the configuration with the items from the given dict into the configuration."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _interpolation_escape(self, namespace):\n        for key, val in namespace.items():\n            if '%' in val:\n                namespace[key] = self.INTERPOLATION_ESCAPE.sub(lambda match: '%' + match.group(0), val)", "response": "Re - escape interpolation strings."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _validate_namespace(self, namespace):\n        # Update config values (so other code can access them in the bootstrap phase)\n        self._update_config(namespace)\n\n        # Validate announce URLs\n        for key, val in namespace[\"announce\"].items():\n            if isinstance(val, basestring):\n                namespace[\"announce\"][key] = val.split()\n\n        # Re-escape output formats\n        self._interpolation_escape(namespace[\"formats\"])\n\n        # Create objects from module specs\n        for factory in (\"engine\",):\n            if isinstance(namespace[factory], basestring):\n                namespace[factory] = pymagic.import_name(namespace[factory])() if namespace[factory] else None\n\n        # Do some standard type conversions\n        for key in namespace:\n            # Split lists\n            if key.endswith(\"_list\") and isinstance(namespace[key], basestring):\n                namespace[key] = [i.strip() for i in namespace[key].replace(',', ' ').split()]\n\n            # Resolve factory and callback handler lists\n            elif any(key.endswith(i) for i in (\"_factories\", \"_callbacks\")) and isinstance(namespace[key], basestring):\n                namespace[key] = [pymagic.import_name(i.strip()) for i in namespace[key].replace(',', ' ').split()]\n\n        # Update config values again\n        self._update_config(namespace)", "response": "Validate the given namespace. This method is idempotent!"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncopies values from loaded INI file to namespace.", "response": "def _set_from_ini(self, namespace, ini_file):\n        \"\"\" Copy values from loaded INI file to namespace.\n        \"\"\"\n        # Isolate global values\n        global_vars = dict((key, val)\n            for key, val in namespace.items()\n            if isinstance(val, basestring)\n        )\n\n        # Copy all sections\n        for section in ini_file.sections():\n            # Get values set so far\n            if section == \"GLOBAL\":\n                raw_vars = global_vars\n            else:\n                raw_vars = namespace.setdefault(section.lower(), {})\n\n            # Override with values set in this INI file\n            raw_vars.update(dict(ini_file.items(section, raw=True)))\n\n            # Interpolate and validate all values\n            if section == \"FORMATS\":\n                self._interpolation_escape(raw_vars)\n            raw_vars.update(dict(\n                (key, validate(key, val))\n                for key, val in ini_file.items(section, vars=raw_vars)\n            ))\n\n        # Update global values\n        namespace.update(global_vars)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _set_defaults(self, namespace, optional_cfg_files):\n        # Add current configuration directory\n        namespace[\"config_dir\"] = self.config_dir\n\n        # Load defaults\n        for idx, cfg_file in enumerate([self.CONFIG_INI] + optional_cfg_files):\n            if any(i in cfg_file for i in set('/' + os.sep)):\n                continue # skip any non-plain filenames\n\n            try:\n                defaults = pymagic.resource_string(\"pyrocore\", \"data/config/\" + cfg_file) #@UndefinedVariable\n            except IOError as exc:\n                if idx and exc.errno == errno.ENOENT:\n                    continue\n                raise\n\n            ini_file = ConfigParser.SafeConfigParser()\n            ini_file.optionxform = str # case-sensitive option names\n            ini_file.readfp(StringIO.StringIO(defaults), \"<defaults>\")\n            self._set_from_ini(namespace, ini_file)", "response": "Set default values in the given dict."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload INI style configuration file into the given namespace.", "response": "def _load_ini(self, namespace, config_file):\n        \"\"\" Load INI style configuration.\n        \"\"\"\n        self.LOG.debug(\"Loading %r...\" % (config_file,))\n        ini_file = ConfigParser.SafeConfigParser()\n        ini_file.optionxform = str # case-sensitive option names\n        if ini_file.read(config_file):\n            self._set_from_ini(namespace, ini_file)\n        else:\n            self.LOG.warning(\"Configuration file %r not found,\"\n                             \" use the command 'pyroadmin --create-config' to create it!\" % (config_file,))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nload the scripted configuration.", "response": "def _load_py(self, namespace, config_file):\n        \"\"\" Load scripted configuration.\n        \"\"\"\n        if config_file and os.path.isfile(config_file):\n            self.LOG.debug(\"Loading %r...\" % (config_file,))\n            exec(compile(open(config_file).read(), config_file, 'exec'),  # pylint: disable=exec-used\n                 vars(config), namespace)\n        else:\n            self.LOG.warning(\"Configuration file %r not found!\" % (config_file,))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load(self, optional_cfg_files=None):\n        optional_cfg_files = optional_cfg_files or []\n\n        # Guard against coding errors\n        if self._loaded:\n            raise RuntimeError(\"INTERNAL ERROR: Attempt to load configuration twice!\")\n\n        try:\n            # Load configuration\n            namespace = {}\n            self._set_defaults(namespace, optional_cfg_files)\n\n            self._load_ini(namespace, os.path.join(self.config_dir, self.CONFIG_INI))\n\n            for cfg_file in optional_cfg_files:\n                if not os.path.isabs(cfg_file):\n                    cfg_file = os.path.join(self.config_dir, cfg_file)\n\n                if os.path.exists(cfg_file):\n                    self._load_ini(namespace, cfg_file)\n\n            self._validate_namespace(namespace)\n            self._load_py(namespace, namespace[\"config_script\"])\n            self._validate_namespace(namespace)\n\n            for callback in namespace[\"config_validator_callbacks\"]:\n                callback()\n        except ConfigParser.ParsingError as exc:\n            raise error.UserError(exc)\n\n        # Ready to go...\n        self._loaded = True", "response": "Load the configuation from either the default location or the given directory."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate default configuration files at either the default location or the given directory.", "response": "def create(self, remove_all_rc_files=False):\n        \"\"\" Create default configuration files at either the default location or the given directory.\n        \"\"\"\n        # Check and create configuration directory\n        if os.path.exists(self.config_dir):\n            self.LOG.debug(\"Configuration directory %r already exists!\" % (self.config_dir,))\n        else:\n            os.mkdir(self.config_dir)\n\n        if remove_all_rc_files:\n            for subdir in ('.', 'rtorrent.d'):\n                config_files = list(glob.glob(os.path.join(os.path.abspath(self.config_dir), subdir, '*.rc')))\n                config_files += list(glob.glob(os.path.join(os.path.abspath(self.config_dir), subdir, '*.rc.default')))\n                for config_file in config_files:\n                    self.LOG.info(\"Removing %r!\" % (config_file,))\n                    os.remove(config_file)\n\n        # Create default configuration files\n        for filepath in sorted(walk_resources(\"pyrocore\", \"data/config\")):\n            # Load from package data\n            text = pymagic.resource_string(\"pyrocore\", \"data/config\" + filepath)\n\n            # Create missing subdirs\n            config_file = self.config_dir + filepath\n            if not os.path.exists(os.path.dirname(config_file)):\n                os.makedirs(os.path.dirname(config_file))\n\n            # Write configuration files\n            config_trail = [\".default\"]\n            if os.path.exists(config_file):\n                self.LOG.debug(\"Configuration file %r already exists!\" % (config_file,))\n            else:\n                config_trail.append('')\n            for i in config_trail:\n                with open(config_file + i, \"w\") as handle:\n                    handle.write(text)\n                self.LOG.info(\"Configuration file %r written!\" % (config_file + i,))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a magnet - uri torrent metafile.", "response": "def make_magnet_meta(self, magnet_uri):\n        \"\"\" Create a magnet-uri torrent.\n        \"\"\"\n        import cgi\n        import hashlib\n\n        if magnet_uri.startswith(\"magnet:\"):\n            magnet_uri = magnet_uri[7:]\n        meta = {\"magnet-uri\": \"magnet:\" + magnet_uri}\n        magnet_params = cgi.parse_qs(magnet_uri.lstrip('?'))\n\n        meta_name = magnet_params.get(\"xt\", [hashlib.sha1(magnet_uri).hexdigest()])[0]\n        if \"dn\" in magnet_params:\n            meta_name = \"%s-%s\" % (magnet_params[\"dn\"][0], meta_name)\n        meta_name = re.sub(r\"[^-_,a-zA-Z0-9]+\", '.', meta_name).strip('.').replace(\"urn.btih.\", \"\")\n\n        if not config.magnet_watch:\n            self.fatal(\"You MUST set the 'magnet_watch' config option!\")\n        meta_path = os.path.join(config.magnet_watch, \"magnet-%s.torrent\" % meta_name)\n        self.LOG.debug(\"Writing magnet-uri metafile %r...\" % (meta_path,))\n\n        try:\n            bencode.bwrite(meta_path, meta)\n        except EnvironmentError as exc:\n            self.fatal(\"Error writing magnet-uri metafile %r (%s)\" % (meta_path, exc,))\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef mainloop(self):\n        if len(self.args) == 1 and \"=urn:btih:\" in self.args[0]:\n            # Handle magnet link\n            self.make_magnet_meta(self.args[0])\n            return\n\n        if not self.args:\n            self.parser.print_help()\n            self.parser.exit()\n        elif len(self.args) < 2:\n            self.parser.error(\"Expected a path and at least one announce URL, got: %s\" % (' '.join(self.args),))\n\n        # Create and configure metafile factory\n        datapath = self.args[0].rstrip(os.sep)\n        metapath = datapath\n        if self.options.output_filename:\n            metapath = self.options.output_filename\n            if os.path.isdir(metapath):\n                metapath = os.path.join(metapath, os.path.basename(datapath))\n        if not metapath.endswith(\".torrent\"):\n            metapath += \".torrent\"\n        torrent = metafile.Metafile(metapath)\n        torrent.ignore.extend(self.options.exclude)\n\n        def callback(meta):\n            \"Callback to set label and resume data.\"\n            if self.options.cross_seed:\n                if self.options.cross_seed == \"@entropy\":\n                    meta[\"info\"][\"entropy\"] = format(random.getrandbits(self.ENTROPY_BITS),\n                                                     'x').zfill(self.ENTROPY_BITS//4)\n                else:\n                    meta[\"info\"][\"x_cross_seed_label\"] = self.options.cross_seed\n            if self.options.no_cross_seed:\n                del meta[\"info\"][\"x_cross_seed\"]\n\n            # Set specific keys?\n            metafile.assign_fields(meta, self.options.set)\n\n        # Create and write the metafile(s)\n        # TODO: make it work better with multiple trackers (hash only once), also create fast-resume file for each tracker\n        meta = torrent.create(datapath, self.args[1:],\n            progress=None if self.options.quiet else metafile.console_progress(),\n            root_name=self.options.root_name, private=self.options.private, no_date=self.options.no_date,\n            comment=self.options.comment, created_by=\"PyroScope %s\" % self.version, callback=callback\n        )\n\n        # Create second metafile with fast-resume?\n        if self.options.hashed:\n            try:\n                metafile.add_fast_resume(meta, datapath)\n            except EnvironmentError as exc:\n                self.fatal(\"Error making fast-resume data (%s)\" % (exc,))\n                raise\n\n            hashed_path = re.sub(r\"\\.torrent$\", \"\", metapath) + \"-resume.torrent\"\n            self.LOG.info(\"Writing fast-resume metafile %r...\" % (hashed_path,))\n            try:\n                bencode.bwrite(hashed_path, meta)\n            except EnvironmentError as exc:\n                self.fatal(\"Error writing fast-resume metafile %r (%s)\" % (hashed_path, exc,))\n                raise", "response": "The main loop of the main process."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef import_name(module_spec, name=None):\n    # Load module\n    module_name = module_spec\n    if name is None:\n        try:\n            module_name, name = module_spec.split(':', 1)\n        except ValueError:\n            raise ValueError(\"Missing object specifier in %r (syntax: 'package.module:object.attr')\" % (module_spec,))\n\n    try:\n        module = __import__(module_name, globals(), {}, [name])\n    except ImportError as exc:\n        raise ImportError(\"Bad module name in %r (%s)\" % (module_spec, exc))\n\n    # Resolve the requested name\n    result = module\n    for attr in name.split('.'):\n        result = getattr(result, attr)\n\n    return result", "response": "Imports identifier C { name from module C { module_spec }."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_class_logger(obj):\n    return logging.getLogger(obj.__class__.__module__ + '.' + obj.__class__.__name__)", "response": "Get a logger specific for the given object s class."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsupport more object types.", "response": "def default(self, o):  # pylint: disable=method-hidden\n        \"\"\"Support more object types.\"\"\"\n        if isinstance(o, set):\n            return list(sorted(o))\n        elif hasattr(o, 'as_dict'):\n            return o.as_dict()\n        else:\n            return super(JSONEncoder, self).default(o)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nformat a byte sized value.", "response": "def fmt_sz(intval):\n    \"\"\" Format a byte sized value.\n    \"\"\"\n    try:\n        return fmt.human_size(intval)\n    except (ValueError, TypeError):\n        return \"N/A\".rjust(len(fmt.human_size(0)))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nformats a UNIX timestamp to an ISO datetime string.", "response": "def fmt_iso(timestamp):\n    \"\"\" Format a UNIX timestamp to an ISO datetime string.\n    \"\"\"\n    try:\n        return fmt.iso_datetime(timestamp)\n    except (ValueError, TypeError):\n        return \"N/A\".rjust(len(fmt.iso_datetime(0)))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fmt_duration(duration):\n    try:\n        return fmt.human_duration(float(duration), 0, 2, True)\n    except (ValueError, TypeError):\n        return \"N/A\".rjust(len(fmt.human_duration(0, 0, 2, True)))", "response": "Format a duration value in seconds to a readable form."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fmt_delta(timestamp):\n    try:\n        return fmt.human_duration(float(timestamp), precision=2, short=True)\n    except (ValueError, TypeError):\n        return \"N/A\".rjust(len(fmt.human_duration(0, precision=2, short=True)))", "response": "Format a UNIX timestamp to a delta."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreplace regex with string.", "response": "def fmt_subst(regex, subst):\n    \"\"\"Replace regex with string.\"\"\"\n    return lambda text: re.sub(regex, subst, text) if text else text"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef preparse(output_format):\n    try:\n        return templating.preparse(output_format, lambda path: os.path.join(config.config_dir, \"templates\", path))\n    except ImportError as exc:\n        if \"tempita\" in str(exc):\n            raise error.UserError(\"To be able to use Tempita templates, install the 'tempita' package (%s)\\n\"\n                \"    Possibly USING THE FOLLOWING COMMAND:\\n\"\n                \"        %s/easy_install tempita\" % (exc, os.path.dirname(sys.executable)))\n        raise\n    except IOError as exc:\n        raise error.LoggableError(\"Cannot read template: {}\".format(exc))", "response": "Do any special processing of a template and return the result."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nexpand the given template.", "response": "def expand_template(template, namespace):\n    \"\"\" Expand the given (preparsed) template.\n        Currently, only Tempita templates are supported.\n\n        @param template: The template, in preparsed form, or as a string (which then will be preparsed).\n        @param namespace: Custom namespace that is added to the predefined defaults\n            and takes precedence over those.\n        @return: The expanded template.\n        @raise LoggableError: In case of typical errors during template execution.\n    \"\"\"\n    # Create helper namespace\n    formatters = dict((name[4:], method)\n        for name, method in globals().items()\n        if name.startswith(\"fmt_\")\n    )\n    helpers = Bunch()\n    helpers.update(formatters)\n\n    # Default templating namespace\n    variables = dict(h=helpers, c=config.custom_template_helpers)\n    variables.update(formatters)  # redundant, for backwards compatibility\n\n    # Provided namespace takes precedence\n    variables.update(namespace)\n\n    # Expand template\n    try:\n        template = preparse(template)\n        return template.substitute(**variables)\n    except (AttributeError, ValueError, NameError, TypeError) as exc:\n        hint = ''\n        if \"column\" in str(exc):\n            try:\n                col = int(str(exc).split(\"column\")[1].split()[0])\n            except (TypeError, ValueError):\n                pass\n            else:\n                hint = \"%svVv\\n\" % (' ' * (col+4))\n\n        content = getattr(template, \"content\", template)\n        raise error.LoggableError(\"%s: %s in template:\\n%s%s\" % (\n            type(exc).__name__, exc, hint,\n            \"\\n\".join(\"%3d: %s\" % (i+1, line) for i, line in enumerate(content.splitlines()))\n        ))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nformats an item according to the given format specification.", "response": "def format_item(format_spec, item, defaults=None):\n    \"\"\" Format an item according to the given output format.\n        The format can be gioven as either an interpolation string,\n        or a Tempita template (which has to start with \"E{lb}E{lb}\"),\n\n        @param format_spec: The output format.\n        @param item: The object, which is automatically wrapped for interpolation.\n        @param defaults: Optional default values.\n    \"\"\"\n    template_engine = getattr(format_spec, \"__engine__\", None)\n\n    # TODO: Make differences between engines transparent\n    if template_engine == \"tempita\" or (not template_engine and format_spec.startswith(\"{{\")):\n        # Set item, or field names for column titles\n        namespace = dict(headers=not bool(item))\n        if item:\n            namespace[\"d\"] = item\n        else:\n            namespace[\"d\"] = Bunch()\n            for name in engine.FieldDefinition.FIELDS:\n                namespace[\"d\"][name] = name.upper()\n\n            # Justify headers to width of a formatted value\n            namespace.update((name[4:], lambda x, m=method: str(x).rjust(len(str(m(0)))))\n                for name, method in globals().items()\n                if name.startswith(\"fmt_\")\n            )\n\n        return expand_template(format_spec, namespace)\n    else:\n        # Interpolation\n        format_spec = getattr(format_spec, \"fmt\", format_spec)\n\n        if item is None:\n            # For headers, ensure we only have string formats\n            format_spec = re.sub(\n                r\"(\\([_.a-zA-Z0-9]+\\)[-#+0 ]?[0-9]*?)[.0-9]*[diouxXeEfFgG]\",\n                lambda m: m.group(1) + 's', format_spec)\n\n        return format_spec % OutputMapping(item, defaults)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef validate_field_list(fields, allow_fmt_specs=False, name_filter=None):\n    formats = [i[4:] for i in globals() if i.startswith(\"fmt_\")]\n\n    try:\n        fields = [i.strip() for i in fields.replace(',', ' ').split()]\n    except AttributeError:\n        # Not a string, expecting an iterable\n        pass\n\n    if name_filter:\n        fields = [name_filter(name) for name in fields]\n\n    for name in fields:\n        if allow_fmt_specs and '.' in name:\n            fullname = name\n            name, fmtspecs = name.split('.', 1)\n            for fmtspec in fmtspecs.split('.'):\n                if fmtspec not in formats and fmtspec != \"raw\":\n                    raise error.UserError(\"Unknown format specification %r in %r\" % (fmtspec, fullname))\n\n        if name not in engine.FieldDefinition.FIELDS and not engine.TorrentProxy.add_manifold_attribute(name):\n            raise error.UserError(\"Unknown field name %r\" % (name,))\n\n    return fields", "response": "Validate the list of fields in the given list."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef validate_sort_fields(sort_fields):\n    # Allow descending order per field by prefixing with '-'\n    descending = set()\n    def sort_order_filter(name):\n        \"Helper to remove flag and memoize sort order\"\n        if name.startswith('-'):\n            name = name[1:]\n            descending.add(name)\n        return name\n\n    # Split and validate field list\n    sort_fields = validate_field_list(sort_fields, name_filter=sort_order_filter)\n    log.debug(\"Sorting order is: %s\" % \", \".join([('-' if i in descending else '') + i\n        for i in sort_fields]))\n\n    # No descending fields?\n    if not descending:\n        return operator.attrgetter(*tuple(sort_fields))\n\n    # Need to provide complex key\n    class Key(object):\n        \"Complex sort order key\"\n        def __init__(self, obj, *args):\n            \"Remember object to be compared\"\n            self.obj = obj\n        def __lt__(self, other):\n            \"Compare to other key\"\n            for field in sort_fields:\n                lhs, rhs = getattr(self.obj, field), getattr(other.obj, field)\n                if lhs == rhs:\n                    continue\n                return rhs < lhs if field in descending else lhs < rhs\n            return False\n\n    return Key", "response": "Validate the given list of sort fields and return a key that can be used in the order of the elements in the list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list of format specifiers and their documentation.", "response": "def formatter_help(cls):\n        \"\"\" Return a list of format specifiers and their documentation.\n        \"\"\"\n        result = [(\"raw\", \"Switch off the default field formatter.\")]\n\n        for name, method in globals().items():\n            if name.startswith(\"fmt_\"):\n                result.append((name[4:], method.__doc__.strip()))\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninterprets a string as a time or time - like object and returns a dictionary with the keys hours minutes and seconds.", "response": "def _interpret_as_minutes(sval, mdict):\n    \"\"\"\n    Times like \"1:22\" are ambiguous; do they represent minutes and seconds\n    or hours and minutes?  By default, timeparse assumes the latter.  Call\n    this function after parsing out a dictionary to change that assumption.\n    \n    >>> import pprint\n    >>> pprint.pprint(_interpret_as_minutes('1:24', {'secs': '24', 'mins': '1'}))\n    {'hours': '1', 'mins': '24'}\n    \"\"\"\n    if (    sval.count(':') == 1 \n        and '.' not in sval\n        and (('hours' not in mdict) or (mdict['hours'] is None))\n        and (('days' not in mdict) or (mdict['days'] is None))\n        and (('weeks' not in mdict) or (mdict['weeks'] is None))\n        #and (('months' not in mdict) or (mdict['months'] is None))\n        #and (('years' not in mdict) or (mdict['years'] is None))\n        ):   \n        mdict['hours'] = mdict['mins']\n        mdict['mins'] = mdict['secs']\n        mdict.pop('secs')\n        pass\n    return mdict"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef timeparse(sval, granularity='seconds'):\n    '''\n    Parse a time expression, returning it as a number of seconds.  If\n    possible, the return value will be an `int`; if this is not\n    possible, the return will be a `float`.  Returns `None` if a time\n    expression cannot be parsed from the given string.\n\n    Arguments:\n    - `sval`: the string value to parse\n\n    >>> timeparse('1:24')\n    84\n    >>> timeparse(':22')\n    22\n    >>> timeparse('1 minute, 24 secs')\n    84\n    >>> timeparse('1m24s')\n    84\n    >>> timeparse('1.2 minutes')\n    72\n    >>> timeparse('1.2 seconds')\n    1.2\n\n    Time expressions can be signed.\n\n    >>> timeparse('- 1 minute')\n    -60\n    >>> timeparse('+ 1 minute')\n    60\n    \n    If granularity is specified as ``minutes``, then ambiguous digits following\n    a colon will be interpreted as minutes; otherwise they are considered seconds.\n    \n    >>> timeparse('1:30')\n    90\n    >>> timeparse('1:30', granularity='minutes')\n    5400\n    '''\n    match = COMPILED_SIGN.match(sval)\n    sign = -1 if match.groupdict()['sign'] == '-' else 1\n    sval = match.groupdict()['unsigned']\n    for timefmt in COMPILED_TIMEFORMATS:\n        match = timefmt.match(sval)\n        if match and match.group(0).strip():\n            mdict = match.groupdict()\n            if granularity == 'minutes':\n                mdict = _interpret_as_minutes(sval, mdict)\n            # if all of the fields are integer numbers\n            if all(v.isdigit() for v in list(mdict.values()) if v):\n                return sign * sum([MULTIPLIERS[k] * int(v, 10) for (k, v) in\n                            list(mdict.items()) if v is not None])\n            # if SECS is an integer number\n            elif ('secs' not in mdict or\n                  mdict['secs'] is None or\n                  mdict['secs'].isdigit()):\n                # we will return an integer\n                return (\n                    sign * int(sum([MULTIPLIERS[k] * float(v) for (k, v) in\n                             list(mdict.items()) if k != 'secs' and v is not None])) +\n                    (int(mdict['secs'], 10) if mdict['secs'] else 0))\n            else:\n                # SECS is a float, we will return a float\n                return sign * sum([MULTIPLIERS[k] * float(v) for (k, v) in\n                            list(mdict.items()) if v is not None])", "response": "Parse a time expression returning it as a number of seconds."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a singleton instance of BigQueryClient.", "response": "def get_client(project_id=None, credentials=None,\n               service_url=None, service_account=None,\n               private_key=None, private_key_file=None,\n               json_key=None, json_key_file=None,\n               readonly=True, swallow_results=True,\n               num_retries=0):\n    \"\"\"Return a singleton instance of BigQueryClient. Either\n    AssertionCredentials or a service account and private key combination need\n    to be provided in order to authenticate requests to BigQuery.\n\n    Parameters\n    ----------\n    project_id : str, optional\n        The BigQuery project id, required unless json_key or json_key_file is\n        provided.\n    credentials : oauth2client.client.SignedJwtAssertionCredentials, optional\n        AssertionCredentials instance to authenticate requests to BigQuery\n        (optional, must provide `service_account` and (`private_key` or\n        `private_key_file`) or (`json_key` or `json_key_file`) if not included\n    service_url : str, optional\n        A URI string template pointing to the location of Google's API\n        discovery service. Requires two parameters {api} and {apiVersion} that\n        when filled in produce an absolute URI to the discovery document for\n        that service. If not set then the default googleapiclient discovery URI\n        is used. See `credentials`\n    service_account : str, optional\n        The Google API service account name. See `credentials`\n    private_key : str, optional\n        The private key associated with the service account in PKCS12 or PEM\n        format. See `credentials`\n    private_key_file : str, optional\n        The name of the file containing the private key associated with the\n        service account in PKCS12 or PEM format. See `credentials`\n    json_key : dict, optional\n        The JSON key associated with the service account. See `credentials`\n    json_key_file : str, optional\n        The name of the JSON key file associated with the service account. See\n        `credentials`.\n    readonly : bool\n        Bool indicating if BigQuery access is read-only. Has no effect if\n        credentials are provided. Default True.\n    swallow_results : bool\n        If set to False, then return the actual response value instead of\n        converting to boolean. Default True.\n    num_retries : int, optional\n        The number of times to retry the request. Default 0 (no retry).\n\n\n    Returns\n    -------\n    BigQueryClient\n        An instance of the BigQuery client.\n    \"\"\"\n\n    if not credentials:\n        assert (service_account and (private_key or private_key_file)) or (\n            json_key or json_key_file), \\\n            'Must provide AssertionCredentials or service account and P12 key\\\n            or JSON key'\n\n    if not project_id:\n        assert json_key or json_key_file, \\\n            'Must provide project_id unless json_key or json_key_file is\\\n            provided'\n\n    if service_url is None:\n        service_url = DISCOVERY_URI\n\n    scope = BIGQUERY_SCOPE_READ_ONLY if readonly else BIGQUERY_SCOPE\n\n    if private_key_file:\n        credentials = _credentials().from_p12_keyfile(service_account,\n                                                      private_key_file,\n                                                      scopes=scope)\n\n    if private_key:\n        try:\n            if isinstance(private_key, basestring):\n                private_key = private_key.decode('utf-8')\n        except NameError:\n            # python3 -- private_key is already unicode\n            pass\n        credentials = _credentials().from_p12_keyfile_buffer(\n            service_account,\n            StringIO(private_key),\n            scopes=scope)\n\n    if json_key_file:\n        with open(json_key_file, 'r') as key_file:\n            json_key = json.load(key_file)\n\n    if json_key:\n        credentials = _credentials().from_json_keyfile_dict(json_key,\n                                                            scopes=scope)\n        if not project_id:\n            project_id = json_key['project_id']\n\n    bq_service = _get_bq_service(credentials=credentials,\n                                 service_url=service_url)\n\n    return BigQueryClient(bq_service, project_id, swallow_results,\n                          num_retries)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngives the BigQuery service return data about all projects.", "response": "def get_projects(bq_service):\n    \"\"\"Given the BigQuery service, return data about all projects.\"\"\"\n    projects_request = bq_service.projects().list().execute()\n\n    projects = []\n    for project in projects_request.get('projects', []):\n        project_data = {\n            'id': project['id'],\n            'name': project['friendlyName']\n        }\n        projects.append(project_data)\n    return projects"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconstruct an authorized BigQuery service object.", "response": "def _get_bq_service(credentials=None, service_url=None):\n    \"\"\"Construct an authorized BigQuery service object.\"\"\"\n\n    assert credentials, 'Must provide ServiceAccountCredentials'\n\n    http = credentials.authorize(Http())\n    service = build(\n        'bigquery',\n        'v2',\n        http=http,\n        discoveryServiceUrl=service_url,\n        cache_discovery=False\n    )\n\n    return service"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsubmit a query job to BigQuery.", "response": "def _submit_query_job(self, query_data):\n        \"\"\" Submit a query job to BigQuery.\n\n            This is similar to BigQueryClient.query, but gives the user\n            direct access to the query method on the offical BigQuery\n            python client.\n\n            For fine-grained control over a query job, see:\n            https://google-api-client-libraries.appspot.com/documentation/bigquery/v2/python/latest/bigquery_v2.jobs.html#query\n\n        Parameters\n        ----------\n        query_data\n            query object as per \"configuration.query\" in\n            https://cloud.google.com/bigquery/docs/reference/v2/jobs#configuration.query\n\n        Returns\n        -------\n        tuple\n            job id and query results if query completed. If dry_run is True,\n            job id will be None and results will be empty if the query is valid\n            or a dict containing the response if invalid.\n\n        Raises\n        ------\n        BigQueryTimeoutException\n            On timeout\n        \"\"\"\n\n        logger.debug('Submitting query job: %s' % query_data)\n\n        job_collection = self.bigquery.jobs()\n\n        try:\n            query_reply = job_collection.query(\n                projectId=self.project_id, body=query_data).execute(\n                num_retries=self.num_retries)\n        except HttpError as e:\n            if query_data.get(\"dryRun\", False):\n                return None, json.loads(e.content.decode('utf8'))\n            raise\n\n        job_id = query_reply['jobReference'].get('jobId')\n        schema = query_reply.get('schema', {'fields': None})['fields']\n        rows = query_reply.get('rows', [])\n        job_complete = query_reply.get('jobComplete', False)\n\n        # raise exceptions if it's not an async query\n        # and job is not completed after timeout\n        if not job_complete and query_data.get(\"timeoutMs\", False):\n            logger.error('BigQuery job %s timeout' % job_id)\n            raise BigQueryTimeoutException()\n\n        return job_id, [self._transform_row(row, schema) for row in rows]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsubmits a job to BigQuery.", "response": "def _insert_job(self, body_object):\n        \"\"\" Submit a job to BigQuery\n\n            Direct proxy to the insert() method of the offical BigQuery\n            python client.\n\n            Able to submit load, link, query, copy, or extract jobs.\n\n            For more details, see:\n            https://google-api-client-libraries.appspot.com/documentation/bigquery/v2/python/latest/bigquery_v2.jobs.html#insert\n\n        Parameters\n        ----------\n        body_object : body object passed to bigquery.jobs().insert()\n\n        Returns\n        -------\n        response of the bigquery.jobs().insert().execute() call\n\n        Raises\n        ------\n        BigQueryTimeoutException on timeout\n        \"\"\"\n\n        logger.debug('Submitting job: %s' % body_object)\n\n        job_collection = self.bigquery.jobs()\n\n        return job_collection.insert(\n            projectId=self.project_id,\n            body=body_object\n        ).execute(num_retries=self.num_retries)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef query(self, query, max_results=None, timeout=0, dry_run=False, use_legacy_sql=None, external_udf_uris=None):\n\n        logger.debug('Executing query: %s' % query)\n\n        query_data = {\n            'query': query,\n            'timeoutMs': timeout * 1000,\n            'dryRun': dry_run,\n            'maxResults': max_results\n        }\n\n        if use_legacy_sql is not None:\n            query_data['useLegacySql'] = use_legacy_sql\n\n        if external_udf_uris:\n            query_data['userDefinedFunctionResources'] = \\\n                [ {'resourceUri': u} for u in external_udf_uris ]\n\n        return self._submit_query_job(query_data)", "response": "Submit a query to BigQuery."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_query_schema(self, job_id):\n\n        query_reply = self.get_query_results(job_id, offset=0, limit=0)\n\n        if not query_reply['jobComplete']:\n            logger.warning('BigQuery job %s not complete' % job_id)\n            raise UnfinishedQueryException()\n\n        return query_reply['schema']['fields']", "response": "Retrieve the schema of a query by job id."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the table schema.", "response": "def get_table_schema(self, dataset, table, project_id=None):\n        \"\"\"Return the table schema.\n\n        Parameters\n        ----------\n        dataset : str\n            The dataset containing the `table`.\n        table : str\n            The table to get the schema for\n        project_id: str, optional\n            The project of the dataset.\n\n        Returns\n        -------\n        list\n            A ``list`` of ``dict`` objects that represent the table schema. If\n            the table doesn't exist, None is returned.\n        \"\"\"\n        project_id = self._get_project_id(project_id)\n\n        try:    \n            result = self.bigquery.tables().get(\n                projectId=project_id,\n                tableId=table,\n                datasetId=dataset).execute(num_retries=self.num_retries)\n        except HttpError as e:\n            if int(e.resp['status']) == 404:\n                logger.warn('Table %s.%s does not exist', dataset, table)\n                return None\n            raise\n\n        return result['schema']['fields']"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the state and number of rows included in the query table by job id.", "response": "def check_job(self, job_id):\n        \"\"\"Return the state and number of results of a query by job id.\n\n        Parameters\n        ----------\n        job_id : str\n            The job id of the query to check.\n\n        Returns\n        -------\n        tuple\n            (``bool``, ``int``) Whether or not the query has completed and the\n            total number of rows included in the query table if it has\n            completed (else 0)\n        \"\"\"\n\n        query_reply = self.get_query_results(job_id, offset=0, limit=0)\n\n        return (query_reply.get('jobComplete', False),\n                int(query_reply.get('totalRows', 0)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretrieves a list of rows from a query table by job id.", "response": "def get_query_rows(self, job_id, offset=None, limit=None, timeout=0):\n        \"\"\"Retrieve a list of rows from a query table by job id.\n        This method will append results from multiple pages together. If you\n        want to manually page through results, you can use `get_query_results`\n        method directly.\n\n        Parameters\n        ----------\n        job_id : str\n            The job id that references a BigQuery query.\n        offset : int, optional\n            The offset of the rows to pull from BigQuery\n        limit : int, optional\n            The number of rows to retrieve from a query table.\n        timeout : float, optional\n            Timeout in seconds.\n\n        Returns\n        -------\n        list\n            A ``list`` of ``dict`` objects that represent table rows.\n        \"\"\"\n\n        # Get query results\n        query_reply = self.get_query_results(job_id, offset=offset,\n                                             limit=limit, timeout=timeout)\n        if not query_reply['jobComplete']:\n            logger.warning('BigQuery job %s not complete' % job_id)\n            raise UnfinishedQueryException()\n\n        schema = query_reply[\"schema\"][\"fields\"]\n        rows = query_reply.get('rows', [])\n        page_token = query_reply.get(\"pageToken\")\n        records = [self._transform_row(row, schema) for row in rows]\n\n        # Append to records if there are multiple pages for query results\n        while page_token and (not limit or len(records) < limit):\n            query_reply = self.get_query_results(\n                job_id, offset=offset, limit=limit, page_token=page_token,\n                timeout=timeout)\n            page_token = query_reply.get(\"pageToken\")\n            rows = query_reply.get('rows', [])\n            records += [self._transform_row(row, schema) for row in rows]\n        return records[:limit] if limit else records"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_dataset(self, dataset_id, project_id=None):\n        dataset = self.get_dataset(dataset_id, project_id)\n        return bool(dataset)", "response": "Check to see if a dataset at dataset_id exists."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_dataset(self, dataset_id, project_id=None):\n        project_id = self._get_project_id(project_id)\n\n        try:            \n            dataset = self.bigquery.datasets().get(\n                projectId=project_id, datasetId=dataset_id).execute(\n                num_retries=self.num_retries)\n        except HttpError:\n            dataset = {}\n\n        return dataset", "response": "Retrieve a dataset if it exists otherwise return an empty dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks to see if a table exists.", "response": "def check_table(self, dataset, table, project_id=None):\n        \"\"\"Check to see if a table exists.\n\n        Parameters\n        ----------\n        dataset : str\n            The dataset to check\n        table : str\n            The name of the table\n        project_id: str, optional\n            The project the table is in\n\n        Returns\n        -------\n        bool\n            True if table exists, else False\n        \"\"\"\n        table = self.get_table(dataset, table, project_id)\n        return bool(table)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretrieves a table if it exists otherwise return an empty dict.", "response": "def get_table(self, dataset, table, project_id=None):\n        \"\"\" Retrieve a table if it exists, otherwise return an empty dict.\n\n        Parameters\n        ----------\n        dataset : str\n            The dataset that the table is in\n        table : str\n            The name of the table\n        project_id: str, optional\n            The project that the table is in\n\n        Returns\n        -------\n        dict\n            Containing the table object if it exists, else empty\n        \"\"\"\n        project_id = self._get_project_id(project_id)\n        try:            \n            table = self.bigquery.tables().get(\n                projectId=project_id, datasetId=dataset,\n                tableId=table).execute(num_retries=self.num_retries)\n        except HttpError:\n            table = {}\n\n        return table"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_table(self, dataset, table, schema,\n                     expiration_time=None, time_partitioning=False,\n                     project_id=None):\n        \"\"\"Create a new table in the dataset.\n\n        Parameters\n        ----------\n        dataset : str\n            The dataset to create the table in\n        table : str\n            The name of the table to create\n        schema : dict\n            The table schema        \n        expiration_time : int or double, optional\n            The expiry time in milliseconds since the epoch.\n        time_partitioning : bool, optional\n            Create a time partitioning.\n        project_id: str, optional\n            The project to create the table in\n\n        Returns\n        -------\n        Union[bool, dict]\n            If the table was successfully created, or response from BigQuery\n            if swallow_results is set to False\n        \"\"\"\n        project_id = self._get_project_id(project_id)\n\n        body = {\n            'schema': {'fields': schema},\n            'tableReference': {\n                'tableId': table,\n                'projectId': project_id,\n                'datasetId': dataset\n            }\n        }\n\n        if expiration_time is not None:\n            body['expirationTime'] = expiration_time\n\n        if time_partitioning:\n            body['timePartitioning'] = {'type': 'DAY'}\n\n        try:\n            table = self.bigquery.tables().insert(\n                projectId=project_id,\n                datasetId=dataset,\n                body=body\n            ).execute(num_retries=self.num_retries)\n            if self.swallow_results:\n                return True\n            else:\n                return table\n\n        except HttpError as e:\n            logger.error(('Cannot create table {0}.{1}.{2}\\n'\n                          'Http Error: {3}').format(project_id, dataset, table, e.content))\n            if self.swallow_results:\n                return False\n            else:\n                return {}", "response": "Creates a new table in the dataset."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef patch_table(self, dataset, table, schema, project_id=None):\n        project_id = self._get_project_id(project_id)\n\n        body = {\n            'schema': {'fields': schema},\n            'tableReference': {\n                'tableId': table,\n                'projectId': project_id,\n                'datasetId': dataset\n            }\n        }\n\n        try:\n            result = self.bigquery.tables().patch(\n                projectId=project_id,\n                datasetId=dataset,\n                body=body\n            ).execute(num_retries=self.num_retries)\n            if self.swallow_results:\n                return True\n            else:\n                return result\n\n        except HttpError as e:\n            logger.error(('Cannot patch table {0}.{1}.{2}\\n'\n                          'Http Error: {3}').format(project_id, dataset, table, e.content))\n            if self.swallow_results:\n                return False\n            else:\n                return {}", "response": "Patch an existing table in the dataset."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a new view in the dataset.", "response": "def create_view(self, dataset, view, query, use_legacy_sql=None, project_id=None):\n        \"\"\"Create a new view in the dataset.\n\n        Parameters\n        ----------\n        dataset : str\n            The dataset to create the view in\n        view : str\n            The name of the view to create\n        query : dict\n            A query that BigQuery executes when the view is referenced.        \n        use_legacy_sql : bool, optional\n            If False, the query will use BigQuery's standard SQL\n            (https://cloud.google.com/bigquery/sql-reference/)\n        project_id: str, optional\n            The project to create the view in\n\n        Returns\n        -------\n        Union[bool, dict]\n            bool indicating if the view was successfully created or not,\n            or response from BigQuery if swallow_results is set to False.\n        \"\"\"\n        project_id = self._get_project_id(project_id)\n\n        body = {\n            'tableReference': {\n                'tableId': view,\n                'projectId': project_id,\n                'datasetId': dataset\n            },\n            'view': {\n                'query': query\n            }\n        }\n\n        if use_legacy_sql is not None:\n            body['view']['useLegacySql'] = use_legacy_sql\n\n        try:\n            view = self.bigquery.tables().insert(\n                projectId=project_id,\n                datasetId=dataset,\n                body=body\n            ).execute(num_retries=self.num_retries)\n            if self.swallow_results:\n                return True\n            else:\n                return view\n\n        except HttpError as e:\n            logger.error(('Cannot create view {0}.{1}\\n'\n                          'Http Error: {2}').format(dataset, view, e.content))\n            if self.swallow_results:\n                return False\n            else:\n                return {}"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef delete_table(self, dataset, table, project_id=None):\n        project_id = self._get_project_id(project_id)\n\n        try:            \n            response = self.bigquery.tables().delete(\n                projectId=project_id,\n                datasetId=dataset,\n                tableId=table\n            ).execute(num_retries=self.num_retries)\n            if self.swallow_results:\n                return True\n            else:\n                return response\n\n        except HttpError as e:\n            logger.error(('Cannot delete table {0}.{1}\\n'\n                          'Http Error: {2}').format(dataset, table, e.content))\n            if self.swallow_results:\n                return False\n            else:\n                return {}", "response": "Delete a table from the dataset."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_tables(self, dataset_id, app_id, start_time, end_time, project_id=None):\n\n        if isinstance(start_time, datetime):\n            start_time = calendar.timegm(start_time.utctimetuple())\n\n        if isinstance(end_time, datetime):\n            end_time = calendar.timegm(end_time.utctimetuple())\n\n        every_table = self._get_all_tables(dataset_id, project_id)\n        app_tables = every_table.get(app_id, {})\n\n        return self._filter_tables_by_time(app_tables, start_time, end_time)", "response": "Retrieve a list of tables that are related to the given app id and are inside the given time range."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nimport data into a BigQuery table from a list of source URIs.", "response": "def import_data_from_uris(\n            self,\n            source_uris,\n            dataset,\n            table,\n            schema=None,            \n            job=None,\n            source_format=None,\n            create_disposition=None,\n            write_disposition=None,\n            encoding=None,\n            ignore_unknown_values=None,\n            max_bad_records=None,\n            allow_jagged_rows=None,\n            allow_quoted_newlines=None,\n            field_delimiter=None,\n            quote=None,\n            skip_leading_rows=None,\n            project_id=None,\n    ):\n        \"\"\"\n        Imports data into a BigQuery table from cloud storage. Optional\n        arguments that are not specified are determined by BigQuery as\n        described:\n            https://developers.google.com/bigquery/docs/reference/v2/jobs\n\n        Parameters\n        ----------\n        source_urls : list\n            A ``list`` of ``str`` objects representing the urls on cloud\n            storage of the form: gs://bucket/filename\n        dataset : str\n            String id of the dataset\n        table : str\n            String id of the table\n        schema : list, optional\n            Represents the BigQuery schema            \n        job : str, optional\n            Identifies the job (a unique job id is automatically generated if\n            not provided)        \n        source_format : str, optional\n            One of the JOB_SOURCE_FORMAT_* constants\n        create_disposition : str, optional\n            One of the JOB_CREATE_* constants\n        write_disposition : str, optional\n            One of the JOB_WRITE_* constants\n        encoding : str, optional\n            One of the JOB_ENCODING_* constants\n        ignore_unknown_values : bool, optional\n            Whether or not to ignore unknown values\n        max_bad_records : int, optional\n            Maximum number of bad records\n        allow_jagged_rows : bool, optional\n            For csv only\n        allow_quoted_newlines : bool, optional\n            For csv only\n        field_delimiter : str, optional\n            For csv only\n        quote : str, optional\n            Quote character for csv only\n        skip_leading_rows : int, optional\n            For csv only\n        project_id: str, optional\n            String id of the project     \n\n        Returns\n        -------\n        dict\n            A BigQuery job response\n\n        Raises\n        ------\n        JobInsertException\n            on http/auth failures or error in result\n        \"\"\"\n        source_uris = source_uris if isinstance(source_uris, list) \\\n            else [source_uris]\n\n        project_id = self._get_project_id(project_id)\n\n        configuration = {\n            \"destinationTable\": {\n                \"projectId\": project_id,\n                \"tableId\": table,\n                \"datasetId\": dataset\n            },\n            \"sourceUris\": source_uris,\n        }\n\n        if max_bad_records:\n            configuration['maxBadRecords'] = max_bad_records\n\n        if ignore_unknown_values:\n            configuration['ignoreUnknownValues'] = ignore_unknown_values\n\n        if create_disposition:\n            configuration['createDisposition'] = create_disposition\n\n        if write_disposition:\n            configuration['writeDisposition'] = write_disposition\n\n        if encoding:\n            configuration['encoding'] = encoding\n\n        if schema:\n            configuration['schema'] = {'fields': schema}\n\n        if source_format:\n            configuration['sourceFormat'] = source_format\n\n        if not job:\n            hex = self._generate_hex_for_uris(source_uris)\n            job = \"{dataset}-{table}-{digest}\".format(\n                dataset=dataset,\n                table=table,\n                digest=hex\n            )\n\n        if source_format == JOB_SOURCE_FORMAT_CSV:\n            if field_delimiter:\n                configuration['fieldDelimiter'] = field_delimiter\n\n            if allow_jagged_rows:\n                configuration['allowJaggedRows'] = allow_jagged_rows\n\n            if allow_quoted_newlines:\n                configuration['allowQuotedNewlines'] = allow_quoted_newlines\n\n            if quote:\n                configuration['quote'] = quote\n\n            if skip_leading_rows:\n                configuration['skipLeadingRows'] = skip_leading_rows\n\n        elif field_delimiter or allow_jagged_rows \\\n                or allow_quoted_newlines or quote or skip_leading_rows:\n            all_values = dict(field_delimiter=field_delimiter,\n                              allow_jagged_rows=allow_jagged_rows,\n                              allow_quoted_newlines=allow_quoted_newlines,\n                              skip_leading_rows=skip_leading_rows,\n                              quote=quote)\n            non_null_values = dict((k, v) for k, v\n                                   in list(all_values.items())\n                                   if v)\n            raise Exception(\"Parameters field_delimiter, allow_jagged_rows, \"\n                            \"allow_quoted_newlines, quote and \"\n                            \"skip_leading_rows are only allowed when \"\n                            \"source_format=JOB_SOURCE_FORMAT_CSV: %s\"\n                            % non_null_values)\n\n        body = {\n            \"configuration\": {\n                'load': configuration\n            },\n            \"jobReference\": self._get_job_reference(job)\n        }\n\n        logger.debug(\"Creating load job %s\" % body)\n        job_resource = self._insert_job(body)\n        self._raise_insert_exception_if_error(job_resource)\n        return job_resource"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nexports data from a BigQuery table to a list of URIs on the cloud storage.", "response": "def export_data_to_uris(\n            self,\n            destination_uris,\n            dataset,\n            table,            \n            job=None,\n            compression=None,\n            destination_format=None,\n            print_header=None,\n            field_delimiter=None,\n            project_id=None,\n    ):\n        \"\"\"\n        Export data from a BigQuery table to cloud storage. Optional arguments\n        that are not specified are determined by BigQuery as described:\n        https://developers.google.com/bigquery/docs/reference/v2/jobs\n\n        Parameters\n        ----------\n        destination_uris : Union[str, list]\n            ``str`` or ``list`` of ``str`` objects representing the URIs on\n            cloud storage of the form: gs://bucket/filename\n        dataset : str\n            String id of the dataset\n        table : str\n            String id of the table        \n        job : str, optional\n            String identifying the job (a unique jobid is automatically\n            generated if not provided)\n        compression : str, optional\n            One of the JOB_COMPRESSION_* constants\n        destination_format : str, optional\n            One of the JOB_DESTination_FORMAT_* constants\n        print_header : bool, optional\n            Whether or not to print the header\n        field_delimiter : str, optional\n            Character separating fields in delimited file\n        project_id: str, optional\n            String id of the project\n\n        Returns\n        -------\n        dict\n            A BigQuery job resource\n\n        Raises\n        ------\n        JobInsertException\n            On http/auth failures or error in result\n        \"\"\"\n        destination_uris = destination_uris \\\n            if isinstance(destination_uris, list) else [destination_uris]\n\n        project_id = self._get_project_id(project_id)\n\n        configuration = {\n            \"sourceTable\": {\n                \"projectId\": project_id,\n                \"tableId\": table,\n                \"datasetId\": dataset\n            },\n            \"destinationUris\": destination_uris,\n        }\n\n        if compression:\n            configuration['compression'] = compression\n\n        if destination_format:\n            configuration['destinationFormat'] = destination_format\n\n        if print_header is not None:\n            configuration['printHeader'] = print_header\n\n        if field_delimiter:\n            configuration['fieldDelimiter'] = field_delimiter\n\n        if not job:\n            hex = self._generate_hex_for_uris(destination_uris)\n            job = \"{dataset}-{table}-{digest}\".format(\n                dataset=dataset,\n                table=table,\n                digest=hex\n            )\n\n        body = {\n            \"configuration\": {\n                'extract': configuration\n            },\n            \"jobReference\": self._get_job_reference(job)\n        }\n\n        logger.info(\"Creating export job %s\" % body)\n        job_resource = self._insert_job(body)\n        self._raise_insert_exception_if_error(job_resource)\n        return job_resource"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwrites the result of a query to a temporary table.", "response": "def write_to_table(\n            self,\n            query,\n            dataset=None,\n            table=None,            \n            external_udf_uris=None,\n            allow_large_results=None,\n            use_query_cache=None,\n            priority=None,\n            create_disposition=None,\n            write_disposition=None,\n            use_legacy_sql=None,\n            maximum_billing_tier=None,\n            flatten=None,\n            project_id=None,\n    ):\n        \"\"\"\n        Write query result to table. If dataset or table is not provided,\n        Bigquery will write the result to temporary table. Optional arguments\n        that are not specified are determined by BigQuery as described:\n        https://developers.google.com/bigquery/docs/reference/v2/jobs\n\n        Parameters\n        ----------\n        query : str\n            BigQuery query string\n        dataset : str, optional\n            String id of the dataset\n        table : str, optional\n            String id of the table        \n        external_udf_uris : list, optional\n            Contains external UDF URIs. If given, URIs must be Google Cloud\n            Storage and have .js extensions.\n        allow_large_results : bool, optional\n            Whether or not to allow large results\n        use_query_cache : bool, optional\n            Whether or not to use query cache\n        priority : str, optional\n            One of the JOB_PRIORITY_* constants\n        create_disposition : str, optional\n            One of the JOB_CREATE_* constants\n        write_disposition : str, optional\n            One of the JOB_WRITE_* constants\n        use_legacy_sql: bool, optional\n            If False, the query will use BigQuery's standard SQL\n            (https://cloud.google.com/bigquery/sql-reference/)\n        maximum_billing_tier : integer, optional\n            Limits the billing tier for this job. Queries that have resource\n            usage beyond this tier will fail (without incurring a charge). If\n            unspecified, this will be set to your project default. For more\n            information,\n            see https://cloud.google.com/bigquery/pricing#high-compute\n        flatten : bool, optional\n            Whether or not to flatten nested and repeated fields\n            in query results\n        project_id: str, optional\n            String id of the project\n\n        Returns\n        -------\n        dict\n            A BigQuery job resource\n\n        Raises\n        ------\n        JobInsertException\n            On http/auth failures or error in result\n        \"\"\"\n\n        configuration = {\n            \"query\": query,\n        }\n\n        project_id = self._get_project_id(project_id)\n\n        if dataset and table:\n            configuration['destinationTable'] = {\n                \"projectId\": project_id,\n                \"tableId\": table,\n                \"datasetId\": dataset\n            }\n\n        if allow_large_results is not None:\n            configuration['allowLargeResults'] = allow_large_results\n\n        if flatten is not None:\n            configuration['flattenResults'] = flatten\n\n        if maximum_billing_tier is not None:\n            configuration['maximumBillingTier'] = maximum_billing_tier\n\n        if use_query_cache is not None:\n            configuration['useQueryCache'] = use_query_cache\n\n        if use_legacy_sql is not None:\n            configuration['useLegacySql'] = use_legacy_sql\n\n        if priority:\n            configuration['priority'] = priority\n\n        if create_disposition:\n            configuration['createDisposition'] = create_disposition\n\n        if write_disposition:\n            configuration['writeDisposition'] = write_disposition\n\n        if external_udf_uris:\n            configuration['userDefinedFunctionResources'] = \\\n                [ {'resourceUri': u} for u in external_udf_uris ]\n\n        body = {\n            \"configuration\": {\n                'query': configuration\n            }\n        }\n\n        logger.info(\"Creating write to table job %s\" % body)\n        job_resource = self._insert_job(body)\n        self._raise_insert_exception_if_error(job_resource)\n        return job_resource"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwaits until the job identified by job_id is done or has failed.", "response": "def wait_for_job(self, job, interval=5, timeout=60):\n        \"\"\"\n        Waits until the job indicated by job_resource is done or has failed\n\n        Parameters\n        ----------\n        job : Union[dict, str]\n            ``dict`` representing a BigQuery job resource, or a ``str``\n            representing the BigQuery job id\n        interval : float, optional\n            Polling interval in seconds, default = 5\n        timeout : float, optional\n            Timeout in seconds, default = 60\n\n        Returns\n        -------\n        dict\n            Final state of the job resouce, as described here:\n            https://developers.google.com/resources/api-libraries/documentation/bigquery/v2/python/latest/bigquery_v2.jobs.html#get\n\n        Raises\n        ------\n        Union[JobExecutingException, BigQueryTimeoutException]\n            On http/auth failures or timeout\n        \"\"\"\n        complete = False\n        job_id = str(job if isinstance(job,\n                                       (six.binary_type, six.text_type, int))\n                     else job['jobReference']['jobId'])\n        job_resource = None\n\n        start_time = time()\n        elapsed_time = 0\n        while not (complete or elapsed_time > timeout):\n            sleep(interval)\n            request = self.bigquery.jobs().get(projectId=self.project_id,\n                                               jobId=job_id)\n            job_resource = request.execute(num_retries=self.num_retries)\n            self._raise_executing_exception_if_error(job_resource)\n            complete = job_resource.get('status').get('state') == u'DONE'\n            elapsed_time = time() - start_time\n\n        # raise exceptions if timeout\n        if not complete:\n            logger.error('BigQuery job %s timeout' % job_id)\n            raise BigQueryTimeoutException()\n\n        return job_resource"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nuploading rows into a table.", "response": "def push_rows(self, dataset, table, rows, insert_id_key=None,\n                  skip_invalid_rows=None, ignore_unknown_values=None,\n                  template_suffix=None, project_id=None):\n        \"\"\"Upload rows to BigQuery table.\n\n        Parameters\n        ----------\n        dataset : str\n            The dataset to upload to\n        table : str\n            The name of the table to insert rows into        \n        rows : list\n            A ``list`` of rows (``dict`` objects) to add to the table\n        insert_id_key : str, optional\n            Key for insertId in row.\n            You can use dot separated key for nested column.\n        skip_invalid_rows : bool, optional\n            Insert all valid rows of a request, even if invalid rows exist.\n        ignore_unknown_values : bool, optional\n            Accept rows that contain values that do not match the schema.\n        template_suffix : str, optional\n            Inserts the rows into an {table}{template_suffix}.\n            If table {table}{template_suffix} doesn't exist, create from {table}.\n        project_id: str, optional\n            The project to upload to\n\n        Returns\n        -------\n        Union[bool, dict]\n            bool indicating if insert succeeded or not, or response\n            from BigQuery if swallow_results is set for False.\n        \"\"\"\n        project_id = self._get_project_id(project_id)\n        table_data = self.bigquery.tabledata()\n\n        rows_data = []\n        for row in rows:\n            each_row = {}\n            each_row[\"json\"] = row\n            if insert_id_key is not None:\n                keys = insert_id_key.split('.')\n                val = reduce(lambda d, key: d.get(key) if d else None, keys, row)\n                if val is not None:\n                    each_row[\"insertId\"] = val\n            rows_data.append(each_row)\n\n        data = {\n            \"kind\": \"bigquery#tableDataInsertAllRequest\",\n            \"rows\": rows_data\n        }\n\n        if skip_invalid_rows is not None:\n            data['skipInvalidRows'] = skip_invalid_rows\n\n        if ignore_unknown_values is not None:\n            data['ignoreUnknownValues'] = ignore_unknown_values\n\n        if template_suffix is not None:\n            data['templateSuffix'] = template_suffix\n\n        try:            \n            response = table_data.insertAll(\n                projectId=project_id,\n                datasetId=dataset,\n                tableId=table,\n                body=data\n            ).execute(num_retries=self.num_retries)\n\n            if response.get('insertErrors'):\n                logger.error('BigQuery insert errors: %s' % response)\n                if self.swallow_results:\n                    return False\n                else:\n                    return response\n\n            if self.swallow_results:\n                return True\n            else:\n                return response\n\n        except HttpError as e:\n            logger.exception('Problem with BigQuery insertAll')\n            if self.swallow_results:\n                return False\n            else:\n                return {\n                    'insertErrors': [{\n                        'errors': [{\n                            'reason': 'httperror',\n                            'message': e\n                        }]\n                    }]\n                }"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nretrieving a list of tables for the dataset.", "response": "def get_all_tables(self, dataset_id, project_id=None):\n        \"\"\"Retrieve a list of tables for the dataset.\n\n        Parameters\n        ----------\n        dataset_id : str\n            The dataset to retrieve table data for.\n        project_id: str\n            Unique ``str`` identifying the BigQuery project contains the dataset\n\n        Returns\n        -------\n        A ``list`` with all table names\n        \"\"\"\n        tables_data = self._get_all_tables_for_dataset(dataset_id, project_id)\n\n        tables = []\n        for table in tables_data.get('tables', []):\n            table_name = table.get('tableReference', {}).get('tableId')\n            if table_name:\n                tables.append(table_name)\n        return tables"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving the list of tables for a dataset.", "response": "def _get_all_tables(self, dataset_id, cache=False, project_id=None):\n        \"\"\"Retrieve the list of tables for dataset, that respect the formats:\n            * appid_YYYY_MM\n            * YYYY_MM_appid\n\n        Parameters\n        ----------\n        dataset_id : str\n            The dataset to retrieve table names for        \n        cache : bool, optional\n            To use cached value or not (default False). Timeout value equals\n            CACHE_TIMEOUT.\n        project_id: str\n            Unique ``str`` identifying the BigQuery project contains the dataset\n\n        Returns\n        -------\n        dict\n            A ``dict`` of app ids mapped to their table names\n        \"\"\"\n        do_fetch = True\n        if cache and self.cache.get(dataset_id):\n            time, result = self.cache.get(dataset_id)\n            if datetime.now() - time < CACHE_TIMEOUT:\n                do_fetch = False\n\n        if do_fetch:\n            result = self._get_all_tables_for_dataset(dataset_id, project_id)\n            self.cache[dataset_id] = (datetime.now(), result)\n\n        return self._parse_table_list_response(result)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving a list of all tables for the dataset.", "response": "def _get_all_tables_for_dataset(self, dataset_id, project_id=None):\n        \"\"\"Retrieve a list of all tables for the dataset.\n\n        Parameters\n        ----------\n        dataset_id : str\n            The dataset to retrieve table names for\n        project_id: str\n            Unique ``str`` identifying the BigQuery project contains the dataset\n\n        Returns\n        -------\n        dict\n            A ``dict`` containing tables key with all tables\n        \"\"\"\n        project_id = self._get_project_id(project_id)\n\n        result = self.bigquery.tables().list(\n            projectId=project_id,\n            datasetId=dataset_id).execute(num_retries=self.num_retries)\n\n        page_token = result.get('nextPageToken')\n        while page_token:\n            res = self.bigquery.tables().list(\n                projectId=project_id,\n                datasetId=dataset_id,\n                pageToken=page_token\n            ).execute(num_retries=self.num_retries)\n            page_token = res.get('nextPageToken')\n            result['tables'] += res.get('tables', [])\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _parse_table_list_response(self, list_response):\n\n        tables = defaultdict(dict)\n\n        for table in list_response.get('tables', []):\n            table_ref = table.get('tableReference')\n\n            if not table_ref:\n                continue\n\n            table_id = table_ref.get('tableId', '')\n\n            year_month, app_id = self._parse_table_name(table_id)\n\n            if not year_month:\n                continue\n\n            table_date = datetime.strptime(year_month, '%Y-%m')\n            unix_seconds = calendar.timegm(table_date.timetuple())\n            tables[app_id].update({table_id: unix_seconds})\n\n        # Turn off defualting\n        tables.default_factory = None\n\n        return tables", "response": "Parse the response received from calling list on tables."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses a table name in the form of YYYY_MM_YYYY or YYYY_MMDD_YYYY and return a tuple consisting of YYYY - MM and the app id.", "response": "def _parse_table_name(self, table_id):\n        \"\"\"Parse a table name in the form of appid_YYYY_MM or\n        YYYY_MM_appid and return a tuple consisting of YYYY-MM and the app id.\n\n        Returns (None, None) in the event of a name like <desc>_YYYYMMDD_<int>\n\n        Parameters\n        ----------\n        table_id : str\n            The table id as listed by BigQuery\n\n        Returns\n        -------\n        tuple\n            (year/month, app id), or (None, None) if the table id cannot be\n            parsed.\n        \"\"\"\n\n        # Prefix date\n        attributes = table_id.split('_')\n        year_month = \"-\".join(attributes[:2])\n        app_id = \"-\".join(attributes[2:])\n\n        # Check if date parsed correctly\n        if year_month.count(\"-\") == 1 and all(\n                [num.isdigit() for num in year_month.split('-')]):\n            return year_month, app_id\n\n        # Postfix date\n        attributes = table_id.split('_')\n        year_month = \"-\".join(attributes[-2:])\n        app_id = \"-\".join(attributes[:-2])\n\n\n        # Check if date parsed correctly\n        if year_month.count(\"-\") == 1 and all(\n                [num.isdigit() for num in year_month.split('-')]) and len(year_month) == 7:\n            return year_month, app_id\n\n        return None, None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _filter_tables_by_time(self, tables, start_time, end_time):\n\n        return [table_name for (table_name, unix_seconds) in tables.items()\n                if self._in_range(start_time, end_time, unix_seconds)]", "response": "Filter a dictionary and return a list of table names based on the range\n            start and end times in unix seconds."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nindicate if the given time falls inside of the given range.", "response": "def _in_range(self, start_time, end_time, time):\n        \"\"\"Indicate if the given time falls inside of the given range.\n\n        Parameters\n        ----------\n        start_time : int\n            The unix time for the start of the range\n        end_time : int\n            The unix time for the end of the range\n        time : int\n            The unix time to check\n\n        Returns\n        -------\n        bool\n            True if the time falls within the range, False otherwise.\n        \"\"\"\n\n        ONE_MONTH = 2764800  # 32 days\n\n        return start_time <= time <= end_time or \\\n            time <= start_time <= time + ONE_MONTH or \\\n            time <= end_time <= time + ONE_MONTH"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nexecuting the query job indicated by the given job id.", "response": "def get_query_results(self, job_id, offset=None, limit=None,\n                          page_token=None, timeout=0):\n        \"\"\"Execute the query job indicated by the given job id. This is direct\n        mapping to bigquery api\n        https://cloud.google.com/bigquery/docs/reference/v2/jobs/getQueryResults\n\n        Parameters\n        ----------\n        job_id : str\n            The job id of the query to check\n        offset : optional\n            The index the result set should start at.\n        limit : int, optional\n            The maximum number of results to retrieve.\n        page_token : optional\n            Page token, returned by previous call, to request the next page of\n            results.\n        timeout : float, optional\n            Timeout in seconds\n\n        Returns\n        -------\n        out\n            The query reply\n        \"\"\"\n\n        job_collection = self.bigquery.jobs()\n        return job_collection.getQueryResults(\n            projectId=self.project_id,\n            jobId=job_id,\n            startIndex=offset,\n            maxResults=limit,\n            pageToken=page_token,\n            timeoutMs=timeout * 1000).execute(num_retries=self.num_retries)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _transform_row(self, row, schema):\n\n        log = {}\n\n        # Match each schema column with its associated row value\n        for index, col_dict in enumerate(schema):\n            col_name = col_dict['name']\n            row_value = row['f'][index]['v']\n\n            if row_value is None:\n                log[col_name] = None\n                continue\n\n            # Recurse on nested records\n            if col_dict['type'] == 'RECORD':\n                row_value = self._recurse_on_row(col_dict, row_value)\n\n            # Otherwise just cast the value\n            elif col_dict['type'] == 'INTEGER':\n                row_value = int(row_value)\n\n            elif col_dict['type'] == 'FLOAT':\n                row_value = float(row_value)\n\n            elif col_dict['type'] == 'BOOLEAN':\n                row_value = row_value in ('True', 'true', 'TRUE')\n\n            elif col_dict['type'] == 'TIMESTAMP':\n                row_value = float(row_value)\n\n            log[col_name] = row_value\n\n        return log", "response": "Apply the given schema to the given BigQuery data row."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _recurse_on_row(self, col_dict, nested_value):\n\n        row_value = None\n\n        # Multiple nested records\n        if col_dict['mode'] == 'REPEATED' and isinstance(nested_value, list):\n            row_value = [self._transform_row(record['v'], col_dict['fields'])\n                         for record in nested_value]\n\n        # A single nested record\n        else:\n            row_value = self._transform_row(nested_value, col_dict['fields'])\n\n        return row_value", "response": "Apply the schema specified by the given dict to the nested value by\n           ."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngiving uris generate and return hex version of it", "response": "def _generate_hex_for_uris(self, uris):\n        \"\"\"Given uris, generate and return hex version of it\n\n        Parameters\n        ----------\n        uris : list\n            Containing all uris\n\n        Returns\n        -------\n        str\n            Hexed uris\n        \"\"\"\n        return sha256((\":\".join(uris) + str(time())).encode()).hexdigest()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a new dataset in BigQuery.", "response": "def create_dataset(self, dataset_id, friendly_name=None, description=None,\n                       access=None, location=None, project_id=None):\n        \"\"\"Create a new BigQuery dataset.\n\n        Parameters\n        ----------\n        dataset_id : str\n            Unique ``str`` identifying the dataset with the project (the\n            referenceID of the dataset, not the integer id of the dataset)        \n        friendly_name: str, optional\n            A human readable name\n        description: str, optional\n            Longer string providing a description\n        access : list, optional\n            Indicating access permissions (see\n            https://developers.google.com/bigquery/docs/reference/v2/datasets#resource)\n        location : str, optional\n            Indicating where dataset should be stored: EU or US (see\n            https://developers.google.com/bigquery/docs/reference/v2/datasets#resource)\n        project_id: str\n            Unique ``str`` identifying the BigQuery project contains the dataset\n\n        Returns\n        -------\n        Union[bool, dict]\n            ``bool`` indicating if dataset was created or not, or response\n            from BigQuery if swallow_results is set for False\n        \"\"\"\n        project_id = self._get_project_id(project_id)\n\n        try:            \n            datasets = self.bigquery.datasets()\n            dataset_data = self.dataset_resource(dataset_id,                                                 \n                                                 project_id=project_id,\n                                                 friendly_name=friendly_name,\n                                                 description=description,\n                                                 access=access,\n                                                 location=location\n                                                 )\n\n            response = datasets.insert(projectId=project_id,\n                                       body=dataset_data).execute(\n                num_retries=self.num_retries)\n            if self.swallow_results:\n                return True\n            else:\n                return response\n        except HttpError as e:\n            logger.error(\n                'Cannot create dataset {0}, {1}'.format(dataset_id, e))\n            if self.swallow_results:\n                return False\n            else:\n                return {}"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlist all datasets in the project.", "response": "def get_datasets(self, project_id=None):\n        \"\"\"List all datasets in the project.\n        \n        Parameters\n        ----------\n        project_id: str\n            Unique ``str`` identifying the BigQuery project contains the dataset\n\n        Returns\n        -------\n        list\n            Dataset resources\n        \"\"\"\n        project_id = self._get_project_id(project_id)\n\n        try:            \n            datasets = self.bigquery.datasets()\n            request = datasets.list(projectId=project_id)\n            result = request.execute(num_retries=self.num_retries)\n            return result.get('datasets', [])\n        except HttpError as e:\n            logger.error(\"Cannot list datasets: {0}\".format(e))\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef delete_dataset(self, dataset_id, delete_contents=False, project_id=None):\n        project_id = self._get_project_id(project_id)\n\n        try:            \n            datasets = self.bigquery.datasets()\n            request = datasets.delete(projectId=project_id,\n                                      datasetId=dataset_id,\n                                      deleteContents=delete_contents)\n            response = request.execute(num_retries=self.num_retries)\n            if self.swallow_results:\n                return True\n            else:\n                return response\n        except HttpError as e:\n            logger.error(\n                'Cannot delete dataset {0}: {1}'.format(dataset_id, e))\n            if self.swallow_results:\n                return False\n            else:\n                return {}", "response": "Delete a BigQuery dataset.\n\n        Parameters\n        ----------\n        dataset_id : str\n            Unique ``str`` identifying the dataset with the project (the\n            referenceId of the dataset)        \n            Unique ``str`` identifying the BigQuery project contains the dataset\n        delete_contents : bool, optional\n            If True, forces the deletion of the dataset even when the dataset\n            contains data (Default = False)\n        project_id: str, optional\n\n        Returns\n        -------\n        Union[bool, dict[\n            ool indicating if the delete was successful or not, or response\n            from BigQuery if swallow_results is set for False\n\n        Raises\n        -------\n        HttpError\n            404 when dataset with dataset_id does not exist"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update_dataset(self, dataset_id, friendly_name=None, description=None,\n                       access=None, project_id=None):\n        \"\"\"Updates information in an existing dataset. The update method\n        replaces the entire dataset resource, whereas the patch method only\n        replaces fields that are provided in the submitted dataset resource.\n\n        Parameters\n        ----------\n        dataset_id : str\n            Unique ``str`` identifying the dataset with the project (the\n            referencedId of the dataset)        \n        friendly_name : str, optional\n            An optional descriptive name for the dataset.\n        description : str, optional\n            An optional description of the dataset.\n        access : list, optional\n            Indicating access permissions\n        project_id: str, optional\n            Unique ``str`` identifying the BigQuery project contains the dataset\n\n        Returns\n        -------\n        Union[bool, dict]\n            ``bool`` indicating if the update was successful or not, or\n            response from BigQuery if swallow_results is set for False.\n        \"\"\"\n        project_id = self._get_project_id(project_id)\n\n        try:            \n            datasets = self.bigquery.datasets()\n            body = self.dataset_resource(dataset_id, \n                                         friendly_name=friendly_name,\n                                         description=description, \n                                         access=access,\n                                         project_id=project_id)\n\n            request = datasets.update(projectId=project_id,\n                                      datasetId=dataset_id,\n                                      body=body)\n            response = request.execute(num_retries=self.num_retries)\n            if self.swallow_results:\n                return True\n            else:\n                return response\n        except HttpError as e:\n            logger.error(\n                'Cannot update dataset {0}: {1}'.format(dataset_id, e))\n            if self.swallow_results:\n                return False\n            else:\n                return {}", "response": "Update the information in an existing dataset."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a dictionary containing the metadata for a specific dataset.", "response": "def dataset_resource(self, ref_id, friendly_name=None, description=None,\n                         access=None, location=None, project_id=None):\n        \"\"\"See\n        https://developers.google.com/bigquery/docs/reference/v2/datasets#resource\n\n        Parameters\n        ----------\n        ref_id : str\n            Dataset id (the reference id, not the integer id)        \n        friendly_name : str, optional\n            An optional descriptive name for the dataset\n        description : str, optional\n            An optional description for the dataset\n        access : list, optional\n            Indicating access permissions\n        location: str, optional, 'EU' or 'US'\n            An optional geographical location for the dataset(EU or US)\n        project_id: str\n            Unique ``str`` identifying the BigQuery project contains the dataset\n\n        Returns\n        -------\n        dict\n            Representing BigQuery dataset resource\n        \"\"\"\n        project_id = self._get_project_id(project_id)\n        \n        data = {\n            \"datasetReference\": {\n                \"datasetId\": ref_id,\n                \"projectId\": project_id\n            }\n        }\n        if friendly_name:\n            data[\"friendlyName\"] = friendly_name\n        if description:\n            data[\"description\"] = description\n        if access:\n            data[\"access\"] = access\n        if location:\n            data[\"location\"] = location\n\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates a BigQuery schema given an example of a record that is to be inserted into BigQuery.", "response": "def schema_from_record(record, timestamp_parser=default_timestamp_parser):\n    \"\"\"Generate a BigQuery schema given an example of a record that is to be\n    inserted into BigQuery.\n\n    Parameters\n    ----------\n    record : dict\n        Example of a record that is to be inserted into BigQuery\n    timestamp_parser : function, optional\n        Unary function taking a ``str`` and returning and ``bool`` that is\n        True if the string represents a date\n\n    Returns\n    -------\n    Schema: list\n    \"\"\"\n    return [describe_field(k, v, timestamp_parser=timestamp_parser)\n            for k, v in list(record.items())]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngiving a value return the corresponding BigQuery type for that value.", "response": "def bigquery_type(o, timestamp_parser=default_timestamp_parser):\n    \"\"\"Given a value, return the matching BigQuery type of that value. Must be\n    one of str/unicode/int/float/datetime/record, where record is a dict\n    containing value which have matching BigQuery types.\n\n    Parameters\n    ----------\n    o : object\n        A Python object\n    time_stamp_parser : function, optional\n        Unary function taking a ``str`` and returning and ``bool`` that is\n        True if the string represents a date\n\n    Returns\n    -------\n    Union[str, None]\n        Name of the corresponding BigQuery type for `o`, or None if no type\n        could be found\n\n    Examples\n    --------\n    >>> bigquery_type(\"abc\")\n    \"string\"\n    >>> bigquery_type(123)\n    \"integer\"\n    \"\"\"\n\n    t = type(o)\n    if t in six.integer_types:\n        return \"integer\"\n    elif (t == six.binary_type and six.PY2) or t == six.text_type:\n        if timestamp_parser and timestamp_parser(o):\n            return \"timestamp\"\n        else:\n            return \"string\"\n    elif t == float:\n        return \"float\"\n    elif t == bool:\n        return \"boolean\"\n    elif t == dict:\n        return \"record\"\n    elif t == datetime:\n        return \"timestamp\"\n    else:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef render_query(dataset, tables, select=None, conditions=None,\n                 groupings=None, having=None, order_by=None, limit=None):\n    \"\"\"Render a query that will run over the given tables using the specified\n    parameters.\n\n    Parameters\n    ----------\n    dataset : str\n        The BigQuery dataset to query data from\n    tables : Union[dict, list]\n        The table in `dataset` to query.\n    select : dict, optional\n        The keys function as column names and the values function as options to\n        apply to the select field such as alias and format.  For example,\n        select['start_time'] might have the form\n        {'alias': 'StartTime', 'format': 'INTEGER-FORMAT_UTC_USEC'}, which\n        would be represented as 'SEC_TO_TIMESTAMP(INTEGER(start_time)) as\n        StartTime' in a query. Pass `None` to select all.\n    conditions : list, optional\n        a ``list`` of ``dict`` objects to filter results by.  Each dict should\n        have the keys 'field', 'type', and 'comparators'. The first two map to\n        strings representing the field (e.g. 'foo') and type (e.g. 'FLOAT').\n        'comparators' maps to another ``dict`` containing the keys 'condition',\n        'negate', and 'value'.\n        If 'comparators' = {'condition': '>=', 'negate': False, 'value': 1},\n        this example will be rendered as 'foo >= FLOAT('1')' in the query.\n        ``list`` of field names to group by\n    order_by : dict, optional\n        Keys = {'field', 'direction'}. `dict` should be formatted as\n        {'field':'TimeStamp, 'direction':'desc'} or similar\n    limit : int, optional\n        Limit the amount of data needed to be returned.\n\n    Returns\n    -------\n    str\n        A rendered query\n    \"\"\"\n\n    if None in (dataset, tables):\n        return None\n\n    query = \"%s %s %s %s %s %s %s\" % (\n        _render_select(select),\n        _render_sources(dataset, tables),\n        _render_conditions(conditions),\n        _render_groupings(groupings),\n        _render_having(having),\n        _render_order(order_by),\n        _render_limit(limit)\n    )\n\n    return query", "response": "Render a query that will run over the given tables."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _render_select(selections):\n\n    if not selections:\n        return 'SELECT *'\n\n    rendered_selections = []\n    for name, options in selections.items():\n        if not isinstance(options, list):\n            options = [options]\n\n        original_name = name\n        for options_dict in options:\n            name = original_name\n            alias = options_dict.get('alias')\n            alias = \"as %s\" % alias if alias else \"\"\n\n            formatter = options_dict.get('format')\n            if formatter:\n                name = _format_select(formatter, name)\n\n            rendered_selections.append(\"%s %s\" % (name, alias))\n\n    return \"SELECT \" + \", \".join(rendered_selections)", "response": "Render the select part of a query."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmodifies the query selector by applying any formatters to it.", "response": "def _format_select(formatter, name):\n    \"\"\"Modify the query selector by applying any formatters to it.\n\n    Parameters\n    ----------\n    formatter : str\n       Hyphen-delimited formatter string where formatters are\n       applied inside-out, e.g. the formatter string\n       SEC_TO_MICRO-INTEGER-FORMAT_UTC_USEC applied to the selector\n       foo would result in FORMAT_UTC_USEC(INTEGER(foo*1000000)).\n    name: str\n        The name of the selector to apply formatters to.\n\n    Returns\n    -------\n    str\n        The formatted selector\n    \"\"\"\n\n    for caster in formatter.split('-'):\n        if caster == 'SEC_TO_MICRO':\n            name = \"%s*1000000\" % name\n        elif ':' in caster:\n            caster, args = caster.split(':')\n            name = \"%s(%s,%s)\" % (caster, name, args)\n        else:\n            name = \"%s(%s)\" % (caster, name)\n\n    return name"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _render_sources(dataset, tables):\n\n    if isinstance(tables, dict):\n        if tables.get('date_range', False):\n            try:\n                dataset_table = '.'.join([dataset, tables['table']])\n                return \"FROM (TABLE_DATE_RANGE([{}], TIMESTAMP('{}'),\"\\\n                    \" TIMESTAMP('{}'))) \".format(dataset_table,\n                                                 tables['from_date'],\n                                                 tables['to_date'])\n            except KeyError as exp:\n                logger.warn(\n                    'Missing parameter %s in selecting sources' % (exp))\n\n    else:\n        return \"FROM \" + \", \".join(\n            [\"[%s.%s]\" % (dataset, table) for table in tables])", "response": "Render the source part of a query."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrendering the conditions part of a query.", "response": "def _render_conditions(conditions):\n    \"\"\"Render the conditions part of a query.\n\n    Parameters\n    ----------\n    conditions : list\n        A list of dictionary items to filter a table.\n\n    Returns\n    -------\n    str\n        A string that represents the \"where\" part of a query\n\n    See Also\n    --------\n    render_query : Further clarification of `conditions` formatting.\n    \"\"\"\n\n    if not conditions:\n        return \"\"\n\n    rendered_conditions = []\n\n    for condition in conditions:\n        field = condition.get('field')\n        field_type = condition.get('type')\n        comparators = condition.get('comparators')\n\n        if None in (field, field_type, comparators) or not comparators:\n            logger.warn('Invalid condition passed in: %s' % condition)\n            continue\n\n        rendered_conditions.append(\n            _render_condition(field, field_type, comparators))\n\n    if not rendered_conditions:\n        return \"\"\n\n    return \"WHERE %s\" % (\" AND \".join(rendered_conditions))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _render_condition(field, field_type, comparators):\n\n    field_type = field_type.upper()\n\n    negated_conditions, normal_conditions = [], []\n\n    for comparator in comparators:\n        condition = comparator.get(\"condition\").upper()\n        negated = \"NOT \" if comparator.get(\"negate\") else \"\"\n        value = comparator.get(\"value\")\n\n        if condition == \"IN\":\n            if isinstance(value, (list, tuple, set)):\n                value = ', '.join(\n                    sorted([_render_condition_value(v, field_type)\n                            for v in value])\n                )\n            else:\n                value = _render_condition_value(value, field_type)\n            value = \"(\" + value + \")\"\n        elif condition == \"IS NULL\" or condition == \"IS NOT NULL\":\n            return field + \" \" + condition\n        elif condition == \"BETWEEN\":\n            if isinstance(value, (tuple, list, set)) and len(value) == 2:\n                value = ' AND '.join(\n                    sorted([_render_condition_value(v, field_type)\n                            for v in value])\n                )\n            elif isinstance(value, (tuple, list, set)) and len(value) != 2:\n                logger.warn('Invalid condition passed in: %s' % condition)\n\n        else:\n            value = _render_condition_value(value, field_type)\n\n        rendered_sub_condition = \"%s%s %s %s\" % (\n            negated, field, condition, value)\n\n        if comparator.get(\"negate\"):\n            negated_conditions.append(rendered_sub_condition)\n        else:\n            normal_conditions.append(rendered_sub_condition)\n\n    rendered_normal = \" AND \".join(normal_conditions)\n    rendered_negated = \" AND \".join(negated_conditions)\n\n    if rendered_normal and rendered_negated:\n        return \"((%s) AND (%s))\" % (rendered_normal, rendered_negated)\n\n    return \"(%s)\" % (rendered_normal or rendered_negated)", "response": "Render a single query condition."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _render_condition_value(value, field_type):\n\n    # BigQuery cannot cast strings to booleans, convert to ints\n    if field_type == \"BOOLEAN\":\n        value = 1 if value else 0\n    elif field_type in (\"STRING\", \"INTEGER\", \"FLOAT\"):\n        value = \"'%s'\" % (value)\n    elif field_type in (\"TIMESTAMP\"):\n        value = \"'%s'\" % (str(value))\n    return \"%s(%s)\" % (field_type, value)", "response": "Render a query condition value."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nrenders the having part of a query.", "response": "def _render_having(having_conditions):\n    \"\"\"Render the having part of a query.\n\n    Parameters\n    ----------\n    having_conditions : list\n        A ``list`` of ``dict``s to filter the rows\n\n    Returns\n    -------\n    str\n        A string that represents the \"having\" part of a query.\n\n    See Also\n    --------\n    render_query : Further clarification of `conditions` formatting.\n    \"\"\"\n    if not having_conditions:\n        return \"\"\n\n    rendered_conditions = []\n\n    for condition in having_conditions:\n        field = condition.get('field')\n        field_type = condition.get('type')\n        comparators = condition.get('comparators')\n\n        if None in (field, field_type, comparators) or not comparators:\n            logger.warn('Invalid condition passed in: %s' % condition)\n            continue\n\n        rendered_conditions.append(\n            _render_condition(field, field_type, comparators))\n\n    if not rendered_conditions:\n        return \"\"\n\n    return \"HAVING %s\" % (\" AND \".join(rendered_conditions))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninitializes the Flask - MQTT addon.", "response": "def init_app(self, app):\n        # type: (Flask) -> None\n        \"\"\"Init the Flask-MQTT addon.\"\"\"\n        self.client_id = app.config.get(\"MQTT_CLIENT_ID\", \"\")\n\n        if isinstance(self.client_id, unicode):\n            self.client._client_id = self.client_id.encode('utf-8')\n        else:\n            self.client._client_id = self.client_id\n\n        self.client._transport = app.config.get(\"MQTT_TRANSPORT\", \"tcp\").lower()\n        self.client._protocol = app.config.get(\"MQTT_PROTOCOL_VERSION\", MQTTv311)\n\n        self.client.on_connect = self._handle_connect\n        self.client.on_disconnect = self._handle_disconnect\n        self.username = app.config.get(\"MQTT_USERNAME\")\n        self.password = app.config.get(\"MQTT_PASSWORD\")\n        self.broker_url = app.config.get(\"MQTT_BROKER_URL\", \"localhost\")\n        self.broker_port = app.config.get(\"MQTT_BROKER_PORT\", 1883)\n        self.tls_enabled = app.config.get(\"MQTT_TLS_ENABLED\", False)\n        self.keepalive = app.config.get(\"MQTT_KEEPALIVE\", 60)\n        self.last_will_topic = app.config.get(\"MQTT_LAST_WILL_TOPIC\")\n        self.last_will_message = app.config.get(\"MQTT_LAST_WILL_MESSAGE\")\n        self.last_will_qos = app.config.get(\"MQTT_LAST_WILL_QOS\", 0)\n        self.last_will_retain = app.config.get(\"MQTT_LAST_WILL_RETAIN\", False)\n\n        if self.tls_enabled:\n            self.tls_ca_certs = app.config[\"MQTT_TLS_CA_CERTS\"]\n            self.tls_certfile = app.config.get(\"MQTT_TLS_CERTFILE\")\n            self.tls_keyfile = app.config.get(\"MQTT_TLS_KEYFILE\")\n            self.tls_cert_reqs = app.config.get(\"MQTT_TLS_CERT_REQS\",\n                                                ssl.CERT_REQUIRED)\n            self.tls_version = app.config.get(\"MQTT_TLS_VERSION\",\n                                              ssl.PROTOCOL_TLSv1)\n            self.tls_ciphers = app.config.get(\"MQTT_TLS_CIPHERS\")\n            self.tls_insecure = app.config.get(\"MQTT_TLS_INSECURE\", False)\n\n        # set last will message\n        if self.last_will_topic is not None:\n            self.client.will_set(\n                self.last_will_topic,\n                self.last_will_message,\n                self.last_will_qos,\n                self.last_will_retain,\n            )\n\n        self._connect()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef subscribe(self, topic, qos=0):\n        # type: (str, int) -> Tuple[int, int]\n        \"\"\"\n        Subscribe to a certain topic.\n\n        :param topic: a string specifying the subscription topic to\n            subscribe to.\n        :param qos: the desired quality of service level for the subscription.\n                    Defaults to 0.\n\n        :rtype: (int, int)\n        :result: (result, mid)\n\n        A topic is a UTF-8 string, which is used by the broker to filter\n        messages for each connected client. A topic consists of one or more\n        topic levels. Each topic level is separated by a forward slash\n        (topic level separator).\n\n        The function returns a tuple (result, mid), where result is\n        MQTT_ERR_SUCCESS to indicate success or (MQTT_ERR_NO_CONN, None) if the\n        client is not currently connected.  mid is the message ID for the\n        subscribe request. The mid value can be used to track the subscribe\n        request by checking against the mid argument in the on_subscribe()\n        callback if it is defined.\n\n        **Topic example:** `myhome/groundfloor/livingroom/temperature`\n\n        \"\"\"\n        # TODO: add support for list of topics\n\n        # don't subscribe if already subscribed\n        # try to subscribe\n        result, mid = self.client.subscribe(topic=topic, qos=qos)\n\n        # if successful add to topics\n        if result == MQTT_ERR_SUCCESS:\n            self.topics[topic] = TopicQos(topic=topic, qos=qos)\n            logger.debug('Subscribed to topic: {0}, qos: {1}'\n                         .format(topic, qos))\n        else:\n            logger.error('Error {0} subscribing to topic: {1}'\n                         .format(result, topic))\n\n        return (result, mid)", "response": "Subscribe to a certain topic and qos."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\npublishing a message to the broker.", "response": "def publish(self, topic, payload=None, qos=0, retain=False):\n        # type: (str, bytes, int, bool) -> Tuple[int, int]\n        \"\"\"\n        Send a message to the broker.\n\n        :param topic: the topic that the message should be published on\n        :param payload: the actual message to send. If not given, or set to\n                        None a zero length message will be used. Passing an\n                        int or float will result in the payload being\n                        converted to a string representing that number.\n                        If you wish to send a true int/float, use struct.pack()\n                        to create the payload you require.\n        :param qos: the quality of service level to use\n        :param retain: if set to True, the message will be set as the\n                       \"last known good\"/retained message for the topic\n\n        :returns: Returns a tuple (result, mid), where result is\n                  MQTT_ERR_SUCCESS to indicate success or MQTT_ERR_NO_CONN\n                  if the client is not currently connected. mid is the message\n                  ID for the publish request.\n\n        \"\"\"\n        if not self.connected:\n            self.client.reconnect()\n\n        result, mid = self.client.publish(topic, payload, qos, retain)\n        if result == MQTT_ERR_SUCCESS:\n            logger.debug('Published topic {0}: {1}'.format(topic, payload))\n        else:\n            logger.error('Error {0} publishing topic {1}'\n                         .format(result, topic))\n\n        return (result, mid)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef on_message(self):\n        # type: () -> Callable\n        \"\"\"Decorator.\n\n        Decorator to handle all messages that have been subscribed and that\n        are not handled via the `on_message` decorator.\n\n        **Note:** Unlike as written in the paho mqtt documentation this\n        callback will not be called if there exists an topic-specific callback\n        added by the `on_topic` decorator.\n\n        **Example Usage:**::\n\n            @mqtt.on_message()\n            def handle_messages(client, userdata, message):\n                print('Received message on topic {}: {}'\n                      .format(message.topic, message.payload.decode()))\n        \"\"\"\n        def decorator(handler):\n            # type: (Callable) -> Callable\n            self.client.on_message = handler\n            return handler\n\n        return decorator", "response": "Decorator to handle all messages that have been subscribed and that have not yet been received via the on_message method."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndecorating a callback function to handle subscritions.", "response": "def on_subscribe(self):\n        # type: () -> Callable\n        \"\"\"Decorate a callback function to handle subscritions.\n\n        **Usage:**::\n\n            @mqtt.on_subscribe()\n            def handle_subscribe(client, userdata, mid, granted_qos):\n                print('Subscription id {} granted with qos {}.'\n                      .format(mid, granted_qos))\n        \"\"\"\n        def decorator(handler):\n            # type: (Callable) -> Callable\n            self.client.on_subscribe = handler\n            return handler\n\n        return decorator"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef on_unsubscribe(self):\n        # type: () -> Callable\n        \"\"\"Decorate a callback funtion to handle unsubscribtions.\n\n        **Usage:**::\n\n            @mqtt.unsubscribe()\n            def handle_unsubscribe(client, userdata, mid)\n                print('Unsubscribed from topic (id: {})'\n                      .format(mid)')\n        \"\"\"\n        def decorator(handler):\n            # type: (Callable) -> Callable\n            self.client.on_unsubscribe = handler\n            return handler\n\n        return decorator", "response": "Decorate a callback funtion to handle unsubscribtions."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef on_log(self):\n        # type: () -> Callable\n        \"\"\"Decorate a callback function to handle MQTT logging.\n\n        **Example Usage:**\n\n        ::\n\n            @mqtt.on_log()\n            def handle_logging(client, userdata, level, buf):\n                print(client, userdata, level, buf)\n        \"\"\"\n        def decorator(handler):\n            # type: (Callable) -> Callable\n            self.client.on_log = handler\n            return handler\n\n        return decorator", "response": "Decorate a callback function to handle MQTT logging."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef list():\n    s3 = boto3.resource('s3')\n    return [b.name for b in s3.buckets.all()]", "response": "Lists buckets by name."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef delete(self, key=None):\n\n        # Delete the whole bucket.\n        if key is None:\n            # Delete everything in the bucket.\n            for key in self.all():\n                key.delete()\n\n            # Delete the bucket.\n            return self._boto_bucket.delete()\n\n        # If a key was passed, delete they key.\n        k = self.key(key)\n        return k.delete()", "response": "Deletes the given key or the whole bucket."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the key to the given value.", "response": "def set(self, value, metadata=dict(), content_type=None):\n        \"\"\"Sets the key to the given value.\"\"\"\n        return self._boto_object.put(Body=value, Metadata=metadata, ContentType=content_type)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef rename(self, new_name):\n        # Write the new object.\n        self.bucket.set(new_name, self.get(), self.meta)\n\n        # Delete the current key.\n        self.delete()\n\n        # Set the new name.\n        self.name = new_name", "response": "Renames the key to a given new name."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_public(self):\n        for grant in self._boto_object.Acl().grants:\n            if 'AllUsers' in grant['Grantee'].get('URI', ''):\n                if grant['Permission'] == 'READ':\n                    return True\n\n        return False", "response": "Returns True if the public - read ACL is set for the Key."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the public URL for the given key.", "response": "def url(self):\n        \"\"\"Returns the public URL for the given key.\"\"\"\n        if self.is_public:\n            return '{0}/{1}/{2}'.format(\n                self.bucket._boto_s3.meta.client.meta.endpoint_url,\n                self.bucket.name,\n                self.name\n            )\n        else:\n            raise ValueError('{0!r} does not have the public-read ACL set. '\n                             'Use the make_public() method to allow for '\n                             'public URL sharing.'.format(self.name))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef temp_url(self, duration=120):\n        return self.bucket._boto_s3.meta.client.generate_presigned_url(\n            'get_object',\n            Params={'Bucket': self.bucket.name, 'Key': self.name},\n            ExpiresIn=duration\n        )", "response": "Returns a temporary URL for the given key."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef eprint(*args, **kwargs):\n    end = kwargs.get(\"end\", \"\\n\")\n    sep = kwargs.get(\"sep\", \" \")\n    (filename, lineno) = inspect.stack()[1][1:3]\n    print(\"{}:{}: \".format(filename, lineno), end=\"\")\n    print(*args, end=end, file=sys.stderr, sep=sep)", "response": "Print an error message to standard error."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef formatException(type, value, tb):\n\n    # Absolute paths to site-packages\n    packages = tuple(join(abspath(p), \"\") for p in sys.path[1:])\n\n    # Highlight lines not referring to files in site-packages\n    lines = []\n    for line in format_exception(type, value, tb):\n        matches = re.search(r\"^  File \\\"([^\\\"]+)\\\", line \\d+, in .+\", line)\n        if matches and matches.group(1).startswith(packages):\n            lines += line\n        else:\n            matches = re.search(r\"^(\\s*)(.*?)(\\s*)$\", line, re.DOTALL)\n            lines.append(matches.group(1) + colored(matches.group(2), \"yellow\") + matches.group(3))\n    return \"\".join(lines).rstrip()", "response": "Format traceback into a single string."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads a single character from standard input and return the equivalent char.", "response": "def get_char(prompt=None):\n    \"\"\"\n    Read a line of text from standard input and return the equivalent char;\n    if text is not a single char, user is prompted to retry. If line can't\n    be read, return None.\n    \"\"\"\n    while True:\n        s = get_string(prompt)\n        if s is None:\n            return None\n        if len(s) == 1:\n            return s[0]\n\n        # Temporarily here for backwards compatibility\n        if prompt is None:\n            print(\"Retry: \", end=\"\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads a line of text from standard input and return the equivalent float.", "response": "def get_float(prompt=None):\n    \"\"\"\n    Read a line of text from standard input and return the equivalent float\n    as precisely as possible; if text does not represent a double, user is\n    prompted to retry. If line can't be read, return None.\n    \"\"\"\n    while True:\n        s = get_string(prompt)\n        if s is None:\n            return None\n        if len(s) > 0 and re.search(r\"^[+-]?\\d*(?:\\.\\d*)?$\", s):\n            try:\n                return float(s)\n            except ValueError:\n                pass\n\n        # Temporarily here for backwards compatibility\n        if prompt is None:\n            print(\"Retry: \", end=\"\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread a line of text from standard input and return the equivalent int.", "response": "def get_int(prompt=None):\n    \"\"\"\n    Read a line of text from standard input and return the equivalent int;\n    if text does not represent an int, user is prompted to retry. If line\n    can't be read, return None.\n    \"\"\"\n    while True:\n        s = get_string(prompt)\n        if s is None:\n            return None\n        if re.search(r\"^[+-]?\\d+$\", s):\n            try:\n                i = int(s, 10)\n                if type(i) is int:  # Could become long in Python 2\n                    return i\n            except ValueError:\n                pass\n\n        # Temporarily here for backwards compatibility\n        if prompt is None:\n            print(\"Retry: \", end=\"\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_string(prompt=None):\n    try:\n        if prompt is not None:\n            print(prompt, end=\"\")\n        s = sys.stdin.readline()\n        if not s:\n            return None\n        return re.sub(r\"(?:\\r|\\r\\n|\\n)$\", \"\", s)\n    except KeyboardInterrupt:\n        sys.exit(\"\")\n    except ValueError:\n        return None", "response": "Reads a line of text from standard input and returns it as a string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _connect(dbapi_connection, connection_record):\n\n    # If back end is sqlite\n    if type(dbapi_connection) is sqlite3.Connection:\n\n        # Respect foreign key constraints by default\n        cursor = dbapi_connection.cursor()\n        cursor.execute(\"PRAGMA foreign_keys=ON\")\n        cursor.close()", "response": "Enables foreign key support."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing an exception returns its message.", "response": "def _parse(self, e):\n        \"\"\"Parses an exception, returns its message.\"\"\"\n\n        # MySQL\n        matches = re.search(r\"^\\(_mysql_exceptions\\.OperationalError\\) \\(\\d+, \\\"(.+)\\\"\\)$\", str(e))\n        if matches:\n            return matches.group(1)\n\n        # PostgreSQL\n        matches = re.search(r\"^\\(psycopg2\\.OperationalError\\) (.+)$\", str(e))\n        if matches:\n            return matches.group(1)\n\n        # SQLite\n        matches = re.search(r\"^\\(sqlite3\\.OperationalError\\) (.+)$\", str(e))\n        if matches:\n            return matches.group(1)\n\n        # Default\n        return str(e)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef execute(self, text, **params):\n\n        class UserDefinedType(sqlalchemy.TypeDecorator):\n            \"\"\"Add support for expandable values, a la https://github.com/sqlalchemy/sqlalchemy/issues/3953.\"\"\"\n\n            # Required class-level attribute\n            # https://docs.sqlalchemy.org/en/latest/core/custom_types.html#sqlalchemy.types.TypeDecorator\n            impl = sqlalchemy.types.UserDefinedType\n\n            def process_literal_param(self, value, dialect):\n                \"\"\"Receive a literal parameter value to be rendered inline within a statement.\"\"\"\n\n                def process(value):\n                    \"\"\"Render a literal value, escaping as needed.\"\"\"\n\n                    # bool\n                    if type(value) is bool:\n                        return sqlalchemy.types.Boolean().literal_processor(dialect)(value)\n\n                    # datetime.date\n                    elif type(value) is datetime.date:\n                        return sqlalchemy.types.String().literal_processor(dialect)(value.strftime(\"%Y-%m-%d\"))\n\n                    # datetime.datetime\n                    elif type(value) is datetime.datetime:\n                        return sqlalchemy.types.String().literal_processor(dialect)(value.strftime(\"%Y-%m-%d %H:%M:%S\"))\n\n                    # datetime.time\n                    elif type(value) is datetime.time:\n                        return sqlalchemy.types.String().literal_processor(dialect)(value.strftime(\"%H:%M:%S\"))\n\n                    # float\n                    elif type(value) is float:\n                        return sqlalchemy.types.Float().literal_processor(dialect)(value)\n\n                    # int\n                    elif type(value) is int:\n                        return sqlalchemy.types.Integer().literal_processor(dialect)(value)\n\n                    # long\n                    elif sys.version_info.major != 3 and type(value) is long:\n                        return sqlalchemy.types.Integer().literal_processor(dialect)(value)\n\n                    # str\n                    elif type(value) is str:\n                        return sqlalchemy.types.String().literal_processor(dialect)(value)\n\n                    # None\n                    elif type(value) is sqlalchemy.sql.elements.Null:\n                        return sqlalchemy.types.NullType().literal_processor(dialect)(value)\n\n                    # Unsupported value\n                    raise RuntimeError(\"unsupported value\")\n\n                # Process value(s), separating with commas as needed\n                if type(value) is list:\n                    return \", \".join([process(v) for v in value])\n                else:\n                    return process(value)\n\n        # Allow only one statement at a time, since SQLite doesn't support multiple\n        # https://docs.python.org/3/library/sqlite3.html#sqlite3.Cursor.execute\n        if len(sqlparse.split(text)) > 1:\n            raise RuntimeError(\"too many statements at once\")\n\n        # Raise exceptions for warnings\n        warnings.filterwarnings(\"error\")\n\n        # Prepare, execute statement\n        try:\n\n            # Construct a new TextClause clause\n            statement = sqlalchemy.text(text)\n\n            # Iterate over parameters\n            for key, value in params.items():\n\n                # Translate None to NULL\n                if value is None:\n                    value = sqlalchemy.sql.null()\n\n                # Bind parameters before statement reaches database, so that bound parameters appear in exceptions\n                # http://docs.sqlalchemy.org/en/latest/core/sqlelement.html#sqlalchemy.sql.expression.text\n                statement = statement.bindparams(sqlalchemy.bindparam(\n                    key, value=value, type_=UserDefinedType()))\n\n            # Stringify bound parameters\n            # http://docs.sqlalchemy.org/en/latest/faq/sqlexpressions.html#how-do-i-render-sql-expressions-as-strings-possibly-with-bound-parameters-inlined\n            statement = str(statement.compile(compile_kwargs={\"literal_binds\": True}))\n\n            # Statement for logging\n            log = re.sub(r\"\\n\\s*\", \" \", sqlparse.format(statement, reindent=True))\n\n            # Execute statement\n            result = self.engine.execute(statement)\n\n            # If SELECT (or INSERT with RETURNING), return result set as list of dict objects\n            if re.search(r\"^\\s*SELECT\", statement, re.I):\n\n                # Coerce any decimal.Decimal objects to float objects\n                # https://groups.google.com/d/msg/sqlalchemy/0qXMYJvq8SA/oqtvMD9Uw-kJ\n                rows = [dict(row) for row in result.fetchall()]\n                for row in rows:\n                    for column in row:\n                        if type(row[column]) is decimal.Decimal:\n                            row[column] = float(row[column])\n                ret = rows\n\n            # If INSERT, return primary key value for a newly inserted row\n            elif re.search(r\"^\\s*INSERT\", statement, re.I):\n                if self.engine.url.get_backend_name() in [\"postgres\", \"postgresql\"]:\n                    result = self.engine.execute(sqlalchemy.text(\"SELECT LASTVAL()\"))\n                    ret = result.first()[0]\n                else:\n                    ret = result.lastrowid\n\n            # If DELETE or UPDATE, return number of rows matched\n            elif re.search(r\"^\\s*(?:DELETE|UPDATE)\", statement, re.I):\n                ret = result.rowcount\n\n            # If some other statement, return True unless exception\n            else:\n                ret = True\n\n        # If constraint violated, return None\n        except sqlalchemy.exc.IntegrityError:\n            self.logger.debug(termcolor.colored(log, \"yellow\"))\n            return None\n\n        # If user errror\n        except sqlalchemy.exc.OperationalError as e:\n            self.logger.debug(termcolor.colored(log, \"red\"))\n            e = RuntimeError(self._parse(e))\n            e.__cause__ = None\n            raise e\n\n        # Return value\n        else:\n            self.logger.debug(termcolor.colored(log, \"green\"))\n            return ret", "response": "Execute a SQL statement."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate a shared access signature for the account service.", "response": "def generate_account_shared_access_signature(self, resource_types, permission,\n                                                 expiry, start=None, ip=None, protocol=None):\n        '''\n        Generates a shared access signature for the table service.\n        Use the returned signature with the sas_token parameter of TableService.\n\n        :param ResourceTypes resource_types:\n            Specifies the resource types that are accessible with the account SAS.\n        :param AccountPermissions permission:\n            The permissions associated with the shared access signature. The \n            user is restricted to operations allowed by the permissions. \n            Required unless an id is given referencing a stored access policy \n            which contains this field. This field must be omitted if it has been \n            specified in an associated stored access policy.\n        :param expiry:\n            The time at which the shared access signature becomes invalid. \n            Required unless an id is given referencing a stored access policy \n            which contains this field. This field must be omitted if it has \n            been specified in an associated stored access policy. Azure will always \n            convert values to UTC. If a date is passed in without timezone info, it \n            is assumed to be UTC.\n        :type expiry: datetime or str\n        :param start:\n            The time at which the shared access signature becomes valid. If \n            omitted, start time for this call is assumed to be the time when the \n            storage service receives the request. Azure will always convert values \n            to UTC. If a date is passed in without timezone info, it is assumed to \n            be UTC.\n        :type start: datetime or str\n        :param str ip:\n            Specifies an IP address or a range of IP addresses from which to accept requests.\n            If the IP address from which the request originates does not match the IP address\n            or address range specified on the SAS token, the request is not authenticated.\n            For example, specifying sip=168.1.5.65 or sip=168.1.5.60-168.1.5.70 on the SAS\n            restricts the request to those IP addresses.\n        :param str protocol:\n            Specifies the protocol permitted for a request made. The default value\n            is https,http. See :class:`~azure.storage.common.models.Protocol` for possible values.\n        :return: A Shared Access Signature (sas) token.\n        :rtype: str\n        '''\n        _validate_not_none('self.account_name', self.account_name)\n        _validate_not_none('self.account_key', self.account_key)\n\n        sas = TableSharedAccessSignature(self.account_name, self.account_key)\n        return sas.generate_account(TableServices(), resource_types, permission,\n                                    expiry, start=start, ip=ip, protocol=protocol)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef generate_table_shared_access_signature(self, table_name, permission=None,\n                                               expiry=None, start=None, id=None,\n                                               ip=None, protocol=None,\n                                               start_pk=None, start_rk=None,\n                                               end_pk=None, end_rk=None):\n        '''\n        Generates a shared access signature for the table.\n        Use the returned signature with the sas_token parameter of TableService.\n\n        :param str table_name:\n            The name of the table to create a SAS token for.\n        :param TablePermissions permission:\n            The permissions associated with the shared access signature. The \n            user is restricted to operations allowed by the permissions. \n            Required unless an id is given referencing a stored access policy \n            which contains this field. This field must be omitted if it has been \n            specified in an associated stored access policy.\n        :param expiry:\n            The time at which the shared access signature becomes invalid. \n            Required unless an id is given referencing a stored access policy \n            which contains this field. This field must be omitted if it has \n            been specified in an associated stored access policy. Azure will always \n            convert values to UTC. If a date is passed in without timezone info, it \n            is assumed to be UTC.\n        :type expiry: datetime or str\n        :param start:\n            The time at which the shared access signature becomes valid. If \n            omitted, start time for this call is assumed to be the time when the \n            storage service receives the request. Azure will always convert values \n            to UTC. If a date is passed in without timezone info, it is assumed to \n            be UTC.\n        :type start: datetime or str\n        :param str id:\n            A unique value up to 64 characters in length that correlates to a \n            stored access policy. To create a stored access policy, use :func:`~set_table_acl`.\n        :param str ip:\n            Specifies an IP address or a range of IP addresses from which to accept requests.\n            If the IP address from which the request originates does not match the IP address\n            or address range specified on the SAS token, the request is not authenticated.\n            For example, specifying sip='168.1.5.65' or sip='168.1.5.60-168.1.5.70' on the SAS\n            restricts the request to those IP addresses.\n        :param str protocol:\n            Specifies the protocol permitted for a request made. The default value\n            is https,http. See :class:`~azure.storage.common.models.Protocol` for possible values.\n        :param str start_pk:\n            The minimum partition key accessible with this shared access \n            signature. startpk must accompany startrk. Key values are inclusive. \n            If omitted, there is no lower bound on the table entities that can \n            be accessed.\n        :param str start_rk:\n            The minimum row key accessible with this shared access signature. \n            startpk must accompany startrk. Key values are inclusive. If \n            omitted, there is no lower bound on the table entities that can be \n            accessed.\n        :param str end_pk:\n            The maximum partition key accessible with this shared access \n            signature. endpk must accompany endrk. Key values are inclusive. If \n            omitted, there is no upper bound on the table entities that can be \n            accessed.\n        :param str end_rk:\n            The maximum row key accessible with this shared access signature. \n            endpk must accompany endrk. Key values are inclusive. If omitted, \n            there is no upper bound on the table entities that can be accessed.\n        :return: A Shared Access Signature (sas) token.\n        :rtype: str\n        '''\n        _validate_not_none('table_name', table_name)\n        _validate_not_none('self.account_name', self.account_name)\n        _validate_not_none('self.account_key', self.account_key)\n\n        sas = TableSharedAccessSignature(self.account_name, self.account_key)\n        return sas.generate_table(\n            table_name,\n            permission=permission,\n            expiry=expiry,\n            start=start,\n            id=id,\n            ip=ip,\n            protocol=protocol,\n            start_pk=start_pk,\n            start_rk=start_rk,\n            end_pk=end_pk,\n            end_rk=end_rk,\n        )", "response": "Generates a shared access signature for the specified table."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_table_service_stats(self, timeout=None):\n        '''\n        Retrieves statistics related to replication for the Table service. It is \n        only available when read-access geo-redundant replication is enabled for \n        the storage account.\n\n        With geo-redundant replication, Azure Storage maintains your data durable \n        in two locations. In both locations, Azure Storage constantly maintains \n        multiple healthy replicas of your data. The location where you read, \n        create, update, or delete data is the primary storage account location. \n        The primary location exists in the region you choose at the time you \n        create an account via the Azure Management Azure classic portal, for \n        example, North Central US. The location to which your data is replicated \n        is the secondary location. The secondary location is automatically \n        determined based on the location of the primary; it is in a second data \n        center that resides in the same region as the primary location. Read-only \n        access is available from the secondary location, if read-access geo-redundant \n        replication is enabled for your storage account.\n\n        :param int timeout:\n            The timeout parameter is expressed in seconds.\n        :return: The table service stats.\n        :rtype: :class:`~azure.storage.common.models.ServiceStats`\n        '''\n        request = HTTPRequest()\n        request.method = 'GET'\n        request.host_locations = self._get_host_locations(primary=False, secondary=True)\n        request.path = '/'\n        request.query = {\n            'restype': 'service',\n            'comp': 'stats',\n            'timeout': _int_to_str(timeout),\n        }\n\n        return self._perform_request(request, _convert_xml_to_service_stats)", "response": "Retrieves statistics related to replication for the Table service."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretrieves the properties of a storage account s Table service including logging analytics and CORS rules.", "response": "def get_table_service_properties(self, timeout=None):\n        '''\n        Gets the properties of a storage account's Table service, including\n        logging, analytics and CORS rules.\n\n        :param int timeout:\n            The server timeout, expressed in seconds.\n        :return: The table service properties.\n        :rtype: :class:`~azure.storage.common.models.ServiceProperties`\n        '''\n        request = HTTPRequest()\n        request.method = 'GET'\n        request.host_locations = self._get_host_locations(secondary=True)\n        request.path = '/'\n        request.query = {\n            'restype': 'service',\n            'comp': 'properties',\n            'timeout': _int_to_str(timeout),\n        }\n\n        return self._perform_request(request, _convert_xml_to_service_properties)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef list_tables(self, num_results=None, marker=None, timeout=None):\n        '''\n        Returns a generator to list the tables. The generator will lazily follow \n        the continuation tokens returned by the service and stop when all tables \n        have been returned or num_results is reached.\n\n        If num_results is specified and the account has more than that number of \n        tables, the generator will have a populated next_marker field once it \n        finishes. This marker can be used to create a new generator if more \n        results are desired.\n\n        :param int num_results:\n            The maximum number of tables to return.\n        :param marker:\n            An opaque continuation object. This value can be retrieved from the \n            next_marker field of a previous generator object if num_results was \n            specified and that generator has finished enumerating results. If \n            specified, this generator will begin returning results from the point \n            where the previous generator stopped.\n        :type marker: obj\n        :param int timeout:\n            The server timeout, expressed in seconds. This function may make multiple \n            calls to the service in which case the timeout value specified will be \n            applied to each individual call.\n        :return: A generator which produces :class:`~azure.storage.common.models.table.Table` objects.\n        :rtype: :class:`~azure.storage.common.models.ListGenerator`:\n        '''\n        operation_context = _OperationContext(location_lock=True)\n        kwargs = {'max_results': num_results, 'marker': marker, 'timeout': timeout,\n                  '_context': operation_context}\n        resp = self._list_tables(**kwargs)\n\n        return ListGenerator(resp, self._list_tables, (), kwargs)", "response": "Returns a generator to list the tables in the account."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list of tables under the specified account.", "response": "def _list_tables(self, max_results=None, marker=None, timeout=None, _context=None):\n        '''\n        Returns a list of tables under the specified account. Makes a single list \n        request to the service. Used internally by the list_tables method.\n\n        :param int max_results:\n            The maximum number of tables to return. A single list request may \n            return up to 1000 tables and potentially a continuation token which \n            should be followed to get additional resutls.\n        :param marker:\n            A dictionary which identifies the portion of the query to be\n            returned with the next query operation. The operation returns a\n            next_marker element within the response body if the list returned\n            was not complete. This value may then be used as a query parameter\n            in a subsequent call to request the next portion of the list of\n            tables. The marker value is opaque to the client.\n        :type marker: obj\n        :param int timeout:\n            The server timeout, expressed in seconds.\n        :return: A list of tables, potentially with a next_marker property.\n        :rtype: list(:class:`~azure.storage.common.models.table.Table`)\n        '''\n        request = HTTPRequest()\n        request.method = 'GET'\n        request.host_locations = self._get_host_locations(secondary=True)\n        request.path = '/Tables'\n        request.headers = {'Accept': TablePayloadFormat.JSON_NO_METADATA}\n        request.query = {\n            '$top': _int_to_str(max_results),\n            'NextTableName': _to_str(marker),\n            'timeout': _int_to_str(timeout),\n        }\n\n        return self._perform_request(request, _convert_json_response_to_tables,\n                                     operation_context=_context)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndelete the specified table and all of its data.", "response": "def delete_table(self, table_name, fail_not_exist=False, timeout=None):\n        '''\n        Deletes the specified table and any data it contains.\n\n        When a table is successfully deleted, it is immediately marked for deletion \n        and is no longer accessible to clients. The table is later removed from \n        the Table service during garbage collection.\n\n        Note that deleting a table is likely to take at least 40 seconds to complete. \n        If an operation is attempted against the table while it was being deleted, \n        an :class:`AzureConflictHttpError` will be thrown.\n\n        :param str table_name:\n            The name of the table to delete.\n        :param bool fail_not_exist:\n            Specifies whether to throw an exception if the table doesn't exist.\n        :param int timeout:\n            The server timeout, expressed in seconds.\n        :return:\n            A boolean indicating whether the table was deleted. If fail_not_exist \n            was set to True, this will throw instead of returning false.\n        :rtype: bool\n        '''\n        _validate_not_none('table_name', table_name)\n        request = HTTPRequest()\n        request.method = 'DELETE'\n        request.host_locations = self._get_host_locations()\n        request.path = '/Tables(\\'' + _to_str(table_name) + '\\')'\n        request.query = {'timeout': _int_to_str(timeout)}\n        request.headers = {_DEFAULT_ACCEPT_HEADER[0]: _DEFAULT_ACCEPT_HEADER[1]}\n\n        if not fail_not_exist:\n            try:\n                self._perform_request(request)\n                return True\n            except AzureHttpError as ex:\n                _dont_fail_not_exist(ex)\n                return False\n        else:\n            self._perform_request(request)\n            return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_table_acl(self, table_name, timeout=None):\n        '''\n        Returns details about any stored access policies specified on the\n        table that may be used with Shared Access Signatures.\n\n        :param str table_name:\n            The name of an existing table.\n        :param int timeout:\n            The server timeout, expressed in seconds.\n        :return: A dictionary of access policies associated with the table.\n        :rtype: dict(str, :class:`~azure.storage.common.models.AccessPolicy`)\n        '''\n        _validate_not_none('table_name', table_name)\n        request = HTTPRequest()\n        request.method = 'GET'\n        request.host_locations = self._get_host_locations(secondary=True)\n        request.path = '/' + _to_str(table_name)\n        request.query = {\n            'comp': 'acl',\n            'timeout': _int_to_str(timeout),\n        }\n\n        return self._perform_request(request, _convert_xml_to_signed_identifiers)", "response": "Returns details about any stored access policies associated with a table."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_table_acl(self, table_name, signed_identifiers=None, timeout=None):\n        '''\n        Sets stored access policies for the table that may be used with Shared \n        Access Signatures. \n        \n        When you set permissions for a table, the existing permissions are replaced. \n        To update the table's permissions, call :func:`~get_table_acl` to fetch \n        all access policies associated with the table, modify the access policy \n        that you wish to change, and then call this function with the complete \n        set of data to perform the update.\n\n        When you establish a stored access policy on a table, it may take up to \n        30 seconds to take effect. During this interval, a shared access signature \n        that is associated with the stored access policy will throw an \n        :class:`AzureHttpError` until the access policy becomes active.\n\n        :param str table_name:\n            The name of an existing table.\n        :param signed_identifiers:\n            A dictionary of access policies to associate with the table. The \n            dictionary may contain up to 5 elements. An empty dictionary \n            will clear the access policies set on the service. \n        :type signed_identifiers: dict(str, :class:`~azure.storage.common.models.AccessPolicy`)\n        :param int timeout:\n            The server timeout, expressed in seconds.\n        '''\n        _validate_not_none('table_name', table_name)\n        _validate_access_policies(signed_identifiers)\n        request = HTTPRequest()\n        request.method = 'PUT'\n        request.host_locations = self._get_host_locations()\n        request.path = '/' + _to_str(table_name)\n        request.query = {\n            'comp': 'acl',\n            'timeout': _int_to_str(timeout),\n        }\n        request.body = _get_request_body(\n            _convert_signed_identifiers_to_xml(signed_identifiers))\n\n        self._perform_request(request)", "response": "Sets the permissions for a table."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a generator to list the entities in the specified table.", "response": "def query_entities(self, table_name, filter=None, select=None, num_results=None,\n                       marker=None, accept=TablePayloadFormat.JSON_MINIMAL_METADATA,\n                       property_resolver=None, timeout=None):\n        '''\n        Returns a generator to list the entities in the table specified. The \n        generator will lazily follow the continuation tokens returned by the \n        service and stop when all entities have been returned or num_results is\n        reached.\n\n        If num_results is specified and the account has more than that number of\n        entities, the generator will have a populated next_marker field once it \n        finishes. This marker can be used to create a new generator if more \n        results are desired.\n\n        :param str table_name:\n            The name of the table to query.\n        :param str filter:\n            Returns only entities that satisfy the specified filter. Note that \n            no more than 15 discrete comparisons are permitted within a $filter \n            string. See http://msdn.microsoft.com/en-us/library/windowsazure/dd894031.aspx \n            for more information on constructing filters.\n        :param str select:\n            Returns only the desired properties of an entity from the set.\n        :param int num_results:\n            The maximum number of entities to return.\n        :param marker:\n            An opaque continuation object. This value can be retrieved from the \n            next_marker field of a previous generator object if max_results was \n            specified and that generator has finished enumerating results. If \n            specified, this generator will begin returning results from the point \n            where the previous generator stopped.\n        :type marker: obj\n        :param str accept:\n            Specifies the accepted content type of the response payload. See \n            :class:`~azure.storage.table.models.TablePayloadFormat` for possible \n            values.\n        :param property_resolver:\n            A function which given the partition key, row key, property name, \n            property value, and the property EdmType if returned by the service, \n            returns the EdmType of the property. Generally used if accept is set \n            to JSON_NO_METADATA.\n        :type property_resolver: func(pk, rk, prop_name, prop_value, service_edm_type)\n        :param int timeout:\n            The server timeout, expressed in seconds. This function may make multiple \n            calls to the service in which case the timeout value specified will be \n            applied to each individual call.\n        :return: A generator which produces :class:`~azure.storage.table.models.Entity` objects.\n        :rtype: :class:`~azure.storage.common.models.ListGenerator`\n        '''\n\n        operation_context = _OperationContext(location_lock=True)\n        if self.key_encryption_key is not None or self.key_resolver_function is not None:\n            # If query already requests all properties, no need to add the metadata columns\n            if select is not None and select != '*':\n                select += ',_ClientEncryptionMetadata1,_ClientEncryptionMetadata2'\n\n        args = (table_name,)\n        kwargs = {'filter': filter, 'select': select, 'max_results': num_results, 'marker': marker,\n                  'accept': accept, 'property_resolver': property_resolver, 'timeout': timeout,\n                  '_context': operation_context}\n        resp = self._query_entities(*args, **kwargs)\n\n        return ListGenerator(resp, self._query_entities, args, kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nquery the table for the specified entity.", "response": "def _query_entities(self, table_name, filter=None, select=None, max_results=None,\n                        marker=None, accept=TablePayloadFormat.JSON_MINIMAL_METADATA,\n                        property_resolver=None, timeout=None, _context=None):\n        '''\n        Returns a list of entities under the specified table. Makes a single list \n        request to the service. Used internally by the query_entities method.\n\n        :param str table_name:\n            The name of the table to query.\n        :param str filter:\n            Returns only entities that satisfy the specified filter. Note that \n            no more than 15 discrete comparisons are permitted within a $filter \n            string. See http://msdn.microsoft.com/en-us/library/windowsazure/dd894031.aspx \n            for more information on constructing filters.\n        :param str select:\n            Returns only the desired properties of an entity from the set.\n        :param int max_results:\n            The maximum number of entities to return.\n        :param obj marker:\n            A dictionary which identifies the portion of the query to be\n            returned with the next query operation. The operation returns a\n            next_marker element within the response body if the list returned\n            was not complete. This value may then be used as a query parameter\n            in a subsequent call to request the next portion of the list of\n            table. The marker value is opaque to the client.\n        :param str accept:\n            Specifies the accepted content type of the response payload. See \n            :class:`~azure.storage.table.models.TablePayloadFormat` for possible \n            values.\n        :param property_resolver:\n            A function which given the partition key, row key, property name, \n            property value, and the property EdmType if returned by the service, \n            returns the EdmType of the property. Generally used if accept is set \n            to JSON_NO_METADATA.\n        :type property_resolver: func(pk, rk, prop_name, prop_value, service_edm_type)\n        :param int timeout:\n            The server timeout, expressed in seconds.\n        :return: A list of entities, potentially with a next_marker property.\n        :rtype: list(:class:`~azure.storage.table.models.Entity`)\n        '''\n        _validate_not_none('table_name', table_name)\n        _validate_not_none('accept', accept)\n        next_partition_key = None if marker is None else marker.get('nextpartitionkey')\n        next_row_key = None if marker is None else marker.get('nextrowkey')\n\n        request = HTTPRequest()\n        request.method = 'GET'\n        request.host_locations = self._get_host_locations(secondary=True)\n        request.path = '/' + _to_str(table_name) + '()'\n        request.headers = {'Accept': _to_str(accept)}\n        request.query = {\n            '$filter': _to_str(filter),\n            '$select': _to_str(select),\n            '$top': _int_to_str(max_results),\n            'NextPartitionKey': _to_str(next_partition_key),\n            'NextRowKey': _to_str(next_row_key),\n            'timeout': _int_to_str(timeout),\n        }\n\n        return self._perform_request(request, _convert_json_response_to_entities,\n                                     [property_resolver, self.require_encryption,\n                                      self.key_encryption_key, self.key_resolver_function],\n                                     operation_context=_context)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef batch(self, table_name, timeout=None):\n        '''\n        Creates a batch object which can be used as a context manager. Commits the batch on exit.\n\n        :param str table_name:\n            The name of the table to commit the batch to.\n        :param int timeout:\n            The server timeout, expressed in seconds.\n        '''\n        batch = TableBatch(self.require_encryption, self.key_encryption_key, self.encryption_resolver_function)\n        yield batch\n        self.commit_batch(table_name, batch, timeout=timeout)", "response": "A context manager that commits a batch on exit."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_entity(self, table_name, partition_key, row_key, select=None,\n                   accept=TablePayloadFormat.JSON_MINIMAL_METADATA,\n                   property_resolver=None, timeout=None):\n        '''\n        Get an entity from the specified table. Throws if the entity does not exist.\n\n        :param str table_name:\n            The name of the table to get the entity from.\n        :param str partition_key:\n            The PartitionKey of the entity.\n        :param str row_key:\n            The RowKey of the entity.\n        :param str select:\n            Returns only the desired properties of an entity from the set.\n        :param str accept:\n            Specifies the accepted content type of the response payload. See \n            :class:`~azure.storage.table.models.TablePayloadFormat` for possible \n            values.\n        :param property_resolver:\n            A function which given the partition key, row key, property name, \n            property value, and the property EdmType if returned by the service, \n            returns the EdmType of the property. Generally used if accept is set \n            to JSON_NO_METADATA.\n        :type property_resolver: func(pk, rk, prop_name, prop_value, service_edm_type)\n        :param int timeout:\n            The server timeout, expressed in seconds.\n        :return: The retrieved entity.\n        :rtype: :class:`~azure.storage.table.models.Entity`\n        '''\n        _validate_not_none('table_name', table_name)\n        request = _get_entity(partition_key, row_key, select, accept)\n        request.host_locations = self._get_host_locations(secondary=True)\n        request.path = _get_entity_path(table_name, partition_key, row_key)\n        request.query['timeout'] = _int_to_str(timeout)\n\n        return self._perform_request(request, _convert_json_response_to_entity,\n                                     [property_resolver, self.require_encryption,\n                                      self.key_encryption_key, self.key_resolver_function])", "response": "Get an entity from the specified table."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef insert_entity(self, table_name, entity, timeout=None):\n        '''\n        Inserts a new entity into the table. Throws if an entity with the same \n        PartitionKey and RowKey already exists.\n\n        When inserting an entity into a table, you must specify values for the \n        PartitionKey and RowKey system properties. Together, these properties \n        form the primary key and must be unique within the table. Both the \n        PartitionKey and RowKey values must be string values; each key value may \n        be up to 64 KB in size. If you are using an integer value for the key \n        value, you should convert the integer to a fixed-width string, because \n        they are canonically sorted. For example, you should convert the value \n        1 to 0000001 to ensure proper sorting.\n\n        :param str table_name:\n            The name of the table to insert the entity into.\n        :param entity:\n            The entity to insert. Could be a dict or an entity object. \n            Must contain a PartitionKey and a RowKey.\n        :type entity: dict or :class:`~azure.storage.table.models.Entity`\n        :param int timeout:\n            The server timeout, expressed in seconds.\n        :return: The etag of the inserted entity.\n        :rtype: str\n        '''\n        _validate_not_none('table_name', table_name)\n\n        request = _insert_entity(entity, self.require_encryption, self.key_encryption_key,\n                                 self.encryption_resolver_function)\n        request.host_locations = self._get_host_locations()\n        request.path = '/' + _to_str(table_name)\n        request.query['timeout'] = _int_to_str(timeout)\n\n        return self._perform_request(request, _extract_etag)", "response": "Inserts a new entity into the table."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update_entity(self, table_name, entity, if_match='*', timeout=None):\n        '''\n        Updates an existing entity in a table. Throws if the entity does not exist. \n        The update_entity operation replaces the entire entity and can be used to \n        remove properties.\n\n        :param str table_name:\n            The name of the table containing the entity to update.\n        :param entity:\n            The entity to update. Could be a dict or an entity object. \n            Must contain a PartitionKey and a RowKey.\n        :type entity: dict or :class:`~azure.storage.table.models.Entity`\n        :param str if_match:\n            The client may specify the ETag for the entity on the \n            request in order to compare to the ETag maintained by the service \n            for the purpose of optimistic concurrency. The update operation \n            will be performed only if the ETag sent by the client matches the \n            value maintained by the server, indicating that the entity has \n            not been modified since it was retrieved by the client. To force \n            an unconditional update, set If-Match to the wildcard character (*).\n        :param int timeout:\n            The server timeout, expressed in seconds.\n        :return: The etag of the entity.\n        :rtype: str\n        '''\n        _validate_not_none('table_name', table_name)\n        request = _update_entity(entity, if_match, self.require_encryption, self.key_encryption_key,\n                                 self.encryption_resolver_function)\n        request.host_locations = self._get_host_locations()\n        request.path = _get_entity_path(table_name, entity['PartitionKey'], entity['RowKey'])\n        request.query['timeout'] = _int_to_str(timeout)\n\n        return self._perform_request(request, _extract_etag)", "response": "Updates an existing entity in a table."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate an existing entity by merging the properties of the entity.", "response": "def merge_entity(self, table_name, entity, if_match='*', timeout=None):\n        '''\n        Updates an existing entity by merging the entity's properties. Throws \n        if the entity does not exist. \n        \n        This operation does not replace the existing entity as the update_entity\n        operation does. A property cannot be removed with merge_entity.\n        \n        Any properties with null values are ignored. All other properties will be \n        updated or added.\n\n        :param str table_name:\n            The name of the table containing the entity to merge.\n        :param entity:\n            The entity to merge. Could be a dict or an entity object. \n            Must contain a PartitionKey and a RowKey.\n        :type entity: dict or :class:`~azure.storage.table.models.Entity`\n        :param str if_match:\n            The client may specify the ETag for the entity on the \n            request in order to compare to the ETag maintained by the service \n            for the purpose of optimistic concurrency. The merge operation \n            will be performed only if the ETag sent by the client matches the \n            value maintained by the server, indicating that the entity has \n            not been modified since it was retrieved by the client. To force \n            an unconditional merge, set If-Match to the wildcard character (*).\n        :param int timeout:\n            The server timeout, expressed in seconds.\n        :return: The etag of the entity.\n        :rtype: str\n        '''\n\n        _validate_not_none('table_name', table_name)\n\n        request = _merge_entity(entity, if_match, self.require_encryption,\n                                self.key_encryption_key)\n        request.host_locations = self._get_host_locations()\n        request.query['timeout'] = _int_to_str(timeout)\n        request.path = _get_entity_path(table_name, entity['PartitionKey'], entity['RowKey'])\n\n        return self._perform_request(request, _extract_etag)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndeleting an existing entity in a table.", "response": "def delete_entity(self, table_name, partition_key, row_key,\n                      if_match='*', timeout=None):\n        '''\n        Deletes an existing entity in a table. Throws if the entity does not exist.\n\n        When an entity is successfully deleted, the entity is immediately marked \n        for deletion and is no longer accessible to clients. The entity is later \n        removed from the Table service during garbage collection.\n\n        :param str table_name:\n            The name of the table containing the entity to delete.\n        :param str partition_key:\n            The PartitionKey of the entity.\n        :param str row_key:\n            The RowKey of the entity.\n        :param str if_match:\n            The client may specify the ETag for the entity on the \n            request in order to compare to the ETag maintained by the service \n            for the purpose of optimistic concurrency. The delete operation \n            will be performed only if the ETag sent by the client matches the \n            value maintained by the server, indicating that the entity has \n            not been modified since it was retrieved by the client. To force \n            an unconditional delete, set If-Match to the wildcard character (*).\n        :param int timeout:\n            The server timeout, expressed in seconds.\n        '''\n        _validate_not_none('table_name', table_name)\n        request = _delete_entity(partition_key, row_key, if_match)\n        request.host_locations = self._get_host_locations()\n        request.query['timeout'] = _int_to_str(timeout)\n        request.path = _get_entity_path(table_name, partition_key, row_key)\n\n        self._perform_request(request)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef insert_or_replace_entity(self, table_name, entity, timeout=None):\n        '''\n        Replaces an existing entity or inserts a new entity if it does not\n        exist in the table. Because this operation can insert or update an\n        entity, it is also known as an \"upsert\" operation.\n\n        If insert_or_replace_entity is used to replace an entity, any properties \n        from the previous entity will be removed if the new entity does not define \n        them.\n\n        :param str table_name:\n            The name of the table in which to insert or replace the entity.\n        :param entity:\n            The entity to insert or replace. Could be a dict or an entity object. \n            Must contain a PartitionKey and a RowKey.\n        :type entity: dict or :class:`~azure.storage.table.models.Entity`\n        :param int timeout:\n            The server timeout, expressed in seconds.\n        :return: The etag of the entity.\n        :rtype: str\n        '''\n        _validate_not_none('table_name', table_name)\n        request = _insert_or_replace_entity(entity, self.require_encryption, self.key_encryption_key,\n                                            self.encryption_resolver_function)\n        request.host_locations = self._get_host_locations()\n        request.query['timeout'] = _int_to_str(timeout)\n        request.path = _get_entity_path(table_name, entity['PartitionKey'], entity['RowKey'])\n\n        return self._perform_request(request, _extract_etag)", "response": "Inserts an existing entity or replaces an existing entity if it does not exist in the table."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninserting an existing entity or merges a new entity.", "response": "def insert_or_merge_entity(self, table_name, entity, timeout=None):\n        '''\n        Merges an existing entity or inserts a new entity if it does not exist\n        in the table. \n\n        If insert_or_merge_entity is used to merge an entity, any properties from \n        the previous entity will be retained if the request does not define or \n        include them.\n\n        :param str table_name:\n            The name of the table in which to insert or merge the entity.\n        :param entity:\n            The entity to insert or merge. Could be a dict or an entity object. \n            Must contain a PartitionKey and a RowKey.\n        :type entity: dict or :class:`~azure.storage.table.models.Entity`\n        :param int timeout:\n            The server timeout, expressed in seconds.\n        :return: The etag of the entity.\n        :rtype: str\n        '''\n\n        _validate_not_none('table_name', table_name)\n        request = _insert_or_merge_entity(entity, self.require_encryption,\n                                          self.key_encryption_key)\n        request.host_locations = self._get_host_locations()\n        request.query['timeout'] = _int_to_str(timeout)\n        request.path = _get_entity_path(table_name, entity['PartitionKey'], entity['RowKey'])\n\n        return self._perform_request(request, _extract_etag)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a class - based entity with fixed values using all of the supported data types.", "response": "def create_entity_class(self):\n        '''\n        Creates a class-based entity with fixed values, using all of the supported data types.\n        '''\n        entity = Entity()\n\n        # Partition key and row key must be strings and are required\n        entity.PartitionKey = 'pk{}'.format(str(uuid.uuid4()).replace('-', ''))\n        entity.RowKey = 'rk{}'.format(str(uuid.uuid4()).replace('-', ''))\n\n        # Some basic types are inferred\n        entity.age = 39  # EdmType.INT64\n        entity.large = 933311100  # EdmType.INT64\n        entity.sex = 'male'  # EdmType.STRING\n        entity.married = True  # EdmType.BOOLEAN\n        entity.ratio = 3.1  # EdmType.DOUBLE\n        entity.birthday = datetime(1970, 10, 4)  # EdmType.DATETIME\n\n        # Binary, Int32 and GUID must be explicitly typed\n        entity.binary = EntityProperty(EdmType.BINARY, b'xyz')\n        entity.other = EntityProperty(EdmType.INT32, 20)\n        entity.clsid = EntityProperty(EdmType.GUID, 'c9da6455-213d-42c9-9a79-3e9149a57833')\n        return entity"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_entity_dict(self):\n        '''\n        Creates a dict-based entity with fixed values, using all of the supported data types.\n        '''\n        entity = {}\n\n        # Partition key and row key must be strings and are required\n        entity['PartitionKey'] = 'pk{}'.format(str(uuid.uuid4()).replace('-', ''))\n        entity['RowKey'] = 'rk{}'.format(str(uuid.uuid4()).replace('-', ''))\n\n        # Some basic types are inferred\n        entity['age'] = 39  # EdmType.INT64\n        entity['large'] = 933311100  # EdmType.INT64\n        entity['sex'] = 'male'  # EdmType.STRING\n        entity['married'] = True  # EdmType.BOOLEAN\n        entity['ratio'] = 3.1  # EdmType.DOUBLE\n        entity['birthday'] = datetime(1970, 10, 4)  # EdmType.DATETIME\n\n        # Binary, Int32 and GUID must be explicitly typed\n        entity['binary'] = EntityProperty(EdmType.BINARY, b'xyz')\n        entity['other'] = EntityProperty(EdmType.INT32, 20)\n        entity['clsid'] = EntityProperty(EdmType.GUID, 'c9da6455-213d-42c9-9a79-3e9149a57833')\n        return entity", "response": "Creates a dict - based entity with fixed values using all of the supported data types."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef insert_entity(self, entity):\n        '''\n        Adds an insert entity operation to the batch. See \n        :func:`~azure.storage.table.tableservice.TableService.insert_entity` for more \n        information on inserts.\n        \n        The operation will not be executed until the batch is committed.\n\n        :param entity:\n            The entity to insert. Could be a dict or an entity object. \n            Must contain a PartitionKey and a RowKey.\n        :type entity: dict or :class:`~azure.storage.table.models.Entity`\n        '''\n        request = _insert_entity(entity, self._require_encryption, self._key_encryption_key,\n                                 self._encryption_resolver)\n        self._add_to_batch(entity['PartitionKey'], entity['RowKey'], request)", "response": "Adds an insert entity operation to the batch."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update_entity(self, entity, if_match='*'):\n        '''\n        Adds an update entity operation to the batch. See \n        :func:`~azure.storage.table.tableservice.TableService.update_entity` for more \n        information on updates.\n        \n        The operation will not be executed until the batch is committed.\n\n        :param entity:\n            The entity to update. Could be a dict or an entity object. \n            Must contain a PartitionKey and a RowKey.\n        :type entity: dict or :class:`~azure.storage.table.models.Entity`\n        :param str if_match:\n            The client may specify the ETag for the entity on the \n            request in order to compare to the ETag maintained by the service \n            for the purpose of optimistic concurrency. The update operation \n            will be performed only if the ETag sent by the client matches the \n            value maintained by the server, indicating that the entity has \n            not been modified since it was retrieved by the client. To force \n            an unconditional update, set If-Match to the wildcard character (*).\n        '''\n        request = _update_entity(entity, if_match, self._require_encryption,\n                                 self._key_encryption_key, self._encryption_resolver)\n        self._add_to_batch(entity['PartitionKey'], entity['RowKey'], request)", "response": "Updates the metadata of a single entity in the table."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a merge entity operation to the batch.", "response": "def merge_entity(self, entity, if_match='*'):\n        '''\n        Adds a merge entity operation to the batch. See \n        :func:`~azure.storage.table.tableservice.TableService.merge_entity` for more \n        information on merges.\n        \n        The operation will not be executed until the batch is committed.\n\n        :param entity:\n            The entity to merge. Could be a dict or an entity object. \n            Must contain a PartitionKey and a RowKey.\n        :type entity: dict or :class:`~azure.storage.table.models.Entity`\n        :param str if_match:\n            The client may specify the ETag for the entity on the \n            request in order to compare to the ETag maintained by the service \n            for the purpose of optimistic concurrency. The merge operation \n            will be performed only if the ETag sent by the client matches the \n            value maintained by the server, indicating that the entity has \n            not been modified since it was retrieved by the client. To force \n            an unconditional merge, set If-Match to the wildcard character (*).\n        '''\n\n        request = _merge_entity(entity, if_match, self._require_encryption,\n                                self._key_encryption_key)\n        self._add_to_batch(entity['PartitionKey'], entity['RowKey'], request)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef delete_entity(self, partition_key, row_key,\n                      if_match='*'):\n        '''\n        Adds a delete entity operation to the batch. See \n        :func:`~azure.storage.table.tableservice.TableService.delete_entity` for more \n        information on deletes.\n\n        The operation will not be executed until the batch is committed.\n\n        :param str partition_key:\n            The PartitionKey of the entity.\n        :param str row_key:\n            The RowKey of the entity.\n        :param str if_match:\n            The client may specify the ETag for the entity on the \n            request in order to compare to the ETag maintained by the service \n            for the purpose of optimistic concurrency. The delete operation \n            will be performed only if the ETag sent by the client matches the \n            value maintained by the server, indicating that the entity has \n            not been modified since it was retrieved by the client. To force \n            an unconditional delete, set If-Match to the wildcard character (*).\n        '''\n        request = _delete_entity(partition_key, row_key, if_match)\n        self._add_to_batch(partition_key, row_key, request)", "response": "Deletes an entity from the table."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninsert or replaces an entity into the batch.", "response": "def insert_or_replace_entity(self, entity):\n        '''\n        Adds an insert or replace entity operation to the batch. See \n        :func:`~azure.storage.table.tableservice.TableService.insert_or_replace_entity` for more \n        information on insert or replace operations.\n\n        The operation will not be executed until the batch is committed.\n\n        :param entity:\n            The entity to insert or replace. Could be a dict or an entity object. \n            Must contain a PartitionKey and a RowKey.\n        :type entity: dict or :class:`~azure.storage.table.models.Entity`\n       '''\n        request = _insert_or_replace_entity(entity, self._require_encryption, self._key_encryption_key,\n                                            self._encryption_resolver)\n        self._add_to_batch(entity['PartitionKey'], entity['RowKey'], request)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef insert_or_merge_entity(self, entity):\n        '''\n        Adds an insert or merge entity operation to the batch. See \n        :func:`~azure.storage.table.tableservice.TableService.insert_or_merge_entity` for more \n        information on insert or merge operations.\n\n        The operation will not be executed until the batch is committed.\n\n        :param entity:\n            The entity to insert or merge. Could be a dict or an entity object. \n            Must contain a PartitionKey and a RowKey.\n        :type entity: dict or :class:`~azure.storage.table.models.Entity`\n        '''\n\n        request = _insert_or_merge_entity(entity, self._require_encryption,\n                                          self._key_encryption_key)\n        self._add_to_batch(entity['PartitionKey'], entity['RowKey'], request)", "response": "Adds an insert or merge entity operation to the Batch."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds the request to the batch.", "response": "def _add_to_batch(self, partition_key, row_key, request):\n        '''\n        Validates batch-specific rules.\n        \n        :param str partition_key:\n            PartitionKey of the entity.\n        :param str row_key:\n            RowKey of the entity.\n        :param request:\n            the request to insert, update or delete entity\n        '''\n        # All same partition keys\n        if self._partition_key:\n            if self._partition_key != partition_key:\n                raise AzureBatchValidationError(_ERROR_INCORRECT_PARTITION_KEY_IN_BATCH)\n        else:\n            self._partition_key = partition_key\n\n        # All different row keys\n        if row_key in self._row_keys:\n            raise AzureBatchValidationError(_ERROR_DUPLICATE_ROW_KEY_IN_BATCH)\n        else:\n            self._row_keys.append(row_key)\n\n        # 100 entities\n        if len(self._requests) >= 100:\n            raise AzureBatchValidationError(_ERROR_TOO_MANY_ENTITIES_IN_BATCH)\n\n        # Add the request to the batch\n        self._requests.append((row_key, request))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _convert_entity_to_json(source):\n    ''' Converts an entity object to json to send.\n    The entity format is:\n    {\n       \"Address\":\"Mountain View\",\n       \"Age\":23,\n       \"AmountDue\":200.23,\n       \"CustomerCode@odata.type\":\"Edm.Guid\",\n       \"CustomerCode\":\"c9da6455-213d-42c9-9a79-3e9149a57833\",\n       \"CustomerSince@odata.type\":\"Edm.DateTime\",\n       \"CustomerSince\":\"2008-07-10T00:00:00\",\n       \"IsActive\":true,\n       \"NumberOfOrders@odata.type\":\"Edm.Int64\",\n       \"NumberOfOrders\":\"255\",\n       \"PartitionKey\":\"mypartitionkey\",\n       \"RowKey\":\"myrowkey\"\n    }\n    '''\n\n    properties = {}\n\n    # set properties type for types we know if value has no type info.\n    # if value has type info, then set the type to value.type\n    for name, value in source.items():\n        mtype = ''\n\n        if isinstance(value, EntityProperty):\n            conv = _EDM_TO_ENTITY_CONVERSIONS.get(value.type)\n            if conv is None:\n                raise TypeError(\n                    _ERROR_TYPE_NOT_SUPPORTED.format(value.type))\n            mtype, value = conv(value.value)\n        else:\n            conv = _PYTHON_TO_ENTITY_CONVERSIONS.get(type(value))\n            if conv is None and sys.version_info >= (3,) and value is None:\n                conv = _to_entity_none\n            if conv is None:\n                raise TypeError(\n                    _ERROR_CANNOT_SERIALIZE_VALUE_TO_ENTITY.format(\n                        type(value).__name__))\n            mtype, value = conv(value)\n\n        # form the property node\n        properties[name] = value\n        if mtype:\n            properties[name + '@odata.type'] = mtype\n\n    # generate the entity_body\n    return dumps(properties)", "response": "Converts an entity object to json to send."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _convert_batch_to_json(batch_requests):\n    '''\n    Create json to send for an array of batch requests.\n\n    batch_requests:\n        an array of requests\n    '''\n    batch_boundary = b'batch_' + _new_boundary()\n    changeset_boundary = b'changeset_' + _new_boundary()\n\n    body = [b'--' + batch_boundary + b'\\n',\n            b'Content-Type: multipart/mixed; boundary=',\n            changeset_boundary + b'\\n\\n']\n\n    content_id = 1\n\n    # Adds each request body to the POST data.\n    for _, request in batch_requests:\n        body.append(b'--' + changeset_boundary + b'\\n')\n        body.append(b'Content-Type: application/http\\n')\n        body.append(b'Content-Transfer-Encoding: binary\\n\\n')\n        body.append(request.method.encode('utf-8'))\n        body.append(b' ')\n        body.append(request.path.encode('utf-8'))\n        body.append(b' HTTP/1.1\\n')\n        body.append(b'Content-ID: ')\n        body.append(str(content_id).encode('utf-8') + b'\\n')\n        content_id += 1\n\n        for name, value in request.headers.items():\n            if name in _SUB_HEADERS:\n                body.append(name.encode('utf-8') + b': ')\n                body.append(value.encode('utf-8') + b'\\n')\n\n        # Add different headers for different request types.\n        if not request.method == 'DELETE':\n            body.append(b'Content-Length: ')\n            body.append(str(len(request.body)).encode('utf-8'))\n            body.append(b'\\n\\n')\n            body.append(request.body + b'\\n')\n\n        body.append(b'\\n')\n\n    body.append(b'--' + changeset_boundary + b'--' + b'\\n')\n    body.append(b'--' + batch_boundary + b'--')\n\n    return b''.join(body), 'multipart/mixed; boundary=' + batch_boundary.decode('utf-8')", "response": "Convert a list of batch requests into a single json."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a table with the specified name and adds entities with the specified number of entities.", "response": "def _create_query_table_encrypted(self, entity_count):\n        '''\n        Creates a table with the specified name and adds entities with the\n        default set of values. PartitionKey is set to 'MyPartition' and RowKey\n        is set to a unique counter value starting at 1 (as a string). The\n        'foo' attribute is set to be encrypted.\n        '''\n        table_name = self._create_table(prefix='querytable')\n        self.service.require_encryption = True\n\n        entity = self._create_entity_for_encryption()\n        with self.service.batch(table_name) as batch:\n            for i in range(1, entity_count + 1):\n                entity['RowKey'] = entity['RowKey'] + str(i)\n                batch.insert_entity(entity)\n        return table_name"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _encrypt_entity(entity, key_encryption_key, encryption_resolver):\n    '''\n    Encrypts the given entity using AES256 in CBC mode with 128 bit padding.\n    Will generate a content-encryption-key (cek) to encrypt the properties either\n    stored in an EntityProperty with the 'encrypt' flag set or those\n    specified by the encryption resolver. This cek is then wrapped using the \n    provided key_encryption_key (kek). Only strings may be encrypted and the\n    result is stored as binary on the service. \n\n    :param entity:\n        The entity to insert. Could be a dict or an entity object.\n    :param object key_encryption_key:\n        The user-provided key-encryption-key. Must implement the following methods:\n        wrap_key(key)--wraps the specified key using an algorithm of the user's choice.\n        get_key_wrap_algorithm()--returns the algorithm used to wrap the specified symmetric key.\n        get_kid()--returns a string key id for this key-encryption-key.\n    :param function(partition_key, row_key, property_name) encryption_resolver:\n        A function that takes in an entities partition key, row key, and property name and returns \n        a boolean that indicates whether that property should be encrypted.\n    :return: An entity with both the appropriate properties encrypted and the \n        encryption data.\n    :rtype: object\n    '''\n\n    _validate_not_none('entity', entity)\n    _validate_not_none('key_encryption_key', key_encryption_key)\n    _validate_key_encryption_key_wrap(key_encryption_key)\n\n    # AES256 uses 256 bit (32 byte) keys and always with 16 byte blocks\n    content_encryption_key = os.urandom(32)\n    entity_initialization_vector = os.urandom(16)\n\n    encrypted_properties = []\n    encrypted_entity = Entity()\n    for key, value in entity.items():\n        # If the property resolver says it should be encrypted\n        # or it is an EntityProperty with the 'encrypt' property set.\n        if (isinstance(value, EntityProperty) and value.encrypt) or \\\n                (encryption_resolver is not None \\\n                         and encryption_resolver(entity['PartitionKey'], entity['RowKey'], key)):\n\n            # Only strings can be encrypted and None is not an instance of str.\n            if isinstance(value, EntityProperty):\n                if value.type == EdmType.STRING:\n                    value = value.value\n                else:\n                    raise ValueError(_ERROR_UNSUPPORTED_TYPE_FOR_ENCRYPTION)\n            if not isinstance(value, str):\n                raise ValueError(_ERROR_UNSUPPORTED_TYPE_FOR_ENCRYPTION)\n\n                # Value is now confirmed to hold a valid string value to be encrypted\n            # and should be added to the list of encrypted properties.\n            encrypted_properties.append(key)\n\n            propertyIV = _generate_property_iv(entity_initialization_vector,\n                                               entity['PartitionKey'], entity['RowKey'],\n                                               key, False)\n\n            # Encode the strings for encryption.\n            value = value.encode('utf-8')\n\n            cipher = _generate_AES_CBC_cipher(content_encryption_key, propertyIV)\n\n            # PKCS7 with 16 byte blocks ensures compatibility with AES.\n            padder = PKCS7(128).padder()\n            padded_data = padder.update(value) + padder.finalize()\n\n            # Encrypt the data.\n            encryptor = cipher.encryptor()\n            encrypted_data = encryptor.update(padded_data) + encryptor.finalize()\n\n            # Set the new value of this key to be a binary EntityProperty for proper serialization.\n            value = EntityProperty(EdmType.BINARY, encrypted_data)\n\n        encrypted_entity[key] = value\n\n    encrypted_properties = dumps(encrypted_properties)\n\n    # Generate the metadata iv.\n    metadataIV = _generate_property_iv(entity_initialization_vector,\n                                       entity['PartitionKey'], entity['RowKey'],\n                                       '_ClientEncryptionMetadata2', False)\n\n    encrypted_properties = encrypted_properties.encode('utf-8')\n\n    cipher = _generate_AES_CBC_cipher(content_encryption_key, metadataIV)\n\n    padder = PKCS7(128).padder()\n    padded_data = padder.update(encrypted_properties) + padder.finalize()\n\n    encryptor = cipher.encryptor()\n    encrypted_data = encryptor.update(padded_data) + encryptor.finalize()\n\n    encrypted_entity['_ClientEncryptionMetadata2'] = EntityProperty(EdmType.BINARY, encrypted_data)\n\n    encryption_data = _generate_encryption_data_dict(key_encryption_key, content_encryption_key,\n                                                     entity_initialization_vector)\n\n    encrypted_entity['_ClientEncryptionMetadata1'] = dumps(encryption_data)\n    return encrypted_entity", "response": "Encrypts the given entity using AES256 with 128 bit padding."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _decrypt_entity(entity, encrypted_properties_list, content_encryption_key, entityIV, isJavaV1):\n    '''\n    Decrypts the specified entity using AES256 in CBC mode with 128 bit padding. Unwraps the CEK \n    using either the specified KEK or the key returned by the key_resolver. Properties \n    specified in the encrypted_properties_list, will be decrypted and decoded to utf-8 strings.\n\n    :param entity:\n        The entity being retrieved and decrypted. Could be a dict or an entity object.\n    :param list encrypted_properties_list:\n        The encrypted list of all the properties that are encrypted.\n    :param bytes[] content_encryption_key:\n        The key used internally to encrypt the entity. Extrated from the entity metadata.\n    :param bytes[] entityIV:\n        The intialization vector used to seed the encryption algorithm. Extracted from the\n        entity metadata.\n    :return: The decrypted entity\n    :rtype: Entity\n    '''\n\n    _validate_not_none('entity', entity)\n\n    decrypted_entity = deepcopy(entity)\n    try:\n        for property in entity.keys():\n            if property in encrypted_properties_list:\n                value = entity[property]\n\n                propertyIV = _generate_property_iv(entityIV,\n                                                   entity['PartitionKey'], entity['RowKey'],\n                                                   property, isJavaV1)\n                cipher = _generate_AES_CBC_cipher(content_encryption_key,\n                                                  propertyIV)\n\n                # Decrypt the property.\n                decryptor = cipher.decryptor()\n                decrypted_data = (decryptor.update(value.value) + decryptor.finalize())\n\n                # Unpad the data.\n                unpadder = PKCS7(128).unpadder()\n                decrypted_data = (unpadder.update(decrypted_data) + unpadder.finalize())\n\n                decrypted_data = decrypted_data.decode('utf-8')\n\n                decrypted_entity[property] = decrypted_data\n\n        decrypted_entity.pop('_ClientEncryptionMetadata1')\n        decrypted_entity.pop('_ClientEncryptionMetadata2')\n        return decrypted_entity\n    except:\n        raise AzureException(_ERROR_DECRYPTION_FAILURE)", "response": "Decrypts the specified entity using AES256 with 128 bit padding. Unwraps the entity using either the specified KEK or the key returned by the key_resolver."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _generate_property_iv(entity_iv, pk, rk, property_name, isJavaV1):\n    '''\n    Uses the entity_iv, partition key, and row key to generate and return\n    the iv for the specified property.\n    '''\n    digest = Hash(SHA256(), default_backend())\n    if not isJavaV1:\n        digest.update(entity_iv +\n                      (rk + pk + property_name).encode('utf-8'))\n    else:\n        digest.update(entity_iv +\n                      (pk + rk + property_name).encode('utf-8'))\n    propertyIV = digest.finalize()\n    return propertyIV[:16]", "response": "Generate the iv for a property."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconstruct an insert entity request.", "response": "def _insert_entity(entity, encryption_required=False,\n                   key_encryption_key=None, encryption_resolver=None):\n    '''\n    Constructs an insert entity request.\n    :param entity:\n        The entity to insert. Could be a dict or an entity object.\n    :param object key_encryption_key:\n        The user-provided key-encryption-key. Must implement the following methods:\n        wrap_key(key)--wraps the specified key using an algorithm of the user's choice.\n        get_key_wrap_algorithm()--returns the algorithm used to wrap the specified symmetric key.\n        get_kid()--returns a string key id for this key-encryption-key.\n    :param function(partition_key, row_key, property_name) encryption_resolver:\n        A function that takes in an entities partition key, row key, and property name and returns\n        a boolean that indicates whether that property should be encrypted.\n    '''\n    _validate_entity(entity, key_encryption_key is not None)\n    _validate_encryption_required(encryption_required, key_encryption_key)\n\n    request = HTTPRequest()\n    request.method = 'POST'\n    request.headers = {\n        _DEFAULT_CONTENT_TYPE_HEADER[0]: _DEFAULT_CONTENT_TYPE_HEADER[1],\n        _DEFAULT_ACCEPT_HEADER[0]: _DEFAULT_ACCEPT_HEADER[1],\n        _DEFAULT_PREFER_HEADER[0]: _DEFAULT_PREFER_HEADER[1]\n    }\n    if key_encryption_key:\n        entity = _encrypt_entity(entity, key_encryption_key, encryption_resolver)\n    request.body = _get_request_body(_convert_entity_to_json(entity))\n\n    return request"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconstructing an update entity request.", "response": "def _update_entity(entity, if_match, encryption_required=False,\n                   key_encryption_key=None, encryption_resolver=None):\n    '''\n    Constructs an update entity request.\n    :param entity:\n        The entity to insert. Could be a dict or an entity object.\n    :param object key_encryption_key:\n        The user-provided key-encryption-key. Must implement the following methods:\n        wrap_key(key)--wraps the specified key using an algorithm of the user's choice.\n        get_key_wrap_algorithm()--returns the algorithm used to wrap the specified symmetric key.\n        get_kid()--returns a string key id for this key-encryption-key.\n    :param function(partition_key, row_key, property_name) encryption_resolver:\n        A function that takes in an entities partition key, row key, and property name and returns\n        a boolean that indicates whether that property should be encrypted.\n    '''\n    _validate_not_none('if_match', if_match)\n    _validate_entity(entity, key_encryption_key is not None)\n    _validate_encryption_required(encryption_required, key_encryption_key)\n\n    request = HTTPRequest()\n    request.method = 'PUT'\n    request.headers = {\n        _DEFAULT_CONTENT_TYPE_HEADER[0]: _DEFAULT_CONTENT_TYPE_HEADER[1],\n        _DEFAULT_ACCEPT_HEADER[0]: _DEFAULT_ACCEPT_HEADER[1],\n        'If-Match': _to_str(if_match),\n    }\n    if key_encryption_key:\n        entity = _encrypt_entity(entity, key_encryption_key, encryption_resolver)\n    request.body = _get_request_body(_convert_entity_to_json(entity))\n\n    return request"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _merge_entity(entity, if_match, require_encryption=False, key_encryption_key=None):\n    '''\n    Constructs a merge entity request.\n    '''\n    _validate_not_none('if_match', if_match)\n    _validate_entity(entity)\n    _validate_encryption_unsupported(require_encryption, key_encryption_key)\n\n    request = HTTPRequest()\n    request.method = 'MERGE'\n    request.headers = {\n        _DEFAULT_CONTENT_TYPE_HEADER[0]: _DEFAULT_CONTENT_TYPE_HEADER[1],\n        _DEFAULT_ACCEPT_HEADER[0]: _DEFAULT_ACCEPT_HEADER[1],\n        'If-Match': _to_str(if_match)\n    }\n    request.body = _get_request_body(_convert_entity_to_json(entity))\n\n    return request", "response": "Constructs a merge entity request."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconstructing an insert or merge entity request.", "response": "def _insert_or_merge_entity(entity, require_encryption=False, key_encryption_key=None):\n    '''\n    Constructs an insert or merge entity request.\n    :param entity:\n        The entity to insert. Could be a dict or an entity object.\n    :param object key_encryption_key:\n        The user-provided key-encryption-key. Must implement the following methods:\n        wrap_key(key)--wraps the specified key using an algorithm of the user's choice.\n        get_key_wrap_algorithm()--returns the algorithm used to wrap the specified symmetric key.\n        get_kid()--returns a string key id for this key-encryption-key.\n    '''\n    _validate_entity(entity)\n    _validate_encryption_unsupported(require_encryption, key_encryption_key)\n\n    request = HTTPRequest()\n    request.method = 'MERGE'\n    request.headers = {\n        _DEFAULT_CONTENT_TYPE_HEADER[0]: _DEFAULT_CONTENT_TYPE_HEADER[1],\n        _DEFAULT_ACCEPT_HEADER[0]: _DEFAULT_ACCEPT_HEADER[1],\n    }\n    request.body = _get_request_body(_convert_entity_to_json(entity))\n\n    return request"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert a JSON response to an entity.", "response": "def _convert_json_response_to_entity(response, property_resolver, require_encryption,\n                                     key_encryption_key, key_resolver):\n    '''\n    :param bool require_encryption:\n        If set, will enforce that the retrieved entity is encrypted and decrypt it.\n    :param object key_encryption_key:\n        The user-provided key-encryption-key. Must implement the following methods:\n        unwrap_key(key, algorithm)--returns the unwrapped form of the specified symmetric key using the \n        string-specified algorithm.\n        get_kid()--returns a string key id for this key-encryption-key.\n     :param function key_resolver(kid):\n        The user-provided key resolver. Uses the kid string to return a key-encryption-key implementing\n        the interface defined above.\n    '''\n    if response is None or response.body is None:\n        return None\n\n    root = loads(response.body.decode('utf-8'))\n    return _decrypt_and_deserialize_entity(root, property_resolver, require_encryption,\n                                           key_encryption_key, key_resolver)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_default_mapping(self, obj):\n        mapping = {v: k for k, v in obj.TYPE_MAPPING.items()}\n        mapping.update({\n            fields.Email: text_type,\n            fields.Dict: dict,\n            fields.Url: text_type,\n            fields.List: list,\n            fields.LocalDateTime: datetime.datetime,\n            fields.Nested: '_from_nested_schema',\n        })\n        return mapping", "response": "Return default mapping if there are no special needs."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfilling out properties field.", "response": "def get_properties(self, obj):\n        \"\"\"Fill out properties field.\"\"\"\n        properties = {}\n\n        for field_name, field in sorted(obj.fields.items()):\n            schema = self._get_schema_for_field(obj, field)\n            properties[field.name] = schema\n\n        return properties"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_required(self, obj):\n        required = []\n\n        for field_name, field in sorted(obj.fields.items()):\n            if field.required:\n                required.append(field.name)\n\n        return required or missing", "response": "Fill out required field."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting schema definition from python type.", "response": "def _from_python_type(self, obj, field, pytype):\n        \"\"\"Get schema definition from python type.\"\"\"\n        json_schema = {\n            'title': field.attribute or field.name,\n        }\n\n        for key, val in TYPE_MAP[pytype].items():\n            json_schema[key] = val\n\n        if field.dump_only:\n            json_schema['readonly'] = True\n\n        if field.default is not missing:\n            json_schema['default'] = field.default\n\n        # NOTE: doubled up to maintain backwards compatibility\n        metadata = field.metadata.get('metadata', {})\n        metadata.update(field.metadata)\n\n        for md_key, md_val in metadata.items():\n            if md_key == 'metadata':\n                continue\n            json_schema[md_key] = md_val\n\n        if isinstance(field, fields.List):\n            json_schema['items'] = self._get_schema_for_field(\n                obj, field.container\n            )\n        return json_schema"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting schema and validators for a field.", "response": "def _get_schema_for_field(self, obj, field):\n        \"\"\"Get schema and validators for field.\"\"\"\n        mapping = self._get_default_mapping(obj)\n        if hasattr(field, '_jsonschema_type_mapping'):\n            schema = field._jsonschema_type_mapping()\n        elif '_jsonschema_type_mapping' in field.metadata:\n            schema = field.metadata['_jsonschema_type_mapping']\n        elif field.__class__ in mapping:\n            pytype = mapping[field.__class__]\n            if isinstance(pytype, basestring):\n                schema = getattr(self, pytype)(obj, field)\n            else:\n                schema = self._from_python_type(\n                    obj, field, pytype\n                )\n        else:\n            raise ValueError('unsupported field type %s' % field)\n\n        # Apply any and all validators that field may have\n        for validator in field.validators:\n            if validator.__class__ in FIELD_VALIDATORS:\n                schema = FIELD_VALIDATORS[validator.__class__](\n                    schema, field, validator, obj\n                )\n        return schema"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _from_nested_schema(self, obj, field):\n        if isinstance(field.nested, basestring):\n            nested = get_class(field.nested)\n        else:\n            nested = field.nested\n\n        name = nested.__name__\n        outer_name = obj.__class__.__name__\n        only = field.only\n        exclude = field.exclude\n\n        # If this is not a schema we've seen, and it's not this schema,\n        # put it in our list of schema defs\n        if name not in self._nested_schema_classes and name != outer_name:\n            wrapped_nested = self.__class__(nested=True)\n            wrapped_dumped = wrapped_nested.dump(\n                nested(only=only, exclude=exclude)\n            )\n\n            # Handle change in return value type between Marshmallow\n            # versions 2 and 3.\n            if marshmallow.__version__.split('.', 1)[0] >= '3':\n                self._nested_schema_classes[name] = wrapped_dumped\n            else:\n                self._nested_schema_classes[name] = wrapped_dumped.data\n            self._nested_schema_classes.update(\n                wrapped_nested._nested_schema_classes\n            )\n\n        # and the schema is just a reference to the def\n        schema = {\n            'type': 'object',\n            '$ref': '#/definitions/{}'.format(name)\n        }\n\n        # NOTE: doubled up to maintain backwards compatibility\n        metadata = field.metadata.get('metadata', {})\n        metadata.update(field.metadata)\n\n        for md_key, md_val in metadata.items():\n            if md_key == 'metadata':\n                continue\n            schema[md_key] = md_val\n\n        if field.many:\n            schema = {\n                'type': [\"array\"] if field.required else ['array', 'null'],\n                'items': schema,\n            }\n\n        return schema", "response": "Convert nested field to a schema."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ntakes obj for later use.", "response": "def dump(self, obj, **kwargs):\n        \"\"\"Take obj for later use: using class name to namespace definition.\"\"\"\n        self.obj = obj\n        return super(JSONSchema, self).dump(obj, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef wrap(self, data):\n        if self.nested:  # no need to wrap, will be in outer defs\n            return data\n\n        name = self.obj.__class__.__name__\n        self._nested_schema_classes[name] = data\n        root = {\n            'definitions': self._nested_schema_classes,\n            '$ref': '#/definitions/{name}'.format(name=name)\n        }\n        return root", "response": "Wrap this with the root schema definitions."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef handle_length(schema, field, validator, parent_schema):\n    if isinstance(field, fields.String):\n        minKey = 'minLength'\n        maxKey = 'maxLength'\n    elif isinstance(field, (fields.List, fields.Nested)):\n        minKey = 'minItems'\n        maxKey = 'maxItems'\n    else:\n        raise ValueError(\"In order to set the Length validator for JSON \"\n                         \"schema, the field must be either a List or a String\")\n\n    if validator.min:\n        schema[minKey] = validator.min\n\n    if validator.max:\n        schema[maxKey] = validator.max\n\n    if validator.equal:\n        schema[minKey] = validator.equal\n        schema[maxKey] = validator.equal\n\n    return schema", "response": "Adds validation logic for marshmallow. validate. Length to the given schema."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef handle_one_of(schema, field, validator, parent_schema):\n    if validator.choices:\n        schema['enum'] = list(validator.choices)\n        schema['enumNames'] = list(validator.labels)\n\n    return schema", "response": "Adds the validation logic for OneOf by setting the enum property to the allowed choices in the validator."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef handle_range(schema, field, validator, parent_schema):\n    if not isinstance(field, fields.Number):\n        return schema\n\n    if validator.min:\n        schema['minimum'] = validator.min\n        schema['exclusiveMinimum'] = True\n    else:\n        schema['minimum'] = 0\n        schema['exclusiveMinimum'] = False\n\n    if validator.max:\n        schema['maximum'] = validator.max\n        schema['exclusiveMaximum'] = True\n\n    return schema", "response": "Adds validation logic for marshmallow. validate. Range to the schema."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking that the selected eigensolver is valid and returns the name of the solver and keyword arguments.", "response": "def check_eigen_solver(eigen_solver, solver_kwds, size=None, nvec=None):\n    \"\"\"Check that the selected eigensolver is valid\n\n    Parameters\n    ----------\n    eigen_solver : string\n        string value to validate\n    size, nvec : int (optional)\n        if both provided, use the specified problem size and number of vectors\n        to determine the optimal method to use with eigen_solver='auto'\n\n    Returns\n    -------\n    eigen_solver : string\n        The eigen solver. This only differs from the input if\n        eigen_solver == 'auto' and `size` is specified.\n    \"\"\"\n    if eigen_solver in BAD_EIGEN_SOLVERS:\n        raise ValueError(BAD_EIGEN_SOLVERS[eigen_solver])\n    elif eigen_solver not in EIGEN_SOLVERS:\n        raise ValueError(\"Unrecognized eigen_solver: '{0}'.\"\n                         \"Should be one of: {1}\".format(eigen_solver,\n                                                        EIGEN_SOLVERS))\n\n    if size is not None and nvec is not None:\n        # do some checks of the eigensolver\n        if eigen_solver == 'lobpcg' and size < 5 * nvec + 1:\n            warnings.warn(\"lobpcg does not perform well with small matrices or \"\n                          \"with large numbers of vectors. Switching to 'dense'\")\n            eigen_solver = 'dense'\n            solver_kwds = None\n\n        elif eigen_solver == 'auto':\n            if size > 200 and nvec < 10:\n                if PYAMG_LOADED:\n                    eigen_solver = 'amg'\n                    solver_kwds = None\n                else:\n                    eigen_solver = 'arpack'\n                    solver_kwds = None\n            else:\n                eigen_solver = 'dense'\n                solver_kwds = None\n\n    return eigen_solver, solver_kwds"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfunctioning to compute the eigendecomposition of a square matrix.", "response": "def eigen_decomposition(G, n_components=8, eigen_solver='auto',\n                        random_state=None,\n                        drop_first=True, largest=True, solver_kwds=None):\n    \"\"\"\n    Function to compute the eigendecomposition of a square matrix.\n\n    Parameters\n    ----------\n    G : array_like or sparse matrix\n        The square matrix for which to compute the eigen-decomposition.\n    n_components : integer, optional\n        The number of eigenvectors to return\n    eigen_solver : {'auto', 'dense', 'arpack', 'lobpcg', or 'amg'}\n        'auto' :\n            attempt to choose the best method for input data (default)\n        'dense' :\n            use standard dense matrix operations for the eigenvalue decomposition.\n            For this method, M must be an array or matrix type.\n            This method should be avoided for large problems.\n        'arpack' :\n            use arnoldi iteration in shift-invert mode. For this method,\n            M may be a dense matrix, sparse matrix, or general linear operator.\n            Warning: ARPACK can be unstable for some problems.  It is best to\n            try several random seeds in order to check results.\n        'lobpcg' :\n            Locally Optimal Block Preconditioned Conjugate Gradient Method.\n            A preconditioned eigensolver for large symmetric positive definite\n            (SPD) generalized eigenproblems.\n        'amg' :\n            Algebraic Multigrid solver (requires ``pyamg`` to be installed)\n            It can be faster on very large, sparse problems, but may also lead\n            to instabilities.\n    random_state : int seed, RandomState instance, or None (default)\n        A pseudo random number generator used for the initialization of the\n        lobpcg eigen vectors decomposition when eigen_solver == 'amg'.\n        By default, arpack is used.\n    solver_kwds : any additional keyword arguments to pass to the selected eigen_solver\n\n    Returns\n    -------\n    lambdas, diffusion_map : eigenvalues, eigenvectors\n    \"\"\"\n    n_nodes = G.shape[0]\n    if drop_first:\n        n_components = n_components + 1\n\n    eigen_solver, solver_kwds = check_eigen_solver(eigen_solver, solver_kwds,\n                                                   size=n_nodes,\n                                                   nvec=n_components)\n    random_state = check_random_state(random_state)\n\n    # Convert G to best type for eigendecomposition\n    if sparse.issparse(G):\n        if G.getformat() is not 'csr':\n            G.tocsr()\n    G = G.astype(np.float)\n\n    # Check for symmetry\n    is_symmetric = _is_symmetric(G)\n\n    # Try Eigen Methods:\n    if eigen_solver == 'arpack':\n        # This matches the internal initial state used by ARPACK\n        v0 = random_state.uniform(-1, 1, G.shape[0])\n        if is_symmetric:\n            if largest:\n                which = 'LM'\n            else:\n                which = 'SM'\n            lambdas, diffusion_map = eigsh(G, k=n_components, which=which,\n                                           v0=v0,**(solver_kwds or {}))\n        else:\n            if largest:\n                which = 'LR'\n            else:\n                which = 'SR'\n            lambdas, diffusion_map = eigs(G, k=n_components, which=which,\n                                          **(solver_kwds or {}))\n        lambdas = np.real(lambdas)\n        diffusion_map = np.real(diffusion_map)\n    elif eigen_solver == 'amg':\n        # separate amg & lobpcg keywords:\n        if solver_kwds is not None:\n            amg_kwds = {}\n            lobpcg_kwds = solver_kwds.copy()\n            for kwd in AMG_KWDS:\n                if kwd in solver_kwds.keys():\n                    amg_kwds[kwd] = solver_kwds[kwd]\n                    del lobpcg_kwds[kwd]\n        else:\n            amg_kwds = None\n            lobpcg_kwds = None\n        if not is_symmetric:\n            raise ValueError(\"lobpcg requires symmetric matrices.\")\n        if not sparse.issparse(G):\n            warnings.warn(\"AMG works better for sparse matrices\")\n        # Use AMG to get a preconditioner and speed up the eigenvalue problem.\n        ml = smoothed_aggregation_solver(check_array(G, accept_sparse = ['csr']),**(amg_kwds or {}))\n        M = ml.aspreconditioner()\n        n_find = min(n_nodes, 5 + 2*n_components)\n        X = random_state.rand(n_nodes, n_find)\n        X[:, 0] = (G.diagonal()).ravel()\n        lambdas, diffusion_map = lobpcg(G, X, M=M, largest=largest,**(lobpcg_kwds or {}))\n        sort_order = np.argsort(lambdas)\n        if largest:\n            lambdas = lambdas[sort_order[::-1]]\n            diffusion_map = diffusion_map[:, sort_order[::-1]]\n        else:\n            lambdas = lambdas[sort_order]\n            diffusion_map = diffusion_map[:, sort_order]\n        lambdas = lambdas[:n_components]\n        diffusion_map = diffusion_map[:, :n_components]\n    elif eigen_solver == \"lobpcg\":\n        if not is_symmetric:\n            raise ValueError(\"lobpcg requires symmetric matrices.\")\n        n_find = min(n_nodes, 5 + 2*n_components)\n        X = random_state.rand(n_nodes, n_find)\n        lambdas, diffusion_map = lobpcg(G, X, largest=largest,**(solver_kwds or {}))\n        sort_order = np.argsort(lambdas)\n        if largest:\n            lambdas = lambdas[sort_order[::-1]]\n            diffusion_map = diffusion_map[:, sort_order[::-1]]\n        else:\n            lambdas = lambdas[sort_order]\n            diffusion_map = diffusion_map[:, sort_order]\n        lambdas = lambdas[:n_components]\n        diffusion_map = diffusion_map[:, :n_components]\n    elif eigen_solver == 'dense':\n        if sparse.isspmatrix(G):\n            G = G.todense()\n        if is_symmetric:\n            lambdas, diffusion_map = eigh(G,**(solver_kwds or {}))\n        else:\n            lambdas, diffusion_map = eig(G,**(solver_kwds or {}))\n            sort_index = np.argsort(lambdas)\n            lambdas = lambdas[sort_index]\n            diffusion_map[:,sort_index]\n        if largest:# eigh always returns eigenvalues in ascending order\n            lambdas = lambdas[::-1] # reverse order the e-values\n            diffusion_map = diffusion_map[:, ::-1] # reverse order the vectors\n        lambdas = lambdas[:n_components]\n        diffusion_map = diffusion_map[:, :n_components]\n    return (lambdas, diffusion_map)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef null_space(M, k, k_skip=1, eigen_solver='arpack',\n               random_state=None, solver_kwds=None):\n    \"\"\"\n    Find the null space of a matrix M: eigenvectors associated with 0 eigenvalues\n\n    Parameters\n    ----------\n    M : {array, matrix, sparse matrix, LinearOperator}\n        Input covariance matrix: should be symmetric positive semi-definite\n    k : integer\n        Number of eigenvalues/vectors to return\n    k_skip : integer, optional\n        Number of low eigenvalues to skip.\n    eigen_solver : {'auto', 'dense', 'arpack', 'lobpcg', or 'amg'}\n        'auto' :\n            algorithm will attempt to choose the best method for input data\n        'dense' :\n            use standard dense matrix operations for the eigenvalue decomposition.\n            For this method, M must be an array or matrix type.  This method should be avoided for large problems.\n        'arpack' :\n            use arnoldi iteration in shift-invert mode. For this method,\n            M may be a dense matrix, sparse matrix, or general linear operator.\n            Warning: ARPACK can be unstable for some problems.  It is best to\n            try several random seeds in order to check results.\n        'lobpcg' :\n            Locally Optimal Block Preconditioned Conjugate Gradient Method.\n            A preconditioned eigensolver for large symmetric positive definite\n            (SPD) generalized eigenproblems.\n        'amg' :\n            AMG requires pyamg to be installed. It can be faster on very large,\n            sparse problems, but may also lead to instabilities.\n    random_state: numpy.RandomState or int, optional\n        The generator or seed used to determine the starting vector for arpack\n        iterations.  Defaults to numpy.random.\n    solver_kwds : any additional keyword arguments to pass to the selected eigen_solver\n\n    Returns\n    -------\n    null_space : estimated k vectors of the null space\n    error : estimated error (sum of eigenvalues)\n\n    Notes\n    -----\n    dense solver key words: see\n        http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.linalg.eigh.html\n        for symmetric problems and\n        http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.linalg.eig.html#scipy.linalg.eig\n        for non symmetric problems.\n    arpack sovler key words: see\n        http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.linalg.eigsh.html\n        for symmetric problems and http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.linalg.eigs.html#scipy.sparse.linalg.eigs\n        for non symmetric problems.\n    lobpcg solver keywords: see\n        http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.lobpcg.html\n    amg solver keywords: see\n        http://pyamg.googlecode.com/svn/branches/1.0.x/Docs/html/pyamg.aggregation.html#module-pyamg.aggregation.aggregation\n        (Note amg solver uses lobpcg and also accepts lobpcg keywords)\n    \"\"\"\n    eigen_solver, solver_kwds = check_eigen_solver(eigen_solver, solver_kwds,\n                                                   size=M.shape[0],\n                                                   nvec=k + k_skip)\n    random_state = check_random_state(random_state)\n\n    if eigen_solver == 'arpack':\n        # This matches the internal initial state used by ARPACK\n        v0 = random_state.uniform(-1, 1, M.shape[0])\n        try:\n            eigen_values, eigen_vectors = eigsh(M, k + k_skip, sigma=0.0,\n                                                v0=v0,**(solver_kwds or {}))\n        except RuntimeError as msg:\n            raise ValueError(\"Error in determining null-space with ARPACK. \"\n                             \"Error message: '%s'. \"\n                             \"Note that method='arpack' can fail when the \"\n                             \"weight matrix is singular or otherwise \"\n                             \"ill-behaved.  method='dense' is recommended. \"\n                             \"See online documentation for more information.\"\n                             % msg)\n\n        return eigen_vectors[:, k_skip:], np.sum(eigen_values[k_skip:])\n    elif eigen_solver == 'dense':\n        if hasattr(M, 'toarray'):\n            M = M.toarray()\n        eigen_values, eigen_vectors = eigh(M, eigvals=(0, k+k_skip),overwrite_a=True,\n                                           **(solver_kwds or {}))\n        index = np.argsort(np.abs(eigen_values))\n        eigen_vectors = eigen_vectors[:, index]\n        eigen_values = eigen_values[index]\n        return eigen_vectors[:, k_skip:k+1], np.sum(eigen_values[k_skip:k+1])\n        # eigen_values, eigen_vectors = eigh(\n            # M, eigvals=(k_skip, k + k_skip - 1), overwrite_a=True)\n        # index = np.argsort(np.abs(eigen_values))\n        # return eigen_vectors[:, index], np.sum(eigen_values)\n    elif (eigen_solver == 'amg' or eigen_solver == 'lobpcg'):\n        # M should be positive semi-definite. Add 1 to make it pos. def.\n        try:\n            M = sparse.identity(M.shape[0]) + M\n            n_components = min(k + k_skip + 10, M.shape[0])\n            eigen_values, eigen_vectors = eigen_decomposition(M, n_components,\n                                                              eigen_solver = eigen_solver,\n                                                              drop_first = False,\n                                                              largest = False,\n                                                              random_state=random_state,\n                                                              solver_kwds=solver_kwds)\n            eigen_values = eigen_values -1\n            index = np.argsort(np.abs(eigen_values))\n            eigen_values = eigen_values[index]\n            eigen_vectors = eigen_vectors[:, index]\n            return eigen_vectors[:, k_skip:k+1], np.sum(eigen_values[k_skip:k+1])\n        except np.linalg.LinAlgError: # try again with bigger increase\n            warnings.warn(\"LOBPCG failed the first time. Increasing Pos Def adjustment.\")\n            M = 2.0*sparse.identity(M.shape[0]) + M\n            n_components = min(k + k_skip + 10, M.shape[0])\n            eigen_values, eigen_vectors = eigen_decomposition(M, n_components,\n                                                              eigen_solver = eigen_solver,\n                                                              drop_first = False,\n                                                              largest = False,\n                                                              random_state=random_state,\n                                                              solver_kwds=solver_kwds)\n            eigen_values = eigen_values - 2\n            index = np.argsort(np.abs(eigen_values))\n            eigen_values = eigen_values[index]\n            eigen_vectors = eigen_vectors[:, index]\n            return eigen_vectors[:, k_skip:k+1], np.sum(eigen_values[k_skip:k+1])\n    else:\n        raise ValueError(\"Unrecognized eigen_solver '%s'\" % eigen_solver)", "response": "This function finds the null space of a matrix M."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef import_object(modname, name):\n    try:\n        __import__(modname)\n        mod = sys.modules[modname]\n        obj = mod\n        for part in name.split('.'):\n            obj = getattr(obj, part)\n        return obj\n    except:\n        return None", "response": "Imports the object given by modname and returns it."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute Lk neighbors and subset to index map for projected == False", "response": "def precompute_optimzation_Y(laplacian_matrix, n_samples, relaxation_kwds):\n    \"\"\"compute Lk, neighbors and subset to index map for projected == False\"\"\"\n    relaxation_kwds.setdefault('presave',False)\n    relaxation_kwds.setdefault('presave_name','pre_comp_current.npy')\n    relaxation_kwds.setdefault('verbose',False)\n    if relaxation_kwds['verbose']:\n        print ('Making Lk and nbhds')\n    Lk_tensor, nbk, si_map = \\\n        compute_Lk(laplacian_matrix, n_samples, relaxation_kwds['subset'])\n    if relaxation_kwds['presave']:\n        raise NotImplementedError('Not yet implemented presave')\n    return { 'Lk': Lk_tensor, 'nbk': nbk, 'si_map': si_map }"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing sparse L matrix neighbors and subset to L matrix index map.", "response": "def compute_Lk(laplacian_matrix,n_samples,subset):\n    \"\"\"\n    Compute sparse L matrix, neighbors and subset to L matrix index map.\n\n    Returns\n    -------\n    Lk_tensor : array-like. Length = n\n        each component correspond to the sparse matrix of Lk, which is\n        generated by extracting the kth row of laplacian and removing zeros.\n    nbk : array-like. Length = n\n        each component correspond to the neighbor index of point k, which is\n        used in slicing the gradient, Y or S arrays.\n    si_map : dictionary.\n        subset index to Lk_tensor (or nbk) index mapping.\n    \"\"\"\n    Lk_tensor = []\n    nbk = []\n    row,column = laplacian_matrix.T.nonzero()\n    nnz_val = np.squeeze(np.asarray(laplacian_matrix.T[(row,column)]))\n    sorted_col_args = np.argsort(column)\n    sorted_col_vals = column[sorted_col_args]\n\n    breaks_row = np.diff(row).nonzero()[0]\n    breaks_col = np.diff(sorted_col_vals).nonzero()[0]\n\n    si_map = {}\n\n    for idx,k in enumerate(subset):\n        if k == 0:\n            nbk.append( column[:breaks_row[k]+1].T )\n            lk = nnz_val[np.sort(sorted_col_args[:breaks_col[k]+1])]\n        elif k == n_samples-1:\n            nbk.append( column[breaks_row[k-1]+1:].T )\n            lk = nnz_val[np.sort(sorted_col_args[breaks_col[k-1]+1:])]\n        else:\n            nbk.append( column[breaks_row[k-1]+1:breaks_row[k]+1].T )\n            lk = nnz_val[np.sort(\n                sorted_col_args[breaks_col[k-1]+1:breaks_col[k]+1])]\n\n        npair = nbk[idx].shape[0]\n        rk = (nbk[idx] == k).nonzero()[0]\n        Lk = sp.sparse.lil_matrix((npair,npair))\n        Lk.setdiag(lk)\n        Lk[:,rk] = -(lk.reshape(-1,1))\n        Lk[rk,:] = -(lk.reshape(1,-1))\n        Lk_tensor.append(sp.sparse.csr_matrix(Lk))\n        si_map[k] = idx\n\n    assert len(Lk_tensor) == subset.shape[0], \\\n        'Size of Lk_tensor should be the same as subset.'\n    return Lk_tensor, nbk, si_map"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef precompute_optimzation_S(laplacian_matrix,n_samples,relaxation_kwds):\n    relaxation_kwds.setdefault('presave',False)\n    relaxation_kwds.setdefault('presave_name','pre_comp_current.npy')\n    relaxation_kwds.setdefault('verbose',False)\n    if relaxation_kwds['verbose']:\n        print ('Pre-computing quantities Y to S conversions')\n        print ('Making A and Pairs')\n    A, pairs = makeA(laplacian_matrix)\n    if relaxation_kwds['verbose']:\n        print ('Making Rk and nbhds')\n    Rk_tensor, nbk = compute_Rk(laplacian_matrix,A,n_samples)\n    # TODO: not quite sure what is ATAinv? why we need this?\n    ATAinv = np.linalg.pinv(A.T.dot(A).todense())\n    if relaxation_kwds['verbose']:\n        print ('Finish calculating pseudo inverse')\n    if relaxation_kwds['presave']:\n        raise NotImplementedError('Not yet implemented presave')\n    return { 'RK': Rk_tensor, 'nbk': nbk,\n             'ATAinv': ATAinv, 'pairs': pairs, 'A': A }", "response": "Compute the pseudo inverse of the S problem for projected mode"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef makeA(laplacian_matrix):\n    n = laplacian_matrix.shape[0]\n    row,col = sp.sparse.triu(laplacian_matrix,k=1).nonzero()\n    pairs = np.asarray((row,col)).T\n    N = row.shape[0]\n    A = sp.sparse.csr_matrix((np.ones(N), (np.arange(N),row)), shape=(N,n)) + \\\n        sp.sparse.csr_matrix((-1*np.ones(N), (np.arange(N),col)), shape=(N,n))\n    return A, pairs", "response": "This function creates the all neighbor permutations."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the kth row of the sparse matrix and neighbors.", "response": "def compute_Rk(L,A,n_samples):\n    # TODO: need to inspect more into compute Rk.\n    \"\"\"\n    Compute sparse L matrix and neighbors.\n\n    Returns\n    -------\n    Rk_tensor : array-like. Length = n\n        each component correspond to the sparse matrix of Lk, which is\n        generated by extracting the kth row of laplacian and removing zeros.\n    nbk : array-like. Length = n\n        each component correspond to the neighbor index of point k, which is\n        used in slicing the gradient, Y or S arrays.\n    \"\"\"\n    laplacian_matrix = L.copy()\n    laplacian_matrix.setdiag(0)\n    laplacian_matrix.eliminate_zeros()\n\n    n = n_samples\n    Rk_tensor = []\n    nbk = []\n    row_A,column_A = A.T.nonzero()\n\n    row,column = laplacian_matrix.nonzero()\n    nnz_val = np.squeeze(np.asarray(laplacian_matrix.T[(row,column)]))\n    sorted_col_args = np.argsort(column)\n    sorted_col_vals = column[sorted_col_args]\n\n    breaks_row_A = np.diff(row_A).nonzero()[0]\n    breaks_col = np.diff(sorted_col_vals).nonzero()[0]\n\n    for k in range(n_samples):\n        if k == 0:\n            nbk.append( column_A[:breaks_row_A[k]+1].T )\n            Rk_tensor.append(\n                nnz_val[np.sort(sorted_col_args[:breaks_col[k]+1])])\n        elif k == n_samples-1:\n            nbk.append( column_A[breaks_row_A[k-1]+1:].T )\n            Rk_tensor.append(\n                nnz_val[np.sort(sorted_col_args[breaks_col[k-1]+1:])])\n        else:\n            nbk.append( column_A[breaks_row_A[k-1]+1:breaks_row_A[k]+1].T )\n            Rk_tensor.append(nnz_val[np.sort(\n                sorted_col_args[breaks_col[k-1]+1:breaks_col[k]+1])])\n\n    return Rk_tensor, nbk"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef automodapi_replace(sourcestr, app, dotoctree=True, docname=None,\n                       warnings=True):\n    \"\"\"\n    Replaces `sourcestr`'s entries of \".. automdapi::\" with the\n    automodapi template form based on provided options.\n\n    This is used with the sphinx event 'source-read' to replace\n    `automodapi`_ entries before sphinx actually processes them, as\n    automodsumm needs the code to be present to generate stub\n    documentation.\n\n    Parameters\n    ----------\n    sourcestr : str\n        The string with sphinx source to be checked for automodapi\n        replacement.\n    app : `sphinx.application.Application`\n        The sphinx application.\n    dotoctree : bool\n        If `True`, a \":toctree:\" option will be added in the \"..\n        automodsumm::\" sections of the template, pointing to the\n        appropriate \"generated\" directory based on the Astropy convention\n        (e.g. in ``docs/api``)\n    docname : str\n        The name of the file for this `sourcestr` (if known - if not, it\n        can be `None`). If not provided and `dotoctree` is `True`, the\n        generated files may end up in the wrong place.\n    warnings : bool\n        If `False`, all warnings that would normally be issued are\n        silenced.\n\n    Returns\n    -------\n    newstr :str\n        The string with automodapi entries replaced with the correct\n        sphinx markup.\n    \"\"\"\n\n    spl = _automodapirex.split(sourcestr)\n    if len(spl) > 1:  # automodsumm is in this document\n\n        if dotoctree:\n            toctreestr = ':toctree: '\n            dirnm = app.config.automodapi_toctreedirnm\n            if not dirnm.endswith(\"/\"):\n                dirnm += \"/\"\n            if docname is not None:\n                toctreestr += '../' * docname.count('/') + dirnm\n            else:\n                toctreestr += dirnm\n        else:\n            toctreestr = ''\n\n        newstrs = [spl[0]]\n        for grp in range(len(spl) // 3):\n            modnm = spl[grp * 3 + 1]\n\n            # find where this is in the document for warnings\n            if docname is None:\n                location = None\n            else:\n                location = (docname, spl[0].count('\\n'))\n\n            # initialize default options\n            toskip = []\n            inhdiag = maindocstr = top_head = True\n            hds = '-^'\n            allowedpkgnms = []\n\n            # look for actual options\n            unknownops = []\n            for opname, args in _automodapiargsrex.findall(spl[grp * 3 + 2]):\n                if opname == 'skip':\n                    toskip.append(args.strip())\n                elif opname == 'no-inheritance-diagram':\n                    inhdiag = False\n                elif opname == 'no-main-docstr':\n                    maindocstr = False\n                elif opname == 'headings':\n                    hds = args\n                elif opname == 'no-heading':\n                    top_head = False\n                elif opname == 'allowed-package-names':\n                    allowedpkgnms.append(args.strip())\n                else:\n                    unknownops.append(opname)\n\n            #join all the allowedpkgnms\n            if len(allowedpkgnms) == 0:\n                allowedpkgnms = ''\n                onlylocals = True\n            else:\n                allowedpkgnms = ':allowed-package-names: ' + ','.join(allowedpkgnms)\n                onlylocals = allowedpkgnms\n\n            # get the two heading chars\n            if len(hds) < 2:\n                msg = 'Not enough headings (got {0}, need 2), using default -^'\n                if warnings:\n                    app.warn(msg.format(len(hds)), location)\n                hds = '-^'\n            h1, h2 = hds.lstrip()[:2]\n\n            # tell sphinx that the remaining args are invalid.\n            if len(unknownops) > 0 and app is not None:\n                opsstrs = ','.join(unknownops)\n                msg = 'Found additional options ' + opsstrs + ' in automodapi.'\n                if warnings:\n                    app.warn(msg, location)\n\n            ispkg, hascls, hasfuncs = _mod_info(modnm, toskip, onlylocals=onlylocals)\n\n            # add automodule directive only if no-main-docstr isn't present\n            if maindocstr:\n                automodline = '.. automodule:: {modname}'.format(modname=modnm)\n            else:\n                automodline = ''\n            if top_head:\n                newstrs.append(automod_templ_modheader.format(modname=modnm,\n                    modhds=h1 * len(modnm),\n                    pkgormod='Package' if ispkg else 'Module',\n                    pkgormodhds=h1 * (8 if ispkg else 7),\n                    automoduleline=automodline))\n            else:\n                newstrs.append(automod_templ_modheader.format(\n                    modname='',\n                    modhds='',\n                    pkgormod='',\n                    pkgormodhds='',\n                    automoduleline=automodline))\n\n            #construct the options for the class/function sections\n            #start out indented at 4 spaces, but need to keep the indentation.\n            clsfuncoptions = []\n            if toctreestr:\n                clsfuncoptions.append(toctreestr)\n            if toskip:\n                clsfuncoptions.append(':skip: ' + ','.join(toskip))\n            if allowedpkgnms:\n                clsfuncoptions.append(allowedpkgnms)\n            clsfuncoptionstr = '\\n    '.join(clsfuncoptions)\n\n            if hasfuncs:\n                newstrs.append(automod_templ_funcs.format(\n                    modname=modnm,\n                    funchds=h2 * 9,\n                    clsfuncoptions=clsfuncoptionstr))\n\n            if hascls:\n                newstrs.append(automod_templ_classes.format(\n                    modname=modnm,\n                    clshds=h2 * 7,\n                    clsfuncoptions=clsfuncoptionstr))\n\n            if inhdiag and hascls:\n                # add inheritance diagram if any classes are in the module\n                newstrs.append(automod_templ_inh.format(\n                    modname=modnm,\n                    clsinhsechds=h2 * 25,\n                    allowedpkgnms=allowedpkgnms))\n\n            newstrs.append(spl[grp * 3 + 3])\n\n        newsourcestr = ''.join(newstrs)\n\n        if app.config.automodapi_writereprocessed:\n            # sometimes they are unicode, sometimes not, depending on how\n            # sphinx has processed things\n            if isinstance(newsourcestr, text_type):\n                ustr = newsourcestr\n            else:\n                ustr = newsourcestr.decode(app.config.source_encoding)\n\n            if docname is None:\n                with open(os.path.join(app.srcdir, 'unknown.automodapi'), 'a') as f:\n                    f.write('\\n**NEW DOC**\\n\\n')\n                    f.write(ustr)\n            else:\n                env = app.builder.env\n                # Determine the filename associated with this doc (specifically\n                # the extension)\n                filename = docname + os.path.splitext(env.doc2path(docname))[1]\n                filename += '.automodapi'\n\n                with open(os.path.join(app.srcdir, filename), 'w') as f:\n                    f.write(ustr)\n\n        return newsourcestr\n    else:\n        return sourcestr", "response": "Replaces the source string with the contents of the automodapi template."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndetermining if a module is a module or a package and whether or not it has classes or functions.", "response": "def _mod_info(modname, toskip=[], onlylocals=True):\n    \"\"\"\n    Determines if a module is a module or a package and whether or not\n    it has classes or functions.\n    \"\"\"\n\n    hascls = hasfunc = False\n\n    for localnm, fqnm, obj in zip(*find_mod_objs(modname, onlylocals=onlylocals)):\n        if localnm not in toskip:\n            hascls = hascls or inspect.isclass(obj)\n            hasfunc = hasfunc or inspect.isroutine(obj)\n            if hascls and hasfunc:\n                break\n\n    # find_mod_objs has already imported modname\n    # TODO: There is probably a cleaner way to do this, though this is pretty\n    # reliable for all Python versions for most cases that we care about.\n    pkg = sys.modules[modname]\n    ispkg = (hasattr(pkg, '__file__') and isinstance(pkg.__file__, str) and\n             os.path.split(pkg.__file__)[1].startswith('__init__.py'))\n\n    return ispkg, hascls, hasfunc"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the affinity matrix with the given method", "response": "def compute_affinity_matrix(adjacency_matrix, method='auto', **kwargs):\n    \"\"\"Compute the affinity matrix with the given method\"\"\"\n    if method == 'auto':\n        method = 'gaussian'\n    return Affinity.init(method, **kwargs).affinity_matrix(adjacency_matrix)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef barycenter_graph(distance_matrix, X, reg=1e-3):\n    (N, d_in) = X.shape\n    (rows, cols) = distance_matrix.nonzero()\n    W = sparse.lil_matrix((N, N)) # best for W[i, nbrs_i] = w/np.sum(w)\n    for i in range(N):\n        nbrs_i = cols[rows == i]\n        n_neighbors_i = len(nbrs_i)\n        v = np.ones(n_neighbors_i, dtype=X.dtype)\n        C = X[nbrs_i] - X[i]\n        G = np.dot(C, C.T)\n        trace = np.trace(G)\n        if trace > 0:\n            R = reg * trace\n        else:\n            R = reg\n        G.flat[::n_neighbors_i + 1] += R\n        w = solve(G, v, sym_pos = True)\n        W[i, nbrs_i] = w / np.sum(w)\n    return W", "response": "Computes the barycenter weighted graph for points in X."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nperforming a Locally Linear Embedding analysis on the data. Parameters ---------- geom : a Geometry object from megaman.geometry.geometry n_components : integer number of coordinates for the manifold. reg : float regularization constant, multiplies the trace of the local covariance matrix of the distances. eigen_solver : {'auto', 'dense', 'arpack', 'lobpcg', or 'amg'} 'auto' : algorithm will attempt to choose the best method for input data 'dense' : use standard dense matrix operations for the eigenvalue decomposition. For this method, M must be an array or matrix type. This method should be avoided for large problems. 'arpack' : use arnoldi iteration in shift-invert mode. For this method, M may be a dense matrix, sparse matrix, or general linear operator. Warning: ARPACK can be unstable for some problems. It is best to try several random seeds in order to check results. 'lobpcg' : Locally Optimal Block Preconditioned Conjugate Gradient Method. A preconditioned eigensolver for large symmetric positive definite (SPD) generalized eigenproblems. 'amg' : AMG requires pyamg to be installed. It can be faster on very large, sparse problems, but may also lead to instabilities. random_state : numpy.RandomState or int, optional The generator or seed used to determine the starting vector for arpack iterations. Defaults to numpy.random. solver_kwds : any additional keyword arguments to pass to the selected eigen_solver Returns ------- Y : array-like, shape [n_samples, n_components] Embedding vectors. squared_error : float Reconstruction error for the embedding vectors. Equivalent to ``norm(Y - W Y, 'fro')**2``, where W are the reconstruction weights. References ---------- .. [1] Roweis, S. & Saul, L. Nonlinear dimensionality reduction by locally linear embedding. Science 290:2323 (2000).", "response": "def locally_linear_embedding(geom, n_components, reg=1e-3,\n                            eigen_solver='auto',  random_state=None,\n                            solver_kwds=None):\n    \"\"\"\n    Perform a Locally Linear Embedding analysis on the data.\n\n    Parameters\n    ----------\n    geom : a Geometry object from megaman.geometry.geometry\n    n_components : integer\n        number of coordinates for the manifold.\n    reg : float\n        regularization constant, multiplies the trace of the local covariance\n        matrix of the distances.\n    eigen_solver : {'auto', 'dense', 'arpack', 'lobpcg', or 'amg'}\n        'auto' :\n            algorithm will attempt to choose the best method for input data\n        'dense' :\n            use standard dense matrix operations for the eigenvalue decomposition.\n            For this method, M must be an array or matrix type.  This method should be avoided for large problems.\n        'arpack' :\n            use arnoldi iteration in shift-invert mode. For this method,\n            M may be a dense matrix, sparse matrix, or general linear operator.\n            Warning: ARPACK can be unstable for some problems.  It is best to\n            try several random seeds in order to check results.\n        'lobpcg' :\n            Locally Optimal Block Preconditioned Conjugate Gradient Method.\n            A preconditioned eigensolver for large symmetric positive definite\n            (SPD) generalized eigenproblems.\n        'amg' :\n            AMG requires pyamg to be installed. It can be faster on very large,\n            sparse problems, but may also lead to instabilities.\n    random_state : numpy.RandomState or int, optional\n        The generator or seed used to determine the starting vector for arpack\n        iterations.  Defaults to numpy.random.\n    solver_kwds : any additional keyword arguments to pass to the selected eigen_solver\n\n    Returns\n    -------\n    Y : array-like, shape [n_samples, n_components]\n        Embedding vectors.\n    squared_error : float\n        Reconstruction error for the embedding vectors. Equivalent to\n        ``norm(Y - W Y, 'fro')**2``, where W are the reconstruction weights.\n\n    References\n    ----------\n\n    .. [1] Roweis, S. & Saul, L. Nonlinear dimensionality reduction\n        by locally linear embedding.  Science 290:2323 (2000).\n    \"\"\"\n    if geom.X is None:\n        raise ValueError(\"Must pass data matrix X to Geometry\")\n    if geom.adjacency_matrix is None:\n        geom.compute_adjacency_matrix()\n    W = barycenter_graph(geom.adjacency_matrix, geom.X, reg=reg)\n    # we'll compute M = (I-W)'(I-W)\n    # depending on the solver, we'll do this differently\n    eigen_solver, solver_kwds = check_eigen_solver(eigen_solver, solver_kwds,\n                                                   size=W.shape[0],\n                                                   nvec=n_components + 1)\n    if eigen_solver != 'dense':\n        M = eye(*W.shape, format=W.format) - W\n        M = (M.T * M).tocsr()\n    else:\n        M = (W.T * W - W.T - W).toarray()\n        M.flat[::M.shape[0] + 1] += 1  # W = W - I = W - I\n    return null_space(M, n_components, k_skip=1, eigen_solver=eigen_solver,\n                      random_state=random_state)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fit(self, X, y=None, input_type='data'):\n        X = self._validate_input(X, input_type)\n        self.fit_geometry(X, input_type)\n        random_state = check_random_state(self.random_state)\n        self.embedding_, self.error_ = locally_linear_embedding(self.geom_,\n                                                                n_components=self.n_components,\n                                                                eigen_solver=self.eigen_solver,\n                                                                random_state=self.random_state,\n                                                                reg=self.reg,\n                                                                solver_kwds=self.solver_kwds)\n        return self", "response": "Fit the model from data in X."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a string representation of a shape.", "response": "def _shape_repr(shape):\n    \"\"\"Return a platform independent reprensentation of an array shape\n    Under Python 2, the `long` type introduces an 'L' suffix when using the\n    default %r format for tuples of integers (typically used to store the shape\n    of an array).\n    Under Windows 64 bit (and Python 2), the `long` type is used by default\n    in numpy shapes even when the integer dimensions are well below 32 bit.\n    The platform specific type causes string messages or doctests to change\n    from one platform to another which is not desirable.\n    Under Python 3, there is no more `long` type so the `L` suffix is never\n    introduced in string representation.\n    >>> _shape_repr((1, 2))\n    '(1, 2)'\n    >>> one = 2 ** 64 / 2 ** 64  # force an upcast to `long` under Python 2\n    >>> _shape_repr((one, 2 * one))\n    '(1, 2)'\n    >>> _shape_repr((1,))\n    '(1,)'\n    >>> _shape_repr(())\n    '()'\n    \"\"\"\n    if len(shape) == 0:\n        return \"()\"\n    joined = \", \".join(\"%d\" % e for e in shape)\n    if len(shape) == 1:\n        # special notation for singleton tuples\n        joined += ','\n    return \"(%s)\" % joined"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning number of samples in array - like x.", "response": "def _num_samples(x):\n    \"\"\"Return number of samples in array-like x.\"\"\"\n    if hasattr(x, 'fit'):\n        # Don't get num_samples from an ensembles length!\n        raise TypeError('Expected sequence or array-like, got '\n                        'estimator %s' % x)\n    if not hasattr(x, '__len__') and not hasattr(x, 'shape'):\n        if hasattr(x, '__array__'):\n            x = np.asarray(x)\n        else:\n            raise TypeError(\"Expected sequence or array-like, got %s\" %\n                            type(x))\n    if hasattr(x, 'shape'):\n        if len(x.shape) == 0:\n            raise TypeError(\"Singleton array %r cannot be considered\"\n                            \" a valid collection.\" % x)\n        return x.shape[0]\n    else:\n        return len(x)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts a sparse matrix to a given format.", "response": "def _ensure_sparse_format(spmatrix, accept_sparse, dtype, copy,\n                          force_all_finite):\n    \"\"\"Convert a sparse matrix to a given format.\n    Checks the sparse format of spmatrix and converts if necessary.\n    Parameters\n    ----------\n    spmatrix : scipy sparse matrix\n        Input to validate and convert.\n    accept_sparse : string, list of string or None (default=None)\n        String[s] representing allowed sparse matrix formats ('csc',\n        'csr', 'coo', 'dok', 'bsr', 'lil', 'dia'). None means that sparse\n        matrix input will raise an error.  If the input is sparse but not in\n        the allowed format, it will be converted to the first listed format.\n    dtype : string, type or None (default=none)\n        Data type of result. If None, the dtype of the input is preserved.\n    copy : boolean (default=False)\n        Whether a forced copy will be triggered. If copy=False, a copy might\n        be triggered by a conversion.\n    force_all_finite : boolean (default=True)\n        Whether to raise an error on np.inf and np.nan in X.\n    Returns\n    -------\n    spmatrix_converted : scipy sparse matrix.\n        Matrix that is ensured to have an allowed type.\n    \"\"\"\n    if accept_sparse in [None, False]:\n        raise TypeError('A sparse matrix was passed, but dense '\n                        'data is required. Use X.toarray() to '\n                        'convert to a dense numpy array.')\n    if dtype is None:\n        dtype = spmatrix.dtype\n\n    changed_format = False\n    if (isinstance(accept_sparse, (list, tuple))\n            and spmatrix.format not in accept_sparse):\n        # create new with correct sparse\n        spmatrix = spmatrix.asformat(accept_sparse[0])\n        changed_format = True\n\n    if dtype != spmatrix.dtype:\n        # convert dtype\n        spmatrix = spmatrix.astype(dtype)\n    elif copy and not changed_format:\n        # force copy\n        spmatrix = spmatrix.copy()\n\n    if force_all_finite:\n        if not hasattr(spmatrix, \"data\"):\n            warnings.warn(\"Can't check %s sparse matrix for nan or inf.\"\n                          % spmatrix.format)\n        else:\n            _assert_all_finite(spmatrix.data)\n    return spmatrix"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef check_symmetric(array, tol=1E-10, raise_warning=True,\n                    raise_exception=False):\n    \"\"\"Make sure that array is 2D, square and symmetric.\n    If the array is not symmetric, then a symmetrized version is returned.\n    Optionally, a warning or exception is raised if the matrix is not\n    symmetric.\n    Parameters\n    ----------\n    array : nd-array or sparse matrix\n        Input object to check / convert. Must be two-dimensional and square,\n        otherwise a ValueError will be raised.\n    tol : float\n        Absolute tolerance for equivalence of arrays. Default = 1E-10.\n    raise_warning : boolean (default=True)\n        If True then raise a warning if conversion is required.\n    raise_exception : boolean (default=False)\n        If True then raise an exception if array is not symmetric.\n    Returns\n    -------\n    array_sym : ndarray or sparse matrix\n        Symmetrized version of the input array, i.e. the average of array\n        and array.transpose(). If sparse, then duplicate entries are first\n        summed and zeros are eliminated.\n    \"\"\"\n    if (array.ndim != 2) or (array.shape[0] != array.shape[1]):\n        raise ValueError(\"array must be 2-dimensional and square. \"\n                         \"shape = {0}\".format(array.shape))\n    if sp.issparse(array):\n        diff = array - array.T\n        # only csr, csc, and coo have `data` attribute\n        if diff.format not in ['csr', 'csc', 'coo']:\n            diff = diff.tocsr()\n        symmetric = np.all(abs(diff.data) < tol)\n    else:\n        symmetric = np.allclose(array, array.T, atol=tol)\n    if not symmetric:\n        if raise_exception:\n            raise ValueError(\"Array must be symmetric\")\n        if raise_warning:\n            warnings.warn(\"Array is not symmetric, and will be converted \"\n                          \"to symmetric by average with its transpose.\")\n        if sp.issparse(array):\n            conversion = 'to' + array.format\n            array = getattr(0.5 * (array + array.T), conversion)()\n        else:\n            array = 0.5 * (array + array.T)\n    return array", "response": "Check if the input array is symmetric."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check_array(array, accept_sparse=None, dtype=\"numeric\", order=None,\n                copy=False, force_all_finite=True, ensure_2d=True,\n                allow_nd=False, ensure_min_samples=1, ensure_min_features=1,\n                warn_on_dtype=False):\n    \"\"\"Input validation on an array, list, sparse matrix or similar.\n    By default, the input is converted to an at least 2nd numpy array.\n    If the dtype of the array is object, attempt converting to float,\n    raising on failure.\n    Parameters\n    ----------\n    array : object\n        Input object to check / convert.\n    accept_sparse : string, list of string or None (default=None)\n        String[s] representing allowed sparse matrix formats, such as 'csc',\n        'csr', etc.  None means that sparse matrix input will raise an error.\n        If the input is sparse but not in the allowed format, it will be\n        converted to the first listed format.\n    dtype : string, type, list of types or None (default=\"numeric\")\n        Data type of result. If None, the dtype of the input is preserved.\n        If \"numeric\", dtype is preserved unless array.dtype is object.\n        If dtype is a list of types, conversion on the first type is only\n        performed if the dtype of the input is not in the list.\n    order : 'F', 'C' or None (default=None)\n        Whether an array will be forced to be fortran or c-style.\n    copy : boolean (default=False)\n        Whether a forced copy will be triggered. If copy=False, a copy might\n        be triggered by a conversion.\n    force_all_finite : boolean (default=True)\n        Whether to raise an error on np.inf and np.nan in X.\n    ensure_2d : boolean (default=True)\n        Whether to make X at least 2d.\n    allow_nd : boolean (default=False)\n        Whether to allow X.ndim > 2.\n    ensure_min_samples : int (default=1)\n        Make sure that the array has a minimum number of samples in its first\n        axis (rows for a 2D array). Setting to 0 disables this check.\n    ensure_min_features : int (default=1)\n        Make sure that the 2D array has some minimum number of features\n        (columns). The default value of 1 rejects empty datasets.\n        This check is only enforced when the input data has effectively 2\n        dimensions or is originally 1D and ``ensure_2d`` is True. Setting to 0\n        disables this check.\n    warn_on_dtype : boolean (default=False)\n        Raise DataConversionWarning if the dtype of the input data structure\n        does not match the requested dtype, causing a memory copy.\n    estimator : str or estimator instance (default=None)\n        If passed, include the name of the estimator in warning messages.\n    Returns\n    -------\n    X_converted : object\n        The converted and validated X.\n    \"\"\"\n    if isinstance(accept_sparse, str):\n        accept_sparse = [accept_sparse]\n\n    # store whether originally we wanted numeric dtype\n    dtype_numeric = dtype == \"numeric\"\n\n    dtype_orig = getattr(array, \"dtype\", None)\n    if not hasattr(dtype_orig, 'kind'):\n        # not a data type (e.g. a column named dtype in a pandas DataFrame)\n        dtype_orig = None\n\n    if dtype_numeric:\n        if dtype_orig is not None and dtype_orig.kind == \"O\":\n            # if input is object, convert to float.\n            dtype = np.float64\n        else:\n            dtype = None\n\n    if isinstance(dtype, (list, tuple)):\n        if dtype_orig is not None and dtype_orig in dtype:\n            # no dtype conversion required\n            dtype = None\n        else:\n            # dtype conversion required. Let's select the first element of the\n            # list of accepted types.\n            dtype = dtype[0]\n\n    if sp.issparse(array):\n        array = _ensure_sparse_format(array, accept_sparse, dtype, copy,\n                                      force_all_finite)\n    else:\n        array = np.array(array, dtype=dtype, order=order, copy=copy)\n\n        if ensure_2d:\n            if array.ndim == 1:\n                if ensure_min_samples >= 2:\n                    raise ValueError(\"%s expects at least 2 samples provided \"\n                                     \"in a 2 dimensional array-like input\"\n                                     % estimator_name)\n                warnings.warn(\n                    \"Passing 1d arrays as data is deprecated in 0.17 and will\"\n                    \"raise ValueError in 0.19. Reshape your data either using \"\n                    \"X.reshape(-1, 1) if your data has a single feature or \"\n                    \"X.reshape(1, -1) if it contains a single sample.\",\n                    DeprecationWarning)\n            array = np.atleast_2d(array)\n            # To ensure that array flags are maintained\n            array = np.array(array, dtype=dtype, order=order, copy=copy)\n\n        # make sure we acually converted to numeric:\n        if dtype_numeric and array.dtype.kind == \"O\":\n            array = array.astype(np.float64)\n        if not allow_nd and array.ndim >= 3:\n            raise ValueError(\"Found array with dim %d. expected <= 2.\"\n                             % (array.ndim))\n        if force_all_finite:\n            _assert_all_finite(array)\n\n    shape_repr = _shape_repr(array.shape)\n    if ensure_min_samples > 0:\n        n_samples = _num_samples(array)\n        if n_samples < ensure_min_samples:\n            raise ValueError(\"Found array with %d sample(s) (shape=%s) while a\"\n                             \" minimum of %d is required.\"\n                             % (n_samples, shape_repr, ensure_min_samples))\n\n    if ensure_min_features > 0 and array.ndim == 2:\n        n_features = array.shape[1]\n        if n_features < ensure_min_features:\n            raise ValueError(\"Found array with %d feature(s) (shape=%s) while\"\n                             \" a minimum of %d is required.\"\n                             % (n_features, shape_repr, ensure_min_features))\n\n    if warn_on_dtype and dtype_orig is not None and array.dtype != dtype_orig:\n        msg = (\"Data with input dtype %s was converted to %s.\"\n               % (dtype_orig, array.dtype))\n        warnings.warn(msg, DataConversionWarning)\n    return array", "response": "Input validation on an array."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fit(self, X, y=None, input_type='affinity'):\n        X = self._validate_input(X, input_type)\n        self.fit_geometry(X, input_type)\n        random_state = check_random_state(self.random_state)            \n        self.embedding_, self.eigen_vectors_, self.P_ = spectral_clustering(self.geom_, K = self.K, \n                                                                   eigen_solver = self.eigen_solver, \n                                                                   random_state = self.random_state, \n                                                                   solver_kwds = self.solver_kwds,\n                                                                   renormalize = self.renormalize,\n                                                                   stabalize = self.stabalize,\n                                                                   additional_vectors = self.additional_vectors)", "response": "Fits the model from data in X to S."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef covar_plotter3d_matplotlib(embedding, rieman_metric, inspect_points_idx, ax,\n                               colors):\n    \"\"\"3 Dimensional Covariance plotter using matplotlib backend.\"\"\"\n    for pts_idx in inspect_points_idx:\n        plot_ellipse_matplotlib( cov=rieman_metric[pts_idx],\n                             pos=embedding[pts_idx],\n                             ax=ax, ec='k', lw=1,\n                             color=colors[pts_idx] )\n    return ax", "response": "3 Dimensional Covariance plotter using matplotlib backend."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef plot_ellipse_matplotlib(cov, pos, ax, nstd=2, **kwargs):\n    from matplotlib.patches import Ellipse\n    from mpl_toolkits.mplot3d import art3d, Axes3D\n    ellipse_param, normal = calc_2d_ellipse_properties(cov,nstd)\n    ellipse_kwds = merge_keywords(ellipse_param, kwargs)\n    ellip = Ellipse(xy=(0,0), **ellipse_kwds)\n    ax.add_patch(ellip)\n\n    ellip = pathpatch_2d_to_3d(ellip, normal=normal)\n    ellip = pathpatch_translate(ellip,pos)\n    return ellip", "response": "Plot an ellipse in 3d using matplotlib backend"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef merge_keywords(x,y):\n    z = x.copy()\n    z.update(y)\n    return z", "response": "Given two dicts merge them into a new dict as a shallow copy."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntransform a 2D patch to a 3D patch.", "response": "def pathpatch_2d_to_3d(pathpatch, z = 0, normal = 'z'):\n    \"\"\"\n    Transforms a 2D Patch to a 3D patch using the given normal vector.\n\n    The patch is projected into they XY plane, rotated about the origin\n    and finally translated by z.\n    \"\"\"\n    if type(normal) is str: #Translate strings to normal vectors\n        index = \"xyz\".index(normal)\n        normal = np.roll((1.0,0,0), index)\n\n    normal /= np.linalg.norm(normal) #Make sure the vector is normalised\n\n    path = pathpatch.get_path() #Get the path and the associated transform\n    trans = pathpatch.get_patch_transform()\n\n    path = trans.transform_path(path) #Apply the transform\n\n    pathpatch.__class__ = art3d.PathPatch3D #Change the class\n    pathpatch._code3d = path.codes #Copy the codes\n    pathpatch._facecolor3d = pathpatch.get_facecolor #Get the face color\n\n    verts = path.vertices #Get the vertices in 2D\n\n    d = np.cross(normal, (0, 0, 1)) #Obtain the rotation vector\n    M = rotation_matrix(d) #Get the rotation matrix\n\n    pathpatch._segment3d = \\\n        np.array([np.dot(M, (x, y, 0)) + (0, 0, z) for x, y in verts])\n    return pathpatch"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef plot_ellipse_plotly(cov, pos, nstd=2,**kwargs):\n    ellipse_param, normal = calc_2d_ellipse_properties(cov,nstd)\n    points = create_ellipse(**ellipse_param)\n\n    points = transform_to_3d(points, normal=normal)\n    points = translate_in_3d(points,pos)\n    return create_ellipse_mesh(points,**kwargs)", "response": "Plot 2d ellipse in 3d using matplotlib backend"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates the properties for 2d ellipse given the covariance matrix.", "response": "def calc_2d_ellipse_properties(cov,nstd=2):\n    \"\"\"Calculate the properties for 2d ellipse given the covariance matrix.\"\"\"\n    def eigsorted(cov):\n        vals, vecs = np.linalg.eigh(cov)\n        order = vals.argsort()[::-1]\n        return vals[order], vecs[:,order]\n\n    vals, vecs = eigsorted(cov)\n    width, height = 2 * nstd * np.sqrt(vals[:2])\n    normal = vecs[:,2] if vecs[2,2] > 0 else -vecs[:,2]\n    d = np.cross(normal, (0, 0, 1))\n    M = rotation_matrix(d)\n    x_trans = np.dot(M,(1,0,0))\n    cos_val = np.dot(vecs[:,0],x_trans)/np.linalg.norm(vecs[:,0])/np.linalg.norm(x_trans)\n    theta = np.degrees(np.arccos(np.clip(cos_val, -1, 1))) # if you really want the angle\n    return { 'width': width, 'height': height, 'angle': theta }, normal"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating a rotation matrix given a vector d.", "response": "def rotation_matrix(d):\n    \"\"\"\n    Calculates a rotation matrix given a vector d. The direction of d\n    corresponds to the rotation axis. The length of d corresponds to\n    the sin of the angle of rotation.\n\n    Variant of: http://mail.scipy.org/pipermail/numpy-discussion/2009-March/040806.html\n    \"\"\"\n    sin_angle = np.linalg.norm(d)\n\n    if sin_angle == 0:\n        return np.identity(3)\n\n    d /= sin_angle\n\n    eye = np.eye(3)\n    ddt = np.outer(d, d)\n    skew = np.array([[    0,  d[2],  -d[1]],\n                     [-d[2],     0,   d[0]],\n                     [ d[1], -d[0],     0]], dtype=np.float64)\n\n    M = ddt + np.sqrt(1 - sin_angle**2) * (eye - ddt) + sin_angle * skew\n    return M"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_ellipse(width,height,angle):\n    angle = angle / 180.0 * np.pi\n    thetas = np.linspace(0,2*np.pi,200)\n    a = width / 2.0\n    b = height / 2.0\n\n    x = a*np.cos(thetas)*np.cos(angle) - b*np.sin(thetas)*np.sin(angle)\n    y = a*np.cos(thetas)*np.sin(angle) + b*np.sin(thetas)*np.cos(angle)\n    z = np.zeros(thetas.shape)\n    return np.vstack((x,y,z)).T", "response": "Create parametric ellipse from 200 points."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nprojecting points into 3d from 2d points.", "response": "def transform_to_3d(points,normal,z=0):\n    \"\"\"Project points into 3d from 2d points.\"\"\"\n    d = np.cross(normal, (0, 0, 1))\n    M = rotation_matrix(d)\n    transformed_points = M.dot(points.T).T + z\n    return transformed_points"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_ellipse_mesh(points,**kwargs):\n    import plotly.graph_objs as go\n    x,y,z = points.T\n    return (go.Mesh3d(x=x,y=y,z=z,**kwargs),\n            go.Scatter3d(x=x, y=y, z=z,\n                         marker=dict(size=0.01),\n                         line=dict(width=2,color='#000000'),\n                         showlegend=False,\n                         hoverinfo='none'\n            )\n    )", "response": "Visualize the ellipse by using the mesh of the points."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef plot_point_cov(points, nstd=2, ax=None, **kwargs):\n    pos = points.mean(axis=0)\n    cov = np.cov(points, rowvar=False)\n    return plot_cov_ellipse(cov, pos, nstd, ax, **kwargs)", "response": "Plots an nstd sigma ellipse based on the mean and covariance of a point."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nplotting an nstd sigma error ellipse based on the specified covariance matrix.", "response": "def plot_cov_ellipse(cov, pos, nstd=2, ax=None, **kwargs):\n    \"\"\"\n    Plots an `nstd` sigma error ellipse based on the specified covariance\n    matrix (`cov`). Additional keyword arguments are passed on to the\n    ellipse patch artist.\n\n    Parameters\n    ----------\n        cov : The 2x2 covariance matrix to base the ellipse on\n        pos : The location of the center of the ellipse. Expects a 2-element\n            sequence of [x0, y0].\n        nstd : The radius of the ellipse in numbers of standard deviations.\n            Defaults to 2 standard deviations.\n        ax : The axis that the ellipse will be plotted on. Defaults to the\n            current axis.\n        Additional keyword arguments are pass on to the ellipse patch.\n\n    Returns\n    -------\n        A matplotlib ellipse artist\n    \"\"\"\n    def eigsorted(cov):\n        vals, vecs = np.linalg.eigh(cov)\n        order = vals.argsort()[::-1]\n        return vals[order], vecs[:,order]\n\n    if ax is None:\n        ax = plt.gca()\n\n    vals, vecs = eigsorted(cov)\n    theta = np.degrees(np.arctan2(*vecs[:,0][::-1]))\n\n    # Width and height are \"full\" widths, not radius\n    width, height = 2 * nstd * np.sqrt(vals)\n    ellip = Ellipse(xy=pos, width=width, height=height, angle=theta, **kwargs)\n\n    ax.add_artist(ellip)\n    return ellip"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nperforms a Local Tangent Space Alignment analysis on the data. Parameters ---------- geom : a Geometry object from megaman.geometry.geometry n_components : integer number of coordinates for the manifold. eigen_solver : {'auto', 'dense', 'arpack', 'lobpcg', or 'amg'} 'auto' : algorithm will attempt to choose the best method for input data 'dense' : use standard dense matrix operations for the eigenvalue decomposition. For this method, M must be an array or matrix type. This method should be avoided for large problems. 'arpack' : use arnoldi iteration in shift-invert mode. For this method, M may be a dense matrix, sparse matrix, or general linear operator. Warning: ARPACK can be unstable for some problems. It is best to try several random seeds in order to check results. 'lobpcg' : Locally Optimal Block Preconditioned Conjugate Gradient Method. A preconditioned eigensolver for large symmetric positive definite (SPD) generalized eigenproblems. 'amg' : AMG requires pyamg to be installed. It can be faster on very large, sparse problems, but may also lead to instabilities. random_state : numpy.RandomState or int, optional The generator or seed used to determine the starting vector for arpack iterations. Defaults to numpy.random. solver_kwds : any additional keyword arguments to pass to the selected eigen_solver Returns ------- embedding : array-like, shape [n_samples, n_components] Embedding vectors. squared_error : float Reconstruction error for the embedding vectors. Equivalent to ``norm(Y - W Y, 'fro')**2``, where W are the reconstruction weights. References ---------- * Zhang, Z. & Zha, H. Principal manifolds and nonlinear dimensionality reduction via tangent space alignment. Journal of Shanghai Univ. 8:406 (2004)", "response": "def ltsa(geom, n_components, eigen_solver='auto',\n         random_state=None, solver_kwds=None):\n    \"\"\"\n    Perform a Local Tangent Space Alignment analysis on the data.\n\n    Parameters\n    ----------\n    geom : a Geometry object from megaman.geometry.geometry\n    n_components : integer\n        number of coordinates for the manifold.\n    eigen_solver : {'auto', 'dense', 'arpack', 'lobpcg', or 'amg'}\n        'auto' :\n            algorithm will attempt to choose the best method for input data\n        'dense' :\n            use standard dense matrix operations for the eigenvalue decomposition.\n            For this method, M must be an array or matrix type.  This method should be avoided for large problems.\n        'arpack' :\n            use arnoldi iteration in shift-invert mode. For this method,\n            M may be a dense matrix, sparse matrix, or general linear operator.\n            Warning: ARPACK can be unstable for some problems.  It is best to\n            try several random seeds in order to check results.\n        'lobpcg' :\n            Locally Optimal Block Preconditioned Conjugate Gradient Method.\n            A preconditioned eigensolver for large symmetric positive definite\n            (SPD) generalized eigenproblems.\n        'amg' :\n            AMG requires pyamg to be installed. It can be faster on very large,\n            sparse problems, but may also lead to instabilities.\n    random_state : numpy.RandomState or int, optional\n        The generator or seed used to determine the starting vector for arpack\n        iterations.  Defaults to numpy.random.\n    solver_kwds : any additional keyword arguments to pass to the selected eigen_solver\n\n    Returns\n    -------\n    embedding : array-like, shape [n_samples, n_components]\n        Embedding vectors.\n    squared_error : float\n        Reconstruction error for the embedding vectors. Equivalent to\n        ``norm(Y - W Y, 'fro')**2``, where W are the reconstruction weights.\n\n    References\n    ----------\n    * Zhang, Z. & Zha, H. Principal manifolds and nonlinear\n      dimensionality reduction via tangent space alignment.\n      Journal of Shanghai Univ.  8:406 (2004)\n    \"\"\"\n    if geom.X is None:\n        raise ValueError(\"Must pass data matrix X to Geometry\")\n    (N, d_in) = geom.X.shape\n    if n_components > d_in:\n        raise ValueError(\"output dimension must be less than or equal \"\n                         \"to input dimension\")\n    # get the distance matrix and neighbors list\n    if geom.adjacency_matrix is None:\n        geom.compute_adjacency_matrix()\n    (rows, cols) = geom.adjacency_matrix.nonzero()\n    eigen_solver, solver_kwds = check_eigen_solver(eigen_solver, solver_kwds,\n                                                   size=geom.adjacency_matrix.shape[0],\n                                                   nvec=n_components + 1)\n    if eigen_solver != 'dense':\n        M = sparse.csr_matrix((N, N))\n    else:\n        M = np.zeros((N, N))\n    for i in range(N):\n        neighbors_i = cols[rows == i]\n        n_neighbors_i = len(neighbors_i)\n        use_svd = (n_neighbors_i > d_in)\n        Xi = geom.X[neighbors_i]\n        Xi -= Xi.mean(0)\n        # compute n_components largest eigenvalues of Xi * Xi^T\n        if use_svd:\n            v = svd(Xi, full_matrices=True)[0]\n        else:\n            Ci = np.dot(Xi, Xi.T)\n            v = eigh(Ci)[1][:, ::-1]\n        Gi = np.zeros((n_neighbors_i, n_components + 1))\n        Gi[:, 1:] = v[:, :n_components]\n        Gi[:, 0] = 1. / np.sqrt(n_neighbors_i)\n        GiGiT = np.dot(Gi, Gi.T)\n        nbrs_x, nbrs_y = np.meshgrid(neighbors_i, neighbors_i)\n        with warnings.catch_warnings():\n            # sparse will complain this is better with lil_matrix but it doesn't work\n            warnings.simplefilter(\"ignore\")\n            M[nbrs_x, nbrs_y] -= GiGiT\n            M[neighbors_i, neighbors_i] += 1\n    return null_space(M, n_components, k_skip=1, eigen_solver=eigen_solver,\n                      random_state=random_state,solver_kwds=solver_kwds)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run_riemannian_relaxation(laplacian, initial_guess,\n                              intrinsic_dim, relaxation_kwds):\n    \"\"\"Helper function for creating a RiemannianRelaxation class.\"\"\"\n    n, s = initial_guess.shape\n    relaxation_kwds = initialize_kwds(relaxation_kwds, n, s, intrinsic_dim)\n    if relaxation_kwds['save_init']:\n        directory = relaxation_kwds['backup_dir']\n        np.save(os.path.join(directory, 'Y0.npy'),initial_guess)\n        sp.io.mmwrite(os.path.join(directory, 'L_used.mtx'),\n                      sp.sparse.csc_matrix(laplacian))\n\n    lossf = relaxation_kwds['lossf']\n    return RiemannianRelaxation.init(lossf, laplacian, initial_guess,\n                                     intrinsic_dim, relaxation_kwds)", "response": "Helper function for creating a RiemannianRelaxation class."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef relax_isometry(self):\n        for ii in range(self.relaxation_kwds['niter']):\n            self.H = self.compute_dual_rmetric()\n\n            self.loss = self.rieman_loss()\n            self.trace_var.update(ii,self.H,self.Y,self.eta,self.loss)\n            self.trace_var.print_report(ii)\n            self.trace_var.save_backup(ii)\n\n            self.compute_gradient()\n\n            self.make_optimization_step(first_iter=(ii == 0))\n\n        self.H = self.compute_dual_rmetric()\n\n        self.trace_var.update(-1,self.H,self.Y,self.eta,self.loss)\n        self.trace_var.print_report(ii)\n        tracevar_path = os.path.join(self.trace_var.backup_dir, 'results.pyc')\n        TracingVariable.save(self.trace_var,tracevar_path)", "response": "Main function for doing riemannian relaxation."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef calc_loss(self, embedding):\n        Hnew = self.compute_dual_rmetric(Ynew=embedding)\n        return self.rieman_loss(Hnew=Hnew)", "response": "Helper function to calculate rieman loss given new embedding"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef compute_dual_rmetric(self,Ynew=None):\n        usedY = self.Y if Ynew is None else Ynew\n        rieman_metric = RiemannMetric(usedY, self.laplacian_matrix)\n        return rieman_metric.get_dual_rmetric()", "response": "Helper function to calculate the dual rmetric"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef automodsumm_to_autosummary_lines(fn, app):\n\n    fullfn = os.path.join(app.builder.env.srcdir, fn)\n\n    with open(fullfn) as fr:\n        if 'astropy_helpers.sphinx.ext.automodapi' in app._extensions:\n            from astropy_helpers.sphinx.ext.automodapi import automodapi_replace\n            # Must do the automodapi on the source to get the automodsumm\n            # that might be in there\n            docname = os.path.splitext(fn)[0]\n            filestr = automodapi_replace(fr.read(), app, True, docname, False)\n        else:\n            filestr = fr.read()\n\n    spl = _automodsummrex.split(filestr)\n    #0th entry is the stuff before the first automodsumm line\n    indent1s = spl[1::5]\n    mods = spl[2::5]\n    opssecs = spl[3::5]\n    indent2s = spl[4::5]\n    remainders = spl[5::5]\n\n    # only grab automodsumm sections and convert them to autosummary with the\n    # entries for all the public objects\n    newlines = []\n\n    #loop over all automodsumms in this document\n    for i, (i1, i2, modnm, ops, rem) in enumerate(zip(indent1s, indent2s, mods,\n                                                    opssecs, remainders)):\n        allindent = i1 + ('' if i2 is None else i2)\n\n        #filter out functions-only and classes-only options if present\n        oplines = ops.split('\\n')\n        toskip = []\n        allowedpkgnms = []\n        funcsonly = clssonly = False\n        for i, ln in reversed(list(enumerate(oplines))):\n            if ':functions-only:' in ln:\n                funcsonly = True\n                del oplines[i]\n            if ':classes-only:' in ln:\n                clssonly = True\n                del oplines[i]\n            if ':skip:' in ln:\n                toskip.extend(_str_list_converter(ln.replace(':skip:', '')))\n                del oplines[i]\n            if ':allowed-package-names:' in ln:\n                allowedpkgnms.extend(_str_list_converter(ln.replace(':allowed-package-names:', '')))\n                del oplines[i]\n        if funcsonly and clssonly:\n            msg = ('Defined both functions-only and classes-only options. '\n                   'Skipping this directive.')\n            lnnum = sum([spl[j].count('\\n') for j in range(i * 5 + 1)])\n            app.warn('[automodsumm]' + msg, (fn, lnnum))\n            continue\n\n        # Use the currentmodule directive so we can just put the local names\n        # in the autosummary table.  Note that this doesn't always seem to\n        # actually \"take\" in Sphinx's eyes, so in `Automodsumm.run`, we have to\n        # force it internally, as well.\n        newlines.extend([i1 + '.. currentmodule:: ' + modnm,\n                         '',\n                         '.. autosummary::'])\n        newlines.extend(oplines)\n\n        ols = True if len(allowedpkgnms) == 0 else allowedpkgnms\n        for nm, fqn, obj in zip(*find_mod_objs(modnm, onlylocals=ols)):\n            if nm in toskip:\n                continue\n            if funcsonly and not inspect.isroutine(obj):\n                continue\n            if clssonly and not inspect.isclass(obj):\n                continue\n            newlines.append(allindent + nm)\n\n    # add one newline at the end of the autosummary block\n    newlines.append('')\n\n    return newlines", "response": "Generates lines from a file with an automodsumm entry suitable for feeding into autosummary."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef compute_adjacency_matrix(X, method='auto', **kwargs):\n    if method == 'auto':\n        if X.shape[0] > 10000:\n            method = 'cyflann'\n        else:\n            method = 'kd_tree'\n    return Adjacency.init(method, **kwargs).adjacency_graph(X.astype('float'))", "response": "Compute an adjacency matrix with the given method"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef split_kwargs(relaxation_kwds):\n    optimizer_keys_list = [\n        'step_method',\n        'linesearch',\n        'eta_max',\n        'eta',\n        'm',\n        'linesearch_first'\n    ]\n    optimizer_kwargs = { k:relaxation_kwds.pop(k) for k in optimizer_keys_list if k in relaxation_kwds }\n    if 'm' in optimizer_kwargs:\n        optimizer_kwargs['momentum'] = optimizer_kwargs.pop('m')\n    return optimizer_kwargs, relaxation_kwds", "response": "Split relaxation keywords to keywords for optimizer and others"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninitializing relaxation keywords. Parameters ---------- relaxation_kwds : dict weights : numpy array, the weights step_method : string { 'fixed', 'momentum' } which optimizers to use linesearch : bool whether to do linesearch in search for eta in optimization verbose : bool whether to print reports to I/O when doing relaxation niter : int number of iterations to run. niter_trace : int number of iterations to be traced. presave : bool whether to store precomputed keywords to files or not. sqrd : bool whether to use squared norm in loss function. Default : True alpha : float shrinkage rate for previous gradient. Default : 0 projected : bool whether or not to optimize via projected gradient descent on differences S lossf : string { 'epsilon', 'rloss' } which loss function to optimize. Default : 'rloss' if n == d, otherwise 'epsilon' subset : numpy array Subset to do relaxation on. sub_dir : string sub_dir used to store the outputs. backup_base_dir : string base directory used to store outputs Final path will be backup_base_dir/sub_dir saveiter : int save backup on every saveiter iterations printiter : int print report on every printiter iterations save_init : bool whether to save Y0 and L before running relaxation.", "response": "def initialize_kwds(relaxation_kwds, n_samples, n_components, intrinsic_dim):\n    \"\"\"\n    Initialize relaxation keywords.\n\n    Parameters\n    ----------\n    relaxation_kwds : dict\n        weights : numpy array, the weights\n        step_method : string { 'fixed', 'momentum' }\n            which optimizers to use\n        linesearch : bool\n            whether to do linesearch in search for eta in optimization\n        verbose : bool\n            whether to print reports to I/O when doing relaxation\n        niter : int\n            number of iterations to run.\n        niter_trace : int\n            number of iterations to be traced.\n        presave : bool\n            whether to store precomputed keywords to files or not.\n        sqrd : bool\n            whether to use squared norm in loss function. Default : True\n        alpha : float\n            shrinkage rate for previous gradient. Default : 0\n        projected : bool\n            whether or not to optimize via projected gradient descent on differences S\n        lossf : string { 'epsilon', 'rloss' }\n            which loss function to optimize.\n            Default : 'rloss' if n == d, otherwise 'epsilon'\n        subset : numpy array\n            Subset to do relaxation on.\n        sub_dir : string\n            sub_dir used to store the outputs.\n        backup_base_dir : string\n            base directory used to store outputs\n            Final path will be backup_base_dir/sub_dir\n        saveiter : int\n            save backup on every saveiter iterations\n        printiter : int\n            print report on every printiter iterations\n        save_init : bool\n            whether to save Y0 and L before running relaxation.\n    \"\"\"\n    new_relaxation_kwds = {\n        'weights': np.array([],dtype=np.float64),\n        'step_method': 'fixed',\n        'linesearch': True,\n        'verbose': False,\n        'niter': 2000,\n        'niter_trace': 0,\n        'presave': False,\n        'sqrd': True,\n        'alpha': 0,\n        'projected': False,\n        'lossf': 'epsilon' if n_components > intrinsic_dim else 'rloss',\n        'subset': np.arange(n_samples),\n        'sub_dir': current_time_str(),\n        'backup_base_dir': default_basedir,\n        'saveiter': 10,\n        'printiter': 1,\n        'save_init': False,\n    }\n\n    new_relaxation_kwds.update(relaxation_kwds)\n\n    backup_dir = os.path.join(new_relaxation_kwds['backup_base_dir'], new_relaxation_kwds['sub_dir'])\n    new_relaxation_kwds['backup_dir'] = backup_dir\n    create_output_dir(backup_dir)\n\n    new_relaxation_kwds = convert_to_int(new_relaxation_kwds)\n\n    if new_relaxation_kwds['weights'].shape[0] != 0:\n        weights = np.absolute(new_relaxation_kwds['weights']).astype(np.float64)\n        new_relaxation_kwds['weights'] = weights / np.sum(weights)\n\n    if new_relaxation_kwds['lossf'] == 'epsilon':\n        new_relaxation_kwds.setdefault('eps_orth', 0.1)\n\n    if n_components != intrinsic_dim and new_relaxation_kwds['lossf'] == 'rloss':\n        raise ValueError('loss function rloss is for n_components equal intrinsic_dim')\n\n    if n_components == intrinsic_dim and new_relaxation_kwds['lossf'] == 'epsilon':\n        raise ValueError('loss function rloss is for n_components equal intrinsic_dim')\n\n    if new_relaxation_kwds['projected'] and new_relaxation_kwds['subset'].shape[0] < n_samples:\n        raise ValueError('Projection derivative not working for subset methods.')\n\n    prefix = 'projected' if new_relaxation_kwds['projected'] else 'nonprojected'\n    new_relaxation_kwds['lossf'] = '{}_{}'.format(prefix,new_relaxation_kwds['lossf'])\n    step_method = new_relaxation_kwds['step_method']\n\n    if new_relaxation_kwds['linesearch'] == True:\n        new_relaxation_kwds.setdefault('linesearch_first', False)\n        init_eta_max = 2**11 if new_relaxation_kwds['projected'] else 2**4\n        new_relaxation_kwds.setdefault('eta_max',init_eta_max)\n    else:\n        new_relaxation_kwds.setdefault('eta', 1.0)\n\n    if step_method == 'momentum':\n        new_relaxation_kwds.setdefault('m', 0.05)\n\n    return new_relaxation_kwds"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfinding the largest graph connected components the contains one given node_id", "response": "def _graph_connected_component(graph, node_id):\n    \"\"\"\n    Find the largest graph connected components the contains one\n    given node\n\n    Parameters\n    ----------\n    graph : array-like, shape: (n_samples, n_samples)\n        adjacency matrix of the graph, non-zero weight means an edge\n        between the nodes\n\n    node_id : int\n        The index of the query node of the graph\n\n    Returns\n    -------\n    connected_components : array-like, shape: (n_samples,)\n        An array of bool value indicates the indexes of the nodes\n        belong to the largest connected components of the given query\n        node\n    \"\"\"\n    connected_components = np.zeros(shape=(graph.shape[0]), dtype=np.bool)\n    connected_components[node_id] = True\n    n_node = graph.shape[0]\n    for i in range(n_node):\n        last_num_component = connected_components.sum()\n        _, node_to_add = np.where(graph[connected_components] != 0)\n        connected_components[node_to_add] = True\n        if last_num_component >= connected_components.sum():\n            break\n    return connected_components"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns True if the graph is fully connected False otherwise.", "response": "def _graph_is_connected(graph):\n    \"\"\"\n    Return whether the graph is connected (True) or Not (False)\n\n    Parameters\n    ----------\n    graph : array-like or sparse matrix, shape: (n_samples, n_samples)\n        adjacency matrix of the graph, non-zero weight means an edge\n        between the nodes\n\n    Returns\n    -------\n    is_connected : bool\n        True means the graph is fully connected and False means not\n    \"\"\"\n    if sparse.isspmatrix(graph):\n        # sparse graph, find all the connected components\n        n_connected_components, _ = connected_components(graph)\n        return n_connected_components == 1\n    else:\n        # dense graph, find all connected components start from node 0\n        return _graph_connected_component(graph, 0).sum() == graph.shape[0]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef compute_diffusion_maps(lapl_type, diffusion_map, lambdas, diffusion_time):\n    # Check that diffusion maps is using the correct laplacian, warn otherwise\n    if lapl_type not in ['geometric', 'renormalized']:\n        warnings.warn(\"for correct diffusion maps embedding use laplacian type 'geometric' or 'renormalized'.\")\n    # Step 5 of diffusion maps:\n    vectors = diffusion_map.copy()\n    psi = vectors/vectors[:,[0]]\n    diffusion_times = diffusion_time\n    if diffusion_time == 0:\n        lambdas = np.abs(lambdas)\n        diffusion_times = np.exp(1. -  np.log(1 - lambdas[1:])/np.log(lambdas[1:]))\n        lambdas = lambdas / (1 - lambdas)\n    else:\n        lambdas = np.abs(lambdas)\n        lambdas = lambdas ** float(diffusion_time)\n    diffusion_map = psi * lambdas\n    return diffusion_map", "response": "Compute the diffusion maps for a given laplacian type and diffusion time."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef spectral_embedding(geom, n_components=8, eigen_solver='auto',\n                       random_state=None, drop_first=True,\n                       diffusion_maps = False, diffusion_time = 0, solver_kwds = None):\n    \"\"\"\n    Project the sample on the first eigen vectors of the graph Laplacian.\n\n    The adjacency matrix is used to compute a normalized graph Laplacian\n    whose  principal eigenvectors (associated to the\n    smallest eigen values) represent the embedding coordinates of the data.\n\n    The ``adjacency`` variable is not strictly the adjacency matrix of a graph but more generally\n    an affinity or similarity matrix between samples (for instance the\n    heat kernel of a euclidean distance matrix or a k-NN matrix).\n    The Laplacian must be symmetric so that the eigen vector decomposition works as expected.\n    This is ensured by the default setting (for more details,\n    see the documentation in geometry.py).\n\n    The data and generic geometric parameters are passed via a Geometry object, which also\n    computes the Laplacian. By default, the 'geometric' Laplacian (or \"debiased\", or \"renormalized\" with\n    alpha=1) is used. This is the Laplacian construction defined in [Coifman and Lafon, 2006] (see also\n    documentation in laplacian.py). Thus, with diffusion_maps=False, spectral embedding is a modification\n    of the Laplacian Eigenmaps algorithm of [Belkin and Nyiogi, 2002], with diffusion_maps=False, geom.laplacian_method\n    ='symmetricnormalized' it is exactly the Laplacian Eigenmaps, with diffusion_maps=True, diffusion_time>0 it\n    is the Diffusion Maps algorithm of [Coifman and Lafon 2006]; diffusion_maps=True and diffusion_time=0 is the same\n    as diffusion_maps=False and default geom.laplacian_method.\n\n    Parameters\n    ----------\n    geom : a Geometry object from megaman.embedding.geometry\n    n_components : integer, optional\n        The dimension of the projection subspace.\n    eigen_solver : {'auto', 'dense', 'arpack', 'lobpcg', or 'amg'}\n        'auto' :\n            algorithm will attempt to choose the best method for input data\n        'dense' :\n            use standard dense matrix operations for the eigenvalue decomposition.\n            For this method, M must be an array or matrix type.  This method should be avoided for large problems.\n        'arpack' :\n            use arnoldi iteration in shift-invert mode. For this method,\n            M may be a dense matrix, sparse matrix, or general linear operator.\n            Warning: ARPACK can be unstable for some problems.  It is best to\n            try several random seeds in order to check results.\n        'lobpcg' :\n            Locally Optimal Block Preconditioned Conjugate Gradient Method.\n            A preconditioned eigensolver for large symmetric positive definite\n            (SPD) generalized eigenproblems.\n        'amg' :\n            AMG requires pyamg to be installed. It can be faster on very large,\n            sparse problems, but may also lead to instabilities.\n    random_state : int seed, RandomState instance, or None (default)\n        A pseudo random number generator used for the initialization of the\n        lobpcg eigen vectors decomposition when eigen_solver == 'amg'.\n        By default, arpack is used.\n    drop_first : bool, optional, default=True\n        Whether to drop the first eigenvector. For spectral embedding, this\n        should be True as the first eigenvector should be constant vector for\n        connected graph, but for spectral clustering, this should be kept as\n        False to retain the first eigenvector.\n    diffusion_map : boolean, optional. Whether to return the diffusion map\n        version by re-scaling the embedding coordinate by the eigenvalues to the power\n        diffusion_time.\n    diffusion_time: if diffusion_map=True, the eigenvectors of the Laplacian are rescaled by\n        (1-lambda)^diffusion_time, where lambda is the corresponding eigenvalue.\n        diffusion_time has the role of scale parameter. One of the main ideas of diffusion framework is\n        that running the diffusion forward in time (taking larger and larger\n        powers of the Laplacian/transition matrix) reveals the geometric structure of X at larger and\n        larger scales (the diffusion process).\n        diffusion_time = 0 empirically provides a reasonable balance from a clustering\n        perspective. Specifically, the notion of a cluster in the data set\n        is quantified as a region in which the probability of escaping this\n        region is low (within a certain time t).\n        Credit to Satrajit Ghosh (http://satra.cogitatum.org/) for description\n    solver_kwds : any additional keyword arguments to pass to the selected eigen_solver\n\n    Returns\n    -------\n    embedding : array, shape=(n_samples, n_components)\n        The reduced samples.\n\n    Notes\n    -----\n    Spectral embedding is most useful when the graph has one connected\n    component. If there graph has many components, the first few eigenvectors\n    will simply uncover the connected components of the graph.\n\n    References\n    ----------\n    * http://en.wikipedia.org/wiki/LOBPCG\n\n    * Toward the Optimal Preconditioned Eigensolver: Locally Optimal\n      Block Preconditioned Conjugate Gradient Method\n      Andrew V. Knyazev\n      http://dx.doi.org/10.1137%2FS1064827500366124\n    \"\"\"\n    random_state = check_random_state(random_state)\n\n    if geom.affinity_matrix is None:\n        geom.compute_affinity_matrix()\n    if not _graph_is_connected(geom.affinity_matrix):\n        warnings.warn(\"Graph is not fully connected: \"\n                      \"spectral embedding may not work as expected.\")\n\n    if geom.laplacian_matrix is None:\n        laplacian = geom.compute_laplacian_matrix(copy=False,\n                                                  return_lapsym=True)\n    else:\n        laplacian = geom.laplacian_matrix\n\n    n_nodes = laplacian.shape[0]\n    lapl_type = geom.laplacian_method\n    eigen_solver, solver_kwds = check_eigen_solver(eigen_solver,solver_kwds,\n                                                   size=laplacian.shape[0],\n                                                   nvec=n_components + 1)\n    re_normalize = False\n    PD_solver = False\n    if eigen_solver in ['amg', 'lobpcg']: # these methods require a symmetric positive definite matrix!\n        epsilon = 2\n        PD_solver = True\n        if lapl_type not in ['symmetricnormalized', 'unnormalized']:\n            re_normalize = True\n            # If lobpcg (or amg with lobpcg) is chosen and\n            # If the Laplacian is non-symmetric then we need to extract:\n            # the w (weight) vector from geometry\n            # and the symmetric Laplacian = S.\n            # The actual Laplacian is L = W^{-1}S  (Where W is the diagonal matrix of w)\n            # Which has the same spectrum as: L* = W^{-1/2}SW^{-1/2} which is symmetric\n            # We calculate the eigen-decomposition of L*: [D, V]\n            # then use W^{-1/2}V  to compute the eigenvectors of L\n            # See (Handbook for Cluster Analysis Chapter 2 Proposition 1).\n            # However, since we censor the affinity matrix A at a radius it is not guaranteed\n            # to be positive definite. But since L = W^{-1}S has maximum eigenvalue 1 (stochastic matrix)\n            # and L* has the same spectrum it also has largest e-value of 1.\n            # therefore if we look at I - L* then this has smallest eigenvalue of 0 and so\n            # must be positive semi-definite. It also has the same spectrum as L* but\n            # lambda(I - L*) = 1 - lambda(L*).\n            # Finally, since we want positive definite not semi-definite we use (1+epsilon)*I\n            # instead of I to make the smallest eigenvalue epsilon.\n            if geom.laplacian_weights is None: # a laplacian existed but it wasn't called with return_lapsym = True\n                geom.compute_laplacian_matrix(copy = False, return_lapsym = True)\n            w = np.array(geom.laplacian_weights)\n            symmetrized_laplacian = geom.laplacian_symmetric.copy()\n            if sparse.isspmatrix(symmetrized_laplacian):\n                symmetrized_laplacian.data /= np.sqrt(w[symmetrized_laplacian.row])\n                symmetrized_laplacian.data /= np.sqrt(w[symmetrized_laplacian.col])\n                symmetrized_laplacian = (1+epsilon)*sparse.identity(n_nodes) - symmetrized_laplacian\n            else:\n                symmetrized_laplacian /= np.sqrt(w)\n                symmetrized_laplacian /= np.sqrt(w[:,np.newaxis])\n                symmetrized_laplacian = (1+epsilon)*np.identity(n_nodes) - symmetrized_laplacian\n        else: # using a symmetric laplacian but adjust to avoid positive definite errors\n            symmetrized_laplacian = geom.laplacian_matrix.copy()\n            if sparse.isspmatrix(symmetrized_laplacian):\n                symmetrized_laplacian = (1+epsilon)*sparse.identity(n_nodes) - symmetrized_laplacian\n            else:\n                symmetrized_laplacian = (1+epsilon)*np.identity(n_nodes) - symmetrized_laplacian\n\n    if PD_solver: # then eI - L was used, fix the eigenvalues\n        lambdas, diffusion_map = eigen_decomposition(symmetrized_laplacian, n_components+1, eigen_solver=eigen_solver,\n                                                     random_state=random_state, drop_first=drop_first, largest = False,\n                                                     solver_kwds=solver_kwds)\n        lambdas = -lambdas + epsilon\n    else:\n        lambdas, diffusion_map = eigen_decomposition(laplacian, n_components+1, eigen_solver=eigen_solver,\n                                                     random_state=random_state, drop_first=drop_first, largest = True,\n                                                     solver_kwds=solver_kwds)\n    if re_normalize:\n        diffusion_map /= np.sqrt(w[:, np.newaxis]) # put back on original Laplacian space\n        diffusion_map /= np.linalg.norm(diffusion_map, axis = 0) # norm 1 vectors\n    # sort the eigenvalues\n    ind = np.argsort(lambdas); ind = ind[::-1]\n    lambdas = lambdas[ind]; lambdas[0] = 0\n    diffusion_map = diffusion_map[:, ind]\n    eigenvalues = lambdas.copy()\n    eigenvectors = diffusion_map.copy()\n    if diffusion_maps:\n        diffusion_map = compute_diffusion_maps(lapl_type, diffusion_map, lambdas, diffusion_time)\n    if drop_first:\n        embedding = diffusion_map[:, 1:(n_components+1)]\n        eigenvectors = eigenvectors[:, 1:(n_components+1)]\n        eigenvalues = eigenvalues[1:(n_components+1)]\n    else:\n        embedding = diffusion_map[:, :n_components]\n        eigenvectors = eigenvectors[:, :(n_components)]\n        eigenvalues = eigenvalues[:(n_components)]\n    return embedding, eigenvalues, eigenvectors", "response": "Spectral embedding of a geometry."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfit the model from data in X y = None.", "response": "def fit(self, X, y=None, input_type='data'):\n        \"\"\"\n        Fit the model from data in X.\n\n        Parameters\n        ----------\n        input_type : string, one of: 'data', 'distance' or 'affinity'.\n            The values of input data X. (default = 'data')\n        X : array-like, shape (n_samples, n_features)\n            Training vector, where n_samples in the number of samples\n            and n_features is the number of features.\n\n        If self.input_type is distance, or affinity:\n\n        X : array-like, shape (n_samples, n_samples),\n            Interpret X as precomputed distance or adjacency graph\n            computed from samples.\n\n        Returns\n        -------\n        self : object\n               Returns the instance itself.\n        \"\"\"\n        X = self._validate_input(X, input_type)\n        self.fit_geometry(X, input_type)\n        random_state = check_random_state(self.random_state)\n        self.embedding_, self.eigenvalues_, self.eigenvectors_ = spectral_embedding(self.geom_,\n                                             n_components = self.n_components,\n                                             eigen_solver = self.eigen_solver,\n                                             random_state = random_state,\n                                             drop_first = self.drop_first,\n                                             diffusion_maps = self.diffusion_maps,\n                                             diffusion_time = self.diffusion_time,\n                                             solver_kwds = self.solver_kwds)\n        self.affinity_matrix_ = self.geom_.affinity_matrix\n        self.laplacian_matrix_ = self.geom_.laplacian_matrix\n        self.laplacian_matrix_type_ = self.geom_.laplacian_method\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes the laplacian matrix with the given method", "response": "def compute_laplacian_matrix(affinity_matrix, method='auto', **kwargs):\n    \"\"\"Compute the laplacian matrix with the given method\"\"\"\n    if method == 'auto':\n        method = 'geometric'\n    return Laplacian.init(method, **kwargs).laplacian_matrix(affinity_matrix)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfit the geometry to the data X and returns the fitted geometry.", "response": "def fit_geometry(self, X=None, input_type='data'):\n        \"\"\"Inputs self.geom, and produces the fitted geometry self.geom_\"\"\"\n        if self.geom is None:\n            self.geom_ = Geometry()\n        elif isinstance(self.geom, Geometry):\n            self.geom_ = self.geom\n        else:\n            try:\n                kwds = dict(**self.geom)\n            except TypeError:\n                raise ValueError(\"geom must be a Geometry instance or \"\n                                 \"a mappable/dictionary\")\n            self.geom_ = Geometry(**kwds)\n\n        if self.radius is not None:\n            self.geom_.set_radius(self.radius, override=False)\n\n        # if self.radius == 'auto':\n        #     if X is not None and input_type != 'affinity':\n        #         self.geom_.set_radius(self.estimate_radius(X, input_type),\n        #                               override=False)\n        # else:\n        #     self.geom_.set_radius(self.radius,\n        #                           override=False)\n\n        if X is not None:\n            self.geom_.set_matrix(X, input_type)\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\napply the optimizations on the state of the object.", "response": "def apply_optimization(self, update_embedding_with, grad, **kwargs):\n        \"\"\"\n        Calculating (Obtaining) the learning rate (eta) and apply optimizations\n        on the embedding states by the specified method.\n\n        Parameters\n        ----------\n        update_embedding_with : function\n            Function used to update the state of RiemannianRelaxation\n            class (Y or S).\n\n        grad : (n x s) array\n            Gradients used in updating the embedding.\n\n        calc_loss : function (used by its child function)\n            Function used to calculated the loss from the temperary state of\n            RiemannianRelaxation instance. (YT or ST)\n\n        loss : float (used by its child function)\n            Loss of the current state of RiemannianRelaxation instance.\n        \"\"\"\n        if self.linesearch:\n            return self._apply_linesearch_optimzation(update_embedding_with,\n                                                      grad, **kwargs)\n        else:\n            return self._apply_fixed_optimization(update_embedding_with,\n                                                  grad, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_radius(self, radius, override=True, X=None, n_components=2):\n        if radius < 0:\n            raise ValueError(\"radius must be non-negative\")\n\n        if override or ('radius' not in self.adjacency_kwds and\n                        'n_neighbors' not in self.adjacency_kwds):\n            self.adjacency_kwds['radius'] = radius\n\n        if override or ('radius' not in self.affinity_kwds):\n            self.affinity_kwds['radius'] = radius", "response": "Set the radius for the adjacency and affinity computation."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the data matrix given the ( string ) input_type", "response": "def set_matrix(self, X, input_type):\n        \"\"\"Set the data matrix given the (string) input_type\"\"\"\n        if input_type == 'data':\n            self.set_data_matrix(X)\n        elif input_type == 'adjacency':\n            self.set_adjacency_matrix(X)\n        elif input_type == 'affinity':\n            self.set_affinity_matrix(X)\n        else:\n            raise ValueError(\"Unrecognized input_type: {0}\".format(input_type))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef compute_affinity_matrix(self, copy=False, **kwargs):\n        if self.adjacency_matrix is None:\n            self.compute_adjacency_matrix()\n\n        kwds = self.affinity_kwds.copy()\n        kwds.update(kwargs)\n        self.affinity_matrix = compute_affinity_matrix(self.adjacency_matrix,\n                                                       self.affinity_method,\n                                                       **kwds)\n        if copy:\n            return self.affinity_matrix.copy()\n        else:\n            return self.affinity_matrix", "response": "This function computes the affinity matrix for the Guassian kernel and bandwidth of the Guassian kernel."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nnote this function will compute the laplacian matrix. In order to acquire the existing laplacian matrix use self.laplacian_matrix as comptute_laplacian_matrix() will re-compute the laplacian matrix. Parameters ---------- copy : boolean, whether to return copied version of the self.laplacian_matrix return_lapsym : boolean, if True returns additionally the symmetrized version of the requested laplacian and the re-normalization weights. **kwargs : see laplacian.py docmuentation for arguments for each method. Returns ------- self.laplacian_matrix : sparse matrix (N_obs, N_obs). The requested laplacian. self.laplacian_symmetric : sparse matrix (N_obs, N_obs) The symmetric laplacian. self.laplacian_weights : ndarray (N_obs,) The renormalization weights used to make laplacian_matrix from laplacian_symmetric", "response": "def compute_laplacian_matrix(self, copy=True, return_lapsym=False, **kwargs):\n        \"\"\"\n        Note: this function will compute the laplacian matrix. In order to acquire\n            the existing laplacian matrix use self.laplacian_matrix as\n            comptute_laplacian_matrix() will re-compute the laplacian matrix.\n\n        Parameters\n        ----------\n        copy : boolean, whether to return copied version of the self.laplacian_matrix\n        return_lapsym : boolean, if True returns additionally the symmetrized version of\n            the requested laplacian and the re-normalization weights.\n        **kwargs : see laplacian.py docmuentation for arguments for each method.\n\n        Returns\n        -------\n        self.laplacian_matrix : sparse matrix (N_obs, N_obs).\n            The requested laplacian.\n        self.laplacian_symmetric : sparse matrix (N_obs, N_obs)\n            The symmetric laplacian.\n        self.laplacian_weights : ndarray (N_obs,)\n            The renormalization weights used to make\n            laplacian_matrix from laplacian_symmetric\n        \"\"\"\n        if self.affinity_matrix is None:\n            self.compute_affinity_matrix()\n\n        kwds = self.laplacian_kwds.copy()\n        kwds.update(kwargs)\n        kwds['full_output'] = return_lapsym\n        result = compute_laplacian_matrix(self.affinity_matrix,\n                                          self.laplacian_method,\n                                          **kwds)\n        if return_lapsym:\n            (self.laplacian_matrix,\n             self.laplacian_symmetric,\n             self.laplacian_weights) = result\n        else:\n            self.laplacian_matrix = result\n\n        if copy:\n            return self.laplacian_matrix.copy()\n        else:\n            return self.laplacian_matrix"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_data_matrix(self, X):\n        X = check_array(X, accept_sparse=sparse_formats)\n        self.X = X", "response": "Set the data matrix of the entry set."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the adjacency matrix for the entry class.", "response": "def set_adjacency_matrix(self, adjacency_mat):\n        \"\"\"\n        Parameters\n        ----------\n        adjacency_mat : sparse matrix (N_obs, N_obs)\n            The adjacency matrix to input.\n        \"\"\"\n        adjacency_mat = check_array(adjacency_mat, accept_sparse=sparse_formats)\n        if adjacency_mat.shape[0] != adjacency_mat.shape[1]:\n            raise ValueError(\"adjacency matrix is not square\")\n        self.adjacency_matrix = adjacency_mat"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_affinity_matrix(self, affinity_mat):\n        affinity_mat = check_array(affinity_mat, accept_sparse=sparse_formats)\n        if affinity_mat.shape[0] != affinity_mat.shape[1]:\n            raise ValueError(\"affinity matrix is not square\")\n        self.affinity_matrix = affinity_mat", "response": "Sets the affinity matrix for the log entry."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the Laplacian matrix for the log entry.", "response": "def set_laplacian_matrix(self, laplacian_mat):\n        \"\"\"\n        Parameters\n        ----------\n        laplacian_mat : sparse matrix (N_obs, N_obs).\n            The Laplacian matrix to input.\n        \"\"\"\n        laplacian_mat = check_array(laplacian_mat, accept_sparse = sparse_formats)\n        if laplacian_mat.shape[0] != laplacian_mat.shape[1]:\n            raise ValueError(\"Laplacian matrix is not square\")\n        self.laplacian_matrix = laplacian_mat"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef riemann_metric( Y, laplacian, n_dim=None, invert_h=False, mode_inv = 'svd'):\n    n_samples = laplacian.shape[0]\n    h_dual_metric = np.zeros((n_samples, n_dim, n_dim ))\n    for i in np.arange(n_dim ):\n        for j in np.arange(i,n_dim ):\n            yij = Y[:,i]*Y[:,j]\n            h_dual_metric[ :, i, j] = 0.5*(laplacian.dot(yij)-Y[:,j]*laplacian.dot(Y[:,i])-Y[:,i]*laplacian.dot(Y[:,j]))\n    for j in np.arange(n_dim-1):\n        for i in np.arange(j+1,n_dim):\n            h_dual_metric[ :,i,j] = h_dual_metric[:,j,i]\n\n    # compute rmetric if requested\n    if( invert_h ):\n        riemann_metric, Hvv, Hsvals, Gsvals = compute_G_from_H( h_dual_metric )\n    else:\n        riemann_metric = Hvv = Hsvals = Gsvals = None\n\n    return h_dual_metric, riemann_metric, Hvv, Hsvals, Gsvals", "response": "Returns the weighted riemann metric for the given data Y."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef compute_G_from_H( H, mdimG = None, mode_inv = \"svd\" ):\n    n_samples = H.shape[0]\n    n_dim = H.shape[2]\n    if mode_inv is 'svd':\n        Huu, Hsvals, Hvv = np.linalg.svd( H )\n        if mdimG is None:\n            Gsvals = 1./Hsvals\n            G = np.zeros((n_samples, n_dim, n_dim))\n            for i in np.arange(n_samples):\n                G[i,:,:] = np.dot(Huu[i,:,:], np.dot( np.diag(Gsvals[i,:]), Hvv[i,:,:]))\n        elif mdimG < n_dim:\n            Gsvals[:,:mdimG] = 1./Hsvals[:,:mdimG]\n            Gsvals[:,mdimG:] = 0.\n            # this can be redone with np.einsum() but it's barbaric\n            G = np.zeros((n_samples, mdimG, mdimG))\n            for i in np.arange(n_samples):\n                G[i,:,:mdimG] = np.dot(Huu[i,:,mdimG], np.dot( np.diag(Gsvals[i,:mdimG]), Hvv[i,:,:mdimG]))\n        else:\n            raise ValueError('mdimG must be <= H.shape[1]')\n        return G, Hvv, Hsvals, Gsvals\n    else:\n        riemann_metric = np.linalg.inv(h_dual_metric)\n        return riemann_metric, None, None, None", "response": "Compute the G matrix from the inverse R. MetricH H."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_dual_rmetric( self, invert_h = False, mode_inv = 'svd' ):\n        if self.H is None:\n            self.H, self.G, self.Hvv, self.Hsvals, self.Gsvals = riemann_metric(self.Y, self.L, self.mdimG, invert_h = invert_h, mode_inv = mode_inv)\n        if invert_h:\n            return self.H, self.G\n        else:\n            return self.H", "response": "Compute the dual Riemannian Metric\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the Reimannian Metric for this class.", "response": "def get_rmetric( self, mode_inv = 'svd', return_svd = False ):\n        \"\"\"\n        Compute the Reimannian Metric\n        \"\"\"\n        if self.H is None:\n            self.H, self.G, self.Hvv, self.Hsval = riemann_metric(self.Y, self.L, self.mdimG, invert_h = True, mode_inv = mode_inv)\n        if self.G is None:\n            self.G, self.Hvv, self.Hsvals,  self.Gsvals = compute_G_from_H( self.H, mode_inv = self.mode_inv )\n        if mode_inv is 'svd' and return_svd:\n            return self.G, self.Hvv, self.Hsvals, self.Gsvals\n        else:\n            return self.G"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsaving relaxation keywords to. txt and. pyc file", "response": "def report_and_save_keywords(self,relaxation_kwds,precomputed_kwds):\n        \"\"\"Save relaxation keywords to .txt and .pyc file\"\"\"\n        report_name = os.path.join(self.backup_dir,'relaxation_keywords.txt')\n        pretty_relax_kwds = pprint.pformat(relaxation_kwds,indent=4)\n        with open(report_name,'w') as wf:\n            wf.write(pretty_relax_kwds)\n        wf.close()\n\n        origin_name = os.path.join(self.backup_dir,'relaxation_keywords.pyc')\n        with open(origin_name,'wb') as ro:\n            pickle.dump(relaxation_kwds,ro,protocol=pickle.HIGHEST_PROTOCOL)\n        ro.close()\n\n        if relaxation_kwds['presave']:\n            precomp_kwds_name = os.path.join(self.backup_dir,\n                                             'precomputed_keywords.pyc')\n            with open(precomp_kwds_name, 'wb') as po:\n                pickle.dump(precomputed_kwds, po,\n                            protocol=pickle.HIGHEST_PROTOCOL)\n            po.close()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nupdating the trace_var in new iteration", "response": "def update(self,iiter,H,Y,eta,loss):\n        \"\"\"Update the trace_var in new iteration\"\"\"\n        if iiter <= self.niter_trace+1:\n            self.H[iiter] = H\n            self.Y[iiter] = Y\n        elif iiter >self.niter - self.niter_trace + 1:\n            self.H[self.ltrace+iiter-self.niter-1] = H\n            self.Y[self.ltrace+iiter-self.niter-1] = Y\n\n        self.etas[iiter] = eta\n        self.loss[iiter] = loss\n        if self.loss[iiter] < self.lmin:\n            self.Yh = Y\n            self.lmin = self.loss[iiter]\n            self.miniter = iiter if not iiter == -1 else self.niter + 1"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nclasses method save for saving TracingVariable.", "response": "def save(cls,instance,filename):\n        \"\"\"Class method save for saving TracingVariable.\"\"\"\n        filename = cls.correct_file_extension(filename)\n        try:\n            with open(filename,'wb') as f:\n                pickle.dump(instance,f,protocol=pickle.HIGHEST_PROTOCOL)\n        except MemoryError as e:\n            print ('{} occurred, will downsampled the saved file by 20.'\n                   .format(type(e).__name__))\n            copy_instance = instance.copy()\n            copy_instance.H = copy_instance.H[::20,:,:]\n            copy_instance.Y = copy_instance.Y[::20,:]\n            with open(filename,'wb') as f:\n                pickle.dump(copy_instance,f,protocol=pickle.HIGHEST_PROTOCOL)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load(cls,filename):\n        filename = cls.correct_file_extension(filename)\n        with open(filename,'rb') as f:\n            return pickle.load(f)", "response": "Load from stored files"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef isomap(geom, n_components=8, eigen_solver='auto',\n           random_state=None, path_method='auto',\n           distance_matrix=None, graph_distance_matrix = None,\n           centered_matrix=None, solver_kwds=None):\n    \"\"\"\n    Parameters\n    ----------\n    geom : a Geometry object from megaman.geometry.geometry\n    n_components : integer, optional\n        The dimension of the projection subspace.\n    eigen_solver : {'auto', 'dense', 'arpack', 'lobpcg', or 'amg'}\n        'auto' :\n            algorithm will attempt to choose the best method for input data\n        'dense' :\n            use standard dense matrix operations for the eigenvalue decomposition.\n            For this method, M must be an array or matrix type.  This method should be avoided for large problems.\n        'arpack' :\n            use arnoldi iteration in shift-invert mode. For this method,\n            M may be a dense matrix, sparse matrix, or general linear operator.\n            Warning: ARPACK can be unstable for some problems.  It is best to\n            try several random seeds in order to check results.\n        'lobpcg' :\n            Locally Optimal Block Preconditioned Conjugate Gradient Method.\n            A preconditioned eigensolver for large symmetric positive definite\n            (SPD) generalized eigenproblems.\n        'amg' :\n            AMG requires pyamg to be installed. It can be faster on very large,\n            sparse problems, but may also lead to instabilities.\n    random_state : int seed, RandomState instance, or None (default)\n        A pseudo random number generator used for the initialization of the\n        lobpcg eigen vectors decomposition when eigen_solver == 'amg'.\n        By default, arpack is used.\n    path_method : string, method for computing graph shortest path. One of :\n        'auto', 'D', 'FW', 'BF', 'J'. See scipy.sparse.csgraph.shortest_path\n        for more information.\n    distance_matrix : sparse Ndarray (n_obs, n_obs), optional. Pairwise distance matrix\n        sparse zeros considered 'infinite'.\n    graph_distance_matrix : Ndarray (n_obs, n_obs), optional. Pairwise graph distance\n        matrix. Output of graph_shortest_path.\n    centered_matrix : Ndarray (n_obs, n_obs), optional. Centered version of\n        graph_distance_matrix\n    solver_kwds : any additional keyword arguments to pass to the selected eigen_solver\n\n    Returns\n    -------\n    embedding : array, shape=(n_samples, n_components)\n        The reduced samples.\n\n    Notes\n    -----\n    \"\"\"\n    # Step 1: use geometry to calculate the distance matrix\n    if ((distance_matrix is None) and (centered_matrix is None)):\n        if geom.adjacency_matrix is None:\n            distance_matrix = geom.compute_adjacency_matrix()\n        else:\n            distance_matrix = geom.adjacency_matrix\n\n    # Step 2: use graph_shortest_path to construct D_G\n    ## WARNING: D_G is an (NxN) DENSE matrix!!\n    if ((graph_distance_matrix is None) and (centered_matrix is None)):\n        graph_distance_matrix = graph_shortest_path(distance_matrix,\n                                                    method=path_method,\n                                                    directed=False)\n\n    # Step 3: center graph distance matrix\n    if centered_matrix is None:\n        centered_matrix = center_matrix(graph_distance_matrix)\n\n\n    # Step 4: compute d largest eigenvectors/values of centered_matrix\n    lambdas, diffusion_map = eigen_decomposition(centered_matrix, n_components,\n                                                 largest=True,\n                                                 eigen_solver=eigen_solver,\n                                                 random_state=random_state,\n                                                 solver_kwds=solver_kwds)\n    # Step 5:\n    # return Y = [sqrt(lambda_1)*V_1, ..., sqrt(lambda_d)*V_d]\n    ind = np.argsort(lambdas); ind = ind[::-1] # sort largest\n    lambdas = lambdas[ind];\n    diffusion_map = diffusion_map[:, ind]\n    embedding = diffusion_map[:, 0:n_components] * np.sqrt(lambdas[0:n_components])\n    return embedding", "response": "This function is used to compute the shortest path of a geometry in a subspace."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfit the model from data in X y.", "response": "def fit(self, X, y=None, input_type='data'):\n        \"\"\"Fit the model from data in X.\n\n        Parameters\n        ----------\n        input_type : string, one of: 'data', 'distance'.\n            The values of input data X. (default = 'data')\n\n        X : array-like, shape (n_samples, n_features)\n            Training vector, where n_samples in the number of samples\n            and n_features is the number of features.\n\n        If self.input_type is 'distance':\n\n        X : array-like, shape (n_samples, n_samples),\n            Interpret X as precomputed distance or adjacency graph\n            computed from samples.\n\n        eigen_solver : {None, 'arpack', 'lobpcg', or 'amg'}\n            The eigenvalue decomposition strategy to use. AMG requires pyamg\n            to be installed. It can be faster on very large, sparse problems,\n            but may also lead to instabilities.\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n\n        X = self._validate_input(X, input_type)\n        self.fit_geometry(X, input_type)\n\n        if not hasattr(self, 'distance_matrix'):\n            self.distance_matrix = None\n        if not hasattr(self, 'graph_distance_matrix'):\n            self.graph_distance_matrix = None\n        if not hasattr(self, 'centered_matrix'):\n            self.centered_matrix = None\n\n        # don't re-compute these if it's already been done.\n        # This might be the case if an eigendecompostion fails and a different sovler is selected\n        if (self.distance_matrix is None and self.geom_.adjacency_matrix is None):\n            self.distance_matrix = self.geom_.compute_adjacency_matrix()\n        elif self.distance_matrix is None:\n            self.distance_matrix = self.geom_.adjacency_matrix\n        if self.graph_distance_matrix is None:\n            self.graph_distance_matrix = graph_shortest_path(self.distance_matrix,\n                                                             method = self.path_method,\n                                                             directed = False)\n        if self.centered_matrix is None:\n            self.centered_matrix = center_matrix(self.graph_distance_matrix)\n\n        self.embedding_ = isomap(self.geom_, n_components=self.n_components,\n                                 eigen_solver=self.eigen_solver,\n                                 random_state=self.random_state,\n                                 path_method = self.path_method,\n                                 distance_matrix = self.distance_matrix,\n                                 graph_distance_matrix = self.graph_distance_matrix,\n                                 centered_matrix = self.centered_matrix,\n                                 solver_kwds = self.solver_kwds)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef k_means_clustering(data,K):\n    N = data.shape[0]\n    centroids, data_norms = orthogonal_initialization(data,K)\n    old_centroids= np.zeros((N,K))\n    labels = []\n    \n    # Run the main k-means algorithm\n    while not _has_converged(centroids, old_centroids):    \n        labels = get_labels(data, centroids,K)                \n        centroids = get_centroids(data,K,labels,centroids,data_norms)\n        old_centroids = centroids\n        \n    return labels", "response": "K - means clustering is an algorithm that takes a data set and returns the labels which represents the clusters of data which are similar to others."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninitializes the centrodis by orthogonal_initialization. Parameters -------------------- X(data): array-like, shape= (m_samples,n_samples) K: integer number of K clusters Returns ------- centroids: array-like, shape (K,n_samples) data_norms: array-like, shape=(1,n_samples)", "response": "def orthogonal_initialization(X,K):\n    \"\"\"\n    Initialize the centrodis by orthogonal_initialization.\n    Parameters    \n    --------------------\n    X(data): array-like, shape= (m_samples,n_samples)\n    K: integer\n        number of K clusters   \n    Returns\n    -------\n    centroids: array-like, shape (K,n_samples)  \n    data_norms: array-like, shape=(1,n_samples)     \n    \"\"\"\n    N,M = X.shape\n    centroids= X[np.random.randint(0, N-1,1),:] \n    data_norms = np.linalg.norm(X, axis = 1)# contains the norm of each data point, only do this once\n         \n    center_norms = np.linalg.norm(centroids, axis=1) # contains the norms of the centers, will need to be updated when new center added\n        \n    for k in range(1,K):    \n        ## Here's where we compute the cosine of the angle between them:\n        # Compute the dot (inner) product between each data point and each center\n        new_center_index,new_center = new_orthogonal_center(X,data_norms,centroids,center_norms =center_norms)\n        centroids = np.vstack((centroids,new_center))          \n        center_norms = np.hstack((center_norms,data_norms[new_center_index]))   \n    return centroids,data_norms"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninitializes the centrodis by orthogonal_initialization. Parameters -------------------- X(data): array-like, shape= (m_samples,n_samples) data_norms: array-like, shape=(1,n_samples) center_norms:array-like,shape=(centroids.shape[0]) centroids: array-like, shape (K,n_samples) Returns ------- new_center: array-like, shape (1,n_samples) new_center_index: integer data index of the new center", "response": "def new_orthogonal_center(X,data_norms,centroids,center_norms=None): \n    \"\"\"\n    Initialize the centrodis by orthogonal_initialization.\n    Parameters    \n    --------------------\n    X(data): array-like, shape= (m_samples,n_samples)\n    data_norms: array-like, shape=(1,n_samples)\n    center_norms:array-like,shape=(centroids.shape[0])\n    centroids: array-like, shape (K,n_samples)        \n    Returns\n    -------\n    new_center: array-like, shape (1,n_samples)\n    new_center_index: integer   \n                        data index of the new center\n    \"\"\"\n    if center_norms is None:\n        center_norms = np.linalg.norm(centroids, axis=1)\n    cosine = np.inner(X,centroids) # cosine[i, j] = np.dot(X[i, :],centroids[j,:])\n    cosine = cosine/center_norms # divide each column by the center norm\n    cosine = cosine / data_norms[:,np.newaxis] # divide each row by the data norm  \n    max_cosine = np.abs(np.max(cosine, 1)) # the largest (absolute) cosine for each data point \n\n    # then we find the index of the new center:\n    new_center_index = np.argmin(max_cosine) # the data index of the new center is the smallest max cosine\n    new_center = X[new_center_index, :]       \n    return new_center_index,new_center"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_labels(data, centroids,K):\n    distances = np.sqrt(((data - centroids[:, np.newaxis])**2).sum(axis=2))\n    return np.argmin(distances, axis=0)", "response": "Returns a label for each piece of data in the dataset containing the minimum distance from each piece of data in the dataset with the given centroids."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_centroids(data,k,labels,centroids,data_norms):\n\n    D = data.shape[1]    \n    for j in range(k):\n        cluster_points = np.where(labels == j)\n        cluster_total = len(cluster_points)\n        if cluster_total == 0:\n            _, temp = new_orthogonal_center(data,data_norms,centroids)\n        else:\n            temp = np.mean(data[cluster_points,:],axis=1)      \n        centroids[j,:] = temp\n    return centroids", "response": "Function to get the closest centroids for each element in the dataset"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nstopping if centroids stop to update", "response": "def _has_converged(centroids, old_centroids):\n    \"\"\"\n    Stop if centroids stop to update\n    Parameters\n    -----------\n    centroids: array-like, shape=(K, n_samples)     \n    old_centroids: array-like, shape=(K, n_samples)\n    ------------    \n    returns\n    True: bool\n    \n    \"\"\"\n    return (set([tuple(a) for a in centroids]) == set([tuple(a) for a in old_centroids]))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_mod_objs(modname, onlylocals=False):\n\n    __import__(modname)\n    mod = sys.modules[modname]\n\n    if hasattr(mod, '__all__'):\n        pkgitems = [(k, mod.__dict__[k]) for k in mod.__all__]\n    else:\n        pkgitems = [(k, mod.__dict__[k]) for k in dir(mod) if k[0] != '_']\n\n    # filter out modules and pull the names and objs out\n    ismodule = inspect.ismodule\n    localnames = [k for k, v in pkgitems if not ismodule(v)]\n    objs = [v for k, v in pkgitems if not ismodule(v)]\n\n    # fully qualified names can be determined from the object's module\n    fqnames = []\n    for obj, lnm in zip(objs, localnames):\n        if hasattr(obj, '__module__') and hasattr(obj, '__name__'):\n            fqnames.append(obj.__module__ + '.' + obj.__name__)\n        else:\n            fqnames.append(modname + '.' + lnm)\n\n    if onlylocals:\n        valids = [fqn.startswith(modname) for fqn in fqnames]\n        localnames = [e for i, e in enumerate(localnames) if valids[i]]\n        fqnames = [e for i, e in enumerate(fqnames) if valids[i]]\n        objs = [e for i, e in enumerate(objs) if valids[i]]\n\n    return localnames, fqnames, objs", "response": "Returns all the public attributes of a module."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns an RGBA representation of the megaman icon", "response": "def get_megaman_image(factor=1):\n    \"\"\"Return an RGBA representation of the megaman icon\"\"\"\n    imfile = os.path.join(os.path.dirname(__file__), 'megaman.png')\n    data = ndimage.imread(imfile) / 255\n    if factor > 1:\n        data = data.repeat(factor, axis=0).repeat(factor, axis=1)\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef generate_megaman_data(sampling=2):\n    data = get_megaman_image()\n    x = np.arange(sampling * data.shape[1]) / float(sampling)\n    y = np.arange(sampling * data.shape[0]) / float(sampling)\n    X, Y = map(np.ravel, np.meshgrid(x, y))\n    C = data[np.floor(Y.max() - Y).astype(int),\n             np.floor(X).astype(int)]\n    return np.vstack([X, Y]).T, C", "response": "Generate 2D point data of the megaman image"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmakes a 2D S - curve from a 1D vector", "response": "def _make_S_curve(x, range=(-0.75, 0.75)):\n    \"\"\"Make a 2D S-curve from a 1D vector\"\"\"\n    assert x.ndim == 1\n    x = x - x.min()\n    theta = 2 * np.pi * (range[0] + (range[1] - range[0]) * x / x.max())\n    X = np.empty((x.shape[0], 2), dtype=float)\n    X[:, 0] = np.sign(theta) * (1 - np.cos(theta))\n    X[:, 1] = np.sin(theta)\n    X *= x.max() / (2 * np.pi * (range[1] - range[0]))\n    return X"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef generate_megaman_manifold(sampling=2, nfolds=2,\n                              rotate=True, random_state=None):\n    \"\"\"Generate a manifold of the megaman data\"\"\"\n    X, c = generate_megaman_data(sampling)\n    for i in range(nfolds):\n        X = np.hstack([_make_S_curve(x) for x in X.T])\n\n    if rotate:\n        rand = check_random_state(random_state)\n        R = rand.randn(X.shape[1], X.shape[1])\n        U, s, VT = np.linalg.svd(R)\n        X = np.dot(X, U)\n\n    return X, c", "response": "Generate a manifold of the megaman data"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn pair of snapshots", "response": "def snapshots_to_send(source_snaps, dest_snaps):\n    \"\"\"return pair of snapshots\"\"\"\n    if len(source_snaps) == 0:\n        raise AssertionError(\"No snapshots exist locally!\")\n    if len(dest_snaps) == 0:\n        # nothing on the remote side, send everything\n        return None, source_snaps[-1]\n    last_remote = dest_snaps[-1]\n    for snap in reversed(source_snaps):\n        if snap == last_remote:\n            # found a common snapshot\n            return last_remote, source_snaps[-1]\n    # sys.stderr.write(\"source:'{}', dest:'{}'\".format(source_snaps, dest_snaps))\n    raise AssertionError(\"Latest snapshot on destination doesn't exist on source!\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef multipart_etag(digests):\n    etag = hashlib.md5()\n    count = 0\n    for dig in digests:\n        count += 1\n        etag.update(binascii.a2b_hex(dig))\n    return '\"{}-{}\"'.format(etag.hexdigest(), count)", "response": "Computes the etag for multipart uploads."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_chunk(self):\n        while not self._eof_reached:\n            read = self.input_stream.read(self.chunk_size - len(self._partial_chunk))\n            if len(read) == 0:\n                self._eof_reached = True\n            self._partial_chunk += read\n            if len(self._partial_chunk) == self.chunk_size or self._eof_reached:\n                chunk = self._partial_chunk\n                self._partial_chunk = \"\"\n                return chunk", "response": "Return complete chunks or None if EOF reached"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nprocessing one result. Block untill one is available.", "response": "def _handle_result(self):\n        \"\"\"Process one result. Block untill one is available\n        \"\"\"\n        result = self.inbox.get()\n        if result.success:\n            if self._verbosity >= VERB_PROGRESS:\n                sys.stderr.write(\"\\nuploaded chunk {} \\n\".format(result.index))\n            self.results.append((result.index, result.md5))\n            self._pending_chunks -= 1\n        else:\n            raise result.traceback"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _send_chunk(self, index, chunk):\n        self._pending_chunks += 1\n        self.outbox.put((index, chunk))", "response": "Send the current chunk to the workers for processing."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntry to get the key from each dict in turn.", "response": "def _get(self, key, section=None, default=_onion_dict_guard):\n        \"\"\"Try to get the key from each dict in turn.\n        If you specify the optional section it looks there first.\n        \"\"\"\n        if section is not None:\n            section_dict = self.__sections.get(section, {})\n            if key in section_dict:\n                return section_dict[key]\n        for d in self.__dictionaries:\n            if key in d:\n                return d[key]\n        if default is _onion_dict_guard:\n            raise KeyError(key)\n        else:\n            return default"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _parse_snapshots(self):\n        try:\n            snap = self._list_snapshots()\n        except OSError as err:\n            logging.error(\"unable to list local snapshots!\")\n            return {}\n        vols = {}\n        for line in snap.splitlines():\n            if len(line) == 0:\n                continue\n            name, used, refer, mountpoint, written = line.split('\\t')\n            vol_name, snap_name = name.split('@', 1)\n            snapshots = vols.setdefault(vol_name, OrderedDict())\n            snapshots[snap_name] = {\n                'name': name,\n                'used': used,\n                'refer': refer,\n                'mountpoint': mountpoint,\n                'written': written,\n            }\n        return vols", "response": "Returns all snapshots grouped by filesystem a dict of OrderedDicts"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef pipe(self, cmd1, cmd2, quiet=False, estimated_size=None, **kwa):\n        if self.has_pv and not quiet:\n            pv = \"pv\" if estimated_size is None else \"pv --size {}\".format(estimated_size)\n            return self.shell(\"{} | {}| {}\".format(cmd1, pv, cmd2), **kwa)\n        else:\n            return self.shell(\"{} | {}\".format(cmd1, cmd2), **kwa)", "response": "Executes commands on the system."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd the appropriate command to compress the zfs stream", "response": "def _compress(self, cmd):\n        \"\"\"Adds the appropriate command to compress the zfs stream\"\"\"\n        compressor = COMPRESSORS.get(self.compressor)\n        if compressor is None:\n            return cmd\n        compress_cmd = compressor['compress']\n        return \"{} | {}\".format(compress_cmd, cmd)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _decompress(self, cmd, s3_snap):\n        compressor = COMPRESSORS.get(s3_snap.compressor)\n        if compressor is None:\n            return cmd\n        decompress_cmd = compressor['decompress']\n        return \"{} | {}\".format(decompress_cmd, cmd)", "response": "Adds the appropriate command to decompress the zfs stream\n       "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupload named snapshot or latest along with any other snapshots required for an incremental backup.", "response": "def backup_incremental(self, snap_name=None, dry_run=False):\n        \"\"\"Uploads named snapshot or latest, along with any other snapshots\n        required for an incremental backup.\n        \"\"\"\n        z_snap = self._snapshot_to_backup(snap_name)\n        to_upload = []\n        current = z_snap\n        uploaded_meta = []\n        while True:\n            s3_snap = self.s3_manager.get(current.name)\n            if s3_snap is not None:\n                if not s3_snap.is_healthy:\n                    # abort everything if we run in to unhealthy snapshots\n                    raise IntegrityError(\n                        \"Broken snapshot detected {}, reason: '{}'\".format(\n                            s3_snap.name, s3_snap.reason_broken\n                        ))\n                break\n            to_upload.append(current)\n            if current.parent is None:\n                break\n            current = current.parent\n        for z_snap in reversed(to_upload):\n            estimated_size = self._parse_estimated_size(\n                self._cmd.shell(\n                    \"zfs send -nvP -i '{}' '{}'\".format(\n                        z_snap.parent.name, z_snap.name),\n                    capture=True))\n            self._cmd.pipe(\n                \"zfs send -i '{}' '{}'\".format(\n                    z_snap.parent.name, z_snap.name),\n                self._compress(\n                    self._pput_cmd(\n                        estimated=estimated_size,\n                        parent=z_snap.parent.name,\n                        s3_prefix=self.s3_manager.s3_prefix,\n                        snap_name=z_snap.name)\n                ),\n                dry_run=dry_run,\n                estimated_size=estimated_size,\n            )\n            uploaded_meta.append({'snap_name': z_snap.name, 'size': estimated_size})\n        return uploaded_meta"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef extrude(self, uem, reference, collar=0.0, skip_overlap=False):\n\n        if collar == 0. and not skip_overlap:\n            return uem\n\n        collars, overlap_regions = [], []\n\n        # build list of collars if needed\n        if collar > 0.:\n            # iterate over all segments in reference\n            for segment in reference.itersegments():\n\n                # add collar centered on start time\n                t = segment.start\n                collars.append(Segment(t - .5 * collar, t + .5 * collar))\n\n                # add collar centered on end time\n                t = segment.end\n                collars.append(Segment(t - .5 * collar, t + .5 * collar))\n\n        # build list of overlap regions if needed\n        if skip_overlap:\n            # iterate over pair of intersecting segments\n            for (segment1, track1), (segment2, track2) in reference.co_iter(reference):\n                if segment1 == segment2 and track1 == track2:\n                    continue\n                # add their intersection\n                overlap_regions.append(segment1 & segment2)\n\n        segments = collars + overlap_regions\n\n        return Timeline(segments=segments).support().gaps(support=uem)", "response": "Extrude a reference boundary collar from a given uem and a reference annotation."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the common timeline between two annotations.", "response": "def common_timeline(self, reference, hypothesis):\n        \"\"\"Return timeline common to both reference and hypothesis\n\n        reference   |--------|    |------------|     |---------|         |----|\n        hypothesis     |--------------| |------|   |----------------|\n        timeline    |--|-----|----|---|-|------|   |-|---------|----|    |----|\n\n        Parameters\n        ----------\n        reference : Annotation\n        hypothesis : Annotation\n\n        Returns\n        -------\n        timeline : Timeline\n        \"\"\"\n        timeline = reference.get_timeline(copy=True)\n        timeline.update(hypothesis.get_timeline(copy=False))\n        return timeline.segmentation()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef project(self, annotation, timeline):\n        projection = annotation.empty()\n        timeline_ = annotation.get_timeline(copy=False)\n        for segment_, segment in timeline_.co_iter(timeline):\n            for track_ in annotation.get_tracks(segment_):\n                track = projection.new_track(segment, candidate=track_)\n                projection[segment, track] = annotation[segment_, track_]\n        return projection", "response": "Project annotation onto timeline segments\n\n       "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef uemify(self, reference, hypothesis, uem=None, collar=0.,\n               skip_overlap=False, returns_uem=False, returns_timeline=False):\n        \"\"\"Crop 'reference' and 'hypothesis' to 'uem' support\n\n        Parameters\n        ----------\n        reference, hypothesis : Annotation\n            Reference and hypothesis annotations.\n        uem : Timeline, optional\n            Evaluation map.\n        collar : float, optional\n            When provided, set the duration of collars centered around\n            reference segment boundaries that are extruded from both reference\n            and hypothesis. Defaults to 0. (i.e. no collar).\n        skip_overlap : bool, optional\n            Set to True to not evaluate overlap regions.\n            Defaults to False (i.e. keep overlap regions).\n        returns_uem : bool, optional\n            Set to True to return extruded uem as well.\n            Defaults to False (i.e. only return reference and hypothesis)\n        returns_timeline : bool, optional\n            Set to True to oversegment reference and hypothesis so that they\n            share the same internal timeline.\n\n        Returns\n        -------\n        reference, hypothesis : Annotation\n            Extruded reference and hypothesis annotations\n        uem : Timeline\n            Extruded uem (returned only when 'returns_uem' is True)\n        timeline : Timeline:\n            Common timeline (returned only when 'returns_timeline' is True)\n        \"\"\"\n\n        # when uem is not provided, use the union of reference and hypothesis\n        # extents -- and warn the user about that.\n        if uem is None:\n            r_extent = reference.get_timeline().extent()\n            h_extent = hypothesis.get_timeline().extent()\n            extent = r_extent | h_extent\n            uem = Timeline(segments=[extent] if extent else [],\n                           uri=reference.uri)\n            warnings.warn(\n                \"'uem' was approximated by the union of 'reference' \"\n                \"and 'hypothesis' extents.\")\n\n        # extrude collars (and overlap regions) from uem\n        uem = self.extrude(uem, reference, collar=collar,\n                           skip_overlap=skip_overlap)\n\n        # extrude regions outside of uem\n        reference = reference.crop(uem, mode='intersection')\n        hypothesis = hypothesis.crop(uem, mode='intersection')\n\n        # project reference and hypothesis on common timeline\n        if returns_timeline:\n            timeline = self.common_timeline(reference, hypothesis)\n            reference = self.project(reference, timeline)\n            hypothesis = self.project(hypothesis, timeline)\n\n        result = (reference, hypothesis)\n        if returns_uem:\n            result += (uem, )\n\n        if returns_timeline:\n            result += (timeline, )\n\n        return result", "response": "Crop reference and hypothesis to uem."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef det_curve(self, cost_miss=100, cost_fa=1, prior_target=0.01,\n                  return_latency=False):\n        \"\"\"DET curve\n\n        Parameters\n        ----------\n        cost_miss : float, optional\n            Cost of missed detections. Defaults to 100.\n        cost_fa : float, optional\n            Cost of false alarms. Defaults to 1.\n        prior_target : float, optional\n            Target trial prior. Defaults to 0.5.\n        return_latency : bool, optional\n            Set to True to return latency.\n            Has no effect when latencies are given at initialization time.\n\n        Returns\n        -------\n        thresholds : numpy array\n            Detection thresholds\n        fpr : numpy array\n            False alarm rate\n        fnr : numpy array\n            False rejection rate\n        eer : float\n            Equal error rate\n        cdet : numpy array\n            Cdet cost function\n        speaker_latency : numpy array\n        absolute_latency : numpy array\n            Speaker and absolute latency when return_latency is set to True.\n        \"\"\"\n\n        if self.latencies is None:\n\n            y_true = np.array([trial['target'] for _, trial in self])\n            scores = np.array([trial['score'] for _, trial in self])\n            fpr, fnr, thresholds, eer =  det_curve(y_true, scores, distances=False)\n            fpr, fnr, thresholds = fpr[::-1], fnr[::-1], thresholds[::-1]\n            cdet = cost_miss * fnr * prior_target + \\\n                   cost_fa * fpr * (1. - prior_target)\n\n            if return_latency:\n                # needed to align the thresholds used in the DET curve\n                # with (self.)thresholds used to compute latencies.\n                indices = np.searchsorted(thresholds, self.thresholds, side='left')\n\n                thresholds = np.take(thresholds, indices, mode='clip')\n                fpr = np.take(fpr, indices, mode='clip')\n                fnr = np.take(fnr, indices, mode='clip')\n                cdet = np.take(cdet, indices, mode='clip')\n                return thresholds, fpr, fnr, eer, cdet, \\\n                       self.speaker_latency, self.absolute_latency\n\n            else:\n                return thresholds, fpr, fnr, eer, cdet\n\n        else:\n\n            y_true = np.array([trial['target'] for _, trial in self])\n            spk_scores = np.array([trial['spk_score'] for _, trial in self])\n            abs_scores = np.array([trial['abs_score'] for _, trial in self])\n\n            result = {}\n            for key, scores in {'speaker': spk_scores,\n                                'absolute': abs_scores}.items():\n\n                result[key] = {}\n\n                for i, latency in enumerate(self.latencies):\n                    fpr, fnr, theta, eer =  det_curve(y_true, scores[:, i],\n                                                      distances=False)\n                    fpr, fnr, theta = fpr[::-1], fnr[::-1], theta[::-1]\n                    cdet = cost_miss * fnr * prior_target + \\\n                           cost_fa * fpr * (1. - prior_target)\n                    result[key][latency] = theta, fpr, fnr, eer, cdet\n\n            return result", "response": "This function calculates the DET curve for the given cost miss and fa."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the hypothesis for given file.", "response": "def get_hypothesis(hypotheses, current_file):\n    \"\"\"Get hypothesis for given file\n\n    Parameters\n    ----------\n    hypotheses : `dict`\n        Speaker diarization hypothesis provided by `load_rttm`.\n    current_file : `dict`\n        File description as given by pyannote.database protocols.\n\n    Returns\n    -------\n    hypothesis : `pyannote.core.Annotation`\n        Hypothesis corresponding to `current_file`.\n    \"\"\"\n\n    uri = current_file['uri']\n\n    if uri in hypotheses:\n        return hypotheses[uri]\n\n    # if the exact 'uri' is not available in hypothesis,\n    # look for matching substring\n    tmp_uri = [u for u in hypotheses if u in uri]\n\n    # no matching speech turns. return empty annotation\n    if len(tmp_uri) == 0:\n        msg = f'Could not find hypothesis for file \"{uri}\"; assuming empty file.'\n        warnings.warn(msg)\n        return Annotation(uri=uri, modality='speaker')\n\n    # exactly one matching file. return it\n    if len(tmp_uri) == 1:\n        hypothesis = hypotheses[tmp_uri[0]]\n        hypothesis.uri = uri\n        return hypothesis\n\n    # more that one matching file. error.\n    msg = f'Found too many hypotheses matching file \"{uri}\" ({uris}).'\n    raise ValueError(msg.format(uri=uri, uris=tmp_uri))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nscoring distributions of a single base class.", "response": "def plot_distributions(y_true, scores, save_to, xlim=None, nbins=100, ymax=3., dpi=150):\n    \"\"\"Scores distributions\n\n    This function will create (and overwrite) the following files:\n        - {save_to}.scores.png\n        - {save_to}.scores.eps\n\n    Parameters\n    ----------\n    y_true : (n_samples, ) array-like\n        Boolean reference.\n    scores : (n_samples, ) array-like\n        Predicted score.\n    save_to : str\n        Files path prefix\n    \"\"\"\n\n    plt.figure(figsize=(12, 12))\n\n    if xlim is None:\n        xlim = (np.min(scores), np.max(scores))\n\n    bins = np.linspace(xlim[0], xlim[1], nbins)\n    plt.hist(scores[y_true], bins=bins, color='g', alpha=0.5, normed=True)\n    plt.hist(scores[~y_true], bins=bins, color='r', alpha=0.5, normed=True)\n\n    # TODO heuristic to estimate ymax from nbins and xlim\n    plt.ylim(0, ymax)\n    plt.tight_layout()\n    plt.savefig(save_to + '.scores.png', dpi=dpi)\n    plt.savefig(save_to + '.scores.eps')\n    plt.close()\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef plot_det_curve(y_true, scores, save_to,\n                   distances=False, dpi=150):\n    \"\"\"DET curve\n\n    This function will create (and overwrite) the following files:\n        - {save_to}.det.png\n        - {save_to}.det.eps\n        - {save_to}.det.txt\n\n    Parameters\n    ----------\n    y_true : (n_samples, ) array-like\n        Boolean reference.\n    scores : (n_samples, ) array-like\n        Predicted score.\n    save_to : str\n        Files path prefix.\n    distances : boolean, optional\n        When True, indicate that `scores` are actually `distances`\n    dpi : int, optional\n        Resolution of .png file. Defaults to 150.\n\n    Returns\n    -------\n    eer : float\n        Equal error rate\n    \"\"\"\n\n    fpr, fnr, thresholds, eer = det_curve(y_true, scores, distances=distances)\n\n    # plot DET curve\n    plt.figure(figsize=(12, 12))\n    plt.loglog(fpr, fnr, 'b')\n    plt.loglog([eer], [eer], 'bo')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('False Negative Rate')\n    plt.xlim(1e-2, 1.)\n    plt.ylim(1e-2, 1.)\n    plt.grid(True)\n    plt.tight_layout()\n    plt.savefig(save_to + '.det.png', dpi=dpi)\n    plt.savefig(save_to + '.det.eps')\n    plt.close()\n\n    # save DET curve in text file\n    txt = save_to + '.det.txt'\n    line = '{t:.6f} {fp:.6f} {fn:.6f}\\n'\n    with open(txt, 'w') as f:\n        for i, (t, fp, fn) in enumerate(zip(thresholds, fpr, fnr)):\n            f.write(line.format(t=t, fp=fp, fn=fn))\n\n    return eer", "response": "This function plots the DET curve of the current language."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef plot_precision_recall_curve(y_true, scores, save_to,\n                                distances=False, dpi=150):\n    \"\"\"Precision/recall curve\n\n    This function will create (and overwrite) the following files:\n        - {save_to}.precision_recall.png\n        - {save_to}.precision_recall.eps\n        - {save_to}.precision_recall.txt\n\n    Parameters\n    ----------\n    y_true : (n_samples, ) array-like\n        Boolean reference.\n    scores : (n_samples, ) array-like\n        Predicted score.\n    save_to : str\n        Files path prefix.\n    distances : boolean, optional\n        When True, indicate that `scores` are actually `distances`\n    dpi : int, optional\n        Resolution of .png file. Defaults to 150.\n\n    Returns\n    -------\n    auc : float\n        Area under precision/recall curve\n    \"\"\"\n\n\n    precision, recall, thresholds, auc = precision_recall_curve(\n        y_true, scores, distances=distances)\n\n    # plot P/R curve\n    plt.figure(figsize=(12, 12))\n    plt.plot(recall, precision, 'b')\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.xlim(0, 1)\n    plt.ylim(0, 1)\n    plt.tight_layout()\n    plt.savefig(save_to + '.precision_recall.png', dpi=dpi)\n    plt.savefig(save_to + '.precision_recall.eps')\n    plt.close()\n\n    # save P/R curve in text file\n    txt = save_to + '.precision_recall.txt'\n    line = '{t:.6f} {p:.6f} {r:.6f}\\n'\n    with open(txt, 'w') as f:\n        for i, (t, p, r) in enumerate(zip(thresholds, precision, recall)):\n            f.write(line.format(t=t, p=p, r=r))\n\n    return auc", "response": "This function plots the precision and recall curve of the current node."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fit(self, scores, y_true):\n\n        # to force equal priors, randomly select (and average over)\n        # up to fifty balanced (i.e. #true == #false) calibration sets.\n        if self.equal_priors:\n\n            counter = Counter(y_true)\n            positive, negative = counter[True], counter[False]\n\n            if positive > negative:\n                majority, minority = True, False\n                n_majority, n_minority = positive, negative\n            else:\n                majority, minority = False, True\n                n_majority, n_minority = negative, positive\n\n            n_splits = min(50, n_majority // n_minority + 1)\n\n            minority_index = np.where(y_true == minority)[0]\n            majority_index = np.where(y_true == majority)[0]\n\n            cv = []\n            for _ in range(n_splits):\n                test_index = np.hstack([\n                    np.random.choice(majority_index,\n                                     size=n_minority,\n                                     replace=False),\n                    minority_index])\n                cv.append(([], test_index))\n            cv = _CVIterableWrapper(cv)\n\n        # to estimate priors from the data itself, use the whole set\n        else:\n            cv = 'prefit'\n\n        self.calibration_ = CalibratedClassifierCV(\n            base_estimator=_Passthrough(), method=self.method, cv=cv)\n        self.calibration_.fit(scores.reshape(-1, 1), y_true)\n\n        return self", "response": "Train calibration set for the given scores and y_true."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget error analysis as Annotation.", "response": "def difference(self, reference, hypothesis, uem=None, uemified=False):\n        \"\"\"Get error analysis as `Annotation`\n\n        Labels are (status, reference_label, hypothesis_label) tuples.\n        `status` is either 'correct', 'confusion', 'missed detection' or\n        'false alarm'.\n        `reference_label` is None in case of 'false alarm'.\n        `hypothesis_label` is None in case of 'missed detection'.\n\n        Parameters\n        ----------\n        uemified : bool, optional\n            Returns \"uemified\" version of reference and hypothesis.\n            Defaults to False.\n\n        Returns\n        -------\n        errors : `Annotation`\n\n        \"\"\"\n\n        R, H, common_timeline = self.uemify(\n            reference, hypothesis, uem=uem,\n            collar=self.collar, skip_overlap=self.skip_overlap,\n            returns_timeline=True)\n\n        errors = Annotation(uri=reference.uri, modality=reference.modality)\n\n        # loop on all segments\n        for segment in common_timeline:\n\n            # list of labels in reference segment\n            rlabels = R.get_labels(segment, unique=False)\n\n            # list of labels in hypothesis segment\n            hlabels = H.get_labels(segment, unique=False)\n\n            _, details = self.matcher(rlabels, hlabels)\n\n            for r, h in details[MATCH_CORRECT]:\n                track = errors.new_track(segment, prefix=MATCH_CORRECT)\n                errors[segment, track] = (MATCH_CORRECT, r, h)\n\n            for r, h in details[MATCH_CONFUSION]:\n                track = errors.new_track(segment, prefix=MATCH_CONFUSION)\n                errors[segment, track] = (MATCH_CONFUSION, r, h)\n\n            for r in details[MATCH_MISSED_DETECTION]:\n                track = errors.new_track(segment,\n                                         prefix=MATCH_MISSED_DETECTION)\n                errors[segment, track] = (MATCH_MISSED_DETECTION, r, None)\n\n            for h in details[MATCH_FALSE_ALARM]:\n                track = errors.new_track(segment, prefix=MATCH_FALSE_ALARM)\n                errors[segment, track] = (MATCH_FALSE_ALARM, None, h)\n\n        if uemified:\n            return reference, hypothesis, errors\n        else:\n            return errors"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nresetting accumulated components and metric values", "response": "def reset(self):\n        \"\"\"Reset accumulated components and metric values\"\"\"\n        if self.parallel:\n            from pyannote.metrics import manager_\n            self.accumulated_ = manager_.dict()\n            self.results_ = manager_.list()\n            self.uris_ = manager_.dict()\n        else:\n            self.accumulated_ = dict()\n            self.results_ = list()\n            self.uris_ = dict()\n        for value in self.components_:\n            self.accumulated_[value] = 0."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef report(self, display=False):\n\n        report = []\n        uris = []\n\n        percent = 'total' in self.metric_components()\n\n        for uri, components in self.results_:\n            row = {}\n            if percent:\n                total = components['total']\n            for key, value in components.items():\n                if key == self.name:\n                    row[key, '%'] = 100 * value\n                elif key == 'total':\n                    row[key, ''] = value\n                else:\n                    row[key, ''] = value\n                    if percent:\n                        if total > 0:\n                            row[key, '%'] = 100 * value / total\n                        else:\n                            row[key, '%'] = np.NaN\n\n            report.append(row)\n            uris.append(uri)\n\n        row = {}\n        components = self.accumulated_\n\n        if percent:\n            total = components['total']\n\n        for key, value in components.items():\n            if key == self.name:\n                row[key, '%'] = 100 * value\n            elif key == 'total':\n                row[key, ''] = value\n            else:\n                row[key, ''] = value\n                if percent:\n                    if total > 0:\n                        row[key, '%'] = 100 * value / total\n                    else:\n                        row[key, '%'] = np.NaN\n\n        row[self.name, '%'] = 100 * abs(self)\n        report.append(row)\n        uris.append('TOTAL')\n\n        df = pd.DataFrame(report)\n\n        df['item'] = uris\n        df = df.set_index('item')\n\n        df.columns = pd.MultiIndex.from_tuples(df.columns)\n\n        df = df[[self.name] + self.metric_components()]\n\n        if display:\n            print(df.to_string(index=True, sparsify=False, justify='right', float_format=lambda f: '{0:.2f}'.format(f)))\n\n        return df", "response": "Evaluate the current object and return a pandas. DataFrame with one column per metric component one row per row per row for accumulated results."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes the confidence interval of accumulated metric values.", "response": "def confidence_interval(self, alpha=0.9):\n        \"\"\"Compute confidence interval on accumulated metric values\n\n        Parameters\n        ----------\n        alpha : float, optional\n            Probability that the returned confidence interval contains\n            the true metric value.\n\n        Returns\n        -------\n        (center, (lower, upper))\n            with center the mean of the conditional pdf of the metric value\n            and (lower, upper) is a confidence interval centered on the median,\n            containing the estimate to a probability alpha.\n\n        See Also:\n        ---------\n        scipy.stats.bayes_mvs\n\n        \"\"\"\n        m, _, _ = scipy.stats.bayes_mvs(\n            [r[self.metric_name_] for _, r in self.results_], alpha=alpha)\n        return m"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef compute_metric(self, components):\n        numerator = components[PRECISION_RELEVANT_RETRIEVED]\n        denominator = components[PRECISION_RETRIEVED]\n        if denominator == 0.:\n            if numerator == 0:\n                return 1.\n            else:\n                raise ValueError('')\n        else:\n            return numerator/denominator", "response": "Compute precision from components"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef compute_metric(self, components):\n        numerator = components[RECALL_RELEVANT_RETRIEVED]\n        denominator = components[RECALL_RELEVANT]\n        if denominator == 0.:\n            if numerator == 0:\n                return 1.\n            else:\n                raise ValueError('')\n        else:\n            return numerator/denominator", "response": "Compute recall from components"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef optimal_mapping(self, reference, hypothesis, uem=None):\n\n        # NOTE that this 'uemification' will not be called when\n        # 'optimal_mapping' is called from 'compute_components' as it\n        # has already been done in 'compute_components'\n        if uem:\n            reference, hypothesis = self.uemify(reference, hypothesis, uem=uem)\n\n        # call hungarian mapper\n        mapping = self.mapper_(hypothesis, reference)\n        return mapping", "response": "Optimal label mapping between two annotation objects."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninitializing a dictionary of well - mixed radiatively active gases with default values.", "response": "def default_absorbers(Tatm,\n                ozone_file = 'apeozone_cam3_5_54.nc',\n                verbose = True,):\n    '''Initialize a dictionary of well-mixed radiatively active gases\n    All values are volumetric mixing ratios.\n\n    Ozone is set to a climatology.\n\n    All other gases are assumed well-mixed:\n\n        - CO2\n        - CH4\n        - N2O\n        - O2\n        - CFC11\n        - CFC12\n        - CFC22\n        - CCL4\n\n    Specific values are based on the AquaPlanet Experiment protocols,\n    except for O2 which is set the realistic value 0.21\n    (affects the RRTMG scheme).\n    '''\n    absorber_vmr = {}\n    absorber_vmr['CO2']   = 348. / 1E6\n    absorber_vmr['CH4']   = 1650. / 1E9\n    absorber_vmr['N2O']   = 306. / 1E9\n    absorber_vmr['O2']    = 0.21\n    absorber_vmr['CFC11'] = 0.\n    absorber_vmr['CFC12'] = 0.\n    absorber_vmr['CFC22'] = 0.\n    absorber_vmr['CCL4']  = 0.\n\n    # Ozone: start with all zeros, interpolate to data if we can\n    xTatm = Tatm.to_xarray()\n    O3 = 0. * xTatm\n    if ozone_file is not None:\n        ozonefilepath = os.path.join(os.path.dirname(__file__), 'data', 'ozone', ozone_file)\n        remotepath_http = 'http://thredds.atmos.albany.edu:8080/thredds/fileServer/CLIMLAB/ozone/' + ozone_file\n        remotepath_opendap = 'http://thredds.atmos.albany.edu:8080/thredds/dodsC/CLIMLAB/ozone/' + ozone_file\n        ozonedata, path = load_data_source(local_path=ozonefilepath,\n                                     remote_source_list=[remotepath_http, remotepath_opendap],\n                                     open_method=xr.open_dataset,\n                                     remote_kwargs={'engine':'pydap'},\n                                     verbose=verbose,)\n        ##  zonal and time average\n        ozone_zon = ozonedata.OZONE.mean(dim=('time','lon')).transpose('lat','lev')\n        if ('lat' in xTatm.dims):\n            O3source = ozone_zon\n        else:\n            weight = np.cos(np.deg2rad(ozonedata.lat))\n            ozone_global = (ozone_zon * weight).mean(dim='lat') / weight.mean(dim='lat')\n            O3source = ozone_global\n        try:\n            O3 = O3source.interp_like(xTatm)\n            # There will be NaNs for gridpoints outside the ozone file domain\n            assert not np.any(np.isnan(O3))\n        except:\n            warnings.warn('Some grid points are beyond the bounds of the ozone file. Ozone values will be extrapolated.')\n            try:\n                # passing fill_value=None to the underlying scipy interpolator\n                # will result in extrapolation instead of NaNs\n                O3 = O3source.interp_like(xTatm, kwargs={'fill_value':None})\n                assert not np.any(np.isnan(O3))\n            except:\n                warnings.warn('Interpolation of ozone data failed. Setting O3 to zero instead.')\n                O3 = 0. * xTatm\n    absorber_vmr['O3'] = O3.values\n    return absorber_vmr"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a Field object defined at the vertical interfaces of the input Field object.", "response": "def init_interface(field):\n    '''Return a Field object defined at the vertical interfaces of the input Field object.'''\n    interface_shape = np.array(field.shape); interface_shape[-1] += 1\n    interfaces = np.tile(False,len(interface_shape)); interfaces[-1] = True\n    interface_zero = Field(np.zeros(interface_shape), domain=field.domain, interfaces=interfaces)\n    return interface_zero"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef do_diagnostics(self):\n        '''Set all the diagnostics from long and shortwave radiation.'''\n        self.OLR = self.subprocess['LW'].flux_to_space\n        self.LW_down_sfc = self.subprocess['LW'].flux_to_sfc\n        self.LW_up_sfc = self.subprocess['LW'].flux_from_sfc\n        self.LW_absorbed_sfc = self.LW_down_sfc - self.LW_up_sfc\n        self.LW_absorbed_atm = self.subprocess['LW'].absorbed\n        self.LW_emission = self.subprocess['LW'].emission\n            #  contributions to OLR from surface and atm. levels\n            #self.diagnostics['OLR_sfc'] = self.flux['sfc2space']\n            #self.diagnostics['OLR_atm'] = self.flux['atm2space']\n        self.ASR = (self.subprocess['SW'].flux_from_space -\n                    self.subprocess['SW'].flux_to_space)\n        #self.SW_absorbed_sfc = (self.subprocess['surface'].SW_from_atm -\n        #                            self.subprocess['surface'].SW_to_atm)\n        self.SW_absorbed_atm = self.subprocess['SW'].absorbed\n        self.SW_down_sfc = self.subprocess['SW'].flux_to_sfc\n        self.SW_up_sfc = self.subprocess['SW'].flux_from_sfc\n        self.SW_absorbed_sfc = self.SW_down_sfc - self.SW_up_sfc\n        self.SW_up_TOA = self.subprocess['SW'].flux_to_space\n        self.SW_down_TOA = self.subprocess['SW'].flux_from_space\n        self.planetary_albedo = (self.subprocess['SW'].flux_to_space /\n                                 self.subprocess['SW'].flux_from_space)", "response": "Set all the diagnostics from long and shortwave radiation."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing saturation vapor pressure as function of temperature T", "response": "def clausius_clapeyron(T):\n    \"\"\"Compute saturation vapor pressure as function of temperature T.\n\n    Input: T is temperature in Kelvin\n    Output: saturation vapor pressure in mb or hPa\n\n    Formula from Rogers and Yau \"A Short Course in Cloud Physics\" (Pergammon Press), p. 16\n    claimed to be accurate to within 0.1% between -30degC and 35 degC\n    Based on the paper by Bolton (1980, Monthly Weather Review).\n\n    \"\"\"\n    Tcel = T - tempCtoK\n    es = 6.112 * exp(17.67*Tcel/(Tcel+243.5))\n    return es"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing saturation specific humidity as function of temperature and pressure.", "response": "def qsat(T,p):\n    \"\"\"Compute saturation specific humidity as function of temperature and pressure.\n\n    Input:  T is temperature in Kelvin\n            p is pressure in hPa or mb\n    Output: saturation specific humidity (dimensionless).\n\n    \"\"\"\n    es = clausius_clapeyron(T)\n    q = eps * es / (p - (1 - eps) * es )\n    return q"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef rho_moist(T,p,q):\n    '''Density of moist air.\n    T is air temperature (K)\n    p is air pressure (hPa)\n    q is specific humidity (hPa)\n\n    returns density in kg/m3\n    '''\n    e = vapor_pressure_from_specific_humidity(p,q)\n    w = mixing_ratio_from_vapor_pressure(p,e)\n    Tv = virtual_temperature_from_mixing_ratio(T,w)\n    return p*100./ Rd/Tv", "response": "Density of moist air."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute the local slope of the pseudoadiabat at given temperature and pressure.", "response": "def pseudoadiabat(T,p):\n    \"\"\"Compute the local slope of the pseudoadiabat at given temperature and pressure\n\n    Inputs:   p is pressure in hPa or mb\n              T is local temperature in Kelvin\n    Output:   dT/dp, the rate of temperature change for pseudoadiabatic ascent\n                in units of K / hPa\n\n    The pseudoadiabat describes changes in temperature and pressure for an air\n    parcel at saturation assuming instantaneous rain-out of the super-saturated water\n\n    Formula consistent with eq. (2.33) from Raymond Pierrehumbert, \"Principles of Planetary Climate\"\n    which nominally accounts for non-dilute effects, but computes the derivative\n    dT/dpa, where pa is the partial pressure of the non-condensible gas.\n\n    Integrating the result dT/dp treating p as total pressure effectively makes the dilute assumption.\n    \"\"\"\n    esoverp = clausius_clapeyron(T) / p\n    Tcel = T - tempCtoK\n    L = (2.501 - 0.00237 * Tcel) * 1.E6   # Accurate form of latent heat of vaporization in J/kg\n    ratio = L / T /  Rv\n    dTdp = (T / p * kappa * (1 + esoverp * ratio) /\n        (1 + kappa * (cpv / Rv + (ratio-1) * ratio) * esoverp))\n    return dTdp"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute the Lifiting Condensation Level for a given temperature and relative humidity.", "response": "def lifting_condensation_level(T, RH):\n    '''Compute the Lifiting Condensation Level (LCL) for a given temperature and relative humidity\n\n    Inputs:  T is temperature in Kelvin\n            RH is relative humidity (dimensionless)\n\n    Output: LCL in meters\n\n    This is height (relative to parcel height) at which the parcel would become saturated during adiabatic ascent.\n\n    Based on approximate formula from Bolton (1980 MWR) as given by Romps (2017 JAS)\n\n    For an exact formula see Romps (2017 JAS), doi:10.1175/JAS-D-17-0102.1\n    '''\n    Tadj = T-55.  # in Kelvin\n    return cp/g*(Tadj - (1/Tadj - log(RH)/2840.)**(-1))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing the Estimated Inversion Strength or EIS in Kelvin following Wood and Bretherton 2006 and J. Climate 2006.", "response": "def estimated_inversion_strength(T0,T700):\n    '''Compute the Estimated Inversion Strength or EIS,\n    following Wood and Bretherton (2006, J. Climate)\n\n    Inputs: T0 is surface temp in Kelvin\n           T700 is air temperature at 700 hPa in Kelvin\n\n    Output: EIS in Kelvin\n\n    EIS is a normalized measure of lower tropospheric stability acccounting for\n    temperature-dependence of the moist adiabat.\n    '''\n    # Interpolate to 850 hPa\n    T850 = (T0+T700)/2.;\n    # Assume 80% relative humidity to compute LCL, appropriate for marine boundary layer\n    LCL = lifting_condensation_level(T0, 0.8)\n    # Lower Tropospheric Stability (theta700 - theta0)\n    LTS = potential_temperature(T700, 700) - T0\n    #  Gammam  = -dtheta/dz is the rate of potential temperature decrease along the moist adiabat\n    #  in K / m\n    Gammam = (g/cp*(1.0 - (1.0 + Lhvap*qsat(T850,850) / Rd / T850) /\n             (1.0 + Lhvap**2 * qsat(T850,850)/ cp/Rv/T850**2)))\n    #  Assume exponential decrease of pressure with scale height given by surface temperature\n    z700 = (Rd*T0/g)*log(1000./700.)\n    return LTS - Gammam*(z700 - LCL)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef Planck_frequency(nu, T):\n    '''The Planck function B(nu,T):\n    the flux density for blackbody radiation in frequency space\n    nu is frequency in 1/s\n    T is temperature in Kelvin\n\n    Formula (3.1) from Raymond Pierrehumbert, \"Principles of Planetary Climate\"\n    '''\n    return 2*hPlanck*nu**3/c_light**2/(exp(hPlanck*nu/kBoltzmann/T)-1)", "response": "The Planck function B"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef Planck_wavenumber(n, T):\n    '''The Planck function (flux density for blackbody radition)\n    in wavenumber space\n    n is wavenumber in 1/cm\n    T is temperature in Kelvin\n\n    Formula from Raymond Pierrehumbert, \"Principles of Planetary Climate\", page 140.\n    '''\n    # convert to mks units\n    n = n*100.\n    return c_light * Planck_frequency(n*c_light, T)", "response": "The Planck function for a given wavenumber."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef Planck_wavelength(l, T):\n    '''The Planck function (flux density for blackbody radiation)\n    in wavelength space\n    l is wavelength in meters\n    T is temperature in Kelvin\n\n    Formula (3.3) from Raymond Pierrehumbert, \"Principles of Planetary Climate\"\n    '''\n    u = hPlanck*c_light/l/kBoltzmann/T\n    return 2*kBoltzmann**5*T**5/hPlanck**4/c_light**3*u**5/(exp(u)-1)", "response": "The Planck function (flux density for blackbody radiation)\n    in wavelength space\n    l is wavelength in meters\n    T is temperature in Kelvin\n\n    Formula (3.3) from Raymond Pierrehumbert, \"Principles of Planetary Climate\""}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing energy flux convergences to get heating rates in unit .", "response": "def _compute_heating_rates(self):\n        \"\"\"Computes energy flux convergences to get heating rates in unit\n        :math:`\\\\textrm{W}/ \\\\textrm{m}^2`.\n\n        This method should be over-ridden by daughter classes.\n\n        \"\"\"\n        for varname in list(self.state.keys()):\n            self.heating_rate[varname] = self.state[varname] * 0."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _make_diffusion_matrix(K, weight1=None, weight2=None):\n\n#           \\\\begin{eqnarray}\n#              y    & = & ax^2 + bx + c \\\\\\\\\n#              f(x) & = & x^2 + 2xy + y^2\n#           \\\\end{eqnarray}\n\n#    .. math::\n#\n#        K   &= [K_0,        &K_1,        &K_2,        &...    ,    &K_{n-1},      &K_{n}] \\\\\\\\\n#        w_1 &= [w_{1,0},    &w_{1,1},    &w_{1,2},    &...    ,    &w_{1,n-1}, \\\\ &w_{1,n}] \\\\\\\\\n#        w_2 &= [w_{2,0}, \\\\ &w_{2,1}, \\\\ &w_{2,2}, \\\\ &... \\\\ , \\\\ &w_{2,n-1}]    &o \\\\\\\\\n#\n#    \"\"\"\n    J = K.size - 1\n    if weight1 is None:\n        weight1 = np.ones_like(K)\n    if weight2 is None:\n        weight2 = np.ones(J)\n    weightedK = weight1 * K\n    Ka1 = weightedK[0:J] / weight2\n    Ka3 = weightedK[1:J+1] / weight2\n    Ka2 = np.insert(Ka1[1:J], 0, 0) + np.append(Ka3[0:J-1], 0)\n    #  Atmosphere tridiagonal matrix\n    #  this code makes a 3xN matrix, suitable for use with solve_banded\n    #diag = np.empty((3, J))\n    #diag[0, 1:] = -Ka3[0:J-1]\n    #diag[1, :] = 1 + Ka2\n    #diag[2, 0:J-1] = -Ka1[1:J]\n    #  Build the full banded matrix instead\n    A = (np.diag(1 + Ka2, k=0) +\n         np.diag(-Ka3[0:J-1], k=1) +\n         np.diag(-Ka1[1:J], k=-1))\n    return A", "response": "Builds the general diffusion matrix for the given array of diffusion vectors."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _make_meridional_diffusion_matrix(K, lataxis):\n    phi_stag = np.deg2rad(lataxis.bounds)\n    phi = np.deg2rad(lataxis.points)\n    weight1 = np.cos(phi_stag)\n    weight2 = np.cos(phi)\n    diag = _make_diffusion_matrix(K, weight1, weight2)\n    return diag, weight1, weight2", "response": "Returns a matrix that is a matrix of the meridional diffusion case."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nusing a banded solver for matrix inversion of a tridiagonal matrix.", "response": "def _solve_implicit_banded(current, banded_matrix):\n    \"\"\"Uses a banded solver for matrix inversion of a tridiagonal matrix.\n\n    Converts the complete listed tridiagonal matrix *(nxn)* into a three row\n    matrix *(3xn)* and calls :py:func:`scipy.linalg.solve_banded()`.\n\n    :param array current:           the current state of the variable for which\n                                    matrix inversion should be computed\n    :param array banded_matrix:     complete diffusion matrix (*dimension: nxn*)\n    :returns:                       output of :py:func:`scipy.linalg.solve_banded()`\n    :rtype:                         array\n\n    \"\"\"\n    #  can improve performance by storing the banded form once and not\n    #  recalculating it...\n    #  but whatever\n    J = banded_matrix.shape[0]\n    diag = np.zeros((3, J))\n    diag[1, :] = np.diag(banded_matrix, k=0)\n    diag[0, 1:] = np.diag(banded_matrix, k=1)\n    diag[2, :-1] = np.diag(banded_matrix, k=-1)\n    return solve_banded((1, 1), diag, current)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nscans given process or domain or dictionary of domains for a diffusion axis and returns appropriate name.", "response": "def _guess_diffusion_axis(process_or_domain):\n    \"\"\"Scans given process, domain or dictionary of domains for a diffusion axis\n    and returns appropriate name.\n\n    In case only one axis with length > 1 in the process or set of domains\n    exists, the name of that axis is returned. Otherwise an error is raised.\n\n    :param process_or_domain:   input from where diffusion axis should be guessed\n    :type process_or_domain:    :class:`~climlab.process.process.Process`,\n                                :class:`~climlab.domain.domain._Domain` or\n                                :py:class:`dict` of domains\n    :raises: :exc:`ValueError` if more than one diffusion axis is possible.\n    :returns:                   name of the diffusion axis\n    :rtype:                     str\n\n    \"\"\"\n    axes = get_axes(process_or_domain)\n    diff_ax = {}\n    for axname, ax in axes.items():\n        if ax.num_points > 1:\n            diff_ax.update({axname: ax})\n    if len(list(diff_ax.keys())) == 1:\n        return list(diff_ax.keys())[0]\n    else:\n        raise ValueError('More than one possible diffusion axis.')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _implicit_solver(self):\n        #if self.update_diffusivity:\n\n        # Time-stepping the diffusion is just inverting this matrix problem:\n        newstate = {}\n        for varname, value in self.state.items():\n            if self.use_banded_solver:\n                newvar = _solve_implicit_banded(value, self._diffTriDiag)\n            else:\n                newvar = np.linalg.solve(self._diffTriDiag, value)\n            newstate[varname] = newvar\n        return newstate", "response": "Invertes and solves the matrix problem for diffusion matrix A and temperature T."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _compute_heating_rates(self):\n        '''Compute energy flux convergences to get heating rates in :math:`W/m^2`.'''\n        self.LW_to_atm = self._compute_emission()\n        self.SW_to_atm = self._compute_reflected_flux()\n        self.heating_rate['Ts'] = ( self.LW_from_atm - self.LW_to_atm\n                                  + self.SW_from_atm - self.SW_to_atm )", "response": "Compute energy flux convergences to get heating rates in : math : W / m^2."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfind iceline according to the surface temperature. This method is called by the private function :func:`~climlab.surface.albedo.Iceline._compute` and updates following attributes according to the freezing temperature ``self.param['Tf']`` and the surface temperature ``self.param['Ts']``: **Object attributes** \\n :ivar Field noice: a Field of booleans which are ``True`` where :math:`T_s \\\\ge T_f` :ivar Field ice: a Field of booleans which are ``True`` where :math:`T_s < T_f` :ivar array icelat: an array with two elements indicating the ice-edge latitudes :ivar float ice_area: fractional area covered by ice (0 - 1) :ivar dict diagnostics: keys ``'icelat'`` and ``'ice_area'`` are updated", "response": "def find_icelines(self):\n        \"\"\"Finds iceline according to the surface temperature.\n\n        This method is called by the private function\n        :func:`~climlab.surface.albedo.Iceline._compute`\n        and updates following attributes according to the freezing temperature\n        ``self.param['Tf']`` and the surface temperature ``self.param['Ts']``:\n\n        **Object attributes** \\n\n\n        :ivar Field noice:      a Field of booleans which are ``True`` where\n                                :math:`T_s \\\\ge T_f`\n        :ivar Field ice:        a Field of booleans which are ``True`` where\n                                :math:`T_s < T_f`\n        :ivar array icelat:     an array with two elements indicating the\n                                ice-edge latitudes\n        :ivar float ice_area:   fractional area covered by ice (0 - 1)\n        :ivar dict diagnostics: keys ``'icelat'`` and ``'ice_area'`` are updated\n\n        \"\"\"\n        Tf = self.param['Tf']\n        Ts = self.state['Ts']\n        lat_bounds = self.domains['Ts'].axes['lat'].bounds\n        self.noice = np.where(Ts >= Tf, True, False)\n        self.ice = np.where(Ts < Tf, True, False)\n        #  Ice cover in fractional area\n        self.ice_area = global_mean(self.ice * np.ones_like(self.Ts))\n        #  Express ice cover in terms of ice edge latitudes\n        if self.ice.all():\n            # 100% ice cover\n            self.icelat = np.array([-0., 0.])\n        elif self.noice.all():\n            # zero ice cover\n            self.icelat = np.array([-90., 90.])\n        else:  # there is some ice edge\n            # Taking np.diff of a boolean array gives True at the boundaries between True and False\n            boundary_indices = np.where(np.diff(self.ice.squeeze()))[0]+1\n            # check for asymmetry case: [-90,x] or [x,90]\n            #  -> boundary_indices hold only one value for icelat\n            if boundary_indices.size == 1:\n                if self.ice[0] == True:   # case: [x,90]\n                    # extend indice array by missing value for northpole\n                    boundary_indices = np.append(boundary_indices, self.ice.size)\n                elif  self.ice[-1] == True:   # case: [-90,x]\n                    # extend indice array by missing value for northpole\n                    boundary_indices = np.insert(boundary_indices,0 ,0)\n            # check for asymmetry case: [-90,x] or [x,90]\n            #  -> boundary_indices hold only one value for icelat\n            if boundary_indices.size == 1:\n                if self.ice[0] == True:   # case: [x,90]\n                    # extend indice array by missing value for northpole\n                    boundary_indices = np.append(boundary_indices, self.ice.size)\n                elif  self.ice[-1] == True:   # case: [-90,x]\n                    # extend indice array by missing value for northpole\n                    boundary_indices = np.insert(boundary_indices,0 ,0)\n            self.icelat = lat_bounds[boundary_indices]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_current_albedo(self):\n        '''Simple step-function albedo based on ice line at temperature Tf.'''\n        ice = self.subprocess['iceline'].ice\n        # noice = self.subprocess['iceline'].diagnostics['noice']\n        cold_albedo = self.subprocess['cold_albedo'].albedo\n        warm_albedo = self.subprocess['warm_albedo'].albedo\n        albedo = Field(np.where(ice, cold_albedo, warm_albedo), domain=self.domains['Ts'])\n        return albedo", "response": "Simple step - function albedo based on ice line at temperature Tf."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmake an exact clone of a process including state and all subprocesses.", "response": "def process_like(proc):\n    \"\"\"Make an exact clone of a process, including state and all subprocesses.\n\n    The creation date is updated.\n\n    :param proc:    process\n    :type proc:     :class:`~climlab.process.process.Process`\n    :return:        new process identical to the given process\n    :rtype:         :class:`~climlab.process.process.Process`\n\n    :Example:\n\n        ::\n\n            >>> import climlab\n            >>> from climlab.process.process import process_like\n\n            >>> model = climlab.EBM()\n            >>> model.subprocess.keys()\n            ['diffusion', 'LW', 'albedo', 'insolation']\n\n            >>> albedo = model.subprocess['albedo']\n            >>> albedo_copy = process_like(albedo)\n\n            >>> albedo.creation_date\n            'Thu, 24 Mar 2016 01:32:25 +0000'\n\n            >>> albedo_copy.creation_date\n            'Thu, 24 Mar 2016 01:33:29 +0000'\n\n    \"\"\"\n    newproc = copy.deepcopy(proc)\n    newproc.creation_date = time.strftime(\"%a, %d %b %Y %H:%M:%S %z\",\n                                          time.localtime())\n    return newproc"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a dictionary of all Axis in a domain or dictionary of domains.", "response": "def get_axes(process_or_domain):\n    \"\"\"Returns a dictionary of all Axis in a domain or dictionary of domains.\n\n    :param process_or_domain:   a process or a domain object\n    :type process_or_domain:    :class:`~climlab.process.process.Process` or\n                                :class:`~climlab.domain.domain._Domain`\n    :raises: :exc:              `TypeError` if input is not or not having a domain\n    :returns:                   dictionary of input's Axis\n    :rtype:                     dict\n\n    :Example:\n\n        ::\n\n            >>> import climlab\n            >>> from climlab.process.process import get_axes\n\n            >>> model = climlab.EBM()\n\n            >>> get_axes(model)\n            {'lat': <climlab.domain.axis.Axis object at 0x7ff13b9dd2d0>,\n             'depth': <climlab.domain.axis.Axis object at 0x7ff13b9dd310>}\n\n    \"\"\"\n    if isinstance(process_or_domain, Process):\n        dom = process_or_domain.domains\n    else:\n        dom = process_or_domain\n    if isinstance(dom, _Domain):\n        return dom.axes\n    elif isinstance(dom, dict):\n        axes = {}\n        for thisdom in list(dom.values()):\n            assert isinstance(thisdom, _Domain)\n            axes.update(thisdom.axes)\n        return axes\n    else:\n        raise TypeError('dom must be a domain or dictionary of domains.')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a dictionary of subproceses to this process.", "response": "def add_subprocesses(self, procdict):\n        \"\"\"Adds a dictionary of subproceses to this process.\n\n        Calls :func:`add_subprocess` for every process given in the\n        input-dictionary. It can also pass a single process, which will\n        be given the name *default*.\n\n        :param procdict:    a dictionary with process names as keys\n        :type procdict:     dict\n\n        \"\"\"\n        if isinstance(procdict, Process):\n            try:\n                name = procdict.name\n            except:\n                name = 'default'\n            self.add_subprocess(name, procdict)\n        else:\n            for name, proc in procdict.items():\n                self.add_subprocess(name, proc)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding a single subprocess to this process.", "response": "def add_subprocess(self, name, proc):\n        \"\"\"Adds a single subprocess to this process.\n\n        :param string name:     name of the subprocess\n        :param proc:            a Process object\n        :type proc:             :class:`~climlab.process.process.Process`\n        :raises: :exc:`ValueError`\n                                if ``proc`` is not a process\n\n        :Example:\n\n            Replacing an albedo subprocess through adding a subprocess with\n            same name::\n\n                >>> from climlab.model.ebm import EBM_seasonal\n                >>> from climlab.surface.albedo import StepFunctionAlbedo\n\n                >>> # creating EBM model\n                >>> ebm_s = EBM_seasonal()\n\n                >>> print ebm_s\n\n            .. code-block:: none\n                :emphasize-lines: 8\n\n                climlab Process of type <class 'climlab.model.ebm.EBM_seasonal'>.\n                State variables and domain shapes:\n                  Ts: (90, 1)\n                The subprocess tree:\n                top: <class 'climlab.model.ebm.EBM_seasonal'>\n                   diffusion: <class 'climlab.dynamics.diffusion.MeridionalDiffusion'>\n                   LW: <class 'climlab.radiation.AplusBT.AplusBT'>\n                   albedo: <class 'climlab.surface.albedo.P2Albedo'>\n                   insolation: <class 'climlab.radiation.insolation.DailyInsolation'>\n\n            ::\n\n                >>> #  creating and adding albedo feedback subprocess\n                >>> step_albedo = StepFunctionAlbedo(state=ebm_s.state, **ebm_s.param)\n                >>> ebm_s.add_subprocess('albedo', step_albedo)\n                >>>\n                >>> print ebm_s\n\n            .. code-block:: none\n                :emphasize-lines: 8\n\n                climlab Process of type <class 'climlab.model.ebm.EBM_seasonal'>.\n                State variables and domain shapes:\n                  Ts: (90, 1)\n                The subprocess tree:\n                top: <class 'climlab.model.ebm.EBM_seasonal'>\n                   diffusion: <class 'climlab.dynamics.diffusion.MeridionalDiffusion'>\n                   LW: <class 'climlab.radiation.AplusBT.AplusBT'>\n                   albedo: <class 'climlab.surface.albedo.StepFunctionAlbedo'>\n                      iceline: <class 'climlab.surface.albedo.Iceline'>\n                      cold_albedo: <class 'climlab.surface.albedo.ConstantAlbedo'>\n                      warm_albedo: <class 'climlab.surface.albedo.P2Albedo'>\n                   insolation: <class 'climlab.radiation.insolation.DailyInsolation'>\n\n        \"\"\"\n        if isinstance(proc, Process):\n            self.subprocess.update({name: proc})\n            self.has_process_type_list = False\n            # Add subprocess diagnostics to parent\n            #  (if there are no name conflicts)\n            for diagname, value in proc.diagnostics.items():\n                #if not (diagname in self.diagnostics or hasattr(self, diagname)):\n                #    self.add_diagnostic(diagname, value)\n                self.add_diagnostic(diagname, value)\n        else:\n            raise ValueError('subprocess must be Process object')"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves a single subprocess from this process.", "response": "def remove_subprocess(self, name, verbose=True):\n        \"\"\"Removes a single subprocess from this process.\n\n        :param string name:     name of the subprocess\n        :param bool verbose:    information whether warning message\n                                should be printed [default: True]\n\n        :Example:\n\n            Remove albedo subprocess from energy balance model::\n\n                >>> import climlab\n                >>> model = climlab.EBM()\n\n                >>> print model\n                climlab Process of type <class 'climlab.model.ebm.EBM'>.\n                State variables and domain shapes:\n                  Ts: (90, 1)\n                The subprocess tree:\n                top: <class 'climlab.model.ebm.EBM'>\n                   diffusion: <class 'climlab.dynamics.diffusion.MeridionalDiffusion'>\n                   LW: <class 'climlab.radiation.AplusBT.AplusBT'>\n                   albedo: <class 'climlab.surface.albedo.StepFunctionAlbedo'>\n                      iceline: <class 'climlab.surface.albedo.Iceline'>\n                      cold_albedo: <class 'climlab.surface.albedo.ConstantAlbedo'>\n                      warm_albedo: <class 'climlab.surface.albedo.P2Albedo'>\n                   insolation: <class 'climlab.radiation.insolation.P2Insolation'>\n\n                >>> model.remove_subprocess('albedo')\n\n                >>> print model\n                climlab Process of type <class 'climlab.model.ebm.EBM'>.\n                State variables and domain shapes:\n                  Ts: (90, 1)\n                The subprocess tree:\n                top: <class 'climlab.model.ebm.EBM'>\n                   diffusion: <class 'climlab.dynamics.diffusion.MeridionalDiffusion'>\n                   LW: <class 'climlab.radiation.AplusBT.AplusBT'>\n                   insolation: <class 'climlab.radiation.insolation.P2Insolation'>\n\n        \"\"\"\n        try:\n            self.subprocess.pop(name)\n        except KeyError:\n            if verbose:\n                print('WARNING: {} not found in subprocess dictionary.'.format(name))\n        self.has_process_type_list = False"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_state(self, name, value):\n        if isinstance(value, Field):\n            # populate domains dictionary with domains from state variables\n            self.domains.update({name: value.domain})\n        else:\n            try:\n                thisdom = self.state[name].domain\n                domshape = thisdom.shape\n            except:\n                raise ValueError('State variable needs a domain.')\n            value = np.atleast_1d(value)\n            if value.shape == domshape:\n                value = Field(value, domain=thisdom)\n            else:\n                raise ValueError('Shape mismatch between existing domain and new state variable.')\n        # set the state dictionary\n        self.state[name] = value\n        for name, value in self.state.items():\n        #convert int dtype to float\n            if np.issubdtype(self.state[name].dtype, np.dtype('int').type):\n                value = self.state[name].astype(float)\n                self.state[name]=value\n        self.__setattr__(name, value)", "response": "Sets the state of the state variable with the given name and value."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _add_field(self, field_type, name, value):\n        try:\n            self.__getattribute__(field_type).update({name: value})\n        except:\n            raise ValueError('Problem with field_type %s'  %field_type)\n        #  Note that if process has attribute name, this will trigger The\n        # setter method for that attribute\n        self.__setattr__(name, value)", "response": "Adds a new field to a specified dictionary."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds a diagnostic variable to the process object.", "response": "def add_diagnostic(self, name, value=None):\n        \"\"\"Create a new diagnostic variable called ``name`` for this process\n        and initialize it with the given ``value``.\n\n        Quantity is accessible in two ways:\n\n            * as a process attribute, i.e. ``proc.name``\n            * as a member of the diagnostics dictionary,\n              i.e. ``proc.diagnostics['name']``\n\n        Use attribute method to set values, e.g.\n        ```proc.name = value ```\n\n        :param str name:        name of diagnostic quantity to be initialized\n        :param array value:     initial value for quantity [default: None]\n\n        :Example:\n\n            Add a diagnostic CO2 variable to an energy balance model::\n\n                >>> import climlab\n                >>> model = climlab.EBM()\n\n                >>> # initialize CO2 variable with value 280 ppm\n                >>> model.add_diagnostic('CO2',280.)\n\n                >>> # access variable directly or through diagnostic dictionary\n                >>> model.CO2\n                280\n                >>> model.diagnostics.keys()\n                ['ASR', 'CO2', 'net_radiation', 'icelat', 'OLR', 'albedo']\n\n        \"\"\"\n        self._diag_vars.append(name)\n        self.__setattr__(name, value)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding an input variable to the process object.", "response": "def add_input(self, name, value=None):\n        '''Create a new input variable called ``name`` for this process\n        and initialize it with the given ``value``.\n\n        Quantity is accessible in two ways:\n\n            * as a process attribute, i.e. ``proc.name``\n            * as a member of the input dictionary,\n              i.e. ``proc.input['name']``\n\n        Use attribute method to set values, e.g.\n        ```proc.name = value ```\n\n        :param str name:        name of diagnostic quantity to be initialized\n        :param array value:     initial value for quantity [default: None]\n        '''\n        self._input_vars.append(name)\n        self.__setattr__(name, value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves a diagnostic from the process. diagnostic dictionary and also delete the associated process attribute.", "response": "def remove_diagnostic(self, name):\n        \"\"\"\tRemoves a diagnostic from the ``process.diagnostic`` dictionary\n        and also delete the associated process attribute.\n\n        :param str name:    name of diagnostic quantity to be removed\n\n        :Example:\n\n            Remove diagnostic variable 'icelat' from energy balance model::\n\n                >>> import climlab\n                >>> model = climlab.EBM()\n\n                >>> # display all diagnostic variables\n                >>> model.diagnostics.keys()\n                ['ASR', 'OLR', 'net_radiation', 'albedo', 'icelat']\n\n                >>> model.remove_diagnostic('icelat')\n                >>> model.diagnostics.keys()\n                ['ASR', 'OLR', 'net_radiation', 'albedo']\n\n                >>> # Watch out for subprocesses that may still want\n                >>>  # to access the diagnostic 'icelat' variable !!!\n\n        \"\"\"\n        #_ = self.diagnostics.pop(name)\n        #delattr(type(self), name)\n        try:\n            delattr(self, name)\n            self._diag_vars.remove(name)\n        except:\n            print('No diagnostic named {} was found.'.format(name))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_xarray(self, diagnostics=False):\n        if diagnostics:\n            dic = self.state.copy()\n            dic.update(self.diagnostics)\n            return state_to_xarray(dic)\n        else:\n            return state_to_xarray(self.state)", "response": "Convert the process variables to xarray format."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef lat(self):\n        try:\n            for domname, dom in self.domains.items():\n                try:\n                    thislat = dom.axes['lat'].points\n                except:\n                    pass\n            return thislat\n        except:\n            raise ValueError('Can\\'t resolve a lat axis.')", "response": "Returns the latitude of the order of the grid centers."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef lat_bounds(self):\n        try:\n            for domname, dom in self.domains.items():\n                try:\n                    thislat = dom.axes['lat'].bounds\n                except:\n                    pass\n            return thislat\n        except:\n            raise ValueError('Can\\'t resolve a lat axis.')", "response": "Returns the bounds of the lat axis of the sequence of domains."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the longitude of grid centers of the sequence in degrees.", "response": "def lon(self):\n        \"\"\"Longitude of grid centers (degrees)\n\n        :getter:    Returns the points of axis ``'lon'`` if availible in the\n                    process's domains.\n        :type:      array\n        :raises: :exc:`ValueError`\n                    if no ``'lon'`` axis can be found.\n\n        \"\"\"\n        try:\n            for domname, dom in self.domains.items():\n                try:\n                    thislon = dom.axes['lon'].points\n                except:\n                    pass\n            return thislon\n        except:\n            raise ValueError('Can\\'t resolve a lon axis.')"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef lon_bounds(self):\n        try:\n            for domname, dom in self.domains.items():\n                try:\n                    thislon = dom.axes['lon'].bounds\n                except:\n                    pass\n            return thislon\n        except:\n            raise ValueError('Can\\'t resolve a lon axis.')", "response": "Returns the bounds of the lon axis of the sequence of domains."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the pressure levels at grid centers of the lev axis.", "response": "def lev(self):\n        \"\"\"Pressure levels at grid centers (hPa or mb)\n\n        :getter:    Returns the points of axis ``'lev'`` if availible in the\n                    process's domains.\n        :type:      array\n        :raises: :exc:`ValueError`\n                    if no ``'lev'`` axis can be found.\n\n        \"\"\"\n        try:\n            for domname, dom in self.domains.items():\n                try:\n                    thislev = dom.axes['lev'].points\n                except:\n                    pass\n            return thislev\n        except:\n            raise ValueError('Can\\'t resolve a lev axis.')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef lev_bounds(self):\n        try:\n            for domname, dom in self.domains.items():\n                try:\n                    thislev = dom.axes['lev'].bounds\n                except:\n                    pass\n            return thislev\n        except:\n            raise ValueError('Can\\'t resolve a lev axis.')", "response": "Returns the pressure levels at the lev axis."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef depth(self):\n        try:\n            for domname, dom in self.domains.items():\n                try:\n                    thisdepth = dom.axes['depth'].points\n                except:\n                    pass\n            return thisdepth\n        except:\n            raise ValueError('Can\\'t resolve a depth axis.')", "response": "Depth at grid centers ( m )"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreads in the Berger and Loutre orbital table as a pandas dataframe convert to xarray", "response": "def _get_Berger_data(verbose=True):\n    '''Read in the Berger and Loutre orbital table as a pandas dataframe, convert to xarray\n    '''\n    # The first column of the data file is used as the row index, and represents kyr from present\n    orbit91_pd, path = load_data_source(local_path = local_path,\n                    remote_source_list = [threddspath, NCDCpath],\n                    open_method = pd.read_csv,\n                    open_method_kwargs = {'delim_whitespace': True, 'skiprows':1},\n                    verbose=verbose,)\n    #  As xarray structure with the dimension named 'kyear'\n    orbit = xr.Dataset(orbit91_pd).rename({'dim_0': 'kyear'})\n    #  Now change names\n    orbit = orbit.rename({'ECC': 'ecc', 'OMEGA': 'long_peri',\n                          'OBL': 'obliquity', 'PREC': 'precession'})\n    # add 180 degrees to long_peri (see lambda definition, Berger 1978 Appendix)\n    orbit['long_peri'] += 180.\n    orbit['precession'] *= -1.\n\n    orbit.attrs['Description'] = 'The Berger and Loutre (1991) orbital data table'\n    orbit.attrs['Citation'] = 'https://doi.org/10.1016/0277-3791(91)90033-Q'\n    orbit.attrs['Source'] = path\n    orbit.attrs['Note'] = 'Longitude of perihelion is defined to be 0 degrees at Northern Vernal Equinox. This differs by 180 degrees from orbit91 source file.'\n    return orbit"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_data_source(local_path,\n               remote_source_list,\n               open_method,\n               open_method_kwargs=dict(),\n               remote_kwargs=dict(),\n               verbose=True):\n    '''Flexible data retreiver to download and cache the data files locally.\n\n    Usage example (this makes a local copy of the ozone data file):\n\n    :Example:\n\n        .. code-block:: python\n\n            from climlab.utils.data_source import load_data_source\n            from xarray import open_dataset\n            ozonename = 'apeozone_cam3_5_54.nc'\n            ozonepath = 'http://thredds.atmos.albany.edu:8080/thredds/fileServer/CLIMLAB/ozone/' + ozonename\n            data, path = load_data_source(local_path=ozonename,\n                                          remote_source_list=[ozonepath],\n                                          open_method=open_dataset)\n            print(data)\n\n\n    The order of operations is\n\n    1. Try to read the data directly from ``local_path``\n    2. If the file doesn't exist then iterate through ``remote_source_list``.\n       Try to download and save the file to ``local_path`` using http request\n       If that works then open the data from ``local_path``.\n    3. As a last resort, try to read the data remotely from URLs in ``remote_source_list``\n\n    In all cases the file is opened and read by the user-supplied ``open_method``\n    (e.g. ``xarray.open_dataset``), with additional keyword arguments supplied\n    as a dictionary through ``open_method_kwargs``.\n    These are passed straight through to ``open_method``.\n\n    Additional keyword arguments in ``remote_kwargs``\n    are only passed to ``open_method`` in option 3 above\n    (remote access, e.g. through OpenDAP)\n\n    Quiet all output by passing ``verbose=False``.\n\n    Returns:\n\n    - ``data`` is the data object returned by the successful call to ``open_method``\n    - ``path`` is the path that resulted in a successful call to ``open_method``.\n    '''\n    try:\n        path = local_path\n        data = open_method(path, **open_method_kwargs)\n        if verbose:\n            print('Opened data from {}'.format(path))\n    #except FileNotFoundError:  # this is a more specific exception in Python 3\n    except IOError:  # works for Py2.7 and Py3.x\n        #  First try to load from remote sources and cache the file locally\n        for source in remote_source_list:\n            try:\n                response = _download_and_cache(source, local_path)\n                data = open_method(local_path, **open_method_kwargs)\n                if verbose:\n                    print('Data retrieved from {} and saved locally.'.format(source))\n                break\n            except Exception:\n                continue\n        else:\n            # as a final resort, try opening the source remotely\n            for source in remote_source_list:\n                path = source\n                try:\n                    # This works fine for Python >= 3.5\n                    #data = open_method(path, **open_method_kwargs, **remote_kwargs)\n                    data = open_method(path, **merge_two_dicts(open_method_kwargs, remote_kwargs))\n                    if verbose:\n                        print('Opened data remotely from {}'.format(source))\n                    break\n                except Exception:\n                    continue\n            else:\n                raise Exception('All data access methods have failed.')\n    return data, path", "response": "Flexible data retreiver to download and cache the data files locally."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef tril(array, k=0):\n    '''Lower triangle of an array.\n    Return a copy of an array with elements above the k-th diagonal zeroed.\n    Need a multi-dimensional version here because numpy.tril does not\n    broadcast for numpy verison < 1.9.'''\n    try:\n        tril_array = np.tril(array, k=k)\n    except:\n        # have to loop\n        tril_array = np.zeros_like(array)\n        shape = array.shape\n        otherdims = shape[:-2]\n        for index in np.ndindex(otherdims):\n            tril_array[index] = np.tril(array[index], k=k)\n    return tril_array", "response": "Lower triangle of an array.\n    Return a copy of an array with elements above the k - th diagonal zeroed."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef flux_up(self, fluxUpBottom, emission=None):\n        '''Compute downwelling radiative flux at interfaces between layers.\n\n        Inputs:\n\n            * fluxDownTop: flux down at top\n            * emission: emission from atmospheric levels (N)\n              defaults to zero if not given\n\n        Returns:\n\n            * vector of downwelling radiative flux between levels (N+1)\n              element 0 is the flux down to the surface.\n\n        '''\n        if emission is None:\n            emission = np.zeros_like(self.absorptivity)\n        E = np.concatenate((emission, np.atleast_1d(fluxUpBottom)), axis=-1)\n        #  dot product (matrix multiplication) along last axes\n        return np.squeeze(matrix_multiply(self.Tup, E[..., np.newaxis]))", "response": "Compute the downwelling radiative flux at interfaces between layers."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes upwelling radiative flux at interfaces between layers.", "response": "def flux_down(self, fluxDownTop, emission=None):\n        '''Compute upwelling radiative flux at interfaces between layers.\n\n        Inputs:\n\n            * fluxUpBottom: flux up from bottom\n            * emission: emission from atmospheric levels (N)\n              defaults to zero if not given\n\n        Returns:\n            * vector of upwelling radiative flux between levels (N+1)\n              element N is the flux up to space.\n\n        '''\n        if emission is None:\n            emission = np.zeros_like(self.absorptivity)\n        E = np.concatenate((np.atleast_1d(fluxDownTop),emission), axis=-1)\n        #  dot product (matrix multiplication) along last axes\n        return np.squeeze(matrix_multiply(self.Tdown, E[..., np.newaxis]))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _compute_heating_rates(self):\n        '''Prepare arguments and call the RRTGM_LW driver to calculate\n        radiative fluxes and heating rates'''\n        (ncol, nlay, icld, permuteseed, irng, idrv, cp,\n                play, plev, tlay, tlev, tsfc,\n                h2ovmr, o3vmr, co2vmr, ch4vmr, n2ovmr, o2vmr,\n                cfc11vmr, cfc12vmr, cfc22vmr, ccl4vmr, emis,\n                inflglw, iceflglw, liqflglw,\n                cldfrac, ciwp, clwp, reic, relq, tauc, tauaer,) = self._prepare_lw_arguments()\n        if icld == 0:  # clear-sky only\n            cldfmcl = np.zeros((ngptlw,ncol,nlay))\n            ciwpmcl = np.zeros((ngptlw,ncol,nlay))\n            clwpmcl = np.zeros((ngptlw,ncol,nlay))\n            reicmcl = np.zeros((ncol,nlay))\n            relqmcl = np.zeros((ncol,nlay))\n            taucmcl = np.zeros((ngptlw,ncol,nlay))\n        else:\n            #  Call the Monte Carlo Independent Column Approximation (McICA, Pincus et al., JC, 2003)\n            (cldfmcl, ciwpmcl, clwpmcl, reicmcl, relqmcl, taucmcl) = \\\n                _rrtmg_lw.climlab_mcica_subcol_lw(\n                            ncol, nlay, icld,\n                            permuteseed, irng, play,\n                            cldfrac, ciwp, clwp, reic, relq, tauc)\n            #  Call the RRTMG_LW driver to compute radiative fluxes\n        (uflx, dflx, hr, uflxc, dflxc, hrc, duflx_dt, duflxc_dt) = \\\n            _rrtmg_lw.climlab_rrtmg_lw(ncol, nlay, icld, idrv,\n                 play, plev, tlay, tlev, tsfc,\n                 h2ovmr, o3vmr, co2vmr, ch4vmr, n2ovmr, o2vmr,\n                 cfc11vmr, cfc12vmr, cfc22vmr, ccl4vmr, emis,\n                 inflglw, iceflglw, liqflglw, cldfmcl,\n                 taucmcl, ciwpmcl, clwpmcl, reicmcl, relqmcl,\n                 tauaer)\n        #  Output is all (ncol,nlay+1) or (ncol,nlay)\n        self.LW_flux_up = _rrtm_to_climlab(uflx) + 0.*self.LW_flux_up\n        self.LW_flux_down = _rrtm_to_climlab(dflx) + 0.*self.LW_flux_down\n        self.LW_flux_up_clr = _rrtm_to_climlab(uflxc) + 0.*self.LW_flux_up_clr\n        self.LW_flux_down_clr = _rrtm_to_climlab(dflxc) + 0.*self.LW_flux_down_clr\n        #  Compute quantities derived from fluxes, including OLR\n        self._compute_LW_flux_diagnostics()\n        #  calculate heating rates from flux divergence\n        LWheating_Wm2 = np.array(np.diff(self.LW_flux_net, axis=-1)) + 0.*self.Tatm\n        LWheating_clr_Wm2 = np.array(np.diff(self.LW_flux_net_clr, axis=-1)) + 0.*self.Tatm\n        self.heating_rate['Ts'] = np.array(-self.LW_flux_net[..., -1, np.newaxis]) + 0.*self.Ts\n        self.heating_rate['Tatm'] = LWheating_Wm2\n        #  Convert to K / day\n        Catm = self.Tatm.domain.heat_capacity\n        self.TdotLW = LWheating_Wm2 / Catm * const.seconds_per_day\n        self.TdotLW_clr = LWheating_clr_Wm2 / Catm * const.seconds_per_day", "response": "Prepare arguments and call the RRTMG_LW driver to calculate the radiative fluxes and heating rates for the specified base class and entry."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprepare arguments needed for both RRTMG_SW and RRTMG_LW with correct dimensions.", "response": "def _prepare_general_arguments(RRTMGobject):\n    '''Prepare arguments needed for both RRTMG_SW and RRTMG_LW with correct dimensions.'''\n    tlay = _climlab_to_rrtm(RRTMGobject.Tatm)\n    tlev = _climlab_to_rrtm(interface_temperature(**RRTMGobject.state))\n    play = _climlab_to_rrtm(RRTMGobject.lev * np.ones_like(tlay))\n    plev = _climlab_to_rrtm(RRTMGobject.lev_bounds * np.ones_like(tlev))\n    ncol, nlay = tlay.shape\n    tsfc = _climlab_to_rrtm_sfc(RRTMGobject.Ts, RRTMGobject.Ts)\n    # GASES -- put them in proper dimensions and units\n    vapor_mixing_ratio = mmr_to_vmr(RRTMGobject.specific_humidity, gas='H2O')\n    h2ovmr   = _climlab_to_rrtm(vapor_mixing_ratio * np.ones_like(RRTMGobject.Tatm))\n    o3vmr    = _climlab_to_rrtm(RRTMGobject.absorber_vmr['O3'] * np.ones_like(RRTMGobject.Tatm))\n    co2vmr   = _climlab_to_rrtm(RRTMGobject.absorber_vmr['CO2'] * np.ones_like(RRTMGobject.Tatm))\n    ch4vmr   = _climlab_to_rrtm(RRTMGobject.absorber_vmr['CH4'] * np.ones_like(RRTMGobject.Tatm))\n    n2ovmr   = _climlab_to_rrtm(RRTMGobject.absorber_vmr['N2O'] * np.ones_like(RRTMGobject.Tatm))\n    o2vmr    = _climlab_to_rrtm(RRTMGobject.absorber_vmr['O2'] * np.ones_like(RRTMGobject.Tatm))\n    cfc11vmr = _climlab_to_rrtm(RRTMGobject.absorber_vmr['CFC11'] * np.ones_like(RRTMGobject.Tatm))\n    cfc12vmr = _climlab_to_rrtm(RRTMGobject.absorber_vmr['CFC12'] * np.ones_like(RRTMGobject.Tatm))\n    cfc22vmr = _climlab_to_rrtm(RRTMGobject.absorber_vmr['CFC22'] * np.ones_like(RRTMGobject.Tatm))\n    ccl4vmr  = _climlab_to_rrtm(RRTMGobject.absorber_vmr['CCL4'] * np.ones_like(RRTMGobject.Tatm))\n    #  Cloud parameters\n    cldfrac = _climlab_to_rrtm(RRTMGobject.cldfrac * np.ones_like(RRTMGobject.Tatm))\n    ciwp = _climlab_to_rrtm(RRTMGobject.ciwp * np.ones_like(RRTMGobject.Tatm))\n    clwp = _climlab_to_rrtm(RRTMGobject.clwp * np.ones_like(RRTMGobject.Tatm))\n    relq = _climlab_to_rrtm(RRTMGobject.r_liq * np.ones_like(RRTMGobject.Tatm))\n    reic = _climlab_to_rrtm(RRTMGobject.r_ice * np.ones_like(RRTMGobject.Tatm))\n\n    return (ncol, nlay, play, plev, tlay, tlev, tsfc,\n            h2ovmr, o3vmr, co2vmr, ch4vmr, n2ovmr, o2vmr, cfc11vmr,\n            cfc12vmr, cfc12vmr, cfc22vmr, ccl4vmr,\n            cldfrac, ciwp, clwp, relq, reic)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes temperature at model layer interfaces.", "response": "def interface_temperature(Ts, Tatm, **kwargs):\n    '''Compute temperature at model layer interfaces.'''\n    #  Actually it's not clear to me how the RRTM code uses these values\n    lev = Tatm.domain.axes['lev'].points\n    lev_bounds = Tatm.domain.axes['lev'].bounds\n    #  Interpolate to layer interfaces\n    f = interp1d(lev, Tatm, axis=-1)  # interpolation function\n    Tinterp = f(lev_bounds[1:-1])\n    #  add TOA value, Assume surface temperature at bottom boundary\n    Ttoa = Tatm[...,0]\n    Tinterp = np.concatenate((Ttoa[..., np.newaxis], Tinterp, Ts), axis=-1)\n    return Tinterp"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprepare field with proper dimension order.", "response": "def _climlab_to_rrtm(field):\n    '''Prepare field with proper dimension order.\n    RRTM code expects arrays with (ncol, nlay)\n    and with pressure decreasing from surface at element 0\n\n    climlab grid dimensions are any of:\n        - (num_lev,) --> (1, num_lev)\n        - (num_lat, num_lev)  --> (num_lat, num_lev)\n        - (num_lat, num_lon, num_lev)  -->  (num_lat*num_lon, num_lev)\n\n        But lat-lon grids not yet supported here!\n\n    Case single column\n    '''\n    # Make this work just with 1D (KM,) arrays\n    #  (KM,)  -->  (1, nlay)\n    try:\n        #  Flip along the last axis to reverse the pressure order\n        field = field[..., ::-1]\n    except:\n        if np.isscalar(field):\n            return field\n        else:\n            raise ValueError('field must be array_like or scalar.')\n    shape = field.shape\n    if len(shape)==1:  #  (num_lev)\n        #  Need to append an extra dimension for singleton horizontal ncol\n        return field[np.newaxis, ...]\n    elif len(shape)==2:  # (num_lat, num_lev)\n        return field\n    elif len(shape) > 2:\n        raise ValueError('lat-lon grids not yet supported here.')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _climlab_to_rrtm_sfc(field, Ts):\n    '''Return an array of size np.squeeze(Ts) to remove the singleton depth dimension'''\n    fieldsqueeze = np.squeeze(field)\n    Tsqueeze = np.squeeze(Ts)\n    return fieldsqueeze * np.ones_like(Tsqueeze)", "response": "Return an array of size np. squeeze to remove the singleton depth dimension"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _climlab_to_cam3(self, field):\n        '''Prepare field with proper dimension order.\n        CAM3 code expects 3D arrays with (KM, JM, 1)\n        and 2D arrays with (JM, 1).\n\n        climlab grid dimensions are any of:\n            - (KM,)\n            - (JM, KM)\n            - (JM, IM, KM)\n\n        (longitude dimension IM not yet implemented here).'''\n        if np.isscalar(field):\n            return field\n        #  Check to see if column vector needs to be replicated over latitude\n        elif self.JM > 1:\n            if (field.shape == (self.KM,)):\n                return np.tile(field[...,np.newaxis], self.JM)\n            else:\n                return np.squeeze(np.transpose(field))[..., np.newaxis]\n        else:  #  1D vertical model\n            return field[..., np.newaxis, np.newaxis]", "response": "Prepare field with proper dimension order."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _cam3_to_climlab(self, field):\n        ''' Output is either (KM, JM, 1) or (JM, 1).\n        Transform this to...\n            - (KM,) or (1,)  if JM==1\n            - (KM, JM) or (JM, 1)   if JM>1\n\n        (longitude dimension IM not yet implemented).'''\n        if self.JM > 1:\n            if len(field.shape)==2:\n                return field\n            elif len(field.shape)==3:\n                return np.squeeze(np.transpose(field))\n        else:\n            return np.squeeze(field)", "response": "Transform the 3D field into climlab format."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef moist_amplification_factor(Tkelvin, relative_humidity=0.8):\n    '''Compute the moisture amplification factor for the moist diffusivity\n    given relative humidity and reference temperature profile.'''\n    deltaT = 0.01\n    #  slope of saturation specific humidity at 1000 hPa\n    dqsdTs = (qsat(Tkelvin+deltaT/2, 1000.) - qsat(Tkelvin-deltaT/2, 1000.)) / deltaT\n    return const.Lhvap / const.cp * relative_humidity * dqsdTs", "response": "Compute the moisture amplification factor for the moist diffusivity\n    given relative humidity and reference temperature profile."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing daily insolation given latitude time of year and orbital parameters.", "response": "def daily_insolation(lat, day, orb=const.orb_present, S0=const.S0, day_type=1):\n    \"\"\"Compute daily average insolation given latitude, time of year and orbital parameters.\n\n    Orbital parameters can be interpolated to any time in the last 5 Myears with\n    ``climlab.solar.orbital.OrbitalTable`` (see example above).\n\n    Longer orbital tables are available with ``climlab.solar.orbital.LongOrbitalTable``\n\n    Inputs can be scalar, ``numpy.ndarray``, or ``xarray.DataArray``.\n\n    The return value will be ``numpy.ndarray`` if **all** the inputs are ``numpy``.\n    Otherwise ``xarray.DataArray``.\n\n    **Function-call argument** \\n\n\n    :param array lat:       Latitude in degrees (-90 to 90).\n    :param array day:       Indicator of time of year. See argument ``day_type``\n                            for details about format.\n    :param dict orb:        a dictionary with three members (as provided by\n                            ``climlab.solar.orbital.OrbitalTable``)\n\n                            * ``'ecc'`` - eccentricity\n\n                                * unit: dimensionless\n                                * default value: ``0.017236``\n\n                            * ``'long_peri'`` - longitude of perihelion (precession angle)\n\n                                * unit: degrees\n                                * default value: ``281.37``\n\n                            * ``'obliquity'`` - obliquity angle\n\n                                * unit: degrees\n                                * default value: ``23.446``\n\n    :param float S0:        solar constant                                  \\n\n                            - unit: :math:`\\\\textrm{W}/\\\\textrm{m}^2`       \\n\n                            - default value: ``1365.2``\n    :param int day_type:    Convention for specifying time of year (+/- 1,2) [optional].\n\n                            *day_type=1* (default):\n                             day input is calendar day (1-365.24), where day 1\n                             is January first. The calendar is referenced to the\n                             vernal equinox which always occurs at day 80.\n\n                            *day_type=2:*\n                             day input is solar longitude (0-360 degrees). Solar\n                             longitude is the angle of the Earth's orbit measured from spring\n                             equinox (21 March). Note that calendar days and solar longitude are\n                             not linearly related because, by Kepler's Second Law, Earth's\n                             angular velocity varies according to its distance from the sun.\n    :raises: :exc:`ValueError`\n                            if day_type is neither 1 nor 2\n    :returns:               Daily average solar radiation in unit\n                            :math:`\\\\textrm{W}/\\\\textrm{m}^2`.\n\n                            Dimensions of output are ``(lat.size, day.size, ecc.size)``\n    :rtype:                 array\n\n\n    Code is fully vectorized to handle array input for all arguments.       \\n\n    Orbital arguments should all have the same sizes.\n    This is automatic if computed from\n    :func:`~climlab.solar.orbital.OrbitalTable.lookup_parameters`\n\n        For more information about computation of solar insolation see the\n        :ref:`Tutorial` chapter.\n\n     \"\"\"\n    # Inputs can be scalar, numpy vector, or xarray.DataArray.\n    #  If numpy, convert to xarray so that it will broadcast correctly\n    lat_is_xarray = True\n    day_is_xarray = True\n    if type(lat) is np.ndarray:\n        lat_is_xarray = False\n        lat = xr.DataArray(lat, coords=[lat], dims=['lat'])\n    if type(day) is np.ndarray:\n        day_is_xarray = False\n        day = xr.DataArray(day, coords=[day], dims=['day'])\n    ecc = orb['ecc']\n    long_peri = orb['long_peri']\n    obliquity = orb['obliquity']\n    # Convert precession angle and latitude to radians\n    phi = deg2rad( lat )\n\n    # lambda_long (solar longitude) is the angular distance along Earth's orbit measured from spring equinox (21 March)\n    if day_type==1: # calendar days\n        lambda_long = solar_longitude(day,orb)\n    elif day_type==2: #solar longitude (1-360) is specified in input, no need to convert days to longitude\n        lambda_long = deg2rad(day)\n    else:\n        raise ValueError('Invalid day_type.')\n\n    # Compute declination angle of the sun\n    delta = arcsin(sin(deg2rad(obliquity)) * sin(lambda_long))\n\n    #  suppress warning message generated by arccos here!\n    oldsettings = np.seterr(invalid='ignore')\n    # Compute Ho, the hour angle at sunrise / sunset\n    #  Check for no sunrise or no sunset: Berger 1978 eqn (8),(9)\n    Ho = xr.where( abs(delta)-pi/2+abs(phi) < 0., # there is sunset/sunrise\n              arccos(-tan(phi)*tan(delta)),\n              # otherwise figure out if it's all night or all day\n              xr.where(phi*delta>0., pi, 0.) )\n    # this is not really the daily average cosine of the zenith angle...\n    #  it's the integral from sunrise to sunset of that quantity...\n    coszen = Ho*sin(phi)*sin(delta) + cos(phi)*cos(delta)*sin(Ho)\n    # Compute insolation: Berger 1978 eq (10)\n    Fsw = S0/pi*( (1+ecc*cos(lambda_long -deg2rad(long_peri)))**2 / (1-ecc**2)**2 * coszen)\n    if not (lat_is_xarray or day_is_xarray):\n        # Dimensional ordering consistent with previous numpy code\n        return Fsw.transpose().values\n    else:\n        return Fsw"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef solar_longitude( day, orb=const.orb_present, days_per_year = None ):\n    if days_per_year is None:\n        days_per_year = const.days_per_year\n    ecc = orb['ecc']\n    long_peri_rad = deg2rad( orb['long_peri'])\n    delta_lambda = (day - 80.) * 2*pi/days_per_year\n    beta = sqrt(1 - ecc**2)\n    lambda_long_m = -2*((ecc/2 + (ecc**3)/8 ) * (1+beta) * sin(-long_peri_rad) -\n        (ecc**2)/4 * (1/2 + beta) * sin(-2*long_peri_rad) + (ecc**3)/8 *\n        (1/3 + beta) * sin(-3*long_peri_rad)) + delta_lambda\n    lambda_long = ( lambda_long_m + (2*ecc - (ecc**3)/4)*sin(lambda_long_m - long_peri_rad) +\n        (5/4)*(ecc**2) * sin(2*(lambda_long_m - long_peri_rad)) + (13/12)*(ecc**3)\n        * sin(3*(lambda_long_m - long_peri_rad)) )\n    return lambda_long", "response": "Estimate solar longitude from calendar day and orbital table."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef single_column(num_lev=30, water_depth=1., lev=None, **kwargs):\n    if lev is None:\n        levax = Axis(axis_type='lev', num_points=num_lev)\n    elif isinstance(lev, Axis):\n        levax = lev\n    else:\n        try:\n            levax = Axis(axis_type='lev', points=lev)\n        except:\n            raise ValueError('lev must be Axis object or pressure array')\n    depthax = Axis(axis_type='depth', bounds=[water_depth, 0.])\n    slab = SlabOcean(axes=depthax, **kwargs)\n    atm = Atmosphere(axes=levax, **kwargs)\n    return slab, atm", "response": "Returns a list of domains for a single column of atmosphere overlying a slab of water."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef zonal_mean_surface(num_lat=90, water_depth=10., lat=None, **kwargs):\n    if lat is None:\n        latax = Axis(axis_type='lat', num_points=num_lat)\n    elif isinstance(lat, Axis):\n        latax = lat\n    else:\n        try:\n            latax = Axis(axis_type='lat', points=lat)\n        except:\n            raise ValueError('lat must be Axis object or latitude array')\n    depthax = Axis(axis_type='depth', bounds=[water_depth, 0.])\n    axes = {'depth': depthax, 'lat': latax}\n    slab = SlabOcean(axes=axes, **kwargs)\n    return slab", "response": "Creates a 1D slab ocean Domain in latitude with uniform water depth."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a 2D slab ocean Domain in latitude and longitude with uniform water depth.", "response": "def surface_2D(num_lat=90, num_lon=180, water_depth=10., lon=None,\n               lat=None, **kwargs):\n    \"\"\"Creates a 2D slab ocean Domain in latitude and longitude with uniform water depth.\n\n    Domain has a single heat capacity according to the specified water depth.\n\n    **Function-call argument** \\n\n\n    :param int num_lat:         number of latitude points [default: 90]\n    :param int num_lon:         number of longitude points [default: 180]\n    :param float water_depth:   depth of the slab ocean in meters [default: 10.]\n    :param lat:                 specification for latitude axis (optional)\n    :type lat:                  :class:`~climlab.domain.axis.Axis` or latitude array\n    :param lon:                 specification for longitude axis (optional)\n    :type lon:                  :class:`~climlab.domain.axis.Axis` or longitude array\n    :raises: :exc:`ValueError`  if `lat` is given but neither Axis nor latitude array.\n    :raises: :exc:`ValueError`  if `lon` is given but neither Axis nor longitude array.\n    :returns:                   surface domain\n    :rtype:                     :class:`SlabOcean`\n\n    :Example:\n\n        ::\n\n            >>> from climlab import domain\n            >>> sfc = domain.surface_2D(num_lat=36, num_lat=72)\n\n            >>> print sfc\n            climlab Domain object with domain_type=ocean and shape=(36, 72, 1)\n\n    \"\"\"\n    if lat is None:\n        latax = Axis(axis_type='lat', num_points=num_lat)\n    elif isinstance(lat, Axis):\n        latax = lat\n    else:\n        try:\n            latax = Axis(axis_type='lat', points=lat)\n        except:\n            raise ValueError('lat must be Axis object or latitude array')\n    if lon is None:\n        lonax = Axis(axis_type='lon', num_points=num_lon)\n    elif isinstance(lon, Axis):\n        lonax = lon\n    else:\n        try:\n            lonax = Axis(axis_type='lon', points=lon)\n        except:\n            raise ValueError('lon must be Axis object or longitude array')\n    depthax = Axis(axis_type='depth', bounds=[water_depth, 0.])\n    axes = {'lat': latax, 'lon': lonax, 'depth': depthax}\n    slab = SlabOcean(axes=axes, **kwargs)\n    return slab"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef zonal_mean_column(num_lat=90, num_lev=30, water_depth=10., lat=None,\n                      lev=None, **kwargs):\n    \"\"\"Creates two Domains with one water cell, a latitude axis and\n    a level/height axis.\n\n        * SlabOcean:    one water cell and a latitude axis above\n          (similar to :func:`zonal_mean_surface`)\n        * Atmosphere: a latitude axis and a level/height axis (two dimensional)\n\n\n    **Function-call argument** \\n\n\n    :param int num_lat:         number of latitude points on the axis\n                                [default: 90]\n    :param int num_lev:         number of pressure levels\n                                (evenly spaced from surface to TOA) [default: 30]\n    :param float water_depth:   depth of the water cell (slab ocean) [default: 10.]\n    :param lat:                 specification for latitude axis (optional)\n    :type lat:                  :class:`~climlab.domain.axis.Axis` or latitude array\n    :param lev:                 specification for height axis (optional)\n    :type lev:                  :class:`~climlab.domain.axis.Axis` or pressure array\n    :raises: :exc:`ValueError`  if `lat` is given but neither Axis nor latitude array.\n    :raises: :exc:`ValueError`  if `lev` is given but neither Axis nor pressure array.\n    :returns:                   a list of 2 Domain objects (slab ocean, atmosphere)\n    :rtype:                     :py:class:`list` of :class:`SlabOcean`, :class:`Atmosphere`\n\n    :Example:\n\n        ::\n\n            >>> from climlab import domain\n            >>> sfc, atm = domain.zonal_mean_column(num_lat=36,num_lev=10)\n\n            >>> print sfc\n            climlab Domain object with domain_type=ocean and shape=(36, 1)\n\n            >>> print atm\n            climlab Domain object with domain_type=atm and shape=(36, 10)\n\n\n    \"\"\"\n    if lat is None:\n        latax = Axis(axis_type='lat', num_points=num_lat)\n    elif isinstance(lat, Axis):\n        latax = lat\n    else:\n        try:\n            latax = Axis(axis_type='lat', points=lat)\n        except:\n            raise ValueError('lat must be Axis object or latitude array')\n    if lev is None:\n        levax = Axis(axis_type='lev', num_points=num_lev)\n    elif isinstance(lev, Axis):\n        levax = lev\n    else:\n        try:\n            levax = Axis(axis_type='lev', points=lev)\n        except:\n            raise ValueError('lev must be Axis object or pressure array')\n\n    depthax = Axis(axis_type='depth', bounds=[water_depth, 0.])\n    #axes = {'depth': depthax, 'lat': latax, 'lev': levax}\n    slab = SlabOcean(axes={'lat':latax, 'depth':depthax}, **kwargs)\n    atm = Atmosphere(axes={'lat':latax, 'lev':levax}, **kwargs)\n    return slab, atm", "response": "Returns a list of two Domains with one water cell and a latitude axis and a level and height axis and zonal mean column."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef box_model_domain(num_points=2, **kwargs):\n    ax = Axis(axis_type='abstract', num_points=num_points)\n    boxes = _Domain(axes=ax, **kwargs)\n    boxes.domain_type = 'box'\n    return boxes", "response": "Creates a box model domain with single abstract axis."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _make_axes_dict(self, axes):\n        if type(axes) is dict:\n            axdict = axes\n        elif type(axes) is Axis:\n            ax = axes\n            axdict = {ax.axis_type: ax}\n        elif axes is None:\n            axdict = {'empty': None}\n        else:\n            raise ValueError('axes needs to be Axis object or dictionary of Axis object')\n        return axdict", "response": "Makes an axes dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute the state variable tendencies in time for implicit processes.", "response": "def _compute(self):\n        \"\"\"Computes the state variable tendencies in time for implicit processes.\n\n        To calculate the new state the :func:`_implicit_solver()` method is\n        called for daughter classes. This however returns the new state of the\n        variables, not just the tendencies. Therefore, the adjustment is\n        calculated which is the difference between the new and the old state\n        and stored in the object's attribute adjustment.\n\n        Calculating the new model states through solving the matrix problem\n        already includes the multiplication with the timestep. The derived\n        adjustment is divided by the timestep to calculate the implicit\n        subprocess tendencies, which can be handeled by the\n        :func:`~climlab.process.time_dependent_process.TimeDependentProcess.compute`\n        method of the parent\n        :class:`~climlab.process.time_dependent_process.TimeDependentProcess` class.\n\n        :ivar dict adjustment:  holding all state variables' adjustments\n                                of the implicit process which are the\n                                differences between the new states (which have\n                                been solved through matrix inversion) and the\n                                old states.\n\n        \"\"\"\n        newstate = self._implicit_solver()\n        adjustment = {}\n        tendencies = {}\n        for name, var in self.state.items():\n            adjustment[name] = newstate[name] - var\n            tendencies[name] = adjustment[name] / self.timestep\n        # express the adjustment (already accounting for the finite time step)\n        #  as a tendency per unit time, so that it can be applied along with explicit\n        self.adjustment = adjustment\n        self._update_diagnostics(newstate)\n        return tendencies"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute energy flux convergences to get heating rates in : math : W^2.", "response": "def _compute_heating_rates(self):\n        \"\"\"Computes energy flux convergences to get heating rates in :math:`W/m^2`.\n\n        \"\"\"\n        self._compute_emission()\n        for varname, value in self.state.items():\n            self.heating_rate[varname] = -self.OLR"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a string representation of the process tree for process top. This method uses the :func:`walk_processes` method to create the process tree. :param top: top process for which process tree string should be created :type top: :class:`~climlab.process.process.Process` :param str name: name of top process :returns: string representation of the process tree :rtype: str :Example: :: >>> import climlab >>> from climlab.utils import walk >>> model = climlab.EBM() >>> proc_tree_str = walk.process_tree(model, name='model') >>> print proc_tree_str model: <class 'climlab.model.ebm.EBM'> diffusion: <class 'climlab.dynamics.diffusion.MeridionalDiffusion'> LW: <class 'climlab.radiation.AplusBT.AplusBT'> albedo: <class 'climlab.surface.albedo.StepFunctionAlbedo'> iceline: <class 'climlab.surface.albedo.Iceline'> cold_albedo: <class 'climlab.surface.albedo.ConstantAlbedo'> warm_albedo: <class 'climlab.surface.albedo.P2Albedo'> insolation: <class 'climlab.radiation.insolation.P2Insolation'>", "response": "def process_tree(top, name='top'):\n    \"\"\"Creates a string representation of the process tree for process top.\n\n    This method uses the :func:`walk_processes` method to create the process tree.\n\n    :param  top:            top process for which process tree string should be\n                            created\n    :type top:              :class:`~climlab.process.process.Process`\n    :param str name:        name of top process\n    :returns:               string representation of the process tree\n    :rtype:                 str\n\n    :Example:\n\n        ::\n\n            >>> import climlab\n            >>> from climlab.utils import walk\n\n            >>> model = climlab.EBM()\n            >>> proc_tree_str = walk.process_tree(model, name='model')\n\n            >>> print proc_tree_str\n            model: <class 'climlab.model.ebm.EBM'>\n               diffusion: <class 'climlab.dynamics.diffusion.MeridionalDiffusion'>\n               LW: <class 'climlab.radiation.AplusBT.AplusBT'>\n               albedo: <class 'climlab.surface.albedo.StepFunctionAlbedo'>\n                  iceline: <class 'climlab.surface.albedo.Iceline'>\n                  cold_albedo: <class 'climlab.surface.albedo.ConstantAlbedo'>\n                  warm_albedo: <class 'climlab.surface.albedo.P2Albedo'>\n               insolation: <class 'climlab.radiation.insolation.P2Insolation'>\n\n    \"\"\"\n    str1 = ''\n    for name, proc, level in walk_processes(top, name, ignoreFlag=True):\n        indent = ' ' * 3 * (level)\n        str1 += ('{}{}: {}\\n'.format(indent, name, type(proc)))\n    return str1"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprepare arguments and call the RRTGM_SW driver to calculate the radiative fluxes and heating rates for a specific entry in the current record.", "response": "def _compute_heating_rates(self):\n        '''Prepare arguments and call the RRTGM_SW driver to calculate\n        radiative fluxes and heating rates'''\n        (ncol, nlay, icld, iaer, permuteseed, irng,\n         play, plev, tlay, tlev, tsfc,\n         h2ovmr, o3vmr, co2vmr, ch4vmr, n2ovmr, o2vmr,\n         aldif, aldir, asdif, asdir, coszen, adjes, dyofyr, scon, isolvar,\n         indsolvar, bndsolvar, solcycfrac,\n         inflgsw, iceflgsw, liqflgsw,\n         cldfrac, ciwp, clwp, reic, relq, tauc, ssac, asmc, fsfc,\n         tauaer, ssaaer, asmaer, ecaer,) = self._prepare_sw_arguments()\n        if icld == 0:  # clear-sky only\n            cldfmcl = np.zeros((ngptsw,ncol,nlay))\n            ciwpmcl = np.zeros((ngptsw,ncol,nlay))\n            clwpmcl = np.zeros((ngptsw,ncol,nlay))\n            reicmcl = np.zeros((ncol,nlay))\n            relqmcl = np.zeros((ncol,nlay))\n            taucmcl = np.zeros((ngptsw,ncol,nlay))\n            ssacmcl = np.zeros((ngptsw,ncol,nlay))\n            asmcmcl = np.zeros((ngptsw,ncol,nlay))\n            fsfcmcl = np.zeros((ngptsw,ncol,nlay))\n        else:\n            #  Call the Monte Carlo Independent Column Approximation (McICA, Pincus et al., JC, 2003)\n            (cldfmcl, ciwpmcl, clwpmcl, reicmcl, relqmcl, taucmcl,\n            ssacmcl, asmcmcl, fsfcmcl) = _rrtmg_sw.climlab_mcica_subcol_sw(\n                            ncol, nlay, icld, permuteseed, irng, play,\n                            cldfrac, ciwp, clwp, reic, relq, tauc, ssac, asmc, fsfc)\n        #  Call the RRTMG_SW driver to compute radiative fluxes\n        (swuflx, swdflx, swhr, swuflxc, swdflxc, swhrc) = \\\n            _rrtmg_sw.climlab_rrtmg_sw(ncol, nlay, icld, iaer,\n                play, plev, tlay, tlev, tsfc,\n                h2ovmr, o3vmr, co2vmr, ch4vmr, n2ovmr, o2vmr,\n                asdir, asdif, aldir, aldif,\n                coszen, adjes, dyofyr, scon, isolvar,\n                inflgsw, iceflgsw, liqflgsw, cldfmcl,\n                taucmcl, ssacmcl, asmcmcl, fsfcmcl,\n                ciwpmcl, clwpmcl, reicmcl, relqmcl,\n                tauaer, ssaaer, asmaer, ecaer,\n                bndsolvar, indsolvar, solcycfrac)\n        #  Output is all (ncol,nlay+1) or (ncol,nlay)\n        self.SW_flux_up = _rrtm_to_climlab(swuflx) + 0.*self.SW_flux_up\n        self.SW_flux_down = _rrtm_to_climlab(swdflx) + 0.*self.SW_flux_down\n        self.SW_flux_up_clr = _rrtm_to_climlab(swuflxc) + 0.*self.SW_flux_up_clr\n        self.SW_flux_down_clr = _rrtm_to_climlab(swdflxc) + 0.*self.SW_flux_down_clr\n        #  Compute quantities derived from fluxes, including ASR\n        self._compute_SW_flux_diagnostics()\n        #  calculate heating rates from flux divergence\n        SWheating_Wm2 = np.array(-np.diff(self.SW_flux_net, axis=-1)) + 0.*self.Tatm\n        SWheating_clr_Wm2 = np.array(-np.diff(self.SW_flux_net_clr, axis=-1)) + 0.*self.Tatm\n        self.heating_rate['Ts'] = np.array(self.SW_flux_net[..., -1, np.newaxis]) + 0.*self.Ts\n        self.heating_rate['Tatm'] = SWheating_Wm2\n        #  Convert to K / day\n        Catm = self.Tatm.domain.heat_capacity\n        self.TdotSW = SWheating_Wm2 / Catm * const.seconds_per_day\n        self.TdotSW_clr = SWheating_clr_Wm2 / Catm * const.seconds_per_day"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes the fluxes for the current class.", "response": "def _compute_fluxes(self):\n        ''' All fluxes are band by band'''\n        self.emission = self._compute_emission()\n        self.emission_sfc = self._compute_emission_sfc()\n        fromspace = self._from_space()\n        self.flux_down = self.trans.flux_down(fromspace, self.emission)\n        self.flux_reflected_up = self.trans.flux_reflected_up(self.flux_down, self.albedo_sfc)\n        # this ensure same dimensions as other fields\n        self.flux_to_sfc = self.flux_down[..., -1, np.newaxis]\n        self.flux_from_sfc = (self.emission_sfc +\n                              self.flux_reflected_up[..., -1, np.newaxis])\n        self.flux_up = self.trans.flux_up(self.flux_from_sfc,\n                            self.emission + self.flux_reflected_up[...,0:-1])\n        self.flux_net = self.flux_up - self.flux_down\n        # absorbed radiation (flux convergence) in W / m**2 (per band)\n        self.absorbed = np.diff(self.flux_net, axis=-1)\n        self.absorbed_total = np.sum(self.absorbed, axis=-1)\n        self.flux_to_space = self._compute_flux_top()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute the contributions to the outgoing flux to space due to emissions from each level and the surface.", "response": "def flux_components_top(self):\n        '''Compute the contributions to the outgoing flux to space due to\n        emissions from each level and the surface.'''\n        N = self.lev.size\n        flux_up_bottom = self.flux_from_sfc\n        emission = np.zeros_like(self.emission)\n        this_flux_up = (np.ones_like(self.Ts) *\n                        self.trans.flux_up(flux_up_bottom, emission))\n        sfcComponent = this_flux_up[..., -1]\n        atmComponents = np.zeros_like(self.Tatm)\n        flux_up_bottom = np.zeros_like(self.Ts)\n        # I'm sure there's a way to write this as a vectorized operation\n        #  but the speed doesn't really matter if it's just for diagnostic\n        #  and we are not calling it every timestep\n        for n in range(N):\n            emission = np.zeros_like(self.emission)\n            emission[..., n] = self.emission[..., n]\n            this_flux_up = self.trans.flux_up(flux_up_bottom, emission)\n            atmComponents[..., n] = this_flux_up[..., -1]\n        return sfcComponent, atmComponents"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef flux_components_bottom(self):\n        '''Compute the contributions to the downwelling flux to surface due to\n        emissions from each level.'''\n        N = self.lev.size\n        atmComponents = np.zeros_like(self.Tatm)\n        flux_down_top = np.zeros_like(self.Ts)\n        #  same comment as above... would be nice to vectorize\n        for n in range(N):\n            emission = np.zeros_like(self.emission)\n            emission[..., n] = self.emission[..., n]\n            this_flux_down = self.trans.flux_down(flux_down_top, emission)\n            atmComponents[..., n] = this_flux_down[..., 0]\n        return atmComponents", "response": "Compute the contributions to the downwelling flux to surface due to\n        emissions from each level."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _compute_heating_rates(self):\n        '''Compute energy flux convergences to get heating rates in :math:`W/m^2`.'''\n        self._compute_flux()\n        self.heating_rate['Ts'] = -self._flux\n        # Modify only the lowest model level\n        self.heating_rate['Tatm'][..., -1, np.newaxis] = self._flux", "response": "Compute energy flux convergences to get heating rates in : math : W / m^2."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef Pn(x):\n    Pn = {}\n    Pn['0'] = P0(x)\n    Pn['1'] = P1(x)\n    Pn['2'] = P2(x)\n    Pn['3'] = P3(x)\n    Pn['4'] = P4(x)\n    Pn['5'] = P5(x)\n    Pn['6'] = P6(x)\n    Pn['8'] = P8(x)\n    Pn['10'] = P10(x)\n    Pn['12'] = P12(x)\n    Pn['14'] = P14(x)\n    Pn['16'] = P16(x)\n    Pn['18'] = P18(x)\n    Pn['20'] = P20(x)\n    Pn['22'] = P22(x)\n    Pn['24'] = P24(x)\n    Pn['26'] = P26(x)\n    Pn['28'] = P28(x)\n    return Pn", "response": "Calculates Legendre polynomials P0 to P28 and returns them\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef inferred_heat_transport(self):\n        phi = np.deg2rad(self.lat)\n        energy_in = np.squeeze(self.net_radiation)\n        return (1E-15 * 2 * np.math.pi * const.a**2 *\n                integrate.cumtrapz(np.cos(phi)*energy_in, x=phi, initial=0.))", "response": "Calculates the inferred heat transport on the latitude grid in unit of mass."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef diffusive_heat_transport(self):\n        phi = np.deg2rad(self.lat)\n        phi_stag = np.deg2rad(self.lat_bounds)\n        D = self.param['D']\n        T = np.squeeze(self.Ts)\n        dTdphi = np.diff(T) / np.diff(phi)\n        dTdphi = np.append(dTdphi, 0.)\n        dTdphi = np.insert(dTdphi, 0, 0.)\n        return (1E-15*-2*np.math.pi*np.cos(phi_stag)*const.a**2*D*dTdphi)", "response": "Compute instantaneous diffusive heat transport in unit : math : \\\\ textrm { PW }"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef rrtmg_lw_gen_source(ext, build_dir):\n    '''Add RRTMG_LW fortran source if Fortran 90 compiler available,\n    if no compiler is found do not try to build the extension.'''\n    thispath = config.local_path\n    module_src = []\n    for item in modules:\n        fullname = join(thispath,'rrtmg_lw_v4.85','gcm_model','modules',item)\n        module_src.append(fullname)\n    for item in src:\n        if item in mod_src:\n            fullname = join(thispath,'sourcemods',item)\n        else:\n            fullname = join(thispath,'rrtmg_lw_v4.85','gcm_model','src',item)\n        module_src.append(fullname)\n    sourcelist = [join(thispath, '_rrtmg_lw.pyf'),\n                  join(thispath, 'Driver.f90')]\n    try:\n        config.have_f90c()\n        return module_src + sourcelist\n    except:\n        print('No Fortran 90 compiler found, not building RRTMG_LW extension!')\n        return None", "response": "Generate RRTMG_LW fortran source for each module in modules and sourcemods."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_timestep(self, timestep=const.seconds_per_day, num_steps_per_year=None):\n        if num_steps_per_year is not None:\n            timestep = const.seconds_per_year / num_steps_per_year\n        # Need a more sensible approach for annual cycle stuff\n        self.timestep = timestep", "response": "Calculates the timestep in unit seconds and calls the setter function of the calendar class object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef compute(self):\n        #  First reset tendencies to zero -- recomputing them is the point of this method\n        for varname in self.tendencies:\n            self.tendencies[varname] *= 0.\n        if not self.has_process_type_list:\n            self._build_process_type_list()\n        tendencies = {}\n        ignored = self._compute_type('diagnostic')\n        tendencies['explicit'] = self._compute_type('explicit')\n        #  Tendencies due to implicit and adjustment processes need to be\n        #  calculated from a state that is already adjusted after explicit stuff\n        #  So apply the tendencies temporarily and then remove them again\n        for name, var in self.state.items():\n            var += tendencies['explicit'][name] * self.timestep\n        # Now compute all implicit processes -- matrix inversions\n        tendencies['implicit'] = self._compute_type('implicit')\n        #  Same deal ... temporarily apply tendencies from implicit step\n        for name, var in self.state.items():\n            var += tendencies['implicit'][name] * self.timestep\n        # Finally compute all instantaneous adjustments -- expressed as explicit forward step\n        tendencies['adjustment'] = self._compute_type('adjustment')\n        #  Now remove the changes from the model state\n        for name, var in self.state.items():\n            var -= ( (tendencies['implicit'][name] + tendencies['explicit'][name]) *\n                    self.timestep)\n        #  Sum up all subprocess tendencies\n        for proctype in ['explicit', 'implicit', 'adjustment']:\n            for varname, tend in tendencies[proctype].items():\n                self.tendencies[varname] += tend\n        # Finally compute my own tendencies, if any\n        self_tend = self._compute()\n        #  Adjustment processes _compute method returns absolute adjustment\n        #  Needs to be converted to rate of change\n        if self.time_type is 'adjustment':\n            for varname, adj in self_tend.items():\n                self_tend[varname] /= self.timestep\n        for varname, tend in self_tend.items():\n            self.tendencies[varname] += tend\n        return self.tendencies", "response": "Computes the tendencies for all state variables given current state and input."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _compute_type(self, proctype):\n        tendencies = {}\n        for varname in self.state:\n            tendencies[varname] = 0. * self.state[varname]\n        for proc in self.process_types[proctype]:\n            #  Asynchronous coupling\n            #  if subprocess has longer timestep than parent\n            #  We compute subprocess tendencies once\n            #   and apply the same tendency at each substep\n            step_ratio = int(proc.timestep / self.timestep)\n            #  Does the number of parent steps divide evenly by the ratio?\n            #  If so, it's time to do a subprocess step.\n            if self.time['steps'] % step_ratio == 0:\n                proc.time['active_now'] = True\n                tenddict = proc.compute()\n            else:\n                # proc.tendencies is unchanged from last subprocess timestep if we didn't recompute it above\n                proc.time['active_now'] = False\n                tenddict = proc.tendencies\n            for name, tend in tenddict.items():\n                tendencies[name] += tend\n            for diagname, value in proc.diagnostics.items():\n                self.__setattr__(diagname, value)\n        return tendencies", "response": "Computes tendencies due to all subprocesses of given type. Also pass all diagnostics up to parent process."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _compute(self):\n        tendencies = {}\n        for name, value in self.state.items():\n            tendencies[name] = value * 0.\n        return tendencies", "response": "Compute the tendencies for the current daughter class"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _build_process_type_list(self):\n        self.process_types = {'diagnostic': [], 'explicit': [], 'implicit': [], 'adjustment': []}\n        #for name, proc, level in walk.walk_processes(self, topdown=self.topdown):\n        #    self.process_types[proc.time_type].append(proc)\n        for name, proc in self.subprocess.items():\n            self.process_types[proc.time_type].append(proc)\n        self.has_process_type_list = True", "response": "Generates a list of processes organized by process type."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating the state variables with computed tendencies.", "response": "def step_forward(self):\n        \"\"\"Updates state variables with computed tendencies.\n\n        Calls the :func:`compute` method to get current tendencies for all\n        process states. Multiplied with the timestep and added up to the state\n        variables is updating all model states.\n\n        :Example:\n\n            ::\n\n                >>> import climlab\n                >>> model = climlab.EBM()\n\n                >>> # checking time step counter\n                >>> model.time['steps']\n                0\n\n                >>> # stepping the model forward\n                >>> model.step_forward()\n\n                >>> # step counter increased\n                >>> model.time['steps']\n                1\n\n        \"\"\"\n        tenddict = self.compute()\n        #  Total tendency is applied as an explicit forward timestep\n        # (already accounting properly for order of operations in compute() )\n        for varname, tend in tenddict.items():\n            self.state[varname] += tend * self.timestep\n        # Update all time counters for this and all subprocesses in the tree\n        #  Also pass diagnostics up the process tree\n        for name, proc, level in walk.walk_processes(self, ignoreFlag=True):\n            if proc.time['active_now']:\n                proc._update_time()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupdates the time of the current calendar.", "response": "def _update_time(self):\n        \"\"\"Increments the timestep counter by one.\n\n        Furthermore ``self.time['days_elapsed']`` and\n        ``self.time['num_steps_per_year']`` are updated.\n\n        The function is called by the time stepping methods.\n\n        \"\"\"\n        self.time['steps'] += 1\n        # time in days since beginning\n        self.time['days_elapsed'] += self.time['timestep'] / const.seconds_per_day\n        if self.time['day_of_year_index'] >= self.time['num_steps_per_year']-1:\n            self._do_new_calendar_year()\n        else:\n            self.time['day_of_year_index'] += 1"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef integrate_years(self, years=1.0, verbose=True):\n        days = years * const.days_per_year\n        numsteps = int(self.time['num_steps_per_year'] * years)\n        if verbose:\n            print(\"Integrating for \" + str(numsteps) + \" steps, \"\n                  + str(days) + \" days, or \" + str(years) + \" years.\")\n        #  begin time loop\n        for count in range(numsteps):\n            # Compute the timestep\n            self.step_forward()\n            if count == 0:\n                # on first step only...\n                #  This implements a generic time-averaging feature\n                # using the list of model state variables\n                self.timeave = self.state.copy()\n                # add any new diagnostics to the timeave dictionary\n                self.timeave.update(self.diagnostics)\n                # reset all values to zero\n                for varname, value in self.timeave.items():\n                # moves on to the next varname if value is None\n                # this preserves NoneType diagnostics\n                    if value is None:\n                        continue\n                    self.timeave[varname] = 0*value\n            # adding up all values for each timestep\n            for varname in list(self.timeave.keys()):\n                try:\n                    self.timeave[varname] += self.state[varname]\n                except:\n                    try:\n                        self.timeave[varname] += self.diagnostics[varname]\n                    except: pass\n        # calculating mean values through dividing the sum by number of steps\n        for varname, value in self.timeave.items():\n            if value is None:\n                continue\n            self.timeave[varname] /= numsteps\n        if verbose:\n            print(\"Total elapsed time is %s years.\"\n                  % str(self.time['days_elapsed']/const.days_per_year))", "response": "Integrates the model by a given number of years."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef integrate_days(self, days=1.0, verbose=True):\n        years = days / const.days_per_year\n        self.integrate_years(years=years, verbose=verbose)", "response": "Integrates the model forward for a specified number of days into years and calls the integrator method."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef integrate_converge(self, crit=1e-4, verbose=True):\n        # implemented by m-kreuzer\n        for varname, value in self.state.items():\n            value_old = copy.deepcopy(value)\n            self.integrate_years(1,verbose=False)\n            while np.max(np.abs(value_old-value)) > crit :\n                value_old = copy.deepcopy(value)\n                self.integrate_years(1,verbose=False)\n        if verbose == True:\n            print(\"Total elapsed time is %s years.\"\n                  % str(self.time['days_elapsed']/const.days_per_year))", "response": "Integrates the model until the model states are converging."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef column_state(num_lev=30,\n                 num_lat=1,\n                 lev=None,\n                 lat=None,\n                 water_depth=1.0):\n    \"\"\"Sets up a state variable dictionary consisting of temperatures\n    for atmospheric column (``Tatm``) and surface mixed layer (``Ts``).\n\n    Surface temperature is always 288 K. Atmospheric temperature is initialized\n    between 278 K at lowest altitude and 200 at top of atmosphere according to\n    the number of levels given.\n\n    **Function-call arguments** \\n\n\n    :param int num_lev:         number of pressure levels\n                                (evenly spaced from surface to top of atmosphere)\n                                [default: 30]\n    :param int num_lat:         number of latitude points on the axis\n                                [default: 1]\n    :param lev:                 specification for height axis (optional)\n    :type lev:                  :class:`~climlab.domain.axis.Axis`\n                                or pressure array\n    :param array lat:           size of array determines dimension of latitude\n                                (optional)\n    :param float water_depth:   *irrelevant*\n\n    :returns:                   dictionary with two temperature\n                                :class:`~climlab.domain.field.Field`\n                                for atmospheric column ``Tatm`` and\n                                surface mixed layer ``Ts``\n    :rtype:                     dict\n\n    :Example:\n\n        ::\n\n            >>> from climlab.domain import initial\n            >>> T_dict = initial.column_state()\n\n            >>> print T_dict\n            {'Tatm': Field([ 200.        ,  202.68965517,  205.37931034,  208.06896552,\n                    210.75862069,  213.44827586,  216.13793103,  218.82758621,\n                    221.51724138,  224.20689655,  226.89655172,  229.5862069 ,\n                    232.27586207,  234.96551724,  237.65517241,  240.34482759,\n                    243.03448276,  245.72413793,  248.4137931 ,  251.10344828,\n                    253.79310345,  256.48275862,  259.17241379,  261.86206897,\n                    264.55172414,  267.24137931,  269.93103448,  272.62068966,\n                    275.31034483,  278.        ]), 'Ts': Field([ 288.])}\n\n    \"\"\"\n    if lat is not None:\n        num_lat = np.array(lat).size\n    if lev is not None:\n        num_lev = np.array(lev).size\n\n    if num_lat is 1:\n        sfc, atm = domain.single_column(water_depth=water_depth,\n                                        num_lev=num_lev,\n                                        lev=lev)\n    else:\n        sfc, atm = domain.zonal_mean_column(water_depth=water_depth,\n                                            num_lev=num_lev,\n                                            lev=lev,\n                                            num_lat=num_lat,\n                                            lat=lat)\n    num_lev = atm.lev.num_points\n    Ts = Field(288.*np.ones(sfc.shape), domain=sfc)\n    Tinitial = np.tile(np.linspace(200., 288.-10., num_lev), sfc.shape)\n    Tatm = Field(Tinitial, domain=atm)\n    state = AttrDict()\n    state['Ts'] = Ts\n    state['Tatm'] = Tatm\n    return state", "response": "Returns a state variable dictionary consisting of temperatures of the given atmospheric column and surface mixed layer."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef surface_state(num_lat=90,\n                  num_lon=None,\n                  water_depth=10.,\n                  T0=12.,\n                  T2=-40.):\n    \"\"\"Sets up a state variable dictionary for a surface model\n    (e.g. :class:`~climlab.model.ebm.EBM`) with a uniform slab ocean depth.\n\n    The domain is either 1D (latitude) or 2D (latitude, longitude)\n    depending on whether the input argument num_lon is supplied.\n\n    Returns a single state variable `Ts`, the temperature of the surface\n    mixed layer (slab ocean).\n\n    The temperature is initialized to a smooth equator-to-pole shape given by\n\n    .. math::\n\n        T(\\phi) = T_0 + T_2 P_2(\\sin\\phi)\n\n    where :math:`\\phi` is latitude, and :math:`P_2` is the second Legendre\n    polynomial :class:`~climlab.utils.legendre.P2`.\n\n    **Function-call arguments** \\n\n\n    :param int num_lat:         number of latitude points [default: 90]\n    :param int num_lat:         (optional) number of longitude points [default: None]\n    :param float water_depth:   depth of the slab ocean in meters [default: 10.]\n    :param float T0:            global-mean initial temperature in :math:`^{\\circ} \\\\textrm{C}` [default: 12.]\n    :param float T2:            2nd Legendre coefficient for equator-to-pole gradient in\n                                initial temperature, in :math:`^{\\circ} \\\\textrm{C}` [default: -40.]\n\n    :returns:                   dictionary with temperature\n                                :class:`~climlab.domain.field.Field`\n                                for surface mixed layer ``Ts``\n    :rtype:                     dict\n\n\n    :Example:\n\n        ::\n\n            >>> from climlab.domain import initial\n            >>> import numpy as np\n\n            >>> T_dict = initial.surface_state(num_lat=36)\n\n            >>> print np.squeeze(T_dict['Ts'])\n            [-27.88584094 -26.97777479 -25.18923361 -22.57456133 -19.21320344\n             -15.20729309 -10.67854785  -5.76457135  -0.61467228   4.61467228\n               9.76457135  14.67854785  19.20729309  23.21320344  26.57456133\n              29.18923361  30.97777479  31.88584094  31.88584094  30.97777479\n              29.18923361  26.57456133  23.21320344  19.20729309  14.67854785\n               9.76457135   4.61467228  -0.61467228  -5.76457135 -10.67854785\n             -15.20729309 -19.21320344 -22.57456133 -25.18923361 -26.97777479\n             -27.88584094]\n\n    \"\"\"\n    if num_lon is None:\n        sfc = domain.zonal_mean_surface(num_lat=num_lat,\n                                        water_depth=water_depth)\n    else:\n        sfc = domain.surface_2D(num_lat=num_lat,\n                                num_lon=num_lon,\n                                water_depth=water_depth)\n    if 'lon' in sfc.axes:\n        lon, lat = np.meshgrid(sfc.axes['lon'].points, sfc.axes['lat'].points)\n    else:\n        lat = sfc.axes['lat'].points\n    sinphi = np.sin(np.deg2rad(lat))\n    initial = T0 + T2 * legendre.P2(sinphi)\n    Ts = Field(initial, domain=sfc)\n    #if num_lon is None:\n    #    Ts = Field(initial, domain=sfc)\n    #else:\n    #    Ts = Field([[initial for k in range(num_lon)]], domain=sfc)\n    state = AttrDict()\n    state['Ts'] = Ts\n    return state", "response": "Returns a state variable dictionary for a surface model of a slab ocean."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cam3_gen_source(ext, build_dir):\n    '''Add CAM3 fortran source if Fortran 90 compiler available,\n    if no compiler is found do not try to build the extension.'''\n    #  Fortran 90 sources in order of compilation\n    fort90source = ['pmgrid.F90',\n                'prescribed_aerosols.F90',\n                'shr_kind_mod.F90',\n                'quicksort.F90',\n                'abortutils.F90',\n                'absems.F90',\n                'wv_saturation.F90',\n                'aer_optics.F90',\n                'cmparray_mod.F90',\n                'shr_const_mod.F90',\n                'physconst.F90',\n                'pkg_cldoptics.F90',\n                'gffgch.F90',\n                'chem_surfvals.F90',\n                'volcrad.F90',\n                'radae.F90',\n                'radlw.F90',\n                'radsw.F90',\n                'crm.F90',]\n    #thispath = abspath(config.local_path)\n    thispath = config.local_path\n    sourcelist = []\n    sourcelist.append(join(thispath,'_cam3.pyf'))\n    for item in fort90source:\n        sourcelist.append(join(thispath, 'src', item))\n    sourcelist.append(join(thispath,'Driver.f90'))\n    try:\n        config.have_f90c()\n        return sourcelist\n    except:\n        print('No Fortran 90 compiler found, not building CAM3 extension!')\n        return None", "response": "Generate the CAM3 fortran source for the given extension."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating the latitude weighted global mean of a field containing latitude dependence.", "response": "def global_mean(field):\n    \"\"\"Calculates the latitude weighted global mean of a field\n    with latitude dependence.\n\n    :param Field field: input field\n    :raises: :exc:`ValueError` if input field has no latitude axis\n    :return: latitude weighted global mean of the field\n    :rtype: float\n\n    :Example:\n\n        initial global mean temperature of EBM model::\n\n            >>> import climlab\n            >>> model = climlab.EBM()\n            >>> climlab.global_mean(model.Ts)\n            Field(11.997968598413685)\n\n    \"\"\"\n    try:\n        lat = field.domain.lat.points\n    except:\n        raise ValueError('No latitude axis in input field.')\n    try:\n        #  Field is 2D latitude / longitude\n        lon = field.domain.lon.points\n        return _global_mean_latlon(field.squeeze())\n    except:\n        #  Field is 1D latitude only (zonal average)\n        lat_radians = np.deg2rad(lat)\n        return _global_mean(field.squeeze(), lat_radians)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbroadcasting a 1D array across another axis.", "response": "def to_latlon(array, domain, axis = 'lon'):\n    \"\"\"Broadcasts a 1D axis dependent array across another axis.\n\n    :param array input_array:   the 1D array used for broadcasting\n    :param domain:              the domain associated with that\n                                array\n    :param axis:                the axis that the input array will\n                                be broadcasted across\n                                [default: 'lon']\n    :return:                    Field with the same shape as the\n                                domain\n    :Example:\n\n        ::\n\n            >>> import climlab\n            >>> from climlab.domain.field import to_latlon\n            >>> import numpy as np\n\n            >>> state = climlab.surface_state(num_lat=3, num_lon=4)\n            >>> m = climlab.EBM_annual(state=state)\n            >>> insolation = np.array([237., 417., 237.])\n            >>> insolation = to_latlon(insolation, domain = m.domains['Ts'])\n            >>> insolation.shape\n            (3, 4, 1)\n            >>> insolation\n            Field([[[ 237.],   [[ 417.],   [[ 237.],\n                    [ 237.],    [ 417.],    [ 237.],\n                    [ 237.],    [ 417.],    [ 237.],\n                    [ 237.]],   [ 417.]],   [ 237.]]])\n\n    \"\"\"\n    #  if array is latitude dependent (has the same shape as lat)\n    axis, array, depth = np.meshgrid(domain.axes[axis].points, array,\n                                    domain.axes['depth'].points)\n    if axis == 'lat':\n    #  if array is longitude dependent (has the same shape as lon)\n        np.swapaxes(array,1,0)\n    return Field(array, domain=domain)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _compute_heating_rates(self):\n        for varname, value in self.state.items():\n            self.heating_rate[varname] = - self.b * (value - global_mean(value))", "response": "Computes energy flux convergences to get heating rates in : math : W^2."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts a climlab. Field object to xarray. DataArray.", "response": "def Field_to_xarray(field):\n    '''Convert a climlab.Field object to xarray.DataArray'''\n    dom = field.domain\n    dims = []; dimlist = []; coords = {};\n    for axname in dom.axes:\n        dimlist.append(axname)\n        try:\n            assert field.interfaces[dom.axis_index[axname]]\n            bounds_name = axname + '_bounds'\n            dims.append(bounds_name)\n            coords[bounds_name] = dom.axes[axname].bounds\n        except:\n            dims.append(axname)\n            coords[axname] = dom.axes[axname].points\n    #  Might need to reorder the data\n    da = DataArray(field.transpose([dom.axis_index[name] for name in dimlist]),\n                      dims=dims, coords=coords)\n    for name in dims:\n        try:\n            da[name].attrs['units'] = dom.axes[name].units\n        except:\n            pass\n    return da"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts a dictionary of climlab. Field objects to xarray. Dataset object.", "response": "def state_to_xarray(state):\n    '''Convert a dictionary of climlab.Field objects to xarray.Dataset\n\n    Input: dictionary of climlab.Field objects\n    (e.g. process.state or process.diagnostics dictionary)\n\n    Output: xarray.Dataset object with all spatial axes,\n    including 'bounds' axes indicating cell boundaries in each spatial dimension.\n\n    Any items in the dictionary that are not instances of climlab.Field\n    are ignored.'''\n    from climlab.domain.field import Field\n\n    ds = Dataset()\n    for name, field in state.items():\n        if isinstance(field, Field):\n            ds[name] = Field_to_xarray(field)\n            dom = field.domain\n            for axname, ax in dom.axes.items():\n                bounds_name = axname + '_bounds'\n                ds.coords[bounds_name] = DataArray(ax.bounds, dims=[bounds_name],\n                                    coords={bounds_name:ax.bounds})\n                try:\n                    ds[bounds_name].attrs['units'] = ax.units\n                except:\n                    pass\n        else:\n            warnings.warn('{} excluded from Dataset because it is not a Field variable.'.format(name))\n    return ds"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts climlab input to xarray format.", "response": "def to_xarray(input):\n    '''Convert climlab input to xarray format.\n\n    If input is a climlab.Field object, return xarray.DataArray\n\n    If input is a dictionary (e.g. process.state or process.diagnostics),\n    return xarray.Dataset object with all spatial axes,\n    including 'bounds' axes indicating cell boundaries in each spatial dimension.\n\n    Any items in the dictionary that are not instances of climlab.Field\n    are ignored.'''\n    from climlab.domain.field import Field\n    if isinstance(input, Field):\n        return Field_to_xarray(input)\n    elif isinstance(input, dict):\n        return state_to_xarray(input)\n    else:\n        raise TypeError('input must be Field object or dictionary of Field objects')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef generate(data, dimOrder, maxWindowSize, overlapPercent, transforms = []):\n\t\n\t# Determine the dimensions of the input data\n\twidth = data.shape[dimOrder.index('w')]\n\theight = data.shape[dimOrder.index('h')]\n\t\n\t# Generate the windows\n\treturn generateForSize(width, height, dimOrder, maxWindowSize, overlapPercent, transforms)", "response": "Generates a set of sliding windows for the specified dataset."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates a set of sliding windows for a dataset with the specified dimensions and order.", "response": "def generateForSize(width, height, dimOrder, maxWindowSize, overlapPercent, transforms = []):\n\t\"\"\"\n\tGenerates a set of sliding windows for a dataset with the specified dimensions and order.\n\t\"\"\"\n\t\n\t# If the input data is smaller than the specified window size,\n\t# clip the window size to the input size on both dimensions\n\twindowSizeX = min(maxWindowSize, width)\n\twindowSizeY = min(maxWindowSize, height)\n\t\n\t# Compute the window overlap and step size\n\twindowOverlapX = int(math.floor(windowSizeX * overlapPercent))\n\twindowOverlapY = int(math.floor(windowSizeY * overlapPercent))\n\tstepSizeX = windowSizeX - windowOverlapX\n\tstepSizeY = windowSizeY - windowOverlapY\n\t\n\t# Determine how many windows we will need in order to cover the input data\n\tlastX = width - windowSizeX\n\tlastY = height - windowSizeY\n\txOffsets = list(range(0, lastX+1, stepSizeX))\n\tyOffsets = list(range(0, lastY+1, stepSizeY))\n\t\n\t# Unless the input data dimensions are exact multiples of the step size,\n\t# we will need one additional row and column of windows to get 100% coverage\n\tif len(xOffsets) == 0 or xOffsets[-1] != lastX:\n\t\txOffsets.append(lastX)\n\tif len(yOffsets) == 0 or yOffsets[-1] != lastY:\n\t\tyOffsets.append(lastY)\n\t\n\t# Generate the list of windows\n\twindows = []\n\tfor xOffset in xOffsets:\n\t\tfor yOffset in yOffsets:\n\t\t\tfor transform in [None] + transforms:\n\t\t\t\twindows.append(SlidingWindow(\n\t\t\t\t\tx=xOffset,\n\t\t\t\t\ty=yOffset,\n\t\t\t\t\tw=windowSizeX,\n\t\t\t\t\th=windowSizeY,\n\t\t\t\t\tdimOrder=dimOrder,\n\t\t\t\t\ttransform=transform\n\t\t\t\t))\n\t\n\treturn windows"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\napplies any transform bound to this window and returns the view of the matrix.", "response": "def apply(self, matrix):\n\t\t\"\"\"\n\t\tSlices the supplied matrix and applies any transform bound to this window\n\t\t\"\"\"\n\t\tview = matrix[ self.indices() ]\n\t\treturn self.transform(view) if self.transform != None else view"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the coordinates of the current window bounds as a tuple of x y w h", "response": "def getRect(self):\n\t\t\"\"\"\n\t\tReturns the window bounds as a tuple of (x,y,w,h)\n\t\t\"\"\"\n\t\treturn (self.x, self.y, self.w, self.h)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef setRect(self, rect):\n\t\tself.x, self.y, self.w, self.h = rect", "response": "Sets the window bounds from a tuple of x y w h."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef indices(self, includeChannel=True):\n\t\tif self.dimOrder == DimOrder.HeightWidthChannel:\n\t\t\t\n\t\t\t# Equivalent to [self.y:self.y+self.h+1, self.x:self.x+self.w+1]\n\t\t\treturn (\n\t\t\t\tslice(self.y, self.y+self.h),\n\t\t\t\tslice(self.x, self.x+self.w)\n\t\t\t)\n\t\t\t\n\t\telif self.dimOrder == DimOrder.ChannelHeightWidth:\n\t\t\t\n\t\t\tif includeChannel is True:\n\t\t\t\t\n\t\t\t\t# Equivalent to [:, self.y:self.y+self.h+1, self.x:self.x+self.w+1]\n\t\t\t\treturn (\n\t\t\t\t\tslice(None, None),\n\t\t\t\t\tslice(self.y, self.y+self.h),\n\t\t\t\t\tslice(self.x, self.x+self.w)\n\t\t\t\t)\n\t\t\t\t\n\t\t\telse:\n\t\t\t\t\n\t\t\t\t# Equivalent to [self.y:self.y+self.h+1, self.x:self.x+self.w+1]\n\t\t\t\treturn (\n\t\t\t\t\tslice(self.y, self.y+self.h),\n\t\t\t\t\tslice(self.x, self.x+self.w)\n\t\t\t\t)\n\t\t\t\n\t\telse:\n\t\t\traise Error('Unsupported order of dimensions: ' + str(self.dimOrder))", "response": "Retrieves the indices for this window as a tuple of slices."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting Splits a list of windows into a series of batches.", "response": "def batchWindows(windows, batchSize):\n\t\"\"\"\n\tSplits a list of windows into a series of batches.\n\t\"\"\"\n\treturn np.array_split(np.array(windows), len(windows) // batchSize)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating a matrix specifying the distance of each point in a window to its centre.", "response": "def generateDistanceMatrix(width, height):\n\t\"\"\"\n\tGenerates a matrix specifying the distance of each point in a window to its centre.\n\t\"\"\"\n\t\n\t# Determine the coordinates of the exact centre of the window\n\toriginX = width / 2\n\toriginY = height / 2\n\t\n\t# Generate the distance matrix\n\tdistances = zerosFactory((height,width), dtype=np.float)\n\tfor index, val in np.ndenumerate(distances):\n\t\ty,x = index\n\t\tdistances[(y,x)] = math.sqrt( math.pow(x - originX, 2) + math.pow(y - originY, 2) )\n\t\n\treturn distances"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncrops a rectangle by the specified number of pixels on each side.", "response": "def cropRect(rect, cropTop, cropBottom, cropLeft, cropRight):\n\t\"\"\"\n\tCrops a rectangle by the specified number of pixels on each side.\n\t\n\tThe input rectangle and return value are both a tuple of (x,y,w,h).\n\t\"\"\"\n\t\n\t# Unpack the rectangle\n\tx, y, w, h = rect\n\t\n\t# Crop by the specified value\n\tx += cropLeft\n\ty += cropTop\n\tw -= (cropLeft + cropRight)\n\th -= (cropTop + cropBottom)\n\t\n\t# Re-pack the padded rect\n\treturn (x,y,w,h)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef padRect(rect, padTop, padBottom, padLeft, padRight, bounds, clipExcess = True):\n\t\n\t# Unpack the rectangle\n\tx, y, w, h = rect\n\t\n\t# Pad by the specified value\n\tx -= padLeft\n\ty -= padTop\n\tw += (padLeft + padRight)\n\th += (padTop + padBottom)\n\t\n\t# Determine if we are clipping overflows/underflows or\n\t# shifting the centre of the rectangle to compensate\n\tif clipExcess == True:\n\t\t\n\t\t# Clip any underflows\n\t\tx = max(0, x)\n\t\ty = max(0, y)\n\t\t\n\t\t# Clip any overflows\n\t\toverflowY = max(0, (y + h) - bounds[0])\n\t\toverflowX = max(0, (x + w) - bounds[1])\n\t\th -= overflowY\n\t\tw -= overflowX\n\t\t\n\telse:\n\t\t\n\t\t# Compensate for any underflows\n\t\tunderflowX = max(0, 0 - x)\n\t\tunderflowY = max(0, 0 - y)\n\t\tx += underflowX\n\t\ty += underflowY\n\t\t\n\t\t# Compensate for any overflows\n\t\toverflowY = max(0, (y + h) - bounds[0])\n\t\toverflowX = max(0, (x + w) - bounds[1])\n\t\tx -= overflowX\n\t\tw += overflowX\n\t\ty -= overflowY\n\t\th += overflowY\n\t\t\n\t\t# If there are still overflows or underflows after our\n\t\t# modifications, we have no choice but to clip them\n\t\tx, y, w, h = padRect((x,y,w,h), 0, 0, 0, 0, bounds, True)\n\t\n\t# Re-pack the padded rect\n\treturn (x,y,w,h)", "response": "Pads a rectangle by the specified values on each individual side and returns the padded rectangle."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef padRectEqually(rect, padding, bounds, clipExcess = True):\n\treturn padRect(rect, padding, padding, padding, padding, bounds, clipExcess)", "response": "Applies equal padding to all sides of a rectangle."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef squareAspect(rect):\n\t\n\t# Determine which dimension needs to be cropped\n\tx,y,w,h = rect\n\tif w > h:\n\t\tcropX = (w - h) // 2\n\t\treturn cropRect(rect, 0, 0, cropX, cropX)\n\telif w < h:\n\t\tcropY = (h - w) // 2\n\t\treturn cropRect(rect, cropY, cropY, 0, 0)\n\t\n\t# Already a square\n\treturn rect", "response": "This function takes a rectangle and returns a tuple of x y w h and crops the rectangle if necessary."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfitting the input rectangle to the target size.", "response": "def fitToSize(rect, targetWidth, targetHeight, bounds):\n\t\"\"\"\n\tPads or crops a rectangle as necessary to achieve the target dimensions,\n\tensuring the modified rectangle falls within the specified bounds.\n\t\n\tThe input rectangle, bounds, and return value are all a tuple of (x,y,w,h).\n\t\"\"\"\n\t\n\t# Determine the difference between the current size and target size\n\tx,y,w,h = rect\n\tdiffX = w - targetWidth\n\tdiffY = h - targetHeight\n\t\n\t# Determine if we are cropping or padding the width\n\tif diffX > 0:\n\t\tcropLeft  = math.floor(diffX / 2)\n\t\tcropRight = diffX - cropLeft\n\t\tx,y,w,h   = cropRect((x,y,w,h), 0, 0, cropLeft, cropRight)\n\telif diffX < 0:\n\t\tpadLeft  = math.floor(abs(diffX) / 2)\n\t\tpadRight = abs(diffX) - padLeft\n\t\tx,y,w,h  = padRect((x,y,w,h), 0, 0, padLeft, padRight, bounds, False)\n\t\n\t# Determine if we are cropping or padding the height\n\tif diffY > 0:\n\t\tcropTop    = math.floor(diffY / 2)\n\t\tcropBottom = diffY - cropTop\n\t\tx,y,w,h    = cropRect((x,y,w,h), cropTop, cropBottom, 0, 0)\n\telif diffY < 0:\n\t\tpadTop    = math.floor(abs(diffY) / 2)\n\t\tpadBottom = abs(diffY) - padTop\n\t\tx,y,w,h   = padRect((x,y,w,h), padTop, padBottom, 0, 0, bounds, False)\n\t\n\treturn (x,y,w,h)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates sliding windows for the specified dataset and applies the specified transformation function to each window.", "response": "def mergeWindows(data, dimOrder, maxWindowSize, overlapPercent, batchSize, transform, progressCallback = None):\n\t\"\"\"\n\tGenerates sliding windows for the specified dataset and applies the specified\n\ttransformation function to each window. Where multiple overlapping windows\n\tinclude an element of the input dataset, the overlap is resolved by computing\n\tthe mean transform result value for that element.\n\t\n\tIrrespective of the order of the dimensions of the input dataset, the\n\ttransformation function should return a NumPy array with dimensions\n\t[batch, height, width, resultChannels].\n\t\n\tIf a progress callback is supplied, it will be called immediately before\n\tapplying the transformation function to each batch of windows. The callback\n\tshould accept the current batch index and number of batches as arguments.\n\t\"\"\"\n\t\n\t# Determine the dimensions of the input data\n\tsourceWidth = data.shape[dimOrder.index('w')]\n\tsourceHeight = data.shape[dimOrder.index('h')]\n\t\n\t# Generate the sliding windows and group them into batches\n\twindows = generate(data, dimOrder, maxWindowSize, overlapPercent)\n\tbatches = batchWindows(windows, batchSize)\n\t\n\t# Apply the transform to the first batch of windows and determine the result dimensionality\n\texemplarResult = transform(data, batches[0])\n\tresultDimensions = exemplarResult.shape[ len(exemplarResult.shape) - 1 ]\n\t\n\t# Create the matrices to hold the sums and counts for the transform result values\n\tsums = np.zeros((sourceHeight, sourceWidth, resultDimensions), dtype=np.float)\n\tcounts = np.zeros((sourceHeight, sourceWidth), dtype=np.uint32)\n\t\n\t# Iterate over the batches and apply the transformation function to each batch\n\tfor batchNum, batch in enumerate(batches):\n\t\t\n\t\t# If a progress callback was supplied, call it\n\t\tif progressCallback != None:\n\t\t\tprogressCallback(batchNum, len(batches))\n\t\t\n\t\t# Apply the transformation function to the current batch\n\t\tbatchResult = transform(data, batch)\n\t\t\n\t\t# Iterate over the windows in the batch and update the sums matrix\n\t\tfor windowNum, window in enumerate(batch):\n\t\t\t\n\t\t\t# Create views into the larger matrices that correspond to the current window\n\t\t\twindowIndices = window.indices(False)\n\t\t\tsumsView = sums[windowIndices]\n\t\t\tcountsView = counts[windowIndices]\n\t\t\t\n\t\t\t# Update the result sums for each of the dataset elements in the window\n\t\t\tsumsView[:] += batchResult[windowNum]\n\t\t\tcountsView[:] += 1\n\t\n\t# Use the sums and the counts to compute the mean values\n\tfor dim in range(0, resultDimensions):\n\t\tsums[:,:,dim] /= counts\n\t\n\t# Return the mean values\n\treturn sums"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _requiredSize(shape, dtype):\n\treturn math.floor(np.prod(np.asarray(shape, dtype=np.uint64)) * np.dtype(dtype).itemsize)", "response": "Determines the number of bytes required to store a NumPy array with the specified shape and dtype."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a new ndarray of the specified shape and datatype storing it in memory if there is sufficient available space.", "response": "def arrayFactory(shape, dtype=float):\n\t\"\"\"\n\tCreates a new ndarray of the specified shape and datatype, storing\n\tit in memory if there is sufficient available space or else using\n\ta memory-mapped temporary file to provide the underlying buffer.\n\t\"\"\"\n\t\n\t# Determine the number of bytes required to store the array\n\trequiredBytes = _requiredSize(shape, dtype)\n\t\n\t# Determine if there is sufficient available memory\n\tvmem = psutil.virtual_memory()\n\tif vmem.available > requiredBytes:\n\t\treturn np.ndarray(shape=shape, dtype=dtype)\n\telse:\n\t\treturn TempfileBackedArray(shape=shape, dtype=dtype)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a new NumPy array filled with zeros.", "response": "def zerosFactory(shape, dtype=float):\n\t\"\"\"\n\tCreates a new NumPy array using `arrayFactory()` and fills it with zeros.\n\t\"\"\"\n\tarr = arrayFactory(shape=shape, dtype=dtype)\n\tarr.fill(0)\n\treturn arr"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef arrayCast(source, dtype):\n\t\n\t# Determine the number of bytes required to store the array\n\trequiredBytes = _requiredSize(source.shape, dtype)\n\t\n\t# Determine if there is sufficient available memory\n\tvmem = psutil.virtual_memory()\n\tif vmem.available > requiredBytes:\n\t\treturn source.astype(dtype, subok=False)\n\telse:\n\t\tdest = arrayFactory(source.shape, dtype)\n\t\tnp.copyto(dest, source, casting='unsafe')\n\t\treturn dest", "response": "Casts a NumPy array to the specified datatype storing the copy\n\tin memory if there is sufficient available space."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndetermines the maximum square window size that can be used based on the specified datatype and amount of currently available system memory.", "response": "def determineMaxWindowSize(dtype, limit=None):\n\t\"\"\"\n\tDetermines the largest square window size that can be used, based on\n\tthe specified datatype and amount of currently available system memory.\n\t\n\tIf `limit` is specified, then this value will be returned in the event\n\tthat it is smaller than the maximum computed size.\n\t\"\"\"\n\tvmem = psutil.virtual_memory()\n\tmaxSize = math.floor(math.sqrt(vmem.available / np.dtype(dtype).itemsize))\n\tif limit is None or limit >= maxSize:\n\t\treturn maxSize\n\telse:\n\t\treturn limit"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nattaches the view to the GUI.", "response": "def attach(self, gui, name=None):\n        \"\"\"Attach the view to the GUI.\"\"\"\n\n        # Disable keyboard pan so that we can use arrows as global shortcuts\n        # in the GUI.\n        self.panzoom.enable_keyboard_pan = False\n\n        gui.add_view(self)\n        self.gui = gui\n\n        # Set the view state.\n        self.set_state(gui.state.get_view_state(self))\n\n        # Call on_select() asynchronously after a delay, and set a busy\n        # cursor.\n        self.async_caller = AsyncCaller(delay=self._callback_delay)\n\n        @gui.connect_\n        def on_select(cluster_ids, **kwargs):\n            # Call this function after a delay unless there is another\n            # cluster selection in the meantime.\n            @self.async_caller.set\n            def update_view():\n                with busy_cursor():\n                    self.on_select(cluster_ids, **kwargs)\n\n        self.actions = Actions(gui,\n                               name=name or self.__class__.__name__,\n                               menu=self.__class__.__name__,\n                               default_shortcuts=self.shortcuts)\n\n        # Update the GUI status message when the `self.set_status()` method\n        # is called, i.e. when the `status` event is raised by the VisPy\n        # view.\n        @self.connect\n        def on_status(e):\n            gui.status_message = e.message\n\n        # Save the view state in the GUI state.\n        @gui.connect_\n        def on_close():\n            gui.state.update_view_state(self, self.state)\n            # NOTE: create_gui() already saves the state, but the event\n            # is registered *before* we add all views.\n            gui.state.save()\n\n        self.show()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_state(self, state):\n        for k, v in state.items():\n            setattr(self, k, v)", "response": "Set the view state."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _fullname(o):\n    return o.__module__ + \".\" + o.__name__ if o.__module__ else o.__name__", "response": "Return the fully - qualified name of a function."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_default_grid():\n    s = \"\"\"\n    time,0A 1A,0A   0B,0A   1B,0A\n    0A,1A   time,1A 0B,1A   1B,1A\n    0A,0B   1A,0B   time,0B 1B,0B\n    0A,1B   1A,1B   0B,1B   time,1B\n    \"\"\".strip()\n    dims = [[_ for _ in re.split(' +', line.strip())]\n            for line in s.splitlines()]\n    return dims", "response": "Return the default grid dimensions."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nyielding the i j dim x y coordinates of all subplots.", "response": "def _iter_subplots(self):\n        \"\"\"Yield (i, j, dim).\"\"\"\n        for i in range(self.n_cols):\n            for j in range(self.n_cols):\n                # Skip lower-diagonal subplots.\n                if i > j:\n                    continue\n                dim = self.grid_dim[i][j]\n                dim_x, dim_y = dim.split(',')\n                yield i, j, dim_x, dim_y"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_axis_label(self, dim):\n        if u(dim[:-1]).isdecimal():\n            n = len(self.channel_ids)\n            return str(self.channel_ids[int(dim[:-1]) % n]) + dim[-1]\n        else:\n            return dim", "response": "Return the channel id from a dimension if applicable."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_axis_data(self, bunch, dim, cluster_id=None, load_all=None):\n        if dim in self.attributes:\n            return self.attributes[dim](cluster_id, load_all=load_all)\n        masks = bunch.get('masks', None)\n        assert dim not in self.attributes  # This is called only on PC data.\n        s = 'ABCDEFGHIJ'\n        # Channel relative index.\n        c_rel = int(dim[:-1])\n        # Get the channel_id from the currently-selected channels.\n        channel_id = self.channel_ids[c_rel % len(self.channel_ids)]\n        # Skup the plot if the channel id is not displayed.\n        if channel_id not in bunch.channel_ids:  # pragma: no cover\n            return None\n        # Get the column index of the current channel in data.\n        c = list(bunch.channel_ids).index(channel_id)\n        # Principal component: A=0, B=1, etc.\n        d = s.index(dim[-1])\n        if masks is not None:\n            masks = masks[:, c]\n        return Bunch(data=bunch.data[:, c, d],\n                     masks=masks,\n                     )", "response": "Extract the points from the data on a given dimension."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_axis_bounds(self, dim, bunch):\n        if dim in self.attributes:\n            # Attribute: specified lim, or compute the min/max.\n            vmin, vmax = bunch['lim']\n            assert vmin is not None\n            assert vmax is not None\n            return vmin, vmax\n        # PC dimensions: use the common scaling.\n        return (-1. / self.scaling, +1. / self.scaling)", "response": "Return the min max of an axis."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nplot feature labels along left and bottom edge of subplots.", "response": "def _plot_labels(self):\n        \"\"\"Plot feature labels along left and bottom edge of subplots\"\"\"\n        # iterate simultaneously over kth row in left column and\n        # kth column in bottom row:\n        br = self.n_cols - 1  # bottom row\n        for k in range(0, self.n_cols):\n            dim_x, _ = self.grid_dim[0][k].split(',')\n            _, dim_y = self.grid_dim[k][br].split(',')\n            # Get the channel ids corresponding to the relative channel indices\n            # specified in the dimensions. Channel 0 corresponds to the first\n            # best channel for the selected cluster, and so on.\n            dim_x = self._get_axis_label(dim_x)\n            dim_y = self._get_axis_label(dim_y)\n            # Left edge of left column of subplots.\n            self[k, 0].text(pos=[-1., 0.],\n                            text=dim_y,\n                            anchor=[-1.03, 0.],\n                            data_bounds=None,\n                            )\n            # Bottom edge of bottom row of subplots.\n            self[br, k].text(pos=[0., -1.],\n                             text=dim_x,\n                             anchor=[0., -1.04],\n                             data_bounds=None,\n                             )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef attach(self, gui):\n        super(FeatureView, self).attach(gui)\n        self.actions.add(self.increase)\n        self.actions.add(self.decrease)\n        self.actions.separator()\n        self.actions.add(self.clear_channels)\n        self.actions.add(self.toggle_automatic_channel_selection)\n\n        gui.connect_(self.on_channel_click)\n        gui.connect_(self.on_request_split)", "response": "Attach the view to the GUI."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef on_channel_click(self, channel_id=None, key=None, button=None):\n        channels = self.channel_ids\n        if channels is None:\n            return\n        if len(channels) == 1:\n            self.on_select()\n            return\n        assert len(channels) >= 2\n        # Get the axis from the pressed button (1, 2, etc.)\n        # axis = 'x' if button == 1 else 'y'\n        d = 0 if button == 1 else 1\n        # Change the first or second best channel.\n        old = channels[d]\n        # Avoid updating the view if the channel doesn't change.\n        if channel_id == old:\n            return\n        channels[d] = channel_id\n        # Ensure that the first two channels are different.\n        if channels[1 - d] == channel_id:\n            channels[1 - d] = old\n        assert channels[0] != channels[1]\n        # Remove duplicate channels.\n        self.channel_ids = _uniq(channels)\n        logger.debug(\"Choose channels %d and %d in feature view.\",\n                     *channels[:2])\n        # Fix the channels temporarily.\n        self.on_select(fixed_channels=True)", "response": "Respond to the click on a channel."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef on_request_split(self):\n        if (self.lasso.count < 3 or\n                not len(self.cluster_ids)):  # pragma: no cover\n            return np.array([], dtype=np.int64)\n        assert len(self.channel_ids)\n\n        # Get the dimensions of the lassoed subplot.\n        i, j = self.lasso.box\n        dim = self.grid_dim[i][j]\n        dim_x, dim_y = dim.split(',')\n\n        # Get all points from all clusters.\n        pos = []\n        spike_ids = []\n\n        for cluster_id in self.cluster_ids:\n            # Load all spikes.\n            bunch = self.features(cluster_id,\n                                  channel_ids=self.channel_ids,\n                                  load_all=True)\n            px = self._get_axis_data(bunch, dim_x, cluster_id=cluster_id,\n                                     load_all=True)\n            py = self._get_axis_data(bunch, dim_y, cluster_id=cluster_id,\n                                     load_all=True)\n            points = np.c_[px.data, py.data]\n\n            # Normalize the points.\n            xmin, xmax = self._get_axis_bounds(dim_x, px)\n            ymin, ymax = self._get_axis_bounds(dim_y, py)\n            r = Range((xmin, ymin, xmax, ymax))\n            points = r.apply(points)\n\n            pos.append(points)\n            spike_ids.append(bunch.spike_ids)\n        pos = np.vstack(pos)\n        spike_ids = np.concatenate(spike_ids)\n\n        # Find lassoed spikes.\n        ind = self.lasso.in_polygon(pos)\n        self.lasso.clear()\n        return np.unique(spike_ids[ind])", "response": "Return the spikes enclosed by the lasso."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the closest box to some position.", "response": "def get_closest_box(self, pos):\n        \"\"\"Get the box closest to some position.\"\"\"\n        pos = np.atleast_2d(pos)\n        d = np.sum((np.array(self.box_pos) - pos) ** 2, axis=1)\n        idx = np.argmin(d)\n        return idx"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update_boxes(self, box_pos, box_size):\n        assert box_pos.shape == (self.n_boxes, 2)\n        assert len(box_size) == 2\n        self.box_bounds = _get_boxes(box_pos,\n                                     size=box_size,\n                                     keep_aspect_ratio=self.keep_aspect_ratio,\n                                     )", "response": "Update the box bounds of the log entry."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nblocks loop until a signal is emitted or timeout elapses.", "response": "def _wait_signal(signal, timeout=None):\n    \"\"\"Block loop until signal emitted, or timeout (ms) elapses.\"\"\"\n    # http://jdreaver.com/posts/2014-07-03-waiting-for-signals-pyside-pyqt.html\n    loop = QEventLoop()\n    signal.connect(loop.quit)\n\n    yield\n\n    if timeout is not None:\n        QTimer.singleShot(timeout, loop.quit)\n    loop.exec_()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef require_qt(func):\n    @wraps(func)\n    def wrapped(*args, **kwargs):\n        if not QApplication.instance():  # pragma: no cover\n            raise RuntimeError(\"A Qt application must be created.\")\n        return func(*args, **kwargs)\n    return wrapped", "response": "Decorator to specify that a function requires a Qt application before it can run."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a Qt application.", "response": "def create_app():\n    \"\"\"Create a Qt application.\"\"\"\n    global QT_APP\n    QT_APP = QApplication.instance()\n    if QT_APP is None:  # pragma: no cover\n        QT_APP = QApplication(sys.argv)\n    return QT_APP"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncall a function after a delay unless another function is set in the meantime.", "response": "def set(self, f):\n        \"\"\"Call a function after a delay, unless another function is set\n        in the meantime.\"\"\"\n        self.stop()\n        self._create_timer(f)\n        self.start()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef stop(self):\n        if self._timer:\n            self._timer.stop()\n            self._timer.deleteLater()", "response": "Stop the current timer if there is one and cancel the async call."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _parse_list(s):\n    # Range: 'x-y'\n    if '-' in s:\n        m, M = map(_parse_arg, s.split('-'))\n        return list(range(m, M + 1))\n    # List of ids: 'x,y,z'\n    elif ',' in s:\n        return list(map(_parse_arg, s.split(',')))\n    else:\n        return _parse_arg(s)", "response": "Parse a comma - separated list of values ( strings or numbers."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwrapping a function s arguments with Qt input dialog.", "response": "def _wrap_callback_args(f, docstring=None):  # pragma: no cover\n    \"\"\"Display a Qt dialog when a function has arguments.\n\n    The user can write function arguments as if it was a snippet.\n\n    \"\"\"\n    def wrapped(checked, *args):\n        if args:\n            return f(*args)\n        if isinstance(f, partial):\n            argspec = inspect.getargspec(f.func)\n        else:\n            argspec = inspect.getargspec(f)\n        f_args = argspec.args\n        if 'self' in f_args:\n            f_args.remove('self')\n        # Remove arguments with defaults from the list.\n        if len(argspec.defaults or ()):\n            f_args = f_args[:-len(argspec.defaults)]\n        # Remove arguments supplied in a partial.\n        if isinstance(f, partial):\n            f_args = f_args[len(f.args):]\n            f_args = [arg for arg in f_args if arg not in f.keywords]\n        # If there are no remaining args, we can just run the fu nction.\n        if not f_args:\n            return f()\n        # There are args, need to display the dialog.\n        # Extract Example: `...` in the docstring to put a predefined text\n        # in the input dialog.\n        r = re.search('Example: `([^`]+)`', docstring)\n        docstring_ = docstring[:r.start()].strip() if r else docstring\n        text = r.group(1) if r else None\n        s, ok = _input_dialog(getattr(f, '__name__', 'action'),\n                              docstring_, text)\n        if not ok or not s:\n            return\n        # Parse user-supplied arguments and call the function.\n        args = _parse_snippet(s)\n        return f(*args)\n    return wrapped"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_shortcut_string(shortcut):\n    if shortcut is None:\n        return ''\n    if isinstance(shortcut, (tuple, list)):\n        return ', '.join([_get_shortcut_string(s) for s in shortcut])\n    if isinstance(shortcut, string_types):\n        if hasattr(QKeySequence, shortcut):\n            shortcut = QKeySequence(getattr(QKeySequence, shortcut))\n        else:\n            return shortcut.lower()\n    assert isinstance(shortcut, QKeySequence)\n    s = shortcut.toString() or ''\n    return str(s).lower()", "response": "Return a string representation of a shortcut."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a QKeySequence or list of QKeySequence from a shortcut string.", "response": "def _get_qkeysequence(shortcut):\n    \"\"\"Return a QKeySequence or list of QKeySequence from a shortcut string.\"\"\"\n    if shortcut is None:\n        return []\n    if isinstance(shortcut, (tuple, list)):\n        return [_get_qkeysequence(s) for s in shortcut]\n    assert isinstance(shortcut, string_types)\n    if hasattr(QKeySequence, shortcut):\n        return QKeySequence(getattr(QKeySequence, shortcut))\n    sequence = QKeySequence.fromString(shortcut)\n    assert not sequence.isEmpty()\n    return sequence"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add(self, callback=None, name=None, shortcut=None, alias=None,\n            docstring=None, menu=None, verbose=True):\n        \"\"\"Add an action with a keyboard shortcut.\"\"\"\n        if callback is None:\n            # Allow to use either add(func) or @add or @add(...).\n            return partial(self.add, name=name, shortcut=shortcut,\n                           alias=alias, menu=menu)\n        assert callback\n\n        # Get the name from the callback function if needed.\n        name = name or callback.__name__\n        alias = alias or _alias(name)\n        name = name.replace('&', '')\n        shortcut = shortcut or self._default_shortcuts.get(name, None)\n\n        # Skip existing action.\n        if name in self._actions_dict:\n            return\n\n        # Set the status tip from the function's docstring.\n        docstring = docstring or callback.__doc__ or name\n        docstring = re.sub(r'[ \\t\\r\\f\\v]{2,}', ' ', docstring.strip())\n\n        # Create and register the action.\n        action = _create_qaction(self.gui, name, callback,\n                                 shortcut,\n                                 docstring=docstring,\n                                 alias=alias,\n                                 )\n        action_obj = Bunch(qaction=action, name=name, alias=alias,\n                           shortcut=shortcut, callback=callback, menu=menu)\n        if verbose and not name.startswith('_'):\n            logger.log(5, \"Add action `%s` (%s).\", name,\n                       _get_shortcut_string(action.shortcut()))\n        self.gui.addAction(action)\n        # Add the action to the menu.\n        menu = menu or self.menu\n        # Do not show private actions in the menu.\n        if menu and not name.startswith('_'):\n            self.gui.get_menu(menu).addAction(action)\n        self._actions_dict[name] = action_obj\n        # Register the alias -> name mapping.\n        self._aliases[alias] = name\n\n        # Set the callback method.\n        if callback:\n            setattr(self, name, callback)", "response": "Add an action with a keyboard shortcut."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef disable(self, name=None):\n        if name is None:\n            for name in self._actions_dict:\n                self.disable(name)\n            return\n        self._actions_dict[name].qaction.setEnabled(False)", "response": "Disable one or all actions."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nenable one or all actions.", "response": "def enable(self, name=None):\n        \"\"\"Enable one or all actions.\"\"\"\n        if name is None:\n            for name in self._actions_dict:\n                self.enable(name)\n            return\n        self._actions_dict[name].qaction.setEnabled(True)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run(self, name, *args):\n        assert isinstance(name, string_types)\n        # Resolve the alias if it is an alias.\n        name = self._aliases.get(name, name)\n        # Get the action.\n        action = self._actions_dict.get(name, None)\n        if not action:\n            raise ValueError(\"Action `{}` doesn't exist.\".format(name))\n        if not name.startswith('_'):\n            logger.debug(\"Execute action `%s`.\", name)\n        return action.callback(*args)", "response": "Execute an action as specified by its name."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef shortcuts(self):\n        return {name: action.shortcut\n                for name, action in self._actions_dict.items()}", "response": "A dictionary of action shortcuts."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _backspace(self):\n        if self.command == ':':\n            return\n        logger.log(5, \"Snippet keystroke `Backspace`.\")\n        self.command = self.command[:-1]", "response": "Erase the last character in the snippet command."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _enter(self):\n        command = self.command\n        logger.log(5, \"Snippet keystroke `Enter`.\")\n        # NOTE: we need to set back the actions (mode_off) before running\n        # the command.\n        self.mode_off()\n        self.run(command)", "response": "Disable the snippet mode and execute the command."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate mock Qt actions for snippet keystrokes.", "response": "def _create_snippet_actions(self):\n        \"\"\"Add mock Qt actions for snippet keystrokes.\n\n        Used to enable snippet mode.\n\n        \"\"\"\n        # One action per allowed character.\n        for i, char in enumerate(self._snippet_chars):\n\n            def _make_func(char):\n                def callback():\n                    logger.log(5, \"Snippet keystroke `%s`.\", char)\n                    self.command += char\n                return callback\n\n            self.actions.add(name='_snippet_{}'.format(i),\n                             shortcut=char,\n                             callback=_make_func(char))\n\n        self.actions.add(name='_snippet_backspace',\n                         shortcut='backspace',\n                         callback=self._backspace)\n        self.actions.add(name='_snippet_activate',\n                         shortcut=('enter', 'return'),\n                         callback=self._enter)\n        self.actions.add(name='_snippet_disable',\n                         shortcut='escape',\n                         callback=self.mode_off)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef run(self, snippet):\n        assert snippet[0] == ':'\n        snippet = snippet[1:]\n        snippet_args = _parse_snippet(snippet)\n        name = snippet_args[0]\n\n        logger.info(\"Processing snippet `%s`.\", snippet)\n        try:\n            # Try to run the snippet on all attached Actions instances.\n            for actions in self.gui.actions:\n                try:\n                    actions.run(name, *snippet_args[1:])\n                    return\n                except ValueError:\n                    # This Actions instance doesn't contain the requested\n                    # snippet, trying the next attached Actions instance.\n                    pass\n            logger.warn(\"Couldn't find action `%s`.\", name)\n        except Exception as e:\n            logger.warn(\"Error when executing snippet: \\\"%s\\\".\", str(e))\n            logger.debug(''.join(traceback.format_exception(*sys.exc_info())))", "response": "Executes a snippet command."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _before_after(n_samples):\n    if not isinstance(n_samples, (tuple, list)):\n        before = n_samples // 2\n        after = n_samples - before\n    else:\n        assert len(n_samples) == 2\n        before, after = n_samples\n        n_samples = before + after\n    assert before >= 0\n    assert after >= 0\n    assert before + after == n_samples\n    return before, after", "response": "Get the number of samples before and after."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a waveform slice.", "response": "def _slice(index, n_samples, margin=None):\n    \"\"\"Return a waveform slice.\"\"\"\n    if margin is None:\n        margin = (0, 0)\n    assert isinstance(n_samples, (tuple, list))\n    assert len(n_samples) == 2\n    before, after = n_samples\n    assert isinstance(margin, (tuple, list))\n    assert len(margin) == 2\n    margin_before, margin_after = margin\n    before += margin_before\n    after += margin_after\n    index = int(index)\n    before = int(before)\n    after = int(after)\n    return slice(max(0, index - before), index + after, None)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads a waveform at a given time.", "response": "def _load_at(self, time, channels=None):\n        \"\"\"Load a waveform at a given time.\"\"\"\n        if channels is None:\n            channels = slice(None, None, None)\n        time = int(time)\n        time_o = time\n        ns = self.n_samples_trace\n        if not (0 <= time_o < ns):\n            raise ValueError(\"Invalid time {0:d}/{1:d}.\".format(time_o, ns))\n        slice_extract = _slice(time_o,\n                               self.n_samples_before_after,\n                               self._filter_margin)\n        extract = self._traces[slice_extract][:, channels].astype(np.float32)\n\n        # Pad the extracted chunk if needed.\n        if slice_extract.start <= 0:\n            extract = _pad(extract, self._n_samples_extract, 'left')\n        elif slice_extract.stop >= ns - 1:\n            extract = _pad(extract, self._n_samples_extract, 'right')\n\n        assert extract.shape[0] == self._n_samples_extract\n        return extract"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nload the waveforms of the specified spikes.", "response": "def get(self, spike_ids, channels=None):\n        \"\"\"Load the waveforms of the specified spikes.\"\"\"\n        if isinstance(spike_ids, slice):\n            spike_ids = _range_from_slice(spike_ids,\n                                          start=0,\n                                          stop=self.n_spikes,\n                                          )\n        if not hasattr(spike_ids, '__len__'):\n            spike_ids = [spike_ids]\n        if channels is None:\n            channels = slice(None, None, None)\n            nc = self.n_channels\n        else:\n            channels = np.asarray(channels, dtype=np.int32)\n            assert np.all(channels < self.n_channels)\n            nc = len(channels)\n\n        # Ensure a list of time samples are being requested.\n        spike_ids = _as_array(spike_ids)\n        n_spikes = len(spike_ids)\n\n        # Initialize the array.\n        # NOTE: last dimension is time to simplify things.\n        shape = (n_spikes, nc, self._n_samples_extract)\n        waveforms = np.zeros(shape, dtype=np.float32)\n\n        # No traces: return null arrays.\n        if self.n_samples_trace == 0:\n            return np.transpose(waveforms, (0, 2, 1))\n\n        # Load all spikes.\n        for i, spike_id in enumerate(spike_ids):\n            assert 0 <= spike_id < self.n_spikes\n            time = self._spike_samples[spike_id]\n\n            # Extract the waveforms on the unmasked channels.\n            try:\n                w = self._load_at(time, channels)\n            except ValueError as e:  # pragma: no cover\n                logger.warn(\"Error while loading waveform: %s\", str(e))\n                continue\n\n            assert w.shape == (self._n_samples_extract, nc)\n\n            waveforms[i, :, :] = w.T\n\n        # Filter the waveforms.\n        waveforms_f = waveforms.reshape((-1, self._n_samples_extract))\n        # Only filter the non-zero waveforms.\n        unmasked = waveforms_f.max(axis=1) != 0\n        waveforms_f[unmasked] = self._filter(waveforms_f[unmasked], axis=1)\n        waveforms_f = waveforms_f.reshape((n_spikes, nc,\n                                           self._n_samples_extract))\n\n        # Remove the margin.\n        margin_before, margin_after = self._filter_margin\n        if margin_after > 0:\n            assert margin_before >= 0\n            waveforms_f = waveforms_f[:, :, margin_before:-margin_after]\n\n        assert waveforms_f.shape == (n_spikes,\n                                     nc,\n                                     self.n_samples_waveforms,\n                                     )\n\n        # NOTE: we transpose before returning the array.\n        return np.transpose(waveforms_f, (0, 2, 1))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_waveform_amplitude(mean_masks, mean_waveforms):\n\n    assert mean_waveforms.ndim == 2\n    n_samples, n_channels = mean_waveforms.shape\n\n    assert mean_masks.ndim == 1\n    assert mean_masks.shape == (n_channels,)\n\n    mean_waveforms = mean_waveforms * mean_masks\n    assert mean_waveforms.shape == (n_samples, n_channels)\n\n    # Amplitudes.\n    m, M = mean_waveforms.min(axis=0), mean_waveforms.max(axis=0)\n    return M - m", "response": "Return the amplitude of the waveforms on all channels."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_mean_masked_features_distance(mean_features_0,\n                                      mean_features_1,\n                                      mean_masks_0,\n                                      mean_masks_1,\n                                      n_features_per_channel=None,\n                                      ):\n    \"\"\"Compute the distance between the mean masked features.\"\"\"\n\n    assert n_features_per_channel > 0\n\n    mu_0 = mean_features_0.ravel()\n    mu_1 = mean_features_1.ravel()\n\n    omeg_0 = mean_masks_0\n    omeg_1 = mean_masks_1\n\n    omeg_0 = np.repeat(omeg_0, n_features_per_channel)\n    omeg_1 = np.repeat(omeg_1, n_features_per_channel)\n\n    d_0 = mu_0 * omeg_0\n    d_1 = mu_1 * omeg_1\n\n    return np.linalg.norm(d_0 - d_1)", "response": "Compute the distance between the mean masked features."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _extend_spikes(spike_ids, spike_clusters):\n    # We find the spikes belonging to modified clusters.\n    # What are the old clusters that are modified by the assignment?\n    old_spike_clusters = spike_clusters[spike_ids]\n    unique_clusters = _unique(old_spike_clusters)\n    # Now we take all spikes from these clusters.\n    changed_spike_ids = _spikes_in_clusters(spike_clusters, unique_clusters)\n    # These are the new spikes that need to be reassigned.\n    extended_spike_ids = np.setdiff1d(changed_spike_ids, spike_ids,\n                                      assume_unique=True)\n    return extended_spike_ids", "response": "Return all spikes belonging to the specified cluster containing the specified\n    spikes."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconcatenating a list of pairs ( spike_ids spike_clusters ).", "response": "def _concatenate_spike_clusters(*pairs):\n    \"\"\"Concatenate a list of pairs (spike_ids, spike_clusters).\"\"\"\n    pairs = [(_as_array(x), _as_array(y)) for (x, y) in pairs]\n    concat = np.vstack(np.hstack((x[:, None], y[:, None]))\n                       for x, y in pairs)\n    reorder = np.argsort(concat[:, 0])\n    concat = concat[reorder, :]\n    return concat[:, 0].astype(np.int64), concat[:, 1].astype(np.int64)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreset the clustering to the original clustering.", "response": "def reset(self):\n        \"\"\"Reset the clustering to the original clustering.\n\n        All changes are lost.\n\n        \"\"\"\n        self._undo_stack.clear()\n        self._spike_clusters = self._spike_clusters_base\n        self._new_cluster_id = self._new_cluster_id_0"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nassigns a new set of spikes to a new set of clusters.", "response": "def _do_assign(self, spike_ids, new_spike_clusters):\n        \"\"\"Make spike-cluster assignments after the spike selection has\n        been extended to full clusters.\"\"\"\n\n        # Ensure spike_clusters has the right shape.\n        spike_ids = _as_array(spike_ids)\n        if len(new_spike_clusters) == 1 and len(spike_ids) > 1:\n            new_spike_clusters = (np.ones(len(spike_ids), dtype=np.int64) *\n                                  new_spike_clusters[0])\n        old_spike_clusters = self._spike_clusters[spike_ids]\n\n        assert len(spike_ids) == len(old_spike_clusters)\n        assert len(new_spike_clusters) == len(spike_ids)\n\n        # Update the spikes per cluster structure.\n        old_clusters = _unique(old_spike_clusters)\n\n        # NOTE: shortcut to a merge if this assignment is effectively a merge\n        # i.e. if all spikes are assigned to a single cluster.\n        # The fact that spike selection has been previously extended to\n        # whole clusters is critical here.\n        new_clusters = _unique(new_spike_clusters)\n        if len(new_clusters) == 1:\n            return self._do_merge(spike_ids, old_clusters, new_clusters[0])\n\n        # We return the UpdateInfo structure.\n        up = _assign_update_info(spike_ids,\n                                 old_spike_clusters,\n                                 new_spike_clusters)\n\n        # We update the new cluster id (strictly increasing during a session).\n        self._new_cluster_id = max(self._new_cluster_id, max(up.added) + 1)\n\n        # We make the assignments.\n        self._spike_clusters[spike_ids] = new_spike_clusters\n        # OPTIM: we update spikes_per_cluster manually.\n        new_spc = _spikes_per_cluster(new_spike_clusters, spike_ids)\n        self._update_cluster_ids(to_remove=old_clusters, to_add=new_spc)\n        return up"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef merge(self, cluster_ids, to=None):\n\n        if not _is_array_like(cluster_ids):\n            raise ValueError(\"The first argument should be a list or \"\n                             \"an array.\")\n\n        cluster_ids = sorted(cluster_ids)\n        if not set(cluster_ids) <= set(self.cluster_ids):\n            raise ValueError(\"Some clusters do not exist.\")\n\n        # Find the new cluster number.\n        if to is None:\n            to = self.new_cluster_id()\n        if to < self.new_cluster_id():\n            raise ValueError(\"The new cluster numbers should be higher than \"\n                             \"{0}.\".format(self.new_cluster_id()))\n\n        # NOTE: we could have called self.assign() here, but we don't.\n        # We circumvent self.assign() for performance reasons.\n        # assign() is a relatively costly operation, whereas merging is a much\n        # cheaper operation.\n\n        # Find all spikes in the specified clusters.\n        spike_ids = _spikes_in_clusters(self.spike_clusters, cluster_ids)\n\n        up = self._do_merge(spike_ids, cluster_ids, to)\n        undo_state = self.emit('request_undo_state', up)\n\n        # Add to stack.\n        self._undo_stack.add((spike_ids, [to], undo_state))\n\n        self.emit('cluster', up)\n        return up", "response": "Merge several clusters into a new cluster."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nassigning a new set of spikes to a new cluster.", "response": "def assign(self, spike_ids, spike_clusters_rel=0):\n        \"\"\"Make new spike cluster assignments.\n\n        Parameters\n        ----------\n\n        spike_ids : array-like\n            List of spike ids.\n        spike_clusters_rel : array-like\n            Relative cluster ids of the spikes in `spike_ids`. This\n            must have the same size as `spike_ids`.\n\n        Returns\n        -------\n\n        up : UpdateInfo instance\n\n        Note\n        ----\n\n        `spike_clusters_rel` contain *relative* cluster indices. Their values\n        don't matter: what matters is whether two give spikes\n        should end up in the same cluster or not. Adding a constant number\n        to all elements in `spike_clusters_rel` results in exactly the same\n        operation.\n\n        The final cluster ids are automatically generated by the `Clustering`\n        class. This is because we must ensure that all modified clusters\n        get brand new ids. The whole library is based on the assumption that\n        cluster ids are unique and \"disposable\". Changing a cluster always\n        results in a new cluster id being assigned.\n\n        If a spike is assigned to a new cluster, then all other spikes\n        belonging to the same cluster are assigned to a brand new cluster,\n        even if they were not changed explicitely by the `assign()` method.\n\n        In other words, the list of spikes affected by an `assign()` is almost\n        always a strict superset of the `spike_ids` parameter. The only case\n        where this is not true is when whole clusters change: this is called\n        a merge. It is implemented in a separate `merge()` method because it\n        is logically much simpler, and faster to execute.\n\n        \"\"\"\n\n        assert not isinstance(spike_ids, slice)\n\n        # Ensure `spike_clusters_rel` is an array-like.\n        if not hasattr(spike_clusters_rel, '__len__'):\n            spike_clusters_rel = spike_clusters_rel * np.ones(len(spike_ids),\n                                                              dtype=np.int64)\n\n        spike_ids = _as_array(spike_ids)\n        if len(spike_ids) == 0:\n            return UpdateInfo()\n        assert len(spike_ids) == len(spike_clusters_rel)\n        assert spike_ids.min() >= 0\n        assert spike_ids.max() < self._n_spikes, \"Some spikes don't exist.\"\n\n        # Normalize the spike-cluster assignment such that\n        # there are only new or dead clusters, not modified clusters.\n        # This implies that spikes not explicitly selected, but that\n        # belong to clusters affected by the operation, will be assigned\n        # to brand new clusters.\n        spike_ids, cluster_ids = _extend_assignment(spike_ids,\n                                                    self._spike_clusters,\n                                                    spike_clusters_rel,\n                                                    self.new_cluster_id(),\n                                                    )\n\n        up = self._do_assign(spike_ids, cluster_ids)\n        undo_state = self.emit('request_undo_state', up)\n\n        # Add the assignment to the undo stack.\n        self._undo_stack.add((spike_ids, cluster_ids, undo_state))\n\n        self.emit('cluster', up)\n        return up"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef undo(self):\n        _, _, undo_state = self._undo_stack.back()\n\n        # Retrieve the initial spike_cluster structure.\n        spike_clusters_new = self._spike_clusters_base.copy()\n\n        # Loop over the history (except the last item because we undo).\n        for spike_ids, cluster_ids, _ in self._undo_stack:\n            # We update the spike clusters accordingly.\n            if spike_ids is not None:\n                spike_clusters_new[spike_ids] = cluster_ids\n\n        # What are the spikes affected by the last changes?\n        changed = np.nonzero(self._spike_clusters !=\n                             spike_clusters_new)[0]\n        clusters_changed = spike_clusters_new[changed]\n\n        up = self._do_assign(changed, clusters_changed)\n        up.history = 'undo'\n        # Add the undo_state object from the undone object.\n        up.undo_state = undo_state\n\n        self.emit('cluster', up)\n        return up", "response": "Undo the last cluster assignment operation. Returns ------- up : UpdateInfo instance of the changes done by this operation."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nredo the last cluster assignment operation. Returns the last cluster assignment operation.", "response": "def redo(self):\n        \"\"\"Redo the last cluster assignment operation.\n\n        Returns\n        -------\n\n        up : UpdateInfo instance of the changes done by this operation.\n\n        \"\"\"\n        # Go forward in the stack, and retrieve the new assignment.\n        item = self._undo_stack.forward()\n        if item is None:\n            # No redo has been performed: abort.\n            return\n\n        # NOTE: the undo_state object is only returned when undoing.\n        # It represents data associated to the state\n        # *before* the action. What might be more useful would be the\n        # undo_state object of the next item in the list (if it exists).\n        spike_ids, cluster_ids, undo_state = item\n        assert spike_ids is not None\n\n        # We apply the new assignment.\n        up = self._do_assign(spike_ids, cluster_ids)\n        up.history = 'redo'\n\n        self.emit('cluster', up)\n        return up"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nincrements some indices in a 1D vector of non - negative integers.", "response": "def _increment(arr, indices):\n    \"\"\"Increment some indices in a 1D vector of non-negative integers.\n    Repeated indices are taken into account.\"\"\"\n    arr = _as_array(arr)\n    indices = _as_array(indices)\n    bbins = np.bincount(indices)\n    arr[:len(bbins)] += bbins\n    return arr"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _symmetrize_correlograms(correlograms):\n\n    n_clusters, _, n_bins = correlograms.shape\n    assert n_clusters == _\n\n    # We symmetrize c[i, j, 0].\n    # This is necessary because the algorithm in correlograms()\n    # is sensitive to the order of identical spikes.\n    correlograms[..., 0] = np.maximum(correlograms[..., 0],\n                                      correlograms[..., 0].T)\n\n    sym = correlograms[..., 1:][..., ::-1]\n    sym = np.transpose(sym, (1, 0, 2))\n\n    return np.dstack((sym, correlograms))", "response": "Return the symmetrized version of the CCG arrays."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef correlograms(spike_times,\n                 spike_clusters,\n                 cluster_ids=None,\n                 sample_rate=1.,\n                 bin_size=None,\n                 window_size=None,\n                 symmetrize=True,\n                 ):\n    \"\"\"Compute all pairwise cross-correlograms among the clusters appearing\n    in `spike_clusters`.\n\n    Parameters\n    ----------\n\n    spike_times : array-like\n        Spike times in seconds.\n    spike_clusters : array-like\n        Spike-cluster mapping.\n    cluster_ids : array-like\n        The list of unique clusters, in any order. That order will be used\n        in the output array.\n    bin_size : float\n        Size of the bin, in seconds.\n    window_size : float\n        Size of the window, in seconds.\n\n    Returns\n    -------\n\n    correlograms : array\n        A `(n_clusters, n_clusters, winsize_samples)` array with all pairwise\n        CCGs.\n\n    \"\"\"\n    assert sample_rate > 0.\n    assert np.all(np.diff(spike_times) >= 0), (\"The spike times must be \"\n                                               \"increasing.\")\n\n    # Get the spike samples.\n    spike_times = np.asarray(spike_times, dtype=np.float64)\n    spike_samples = (spike_times * sample_rate).astype(np.int64)\n\n    spike_clusters = _as_array(spike_clusters)\n\n    assert spike_samples.ndim == 1\n    assert spike_samples.shape == spike_clusters.shape\n\n    # Find `binsize`.\n    bin_size = np.clip(bin_size, 1e-5, 1e5)  # in seconds\n    binsize = int(sample_rate * bin_size)  # in samples\n    assert binsize >= 1\n\n    # Find `winsize_bins`.\n    window_size = np.clip(window_size, 1e-5, 1e5)  # in seconds\n    winsize_bins = 2 * int(.5 * window_size / bin_size) + 1\n\n    assert winsize_bins >= 1\n    assert winsize_bins % 2 == 1\n\n    # Take the cluster oder into account.\n    if cluster_ids is None:\n        clusters = _unique(spike_clusters)\n    else:\n        clusters = _as_array(cluster_ids)\n    n_clusters = len(clusters)\n\n    # Like spike_clusters, but with 0..n_clusters-1 indices.\n    spike_clusters_i = _index_of(spike_clusters, clusters)\n\n    # Shift between the two copies of the spike trains.\n    shift = 1\n\n    # At a given shift, the mask precises which spikes have matching spikes\n    # within the correlogram time window.\n    mask = np.ones_like(spike_samples, dtype=np.bool)\n\n    correlograms = _create_correlograms_array(n_clusters, winsize_bins)\n\n    # The loop continues as long as there is at least one spike with\n    # a matching spike.\n    while mask[:-shift].any():\n        # Number of time samples between spike i and spike i+shift.\n        spike_diff = _diff_shifted(spike_samples, shift)\n\n        # Binarize the delays between spike i and spike i+shift.\n        spike_diff_b = spike_diff // binsize\n\n        # Spikes with no matching spikes are masked.\n        mask[:-shift][spike_diff_b > (winsize_bins // 2)] = False\n\n        # Cache the masked spike delays.\n        m = mask[:-shift].copy()\n        d = spike_diff_b[m]\n\n        # # Update the masks given the clusters to update.\n        # m0 = np.in1d(spike_clusters[:-shift], clusters)\n        # m = m & m0\n        # d = spike_diff_b[m]\n        d = spike_diff_b[m]\n\n        # Find the indices in the raveled correlograms array that need\n        # to be incremented, taking into account the spike clusters.\n        indices = np.ravel_multi_index((spike_clusters_i[:-shift][m],\n                                        spike_clusters_i[+shift:][m],\n                                        d),\n                                       correlograms.shape)\n\n        # Increment the matching spikes in the correlograms array.\n        _increment(correlograms.ravel(), indices)\n\n        shift += 1\n\n    # Remove ACG peaks.\n    correlograms[np.arange(n_clusters),\n                 np.arange(n_clusters),\n                 0] = 0\n\n    if symmetrize:\n        return _symmetrize_correlograms(correlograms)\n    else:\n        return correlograms", "response": "Compute all pairwise cross - correlograms among the clusters appearing in a given spike times."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the bin and window sizes.", "response": "def set_bin_window(self, bin_size=None, window_size=None):\n        \"\"\"Set the bin and window sizes.\"\"\"\n        bin_size = bin_size or self.bin_size\n        window_size = window_size or self.window_size\n        assert 1e-6 < bin_size < 1e3\n        assert 1e-6 < window_size < 1e3\n        assert bin_size < window_size\n        self.bin_size = bin_size\n        self.window_size = window_size\n        # Set the status message.\n        b, w = self.bin_size * 1000, self.window_size * 1000\n        self.set_status('Bin: {:.1f} ms. Window: {:.1f} ms.'.format(b, w))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef attach(self, gui):\n        super(CorrelogramView, self).attach(gui)\n        self.actions.add(self.toggle_normalization, shortcut='n')\n        self.actions.separator()\n        self.actions.add(self.set_bin, alias='cb')\n        self.actions.add(self.set_window, alias='cw')", "response": "Attach the view to the GUI."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _md5(path, blocksize=2 ** 20):\n    m = hashlib.md5()\n    with open(path, 'rb') as f:\n        while True:\n            buf = f.read(blocksize)\n            if not buf:\n                break\n            m.update(buf)\n    return m.hexdigest()", "response": "Compute the checksum of a file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndownload a binary file from an URL and save it to a file.", "response": "def download_file(url, output_path):\n    \"\"\"Download a binary file from an URL.\n\n    The checksum will be downloaded from `URL + .md5`. If this download\n    succeeds, the file's MD5 will be compared to the expected checksum.\n\n    Parameters\n    ----------\n\n    url : str\n        The file's URL.\n    output_path : str\n        The path where the file is to be saved.\n\n    \"\"\"\n    output_path = op.realpath(output_path)\n    assert output_path is not None\n    if op.exists(output_path):\n        checked = _check_md5_of_url(output_path, url)\n        if checked is False:\n            logger.debug(\"The file `%s` already exists \"\n                         \"but is invalid: redownloading.\", output_path)\n        elif checked is True:\n            logger.debug(\"The file `%s` already exists: skipping.\",\n                         output_path)\n            return output_path\n    r = _download(url, stream=True)\n    _save_stream(r, output_path)\n    if _check_md5_of_url(output_path, url) is False:\n        logger.debug(\"The checksum doesn't match: retrying the download.\")\n        r = _download(url, stream=True)\n        _save_stream(r, output_path)\n        if _check_md5_of_url(output_path, url) is False:\n            raise RuntimeError(\"The checksum of the downloaded file \"\n                               \"doesn't match the provided checksum.\")\n    return"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _make_class(cls, **kwargs):\n    kwargs = {k: (v if v is not None else getattr(cls, k, None))\n              for k, v in kwargs.items()}\n    # The class name contains a hash of the custom parameters.\n    name = cls.__name__ + '_' + _hash(kwargs)\n    if name not in _CLASSES:\n        logger.log(5, \"Create class %s %s.\", name, kwargs)\n        cls = type(name, (cls,), kwargs)\n        _CLASSES[name] = cls\n    return _CLASSES[name]", "response": "Return a custom Visual class with given parameters."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a plot item.", "response": "def _add_item(self, cls, *args, **kwargs):\n        \"\"\"Add a plot item.\"\"\"\n        box_index = kwargs.pop('box_index', self._default_box_index)\n\n        data = cls.validate(*args, **kwargs)\n        n = cls.vertex_count(**data)\n\n        if not isinstance(box_index, np.ndarray):\n            k = len(self._default_box_index)\n            box_index = _get_array(box_index, (n, k))\n        data['box_index'] = box_index\n\n        if cls not in self._items:\n            self._items[cls] = []\n        self._items[cls].append(data)\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef scatter(self, *args, **kwargs):\n        cls = _make_class(ScatterVisual,\n                          _default_marker=kwargs.pop('marker', None),\n                          )\n        return self._add_item(cls, *args, **kwargs)", "response": "Add a scatter plot."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef build(self):\n        for cls, data_list in self._items.items():\n            # Some variables are not concatenated. They are specified\n            # in `allow_list`.\n            data = _accumulate(data_list, cls.allow_list)\n            box_index = data.pop('box_index')\n            visual = cls()\n            self.add_visual(visual)\n            visual.set_data(**data)\n            # NOTE: visual.program.__contains__ is implemented in vispy master\n            # so we can replace this with `if 'a_box_index' in visual.program`\n            # after the next VisPy release.\n            if 'a_box_index' in visual.program._code_variables:\n                visual.program['a_box_index'] = box_index.astype(np.float32)\n        # TODO: refactor this when there is the possibility to update existing\n        # visuals without recreating the whole scene.\n        if self.lasso:\n            self.lasso.create_visual()\n        self.update()", "response": "Build all added items."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _range_from_slice(myslice, start=None, stop=None, step=None, length=None):\n    assert isinstance(myslice, slice)\n    # Find 'step'.\n    step = myslice.step if myslice.step is not None else step\n    if step is None:\n        step = 1\n    # Find 'start'.\n    start = myslice.start if myslice.start is not None else start\n    if start is None:\n        start = 0\n    # Find 'stop' as a function of length if 'stop' is unspecified.\n    stop = myslice.stop if myslice.stop is not None else stop\n    if length is not None:\n        stop_inferred = floor(start + step * length)\n        if stop is not None and stop < stop_inferred:\n            raise ValueError(\"'stop' ({stop}) and \".format(stop=stop) +\n                             \"'length' ({length}) \".format(length=length) +\n                             \"are not compatible.\")\n        stop = stop_inferred\n    if stop is None and length is None:\n        raise ValueError(\"'stop' and 'length' cannot be both unspecified.\")\n    myrange = np.arange(start, stop, step)\n    # Check the length if it was specified.\n    if length is not None:\n        assert len(myrange) == length\n    return myrange", "response": "Convert a slice to an array of integers."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _unique(x):\n    if x is None or len(x) == 0:\n        return np.array([], dtype=np.int64)\n    # WARNING: only keep positive values.\n    # cluster=-1 means \"unclustered\".\n    x = _as_array(x)\n    x = x[x >= 0]\n    bc = np.bincount(x)\n    return np.nonzero(bc)[0]", "response": "Faster version of np. unique."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nnormalizes an array into [ 0 1 ).", "response": "def _normalize(arr, keep_ratio=False):\n    \"\"\"Normalize an array into [0, 1].\"\"\"\n    (x_min, y_min), (x_max, y_max) = arr.min(axis=0), arr.max(axis=0)\n\n    if keep_ratio:\n        a = 1. / max(x_max - x_min, y_max - y_min)\n        ax = ay = a\n        bx = .5 - .5 * a * (x_max + x_min)\n        by = .5 - .5 * a * (y_max + y_min)\n    else:\n        ax = 1. / (x_max - x_min)\n        ay = 1. / (y_max - y_min)\n        bx = -x_min / (x_max - x_min)\n        by = -y_min / (y_max - y_min)\n\n    arr_n = arr.copy()\n    arr_n[:, 0] *= ax\n    arr_n[:, 0] += bx\n    arr_n[:, 1] *= ay\n    arr_n[:, 1] += by\n\n    return arr_n"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _index_of(arr, lookup):\n    # Equivalent of np.digitize(arr, lookup) - 1, but much faster.\n    # TODO: assertions to disable in production for performance reasons.\n    # TODO: np.searchsorted(lookup, arr) is faster on small arrays with large\n    # values\n    lookup = np.asarray(lookup, dtype=np.int32)\n    m = (lookup.max() if len(lookup) else 0) + 1\n    tmp = np.zeros(m + 1, dtype=np.int)\n    # Ensure that -1 values are kept.\n    tmp[-1] = -1\n    if len(lookup):\n        tmp[lookup] = np.arange(len(lookup))\n    return tmp[arr]", "response": "Replace scalars in an array by their indices in a lookup table."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\npad an array with zeros along the first axis.", "response": "def _pad(arr, n, dir='right'):\n    \"\"\"Pad an array with zeros along the first axis.\n\n    Parameters\n    ----------\n\n    n : int\n        Size of the returned array in the first axis.\n    dir : str\n        Direction of the padding. Must be one 'left' or 'right'.\n\n    \"\"\"\n    assert dir in ('left', 'right')\n    if n < 0:\n        raise ValueError(\"'n' must be positive: {0}.\".format(n))\n    elif n == 0:\n        return np.zeros((0,) + arr.shape[1:], dtype=arr.dtype)\n    n_arr = arr.shape[0]\n    shape = (n,) + arr.shape[1:]\n    if n_arr == n:\n        assert arr.shape == shape\n        return arr\n    elif n_arr < n:\n        out = np.zeros(shape, dtype=arr.dtype)\n        if dir == 'left':\n            out[-n_arr:, ...] = arr\n        elif dir == 'right':\n            out[:n_arr, ...] = arr\n        assert out.shape == shape\n        return out\n    else:\n        if dir == 'left':\n            out = arr[-n:, ...]\n        elif dir == 'right':\n            out = arr[:n, ...]\n        assert out.shape == shape\n        return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_padded(data, start, end):\n    if start < 0 and end > data.shape[0]:\n        raise RuntimeError()\n    if start < 0:\n        start_zeros = np.zeros((-start, data.shape[1]),\n                               dtype=data.dtype)\n        return np.vstack((start_zeros, data[:end]))\n    elif end > data.shape[0]:\n        end_zeros = np.zeros((end - data.shape[0], data.shape[1]),\n                             dtype=data.dtype)\n        return np.vstack((data[start:], end_zeros))\n    else:\n        return data[start:end]", "response": "Return data with zeros outside array bounds\nWorkItem"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _in_polygon(points, polygon):\n    from matplotlib.path import Path\n    points = _as_array(points)\n    polygon = _as_array(polygon)\n    assert points.ndim == 2\n    assert polygon.ndim == 2\n    if len(polygon):\n        polygon = np.vstack((polygon, polygon[0]))\n    path = Path(polygon, closed=True)\n    return path.contains_points(points)", "response": "Return the points that are inside a polygon."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a list of pairs ( cluster similarity ) sorted by decreasing similarity to a given cluster.", "response": "def get_closest_clusters(cluster_id, cluster_ids, sim_func, max_n=None):\n    \"\"\"Return a list of pairs `(cluster, similarity)` sorted by decreasing\n    similarity to a given cluster.\"\"\"\n    l = [(_as_scalar(candidate), _as_scalar(sim_func(cluster_id, candidate)))\n         for candidate in _as_scalars(cluster_ids)]\n    l = sorted(l, key=itemgetter(1), reverse=True)\n    max_n = None or len(l)\n    return l[:max_n]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading a. npy array.", "response": "def read_array(path, mmap_mode=None):\n    \"\"\"Read a .npy array.\"\"\"\n    file_ext = op.splitext(path)[1]\n    if file_ext == '.npy':\n        return np.load(path, mmap_mode=mmap_mode)\n    raise NotImplementedError(\"The file extension `{}` \".format(file_ext) +\n                              \"is not currently supported.\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwriting an array to a. npy file.", "response": "def write_array(path, arr):\n    \"\"\"Write an array to a .npy file.\"\"\"\n    file_ext = op.splitext(path)[1]\n    if file_ext == '.npy':\n        return np.save(path, arr)\n    raise NotImplementedError(\"The file extension `{}` \".format(file_ext) +\n                              \"is not currently supported.\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _start_stop(item):\n    if isinstance(item, tuple):\n        item = item[0]\n    if isinstance(item, slice):\n        # Slice.\n        if item.step not in (None, 1):\n            raise NotImplementedError()\n        return item.start, item.stop\n    elif isinstance(item, (list, np.ndarray)):\n        # List or array of indices.\n        return np.min(item), np.max(item)\n    else:\n        # Integer.\n        return item, item + 1", "response": "Find the start and stop indices of a __getitem__ item."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a virtual concatenate of several NumPy arrays.", "response": "def _concatenate_virtual_arrays(arrs, cols=None, scaling=None):\n    \"\"\"Return a virtual concatenate of several NumPy arrays.\"\"\"\n    return None if not len(arrs) else ConcatenatedArrays(arrs, cols,\n                                                         scaling=scaling)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _excerpt_step(n_samples, n_excerpts=None, excerpt_size=None):\n    assert n_excerpts >= 2\n    step = max((n_samples - excerpt_size) // (n_excerpts - 1),\n               excerpt_size)\n    return step", "response": "Compute the step of an excerpt set as a function of the number\n    of excerpts and their sizes."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nyields the bounds of a single chunk.", "response": "def chunk_bounds(n_samples, chunk_size, overlap=0):\n    \"\"\"Return chunk bounds.\n\n    Chunks have the form:\n\n        [ overlap/2 | chunk_size-overlap | overlap/2 ]\n        s_start   keep_start           keep_end     s_end\n\n    Except for the first and last chunks which do not have a left/right\n    overlap.\n\n    This generator yields (s_start, s_end, keep_start, keep_end).\n\n    \"\"\"\n    s_start = 0\n    s_end = chunk_size\n    keep_start = s_start\n    keep_end = s_end - overlap // 2\n    yield s_start, s_end, keep_start, keep_end\n\n    while s_end - overlap + chunk_size < n_samples:\n        s_start = s_end - overlap\n        s_end = s_start + chunk_size\n        keep_start = keep_end\n        keep_end = s_end - overlap // 2\n        if s_start < s_end:\n            yield s_start, s_end, keep_start, keep_end\n\n    s_start = s_end - overlap\n    s_end = n_samples\n    keep_start = keep_end\n    keep_end = s_end\n    if s_start < s_end:\n        yield s_start, s_end, keep_start, keep_end"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nyields n_excerpts excerpt locations in a sequence of n_samples.", "response": "def excerpts(n_samples, n_excerpts=None, excerpt_size=None):\n    \"\"\"Yield (start, end) where start is included and end is excluded.\"\"\"\n    assert n_excerpts >= 2\n    step = _excerpt_step(n_samples,\n                         n_excerpts=n_excerpts,\n                         excerpt_size=excerpt_size)\n    for i in range(n_excerpts):\n        start = i * step\n        if start >= n_samples:\n            break\n        end = min(start + excerpt_size, n_samples)\n        yield start, end"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget a data chunk.", "response": "def data_chunk(data, chunk, with_overlap=False):\n    \"\"\"Get a data chunk.\"\"\"\n    assert isinstance(chunk, tuple)\n    if len(chunk) == 2:\n        i, j = chunk\n    elif len(chunk) == 4:\n        if with_overlap:\n            i, j = chunk[:2]\n        else:\n            i, j = chunk[2:]\n    else:\n        raise ValueError(\"'chunk' should have 2 or 4 elements, \"\n                         \"not {0:d}\".format(len(chunk)))\n    return data[i:j, ...]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the ids of all spikes belonging to the specified clusters.", "response": "def _spikes_in_clusters(spike_clusters, clusters):\n    \"\"\"Return the ids of all spikes belonging to the specified clusters.\"\"\"\n    if len(spike_clusters) == 0 or len(clusters) == 0:\n        return np.array([], dtype=np.int)\n    return np.nonzero(np.in1d(spike_clusters, clusters))[0]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _spikes_per_cluster(spike_clusters, spike_ids=None):\n    if spike_clusters is None or not len(spike_clusters):\n        return {}\n    if spike_ids is None:\n        spike_ids = np.arange(len(spike_clusters)).astype(np.int64)\n    # NOTE: this sort method is stable, so spike ids are increasing\n    # among any cluster. Therefore we don't have to sort again down here,\n    # when creating the spikes_in_clusters dictionary.\n    rel_spikes = np.argsort(spike_clusters, kind='mergesort')\n    abs_spikes = spike_ids[rel_spikes]\n    spike_clusters = spike_clusters[rel_spikes]\n\n    diff = np.empty_like(spike_clusters)\n    diff[0] = 1\n    diff[1:] = np.diff(spike_clusters)\n\n    idx = np.nonzero(diff > 0)[0]\n    clusters = spike_clusters[idx]\n\n    # NOTE: we don't have to sort abs_spikes[...] here because the argsort\n    # using 'mergesort' above is stable.\n    spikes_in_clusters = {clusters[i]: abs_spikes[idx[i]:idx[i + 1]]\n                          for i in range(len(clusters) - 1)}\n    spikes_in_clusters[clusters[-1]] = abs_spikes[idx[-1]:]\n\n    return spikes_in_clusters", "response": "Return a dictionary containing the spike ids and the spikes in a cluster."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts a dictionary { cluster : spikes array } to a spikes array.", "response": "def _flatten_per_cluster(per_cluster):\n    \"\"\"Convert a dictionary {cluster: spikes} to a spikes array.\"\"\"\n    return np.sort(np.concatenate(list(per_cluster.values()))).astype(np.int64)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes the mean of a spike - dependent quantity for every cluster.", "response": "def grouped_mean(arr, spike_clusters):\n    \"\"\"Compute the mean of a spike-dependent quantity for every cluster.\n\n    The two arguments should be 1D array with `n_spikes` elements.\n\n    The output is a 1D array with `n_clusters` elements. The clusters are\n    sorted in increasing order.\n\n    \"\"\"\n    arr = np.asarray(arr)\n    spike_clusters = np.asarray(spike_clusters)\n    assert arr.ndim == 1\n    assert arr.shape[0] == len(spike_clusters)\n    cluster_ids = _unique(spike_clusters)\n    spike_clusters_rel = _index_of(spike_clusters, cluster_ids)\n    spike_counts = np.bincount(spike_clusters_rel)\n    assert len(spike_counts) == len(cluster_ids)\n    t = np.zeros(len(cluster_ids))\n    # Compute the sum with possible repetitions.\n    np.add.at(t, spike_clusters_rel, arr)\n    return t / spike_counts"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef regular_subset(spikes, n_spikes_max=None, offset=0):\n    assert spikes is not None\n    # Nothing to do if the selection already satisfies n_spikes_max.\n    if n_spikes_max is None or len(spikes) <= n_spikes_max:  # pragma: no cover\n        return spikes\n    step = math.ceil(np.clip(1. / n_spikes_max * len(spikes),\n                             1, len(spikes)))\n    step = int(step)\n    # Note: randomly-changing selections are confusing...\n    my_spikes = spikes[offset::step][:n_spikes_max]\n    assert len(my_spikes) <= len(spikes)\n    assert len(my_spikes) <= n_spikes_max\n    return my_spikes", "response": "Prune the current selection to get at most n_spikes_max spikes."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef select_spikes(cluster_ids=None,\n                  max_n_spikes_per_cluster=None,\n                  spikes_per_cluster=None,\n                  batch_size=None,\n                  subset=None,\n                  ):\n    \"\"\"Return a selection of spikes belonging to the specified clusters.\"\"\"\n    subset = subset or 'regular'\n    assert _is_array_like(cluster_ids)\n    if not len(cluster_ids):\n        return np.array([], dtype=np.int64)\n    if max_n_spikes_per_cluster in (None, 0):\n        selection = {c: spikes_per_cluster(c) for c in cluster_ids}\n    else:\n        assert max_n_spikes_per_cluster > 0\n        selection = {}\n        n_clusters = len(cluster_ids)\n        for cluster in cluster_ids:\n            # Decrease the number of spikes per cluster when there\n            # are more clusters.\n            n = int(max_n_spikes_per_cluster * exp(-.1 * (n_clusters - 1)))\n            n = max(1, n)\n            spike_ids = spikes_per_cluster(cluster)\n            if subset == 'regular':\n                # Regular subselection.\n                if batch_size is None or len(spike_ids) <= max(batch_size, n):\n                    spike_ids = regular_subset(spike_ids, n_spikes_max=n)\n                else:\n                    # Batch selections of spikes.\n                    spike_ids = get_excerpts(spike_ids,\n                                             n // batch_size,\n                                             batch_size)\n            elif subset == 'random' and len(spike_ids) > n:\n                # Random subselection.\n                spike_ids = np.random.choice(spike_ids, n, replace=False)\n                spike_ids = np.unique(spike_ids)\n            selection[cluster] = spike_ids\n    return _flatten_per_cluster(selection)", "response": "Return a selection of spikes belonging to the specified clusters."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _accumulate(data_list, no_concat=()):\n    acc = Accumulator()\n    for data in data_list:\n        for name, val in data.items():\n            acc.add(name, val)\n    out = {name: acc[name] for name in acc.names if name not in no_concat}\n\n    # Some variables should not be concatenated but should be kept as lists.\n    # This is when there can be several arrays of variable length (NumPy\n    # doesn't support ragged arrays).\n    out.update({name: acc.get(name) for name in no_concat})\n    return out", "response": "Concatenate a list of dicts with some names and arrays."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_recording(self, index):\n        assert index >= 0\n        recs = np.nonzero((index - self.offsets[:-1]) >= 0)[0]\n        if len(recs) == 0:  # pragma: no cover\n            # If the index is greater than the total size,\n            # return the last recording.\n            return len(self.arrs) - 1\n        # Return the last recording such that the index is greater than\n        # its offset.\n        return recs[-1]", "response": "Return the recording that contains a given index."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef apply_filter(x, filter=None, axis=0):\n    x = _as_array(x)\n    if x.shape[axis] == 0:\n        return x\n    b, a = filter\n    return signal.filtfilt(b, a, x, axis=axis)", "response": "Apply a filter to an array."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing the whitening matrix.", "response": "def fit(self, x, fudge=1e-18):\n        \"\"\"Compute the whitening matrix.\n\n        Parameters\n        ----------\n\n        x : array\n            An `(n_samples, n_channels)` array.\n\n        \"\"\"\n        assert x.ndim == 2\n        ns, nc = x.shape\n        x_cov = np.cov(x, rowvar=0)\n        assert x_cov.shape == (nc, nc)\n        d, v = np.linalg.eigh(x_cov)\n        d = np.diag(1. / np.sqrt(d + fudge))\n        # This is equivalent, but seems much slower...\n        # w = np.einsum('il,lk,jk->ij', v, d, v)\n        w = np.dot(np.dot(v, d), v.T)\n        self._matrix = w\n        return w"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the current element.", "response": "def current_item(self):\n        \"\"\"Return the current element.\"\"\"\n        if self._history and self._index >= 0:\n            self._check_index()\n            return self._history[self._index]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _check_index(self):\n        assert 0 <= self._index <= len(self._history) - 1\n        # There should always be the base item at least.\n        assert len(self._history) >= 1", "response": "Check that the index is without the bounds of _history."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\niterate through successive history items.", "response": "def iter(self, start=0, end=None):\n        \"\"\"Iterate through successive history items.\n\n        Parameters\n        ----------\n        end : int\n            Index of the last item to loop through + 1.\n\n        start : int\n            Initial index for the loop (0 by default).\n\n        \"\"\"\n        if end is None:\n            end = self._index + 1\n        elif end == 0:\n            raise StopIteration()\n        if start >= end:\n            raise StopIteration()\n        # Check arguments.\n        assert 0 <= end <= len(self._history)\n        assert 0 <= start <= end - 1\n        for i in range(start, end):\n            yield self._history[i]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds an item to the history.", "response": "def add(self, item):\n        \"\"\"Add an item in the history.\"\"\"\n        self._check_index()\n        # Possibly truncate the history up to the current point.\n        self._history = self._history[:self._index + 1]\n        # Append the item\n        self._history.append(item)\n        # Increment the index.\n        self._index += 1\n        self._check_index()\n        # Check that the current element is what was provided to the function.\n        assert id(self.current_item) == id(item)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngo back in history if possible. Returns None if no undone item.", "response": "def back(self):\n        \"\"\"Go back in history if possible.\n\n        Return the undone item.\n\n        \"\"\"\n        if self._index <= 0:\n            return None\n        undone = self.current_item\n        self._index -= 1\n        self._check_index()\n        return undone"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef forward(self):\n        if self._index >= len(self._history) - 1:\n            return None\n        self._index += 1\n        self._check_index()\n        return self.current_item", "response": "Go forward in history if possible. Returns the current item after going forward."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a controller to the current action.", "response": "def add_to_current_action(self, controller):\n        \"\"\"Add a controller to the current action.\"\"\"\n        item = self.current_item\n        self._history[self._index] = item + (controller,)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nundoing the last action.", "response": "def undo(self):\n        \"\"\"Undo the last action.\n\n        This will call `undo()` on all controllers involved in this action.\n\n        \"\"\"\n        controllers = self.back()\n        if controllers is None:\n            ups = ()\n        else:\n            ups = tuple([controller.undo()\n                        for controller in controllers])\n        if self.process_ups is not None:\n            return self.process_ups(ups)\n        else:\n            return ups"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nre-does the last action.", "response": "def redo(self):\n        \"\"\"Redo the last action.\n\n        This will call `redo()` on all controllers involved in this action.\n\n        \"\"\"\n        controllers = self.forward()\n        if controllers is None:\n            ups = ()\n        else:\n            ups = tuple([controller.redo() for\n                         controller in controllers])\n        if self.process_ups is not None:\n            return self.process_ups(ups)\n        else:\n            return ups"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ninsert the GLSL snippet in a shader.", "response": "def _insert_glsl(vertex, fragment, to_insert):\n    \"\"\"Insert snippets in a shader.\n\n    to_insert is a dict `{(shader_type, location): snippet}`.\n\n    Snippets can contain `{{ var }}` placeholders for the transformed variable\n    name.\n\n    \"\"\"\n    # Find the place where to insert the GLSL snippet.\n    # This is \"gl_Position = transform(data_var_name);\" where\n    # data_var_name is typically an attribute.\n    vs_regex = re.compile(r'gl_Position = transform\\(([\\S]+)\\);')\n    r = vs_regex.search(vertex)\n    if not r:\n        logger.debug(\"The vertex shader doesn't contain the transform \"\n                     \"placeholder: skipping the transform chain \"\n                     \"GLSL insertion.\")\n        return vertex, fragment\n    assert r\n    logger.log(5, \"Found transform placeholder in vertex code: `%s`\",\n               r.group(0))\n\n    # Find the GLSL variable with the data (should be a `vec2`).\n    var = r.group(1)\n    assert var and var in vertex\n\n    # Headers.\n    vertex = to_insert['vert', 'header'] + '\\n\\n' + vertex\n    fragment = to_insert['frag', 'header'] + '\\n\\n' + fragment\n\n    # Get the pre and post transforms.\n    vs_insert = to_insert['vert', 'before_transforms']\n    vs_insert += to_insert['vert', 'transforms']\n    vs_insert += to_insert['vert', 'after_transforms']\n\n    # Insert the GLSL snippet in the vertex shader.\n    vertex = vs_regex.sub(indent(vs_insert), vertex)\n\n    # Now, we make the replacements in the fragment shader.\n    fs_regex = re.compile(r'(void main\\(\\)\\s*\\{)')\n    # NOTE: we add the `void main(){` that was removed by the regex.\n    fs_insert = '\\\\1\\n' + to_insert['frag', 'before_transforms']\n    fragment = fs_regex.sub(indent(fs_insert), fragment)\n\n    # Replace the transformed variable placeholder by its name.\n    vertex = vertex.replace('{{ var }}', var)\n\n    return vertex, fragment"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ninserts the GLSL snippets of a transform chain.", "response": "def add_transform_chain(self, tc):\n        \"\"\"Insert the GLSL snippets of a transform chain.\"\"\"\n        # Generate the transforms snippet.\n        for t in tc.gpu_transforms:\n            if isinstance(t, Clip):\n                # Set the varying value in the vertex shader.\n                self.insert_vert('v_temp_pos_tr = temp_pos_tr;')\n                continue\n            self.insert_vert(t.glsl('temp_pos_tr'))\n        # Clipping.\n        clip = tc.get('Clip')\n        if clip:\n            self.insert_frag(clip.glsl('v_temp_pos_tr'), 'before_transforms')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef insert_into_shaders(self, vertex, fragment):\n        to_insert = defaultdict(str)\n        to_insert.update({key: '\\n'.join(self._to_insert[key]) + '\\n'\n                          for key in self._to_insert})\n        return _insert_glsl(vertex, fragment, to_insert)", "response": "Insert the shader code into the shader code."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_visual(self, visual):\n        # Retrieve the visual's GLSL inserter.\n        inserter = visual.inserter\n        # Add the visual's transforms.\n        inserter.add_transform_chain(visual.transforms)\n        # Then, add the canvas' transforms.\n        canvas_transforms = visual.canvas_transforms_filter(self.transforms)\n        inserter.add_transform_chain(canvas_transforms)\n        # Also, add the canvas' inserter.\n        inserter += self.inserter\n        # Now, we insert the transforms GLSL into the shaders.\n        vs, fs = visual.vertex_shader, visual.fragment_shader\n        vs, fs = inserter.insert_into_shaders(vs, fs)\n        # Finally, we create the visual's program.\n        visual.program = gloo.Program(vs, fs)\n        logger.log(5, \"Vertex shader: %s\", vs)\n        logger.log(5, \"Fragment shader: %s\", fs)\n        # Initialize the size.\n        visual.on_resize(self.size)\n        # Register the visual in the list of visuals in the canvas.\n        self.visuals.append(visual)\n        self.events.visual_added(visual=visual)", "response": "Add a visual to the canvas and build its program by the same\n        occasion."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef on_resize(self, event):\n        self.context.set_viewport(0, 0, event.size[0], event.size[1])\n        for visual in self.visuals:\n            visual.on_resize(event.size)\n        self.update()", "response": "Resize the OpenGL context."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nattaches this interact to a canvas.", "response": "def attach(self, canvas):\n        \"\"\"Attach this interact to a canvas.\"\"\"\n        self.canvas = canvas\n\n        @canvas.connect\n        def on_visual_added(e):\n            self.update_program(e.visual.program)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update(self):\n        if not self.canvas:\n            return\n        for visual in self.canvas.visuals:\n            self.update_program(visual.program)\n        self.canvas.update()", "response": "Update all visuals in the attached canvas."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a phy. log file with DEBUG level in the current directory.", "response": "def _add_log_file(filename):\n    \"\"\"Create a `phy.log` log file with DEBUG level in the\n    current directory.\"\"\"\n    handler = logging.FileHandler(filename)\n\n    handler.setLevel(logging.DEBUG)\n    formatter = _Formatter(fmt=_logger_fmt,\n                           datefmt='%Y-%m-%d %H:%M:%S')\n    handler.setFormatter(formatter)\n    logging.getLogger().addHandler(handler)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _run_cmd(cmd, ctx, glob, loc):  # pragma: no cover\n    if PDB:\n        _enable_pdb()\n    if IPYTHON:\n        from IPython import start_ipython\n        args_ipy = ['-i', '--gui=qt']\n        ns = glob.copy()\n        ns.update(loc)\n        return start_ipython(args_ipy, user_ns=ns)\n    # Profiling. The builtin `profile` is added in __init__.\n    prof = __builtins__.get('profile', None)\n    if prof:\n        prof = __builtins__['profile']\n        return _profile(prof, cmd, glob, loc)\n    return exec_(cmd, glob, loc)", "response": "Run a command with optionally a debugger IPython or profiling."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nloading all plugins and attach them to a CLI object.", "response": "def load_cli_plugins(cli, config_dir=None):\n    \"\"\"Load all plugins and attach them to a CLI object.\"\"\"\n    from .config import load_master_config\n\n    config = load_master_config(config_dir=config_dir)\n    plugins = discover_plugins(config.Plugins.dirs)\n\n    for plugin in plugins:\n        if not hasattr(plugin, 'attach_to_cli'):  # pragma: no cover\n            continue\n        logger.debug(\"Attach plugin `%s` to CLI.\", _fullname(plugin))\n        # NOTE: plugin is a class, so we need to instantiate it.\n        try:\n            plugin().attach_to_cli(cli)\n        except Exception as e:  # pragma: no cover\n            logger.error(\"Error when loading plugin `%s`: %s\", plugin, e)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the mouse coordinates in NDC taking panzoom into account.", "response": "def get_mouse_pos(self, pos):\n        \"\"\"Return the mouse coordinates in NDC, taking panzoom into account.\"\"\"\n        position = np.asarray(self._normalize(pos))\n        zoom = np.asarray(self._zoom_aspect())\n        pan = np.asarray(self.pan)\n        mouse_pos = ((position / zoom) - pan)\n        return mouse_pos"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\npanning the view by a given amount.", "response": "def pan_delta(self, d):\n        \"\"\"Pan the view by a given amount.\"\"\"\n        dx, dy = d\n\n        pan_x, pan_y = self.pan\n        zoom_x, zoom_y = self._zoom_aspect(self._zoom)\n\n        self.pan = (pan_x + dx / zoom_x, pan_y + dy / zoom_y)\n        self.update()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef zoom_delta(self, d, p=(0., 0.), c=1.):\n        dx, dy = d\n        x0, y0 = p\n\n        pan_x, pan_y = self._pan\n        zoom_x, zoom_y = self._zoom\n        zoom_x_new, zoom_y_new = (zoom_x * math.exp(c * self._zoom_coeff * dx),\n                                  zoom_y * math.exp(c * self._zoom_coeff * dy))\n\n        zoom_x_new = max(min(zoom_x_new, self._zmax), self._zmin)\n        zoom_y_new = max(min(zoom_y_new, self._zmax), self._zmin)\n\n        self.zoom = zoom_x_new, zoom_y_new\n\n        if self._zoom_to_pointer:\n            zoom_x, zoom_y = self._zoom_aspect((zoom_x,\n                                                zoom_y))\n            zoom_x_new, zoom_y_new = self._zoom_aspect((zoom_x_new,\n                                                        zoom_y_new))\n\n            self.pan = (pan_x - x0 * (1. / zoom_x - 1. / zoom_x_new),\n                        pan_y - y0 * (1. / zoom_y - 1. / zoom_y_new))\n\n        self.update()", "response": "Zoom the view by a given amount."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nzoom to fit a box.", "response": "def set_range(self, bounds, keep_aspect=False):\n        \"\"\"Zoom to fit a box.\"\"\"\n        # a * (v0 + t) = -1\n        # a * (v1 + t) = +1\n        # =>\n        # a * (v1 - v0) = 2\n        bounds = np.asarray(bounds, dtype=np.float64)\n        v0 = bounds[:2]\n        v1 = bounds[2:]\n        pan = -.5 * (v0 + v1)\n        zoom = 2. / (v1 - v0)\n        if keep_aspect:\n            zoom = zoom.min() * np.ones(2)\n        self.set_pan_zoom(pan=pan, zoom=zoom)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_range(self):\n        p, z = np.asarray(self.pan), np.asarray(self.zoom)\n        x0, y0 = -1. / z - p\n        x1, y1 = +1. / z - p\n        return (x0, y0, x1, y1)", "response": "Return the bounds currently visible."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\npans and zoom with the mouse.", "response": "def on_mouse_move(self, event):\n        \"\"\"Pan and zoom with the mouse.\"\"\"\n        if event.modifiers:\n            return\n        if event.is_dragging:\n            x0, y0 = self._normalize(event.press_event.pos)\n            x1, y1 = self._normalize(event.last_event.pos)\n            x, y = self._normalize(event.pos)\n            dx, dy = x - x1, y - y1\n            if event.button == 1:\n                self.pan_delta((dx, dy))\n            elif event.button == 2:\n                c = np.sqrt(self.size[0]) * .03\n                self.zoom_delta((dx, dy), (x0, y0), c=c)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nzoom with the mouse wheel.", "response": "def on_mouse_wheel(self, event):\n        \"\"\"Zoom with the mouse wheel.\"\"\"\n        # NOTE: not called on OS X because of touchpad\n        if event.modifiers:\n            return\n        dx = np.sign(event.delta[1]) * self._wheel_coeff\n        # Zoom toward the mouse pointer.\n        x0, y0 = self._normalize(event.pos)\n        self.zoom_delta((dx, dx), (x0, y0))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\npanning and zoom with the keyboard.", "response": "def on_key_press(self, event):\n        \"\"\"Pan and zoom with the keyboard.\"\"\"\n        # Zooming with the keyboard.\n        key = event.key\n        if event.modifiers:\n            return\n\n        # Pan.\n        if self.enable_keyboard_pan and key in self._arrows:\n            self._pan_keyboard(key)\n\n        # Zoom.\n        if key in self._pm:\n            self._zoom_keyboard(key)\n\n        # Reset with 'R'.\n        if key == 'R':\n            self.reset()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef attach(self, canvas):\n        super(PanZoom, self).attach(canvas)\n        canvas.panzoom = self\n\n        canvas.transforms.add_on_gpu([self._translate, self._scale])\n        # Add the variable declarations.\n        vs = ('uniform vec2 {};\\n'.format(self.pan_var_name) +\n              'uniform vec2 {};\\n'.format(self.zoom_var_name))\n        canvas.inserter.insert_vert(vs, 'header')\n\n        canvas.connect(self.on_resize)\n        canvas.connect(self.on_mouse_move)\n        canvas.connect(self.on_touch)\n        canvas.connect(self.on_key_press)\n\n        if self.enable_mouse_wheel:\n            canvas.connect(self.on_mouse_wheel)\n\n        self._set_canvas_aspect()", "response": "Attach this interact to a canvas."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprocesses NumPy - like function docstrings.", "response": "def _replace_docstring_header(paragraph):\n    \"\"\"Process NumPy-like function docstrings.\"\"\"\n\n    # Replace Markdown headers in docstrings with light headers in bold.\n    paragraph = re.sub(_docstring_header_pattern,\n                       r'*\\1*',\n                       paragraph,\n                       )\n\n    paragraph = re.sub(_docstring_parameters_pattern,\n                       r'\\n* `\\1` (\\2)\\n',\n                       paragraph,\n                       )\n\n    return paragraph"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _iter_vars(mod):\n    vars = sorted(var for var in dir(mod) if _is_public(var))\n    for var in vars:\n        yield getattr(mod, var)", "response": "Iterate through a list of variables define in a module s\n    public namespace."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _function_header(subpackage, func):\n    args = inspect.formatargspec(*inspect.getfullargspec(func))\n    return \"{name}{args}\".format(name=_full_name(subpackage, func),\n                                   args=args,\n                                   )", "response": "Generate the docstring of a function."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _doc_method(klass, func):\n    argspec = inspect.getfullargspec(func)\n    # Remove first 'self' argument.\n    if argspec.args and argspec.args[0] == 'self':\n        del argspec.args[0]\n    args = inspect.formatargspec(*argspec)\n    header = \"{klass}.{name}{args}\".format(klass=klass.__name__,\n                                             name=_name(func),\n                                             args=args,\n                                             )\n    docstring = _doc(func)\n    return _concat(header, docstring)", "response": "Generates the docstring of a method."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates the docstring of a property.", "response": "def _doc_property(klass, prop):\n    \"\"\"Generate the docstring of a property.\"\"\"\n    header = \"{klass}.{name}\".format(klass=klass.__name__,\n                                       name=_name(prop),\n                                       )\n    docstring = _doc(prop)\n    return _concat(header, docstring)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _generate_paragraphs(package, subpackages):\n\n    # API doc of each module.\n    for subpackage in _iter_subpackages(package, subpackages):\n        subpackage_name = subpackage.__name__\n\n        yield \"## {}\".format(subpackage_name)\n\n        # Subpackage documentation.\n        yield _doc(_import_module(subpackage_name))\n\n        # List of top-level functions in the subpackage.\n        for func in _iter_functions(subpackage):\n            yield '##### ' + _doc_function(subpackage, func)\n\n        # All public classes.\n        for klass in _iter_classes(subpackage):\n\n            # Class documentation.\n            yield \"### {}\".format(_full_name(subpackage, klass))\n            yield _doc(klass)\n\n            yield \"#### Methods\"\n            for method in _iter_methods(klass, package):\n                yield '##### ' + _doc_method(klass, method)\n\n            yield \"#### Properties\"\n            for prop in _iter_properties(klass, package):\n                yield '##### ' + _doc_property(klass, prop)", "response": "Generate the paragraphs of the API documentation."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding a column for a given label field.", "response": "def _add_field_column(self, field):  # pragma: no cover\n        \"\"\"Add a column for a given label field.\"\"\"\n        @self.add_column(name=field)\n        def get_my_label(cluster_id):\n            return self.cluster_meta.get(field, cluster_id)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nselecting spikes from the specified clusters and emit the select event on the GUI.", "response": "def _emit_select(self, cluster_ids, **kwargs):\n        \"\"\"Choose spikes from the specified clusters and emit the\n        `select` event on the GUI.\"\"\"\n        # Remove non-existing clusters from the selection.\n        cluster_ids = self._keep_existing_clusters(cluster_ids)\n        logger.debug(\"Select cluster(s): %s.\",\n                     ', '.join(map(str, cluster_ids)))\n        self.emit('select', cluster_ids, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninitializes the cluster view with cluster data.", "response": "def _update_cluster_view(self):\n        \"\"\"Initialize the cluster view with cluster data.\"\"\"\n        logger.log(5, \"Update the cluster view.\")\n        cluster_ids = [int(c) for c in self.clustering.cluster_ids]\n        self.cluster_view.set_rows(cluster_ids)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate the similarity view with matches for the specified cluster_ids.", "response": "def _update_similarity_view(self):\n        \"\"\"Update the similarity view with matches for the specified\n        clusters.\"\"\"\n        if not self.similarity:\n            return\n        selection = self.cluster_view.selected\n        if not len(selection):\n            return\n        cluster_id = selection[0]\n        cluster_ids = self.clustering.cluster_ids\n        self._best = cluster_id\n        logger.log(5, \"Update the similarity view.\")\n        # This is a list of pairs (closest_cluster, similarity).\n        similarities = self.similarity(cluster_id)\n        # We save the similarity values wrt the currently-selected clusters.\n        # Note that we keep the order of the output of the self.similary()\n        # function.\n        clusters_sim = OrderedDict([(int(cl), s) for (cl, s) in similarities])\n        # List of similar clusters, remove non-existing ones.\n        clusters = [c for c in clusters_sim.keys()\n                    if c in cluster_ids]\n        # The similarity view will use these values.\n        self._current_similarity_values = clusters_sim\n        # Set the rows of the similarity view.\n        # TODO: instead of the self._current_similarity_values hack,\n        # give the possibility to specify the values here (?).\n        self.similarity_view.set_rows([c for c in clusters\n                                       if c not in selection])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate the cluster views after clustering actions.", "response": "def on_cluster(self, up):\n        \"\"\"Update the cluster views after clustering actions.\"\"\"\n\n        similar = self.similarity_view.selected\n\n        # Reinitialize the cluster view if clusters have changed.\n        if up.added:\n            self._update_cluster_view()\n\n        # Select all new clusters in view 1.\n        if up.history == 'undo':\n            # Select the clusters that were selected before the undone\n            # action.\n            clusters_0, clusters_1 = up.undo_state[0]['selection']\n            # Select rows in the tables.\n            self.cluster_view.select(clusters_0, up=up)\n            self.similarity_view.select(clusters_1, up=up)\n        elif up.added:\n            if up.description == 'assign':\n                # NOTE: we change the order such that the last selected\n                # cluster (with a new color) is the split cluster.\n                added = list(up.added[1:]) + [up.added[0]]\n            else:\n                added = up.added\n            # Select the new clusters in the cluster view.\n            self.cluster_view.select(added, up=up)\n            if similar:\n                self.similarity_view.next()\n        elif up.metadata_changed:\n            # Select next in similarity view if all moved are in that view.\n            if set(up.metadata_changed) <= set(similar):\n                next_cluster = self.similarity_view.get_next_id()\n                self._update_similarity_view()\n                if next_cluster is not None:\n                    # Select the cluster in the similarity view.\n                    self.similarity_view.select([next_cluster])\n            # Otherwise, select next in cluster view.\n            else:\n                self._update_cluster_view()\n                # Determine if there is a next cluster set from a\n                # previous clustering action.\n                cluster = up.metadata_changed[0]\n                next_cluster = self.cluster_meta.get('next_cluster', cluster)\n                logger.debug(\"Get next_cluster for %d: %s.\",\n                             cluster, next_cluster)\n                # If there is not, fallback on the next cluster in the list.\n                if next_cluster is None:\n                    self.cluster_view.select([cluster], do_emit=False)\n                    self.cluster_view.next()\n                else:\n                    self.cluster_view.select([next_cluster])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nselect a list of clusters.", "response": "def select(self, *cluster_ids):\n        \"\"\"Select a list of clusters.\"\"\"\n        # HACK: allow for `select(1, 2, 3)` in addition to `select([1, 2, 3])`\n        # This makes it more convenient to select multiple clusters with\n        # the snippet: `:c 1 2 3` instead of `:c 1,2,3`.\n        if cluster_ids and isinstance(cluster_ids[0], (tuple, list)):\n            cluster_ids = list(cluster_ids[0]) + list(cluster_ids[1:])\n        # Remove non-existing clusters from the selection.\n        cluster_ids = self._keep_existing_clusters(cluster_ids)\n        # Update the cluster view selection.\n        self.cluster_view.select(cluster_ids)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef merge(self, cluster_ids=None, to=None):\n        if cluster_ids is None:\n            cluster_ids = self.selected\n        if len(cluster_ids or []) <= 1:\n            return\n        self.clustering.merge(cluster_ids, to=to)\n        self._global_history.action(self.clustering)", "response": "Merge the selected clusters."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef split(self, spike_ids=None, spike_clusters_rel=0):\n        if spike_ids is None:\n            spike_ids = self.emit('request_split', single=True)\n            spike_ids = np.asarray(spike_ids, dtype=np.int64)\n            assert spike_ids.dtype == np.int64\n            assert spike_ids.ndim == 1\n        if len(spike_ids) == 0:\n            msg = (\"You first need to select spikes in the feature \"\n                   \"view with a few Ctrl+Click around the spikes \"\n                   \"that you want to split.\")\n            self.emit('error', msg)\n            return\n        self.clustering.split(spike_ids,\n                              spike_clusters_rel=spike_clusters_rel)\n        self._global_history.action(self.clustering)", "response": "Split the selected spikes."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the labels of all clusters for a given field.", "response": "def get_labels(self, field):\n        \"\"\"Return the labels of all clusters, for a given field.\"\"\"\n        return {c: self.cluster_meta.get(field, c)\n                for c in self.clustering.cluster_ids}"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef label(self, name, value, cluster_ids=None):\n        if cluster_ids is None:\n            cluster_ids = self.cluster_view.selected\n        if not hasattr(cluster_ids, '__len__'):\n            cluster_ids = [cluster_ids]\n        if len(cluster_ids) == 0:\n            return\n        self.cluster_meta.set(name, cluster_ids, value)\n        self._global_history.action(self.cluster_meta)", "response": "Assign a label to clusters.\n        Example : quality 3"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef move(self, group, cluster_ids=None):\n        if isinstance(cluster_ids, string_types):\n            logger.warn(\"The list of clusters should be a list of integers, \"\n                        \"not a string.\")\n            return\n        self.label('group', group, cluster_ids=cluster_ids)", "response": "Assign a group to some clusters."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef next(self):\n        if not self.selected:\n            self.cluster_view.next()\n        else:\n            self.similarity_view.next()", "response": "Select the next cluster."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsave the manual clustering back to disk.", "response": "def save(self):\n        \"\"\"Save the manual clustering back to disk.\"\"\"\n        spike_clusters = self.clustering.spike_clusters\n        groups = {c: self.cluster_meta.get('group', c) or 'unsorted'\n                  for c in self.clustering.cluster_ids}\n        # List of tuples (field_name, dictionary).\n        labels = [(field, self.get_labels(field))\n                  for field in self.cluster_meta.fields\n                  if field not in ('next_cluster')]\n        # TODO: add option in add_field to declare a field unsavable.\n        self.emit('request_save', spike_clusters, groups, *labels)\n        # Cache the spikes_per_cluster array.\n        self._save_spikes_per_cluster()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_cluster_meta(cluster_groups):\n    meta = ClusterMeta()\n    meta.add_field('group')\n\n    cluster_groups = cluster_groups or {}\n    data = {c: {'group': v} for c, v in cluster_groups.items()}\n    meta.from_dict(data)\n\n    return meta", "response": "Return a ClusterMeta instance with cluster group support."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a field with an optional default value.", "response": "def add_field(self, name, default_value=None):\n        \"\"\"Add a field with an optional default value.\"\"\"\n        self._fields[name] = default_value\n\n        def func(cluster):\n            return self.get(name, cluster)\n\n        setattr(self, name, func)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_dict(self, dic):\n        self._reset_data()\n        for cluster, vals in dic.items():\n            for field, value in vals.items():\n                self.set(field, [cluster], value, add_to_stack=False)\n        self._data_base = deepcopy(self._data)", "response": "Import data from a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_dict(self, field):\n        assert field in self._fields, \"This field doesn't exist\"\n        return {cluster: self.get(field, cluster)\n                for cluster in self._data.keys()}", "response": "Export data to a dictionary for a particular\n        field."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set(self, field, clusters, value, add_to_stack=True):\n        # Add the field if it doesn't exist.\n        if field not in self._fields:\n            self.add_field(field)\n        assert field in self._fields\n\n        clusters = _as_list(clusters)\n        for cluster in clusters:\n            if cluster not in self._data:\n                self._data[cluster] = {}\n            self._data[cluster][field] = value\n\n        up = UpdateInfo(description='metadata_' + field,\n                        metadata_changed=clusters,\n                        metadata_value=value,\n                        )\n        undo_state = self.emit('request_undo_state', up)\n\n        if add_to_stack:\n            self._undo_stack.add((clusters, field, value, up, undo_state))\n            self.emit('cluster', up)\n\n        return up", "response": "Set the value of one of several clusters."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nretrieve the value of one cluster.", "response": "def get(self, field, cluster):\n        \"\"\"Retrieve the value of one cluster.\"\"\"\n        if _is_list(cluster):\n            return [self.get(field, c) for c in cluster]\n        assert field in self._fields\n        default = self._fields[field]\n        return self._data.get(cluster, {}).get(field, default)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate metadata of some clusters given the metadata of their ascendants.", "response": "def set_from_descendants(self, descendants):\n        \"\"\"Update metadata of some clusters given the metadata of their\n        ascendants.\"\"\"\n        for field in self.fields:\n\n            # This gives a set of metadata values of all the parents\n            # of any new cluster.\n            candidates = defaultdict(set)\n            for old, new in descendants:\n                candidates[new].add(self.get(field, old))\n\n            # Loop over all new clusters.\n            for new, vals in candidates.items():\n                vals = list(vals)\n                default = self._fields[field]\n                # If all the parents have the same value, assign it to\n                # the new cluster if it is not the default.\n                if len(vals) == 1 and vals[0] != default:\n                    self.set(field, new, vals[0])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef undo(self):\n        args = self._undo_stack.back()\n        if args is None:\n            return\n        self._data = deepcopy(self._data_base)\n        for clusters, field, value, up, undo_state in self._undo_stack:\n            if clusters is not None:\n                self.set(field, clusters, value, add_to_stack=False)\n\n        # Return the UpdateInfo instance of the undo action.\n        up, undo_state = args[-2:]\n        up.history = 'undo'\n        up.undo_state = undo_state\n\n        self.emit('cluster', up)\n        return up", "response": "Undo the last metadata change. Returns the last update info instance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nredoing the next metadata change. Returns the UpdateInfo instance of the redo action.", "response": "def redo(self):\n        \"\"\"Redo the next metadata change.\n\n        Returns\n        -------\n\n        up : UpdateInfo instance\n        \"\"\"\n        args = self._undo_stack.forward()\n        if args is None:\n            return\n        clusters, field, value, up, undo_state = args\n        self.set(field, clusters, value, add_to_stack=False)\n\n        # Return the UpdateInfo instance of the redo action.\n        up.history = 'redo'\n\n        self.emit('cluster', up)\n        return up"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the largest x such f(x) is True.", "response": "def _binary_search(f, xmin, xmax, eps=1e-9):\n    \"\"\"Return the largest x such f(x) is True.\"\"\"\n    middle = (xmax + xmin) / 2.\n    while xmax - xmin > eps:\n        assert xmin < xmax\n        middle = (xmax + xmin) / 2.\n        if f(xmax):\n            return xmax\n        if not f(xmin):\n            return xmin\n        if f(middle):\n            xmin = middle\n        else:\n            xmax = middle\n    return middle"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_boxes(pos, size=None, margin=0, keep_aspect_ratio=True):\n\n    # Get x, y.\n    pos = np.asarray(pos, dtype=np.float64)\n    x, y = pos.T\n    x = x[:, np.newaxis]\n    y = y[:, np.newaxis]\n\n    w, h = size if size is not None else _get_box_size(x, y, margin=margin)\n\n    x0, y0 = x - w, y - h\n    x1, y1 = x + w, y + h\n\n    # Renormalize the whole thing by keeping the aspect ratio.\n    x0min, y0min, x1max, y1max = x0.min(), y0.min(), x1.max(), y1.max()\n    if not keep_aspect_ratio:\n        b = (x0min, y0min, x1max, y1max)\n    else:\n        dx = x1max - x0min\n        dy = y1max - y0min\n        if dx > dy:\n            b = (x0min, (y1max + y0min) / 2. - dx / 2.,\n                 x1max, (y1max + y0min) / 2. + dx / 2.)\n        else:\n            b = ((x1max + x0min) / 2. - dy / 2., y0min,\n                 (x1max + x0min) / 2. + dy / 2., y1max)\n    r = Range(from_bounds=b,\n              to_bounds=(-1, -1, 1, 1))\n    return np.c_[r.apply(np.c_[x0, y0]), r.apply(np.c_[x1, y1])]", "response": "Generate non - overlapping boxes in NDC from a set of positions."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_texture(arr, default, n_items, from_bounds):\n    if not hasattr(default, '__len__'):  # pragma: no cover\n        default = [default]\n    n_cols = len(default)\n    if arr is None:  # pragma: no cover\n        arr = np.tile(default, (n_items, 1))\n    assert arr.shape == (n_items, n_cols)\n    # Convert to 3D texture.\n    arr = arr[np.newaxis, ...].astype(np.float64)\n    assert arr.shape == (1, n_items, n_cols)\n    # NOTE: we need to cast the texture to [0., 1.] (float texture).\n    # This is easy as soon as we assume that the signal bounds are in\n    # [-1, 1].\n    assert len(from_bounds) == 2\n    m, M = map(float, from_bounds)\n    assert np.all(arr >= m)\n    assert np.all(arr <= M)\n    arr = (arr - m) / (M - m)\n    assert np.all(arr >= 0)\n    assert np.all(arr <= 1.)\n    return arr", "response": "Prepare data to be uploaded as a texture."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_array(val, shape, default=None, dtype=np.float64):\n    assert val is not None or default is not None\n    if hasattr(val, '__len__') and len(val) == 0:  # pragma: no cover\n        val = None\n    # Do nothing if the array is already correct.\n    if (isinstance(val, np.ndarray) and\n            val.shape == shape and\n            val.dtype == dtype):\n        return val\n    out = np.zeros(shape, dtype=dtype)\n    # This solves `ValueError: could not broadcast input array from shape (n)\n    # into shape (n, 1)`.\n    if val is not None and isinstance(val, np.ndarray):\n        if val.size == out.size:\n            val = val.reshape(out.shape)\n    out.flat[:] = val if val is not None else default\n    assert out.shape == shape\n    return out", "response": "Ensure an object is an array with the specified shape."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprepare data bounds possibly using min or max of the data.", "response": "def _get_data_bounds(data_bounds, pos=None, length=None):\n    \"\"\"\"Prepare data bounds, possibly using min/max of the data.\"\"\"\n    if data_bounds is None or (isinstance(data_bounds, string_types) and\n                               data_bounds == 'auto'):\n        if pos is not None and len(pos):\n            m, M = pos.min(axis=0), pos.max(axis=0)\n            data_bounds = [m[0], m[1], M[0], M[1]]\n        else:\n            data_bounds = NDC\n    data_bounds = np.atleast_2d(data_bounds)\n\n    ind_x = data_bounds[:, 0] == data_bounds[:, 2]\n    ind_y = data_bounds[:, 1] == data_bounds[:, 3]\n    if np.sum(ind_x):\n        data_bounds[ind_x, 0] -= 1\n        data_bounds[ind_x, 2] += 1\n    if np.sum(ind_y):\n        data_bounds[ind_y, 1] -= 1\n        data_bounds[ind_y, 3] += 1\n\n    # Extend the data_bounds if needed.\n    if length is None:\n        length = pos.shape[0] if pos is not None else 1\n    if data_bounds.shape[0] == 1:\n        data_bounds = np.tile(data_bounds, (length, 1))\n\n    # Check the shape of data_bounds.\n    assert data_bounds.shape == (length, 4)\n\n    _check_data_bounds(data_bounds)\n    return data_bounds"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprepares an index attribute for GPU uploading.", "response": "def _get_index(n_items, item_size, n):\n    \"\"\"Prepare an index attribute for GPU uploading.\"\"\"\n    index = np.arange(n_items)\n    index = np.repeat(index, item_size)\n    index = index.astype(np.float64)\n    assert index.shape == (n,)\n    return index"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads a shader file.", "response": "def _load_shader(filename):\n    \"\"\"Load a shader file.\"\"\"\n    curdir = op.dirname(op.realpath(__file__))\n    glsl_path = op.join(curdir, 'glsl')\n    path = op.join(glsl_path, filename)\n    with open(path, 'r') as f:\n        return f.read()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _random_color(h_range=(0., 1.),\n                  s_range=(.5, 1.),\n                  v_range=(.5, 1.),\n                  ):\n    \"\"\"Generate a random RGB color.\"\"\"\n    h, s, v = uniform(*h_range), uniform(*s_range), uniform(*v_range)\n    r, g, b = hsv_to_rgb(np.array([[[h, s, v]]])).flat\n    return r, g, b", "response": "Generate a random RGB color."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _is_bright(rgb):\n    r, g, b = rgb\n    gray = 0.299 * r + 0.587 * g + 0.114 * b\n    return gray >= .5", "response": "Return whether a RGB color is bright or not."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _bunchify(b):\n    assert isinstance(b, dict)\n    b = Bunch(b)\n    for k in b:\n        if isinstance(b[k], dict):\n            b[k] = Bunch(b[k])\n    return b", "response": "Ensure all dict elements are Bunch."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _as_list(obj):\n    if obj is None:\n        return None\n    elif isinstance(obj, string_types):\n        return [obj]\n    elif isinstance(obj, tuple):\n        return list(obj)\n    elif not hasattr(obj, '__len__'):\n        return [obj]\n    else:\n        return obj", "response": "Ensure an object is a list."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts an object to a numerical NumPy array.", "response": "def _as_array(arr, dtype=None):\n    \"\"\"Convert an object to a numerical NumPy array.\n\n    Avoid a copy if possible.\n\n    \"\"\"\n    if arr is None:\n        return None\n    if isinstance(arr, np.ndarray) and dtype is None:\n        return arr\n    if isinstance(arr, integer_types + (float,)):\n        arr = [arr]\n    out = np.asarray(arr)\n    if dtype is not None:\n        if out.dtype != dtype:\n            out = out.astype(dtype)\n    if out.dtype not in _ACCEPTED_ARRAY_DTYPES:\n        raise ValueError(\"'arr' seems to have an invalid dtype: \"\n                         \"{0:s}\".format(str(out.dtype)))\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _glslify(r):\n    if isinstance(r, string_types):\n        return r\n    else:\n        assert 2 <= len(r) <= 4\n        return 'vec{}({})'.format(len(r), ', '.join(map(str, r)))", "response": "Transform a string or a n - tuple to a valid GLSL expression."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting from pixels to normalized device coordinates ( in - 1 1.", "response": "def pixels_to_ndc(pos, size=None):\n    \"\"\"Convert from pixels to normalized device coordinates (in [-1, 1]).\"\"\"\n    pos = np.asarray(pos, dtype=np.float64)\n    size = np.asarray(size, dtype=np.float64)\n    pos = pos / (size / 2.) - 1\n    # Flip y, because the origin in pixels is at the top left corner of the\n    # window.\n    pos[1] = -pos[1]\n    return pos"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_on_cpu(self, transforms):\n        if not isinstance(transforms, list):\n            transforms = [transforms]\n        self.cpu_transforms.extend(transforms or [])\n        return self", "response": "Add some transforms to the CPU."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_on_gpu(self, transforms):\n        if not isinstance(transforms, list):\n            transforms = [transforms]\n        self.gpu_transforms.extend(transforms or [])\n        return self", "response": "Add some transforms to the GPU."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get(self, class_name):\n        for transform in self.cpu_transforms + self.gpu_transforms:\n            if transform.__class__.__name__ == class_name:\n                return transform", "response": "Get a transform in the chain from its name."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef remove(self, name):\n        cpu_transforms = self._remove_transform(self.cpu_transforms, name)\n        gpu_transforms = self._remove_transform(self.gpu_transforms, name)\n        return (TransformChain().add_on_cpu(cpu_transforms).\n                add_on_gpu(gpu_transforms))", "response": "Remove a transform in the chain."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef apply(self, arr):\n        for t in self.cpu_transforms:\n            arr = t.apply(arr)\n        return arr", "response": "Apply all CPU transforms on an array."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef inverse(self):\n        transforms = self.cpu_transforms + self.gpu_transforms\n        inv_transforms = [transform.inverse()\n                          for transform in transforms[::-1]]\n        return TransformChain().add_on_cpu(inv_transforms)", "response": "Return the inverse chain of transforms."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_on_name(self, func):\n        r = re.match(\"^on_(.+)$\", func.__name__)\n        if r:\n            event = r.group(1)\n        else:\n            raise ValueError(\"The function name should be \"\n                             \"`on_<eventname>`().\")\n        return event", "response": "Return the eventname when the function name is on_<eventname >()`."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _create_emitter(self, event):\n        if not hasattr(self, event):\n            setattr(self, event,\n                    lambda *args, **kwargs: self.emit(event, *args, **kwargs))", "response": "Create a method that emits an event of the same name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef connect(self, func=None, event=None, set_method=False):\n        if func is None:\n            return partial(self.connect, set_method=set_method)\n\n        # Get the event name from the function.\n        if event is None:\n            event = self._get_on_name(func)\n\n        # We register the callback function.\n        self._callbacks[event].append(func)\n\n        # A new method self.event() emitting the event is created.\n        if set_method:\n            self._create_emitter(event)\n\n        return func", "response": "Register a callback function to a given event."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nemitting an event and return the list of callback outputs.", "response": "def emit(self, event, *args, **kwargs):\n        \"\"\"Call all callback functions registered with an event.\n\n        Any positional and keyword arguments can be passed here, and they will\n        be forwarded to the callback functions.\n\n        Return the list of callback return results.\n\n        \"\"\"\n        callbacks = self._callbacks.get(event, [])\n        # Call the last callback if this is a single event.\n        single = kwargs.pop('single', None)\n        if single and callbacks:\n            return callbacks[-1](*args, **kwargs)\n        # Otherwise, return the list of callback outputs.\n        res = []\n        for callback in callbacks:\n            res.append(callback(*args, **kwargs))\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_progress_message(self, message, line_break=False):\n\n        end = '\\r' if not line_break else None\n\n        @self.connect\n        def on_progress(value, value_max, **kwargs):\n            kwargs['end'] = None if value == value_max else end\n            _default_on_progress(message, value, value_max, **kwargs)", "response": "Set a progress message."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting a complete message.", "response": "def set_complete_message(self, message):\n        \"\"\"Set a complete message.\"\"\"\n\n        @self.connect\n        def on_complete(**kwargs):\n            _default_on_complete(message, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting a plugin class from its name.", "response": "def get_plugin(name):\n    \"\"\"Get a plugin class from its name.\"\"\"\n    for plugin in IPluginRegistry.plugins:\n        if name in plugin.__name__:\n            return plugin\n    raise ValueError(\"The plugin %s cannot be found.\" % name)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef discover_plugins(dirs):\n    # Scan all subdirectories recursively.\n    for path in _iter_plugin_files(dirs):\n        filename = op.basename(path)\n        subdir = op.dirname(path)\n        modname, ext = op.splitext(filename)\n        file, path, descr = imp.find_module(modname, [subdir])\n        if file:\n            # Loading the module registers the plugin in\n            # IPluginRegistry.\n            try:\n                mod = imp.load_module(modname, file, path, descr)  # noqa\n            except Exception as e:  # pragma: no cover\n                logger.exception(e)\n            finally:\n                file.close()\n    return IPluginRegistry.plugins", "response": "Discover the plugin classes contained in Python files."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nemit close_widget event and emits close_widget event", "response": "def closeEvent(self, e):\n        \"\"\"Qt slot when the window is closed.\"\"\"\n        self.emit('close_widget')\n        super(DockWidget, self).closeEvent(e)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_view_index(self, view):\n        name = view.__class__.__name__\n        return len(self.list_views(name))", "response": "Returns the index of a given view in the GUI."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a view to the main window.", "response": "def add_view(self,\n                 view,\n                 name=None,\n                 position=None,\n                 closable=False,\n                 floatable=True,\n                 floating=None):\n        \"\"\"Add a widget to the main window.\"\"\"\n\n        # Set the name in the view.\n        view.view_index = self._get_view_index(view)\n        # The view name is `<class_name><view_index>`, e.g. `MyView0`.\n        view.name = name or view.__class__.__name__ + str(view.view_index)\n\n        # Get the Qt canvas for VisPy and matplotlib views.\n        widget = _try_get_vispy_canvas(view)\n        widget = _try_get_matplotlib_canvas(widget)\n\n        dock_widget = _create_dock_widget(widget, view.name,\n                                          closable=closable,\n                                          floatable=floatable,\n                                          )\n        self.addDockWidget(_get_dock_position(position), dock_widget)\n        if floating is not None:\n            dock_widget.setFloating(floating)\n        dock_widget.view = view\n\n        # Emit the close_view event when the dock widget is closed.\n        @dock_widget.connect_\n        def on_close_widget():\n            self.emit('close_view', view)\n\n        dock_widget.show()\n        self.emit('add_view', view)\n        logger.log(5, \"Add %s to GUI.\", view.name)\n        return dock_widget"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef list_views(self, name='', is_visible=True):\n        children = self.findChildren(QWidget)\n        return [child.view for child in children\n                if isinstance(child, QDockWidget) and\n                child.view.name.startswith(name) and\n                (child.isVisible() if is_visible else True) and\n                child.width() >= 10 and\n                child.height() >= 10\n                ]", "response": "List all views which name start with a given string."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a view from its name.", "response": "def get_view(self, name, is_visible=True):\n        \"\"\"Return a view from its name.\"\"\"\n        views = self.list_views(name, is_visible=is_visible)\n        return views[0] if views else None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the number of opened views.", "response": "def view_count(self):\n        \"\"\"Return the number of opened views.\"\"\"\n        views = self.list_views()\n        counts = defaultdict(lambda: 0)\n        for view in views:\n            counts[view.name] += 1\n        return dict(counts)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_menu(self, name):\n        if name not in self._menus:\n            self._menus[name] = self.menuBar().addMenu(name)\n        return self._menus[name]", "response": "Return or create a menu."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrestoring the geometry state of the main window and the docks.", "response": "def restore_geometry_state(self, gs):\n        \"\"\"Restore the position of the main window and the docks.\n\n        The gui widgets need to be recreated first.\n\n        This function can be called in `on_show()`.\n\n        \"\"\"\n        if not gs:\n            return\n        if gs.get('geometry', None):\n            self.restoreGeometry((gs['geometry']))\n        if gs.get('state', None):\n            self.restoreState((gs['state']))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate the state of a view.", "response": "def update_view_state(self, view, state):\n        \"\"\"Update the state of a view.\"\"\"\n        if view.name not in self:\n            self[view.name] = Bunch()\n        self[view.name].update(state)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load(self):\n        if not op.exists(self.path):\n            logger.debug(\"The GUI state file `%s` doesn't exist.\", self.path)\n            # TODO: create the default state.\n            return\n        assert op.exists(self.path)\n        logger.debug(\"Load the GUI state from `%s`.\", self.path)\n        self.update(_bunchify(_load_json(self.path)))", "response": "Load the state from the JSON file in the config dir."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsaves the state to the JSON file in the config dir.", "response": "def save(self):\n        \"\"\"Save the state to the JSON file in the config dir.\"\"\"\n        logger.debug(\"Save the GUI state to `%s`.\", self.path)\n        _save_json(self.path, {k: v for k, v in self.items()\n                               if k not in ('config_dir', 'name')})"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncaches a function using the context s cache directory.", "response": "def cache(self, f):\n        \"\"\"Cache a function using the context's cache directory.\"\"\"\n        if self._memory is None:  # pragma: no cover\n            logger.debug(\"Joblib is not installed: skipping cacheing.\")\n            return f\n        assert f\n        # NOTE: discard self in instance methods.\n        if 'self' in inspect.getargspec(f).args:\n            ignore = ['self']\n        else:\n            ignore = None\n        disk_cached = self._memory.cache(f, ignore=ignore)\n        return disk_cached"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncaches a function in memory using an internal dictionary.", "response": "def memcache(self, f):\n        \"\"\"Cache a function in memory using an internal dictionary.\"\"\"\n        name = _fullname(f)\n        cache = self.load_memcache(name)\n\n        @wraps(f)\n        def memcached(*args):\n            \"\"\"Cache the function in memory.\"\"\"\n            # The arguments need to be hashable. Much faster than using hash().\n            h = args\n            out = cache.get(h, None)\n            if out is None:\n                out = f(*args)\n                cache[h] = out\n            return out\n        return memcached"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef save(self, name, data, location='local', kind='json'):\n        file_ext = '.json' if kind == 'json' else '.pkl'\n        path = self._get_path(name, location, file_ext=file_ext)\n        _ensure_dir_exists(op.dirname(path))\n        logger.debug(\"Save data to `%s`.\", path)\n        if kind == 'json':\n            _save_json(path, data)\n        else:\n            _save_pickle(path, data)", "response": "Save a dictionary in a JSON file within the cache directory."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load(self, name, location='local'):\n        path = self._get_path(name, location, file_ext='.json')\n        if op.exists(path):\n            return _load_json(path)\n        path = self._get_path(name, location, file_ext='.pkl')\n        if op.exists(path):\n            return _load_pickle(path)\n        logger.debug(\"The file `%s` doesn't exist.\", path)\n        return {}", "response": "Load saved data from the cache directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _ensure_dir_exists(path):\n    if not op.exists(path):\n        os.makedirs(path)\n    assert op.exists(path) and op.isdir(path)", "response": "Ensure a directory exists."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading a Python or JSON config file.", "response": "def load_config(path=None):\n    \"\"\"Load a Python or JSON config file.\"\"\"\n    if not path or not op.exists(path):\n        return Config()\n    path = op.realpath(path)\n    dirpath, filename = op.split(path)\n    file_ext = op.splitext(path)[1]\n    logger.debug(\"Load config file `%s`.\", path)\n    if file_ext == '.py':\n        config = PyFileConfigLoader(filename, dirpath,\n                                    log=logger).load_config()\n    elif file_ext == '.json':\n        config = JSONFileConfigLoader(filename, dirpath,\n                                      log=logger).load_config()\n    return config"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nloading a master Config file from ~. phy / phy_config. py.", "response": "def load_master_config(config_dir=None):\n    \"\"\"Load a master Config file from `~/.phy/phy_config.py`.\"\"\"\n    config_dir = config_dir or phy_config_dir()\n    path = op.join(config_dir, 'phy_config.py')\n    # Create a default config file if necessary.\n    if not op.exists(path):\n        _ensure_dir_exists(op.dirname(path))\n        logger.debug(\"Creating default phy config file at `%s`.\", path)\n        with open(path, 'w') as f:\n            f.write(_default_config(config_dir=config_dir))\n    assert op.exists(path)\n    return load_config(path)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef save_config(path, config):\n    import json\n    config['version'] = 1\n    with open(path, 'w') as f:\n        json.dump(config, f)", "response": "Save a config object to a JSON file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting a list of edges into an adjacency list.", "response": "def _edges_to_adjacency_list(edges):\n    \"\"\"Convert a list of edges into an adjacency list.\"\"\"\n    adj = {}\n    for i, j in edges:\n        if i in adj:  # pragma: no cover\n            ni = adj[i]\n        else:\n            ni = adj[i] = set()\n        if j in adj:\n            nj = adj[j]\n        else:\n            nj = adj[j] = set()\n        ni.add(j)\n        nj.add(i)\n    return adj"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the positions of a probe channel group.", "response": "def _probe_positions(probe, group):\n    \"\"\"Return the positions of a probe channel group.\"\"\"\n    positions = probe['channel_groups'][group]['geometry']\n    channels = _probe_channels(probe, group)\n    return np.array([positions[channel] for channel in channels])"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns an adjacency list of a whole probe.", "response": "def _probe_adjacency_list(probe):\n    \"\"\"Return an adjacency list of a whole probe.\"\"\"\n    cgs = probe['channel_groups'].values()\n    graphs = [cg['graph'] for cg in cgs]\n    edges = list(itertools.chain(*graphs))\n    adjacency_list = _edges_to_adjacency_list(edges)\n    return adjacency_list"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads one of the built - in probes.", "response": "def load_probe(name):\n    \"\"\"Load one of the built-in probes.\"\"\"\n    if op.exists(name):\n        # The argument can be either a path to a PRB file.\n        path = name\n    else:\n        # Or the name of a built-in probe.\n        curdir = op.realpath(op.dirname(__file__))\n        path = op.join(curdir, 'probes/{}.prb'.format(name))\n    if not op.exists(path):\n        raise IOError(\"The probe `{}` cannot be found.\".format(name))\n    return MEA(probe=_read_python(path))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the list of built - in probes.", "response": "def list_probes():\n    \"\"\"Return the list of built-in probes.\"\"\"\n    curdir = op.realpath(op.dirname(__file__))\n    return [op.splitext(fn)[0] for fn in os.listdir(op.join(curdir, 'probes'))\n            if fn.endswith('.prb')]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef linear_positions(n_channels):\n    return np.c_[np.zeros(n_channels),\n                 np.linspace(0., 1., n_channels)]", "response": "Linear channel positions along the vertical axis."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef staggered_positions(n_channels):\n    i = np.arange(n_channels - 1)\n    x, y = (-1) ** i * (5 + i), 10 * (i + 1)\n    pos = np.flipud(np.r_[np.zeros((1, 2)), np.c_[x, y]])\n    return pos", "response": "Generate channel positions for a staggered probe."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef change_channel_group(self, group):\n        assert self._probe is not None\n        self._channels = _probe_channels(self._probe, group)\n        self._positions = _probe_positions(self._probe, group)", "response": "Change the current channel group."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef attach(self, gui):\n        super(WaveformView, self).attach(gui)\n        self.actions.add(self.toggle_waveform_overlap)\n        self.actions.add(self.toggle_show_labels)\n        self.actions.separator()\n\n        # Box scaling.\n        self.actions.add(self.widen)\n        self.actions.add(self.narrow)\n        self.actions.separator()\n        self.actions.add(self.increase)\n        self.actions.add(self.decrease)\n        self.actions.separator()\n\n        # Probe scaling.\n        self.actions.add(self.extend_horizontally)\n        self.actions.add(self.shrink_horizontally)\n        self.actions.separator()\n        self.actions.add(self.extend_vertically)\n        self.actions.add(self.shrink_vertically)\n\n        # We forward the event from VisPy to the phy GUI.\n        @self.connect\n        def on_channel_click(e):\n            gui.emit('channel_click',\n                     channel_id=e.channel_id,\n                     key=e.key,\n                     button=e.button,\n                     )", "response": "Attach the view to the GUI."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef vertex_count(y=None, **kwargs):\n        return y.size if isinstance(y, np.ndarray) else sum(len(_) for _ in y)", "response": "Take the output of validate() as input."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef build(self):\n        if self.is_built():  # pragma: no cover\n            return\n        with _wait_signal(self.loadFinished, 20):\n            self.rebuild()\n        self._built = True", "response": "Build the full HTML source."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding an object to Javascript.", "response": "def add_to_js(self, name, var):\n        \"\"\"Add an object to Javascript.\"\"\"\n        frame = self.page().mainFrame()\n        frame.addToJavaScriptWindowObject(name, var)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef eval_js(self, expr):\n        if not self.is_built():\n            self._pending_js_eval.append(expr)\n            return\n        logger.log(5, \"Evaluate Javascript: `%s`.\", expr)\n        out = self.page().mainFrame().evaluateJavaScript(expr)\n        return _to_py(out)", "response": "Evaluate a Javascript expression."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a column function which takes an id as argument and returns a value.", "response": "def add_column(self, func, name=None, show=True):\n        \"\"\"Add a column function which takes an id as argument and\n        returns a value.\"\"\"\n        assert func\n        name = name or func.__name__\n        if name == '<lambda>':\n            raise ValueError(\"Please provide a valid name for \" + name)\n        d = {'func': func,\n             'show': show,\n             }\n        self._columns[name] = d\n\n        # Update the headers in the widget.\n        data = _create_json_dict(cols=self.column_names,\n                                 )\n        self.eval_js('table.setHeaders({});'.format(data))\n\n        return func"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nlisting of column names.", "response": "def column_names(self):\n        \"\"\"List of column names.\"\"\"\n        return [name for (name, d) in self._columns.items()\n                if d.get('show', True)]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a row dictionary for a given object id.", "response": "def _get_row(self, id):\n        \"\"\"Create a row dictionary for a given object id.\"\"\"\n        return {name: d['func'](id) for (name, d) in self._columns.items()}"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_rows(self, ids):\n        # NOTE: make sure we have integers and not np.generic objects.\n        assert all(isinstance(i, int) for i in ids)\n\n        # Determine the sort column and dir to set after the rows.\n        sort_col, sort_dir = self.current_sort\n        default_sort_col, default_sort_dir = self.default_sort\n\n        sort_col = sort_col or default_sort_col\n        sort_dir = sort_dir or default_sort_dir or 'desc'\n\n        # Set the rows.\n        logger.log(5, \"Set %d rows in the table.\", len(ids))\n        items = [self._get_row(id) for id in ids]\n        # Sort the rows before passing them to the widget.\n        # if sort_col:\n        #     items = sorted(items, key=itemgetter(sort_col),\n        #                    reverse=(sort_dir == 'desc'))\n        data = _create_json_dict(items=items,\n                                 cols=self.column_names,\n                                 )\n        self.eval_js('table.setData({});'.format(data))\n\n        # Sort.\n        if sort_col:\n            self.sort_by(sort_col, sort_dir)", "response": "Set the rows of the table."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsorts by a given variable.", "response": "def sort_by(self, name, sort_dir='asc'):\n        \"\"\"Sort by a given variable.\"\"\"\n        logger.log(5, \"Sort by `%s` %s.\", name, sort_dir)\n        self.eval_js('table.sortBy(\"{}\", \"{}\");'.format(name, sort_dir))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef select(self, ids, do_emit=True, **kwargs):\n        # Select the rows without emiting the event.\n        self.eval_js('table.select({}, false);'.format(dumps(ids)))\n        if do_emit:\n            # Emit the event manually if needed.\n            self.emit('select', ids, **kwargs)", "response": "Select some rows in the table."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef select_traces(traces, interval, sample_rate=None):\n    start, end = interval\n    i, j = round(sample_rate * start), round(sample_rate * end)\n    i, j = int(i), int(j)\n    traces = traces[i:j, :]\n    # traces = traces - np.mean(traces, axis=0)\n    return traces", "response": "Load traces in an interval."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_interval(self, interval=None, change_status=True,\n                     force_update=False):\n        \"\"\"Display the traces and spikes in a given interval.\"\"\"\n        if interval is None:\n            interval = self._interval\n        interval = self._restrict_interval(interval)\n        if not force_update and interval == self._interval:\n            return\n        self._interval = interval\n        start, end = interval\n        self.clear()\n\n        # Set the status message.\n        if change_status:\n            self.set_status('Interval: {:.3f} s - {:.3f} s'.format(start, end))\n\n        # Load the traces.\n        traces = self.traces(interval)\n\n        # Find the data bounds.\n        ymin, ymax = traces.data.min(), traces.data.max()\n        data_bounds = (start, ymin, end, ymax)\n\n        # Used for spike click.\n        self._data_bounds = data_bounds\n        self._waveform_times = []\n\n        # Plot the traces.\n        self._plot_traces(traces.data,\n                          color=traces.get('color', None),\n                          data_bounds=data_bounds,\n                          )\n\n        # Plot the spikes.\n        waveforms = traces.waveforms\n        assert isinstance(waveforms, list)\n        for w in waveforms:\n            self._plot_waveforms(waveforms=w.data,\n                                 color=w.color,\n                                 channel_ids=w.get('channel_ids', None),\n                                 start_time=w.start_time,\n                                 data_bounds=data_bounds,\n                                 )\n            self._waveform_times.append((w.start_time,\n                                         w.spike_id,\n                                         w.spike_cluster,\n                                         w.get('channel_ids', None),\n                                         ))\n\n        # Plot the labels.\n        if self.do_show_labels:\n            self._plot_labels(traces.data, data_bounds=data_bounds)\n\n        self.build()\n        self.update()", "response": "Display the traces and spikes in a given interval."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef attach(self, gui):\n        super(TraceView, self).attach(gui)\n        self.actions.add(self.go_to, alias='tg')\n        self.actions.separator()\n        self.actions.add(self.shift, alias='ts')\n        self.actions.add(self.go_right)\n        self.actions.add(self.go_left)\n        self.actions.separator()\n        self.actions.add(self.increase)\n        self.actions.add(self.decrease)\n        self.actions.separator()\n        self.actions.add(self.widen)\n        self.actions.add(self.narrow)\n        self.actions.separator()\n        self.actions.add(self.toggle_show_labels)\n\n        # We forward the event from VisPy to the phy GUI.\n        @self.connect\n        def on_spike_click(e):\n            logger.log(5, \"Spike click on channel %s, spike %s, cluster %s.\",\n                       e.channel_id, e.spike_id, e.cluster_id)\n            gui.emit('spike_click',\n                     channel_id=e.channel_id,\n                     spike_id=e.spike_id,\n                     cluster_id=e.cluster_id,\n                     )", "response": "Attach the view to the GUI."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef half_duration(self):\n        if self._interval is not None:\n            a, b = self._interval\n            return (b - a) * .5\n        else:\n            return self.interval_duration * .5", "response": "Half of the duration of the current interval."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngoing to a specific time.", "response": "def go_to(self, time):\n        \"\"\"Go to a specific time (in seconds).\"\"\"\n        half_dur = self.half_duration\n        self.set_interval((time - half_dur, time + half_dur))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef widen(self):\n        t, h = self.time, self.half_duration\n        h *= self.scaling_coeff_x\n        self.set_interval((t - h, t + h))", "response": "Increase the interval size."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndecrease the interval size.", "response": "def narrow(self):\n        \"\"\"Decrease the interval size.\"\"\"\n        t, h = self.time, self.half_duration\n        h /= self.scaling_coeff_x\n        self.set_interval((t - h, t + h))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninitializes auth from HASS config.", "response": "def auth_from_hass_config(path=None, **kwargs):\n    \"\"\"Initialize auth from HASS config.\"\"\"\n    if path is None:\n        path = config.find_hass_config()\n\n    return Auth(os.path.join(path, \".storage/auth\"), **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn name for user.", "response": "def user_name(self, user_id):\n        \"\"\"Return name for user.\"\"\"\n        user = self.users.get(user_id)\n\n        if user is None:\n            return \"Unknown user ({})\".format(user_id)\n\n        return user[\"name\"]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nputs together the default configuration directory based on the OS.", "response": "def default_hass_config_dir():\n    \"\"\"Put together the default configuration directory based on the OS.\"\"\"\n    data_dir = os.getenv(\"APPDATA\") if os.name == \"nt\" else os.path.expanduser(\"~\")\n    return os.path.join(data_dir, \".homeassistant\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntry to find HASS config.", "response": "def find_hass_config():\n    \"\"\"Try to find HASS config.\"\"\"\n    if \"HASSIO_TOKEN\" in os.environ:\n        return \"/config\"\n\n    config_dir = default_hass_config_dir()\n\n    if os.path.isdir(config_dir):\n        return config_dir\n\n    raise ValueError(\n        \"Unable to automatically find the location of Home Assistant \"\n        \"config. Please pass it in.\"\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _secret_yaml(loader, node):\n    fname = os.path.join(os.path.dirname(loader.name), \"secrets.yaml\")\n\n    try:\n        with open(fname, encoding=\"utf-8\") as secret_file:\n            secrets = YAML(typ=\"safe\").load(secret_file)\n    except FileNotFoundError:\n        raise ValueError(\"Secrets file {} not found\".format(fname)) from None\n\n    try:\n        return secrets[node.value]\n    except KeyError:\n        raise ValueError(\"Secret {} not found\".format(node.value)) from None", "response": "Load secrets and embed it into the configuration YAML."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _include_yaml(loader, node):\n    return load_yaml(os.path.join(os.path.dirname(loader.name), node.value))", "response": "Load another YAML file and embeds it using the!include tag."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _stub_tag(constructor, node):\n    seen = getattr(constructor, \"_stub_seen\", None)\n\n    if seen is None:\n        seen = constructor._stub_seen = set()\n\n    if node.tag not in seen:\n        print(\"YAML tag {} is not supported\".format(node.tag))\n        seen.add(node.tag)\n\n    return {}", "response": "Stub a constructor with a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load_yaml(fname):\n    yaml = YAML(typ=\"safe\")\n    # Compat with HASS\n    yaml.allow_duplicate_keys = True\n    # Stub HASS constructors\n    HassSafeConstructor.name = fname\n    yaml.Constructor = HassSafeConstructor\n\n    with open(fname, encoding=\"utf-8\") as conf_file:\n        # If configuration file is empty YAML returns None\n        # We convert that to an empty dict\n        return yaml.load(conf_file) or {}", "response": "Load a YAML file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef db_url_from_hass_config(path):\n    config = load_hass_config(path)\n    default_path = os.path.join(path, \"home-assistant_v2.db\")\n    default_url = \"sqlite:///{}\".format(default_path)\n\n    recorder = config.get(\"recorder\")\n\n    if recorder:\n        db_url = recorder.get(\"db_url\")\n        if db_url is not None:\n            return db_url\n\n    if not os.path.isfile(default_path):\n        raise ValueError(\n            \"Unable to determine DB url from hass config at {}\".format(path)\n        )\n\n    return default_url", "response": "Find the recorder database url from a HASS config dir."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef localize(dt):\n    if dt.tzinfo is UTC:\n        return (dt + LOCAL_UTC_OFFSET).replace(tzinfo=None)\n    # No TZ info so not going to assume anything, return as-is.\n    return dt", "response": "Localize a datetime object to local time."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert a SQLAlchemy datetime string to a datetime object.", "response": "def sqlalch_datetime(dt):\n    \"\"\"Convert a SQLAlchemy datetime string to a datetime object.\"\"\"\n    if isinstance(dt, str):\n        return datetime.strptime(dt, \"%Y-%m-%d %H:%M:%S.%f\").replace(tzinfo=UTC)\n    if dt.tzinfo is not None and dt.tzinfo.utcoffset(dt) is not None:\n        return dt.astimezone(UTC)\n    return dt.replace(tzinfo=UTC)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ninitializes a database from HASS config.", "response": "def db_from_hass_config(path=None, **kwargs):\n    \"\"\"Initialize a database from HASS config.\"\"\"\n    if path is None:\n        path = config.find_hass_config()\n\n    url = config.db_url_from_hass_config(path)\n    return HassDatabase(url, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a version of the DB url with the password stripped out.", "response": "def stripped_db_url(url):\n    \"\"\"Return a version of the DB url with the password stripped out.\"\"\"\n    parsed = urlparse(url)\n\n    if parsed.password is None:\n        return url\n\n    return parsed._replace(\n        netloc=\"{}:***@{}\".format(parsed.username, parsed.hostname)\n    ).geturl()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nperform a query where query is a string.", "response": "def perform_query(self, query, **params):\n        \"\"\"Perform a query, where query is a string.\"\"\"\n        try:\n            return self.engine.execute(query, params)\n        except:\n            print(\"Error with query: {}\".format(query))\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfetching entities for which we have data.", "response": "def fetch_entities(self):\n        \"\"\"Fetch entities for which we have data.\"\"\"\n        query = text(\n            \"\"\"\n            SELECT entity_id\n            FROM states\n            GROUP BY entity_id\n            \"\"\"\n        )\n        response = self.perform_query(query)\n\n        # Parse the domains from the entities.\n        entities = {}\n        domains = set()\n\n        for [entity] in response:\n            domain = entity.split(\".\")[0]\n            domains.add(domain)\n            entities.setdefault(domain, []).append(entity)\n\n        self._domains = list(domains)\n        self._entities = entities\n        print(\"There are {} entities with data\".format(len(entities)))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fetch_data_by_list(self, entities: List[str], limit=50000):\n\n        if not len(set([e.split(\".\")[0] for e in entities])) == 1:\n            print(\"Error: entities must be from same domain.\")\n            return\n\n        if len(entities) == 1:\n            print(\"Must pass more than 1 entity.\")\n            return\n        query = text(\n            \"\"\"\n            SELECT entity_id, state, last_changed\n            FROM states\n            WHERE entity_id in ({})\n            AND NOT state='unknown'\n            ORDER BY last_changed DESC\n            LIMIT :limit\n            \"\"\".format(\n                \",\".join(\"'{}'\".format(ent) for ent in entities)\n            )\n        )\n\n        response = self.perform_query(query, limit=limit)\n        df = pd.DataFrame(response.fetchall())\n        df.columns = [\"entity\", \"state\", \"last_changed\"]\n        df = df.set_index(\"last_changed\")  # Set the index on datetime\n        df.index = pd.to_datetime(df.index, errors=\"ignore\", utc=True)\n        try:\n            df[\"state\"] = (\n                df[\"state\"].mask(df[\"state\"].eq(\"None\")).dropna().astype(float)\n            )\n            df = df.pivot_table(index=\"last_changed\", columns=\"entity\", values=\"state\")\n            df = df.fillna(method=\"ffill\")\n            df = df.dropna()  # Drop any remaining nan.\n            return df\n        except:\n            print(\"Error: entities were not all numericals, unformatted df.\")\n            return df", "response": "Basic query from list of entities and return a DataFrame with the data."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fetch_all_data(self, limit=50000):\n        # Query text\n        query = text(\n            \"\"\"\n            SELECT domain, entity_id, state, last_changed\n            FROM states\n            WHERE\n                state NOT IN ('unknown', 'unavailable')\n            ORDER BY last_changed DESC\n            LIMIT :limit\n            \"\"\"\n        )\n\n        try:\n            print(\"Querying the database, this could take a while\")\n            response = self.perform_query(query, limit=limit)\n            master_df = pd.DataFrame(response.fetchall())\n            print(\"master_df created successfully.\")\n            self._master_df = master_df.copy()\n            self.parse_all_data()\n        except:\n            raise ValueError(\"Error querying the database.\")", "response": "Fetch all data for all entities."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses the master df.", "response": "def parse_all_data(self):\n        \"\"\"Parses the master df.\"\"\"\n        self._master_df.columns = [\"domain\", \"entity\", \"state\", \"last_changed\"]\n\n        # Check if state is float and store in numericals category.\n        self._master_df[\"numerical\"] = self._master_df[\"state\"].apply(\n            lambda x: functions.isfloat(x)\n        )\n\n        # Multiindexing\n        self._master_df.set_index(\n            [\"domain\", \"entity\", \"numerical\", \"last_changed\"], inplace=True\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef correlations(self):\n        corr_df = self._sensors_num_df.corr()\n        corr_names = []\n        corrs = []\n\n        for i in range(len(corr_df.index)):\n            for j in range(len(corr_df.index)):\n                c_name = corr_df.index[i]\n                r_name = corr_df.columns[j]\n                corr_names.append(\"%s-%s\" % (c_name, r_name))\n                corrs.append(corr_df.ix[i, j])\n\n        corrs_all = pd.DataFrame(index=corr_names)\n        corrs_all[\"value\"] = corrs\n        corrs_all = corrs_all.dropna().drop(\n            corrs_all[(corrs_all[\"value\"] == float(1))].index\n        )\n        corrs_all = corrs_all.drop(corrs_all[corrs_all[\"value\"] == float(-1)].index)\n\n        corrs_all = corrs_all.sort_values(\"value\", ascending=False)\n        corrs_all = corrs_all.drop_duplicates()\n        return corrs_all", "response": "Calculate the correlation coefficients for all the sensors in the cluster."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef export_to_csv(self, entities: List[str], filename=\"sensors.csv\"):\n        if not set(entities).issubset(set(self._sensors_num_df.columns.tolist())):\n            print(\"Invalid entities entered, aborting export_to_csv\")\n            return\n        try:\n            self._sensors_num_df[entities].to_csv(path_or_buf=filename)\n            print(f\"Successfully exported entered entities to {filename}\")\n        except Exception as exc:\n            print(exc)", "response": "Export selected sensor data to a csv file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_sf_database(db, model=None):\n    from django.db import connections\n    if db is None:\n        return getattr(model, '_salesforce_object', False)\n    engine = connections[db].settings_dict['ENGINE']\n    return engine == 'salesforce.backend' or connections[db].vendor == 'salesforce'", "response": "The alias is a Salesforce database."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef db_for_write(self, model, **hints):\n        if 'instance' in hints:\n            db = hints['instance']._state.db\n            if db:\n                return db\n        if getattr(model, '_salesforce_object', False):\n            return self.sf_alias", "response": "Returns the db_alias for the given model."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nallows to migrate to a specific SF database.", "response": "def allow_migrate(self, db, app_label, model_name=None, **hints):\n        \"\"\"\n        Don't attempt to sync SF models to non SF databases and vice versa.\n        \"\"\"\n        if model_name:\n            model = apps.get_model(app_label, model_name)\n        else:\n            # hints are used with less priority, because many hints are dynamic\n            # models made by migrations on a '__fake__' module which are not\n            # SalesforceModels\n            model = hints.get('model')\n\n        if hasattr(model, '_salesforce_object'):\n            # SF models can be migrated if SALESFORCE_DB_ALIAS is e.g.\n            # a sqlite3 database or any non-SF database.\n            if not (is_sf_database(db) or db == self.sf_alias):\n                return False\n        else:\n            if is_sf_database(db) or self.sf_alias != 'default' and db == self.sf_alias:\n                return False\n        # TODO: It is usual that \"migrate\" is currently disallowed for SF.\n        # In the future it can be implemented to do a deep check by\n        # introspection of compatibily between Django models and SF database.\n        if hasattr(model, '_salesforce_object'):\n            # return False\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncustomizing the lazy field", "response": "def update(self, **kwargs):\n        \"\"\"Customize the lazy field\"\"\"\n        assert not self.called\n        self.kw.update(kwargs)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a normal field from the lazy field", "response": "def create(self):\n        \"\"\"Create a normal field from the lazy field\"\"\"\n        assert not self.called\n        return self.klass(*self.args, **self.kw)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_queryset(self):\n        if router.is_sf_database(self.db):\n            q = models_sql_query.SalesforceQuery(self.model, where=compiler.SalesforceWhereNode)\n            return query.SalesforceQuerySet(self.model, query=q, using=self.db)\n        return super(SalesforceManager, self).get_queryset()", "response": "Returns a QuerySet which access remote SF objects."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_attname_column(self):\n        # See \"A guide to Field parameters\": django/db/models/fields/__init__.py\n        #   * attname:   The attribute to use on the model object. This is the same as\n        #                \"name\", except in the case of ForeignKeys, where \"_id\" is\n        #                appended.\n        #   * column:    The database column for this field. This is the same as\n        #                \"attname\", except if db_column is specified.\n        attname = self.get_attname()\n        if self.db_column is not None:\n            # explicit name\n            column = self.db_column\n        else:\n            if not self.name.islower():\n                # a Salesforce style name e.g. 'LastName' or 'MyCustomField'\n                column = self.name\n            else:\n                # a Django style name like 'last_name' or 'my_custom_field'\n                column = self.name.title().replace('_', '')\n            # Fix custom fields\n            if self.sf_custom:\n                column = self.sf_namespace + column + '__c'\n        return attname, column", "response": "Get the database column name automatically in most cases."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef prep_for_deserialize(model, record, using, init_list=None):  # pylint:disable=unused-argument\n    # TODO the parameter 'using' is not currently important.\n    attribs = record.pop('attributes')  # NOQA pylint:disable=unused-variable\n\n    mod = model.__module__.split('.')\n    if hasattr(model._meta, 'app_label'):\n        app_label = getattr(model._meta, 'app_label')\n    elif mod[-1] == 'models':\n        app_label = mod[-2]\n    else:\n        raise ImproperlyConfigured(\"Can't discover the app_label for %s, you must specify it via model meta options.\")\n\n    if len(record.keys()) == 1 and model._meta.db_table in record:\n        # this is for objects with ManyToManyField and OneToOneField\n        while len(record) == 1:\n            record = list(record.values())[0]\n            if record is None:\n                return None\n\n    fields = prep_for_deserialize_inner(model, record, init_list=init_list)\n\n    if init_list and set(init_list).difference(fields).difference([SF_PK]):\n        raise DatabaseError(\"Not found some expected fields\")\n\n    return dict(\n        model='.'.join([app_label, model.__name__]),\n        pk=record.pop('Id'),\n        fields=fields,\n    )", "response": "Convert a record from SFDC to dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef extract_values(query):\n    # pylint\n    if isinstance(query, subqueries.UpdateQuery):\n        row = query.values\n        return extract_values_inner(row, query)\n    if isinstance(query, subqueries.InsertQuery):\n        ret = []\n        for row in query.objs:\n            ret.append(extract_values_inner(row, query))\n        return ret\n    raise NotSupportedError", "response": "Extract values from insert or update query."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef str_dict(some_dict):\n    return {str(k): str(v) for k, v in some_dict.items()}", "response": "Convert dict of ascii str or unicode to dict of str if necessary"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsends a query to the Salesforce API and return the result.", "response": "def execute(self, q, args=()):\n        \"\"\"\n        Send a query to the Salesforce API.\n        \"\"\"\n        # pylint:disable=too-many-branches\n        self.rowcount = None\n        response = None\n        if self.query is None:\n            self.execute_select(q, args)\n        else:\n            response = self.execute_django(q, args)\n            if isinstance(response, list):\n                return\n\n        # the encoding is detected automatically, e.g. from headers\n        if response and response.text:\n            # parse_float set to decimal.Decimal to avoid precision errors when\n            # converting from the json number to a float and then to a Decimal object\n            # on a model's DecimalField. This converts from json number directly\n            # to a Decimal object\n            data = response.json(parse_float=decimal.Decimal)\n            # a SELECT query\n            if 'totalSize' in data:\n                # SELECT\n                self.rowcount = data['totalSize']\n            # a successful INSERT query, return after getting PK\n            elif('success' in data and 'id' in data):\n                self.lastrowid = data['id']\n                return\n            elif 'compositeResponse' in data:\n                # TODO treat error reporting for composite requests\n                self.lastrowid = [x['body']['id'] if x['body'] is not None else x['referenceId']\n                                  for x in data['compositeResponse']]\n                return\n            elif data['hasErrors'] is False:\n                # it is from Composite Batch request\n                # save id from bulk_create even if Django don't use it\n                if data['results'] and data['results'][0]['result']:\n                    self.lastrowid = [item['result']['id'] for item in data['results']]\n                return\n            # something we don't recognize\n            else:\n                raise DatabaseError(data)\n\n            if not q.upper().startswith('SELECT COUNT() FROM'):\n                self.first_row = data['records'][0] if data['records'] else None"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nexecute a Django query and return the result.", "response": "def execute_django(self, soql, args=()):\n        \"\"\"\n        Fixed execute for queries coming from Django query compilers\n        \"\"\"\n        response = None\n        sqltype = soql.split(None, 1)[0].upper()\n        if isinstance(self.query, subqueries.InsertQuery):\n            response = self.execute_insert(self.query)\n        elif isinstance(self.query, subqueries.UpdateQuery):\n            response = self.execute_update(self.query)\n        elif isinstance(self.query, subqueries.DeleteQuery):\n            response = self.execute_delete(self.query)\n        elif isinstance(self.query, RawQuery):\n            self.execute_select(soql, args)\n        elif sqltype in ('SAVEPOINT', 'ROLLBACK', 'RELEASE'):\n            log.info(\"Ignored SQL command '%s'\", sqltype)\n            return\n        elif isinstance(self.query, Query):\n            self.execute_select(soql, args)\n        else:\n            raise DatabaseError(\"Unsupported query: type %s: %s\" % (type(self.query), self.query))\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_pks_from_query(self, query):\n        where = query.where\n        sql = None\n        if where.connector == 'AND' and not where.negated and len(where.children) == 1:\n            # simple cases are optimized, especially because a suboptimal\n            # nested query based on the same table is not allowed by SF\n            child = where.children[0]\n            if (child.lookup_name in ('exact', 'in') and child.lhs.target.column == 'Id'\n                    and not child.bilateral_transforms and child.lhs.target.model is self.query.model):\n                pks = child.rhs\n                if child.lookup_name == 'exact':\n                    assert isinstance(pks, text_type)\n                    return [pks]\n                # lookup_name 'in'\n                assert not child.bilateral_transforms\n                if isinstance(pks, (tuple, list)):\n                    return pks\n                if DJANGO_111_PLUS:\n                    assert isinstance(pks, Query) and type(pks).__name__ == 'SalesforceQuery'\n                    # # alternative solution:\n                    # return list(salesforce.backend.query.SalesforceQuerySet(pk.model, query=pk, using=pk._db))\n\n                    sql, params = pks.get_compiler('salesforce').as_sql()\n                else:\n                    assert isinstance(pks, salesforce.backend.query.SalesforceQuerySet)\n                    return [x.pk for x in pks]\n        if not sql:\n            # a subquery is necessary in this case\n            where_sql, params = where.as_sql(query.get_compiler('salesforce'), self.db.connection)\n            sql = \"SELECT Id FROM {} WHERE {}\".format(query.model._meta.db_table, where_sql)\n        with self.db.cursor() as cur:\n            cur.execute(sql, params)\n            assert len(cur.description) == 1 and cur.description[0][0] == 'Id'\n            return [x[0] for x in cur]", "response": "Prepare primary keys for update and delete queries"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nlists Available REST API Versions", "response": "def versions_request(self):\n        \"\"\"List Available REST API Versions\"\"\"\n        ret = self.handle_api_exceptions('GET', '', api_ver='')\n        return [str_dict(x) for x in ret.json()]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fix_field_params_repr(params):\n    class ReprUnicode(text_type):\n        def __new__(cls, text):\n            return text_type.__new__(cls, text)\n\n        def __repr__(self):\n            out = repr(text_type(self))\n            return out[1:] if out.startswith(\"u'\") or out.startswith('u\"') else out\n\n    class ReprChoices(list):\n        def __new__(cls, choices):\n            return list.__new__(cls, choices)\n\n        def __repr__(self):\n            out = []\n            for x_0, x_1 in self:\n                out.append('(%s, %s)' % (\n                    repr(ReprUnicode(x_0) if isinstance(x_0, text_type) else x_0),\n                    repr(ReprUnicode(x_1) if isinstance(x_1, text_type) else x_1)\n                ))\n            return '[%s]' % (', '.join(out))\n    if PY3:\n        return params\n    out = OrderedDict()\n    for k, v in params.items():\n        if k == 'choices' and v:\n            v = ReprChoices(v)\n        elif isinstance(v, text_type):\n            v = ReprUnicode(v)\n        out[k] = v\n    return out", "response": "Fixes repr of field_params for Python 2 with future text_type_literals."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fix_international(text):\n    \"Fix excaped international characters back to utf-8\"\n    class SmartInternational(str):\n        def __new__(cls, text):\n            return str.__new__(cls, text)\n\n        def endswith(self, string):\n            return super(SmartInternational, self).endswith(str(string))\n    if PY3:\n        return text\n    out = []\n    last = 0\n    for match in re.finditer(r'(?<=[^\\\\])(?:\\\\x[0-9a-f]{2}|\\\\u[0-9a-f]{4})', text):\n        start, end, group = match.start(), match.end(), match.group()\n        out.append(text[last:start])\n        c = group.decode('unicode_escape')\n        out.append(c if ord(c) > 160 and ord(c) != 173 else group)\n        last = end\n    out.append(text[last:])\n    return SmartInternational(''.join(out).encode('utf-8'))", "response": "Fix excaped international characters back to utf - 8"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a sequence of lines of code needed to construct the inner Meta class for the given database table name.", "response": "def get_meta(self, table_name, constraints=None, column_to_field_name=None, is_view=False, is_partition=None):\n        \"\"\"\n        Return a sequence comprising the lines of code necessary\n        to construct the inner Meta class for the model corresponding\n        to the given database table name.\n        \"\"\"\n        # pylint:disable=arguments-differ,too-many-arguments,unused-argument\n        meta = [\"    class Meta(models.Model.Meta):\",\n                \"        db_table = '%s'\" % table_name]\n        if self.connection.vendor == 'salesforce':\n            for line in self.connection.introspection.get_additional_meta(table_name):\n                meta.append(\"        \" + line)\n        meta.append(\"\")\n        return meta"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the given path relative to this file.", "response": "def relative_path(path):\n    \"\"\"\n    Return the given path relative to this file.\n    \"\"\"\n    return os.path.join(os.path.dirname(__file__), path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_tagged_version():\n    with open(relative_path('salesforce/__init__.py'), 'r') as fd:\n        version = re.search(r'^__version__\\s*=\\s*[\\'\"]([^\\'\"]*)[\\'\"]',\n                            fd.read(), re.MULTILINE).group(1)\n    return version", "response": "Determine the current version of this package."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_auth(self):\n        if self.dynamic:\n            return self.dynamic\n        if self.settings_dict['USER'] == 'dynamic auth':\n            return {'instance_url': self.settings_dict['HOST']}\n        # If another thread is running inside this method, wait for it to\n        # finish. Always release the lock no matter what happens in the block\n        db_alias = self.db_alias\n        with oauth_lock:\n            if db_alias not in oauth_data:\n                oauth_data[db_alias] = self.authenticate()\n            return oauth_data[db_alias]", "response": "Get the authentication data for the current user."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the access token dynamically according to the current user.", "response": "def dynamic_start(self, access_token, instance_url=None, **kw):\n        \"\"\"\n        Set the access token dynamically according to the current user.\n\n        More parameters can be set.\n        \"\"\"\n        self.dynamic = {'access_token': str(access_token), 'instance_url': str(instance_url)}\n        self.dynamic.update(kw)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef authenticate(self):\n        settings_dict = self.settings_dict\n        url = ''.join([settings_dict['HOST'], '/services/oauth2/token'])\n\n        log.info(\"attempting authentication to %s\", settings_dict['HOST'])\n        self._session.mount(settings_dict['HOST'], HTTPAdapter(max_retries=get_max_retries()))\n        auth_params = {\n            'grant_type':    'password',\n            'client_id':     settings_dict['CONSUMER_KEY'],\n            'client_secret': settings_dict['CONSUMER_SECRET'],\n            'username':      settings_dict['USER'],\n            'password':      settings_dict['PASSWORD'],\n        }\n        response = self._session.post(url, data=auth_params)\n        if response.status_code == 200:\n            # prefer str in Python 2 due to other API\n            response_data = {str(k): str(v) for k, v in response.json().items()}\n            # Verify signature (not important for this auth mechanism)\n            calc_signature = (\n                base64.b64encode(\n                    hmac.new(\n                        key=settings_dict['CONSUMER_SECRET'].encode('ascii'),\n                        msg=(response_data['id'] + response_data['issued_at']).encode('ascii'),\n                        digestmod=hashlib.sha256\n                    ).digest()\n                )\n            ).decode('ascii')\n            if calc_signature == response_data['signature']:\n                log.info(\"successfully authenticated %s\", settings_dict['USER'])\n            else:\n                raise IntegrityError('Invalid auth signature received')\n        else:\n            raise LookupError(\"oauth failed: %s: %s\" % (settings_dict['USER'], response.text))\n        return response_data", "response": "Authenticate to the Salesforce API with the provided credentials."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef mark_quoted_strings(sql):\n    # pattern of a string parameter (pm), a char escaped by backslash (bs)\n    # out_pattern: characters valid in SOQL\n    pm_pattern = re.compile(r\"'[^\\\\']*(?:\\\\[\\\\'][^\\\\']*)*'\")\n    bs_pattern = re.compile(r\"\\\\([\\\\'])\")\n    out_pattern = re.compile(r\"^(?:[-!()*+,.:<=>\\w\\s|%s])*$\")\n    missing_apostrophe = \"invalid character in SOQL or a missing apostrophe\"\n    start = 0\n    out = []\n    params = []\n    for match in pm_pattern.finditer(sql):\n        out.append(sql[start:match.start()])\n        assert out_pattern.match(sql[start:match.start()]), missing_apostrophe\n        params.append(bs_pattern.sub('\\\\1', sql[match.start() + 1:match.end() - 1]))\n        start = match.end()\n    out.append(sql[start:])\n    assert out_pattern.match(sql[start:]), missing_apostrophe\n    return '@'.join(out), params", "response": "Mark all quoted strings in the SOQL by '@' and get them as params and\n    with respect to all escaped backslashes and quotes."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef subst_quoted_strings(sql, params):\n    parts = sql.split('@')\n    params_dont_match = \"number of parameters doesn' match the transformed query\"\n    assert len(parts) == len(params) + 1, params_dont_match  # would be internal error\n    out = []\n    for i, param in enumerate(params):\n        out.append(parts[i])\n        out.append(\"'%s'\" % param.replace('\\\\', '\\\\\\\\').replace(\"\\'\", \"\\\\\\'\"))\n    out.append(parts[-1])\n    return ''.join(out)", "response": "Reverse operation to mark_quoted_strings - substitutes '@' by params.\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind the pair of opening and closing parentheses.", "response": "def find_closing_parenthesis(sql, startpos):\n    \"\"\"Find the pair of opening and closing parentheses.\n\n    Starts search at the position startpos.\n    Returns tuple of positions (opening, closing) if search succeeds, otherwise None.\n    \"\"\"\n    pattern = re.compile(r'[()]')\n    level = 0\n    opening = []\n    for match in pattern.finditer(sql, startpos):\n        par = match.group()\n        if par == '(':\n            if level == 0:\n                opening = match.start()\n            level += 1\n        if par == ')':\n            assert level > 0, \"missing '(' before ')'\"\n            level -= 1\n            if level == 0:\n                closing = match.end()\n                return opening, closing"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalls a func on every part of SOQL query except nested SELECT...", "response": "def transform_except_subquery(sql, func):\n    \"\"\"Call a func on every part of SOQL query except nested (SELECT ...)\"\"\"\n    start = 0\n    out = []\n    while sql.find('(SELECT', start) > -1:\n        pos = sql.find('(SELECT', start)\n        out.append(func(sql[start:pos]))\n        start, pos = find_closing_parenthesis(sql, pos)\n        out.append(sql[start:pos])\n        start = pos\n    out.append(func(sql[start:len(sql)]))\n    return ''.join(out)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsplit on subqueries and replace them by '&'.", "response": "def split_subquery(sql):\n    \"\"\"Split on subqueries and replace them by '&'.\"\"\"\n    sql, params = mark_quoted_strings(sql)\n    sql = simplify_expression(sql)\n    _ = params  # NOQA\n    start = 0\n    out = []\n    subqueries = []\n    pattern = re.compile(r'\\(SELECT\\b', re.I)\n    match = pattern.search(sql, start)\n    while match:\n        out.append(sql[start:match.start() + 1] + '&')\n        start, pos = find_closing_parenthesis(sql, match.start())\n        start, pos = start + 1, pos - 1\n        subqueries.append(split_subquery(sql[start:pos]))\n        start = pos\n        match = pattern.search(sql, start)\n    out.append(sql[start:len(sql)])\n    return ''.join(out), subqueries"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsimplifies a string into a single expression.", "response": "def simplify_expression(txt):\n    \"\"\"Remove all unecessary whitespace and some very usual space\"\"\"\n    minimal = re.sub(r'\\s', ' ',\n                     re.sub(r'\\s(?=\\W)', '',\n                            re.sub(r'(?<=\\W)\\s', '',\n                                   txt.strip())))\n    # add space before some \"(\" and after some \")\"\n    return re.sub(r'\\)(?=\\w)', ') ',\n                  re.sub(r'(,|\\b(?:{}))\\('.format('|'.join(RESERVED_WORDS)), '\\\\1 (', minimal)\n                  )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating Force. com SOQL tree structure from a SQL query string.", "response": "def _from_sql(self, soql):\n        \"\"\"Create Force.com SOQL tree structure from SOQL\"\"\"\n        # pylint:disable=too-many-branches,too-many-nested-blocks\n        assert not self.soql, \"Don't use _from_sql method directly\"\n        self.soql = soql\n        soql, self.subqueries = split_subquery(soql)\n        match_parse = re.match(r'SELECT (.*) FROM (\\w+)\\b(.*)$', soql, re.I)\n        if not match_parse:\n            raise ProgrammingError('Invalid SQL: %s' % self.soql)\n        fields_sql, self.root_table, self.extra_soql = match_parse.groups()\n        fields = [x.strip() for x in fields_sql.split(',')]\n        self.is_aggregation = bool(pattern_groupby.search(self.extra_soql) or\n                                   pattern_aggregation.search(fields[0]))\n        self.is_plain_count = fields[0].upper() == 'COUNT()'\n        consumed_subqueries = 0\n        expr_alias_counter = 0\n        #\n        if not self.is_plain_count:\n            for field in fields:\n                if self.is_aggregation:\n                    match = re.search(r'\\b\\w+$', field)\n                    if match:\n                        alias = match.group()\n                        assert alias not in RESERVED_WORDS, \"invalid alias name\"\n                        if match.start() > 0 and field[match.start() - 1] == ' ':\n                            field = field[match.start() - 1]\n                    else:\n                        alias = 'expr{}'.format(expr_alias_counter)\n                        expr_alias_counter += 1\n                    assert '&' not in field, \"Subquery not expected as field in aggregation query\"\n                elif '&' in field:\n                    assert field == '(&)'  # verify that the subquery was in parentheses\n                    subquery = QQuery(self.subqueries[consumed_subqueries][0])\n                    consumed_subqueries += 1\n                    self.has_child_rel_field = True\n                    field = subquery\n                    # TODO more child relationships to the same table\n                    alias = subquery.root_table\n                else:\n                    alias = field\n                    if '.' in alias:\n                        if alias.split('.', 1)[0].lower() == self.root_table.lower():\n                            alias = alias.split('.', 1)[1]\n                        if '.' in alias:\n                            # prepare paths for possible empty outer joins\n                            subroots = self.subroots\n                            root_crumbs = alias.lower().split('.')[:-1]\n                            for scrumb in root_crumbs:\n                                subroots.setdefault(scrumb, {})\n                                subroots = subroots[scrumb]\n                self.aliases.append(alias)\n                self.fields.append(field)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _make_flat(self, row_dict, path, subroots):\n        # can get a cursor parameter, if introspection should be possible on the fly\n        out = {}\n        for k, v in row_dict.items():\n            klc = k.lower()  # \"key lower case\"\n            if (not (isinstance(v, dict) and 'attributes' in v)\n                    or ('done' in v and 'records' in v and 'totalSize' in v)):\n                # :\n                if klc not in subroots:\n                    out[klc] = v\n                else:\n                    strpath = '.'.join(path + (klc,)) + '.'\n                    strip_pos = len(strpath) - len(klc + '.')\n                    for alias in self.aliases:\n                        if alias.lower().startswith(strpath):\n                            out[alias.lower()[strip_pos:]] = None  # empty outer join field names\n            else:\n                new_subroots = subroots[klc] if k != 'attributes' else {}\n                for sub_k, sub_v in self._make_flat(v, path + (klc,), new_subroots).items():\n                    out[k.lower() + '.' + sub_k] = sub_v\n        return out", "response": "Replace the nested dict objects by a flat dict with keys object. name."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses the REST API response to DB API cursor flat response", "response": "def parse_rest_response(self, records, rowcount, row_type=list):\n        \"\"\"Parse the REST API response to DB API cursor flat response\"\"\"\n        if self.is_plain_count:\n            # result of \"SELECT COUNT() FROM ... WHERE ...\"\n            assert list(records) == []\n            yield rowcount  # originally [resp.json()['totalSize']]\n        else:\n            while True:\n                for row_deep in records:\n                    assert self.is_aggregation == (row_deep['attributes']['type'] == 'AggregateResult')\n                    row_flat = self._make_flat(row_deep, path=(), subroots=self.subroots)\n                    # TODO Will be the expression \"or x['done']\" really correct also for long subrequests?\n                    assert all(not isinstance(x, dict) or x['done'] for x in row_flat)\n                    if issubclass(row_type, dict):\n                        yield {k: fix_data_type(row_flat[k.lower()]) for k in self.aliases}\n                    else:\n                        yield [fix_data_type(row_flat[k.lower()]) for k in self.aliases]\n                # if not resp['done']:\n                #     if not cursor:\n                #         raise ProgrammingError(\"Must get a cursor\")\n                #     resp = cursor.query_more(resp['nextRecordsUrl']).json()\n                # else:\n                #     break\n                break"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the count of the current object.", "response": "def get_count(self, using):\n        # TODO maybe can be removed soon\n        \"\"\"\n        Performs a COUNT() query using the current filter constraints.\n        \"\"\"\n        obj = self.clone()\n        obj.add_annotation(Count('pk'), alias='x_sf_count', is_summary=True)  # pylint: disable=no-member\n        number = obj.get_aggregation(using, ['x_sf_count'])['x_sf_count']  # pylint: disable=no-member\n        if number is None:\n            number = 0\n        return number"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nallowing querying for also deleted or merged records.", "response": "def query_all(self):\n        \"\"\"\n        Allows querying for also deleted or merged records.\n            Lead.objects.query_all().filter(IsDeleted=True,...)\n        https://www.salesforce.com/us/developer/docs/api_rest/Content/resources_queryall.htm\n        \"\"\"\n        if DJANGO_20_PLUS:\n            obj = self._clone()\n        else:\n            obj = self._clone(klass=SalesforceQuerySet)  # pylint: disable=unexpected-keyword-arg\n        obj.query.set_query_all()\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef make_dynamic_fields(pattern_module, dynamic_field_patterns, attrs):\n    # pylint:disable=invalid-name,too-many-branches,too-many-locals\n    import re\n    attr_meta = attrs['Meta']\n    db_table = getattr(attr_meta, 'db_table', None)\n    if not db_table:\n        raise RuntimeError('The \"db_table\" must be set in Meta if \"dynamic_field_patterns\" is used.')\n    is_custom_model = getattr(attr_meta, 'custom', False)\n\n    patterns = []\n    for pat in dynamic_field_patterns:\n        enabled = True\n        if pat.startswith('-'):\n            enabled = False\n            pat = pat[1:]\n        patterns.append((enabled, re.compile(r'^(?:{})$'.format(pat), re.I)))\n\n    used_columns = []\n    for name, attr in attrs.items():\n        if isinstance(attr, SfField):\n            field = attr\n            if field.sf_custom is None and is_custom_model:\n                field.sf_custom = True\n            if not field.name:\n                field.name = name\n            attname, column = field.get_attname_column()  # pylint:disable=unused-variable\n            used_columns.append(column)\n\n    if not pattern_module:\n        raise RuntimeError(\"a pattern_module is required for dynamic fields.\")\n    for name, obj in vars(pattern_module).items():\n        if not name.startswith('_') and isclass(obj) and issubclass(obj, ModelTemplate):\n            default_table = obj.__name__\n            if getattr(getattr(obj, 'Meta', None), 'db_table', default_table) == db_table:\n                cls = obj\n                break\n    else:\n        # not found db_table model, but decide between warning or exception\n        if any(not x.startswith('__') for x in dir(pattern_module)):\n            raise RuntimeError(\"No Model for table '%s' found in the module '%s'\"\n                               % (db_table, pattern_module.__name__))\n        warnings.warn(\"The module '%s' is empty. (It is OK if you are \"\n                      \"rewriting new Models by pipe from inspectdb command.)\"\n                      % pattern_module.__name__)\n        return\n    lazy_fields = [(name, obj) for name, obj in vars(cls).items()\n                   if isinstance(obj, LazyField) and issubclass(obj.klass, SfField)\n                   ]\n    for name, obj in sorted(lazy_fields, key=lambda name_obj: name_obj[1].counter):\n        for enabled, pat in patterns:\n            if pat.match(name):\n                break\n        else:\n            enabled = False\n        if enabled:\n            if issubclass(obj.klass, ForeignKey):\n                to = obj.kw['to']\n                if isclass(to) and issubclass(to, ModelTemplate):\n                    obj.kw['to'] = to.__name__\n            field = obj.create()\n            attrs[name] = field\n    assert pattern_module", "response": "This function creates some Salesforce fields from a pattern_module models. py file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_table_list(self, cursor):\n        \"Returns a list of table names in the current database.\"\n        result = [TableInfo(SfProtectName(x['name']), 't') for x in self.table_list_cache['sobjects']]\n        return result", "response": "Returns a list of table names in the current database."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a description of the table with the DB - API cursor. description interface.", "response": "def get_table_description(self, cursor, table_name):\n        \"Returns a description of the table, with the DB-API cursor.description interface.\"\n        # pylint:disable=too-many-locals,unused-argument\n        result = []\n        for field in self.table_description_cache(table_name)['fields']:\n            params = OrderedDict()\n            if field['label'] and field['label'] != camel_case_to_spaces(re.sub('__c$', '', field['name'])).title():\n                params['verbose_name'] = field['label']\n            if not field['updateable'] or not field['createable']:\n                # Fields that are result of a formula or system fields modified\n                # by triggers or by other apex code\n                sf_read_only = (0 if field['updateable'] else 1) | (0 if field['createable'] else 2)\n                # use symbolic names NOT_UPDATEABLE, NON_CREATABLE, READ_ONLY instead of 1, 2, 3\n                params['sf_read_only'] = reverse_models_names[sf_read_only]\n            if field['defaultValue'] is not None:\n                params['default'] = field['defaultValue']\n            if field['inlineHelpText']:\n                params['help_text'] = field['inlineHelpText']\n            if field['picklistValues']:\n                params['choices'] = [(x['value'], x['label']) for x in field['picklistValues'] if x['active']]\n            if field['defaultedOnCreate'] and field['createable']:\n                params['default'] = SymbolicModelsName('DEFAULTED_ON_CREATE')\n            if field['type'] == 'reference' and not field['referenceTo']:\n                params['ref_comment'] = 'No Reference table'\n                field['type'] = 'string'\n            if field['calculatedFormula']:\n                # calculated formula field are without length in Salesforce 45 Spring '19,\n                # but Django requires a length, though the field is read only and never written\n                field['length'] = 1300\n            # We prefer \"length\" over \"byteLength\" for \"internal_size\".\n            # (because strings have usually: byteLength == 3 * length)\n            result.append(FieldInfo(\n                field['name'],       # name,\n                field['type'],       # type_code,\n                field['length'],     # display_size,\n                field['length'],     # internal_size,\n                field['precision'],  # precision,\n                field['scale'],      # scale,\n                field['nillable'],   # null_ok,\n                params.get('default'),  # default\n                params,\n            ))\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a dictionary of all relationships to the given table.", "response": "def get_relations(self, cursor, table_name):\n        \"\"\"\n        Returns a dictionary of {field_index: (field_index_other_table, other_table)}\n        representing all relationships to the given table. Indexes are 0-based.\n        \"\"\"\n        # pylint:disable=global-statement,too-many-locals,too-many-nested-blocks,unused-argument\n        def table2model(table_name):\n            return SfProtectName(table_name).title().replace(' ', '').replace('-', '')\n        global last_introspected_model, last_read_only, last_refs\n        global last_with_important_related_name  # pylint:disable=invalid-name\n        result = {}\n        reverse = {}\n        last_with_important_related_name = []\n        last_read_only = {}\n        last_refs = {}\n        for _, field in enumerate(self.table_description_cache(table_name)['fields']):\n            if field['type'] == 'reference' and field['referenceTo']:\n                reference_to_name = SfProtectName(field['referenceTo'][0])\n                relationship_order = field['relationshipOrder']\n                if relationship_order is None:\n                    relationship_tmp = set()\n                    for rel in field['referenceTo']:\n                        for chld in self.table_description_cache(rel)['childRelationships']:\n                            if chld['childSObject'] == table_name and chld['field'] == field['name']:\n                                relationship_tmp.add(chld['cascadeDelete'])\n                    assert len(relationship_tmp) <= 1\n                    if True in relationship_tmp:\n                        relationship_order = '*'\n                last_refs[field['name']] = (field['referenceTo'], relationship_order)\n                result[field['name']] = ('Id', reference_to_name)\n                reverse.setdefault(reference_to_name, []).append(field['name'])\n                if not field['updateable'] or not field['createable']:\n                    sf_read_only = (0 if field['updateable'] else 1) | (0 if field['createable'] else 2)\n                    # use symbolic names NOT_UPDATEABLE, NON_CREATABLE, READ_ONLY instead of 1, 2, 3\n                    last_read_only[field['name']] = reverse_models_names[sf_read_only]\n        for ref, ilist in reverse.items():\n            # Example of back_collision: a class Aaa has a ForeignKey to a class\n            # Bbb and the class Bbb has any field with the name 'aaa'.\n            back_name_collisions = [\n                x['name'] for x in self.table_description_cache(ref)['fields']\n                if re.sub('Id$' if x['type'] == 'reference' else '', '',\n                          re.sub('__c$', '', x['name'])\n                          ).replace('_', '').lower() == table2model(table_name).lower()]\n            # add `related_name` only if necessary\n            if len(ilist) > 1 or back_name_collisions:\n                last_with_important_related_name.extend(ilist)\n        last_introspected_model = table2model(table_name)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_constraints(self, cursor, table_name):\n        result = {}\n        for field in self.table_description_cache(table_name)['fields']:\n            if field['type'] in ('id', 'reference') or field['unique']:\n                result[field['name']] = dict(\n                    columns=[field['name']],\n                    primary_key=(field['type'] == 'id'),\n                    unique=field['unique'],\n                    foreign_key=(field['referenceTo'], 'id'),\n                    check=False,\n                )\n        return result", "response": "Returns any constraints or keys that are defined for the table_name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\npreparing excetion params or only an exception message", "response": "def prepare_exception(obj, messages=None, response=None, verbs=None):\n    \"\"\"Prepare excetion params or only an exception message\n\n    parameters:\n        messages: list of strings, that will be separated by new line\n        response: response from a request to SFDC REST API\n        verbs: list of options about verbosity\n    \"\"\"\n    # pylint:disable=too-many-branches\n    verbs = set(verbs or [])\n    known_options = ['method+url']\n    if messages is None:\n        messages = []\n    if isinstance(messages, (text_type, str)):\n        messages = [messages]\n    assert isinstance(messages, list)\n    assert not verbs.difference(known_options)\n\n    data = None\n    # a boolean from a failed response is False, though error messages in json should be decoded\n    if response is not None and 'json' in response.headers.get('Content-Type', '') and response.text:\n        data = json.loads(response.text)\n        if data:\n            data_0 = data[0]\n            if 'errorCode' in data_0:\n                subreq = ''\n                if 'referenceId' in data_0:\n                    subreq = \" (in subrequest {!r})\".format(data_0['referenceId'])\n                messages = [data_0['errorCode'] + subreq] + messages\n            if data_0.get('fields'):\n                messages.append('FIELDS: {}'.format(data_0['fields']))\n            if len(data) > 1:\n                messages.append('MORE_ERRORS ({})'.format(len(data)))\n    if 'method+url' in verbs:\n        method = response.request.method\n        url = response.request.url\n        if len(url) > 100:\n            url = url[:100] + '...'\n        data_info = ''\n        if (method in ('POST', 'PATCH') and\n                (not response.request.body or 'json' not in response.request.headers['content-type'])):\n            data_info = ' (without json request data)'\n        messages.append('in {} \"{}\"{}'.format(method, url, data_info))\n    separ = '\\n    '\n    if not PY3:\n        messages = [x if isinstance(x, str) else x.encode('utf-8') for x in messages]\n    messages = [x.replace('\\n', separ) for x in messages]\n    message = separ.join(messages)\n    if obj:\n        obj.data = data\n        obj.response = response\n        obj.verbs = verbs\n    return message"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef warn_sf(messages, response, verbs=None, klass=SalesforceWarning):\n    warnings.warn(klass(messages, response, verbs), stacklevel=2)", "response": "Issue a warning SalesforceWarning with message combined from message and data from SFDC response"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the FROM clause converted the SOQL dialect.", "response": "def get_from_clause(self):\n        \"\"\"\n        Return the FROM clause, converted the SOQL dialect.\n\n        It should be only the name of base object, even in parent-to-child and\n        child-to-parent relationships queries.\n        \"\"\"\n        self.query_topology()\n        root_table = self.soql_trans[self.root_alias]\n        return [root_table], []"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef quote_name_unless_alias(self, name):\n        r = self.connection.ops.quote_name(name)\n        self.quote_cache[name] = r\n        return r", "response": "Quote a table name unless it is an alias."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef execute_sql(self, result_type=constants.MULTI, chunked_fetch=False,\n                    chunk_size=constants.GET_ITERATOR_CHUNK_SIZE):\n        \"\"\"\n        Run the query against the database and returns the result(s). The\n        return value is a single data item if result_type is SINGLE, or an\n        iterator over the results if the result_type is MULTI.\n\n        result_type is either MULTI (use fetchmany() to retrieve all rows),\n        SINGLE (only retrieve a single row), or None. In this last case, the\n        cursor is returned if any query is executed, since it's used by\n        subclasses such as InsertQuery). It's possible, however, that no query\n        is needed, as the filters describe an empty set. In that case, None is\n        returned, to avoid any unnecessary database interaction.\n        \"\"\"\n        try:\n            sql, params = self.as_sql()\n            if not sql:\n                raise EmptyResultSet\n        except EmptyResultSet:\n            if result_type == constants.MULTI:\n                return iter([])\n            else:\n                return\n\n        cursor = self.connection.cursor()\n        cursor.prepare_query(self.query)\n        cursor.execute(sql, params)\n\n        if not result_type or result_type == 'cursor':\n            return cursor\n\n        if result_type == constants.SINGLE:\n            return cursor.fetchone()\n\n        # The MULTI case.\n        result = iter(lambda: cursor.fetchmany(chunk_size),\n                      self.connection.features.empty_fetchmany_value)\n        if not chunked_fetch and not self.connection.features.can_use_chunked_reads:\n            # If we are using non-chunked reads, we return the same data\n            # structure as normally, but ensure it is all read into memory\n            # before going any further. Use chunked_fetch if requested.\n            return list(result)\n        return result", "response": "Executes the SQL query against the database and returns the result."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the SQL string for this query.", "response": "def as_sql(self, with_limits=True, with_col_aliases=False, subquery=False):  # pylint:disable=arguments-differ\n        # the argument `subquery` is only for old Django 1.10\n        # pylint:disable=too-many-locals,too-many-branches,too-many-statements\n        \"\"\"\n        Creates the SQL for this query. Returns the SQL string and list of\n        parameters.\n\n        If 'with_limits' is False, any limit/offset information is not included\n        in the query.\n        \"\"\"\n        # After executing the query, we must get rid of any joins the query\n        # setup created. So, take note of alias counts before the query ran.\n        # However we do not want to get rid of stuff done in pre_sql_setup(),\n        # as the pre_sql_setup will modify query state in a way that forbids\n        # another run of it.\n        if with_limits and self.query.low_mark == self.query.high_mark:\n            return '', ()\n        self.subquery = subquery\n        refcounts_before = self.query.alias_refcount.copy()\n        soql_trans = self.query_topology()\n        try:\n            extra_select, order_by, group_by = self.pre_sql_setup()\n            if with_limits and self.query.low_mark == self.query.high_mark:\n                return '', ()\n            distinct_fields = self.get_distinct()\n\n            # This must come after 'select', 'ordering', and 'distinct' -- see\n            # docstring of get_from_clause() for details.\n            from_, f_params = self.get_from_clause()\n\n            where, w_params = self.compile(self.where) if self.where is not None else (\"\", [])\n            having, h_params = self.compile(self.having) if self.having is not None else (\"\", [])\n            params = []\n            result = ['SELECT']\n\n            if self.query.distinct:\n                result.append(self.connection.ops.distinct_sql(distinct_fields))\n\n            out_cols = []\n            col_idx = 1\n            for _, (s_sql, s_params), alias in self.select + extra_select:\n                if alias:\n                    # fixed by removing 'AS'\n                    s_sql = '%s %s' % (s_sql, self.connection.ops.quote_name(alias))\n                elif with_col_aliases and not isinstance(with_col_aliases, salesforce.backend.base.DatabaseWrapper):\n                    s_sql = '%s AS %s' % (s_sql, 'Col%d' % col_idx)\n                    col_idx += 1\n                if soql_trans and re.match(r'^\\w+\\.\\w+$', s_sql):\n                    tab_name, col_name = s_sql.split('.')\n                    s_sql = '%s.%s' % (soql_trans[tab_name], col_name)\n                params.extend(s_params)\n                out_cols.append(s_sql)\n\n            result.append(', '.join(out_cols))\n\n            result.append('FROM')\n            result.extend(from_)\n            params.extend(f_params)\n\n            if where:\n                result.append('WHERE %s' % where)\n                params.extend(w_params)\n\n            grouping = []\n            for g_sql, g_params in group_by:\n                grouping.append(g_sql)\n                params.extend(g_params)\n            if grouping:\n                if distinct_fields:\n                    raise NotSupportedError(\n                        \"annotate() + distinct(fields) is not implemented.\")\n                if not order_by:\n                    order_by = self.connection.ops.force_no_ordering()\n                result.append('GROUP BY %s' % ', '.join(grouping))\n\n            if having:\n                result.append('HAVING %s' % having)\n                params.extend(h_params)\n\n            if order_by:\n                ordering = []\n                for _, (o_sql, o_params, _) in order_by:\n                    ordering.append(o_sql)\n                    params.extend(o_params)\n                result.append('ORDER BY %s' % ', '.join(ordering))\n\n            if with_limits:\n                if self.query.high_mark is not None:\n                    result.append('LIMIT %d' % (self.query.high_mark - self.query.low_mark))\n                if self.query.low_mark:\n                    if self.query.high_mark is None:\n                        val = self.connection.ops.no_limit_value()\n                        if val:\n                            result.append('LIMIT %d' % val)\n                    result.append('OFFSET %d' % self.query.low_mark)\n\n            if self.query.select_for_update and self.connection.features.has_select_for_update:\n                if self.connection.get_autocommit():\n                    raise TransactionManagementError(\n                        \"select_for_update cannot be used outside of a transaction.\"\n                    )\n\n                # If we've been asked for a NOWAIT query but the backend does\n                # not support it, raise a DatabaseError otherwise we could get\n                # an unexpected deadlock.\n                nowait = self.query.select_for_update_nowait\n                if nowait and not self.connection.features.has_select_for_update_nowait:\n                    raise DatabaseError('NOWAIT is not supported on this database backend.')\n                result.append(self.connection.ops.for_update_sql(nowait=nowait))\n\n            return ' '.join(result), tuple(params)\n        finally:\n            # Finally do cleanup - get rid of the joins we created above.\n            self.query.reset_refcounts(refcounts_before)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef as_salesforce(self, compiler, connection):\n\n        # *** patch 1 (add) begin\n        # # prepare SOQL translations\n        if not isinstance(compiler, SQLCompiler):\n            # future fix for DJANGO_20_PLUS, when deprecated \"use_for_related_fields\"\n            # removed from managers,\n            # \"str(<UpdateQuery...>)\" or \"<UpdateQuery...>.get_compiler('default').as_sql()\"\n            return super(SalesforceWhereNode, self).as_sql(compiler, connection)\n        soql_trans = compiler.query_topology()\n        # *** patch 1 end\n\n        result = []\n        result_params = []\n        if self.connector == AND:\n            full_needed, empty_needed = len(self.children), 1\n        else:\n            full_needed, empty_needed = 1, len(self.children)\n\n        for child in self.children:\n            try:\n                sql, params = compiler.compile(child)\n            except EmptyResultSet:\n                empty_needed -= 1\n            else:\n                if sql:\n\n                    # *** patch 2 (add) begin\n                    # # translate the alias of child to SOQL name\n                    x_match = re.match(r'(\\w+)\\.(.*)', sql)\n                    if x_match:\n                        x_table, x_field = x_match.groups()\n                        sql = '%s.%s' % (soql_trans[x_table], x_field)\n                        # print('sql params:', sql, params)\n                    # *** patch 2 end\n\n                    result.append(sql)\n                    result_params.extend(params)\n                else:\n                    full_needed -= 1\n            # Check if this node matches nothing or everything.\n            # First check the amount of full nodes and empty nodes\n            # to make this node empty/full.\n            # Now, check if this node is full/empty using the\n            # counts.\n            if empty_needed == 0:\n                if self.negated:\n                    return '', []\n                else:\n                    raise EmptyResultSet\n            if full_needed == 0:\n                if self.negated:\n                    raise EmptyResultSet\n                else:\n                    return '', []\n        conn = ' %s ' % self.connector\n        sql_string = conn.join(result)\n        if sql_string:\n            if self.negated:\n                # *** patch 3 (remove) begin\n                # # Some backends (Oracle at least) need parentheses\n                # # around the inner SQL in the negated case, even if the\n                # # inner SQL contains just a single expression.\n                # sql_string = 'NOT (%s)' % sql_string\n                # *** patch 3 (add)\n                # SOQL requires parentheses around \"NOT\" expression, if combined with AND/OR\n                sql_string = '(NOT (%s))' % sql_string\n                # *** patch 3 end\n\n            # *** patch 4 (combine two versions into one compatible) begin\n            # elif len(result) > 1:                                    # Django 1.10, 1.11\n            # elif len(result) > 1 or self.resolved:                   # Django 2.0, 2.1\n            elif len(result) > 1 or getattr(self, 'resolved', False):  # compatible code\n                # *** patch 4 end\n\n                sql_string = '(%s)' % sql_string\n        return sql_string, result_params", "response": "Return the SQL version of the where clause and the value to be used in the where clause."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_soap_client(db_alias, client_class=None):\n    if not beatbox:\n        raise InterfaceError(\"To use SOAP API, you'll need to install the Beatbox package.\")\n    if client_class is None:\n        client_class = beatbox.PythonClient\n    soap_client = client_class()\n\n    # authenticate\n    connection = connections[db_alias]\n    # verify the authenticated connection, because Beatbox can not refresh the token\n    cursor = connection.cursor()\n    cursor.urls_request()\n    auth_info = connections[db_alias].sf_session.auth\n\n    access_token = auth_info.get_auth()['access_token']\n    assert access_token[15] == '!'\n    org_id = access_token[:15]\n    url = '/services/Soap/u/{version}/{org_id}'.format(version=salesforce.API_VERSION,\n                                                       org_id=org_id)\n    soap_client.useSession(access_token, auth_info.instance_url + url)\n    return soap_client", "response": "Create a SOAP client for the current user logged in the db_alias"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef convert_lead(lead, converted_status=None, **kwargs):\n    # pylint:disable=protected-access\n    if not beatbox:\n        raise InterfaceError(\"To use convert_lead, you'll need to install the Beatbox library.\")\n\n    accepted_kw = set(('accountId', 'contactId', 'doNotCreateOpportunity',\n                       'opportunityName', 'overwriteLeadSource', 'ownerId',\n                       'sendNotificationEmail'))\n    assert all(x in accepted_kw for x in kwargs)\n\n    db_alias = lead._state.db\n    if converted_status is None:\n        converted_status = connections[db_alias].introspection.converted_lead_status\n    soap_client = get_soap_client(db_alias)\n\n    # convert\n    kwargs['leadId'] = lead.pk\n    kwargs['convertedStatus'] = converted_status\n    response = soap_client.convertLead(kwargs)\n\n    ret = dict((x._name[1], str(x)) for x in response)\n\n    if \"errors\" in str(ret):\n        raise DatabaseError(\"The Lead conversion failed: {0}, leadId={1}\"\n                            .format(ret['errors'], ret['leadId']))\n    return ret", "response": "Convert a Lead object to a new object in the Seasonal Season."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the highest API version supported by all databases in db_aliases", "response": "def set_highest_api_version(db_aliases):\n    \"\"\"Set the highest version of Force.com API supported by all databases in db_aliases\n    \"\"\"\n    if not isinstance(db_aliases, (list, tuple)):\n        db_aliases = [db_aliases]\n    max_version = max(connections[db_alias].cursor().versions_request()[-1]['version']\n                      for db_alias in db_aliases)\n    setattr(salesforce, 'API_VERSION', max_version)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsignal that DB - API 2. 0 extensions are reported by warnings at run - time.", "response": "def signalize_extensions():\n    \"\"\"DB API 2.0 extension are reported by warnings at run-time.\"\"\"\n    warnings.warn(\"DB-API extension cursor.rownumber used\", SalesforceWarning)\n    warnings.warn(\"DB-API extension connection.<exception> used\", SalesforceWarning)  # TODO\n    warnings.warn(\"DB-API extension cursor.connection used\", SalesforceWarning)\n    # not implemented DB-API extension cursor.scroll(, SalesforceWarning)\n    warnings.warn(\"DB-API extension cursor.messages used\", SalesforceWarning)\n    warnings.warn(\"DB-API extension connection.messages used\", SalesforceWarning)\n    warnings.warn(\"DB-API extension cursor.next(, SalesforceWarning) used\")\n    warnings.warn(\"DB-API extension cursor.__iter__(, SalesforceWarning) used\")\n    warnings.warn(\"DB-API extension cursor.lastrowid used\", SalesforceWarning)\n    warnings.warn(\"DB-API extension .errorhandler used\", SalesforceWarning)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\npatching getaddrinfo with default family IPv4", "response": "def getaddrinfo_wrapper(host, port, family=socket.AF_INET, socktype=0, proto=0, flags=0):\n    \"\"\"Patched 'getaddrinfo' with default family IPv4 (enabled by settings IPV4_ONLY=True)\"\"\"\n    return orig_getaddrinfo(host, port, family, socktype, proto, flags)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nquoting a string literal in a sequence of single quotes.", "response": "def quoted_string_literal(txt):\n    \"\"\"\n    SOQL requires single quotes to be escaped.\n    http://www.salesforce.com/us/developer/docs/soql_sosl/Content/sforce_api_calls_soql_select_quotedstringescapes.htm\n    \"\"\"\n    try:\n        return \"'%s'\" % (txt.replace(\"\\\\\", \"\\\\\\\\\").replace(\"'\", \"\\\\'\"),)\n    except TypeError:\n        raise NotSupportedError(\"Cannot quote %r objects: %r\" % (type(txt), txt))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting an argument to a SOQL string.", "response": "def arg_to_soql(arg):\n    \"\"\"\n    Perform necessary SOQL quoting on the arg.\n    \"\"\"\n    conversion = sql_conversions.get(type(arg))\n    if conversion:\n        return conversion(arg)\n    for type_ in subclass_conversions:\n        if isinstance(arg, type_):\n            return sql_conversions[type_](arg)\n    return sql_conversions[str](arg)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts an argument to a JSON object.", "response": "def arg_to_json(arg):\n    \"\"\"\n    Perform necessary JSON conversion on the arg.\n    \"\"\"\n    conversion = json_conversions.get(type(arg))\n    if conversion:\n        return conversion(arg)\n    for type_ in subclass_conversions:\n        if isinstance(arg, type_):\n            return json_conversions[type_](arg)\n    return json_conversions[str](arg)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmerging two or more dict including kw into result dict.", "response": "def merge_dict(dict_1, *other, **kw):\n    \"\"\"Merge two or more dict including kw into result dict.\"\"\"\n    tmp = dict_1.copy()\n    for x in other:\n        tmp.update(x)\n    tmp.update(kw)\n    return tmp"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nauthenticate and get the name of assigned SFDC data server", "response": "def make_session(self):\n        \"\"\"Authenticate and get the name of assigned SFDC data server\"\"\"\n        with connect_lock:\n            if self._sf_session is None:\n                sf_session = requests.Session()\n                # TODO configurable class Salesforce***Auth\n                sf_session.auth = SalesforcePasswordAuth(db_alias=self.alias,\n                                                         settings_dict=self.settings_dict)\n                sf_instance_url = sf_session.auth.instance_url\n                sf_requests_adapter = HTTPAdapter(max_retries=get_max_retries())\n                sf_session.mount(sf_instance_url, sf_requests_adapter)\n                # Additional headers work, but the same are added automatically by \"requests' package.\n                # sf_session.header = {'accept-encoding': 'gzip, deflate', 'connection': 'keep-alive'} # TODO\n                self._sf_session = sf_session"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\njoins the URL of REST_API TracingEnabled", "response": "def rest_api_url(self, *url_parts, **kwargs):\n        \"\"\"Join the URL of REST_API\n\n        parameters:\n            upl_parts:  strings that are joined to the url by \"/\".\n                a REST url like https://na1.salesforce.com/services/data/v44.0/\n                is usually added, but not if the first string starts with https://\n            api_ver:  API version that should be used instead of connection.api_ver\n                default. A special api_ver=\"\" can be used to omit api version\n                (for request to ask for available api versions)\n            relative: If `relative` is true then the url is without domain\n        Examples: self.rest_api_url(\"query?q=select+id+from+Organization\")\n                  self.rest_api_url(\"sobject\", \"Contact\", id, api_ver=\"45.0\")\n                  self.rest_api_url(api_ver=\"\")   # versions request\n                  self.rest_api_url(\"sobject\", relative=True)\n                  self.rest_api_url(\"/services/data/v45.0\")\n        Output:\n\n                  https://na1.salesforce.com/services/data/v44.0/query?q=select+id+from+Organization\n                  https://na1.salesforce.com/services/data/v45.0/sobject/Contact/003DD00000000XYAAA\n                  https://na1.salesforce.com/services/data\n                  /services/data/v45.0\n                  https://na1.salesforce.com/services/data/44.0\n        \"\"\"\n        url_parts = list(url_parts)\n        if url_parts and re.match(r'^(?:https|mock)://', url_parts[0]):\n            return '/'.join(url_parts)\n        relative = kwargs.pop('relative', False)\n        api_ver = kwargs.pop('api_ver', None)\n        api_ver = api_ver if api_ver is not None else self.api_ver\n        assert not kwargs\n        if not relative:\n            base = [self.sf_session.auth.instance_url]\n        else:\n            base = ['']\n        if url_parts and url_parts[0].startswith('/'):\n            prefix = []\n            url_parts[0] = url_parts[0][1:]\n        else:\n            prefix = ['services/data']\n            if api_ver:\n                prefix += ['v{api_ver}'.format(api_ver=api_ver)]\n        return '/'.join(base + prefix + url_parts)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef handle_api_exceptions(self, method, *url_parts, **kwargs):\n        # The outer part - about error handler\n        assert method in ('HEAD', 'GET', 'POST', 'PATCH', 'DELETE')\n        cursor_context = kwargs.pop('cursor_context', None)\n        errorhandler = cursor_context.errorhandler if cursor_context else self.errorhandler\n        catched_exceptions = (SalesforceError, requests.exceptions.RequestException) if errorhandler else ()\n        try:\n            return self.handle_api_exceptions_inter(method, *url_parts, **kwargs)\n\n        except catched_exceptions:\n            # nothing is catched usually and error handler not used\n            exc_class, exc_value, _ = sys.exc_info()\n            errorhandler(self, cursor_context, exc_class, exc_value)\n            raise", "response": "Call REST API and handle exceptions."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef raise_errors(self, response):\n        # Errors: 400, 403 permissions or REQUEST_LIMIT_EXCEEDED, 404, 405, 415, 500)\n        # TODO extract a case ID for Salesforce support from code 500 messages\n\n        # TODO disabled 'debug_verbs' temporarily, after writing better default messages\n        verb = self.debug_verbs  # NOQA pylint:disable=unused-variable\n        method = response.request.method\n        data = None\n        is_json = 'json' in response.headers.get('Content-Type', '') and response.text\n        if is_json:\n            data = json.loads(response.text)\n        if not (isinstance(data, list) and data and 'errorCode' in data[0]):\n            messages = [response.text] if is_json else []\n            raise OperationalError(\n                ['HTTP error \"%d %s\":' % (response.status_code, response.reason)]\n                + messages, response, ['method+url'])\n\n        # Other Errors are reported in the json body\n        err_msg = data[0]['message']\n        err_code = data[0]['errorCode']\n        if response.status_code == 404:  # ResourceNotFound\n            if method == 'DELETE' and err_code in ('ENTITY_IS_DELETED', 'INVALID_CROSS_REFERENCE_KEY'):\n                # It was a delete command and the object is in trash bin or it is\n                # completely deleted or it could be a valid Id for this sobject type.\n                # Then we accept it with a warning, similarly to delete by a classic database query:\n                # DELETE FROM xy WHERE id = 'something_deleted_yet'\n                warn_sf([err_msg, \"Object is deleted before delete or update\"], response, ['method+url'])\n                # TODO add a warning and add it to messages\n                return None\n        if err_code in ('NOT_FOUND',           # 404 e.g. invalid object type in url path or url query?q=select ...\n                        'METHOD_NOT_ALLOWED',  # 405 e.g. patch instead of post\n                        ):                     # both need to report the url\n            raise SalesforceError([err_msg], response, ['method+url'])\n        # it is good e.g for these errorCode: ('INVALID_FIELD', 'MALFORMED_QUERY', 'INVALID_FIELD_FOR_INSERT_UPDATE')\n        raise SalesforceError([err_msg], response)", "response": "Raise an exception if any of the errors are found in the resource table."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalling a composite request with subrequests error handling", "response": "def composite_request(self, data):\n        \"\"\"Call a 'composite' request with subrequests, error handling\n\n        A fake object for request/response is created for a subrequest in case\n        of error, to be possible to use the same error hanler with a clear\n        message as with an individual request.\n        \"\"\"\n        post_data = {'compositeRequest': data, 'allOrNone': True}\n        resp = self.handle_api_exceptions('POST', 'composite', json=post_data)\n        comp_resp = resp.json()['compositeResponse']\n        is_ok = all(x['httpStatusCode'] < 400 for x in comp_resp)\n        if is_ok:\n            return resp\n\n        # construct an equivalent of individual bad request/response\n        bad_responses = {\n            i: x for i, x in enumerate(comp_resp)\n            if not (x['httpStatusCode'] == 400\n                    and x['body'][0]['errorCode'] in ('PROCESSING_HALTED', 'ALL_OR_NONE_OPERATION_ROLLED_BACK'))\n        }\n        if len(bad_responses) != 1:\n            raise InternalError(\"Too much or too many subrequests with an individual error\")\n        bad_i, bad_response = bad_responses.popitem()\n        bad_request = data[bad_i]\n\n        bad_req = FakeReq(bad_request['method'], bad_request['url'], bad_request.get('body'),\n                          bad_request.get('httpHeaders', {}), context={bad_i: bad_request['referenceId']})\n\n        body = [merge_dict(x, referenceId=bad_response['referenceId'])\n                for x in bad_response['body']]\n        bad_resp_headers = bad_response['httpHeaders'].copy()\n        bad_resp_headers.update({'Content-Type': resp.headers['Content-Type']})\n\n        bad_resp = FakeResp(bad_response['httpStatusCode'], json.dumps(body), bad_req, bad_resp_headers)\n\n        self.raise_errors(bad_resp)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef align_after(self, offset):\n        f = self.reader\n        if offset <= 0:  # FIXME what is a negative offset??\n            f.seek(0)\n            self._block_count = 0\n            self._read_header()  # FIXME we can't extimate how big it is...\n            return\n        sm = self.sync_marker\n        sml = len(sm)\n        pos = offset\n        while pos < self.file_length - sml:\n            f.seek(pos)\n            data = f.read(self.FORWARD_WINDOW_SIZE)\n            sync_offset = data.find(sm)\n            if sync_offset > -1:\n                f.seek(pos + sync_offset)\n                self._block_count = 0\n                return\n            pos += len(data)", "response": "Aligns the sync point after the given offset and aligns just after that."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_progress(self):\n        pos = self.reader.reader.tell()\n        return min((pos - self.region_start) /\n                   float(self.region_end - self.region_start),\n                   1.0)", "response": "Get the current progress of the current region."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks if the given file is an executable file.", "response": "def is_exe(fpath):\n    \"\"\"\n    Path references an executable file.\n    \"\"\"\n    return os.path.isfile(fpath) and os.access(fpath, os.X_OK)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck if a file is readable.", "response": "def is_readable(fpath):\n    \"\"\"\n    Path references a readable file.\n    \"\"\"\n    return os.path.isfile(fpath) and os.access(fpath, os.R_OK)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\njoins path name components into a single HDFS path.", "response": "def join(*parts):\n    \"\"\"\n    Join path name components, inserting ``/`` as needed.\n\n    If any component is an absolute path (see :func:`isabs`), all\n    previous components will be discarded.  However, full URIs (see\n    :func:`isfull`) take precedence over incomplete ones:\n\n    .. code-block:: python\n\n      >>> import pydoop.hdfs.path as hpath\n      >>> hpath.join('bar', '/foo')\n      '/foo'\n      >>> hpath.join('hdfs://host:1/', '/foo')\n      'hdfs://host:1/foo'\n\n    Note that this is *not* the reverse of :func:`split`, but rather a\n    specialized version of :func:`os.path.join`. No check is made to determine\n    whether the returned string is a valid HDFS path.\n    \"\"\"\n    try:\n        path = [parts[0]]\n    except IndexError:\n        raise TypeError(\"need at least one argument\")\n    for p in parts[1:]:\n        path[-1] = path[-1].rstrip(\"/\")\n        full = isfull(path[0])\n        if isfull(p) or (isabs(p) and not full):\n            path = [p]\n        else:\n            path.append(p.lstrip('/'))\n    return \"/\".join(path)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef abspath(hdfs_path, user=None, local=False):\n    if local:\n        return 'file:%s' % os.path.abspath(hdfs_path)\n    if isfull(hdfs_path):\n        return hdfs_path\n    hostname, port, path = split(hdfs_path, user=user)\n    if hostname:\n        fs = hdfs_fs.hdfs(hostname, port)\n        apath = join(\"hdfs://%s:%s\" % (fs.host, fs.port), path)\n        fs.close()\n    else:\n        apath = \"file:%s\" % os.path.abspath(path)\n    return apath", "response": "Return an absolute path for hdfs_path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dirname(hdfs_path):\n    scheme, netloc, path = parse(hdfs_path)\n    return unparse(scheme, netloc, os.path.dirname(path))", "response": "Return the directory component of HDFS_PATH."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef exists(hdfs_path, user=None):\n    hostname, port, path = split(hdfs_path, user=user)\n    fs = hdfs_fs.hdfs(hostname, port)\n    retval = fs.exists(path)\n    fs.close()\n    return retval", "response": "Return True if HDFS path exists in the default HDFS."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the kind of item that the path references.", "response": "def kind(path, user=None):\n    \"\"\"\n    Get the kind of item (\"file\" or \"directory\") that the path references.\n\n    Return :obj:`None` if ``path`` doesn't exist.\n    \"\"\"\n    hostname, port, path = split(path, user=user)\n    fs = hdfs_fs.hdfs(hostname, port)\n    try:\n        return fs.get_path_info(path)['kind']\n    except IOError:\n        return None\n    finally:\n        fs.close()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nexpanding ~user with the user home directory.", "response": "def expanduser(path):\n    \"\"\"\n    Replace initial ``~`` or ``~user`` with the user's home directory.\n\n    **NOTE:** if the default file system is HDFS, the ``~user`` form is\n    expanded regardless of the user's existence.\n    \"\"\"\n    if hdfs_fs.default_is_local():\n        return os.path.expanduser(path)\n    m = re.match(r'^~([^/]*)', path)\n    if m is None:\n        return path\n    user = m.groups()[0] or common.DEFAULT_USER\n    return '/user/%s%s' % (user, path[m.end(1):])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef stat(path, user=None):\n    host, port, path_ = split(path, user)\n    fs = hdfs_fs.hdfs(host, port, user)\n    retval = StatResult(fs.get_path_info(path_))\n    if not host:\n        _update_stat(retval, path_)\n    fs.close()\n    return retval", "response": "Returns a StatResult object for the given path."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef islink(path, user=None):\n    host, _, path_ = split(path, user)\n    if host:\n        return False  # libhdfs does not support fs.FileStatus.isSymlink()\n    return os.path.islink(path_)", "response": "Return True if path is a symbolic link."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn True if path is a mount point.", "response": "def ismount(path):\n    \"\"\"\n    Return :obj:`True` if ``path`` is a mount point.\n\n    This function always returns :obj:`False` for non-local paths.\n    \"\"\"\n    host, _, path_ = split(path, None)\n    if host:\n        return False\n    return os.path.ismount(path_)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef normpath(path):\n    scheme, netloc, path_ = parse(path)\n    return unparse(scheme, netloc, os.path.normpath(path_))", "response": "Normalize ``path``, collapsing redundant separators and up-level refs."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef realpath(path):\n    scheme, netloc, path_ = parse(path)\n    if scheme == 'file' or hdfs_fs.default_is_local():\n        return unparse(scheme, netloc, os.path.realpath(path_))\n    return path", "response": "Return path with symlinks resolved."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef samefile(path1, path2, user=None):\n    def tr(p):\n        return abspath(normpath(realpath(p)), user=user)\n    return tr(path1) == tr(path2)", "response": "Return True if both path arguments refer to the same file."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns True if path is a file or a directory.", "response": "def access(path, mode, user=None):\n    \"\"\"\n    Perform the equivalent of :func:`os.access` on ``path``.\n    \"\"\"\n    scheme = parse(path)[0]\n    if scheme == 'file' or hdfs_fs.default_is_local():\n        return os.access(path, mode)\n    if user is None:\n        user = common.DEFAULT_USER\n    st = stat(path)\n    if st.st_uid == user:\n        mode <<= 6\n    else:\n        try:\n            groups = common.get_groups(user)\n        except KeyError:\n            # user isn't recognized on the system.  No group\n            # information available\n            groups = []\n        if st.st_gid in groups:\n            mode <<= 3\n    return (st.st_mode & mode) == mode"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the value of a key as an int.", "response": "def get_int(self, key, default=None):\n        \"\"\"\n        Same as :meth:`dict.get`, but the value is converted to an int.\n        \"\"\"\n        value = self.get(key, default)\n        return None if value is None else int(value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_float(self, key, default=None):\n        value = self.get(key, default)\n        return None if value is None else float(value)", "response": "Returns the float value of the key if it exists otherwise returns None."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets a boolean value from the cache.", "response": "def get_bool(self, key, default=None):\n        \"\"\"\n        Same as :meth:`dict.get`, but the value is converted to a bool.\n\n        The boolean value is considered, respectively, :obj:`True` or\n        :obj:`False` if the string is equal, ignoring case, to\n        ``'true'`` or ``'false'``.\n        \"\"\"\n        v = self.get(key, default)\n        if v != default:\n            v = v.strip().lower()\n            if v == 'true':\n                v = True\n            elif v == 'false':\n                v = False\n            elif default is None:\n                raise RuntimeError(\"invalid bool string: %s\" % v)\n            else:\n                v = default\n        return v"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef default_is_local(hadoop_conf=None, hadoop_home=None):\n    params = pydoop.hadoop_params(hadoop_conf, hadoop_home)\n    for k in 'fs.defaultFS', 'fs.default.name':\n        if not params.get(k, 'file:').startswith('file:'):\n            return False\n    return True", "response": "Returns True if Hadoop is configured to use the local file system?"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nclose the HDFS handle and remove the cache entries.", "response": "def close(self):\n        \"\"\"\n        Close the HDFS handle (disconnect).\n        \"\"\"\n        self.__status.refcount -= 1\n        if self.refcount == 0:\n            self.fs.close()\n            for k, status in list(self._CACHE.items()):  # yes, we want a copy\n                if status.refcount == 0:\n                    del self._CACHE[k]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nopening an HDFS file and return a file handle.", "response": "def open_file(self, path,\n                  mode=\"r\",\n                  buff_size=0,\n                  replication=0,\n                  blocksize=0,\n                  encoding=None,\n                  errors=None):\n        \"\"\"\n        Open an HDFS file.\n\n        Supported opening modes are \"r\", \"w\", \"a\". In addition, a\n        trailing \"t\" can be added to specify text mode (e.g., \"rt\" =\n        open for reading text).\n\n        Pass 0 as ``buff_size``, ``replication`` or ``blocksize`` if you want\n        to use the \"configured\" values, i.e., the ones set in the Hadoop\n        configuration files.\n\n        :type path: str\n        :param path: the full path to the file\n        :type mode: str\n        :param mode: opening mode\n        :type buff_size: int\n        :param buff_size: read/write buffer size in bytes\n        :type replication: int\n        :param replication: HDFS block replication\n        :type blocksize: int\n        :param blocksize: HDFS block size\n        :rtpye: :class:`~.file.hdfs_file`\n        :return: handle to the open file\n\n        \"\"\"\n        _complain_ifclosed(self.closed)\n        if not path:\n            raise ValueError(\"Empty path\")\n        m, is_text = common.parse_mode(mode)\n        if not self.host:\n            fret = local_file(self, path, m)\n            if is_text:\n                cls = io.BufferedReader if m == \"r\" else io.BufferedWriter\n                fret = TextIOWrapper(cls(fret), encoding, errors)\n            return fret\n        f = self.fs.open_file(path, m, buff_size, replication, blocksize)\n        cls = FileIO if is_text else hdfs_file\n        fret = cls(f, self, mode)\n        return fret"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef capacity(self):\n        _complain_ifclosed(self.closed)\n        if not self.__status.host:\n            raise RuntimeError('Capacity is not defined for a local fs')\n        return self.fs.get_capacity()", "response": "Return the raw capacity of the filesystem."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef copy(self, from_path, to_hdfs, to_path):\n        _complain_ifclosed(self.closed)\n        if isinstance(to_hdfs, self.__class__):\n            to_hdfs = to_hdfs.fs\n        return self.fs.copy(from_path, to_hdfs, to_path)", "response": "Copy file from one filesystem to another."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_directory(self, path):\n        _complain_ifclosed(self.closed)\n        return self.fs.create_directory(path)", "response": "Create a directory at the given path."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndeleting the file or directory at the given path.", "response": "def delete(self, path, recursive=True):\n        \"\"\"\n        Delete ``path``.\n\n        :type path: str\n        :param path: the path of the file or directory\n        :type recursive: bool\n        :param recursive: if ``path`` is a directory, delete it recursively\n          when :obj:`True`\n        :raises: :exc:`~exceptions.IOError` when ``recursive`` is\n          :obj:`False` and directory is non-empty\n        \"\"\"\n        _complain_ifclosed(self.closed)\n        return self.fs.delete(path, recursive)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef exists(self, path):\n        _complain_ifclosed(self.closed)\n        return self.fs.exists(path)", "response": "Check if a given path exists on the filesystem."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_hosts(self, path, start, length):\n        _complain_ifclosed(self.closed)\n        return self.fs.get_hosts(path, start, length)", "response": "Get the hosts that are stored in the file at the given path."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_path_info(self, path):\n        _complain_ifclosed(self.closed)\n        return self.fs.get_path_info(path)", "response": "Get information about path as a dict of properties."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef list_directory(self, path):\n        _complain_ifclosed(self.closed)\n        return self.fs.list_directory(path)", "response": "r Returns a list of files and directories in path."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrenaming file. :type from_path: str :param from_path: the path of the source file :type to_path: str :param to_path: the path of the destination file :raises: :exc:`~exceptions.IOError`", "response": "def rename(self, from_path, to_path):\n        \"\"\"\n        Rename file.\n\n        :type from_path: str\n        :param from_path: the path of the source file\n        :type to_path: str\n        :param to_path: the path of the destination file\n        :raises: :exc:`~exceptions.IOError`\n        \"\"\"\n        _complain_ifclosed(self.closed)\n        return self.fs.rename(from_path, to_path)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_replication(self, path, replication):\n        _complain_ifclosed(self.closed)\n        return self.fs.set_replication(path, replication)", "response": "r Set the replication value of the file with the given path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_working_directory(self, path):\n        _complain_ifclosed(self.closed)\n        return self.fs.set_working_directory(path)", "response": "r Sets the working directory to path."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef working_directory(self):\n        _complain_ifclosed(self.closed)\n        wd = self.fs.get_working_directory()\n        return wd", "response": "Get the current working directory."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef chown(self, path, user='', group=''):\n        _complain_ifclosed(self.closed)\n        return self.fs.chown(path, user, group)", "response": "Change file owner and group."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing the mode integer from a mode string.", "response": "def __compute_mode_from_string(self, path, mode_string):\n        \"\"\"\n        Scan a unix-style mode string and apply it to ``path``.\n\n        :type mode_string: str\n        :param mode_string: see ``man chmod`` for details. ``X``, ``s``\n          and ``t`` modes are not supported.  The string should match the\n          following regular expression: ``[ugoa]*[-+=]([rwx]*)``.\n        :rtype: int\n        :return: a new mode integer resulting from applying ``mode_string``\n          to ``path``.\n        :raises: :exc:`~exceptions.ValueError` if ``mode_string`` is invalid.\n        \"\"\"\n        Char_to_perm_byte = {'r': 4, 'w': 2, 'x': 1}\n        Fields = (('u', 6), ('g', 3), ('o', 0))\n        # --\n        m = re.match(r\"\\s*([ugoa]*)([-+=])([rwx]*)\\s*\", mode_string)\n        if not m:\n            raise ValueError(\"Invalid mode string %s\" % mode_string)\n        who = m.group(1)\n        what_op = m.group(2)\n        which_perm = m.group(3)\n        # --\n        old_mode = self.fs.get_path_info(path)['permissions']\n        # The mode to be applied by the operation, repeated three\n        # times in a list, for user, group, and other respectively.\n        # Initially these are identical, but some may change if we\n        # have to respect the umask setting.\n        op_perm = [\n            reduce(ops.ior, [Char_to_perm_byte[c] for c in which_perm])\n        ] * 3\n        if 'a' in who:\n            who = 'ugo'\n        elif who == '':\n            who = 'ugo'\n            # erase the umask bits\n            inverted_umask = ~self.__get_umask()\n            for i, field in enumerate(Fields):\n                op_perm[i] &= (inverted_umask >> field[1]) & 0x7\n        # for each user, compute the permission bit and set it in the mode\n        new_mode = 0\n        for i, tpl in enumerate(Fields):\n            field, shift = tpl\n            # shift by the bits specified for the field; keep only the\n            # 3 lowest bits\n            old = (old_mode >> shift) & 0x7\n            if field in who:\n                if what_op == '-':\n                    new = old & ~op_perm[i]\n                elif what_op == '=':\n                    new = op_perm[i]\n                elif what_op == '+':\n                    new = old | op_perm[i]\n                else:\n                    raise RuntimeError(\n                        \"unexpected permission operation %s\" % what_op\n                    )\n            else:\n                # copy the previous permissions\n                new = old\n            new_mode |= new << shift\n        return new_mode"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchanges file mode bits.", "response": "def chmod(self, path, mode):\n        \"\"\"\n        Change file mode bits.\n\n        :type path: str\n        :param path: the path to the file or directory\n        :type mode: int\n        :param mode: the bitmask to set it to (e.g., 0777)\n        :raises: :exc:`~exceptions.IOError`\n        \"\"\"\n        _complain_ifclosed(self.closed)\n        try:\n            return self.fs.chmod(path, mode)\n        except TypeError:\n            mode = self.__compute_mode_from_string(path, mode)\n            return self.fs.chmod(path, mode)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchanging file last access and modification times.", "response": "def utime(self, path, mtime, atime):\n        \"\"\"\n        Change file last access and modification times.\n\n        :type path: str\n        :param path: the path to the file or directory\n        :type mtime: int\n        :param mtime: new modification time in seconds\n        :type atime: int\n        :param atime: new access time in seconds\n        :raises: :exc:`~exceptions.IOError`\n        \"\"\"\n        _complain_ifclosed(self.closed)\n        return self.fs.utime(path, int(mtime), int(atime))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef walk(self, top):\n        if not top:\n            raise ValueError(\"Empty path\")\n        if isinstance(top, basestring):\n            top = self.get_path_info(top)\n        yield top\n        if top['kind'] == 'directory':\n            for info in self.list_directory(top['name']):\n                for item in self.walk(info):\n                    yield item", "response": "Generator function that walks the tree and returns all files and directories in the tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nremoves a file or directory tree.", "response": "def rm_rf(path, dry_run=False):\n    \"\"\"\n    Remove a file or directory tree.\n\n    Won't throw an exception, even if the removal fails.\n    \"\"\"\n    log.info(\"removing %s\" % path)\n    if dry_run:\n        return\n    try:\n        if os.path.isdir(path) and not os.path.islink(path):\n            shutil.rmtree(path)\n        else:\n            os.remove(path)\n    except OSError:\n        pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfinalizes the extended compiler so that the external compiler can build the resources.", "response": "def __finalize_hdfs(self, ext):\n        \"\"\"\\\n        Adds a few bits that depend on the specific environment.\n\n        Delaying this until the build_ext phase allows non-build commands\n        (e.g., sdist) to be run without java.\n        \"\"\"\n        java_home = jvm.get_java_home()\n        jvm_lib_path, _ = jvm.get_jvm_lib_path_and_name(java_home)\n        ext.include_dirs = jvm.get_include_dirs() + ext.include_dirs\n        ext.libraries = jvm.get_libraries()\n        ext.library_dirs = [os.path.join(java_home, \"Libraries\"), jvm_lib_path]\n        ext.define_macros = jvm.get_macros()\n        ext.extra_link_args = ['-Wl,-rpath,%s' % jvm_lib_path]\n        if self.__have_better_tls():\n            ext.define_macros.append((\"HAVE_BETTER_TLS\", None))\n        try:\n            # too many warnings in libhdfs\n            self.compiler.compiler_so.remove(\"-Wsign-compare\")\n        except (AttributeError, ValueError):\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run_tool_cmd(tool, cmd, args=None, properties=None, hadoop_conf_dir=None,\n                 logger=None, keep_streams=True):\n    \"\"\"\n    Run a Hadoop command.\n\n    If ``keep_streams`` is set to :obj:`True` (the default), the\n    stdout and stderr of the command will be buffered in memory.  If\n    the command succeeds, the former will be returned; if it fails, a\n    ``RunCmdError`` will be raised with the latter as the message.\n    This mode is appropriate for short-running commands whose \"result\"\n    is represented by their standard output (e.g., ``\"dfsadmin\",\n    [\"-safemode\", \"get\"]``).\n\n    If ``keep_streams`` is set to :obj:`False`, the command will write\n    directly to the stdout and stderr of the calling process, and the\n    return value will be empty.  This mode is appropriate for long\n    running commands that do not write their \"real\" output to stdout\n    (such as pipes).\n\n    .. code-block:: python\n\n      >>> hadoop_classpath = run_cmd('classpath')\n    \"\"\"\n    if logger is None:\n        logger = utils.NullLogger()\n    _args = [tool]\n    if hadoop_conf_dir:\n        _args.extend([\"--config\", hadoop_conf_dir])\n    _args.append(cmd)\n    if properties:\n        _args.extend(_construct_property_args(properties))\n    if args:\n        if isinstance(args, basestring):\n            args = shlex.split(args)\n        _merge_csv_args(args)\n        gargs = _pop_generic_args(args)\n        for seq in gargs, args:\n            _args.extend(map(str, seq))\n    logger.debug('final args: %r', (_args,))\n    if keep_streams:\n        p = subprocess.Popen(\n            _args, stdout=subprocess.PIPE, stderr=subprocess.PIPE\n        )\n        error = \"\"\n        stderr_iterator = iter(p.stderr.readline, b\"\")\n        for line in stderr_iterator:\n            error += line\n            logger.info(\"cmd stderr line: %s\", line.strip())\n\n        output, _ = p.communicate()\n    else:\n        p = subprocess.Popen(_args, stdout=None, stderr=None, bufsize=1)\n        ret = p.wait()\n        error = 'command exited with %d status' % ret if ret else ''\n        output = ''\n    if p.returncode:\n        raise RunCmdError(p.returncode, ' '.join(_args), error)\n    return output", "response": "Runs a Hadoop command."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_task_trackers(properties=None, hadoop_conf_dir=None, offline=False):\n    if offline:\n        if not hadoop_conf_dir:\n            hadoop_conf_dir = pydoop.hadoop_conf()\n            slaves = os.path.join(hadoop_conf_dir, \"slaves\")\n        try:\n            with open(slaves) as f:\n                task_trackers = [(l.strip(), 0) for l in f]\n        except IOError:\n            task_trackers = []\n    else:\n        # run JobClient directly (avoids \"hadoop job\" deprecation)\n        stdout = run_class(\n            \"org.apache.hadoop.mapred.JobClient\", [\"-list-active-trackers\"],\n            properties=properties, hadoop_conf_dir=hadoop_conf_dir,\n            keep_streams=True\n        )\n        task_trackers = []\n        for line in stdout.splitlines():\n            if not line:\n                continue\n            line = line.split(\":\")\n            task_trackers.append((line[0].split(\"_\")[1], int(line[-1])))\n    return task_trackers", "response": "Get the list of task trackers in the Hadoop cluster."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_num_nodes(properties=None, hadoop_conf_dir=None, offline=False):\n    return len(get_task_trackers(properties, hadoop_conf_dir, offline))", "response": "Get the number of nodes in the Hadoop cluster."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef dfs(args=None, properties=None, hadoop_conf_dir=None):\n    # run FsShell directly (avoids \"hadoop dfs\" deprecation)\n    return run_class(\n        \"org.apache.hadoop.fs.FsShell\", args, properties,\n        hadoop_conf_dir=hadoop_conf_dir, keep_streams=True\n    )", "response": "Run the Hadoop file system shell."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if path exists in default HDFS.", "response": "def path_exists(path, properties=None, hadoop_conf_dir=None):\n    \"\"\"\n    Return :obj:`True` if ``path`` exists in the default HDFS.\n\n    Keyword arguments are passed to :func:`dfs`.\n\n    This function does the same thing as :func:`hdfs.path.exists\n    <pydoop.hdfs.path.exists>`, but it uses a wrapper for the Hadoop\n    shell rather than the hdfs extension.\n    \"\"\"\n    try:\n        dfs([\"-stat\", path], properties, hadoop_conf_dir=hadoop_conf_dir)\n    except RuntimeError:\n        return False\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nruns a jar on Hadoop.", "response": "def run_jar(jar_name, more_args=None, properties=None, hadoop_conf_dir=None,\n            keep_streams=True):\n    \"\"\"\n    Run a jar on Hadoop (``hadoop jar`` command).\n\n    All arguments are passed to :func:`run_cmd` (``args = [jar_name] +\n    more_args``) .\n    \"\"\"\n    if hu.is_readable(jar_name):\n        args = [jar_name]\n        if more_args is not None:\n            args.extend(more_args)\n        return run_cmd(\n            \"jar\", args, properties, hadoop_conf_dir=hadoop_conf_dir,\n            keep_streams=keep_streams\n        )\n    else:\n        raise ValueError(\"Can't read jar file %s\" % jar_name)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run_class(class_name, args=None, properties=None, classpath=None,\n              hadoop_conf_dir=None, logger=None, keep_streams=True):\n    \"\"\"\n    Run a Java class with Hadoop (equivalent of running ``hadoop\n    <class_name>`` from the command line).\n\n    Additional ``HADOOP_CLASSPATH`` elements can be provided via\n    ``classpath`` (either as a non-string sequence where each element\n    is a classpath element or as a ``':'``-separated string).  Other\n    arguments are passed to :func:`run_cmd`.\n\n    .. code-block:: python\n\n      >>> cls = 'org.apache.hadoop.fs.FsShell'\n      >>> try: out = run_class(cls, args=['-test', '-e', 'file:/tmp'])\n      ... except RunCmdError: tmp_exists = False\n      ... else: tmp_exists = True\n\n    .. note::\n\n      ``HADOOP_CLASSPATH`` makes dependencies available **only on the\n      client side**.  If you are running a MapReduce application, use\n      ``args=['-libjars', 'jar1,jar2,...']`` to make them available to\n      the server side as well.\n    \"\"\"\n    if logger is None:\n        logger = utils.NullLogger()\n    old_classpath = None\n    if classpath:\n        old_classpath = os.getenv('HADOOP_CLASSPATH', '')\n        if isinstance(classpath, basestring):\n            classpath = [classpath]\n        # Prepend the classpaths provided by the user to the existing\n        # HADOOP_CLASSPATH value.  Order matters.  We could work a little\n        # harder to avoid duplicates, but it's not essential\n        os.environ['HADOOP_CLASSPATH'] = \":\".join(\n            classpath + old_classpath.split(':', 1)\n        )\n        logger.debug('HADOOP_CLASSPATH: %r', os.getenv('HADOOP_CLASSPATH'))\n    try:\n        res = run_cmd(class_name, args, properties,\n                      hadoop_conf_dir=hadoop_conf_dir, logger=logger,\n                      keep_streams=keep_streams)\n    finally:\n        if old_classpath is not None:\n            os.environ['HADOOP_CLASSPATH'] = old_classpath\n    return res", "response": "Run a Java class with Hadoop."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run_pipes(executable, input_path, output_path, more_args=None,\n              properties=None, force_pydoop_submitter=False,\n              hadoop_conf_dir=None, logger=None, keep_streams=False):\n    \"\"\"\n    Run a pipes command.\n\n    ``more_args`` (after setting input/output path) and ``properties``\n    are passed to :func:`run_cmd`.\n\n    If not specified otherwise, this function sets the properties\n    ``mapreduce.pipes.isjavarecordreader`` and\n    ``mapreduce.pipes.isjavarecordwriter`` to ``\"true\"``.\n\n    This function works around a bug in Hadoop pipes that affects\n    versions of Hadoop with security when the local file system is\n    used as the default FS (no HDFS); see\n    https://issues.apache.org/jira/browse/MAPREDUCE-4000.  In those\n    set-ups, the function uses Pydoop's own pipes submitter\n    application.  You can force the use of Pydoop's submitter by\n    passing the argument force_pydoop_submitter=True.\n    \"\"\"\n    if logger is None:\n        logger = utils.NullLogger()\n    if not hdfs.path.exists(executable):\n        raise IOError(\"executable %s not found\" % executable)\n    if not hdfs.path.exists(input_path) and not (set(input_path) & GLOB_CHARS):\n        raise IOError(\"input path %s not found\" % input_path)\n    if properties is None:\n        properties = {}\n    properties.setdefault('mapreduce.pipes.isjavarecordreader', 'true')\n    properties.setdefault('mapreduce.pipes.isjavarecordwriter', 'true')\n    if force_pydoop_submitter:\n        use_pydoop_submit = True\n    else:\n        use_pydoop_submit = False\n        ver = pydoop.hadoop_version_info()\n        if ver.has_security():\n            if ver.is_cdh_mrv2() and hdfs.default_is_local():\n                raise RuntimeError(\"mrv2 on local fs not supported yet\")\n            use_pydoop_submit = hdfs.default_is_local()\n    args = [\n        \"-program\", executable,\n        \"-input\", input_path,\n        \"-output\", output_path,\n    ]\n    if more_args is not None:\n        args.extend(more_args)\n    if use_pydoop_submit:\n        submitter = \"it.crs4.pydoop.pipes.Submitter\"\n        pydoop_jar = pydoop.jar_path()\n        args.extend((\"-libjars\", pydoop_jar))\n        return run_class(submitter, args, properties,\n                         classpath=pydoop_jar, logger=logger,\n                         keep_streams=keep_streams)\n    else:\n        return run_mapred_cmd(\"pipes\", args=args, properties=properties,\n                              hadoop_conf_dir=hadoop_conf_dir, logger=logger,\n                              keep_streams=keep_streams)", "response": "Run a piped command."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfinds a jar in the build directory.", "response": "def find_jar(jar_name, root_path=None):\n    \"\"\"\n    Look for the named jar in:\n\n    #. ``root_path``, if specified\n    #. working directory -- ``PWD``\n    #. ``${PWD}/build``\n    #. ``/usr/share/java``\n\n    Return the full path of the jar if found; else return :obj:`None`.\n    \"\"\"\n    jar_name = os.path.basename(jar_name)\n    root = root_path or os.getcwd()\n    paths = (root, os.path.join(root, \"build\"), \"/usr/share/java\")\n    for p in paths:\n        p = os.path.join(p, jar_name)\n        if os.path.exists(p):\n            return p\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncollects all mapreduce output in mr_out_dir and return them as a single string.", "response": "def collect_output(mr_out_dir, out_file=None):\n    \"\"\"\n    Return all mapreduce output in ``mr_out_dir``.\n\n    Append the output to ``out_file`` if provided.  Otherwise, return\n    the result as a single string (it is the caller's responsibility to\n    ensure that the amount of data retrieved fits into memory).\n    \"\"\"\n    if out_file is None:\n        output = []\n        for fn in iter_mr_out_files(mr_out_dir):\n            with hdfs.open(fn, \"rt\") as f:\n                output.append(f.read())\n        return \"\".join(output)\n    else:\n        block_size = 16777216\n        with open(out_file, 'a') as o:\n            for fn in iter_mr_out_files(mr_out_dir):\n                with hdfs.open(fn) as f:\n                    data = f.read(block_size)\n                    while len(data) > 0:\n                        o.write(data)\n                        data = f.read(block_size)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_input(self, input_, put=False):\n        if put and self.wd:\n            self.logger.info(\"copying input data to HDFS\")\n            hdfs.put(input_, self.input)\n        else:\n            self.input = input_\n            self.logger.info(\"assigning input to %s\", self.input)", "response": "Set the input path for the job."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the output path for the job.", "response": "def set_output(self, output):\n        \"\"\"\n        Set the output path for the job.  Optional if the runner has been\n        instantiated with a prefix.\n        \"\"\"\n        self.output = output\n        self.logger.info(\"assigning output to %s\", self.output)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_exe(self, pipes_code):\n        if not self.output:\n            raise RuntimeError(\"no output directory, can't create launcher\")\n        parent = hdfs.path.dirname(hdfs.path.abspath(self.output.rstrip(\"/\")))\n        self.exe = hdfs.path.join(parent, utils.make_random_str())\n        hdfs.dump(pipes_code, self.exe)", "response": "Dump launcher code to the distributed file system."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrunning the pipes job.", "response": "def run(self, **kwargs):\n        \"\"\"\n        Run the pipes job.  Keyword arguments are passed to :func:`run_pipes`.\n        \"\"\"\n        if not (self.input and self.output and self.exe):\n            raise RuntimeError(\"setup incomplete, can't run\")\n        self.logger.info(\"running MapReduce application\")\n        run_pipes(self.exe, self.input, self.output, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncollect the output of the current job.", "response": "def collect_output(self, out_file=None):\n        \"\"\"\n        Run :func:`collect_output` on the job's output directory.\n        \"\"\"\n        if self.logger.isEnabledFor(logging.INFO):\n            self.logger.info(\n                \"collecting output %s\", \" to %s\" % out_file if out_file else ''\n            )\n            self.logger.info(\"self.output %s\", self.output)\n        return collect_output(self.output, out_file)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nopen a file returning an HDFSFile object.", "response": "def open(hdfs_path, mode=\"r\", buff_size=0, replication=0, blocksize=0,\n         user=None, encoding=None, errors=None):\n    \"\"\"\n    Open a file, returning an :class:`~.file.hdfs_file` object.\n\n    ``hdfs_path`` and ``user`` are passed to :func:`~path.split`,\n    while the other args are passed to the :class:`~.file.hdfs_file`\n    constructor.\n    \"\"\"\n    host, port, path_ = path.split(hdfs_path, user)\n    fs = hdfs(host, port, user)\n    return fs.open_file(path_, mode, buff_size, replication, blocksize,\n                        encoding, errors)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load(hdfs_path, **kwargs):\n    m, _ = common.parse_mode(kwargs.get(\"mode\", \"r\"))\n    if m != \"r\":\n        raise ValueError(\"opening mode must be readonly\")\n    with open(hdfs_path, **kwargs) as fi:\n        data = fi.read()\n    fi.fs.close()\n    return data", "response": "\\ Reads the content of HDFS_PATH and returns it."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef put(src_path, dest_hdfs_path, **kwargs):\n    cp(path.abspath(src_path, local=True), dest_hdfs_path, **kwargs)", "response": "\\ Copy the contents of src_path to dest_hdfs_path."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get(src_hdfs_path, dest_path, **kwargs):\n    cp(src_hdfs_path, path.abspath(dest_path, local=True), **kwargs)", "response": "\\ Copy the contents of src_hdfs_path to dest_path."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef mkdir(hdfs_path, user=None):\n    host, port, path_ = path.split(hdfs_path, user)\n    fs = hdfs(host, port, user)\n    retval = fs.create_directory(path_)\n    fs.close()\n    return retval", "response": "Create a directory and its parents as needed."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef rm(hdfs_path, recursive=True, user=None):\n    host, port, path_ = path.split(hdfs_path, user)\n    fs = hdfs(host, port, user)\n    retval = fs.delete(path_, recursive=recursive)\n    fs.close()\n    return retval", "response": "Remove a file or directory or directory."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of dictionaries of file properties.", "response": "def lsl(hdfs_path, user=None, recursive=False):\n    \"\"\"\n    Return a list of dictionaries of file properties.\n\n    If ``hdfs_path`` is a file, there is only one item corresponding to\n    the file itself; if it is a directory and ``recursive`` is\n    :obj:`False`, each list item corresponds to a file or directory\n    contained by it; if it is a directory and ``recursive`` is\n    :obj:`True`, the list contains one item for every file or directory\n    in the tree rooted at ``hdfs_path``.\n    \"\"\"\n    host, port, path_ = path.split(hdfs_path, user)\n    fs = hdfs(host, port, user)\n    if not recursive:\n        dir_list = fs.list_directory(path_)\n    else:\n        treewalk = fs.walk(path_)\n        top = next(treewalk)\n        if top['kind'] == 'directory':\n            dir_list = list(treewalk)\n        else:\n            dir_list = [top]\n    fs.close()\n    return dir_list"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a list of hdfs paths. Works in the same way as :func:`lsl`, except for the fact that list items are hdfs paths instead of dictionaries of properties.", "response": "def ls(hdfs_path, user=None, recursive=False):\n    \"\"\"\n    Return a list of hdfs paths.\n\n    Works in the same way as :func:`lsl`, except for the fact that list\n    items are hdfs paths instead of dictionaries of properties.\n    \"\"\"\n    dir_list = lsl(hdfs_path, user, recursive)\n    return [d[\"name\"] for d in dir_list]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef chmod(hdfs_path, mode, user=None):\n    host, port, path_ = path.split(hdfs_path, user)\n    fs = hdfs(host, port, user)\n    retval = fs.chmod(path_, mode)\n    fs.close()\n    return retval", "response": "Change file mode bits."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmove or rename src to dest.", "response": "def move(src, dest, user=None):\n    \"\"\"\n    Move or rename src to dest.\n    \"\"\"\n    src_host, src_port, src_path = path.split(src, user)\n    dest_host, dest_port, dest_path = path.split(dest, user)\n    src_fs = hdfs(src_host, src_port, user)\n    dest_fs = hdfs(dest_host, dest_port, user)\n    try:\n        retval = src_fs.move(src_path, dest_fs, dest_path)\n        return retval\n    finally:\n        src_fs.close()\n        dest_fs.close()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrename a node from one node to another.", "response": "def rename(from_path, to_path, user=None):\n    \"\"\"\n    See :meth:`fs.hdfs.rename`.\n    \"\"\"\n    fhost, fport, fpath = path.split(from_path, user)\n    thost, tport, tpath = path.split(to_path, user)\n    with hdfs(thost, tport, user) as fs:\n        chost, cport = fs.host, fs.port\n    with hdfs(fhost, fport, user) as fs:\n        if fs.host != chost or fs.port != cport:\n            raise RuntimeError(\"can't do a cross-fs rename\")\n        return fs.rename(fpath, tpath)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef renames(from_path, to_path, user=None):\n    to_dir = path.dirname(to_path)\n    if to_dir:\n        mkdir(to_dir, user=user)\n    rename(from_path, to_path, user=user)", "response": "Rename from_path to to_path creating parents as needed."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads and return a line of text.", "response": "def readline(self):\n        \"\"\"\n        Read and return a line of text.\n\n        :rtype: str\n        :return: the next line of text in the file, including the\n          newline character\n        \"\"\"\n        _complain_ifclosed(self.closed)\n        line = self.f.readline()\n        if self.__encoding:\n            return line.decode(self.__encoding, self.__errors)\n        else:\n            return line"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read(self, length=-1):\n        _complain_ifclosed(self.closed)\n        # NOTE: libhdfs read stops at block boundaries: it is *essential*\n        # to ensure that we actually read the required number of bytes.\n        if length < 0:\n            length = self.size\n        chunks = []\n        while 1:\n            if length <= 0:\n                break\n            c = self.f.read(min(self.buff_size, length))\n            if c == b\"\":\n                break\n            chunks.append(c)\n            length -= len(c)\n        data = b\"\".join(chunks)\n        if self.__encoding:\n            return data.decode(self.__encoding, self.__errors)\n        else:\n            return data", "response": "Read length bytes from the file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef seek(self, position, whence=os.SEEK_SET):\n        _complain_ifclosed(self.closed)\n        return self.f.seek(position, whence)", "response": "Seek to the specified position in the file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write(self, data):\n        _complain_ifclosed(self.closed)\n        if self.__encoding:\n            self.f.write(data.encode(self.__encoding, self.__errors))\n            return len(data)\n        else:\n            return self.f.write(data)", "response": "Writes data to the file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pread_chunk(self, position, chunk):\n        _complain_ifclosed(self.closed)\n        if position > self.size:\n            raise IOError(\"position cannot be past EOF\")\n        return self.f.raw.pread_chunk(position, chunk)", "response": "r Reads a chunk of data from the file at the specified position."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconfiguring job based on the arguments provided.", "response": "def set_args(self, args, unknown_args=None):\n        \"\"\"\n        Configure job, based on the arguments provided.\n        \"\"\"\n        if unknown_args is None:\n            unknown_args = []\n        self.logger.setLevel(getattr(logging, args.log_level))\n\n        parent = hdfs.path.dirname(hdfs.path.abspath(args.output.rstrip(\"/\")))\n        self.remote_wd = hdfs.path.join(\n            parent, utils.make_random_str(prefix=\"pydoop_submit_\")\n        )\n        self.remote_exe = hdfs.path.join(self.remote_wd, str(uuid.uuid4()))\n        self.properties[JOB_NAME] = args.job_name or 'pydoop'\n        self.properties[IS_JAVA_RR] = (\n            'false' if args.do_not_use_java_record_reader else 'true'\n        )\n        self.properties[IS_JAVA_RW] = (\n            'false' if args.do_not_use_java_record_writer else 'true'\n        )\n        self.properties[JOB_REDUCES] = args.num_reducers\n        if args.job_name:\n            self.properties[JOB_NAME] = args.job_name\n        self.properties.update(args.job_conf or {})\n        self.__set_files_to_cache(args)\n        self.__set_archives_to_cache(args)\n        self.requested_env = self._env_arg_to_dict(args.set_env or [])\n        self.args = args\n        self.unknown_args = unknown_args"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck directories above the remote module and issue a warning if they are not readable by the task tracker.", "response": "def __warn_user_if_wd_maybe_unreadable(self, abs_remote_path):\n        \"\"\"\n        Check directories above the remote module and issue a warning if\n        they are not traversable by all users.\n\n        The reasoning behind this is mainly aimed at set-ups with a\n        centralized Hadoop cluster, accessed by all users, and where\n        the Hadoop task tracker user is not a superuser; an example\n        may be if you're running a shared Hadoop without HDFS (using\n        only a POSIX shared file system).  The task tracker correctly\n        changes user to the job requester's user for most operations,\n        but not when initializing the distributed cache, so jobs who\n        want to place files not accessible by the Hadoop user into\n        dist cache fail.\n        \"\"\"\n        host, port, path = hdfs.path.split(abs_remote_path)\n        if host == '' and port == 0:  # local file system\n            host_port = \"file:///\"\n        else:\n            # FIXME: this won't work with any scheme other than\n            # hdfs:// (e.g., s3)\n            host_port = \"hdfs://%s:%s/\" % (host, port)\n        path_pieces = path.strip('/').split(os.path.sep)\n        fs = hdfs.hdfs(host, port)\n        for i in range(0, len(path_pieces)):\n            part = os.path.join(\n                host_port, os.path.sep.join(path_pieces[0: i + 1])\n            )\n            permissions = fs.get_path_info(part)['permissions']\n            if permissions & 0o111 != 0o111:\n                self.logger.warning(\n                    (\"remote module %s may not be readable by the task \"\n                     \"tracker when initializing the distributed cache.  \"\n                     \"Permissions on %s: %s\"),\n                    abs_remote_path, part, oct(permissions)\n                )\n                break"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a docker - py client configured using environment variables AttributeNames and DOCKER_CLIENT_TIMEOUT.", "response": "def docker_client():\n    \"\"\"\n    Returns a docker-py client configured using environment variables\n    according to the same logic as the official Docker client.\n    \"\"\"\n    cert_path = os.environ.get('DOCKER_CERT_PATH', '')\n    if cert_path == '':\n        cert_path = os.path.join(os.environ.get('HOME', ''), '.docker')\n\n    base_url = os.environ.get('DOCKER_HOST')\n    tls_config = None\n\n    if os.environ.get('DOCKER_TLS_VERIFY', '') != '':\n        parts = base_url.split('://', 1)\n        base_url = '%s://%s' % ('https', parts[1])\n\n        client_cert = (os.path.join(cert_path, 'cert.pem'),\n                       os.path.join(cert_path, 'key.pem'))\n        ca_cert = os.path.join(cert_path, 'ca.pem')\n\n        tls_config = tls.TLSConfig(\n            ssl_version=ssl.PROTOCOL_TLSv1,\n            verify=True,\n            assert_hostname=False,\n            client_cert=client_cert,\n            ca_cert=ca_cert,\n        )\n\n    timeout = int(os.environ.get('DOCKER_CLIENT_TIMEOUT', 60))\n    return Client(\n        base_url=base_url, tls=tls_config, version='1.15', timeout=timeout\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntrying getting the java home from system properties.", "response": "def get_java_home():\n    \"\"\"\\\n    Try getting JAVA_HOME from system properties.\n\n    We are interested in the JDK home, containing include/jni.h, while the\n    java.home property points to the JRE home. If a JDK is installed, however,\n    the two are (usually) related: the JDK home is either the same directory\n    as the JRE home (recent java versions) or its parent (and java.home points\n    to jdk_home/jre).\n    \"\"\"\n    error = RuntimeError(\"java home not found, try setting JAVA_HOME\")\n    try:\n        return os.environ[\"JAVA_HOME\"]\n    except KeyError:\n        wd = tempfile.mkdtemp(prefix='pydoop_')\n        jclass = \"Temp\"\n        jsrc = os.path.join(wd, \"%s.java\" % jclass)\n        with open(jsrc, \"w\") as f:\n            f.write(JPROG.substitute(classname=jclass))\n        try:\n            subprocess.check_call([\"javac\", jsrc])\n            path = subprocess.check_output(\n                [\"java\", \"-cp\", wd, jclass], universal_newlines=True\n            )\n        except (OSError, UnicodeDecodeError, subprocess.CalledProcessError):\n            raise error\n        finally:\n            shutil.rmtree(wd)\n        path = os.path.normpath(path.strip())\n        if os.path.exists(os.path.join(path, \"include\", \"jni.h\")):\n            return path\n        path = os.path.dirname(path)\n        if os.path.exists(os.path.join(path, \"include\", \"jni.h\")):\n            return path\n        raise error"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run_task(factory, **kwargs):\n    context = TaskContext(factory, **kwargs)\n    pstats_dir = kwargs.get(\"pstats_dir\", os.getenv(PSTATS_DIR))\n    if pstats_dir:\n        import cProfile\n        import tempfile\n        import pydoop.hdfs as hdfs\n        hdfs.mkdir(pstats_dir)\n        fd, pstats_fn = tempfile.mkstemp(suffix=\".pstats\")\n        os.close(fd)\n        cProfile.runctx(\n            \"_run(context, **kwargs)\", globals(), locals(),\n            filename=pstats_fn\n        )\n        pstats_fmt = kwargs.get(\n            \"pstats_fmt\",\n            os.getenv(PSTATS_FMT, DEFAULT_PSTATS_FMT)\n        )\n        name = pstats_fmt % (\n            context.task_type,\n            context.get_task_partition(),\n            os.path.basename(pstats_fn)\n        )\n        hdfs.put(pstats_fn, hdfs.path.join(pstats_dir, name))\n    else:\n        _run(context, **kwargs)", "response": "Run a MapReduce task."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreporting the current progress of the cancelling.", "response": "def progress(self):\n        \"\"\"\\\n        Report progress to the Java side.\n\n        This needs to flush the uplink stream, but too many flushes can\n        disrupt performance, so we actually talk to upstream once per second.\n        \"\"\"\n        now = time()\n        if now - self.last_progress_t > 1:\n            self.last_progress_t = now\n            if self.status:\n                self.uplink.status(self.status)\n                self.status = None\n            self.__spill_counters()\n            self.uplink.progress(self.progress_value)\n            self.uplink.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nhandles an output key value pair.", "response": "def emit(self, key, value):\n        \"\"\"\\\n        Handle an output key/value pair.\n\n        Reporting progress is strictly necessary only when using a Python\n        record writer, because sending an output key/value pair is an implicit\n        progress report. To take advantage of this, however, we would be\n        forced to flush the uplink stream at every output, and that would be\n        too costly. Rather than add a specific timer for this, we just call\n        progress unconditionally and piggyback on its timer instead. Note that\n        when a combiner is caching there is no actual output, so in that case\n        we would need an explicit progress report anyway.\n        \"\"\"\n        if self.__spilling:\n            self.__actual_emit(key, value)\n        else:\n            # key must be hashable\n            self.__cache.setdefault(key, []).append(value)\n            self.__cache_size += sizeof(key) + sizeof(value)\n            if self.__cache_size >= self.__spill_size:\n                self.__spill_all()\n        self.progress()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef merge_templates(self, replacements, separator):\n\n        #TYPE PARAM CONTROL AND SPLIT\n        valid_separators = {'page_break', 'column_break', 'textWrapping_break', 'continuous_section', 'evenPage_section', 'nextColumn_section', 'nextPage_section', 'oddPage_section'}\n        if not separator in valid_separators:\n            raise ValueError(\"Invalid separator argument\")\n        type, sepClass = separator.split(\"_\")\n  \n\n        #GET ROOT - WORK WITH DOCUMENT\n        for part in self.parts.values():\n            root = part.getroot()\n            tag = root.tag\n            if tag == '{%(w)s}ftr' % NAMESPACES or tag == '{%(w)s}hdr' % NAMESPACES:\n                continue\n\t\t\n            if sepClass == 'section':\n\n                #FINDING FIRST SECTION OF THE DOCUMENT\n                firstSection = root.find(\"w:body/w:p/w:pPr/w:sectPr\", namespaces=NAMESPACES)\n                if firstSection == None:\n                    firstSection = root.find(\"w:body/w:sectPr\", namespaces=NAMESPACES)\n\t\t\t\n                #MODIFY TYPE ATTRIBUTE OF FIRST SECTION FOR MERGING\n                nextPageSec = deepcopy(firstSection)\n                for child in nextPageSec:\n                #Delete old type if exist\n                    if child.tag == '{%(w)s}type' % NAMESPACES:\n                        nextPageSec.remove(child)\n                #Create new type (def parameter)\n                newType = etree.SubElement(nextPageSec, '{%(w)s}type'  % NAMESPACES)\n                newType.set('{%(w)s}val'  % NAMESPACES, type)\n\n                #REPLACING FIRST SECTION\n                secRoot = firstSection.getparent()\n                secRoot.replace(firstSection, nextPageSec)\n\n            #FINDING LAST SECTION OF THE DOCUMENT\n            lastSection = root.find(\"w:body/w:sectPr\", namespaces=NAMESPACES)\n\n            #SAVING LAST SECTION\n            mainSection = deepcopy(lastSection)\n            lsecRoot = lastSection.getparent()\n            lsecRoot.remove(lastSection)\n\n            #COPY CHILDREN ELEMENTS OF BODY IN A LIST\n            childrenList = root.findall('w:body/*', namespaces=NAMESPACES)\n\n            #DELETE ALL CHILDREN OF BODY\n            for child in root:\n                if child.tag == '{%(w)s}body' % NAMESPACES:\n                    child.clear()\n\n            #REFILL BODY AND MERGE DOCS - ADD LAST SECTION ENCAPSULATED OR NOT\n            lr = len(replacements)\n            lc = len(childrenList)\n            parts = []\n            for i, repl in enumerate(replacements):\n                for (j, n) in enumerate(childrenList):\n                    element = deepcopy(n)\n                    for child in root:\n                        if child.tag == '{%(w)s}body' % NAMESPACES:\n                            child.append(element)\n                            parts.append(element)\n                            if (j + 1) == lc:\n                                if (i + 1) == lr:\n                                    child.append(mainSection)\n                                    parts.append(mainSection)\n                                else:\n                                    if sepClass == 'section':\n                                        intSection = deepcopy(mainSection)\n                                        p   = etree.SubElement(child, '{%(w)s}p'  % NAMESPACES)\n                                        pPr = etree.SubElement(p, '{%(w)s}pPr'  % NAMESPACES)\n                                        pPr.append(intSection)\n                                        parts.append(p)\n                                    elif sepClass == 'break':\n                                        pb   = etree.SubElement(child, '{%(w)s}p'  % NAMESPACES)\n                                        r = etree.SubElement(pb, '{%(w)s}r'  % NAMESPACES)\n                                        nbreak = Element('{%(w)s}br' % NAMESPACES)\n                                        nbreak.attrib['{%(w)s}type' % NAMESPACES] = type\n                                        r.append(nbreak)\n\n                    self.merge(parts, **repl)", "response": "Creates a copy of the template and replaces it with the new ones."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef importpath(path, error_text=None):\n    result = None\n    attrs = []\n    parts = path.split('.')\n    exception = None\n    while parts:\n        try:\n            result = __import__('.'.join(parts), {}, {}, [''])\n        except ImportError as e:\n            if exception is None:\n                exception = e\n            attrs = parts[-1:] + attrs\n            parts = parts[:-1]\n        else:\n            break\n    for attr in attrs:\n        try:\n            result = getattr(result, attr)\n        except (AttributeError, ValueError) as e:\n            if error_text is not None:\n                raise ImproperlyConfigured('Error: %s can import \"%s\"' % (\n                    error_text, path))\n            else:\n                raise exception\n    return result", "response": "Imports value by specified path."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef google_v3(address):\n    try:\n        g = geocoders.GoogleV3(api_key=settings.EASY_MAPS_GOOGLE_KEY or settings.EASY_MAPS_GOOGLE_MAPS_API_KEY)\n\n        results = g.geocode(smart_str(address), exactly_one=False)\n        if results is not None:\n            return results[0]\n\n        raise Error(\"No results found for '%s'\" % address)\n    except (UnboundLocalError, ValueError, GeocoderServiceError) as e:\n        raise Error(e)", "response": "Given an address return a tuple using Google Geocoding API v3."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef forwards(self, orm):\n        \"Write your forwards methods here.\"\n        # for every status\n        for status in orm['booking.BookingStatus'].objects.all():\n            # iterate over the old renamed translation instances\n            for statustrans_old in orm['booking.BookingStatusTranslationRenamed'].objects.filter(status=status):\n                orm['booking.BookingStatusTranslation'].objects.create(\n                    name=statustrans_old.name,\n                    language_code=statustrans_old.language,\n                    master=status,\n                )", "response": "Write your forwards methods here."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_booking(request):\n    booking = None\n    if request.user.is_authenticated():\n        try:\n            booking = Booking.objects.get(\n                user=request.user,\n                booking_status__slug='inprogress')\n        except Booking.DoesNotExist:\n            # The user does not have any open bookings\n            pass\n    else:\n        session = Session.objects.get(\n            session_key=request.session.session_key)\n        try:\n            booking = Booking.objects.get(session=session)\n        except Booking.DoesNotExist:\n            # The user does not have any bookings in his session\n            pass\n    return booking", "response": "Returns the booking that is in - progress for the current user or None if there is no such booking."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef persist_booking(booking, user):\n    if booking is not None:\n        existing_bookings = Booking.objects.filter(\n            user=user, booking_status__slug='inprogress').exclude(\n            pk=booking.pk)\n        existing_bookings.delete()\n\n        booking.session = None\n        booking.user = user\n        booking.save()", "response": "Saves the in - progress booking to the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_parent(self, parent):\n        self.parent = parent\n\n        if self.config_type == 'edit':\n            self.rel_path_fwd = 'edit %s\\n' % self.get_name()\n            self.rel_path_bwd = 'next\\n'\n        elif self.config_type == 'config':\n            self.rel_path_fwd = 'config %s\\n' % self.get_name()\n            self.rel_path_bwd = 'end\\n'\n\n        self.full_path_fwd = '%s%s' % (self.get_parent().full_path_fwd, self.rel_path_fwd)\n        self.full_path_bwd = '%s%s' % (self.rel_path_bwd, self.get_parent().full_path_bwd)", "response": "Sets the parent of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_text(self, relative=False, indent_level=0, clean_empty_block=False):\n        if relative:\n            fwd = self.rel_path_fwd\n            bwd = self.rel_path_bwd\n        else:\n            fwd = self.full_path_fwd\n            bwd = self.full_path_bwd\n\n        indent = 4*indent_level*' '\n        pre = '%s%s' % (indent, fwd)\n        post = '%s%s' % (indent, bwd)\n\n        text = ''\n        for param, value in self.iterparams():\n            text += '  %sset %s %s\\n' % (indent, param, value)\n\n        for key, block in self.iterblocks():\n            text += block.to_text(True, indent_level+1)\n\n        if len(text) > 0 or not clean_empty_block:\n            text = '%s%s%s' % (pre, text, post)\n        return text", "response": "This method returns the object model in text format."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_config_output(self, output):\n        regexp = re.compile('^(config |edit |set |end$|next$)(.*)')\n        current_block = self\n\n        if isinstance(output, py23_compat.string_types):\n            output = output.splitlines()\n\n        for line in output:\n            if 'uuid' in line:\n                continue\n            if 'snmp-index' in line:\n                continue\n            line = line.strip()\n            result = regexp.match(line)\n\n            if result is not None:\n                action = result.group(1).strip()\n                data = result.group(2).strip()\n\n                if action == 'config' or action == 'edit':\n                    data = data.replace('\"', '')\n                    if data not in current_block.get_block_names():\n                        config_block = FortiConfig(data, action, current_block)\n                        current_block[data] = config_block\n                    else:\n                        config_block = current_block[data]\n                    current_block = config_block\n                elif action == 'end' or action == 'next':\n                    current_block = current_block.get_parent()\n                elif action == 'set':\n                    split_data = data.split(' ')\n                    parameter = split_data[0]\n                    data = split_data[1:]\n                    current_block.set_param(parameter, ' '.join(data))", "response": "This method will parse the output of FortiOS config command and return a FortiConfig object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nopens the ssh session with the device.", "response": "def open(self):\n        \"\"\"\n        Opens the ssh session with the device.\n        \"\"\"\n\n        logger.debug('Connecting to device %s, vdom %s' % (self.hostname, self.vdom))\n\n        self.ssh = paramiko.SSHClient()\n        self.ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n\n        cfg = {\n            'hostname': self.hostname,\n            'timeout': self.timeout,\n            'username': self.username,\n            'password': self.password,\n            'key_filename': self.keyfile\n        }\n\n        if os.path.exists(os.path.expanduser(\"~/.ssh/config\")):\n            ssh_config = paramiko.SSHConfig()\n            user_config_file = os.path.expanduser(\"~/.ssh/config\")\n            with io.open(user_config_file, 'rt', encoding='utf-8') as f:\n                ssh_config.parse(f)\n\n            host_conf = ssh_config.lookup(self.hostname)\n            if host_conf:\n                if 'proxycommand' in host_conf:\n                    cfg['sock'] = paramiko.ProxyCommand(host_conf['proxycommand'])\n                if 'user' in host_conf:\n                    cfg['username'] = host_conf['user']\n                if 'identityfile' in host_conf:\n                    cfg['key_filename'] = host_conf['identityfile']\n                if 'hostname' in host_conf:\n                    cfg['hostname'] = host_conf['hostname']\n\n        self.ssh.connect(**cfg)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _read_wrapper(data):\n        # Paramiko (strangely) in PY3 returns an int here.\n        if isinstance(data, int):\n            data = chr(data)\n        # Ensure unicode\n        return py23_compat.text_type(data)", "response": "Ensure unicode always returned on read."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nexecute a command on the device and return a list of strings containing the output.", "response": "def execute_command(self, command):\n        \"\"\"\n        This method will execute the commands on the device without as if you were just connected to it (it will not\n        enter into any vdom). This method is not recommended unless you are 100% sure of what you are doing.\n\n\n        Args:\n            * **command** (str) -- Command to execute.\n\n        Returns:\n            A list of strings containing the output.\n\n        Raises:\n            exceptions.CommandExecutionException -- If it detects any problem with the command.\n        \"\"\"\n        logger.debug('Executing commands:\\n %s' % command)\n\n        err_msg = 'Something happened when executing some commands on device'\n\n        chan = self.ssh.get_transport().open_session()\n        chan.settimeout(5)\n\n        chan.exec_command(command)\n\n        error_chan = chan.makefile_stderr()\n        output_chan = chan.makefile()\n\n        error = ''\n        output = ''\n        for e in error_chan.read():\n            error = error + self._read_wrapper(e)\n        for o in output_chan.read():\n            output = output + self._read_wrapper(o)\n\n        if len(error) > 0:\n            msg = '%s %s:\\n%s\\n%s' % (err_msg, self.ssh.get_host_keys().keys()[0], command, error)\n            logger.error(msg)\n            raise exceptions.CommandExecutionException(msg)\n\n        regex = re.compile('Command fail')\n        if len(regex.findall(output)) > 0:\n            msg = '%s %s:\\n%s\\n%s' % (err_msg, self.ssh.get_host_keys().keys()[0], command, output)\n            logger.error(msg)\n            raise exceptions.CommandExecutionException(msg)\n\n        output = output.splitlines()\n\n        # We look for the prompt and remove it\n        i = 0\n        for line in output:\n            current_line = line.split('#')\n\n            if len(current_line) > 1:\n                output[i] = current_line[1]\n            else:\n                output[i] = current_line[0]\n            i += 1\n\n        return output[:-1]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef compare_config(self, other=None, text=False):\n        if other is None:\n            other = self.candidate_config\n\n        if not text:\n            return self.running_config.compare_config(other)\n        else:\n            diff = Differ()\n            result = diff.compare(\n                self.running_config.to_text().splitlines(),\n                other.to_text().splitlines()\n            )\n            return '\\n'.join(result)", "response": "Compares the running config with the candidate config."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _commit(self, config_text=None, force=False, reload_original_config=True):\n        def _execute(config_text):\n            if config_text is None:\n                config_text = self.compare_config()\n\n            config_vdom = \"\"\n\n            if self.vdom is None:\n                pre = ''\n            elif not 'global' in self.vdom:\n                config_vdom = 'conf vdom\\n  edit %s\\n' % self.vdom\n                pre = 'conf global\\n    '\n            else:\n                pre = 'conf global\\n    '\n\n            cmd = '%sexecute batch start\\n' % pre\n            cmd += config_vdom\n            cmd += config_text\n            cmd += '\\nexecute batch end\\n'\n\n            self.execute_command(cmd)\n            last_log = self.execute_command('%sexecute batch lastlog' % pre)\n\n            return self._parse_batch_lastlog(last_log)\n\n        logger.info('Committing config ')\n\n        wrong_commands = _execute(config_text)\n\n        self._reload_config(reload_original_config)\n\n        retry_codes = [-3, -23]\n        retries = 5\n        while retries > 0:\n            retries -= 1\n            for wc in wrong_commands:\n                if int(wc[0]) in retry_codes:\n                    if config_text is None:\n                        config_text = self.compare_config()\n                    wrong_commands = _execute(config_text)\n                    self._reload_config(reload_original_config=False)\n                    break\n\n        if len(wrong_commands) > 0:\n            exit_code = -2\n            logging.debug('List of commands that failed: %s' % wrong_commands)\n\n            if not force:\n                exit_code = -1\n                self.rollback()\n\n            if exit_code < 0:\n                raise exceptions.FailedCommit(wrong_commands)", "response": "Commits the running config."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef rollback(self):\n        logger.info('Rolling back changes')\n\n        config_text = self.compare_config(other=self.original_config)\n\n        if len(config_text) > 0:\n            return self._commit(config_text, force=True, reload_original_config=False)", "response": "This method will rollback all changes and go to the original config"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef generate_states(self, initial_condition=None, with_noise=True, stateseq=None):\n        from pybasicbayes.util.stats import sample_discrete\n        # Generate from the prior and raise exception if unstable\n        T, K, n = self.T, self.num_states, self.D_latent\n        A = self.trans_matrix\n\n        # Initialize discrete state sequence\n        dss = -1 * np.ones(T, dtype=np.int32) if stateseq is None else stateseq.astype(np.int32)\n        assert dss.shape == (T,)\n        gss = np.empty((T,n), dtype='double')\n\n        if initial_condition is None:\n            if dss[0] == -1:\n                dss[0] = sample_discrete(self.pi_0)\n            gss[0] = self.init_dynamics_distns[dss[0]].rvs()\n        else:\n            dss[0] = initial_condition[0]\n            gss[0] = initial_condition[1]\n\n        for t in range(1,T):\n            # Sample discrete state given previous continuous state\n            if with_noise:\n                # Sample discre=te state from recurrent transition matrix\n                if dss[t] == -1:\n                    dss[t] = sample_discrete(A[dss[t-1], :])\n\n                # Sample continuous state given current discrete state\n                gss[t] = self.dynamics_distns[dss[t-1]].\\\n                    rvs(x=np.hstack((gss[t-1][None,:], self.inputs[t-1][None,:])),\n                        return_xy=False)\n            else:\n                # Pick the most likely next discrete state and continuous state\n                if dss[t] == -1:\n                    dss[t] = np.argmax(A[dss[t-1], :])\n\n                gss[t] = self.dynamics_distns[dss[t-1]]. \\\n                    predict(np.hstack((gss[t-1][None,:], self.inputs[t-1][None,:])))\n            assert np.all(np.isfinite(gss[t])), \"SLDS appears to be unstable!\"\n\n        self.stateseq = dss\n        self.gaussian_states = gss", "response": "Generate the discrete and continuous states from the prior and raise an exception if unstable."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef vbem_aBl(self):\n        vbem_aBl = np.zeros((self.T, self.num_states))\n        ids, dds, eds = self.init_dynamics_distns, self.dynamics_distns, self.emission_distns\n\n        for k, (id, dd) in enumerate(zip(ids, dds)):\n            vbem_aBl[0, k] = expected_gaussian_logprob(id.mu, id.sigma, self.E_init_stats)\n            vbem_aBl[:-1, k] += expected_regression_log_prob(dd, self.E_dynamics_stats)\n\n        if self.single_emission:\n            ed = self.emission_distns[0]\n            vbem_aBl += expected_regression_log_prob(ed, self.E_emission_stats)[:,None]\n        else:\n            for k, ed in enumerate(self.emission_distns):\n                vbem_aBl[:, k] += expected_regression_log_prob(ed, self.E_emission_stats)\n\n        vbem_aBl[np.isnan(vbem_aBl).any(1)] = 0.\n        return vbem_aBl", "response": "Return the expected log likelihoods for each class in the discrete states."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef expected_log_joint_probability(self):\n        # E_{q(z)}[log p(z)]\n        from pyslds.util import expected_hmm_logprob\n        elp = expected_hmm_logprob(\n            self.pi_0, self.trans_matrix,\n            (self.expected_states, self.expected_transcounts, self._normalizer))\n\n        # E_{q(x)}[log p(y, x | z)]  is given by aBl\n        # To get E_{q(x)}[ aBl ] we multiply and sum\n        elp += np.sum(self.expected_states * self.vbem_aBl)\n        return elp", "response": "Compute the expected log joint probability of the current state."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef mf_aBl(self):\n        mf_aBl = self._mf_aBl = np.zeros((self.T, self.num_states))\n        ids, dds, eds = self.init_dynamics_distns, self.dynamics_distns, \\\n            self.emission_distns\n\n        for idx, (d1, d2, d3) in enumerate(zip(ids, dds, eds)):\n            mf_aBl[0,idx] = d1.expected_log_likelihood(\n                stats=self.E_init_stats)\n            mf_aBl[:-1,idx] += d2.expected_log_likelihood(\n                stats=self.E_dynamics_stats)\n            mf_aBl[:,idx] += d3.expected_log_likelihood(\n                stats=self.E_emission_stats)\n\n        mf_aBl[np.isnan(mf_aBl).any(1)] = 0.\n        return mf_aBl", "response": "A method to compute the expected log likelihoods for each of the discrete states."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing the log likelihood of the masked data given the latent discrete and continuous states.", "response": "def heldout_log_likelihood(self, test_mask=None):\n        \"\"\"\n        Compute the log likelihood of the masked data given the latent\n        discrete and continuous states.\n        \"\"\"\n        if test_mask is None:\n            # If a test mask is not supplied, use the negation of this object's mask\n            if self.mask is None:\n                return 0\n            else:\n                test_mask = ~self.mask\n\n        xs = np.hstack((self.gaussian_states, self.inputs))\n        if self.single_emission:\n            return self.emission_distns[0].\\\n                log_likelihood((xs, self.data), mask=test_mask).sum()\n        else:\n            hll = 0\n            z = self.stateseq\n            for idx, ed in enumerate(self.emission_distns):\n                hll += ed.log_likelihood((xs[z == idx], self.data[z == idx]),\n                                         mask=test_mask[z == idx]).sum()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets an empirical rate from a Smooth count data", "response": "def empirical_rate(data, sigma=3.0):\n        \"\"\"\n        Smooth count data to get an empirical rate\n        \"\"\"\n        from scipy.ndimage.filters import gaussian_filter1d\n        return 0.001 + gaussian_filter1d(data.astype(np.float), sigma, axis=0)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nestimating the parameters of an AR observation model by fitting an AR model to the entire dataset and then using the inferred noise covariance as the prior mean.", "response": "def get_empirical_ar_params(train_datas, params):\n    \"\"\"\n    Estimate the parameters of an AR observation model\n    by fitting a single AR model to the entire dataset.\n    \"\"\"\n    assert isinstance(train_datas, list) and len(train_datas) > 0\n    datadimension = train_datas[0].shape[1]\n    assert params[\"nu_0\"] > datadimension + 1\n\n    # Initialize the observation parameters\n    obs_params = dict(nu_0=params[\"nu_0\"],\n                      S_0=params['S_0'],\n                      M_0=params['M_0'],\n                      K_0=params['K_0'],\n                      affine=params['affine'])\n\n\n    # Fit an AR model to the entire dataset\n    obs_distn = AutoRegression(**obs_params)\n    obs_distn.max_likelihood(train_datas)\n\n    # Use the inferred noise covariance as the prior mean\n    # E_{IW}[S] = S_0 / (nu_0 - datadimension - 1)\n    obs_params[\"S_0\"] = obs_distn.sigma * (params[\"nu_0\"] - datadimension - 1)\n    obs_params[\"M_0\"] = obs_distn.A.copy()\n\n    return obs_params"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef expected_hmm_logprob(pi_0, trans_matrix, stats):\n    E_z, sum_E_ztztp1T, _ = stats\n    T, K = E_z.shape\n    assert sum_E_ztztp1T.shape == (K, K)\n\n    out = 0\n    out += np.dot(E_z[0], np.log(pi_0))\n    out += np.sum(sum_E_ztztp1T * np.log(trans_matrix))\n    return out", "response": "Compute expected HMM logprob."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef expected_dense_regression_log_prob(A, Sigma, stats):\n    yyT, yxT, xxT, n = stats[-4:]\n\n    contract = 'ij,nij->n' if yyT.ndim == 3 else 'ij,ij->'\n    D = A.shape[0]\n    Si = np.linalg.inv(Sigma)\n    SiA = Si.dot(A)\n    ASiA = A.T.dot(SiA)\n\n    out = -1. / 2 * np.einsum(contract, ASiA, xxT)\n    out += np.einsum(contract, SiA, yxT)\n    out += -1. / 2 * np.einsum(contract, Si, yyT)\n    out += -D / 2 * np.log(2 * np.pi) + n / 2. * np.linalg.slogdet(Si)[1]\n    return out", "response": "This function computes the expected log likelihood of p y x where y ~ N Ax Sigma is wrt q ( y x x."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef expected_diag_regression_log_prob(A, sigmasq, stats):\n    D_out, D_in = A.shape\n    assert sigmasq.shape == (D_out,)\n\n    ysq, yxT, xxT, mask = stats[-4:]\n    T = ysq.shape[0]\n    assert ysq.shape == (T, D_out)\n    assert yxT.shape == (T, D_out, D_in)\n\n    # xxT has different shapes depending on whether or not data is missing\n    # with missing data, it is (T, Dout, Din, Din) since we need to mask\n    # off certain xxT pairs.  If there's no mask, it's always the same for\n    # every output dimension.  To make it easy, we just broadcast along\n    # the Dout dimension for the no-missing-data case.\n    if xxT.shape == (T, D_in, D_in):\n        xxT = xxT[:,None,:,:]\n    else:\n        assert xxT.shape == (T, D_out, D_in, D_in)\n\n    assert mask.shape == (T, D_out)\n\n    AAT = np.array([np.outer(a, a) for a in A])\n    n = mask.sum(1)\n\n    J_node = AAT[None, :, :, :] / sigmasq[None, :, None, None]\n    h_node = (mask / sigmasq)[:,:,None] * A[None, :, :]\n\n    out = -1 / 2. * np.sum(J_node * xxT, axis=(1, 2, 3))\n    out += np.sum(h_node * yxT, axis=(1, 2))\n    out += -1 / 2. * np.sum(mask / sigmasq * ysq, axis=1)\n    out += -n / 2. * np.log(2 * np.pi)\n    out += -1 / 2. * np.sum(mask * np.log(sigmasq), axis=1)\n    assert out.shape == (T,)\n    return out", "response": "This function computes the expected log likelihood of a diagonal regression matrix."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncapturing a new page into the archive. org.", "response": "def capture(\n    target_url,\n    user_agent=\"savepagenow (https://github.com/pastpages/savepagenow)\",\n    accept_cache=False\n):\n    \"\"\"\n    Archives the provided URL using archive.org's Wayback Machine.\n\n    Returns the archive.org URL where the capture is stored.\n\n    Raises a CachedPage exception if archive.org declines to conduct a new\n    capture and returns a previous snapshot instead.\n\n    To silence that exception, pass into True to the ``accept_cache`` keyword\n    argument.\n    \"\"\"\n    # Put together the URL that will save our request\n    domain = \"https://web.archive.org\"\n    save_url = urljoin(domain, \"/save/\")\n    request_url = save_url + target_url\n\n    # Send the capture request to achive.org\n    headers = {\n        'User-Agent': user_agent,\n    }\n    response = requests.get(request_url, headers=headers)\n\n    # If it has an error header, raise that.\n    has_error_header = 'X-Archive-Wayback-Runtime-Error' in response.headers\n    if has_error_header:\n        error_header = response.headers['X-Archive-Wayback-Runtime-Error']\n        if error_header == 'RobotAccessControlException: Blocked By Robots':\n            raise BlockedByRobots(\"archive.org returned blocked by robots.txt error\")\n        else:\n            raise WaybackRuntimeError(error_header)\n\n    # If it has an error code, raise that\n    if response.status_code in [403, 502]:\n        raise WaybackRuntimeError(response.headers)\n\n    # Put together the URL where this page is archived\n    try:\n        archive_id = response.headers['Content-Location']\n    except KeyError:\n        # If it can't find that key raise the error\n        raise WaybackRuntimeError(dict(status_code=response.status_code, headers=response.headers))\n    archive_url = urljoin(domain, archive_id)\n\n    # Determine if the response was cached\n    cached = 'X-Page-Cache' in response.headers and response.headers['X-Page-Cache'] == 'HIT'\n    if cached:\n        if not accept_cache:\n            raise CachedPage(\"archive.org returned a cached version of this page: {}\".format(\n                archive_url\n            ))\n\n    # Return that\n    return archive_url"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef capture_or_cache(\n    target_url,\n    user_agent=\"savepagenow (https://github.com/pastpages/savepagenow)\"\n):\n    \"\"\"\n    Archives the provided URL using archive.org's Wayback Machine, unless\n    the page has been recently captured.\n\n    Returns a tuple with the archive.org URL where the capture is stored,\n    along with a boolean indicating if a new capture was conducted.\n\n    If the boolean is True, archive.org conducted a new capture. If it is False,\n    archive.org has returned a recently cached capture instead, likely taken\n    in the previous minutes.\n    \"\"\"\n    try:\n        return capture(target_url, user_agent=user_agent, accept_cache=False), True\n    except CachedPage:\n        return capture(target_url, user_agent=user_agent, accept_cache=True), False", "response": "Capture a page or cache it."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a unique id for a given element", "response": "def get_unique_id(element):\n    \"\"\"Returns a unique id for a given element\"\"\"\n    this_id = make_id(element)\n    dup = True\n    while dup:\n        if this_id not in ids:\n            dup = False\n            ids.append(this_id)\n        else:\n            this_id = make_id(element)\n    return ids[-1]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_xml_type(val):\n    if type(val).__name__ in ('str', 'unicode'):\n        return 'str'\n    if type(val).__name__ in ('int', 'long'):\n        return 'int'\n    if type(val).__name__ == 'float':\n        return 'float'\n    if type(val).__name__ == 'bool':\n        return 'bool'\n    if isinstance(val, numbers.Number):\n        return 'number'\n    if type(val).__name__ == 'NoneType':\n        return 'null'\n    if isinstance(val, dict):\n        return 'dict'\n    if isinstance(val, collections.Iterable):\n        return 'list'\n    return type(val).__name__", "response": "Returns the data type for the xml type attribute"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef make_attrstring(attr):\n    attrstring = ' '.join(['%s=\"%s\"' % (k, v) for k, v in attr.items()])\n    return '%s%s' % (' ' if attrstring != '' else '', attrstring)", "response": "Returns an attribute string in the form key = val"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef key_is_valid_xml(key):\n    LOG.info('Inside key_is_valid_xml(). Testing \"%s\"' % (unicode_me(key)))\n    test_xml = '<?xml version=\"1.0\" encoding=\"UTF-8\" ?><%s>foo</%s>' % (key, key)\n    try:\n        parseString(test_xml)\n        return True\n    except Exception: # minidom does not implement exceptions well\n        return False", "response": "Checks that a key is a valid XML name."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntests an XML name and fixes it if invalid", "response": "def make_valid_xml_name(key, attr):\n    \"\"\"Tests an XML name and fixes it if invalid\"\"\"\n    LOG.info('Inside make_valid_xml_name(). Testing key \"%s\" with attr \"%s\"' % (\n        unicode_me(key), unicode_me(attr))\n    )\n    key = escape_xml(key)\n    attr = escape_xml(attr)\n    \n    # pass through if key is already valid\n    if key_is_valid_xml(key):\n        return key, attr\n        \n    # prepend a lowercase n if the key is numeric\n    if key.isdigit():\n        return 'n%s' % (key), attr\n        \n    # replace spaces with underscores if that fixes the problem\n    if key_is_valid_xml(key.replace(' ', '_')):\n        return key.replace(' ', '_'), attr\n        \n    # key is still invalid - move it into a name attribute\n    attr['name'] = key\n    key = 'key'\n    return key, attr"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef convert(obj, ids, attr_type, item_func, cdata, parent='root'):\n    \n    LOG.info('Inside convert(). obj type is: \"%s\", obj=\"%s\"' % (type(obj).__name__, unicode_me(obj)))\n    \n    item_name = item_func(parent)\n    \n    if isinstance(obj, numbers.Number) or type(obj) in (str, unicode):\n        return convert_kv(item_name, obj, attr_type, cdata)\n        \n    if hasattr(obj, 'isoformat'):\n        return convert_kv(item_name, obj.isoformat(), attr_type, cdata)\n        \n    if type(obj) == bool:\n        return convert_bool(item_name, obj, attr_type, cdata)\n        \n    if obj is None:\n        return convert_none(item_name, '', attr_type, cdata)\n        \n    if isinstance(obj, dict):\n        return convert_dict(obj, ids, parent, attr_type, item_func, cdata)\n        \n    if isinstance(obj, collections.Iterable):\n        return convert_list(obj, ids, parent, attr_type, item_func, cdata)\n        \n    raise TypeError('Unsupported data type: %s (%s)' % (obj, type(obj).__name__))", "response": "Convert an object to the right function to convert them \n   "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef convert_dict(obj, ids, parent, attr_type, item_func, cdata):\n    LOG.info('Inside convert_dict(): obj type is: \"%s\", obj=\"%s\"' % (\n        type(obj).__name__, unicode_me(obj))\n    )\n    output = []\n    addline = output.append\n    \n    item_name = item_func(parent)\n    \n    for key, val in obj.items():\n        LOG.info('Looping inside convert_dict(): key=\"%s\", val=\"%s\", type(val)=\"%s\"' % (\n            unicode_me(key), unicode_me(val), type(val).__name__)\n        )\n\n        attr = {} if not ids else {'id': '%s' % (get_unique_id(parent)) }\n\n        key, attr = make_valid_xml_name(key, attr)\n\n        if isinstance(val, numbers.Number) or type(val) in (str, unicode):\n            addline(convert_kv(key, val, attr_type, attr, cdata))\n\n        elif hasattr(val, 'isoformat'): # datetime\n            addline(convert_kv(key, val.isoformat(), attr_type, attr, cdata))\n\n        elif type(val) == bool:\n            addline(convert_bool(key, val, attr_type, attr, cdata))\n\n        elif isinstance(val, dict):\n            if attr_type:\n                attr['type'] = get_xml_type(val)\n            addline('<%s%s>%s</%s>' % (\n                key, make_attrstring(attr), \n                convert_dict(val, ids, key, attr_type, item_func, cdata), \n                key\n                )\n            )\n\n        elif isinstance(val, collections.Iterable):\n            if attr_type:\n                attr['type'] = get_xml_type(val)\n            addline('<%s%s>%s</%s>' % (\n                key, \n                make_attrstring(attr), \n                convert_list(val, ids, key, attr_type, item_func, cdata), \n                key\n                )\n            )\n\n        elif val is None:\n            addline(convert_none(key, val, attr_type, attr, cdata))\n\n        else:\n            raise TypeError('Unsupported data type: %s (%s)' % (\n                val, type(val).__name__)\n            )\n\n    return ''.join(output)", "response": "Converts a dict into an XML string."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts a list into an XML string.", "response": "def convert_list(items, ids, parent, attr_type, item_func, cdata):\n    \"\"\"Converts a list into an XML string.\"\"\"\n    LOG.info('Inside convert_list()')\n    output = []\n    addline = output.append\n\n    item_name = item_func(parent)\n\n    if ids:\n        this_id = get_unique_id(parent)\n\n    for i, item in enumerate(items):\n        LOG.info('Looping inside convert_list(): item=\"%s\", item_name=\"%s\", type=\"%s\"' % (\n            unicode_me(item), item_name, type(item).__name__)\n        )\n        attr = {} if not ids else { 'id': '%s_%s' % (this_id, i+1) }\n        if isinstance(item, numbers.Number) or type(item) in (str, unicode):\n            addline(convert_kv(item_name, item, attr_type, attr, cdata))\n            \n        elif hasattr(item, 'isoformat'): # datetime\n            addline(convert_kv(item_name, item.isoformat(), attr_type, attr, cdata))\n            \n        elif type(item) == bool:\n            addline(convert_bool(item_name, item, attr_type, attr, cdata))\n            \n        elif isinstance(item, dict):\n            if not attr_type:\n                addline('<%s>%s</%s>' % (\n                    item_name, \n                    convert_dict(item, ids, parent, attr_type, item_func, cdata),\n                    item_name, \n                    )\n                )\n            else:\n                addline('<%s type=\"dict\">%s</%s>' % (\n                    item_name, \n                    convert_dict(item, ids, parent, attr_type, item_func, cdata),\n                    item_name, \n                    )\n                )\n\n        elif isinstance(item, collections.Iterable):\n            if not attr_type:\n                addline('<%s %s>%s</%s>' % (\n                    item_name, make_attrstring(attr), \n                    convert_list(item, ids, item_name, attr_type, item_func, cdata),\n                    item_name, \n                    )\n                )\n            else:\n                addline('<%s type=\"list\"%s>%s</%s>' % (\n                    item_name, make_attrstring(attr), \n                    convert_list(item, ids, item_name, attr_type, item_func, cdata),\n                    item_name, \n                    )\n                )\n                \n        elif item is None:\n            addline(convert_none(item_name, None, attr_type, attr, cdata))\n            \n        else:\n            raise TypeError('Unsupported data type: %s (%s)' % (\n                item, type(item).__name__)\n            )\n    return ''.join(output)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef convert_kv(key, val, attr_type, attr={}, cdata=False):\n    LOG.info('Inside convert_kv(): key=\"%s\", val=\"%s\", type(val) is: \"%s\"' % (\n        unicode_me(key), unicode_me(val), type(val).__name__)\n    )\n\n    key, attr = make_valid_xml_name(key, attr)\n\n    if attr_type:\n        attr['type'] = get_xml_type(val)\n    attrstring = make_attrstring(attr)\n    return '<%s%s>%s</%s>' % (\n        key, attrstring, \n        wrap_cdata(val) if cdata == True else escape_xml(val),\n        key\n    )", "response": "Converts a number or string into an XML element."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting a boolean into an XML element.", "response": "def convert_bool(key, val, attr_type, attr={}, cdata=False):\n    \"\"\"Converts a boolean into an XML element\"\"\"\n    LOG.info('Inside convert_bool(): key=\"%s\", val=\"%s\", type(val) is: \"%s\"' % (\n        unicode_me(key), unicode_me(val), type(val).__name__)\n    )\n\n    key, attr = make_valid_xml_name(key, attr)\n\n    if attr_type:\n        attr['type'] = get_xml_type(val)\n    attrstring = make_attrstring(attr)\n    return '<%s%s>%s</%s>' % (key, attrstring, unicode(val).lower(), key)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert a null value into an XML element.", "response": "def convert_none(key, val, attr_type, attr={}, cdata=False):\n    \"\"\"Converts a null value into an XML element\"\"\"\n    LOG.info('Inside convert_none(): key=\"%s\"' % (unicode_me(key)))\n\n    key, attr = make_valid_xml_name(key, attr)\n\n    if attr_type:\n        attr['type'] = get_xml_type(val)\n    attrstring = make_attrstring(attr)\n    return '<%s%s></%s>' % (key, attrstring, key)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef dicttoxml(obj, root=True, custom_root='root', ids=False, attr_type=True,\n    item_func=default_item_func, cdata=False):\n    \"\"\"Converts a python object into XML.\n    Arguments:\n    - root specifies whether the output is wrapped in an XML root element\n      Default is True\n    - custom_root allows you to specify a custom root element.\n      Default is 'root'\n    - ids specifies whether elements get unique ids.\n      Default is False\n    - attr_type specifies whether elements get a data type attribute.\n      Default is True\n    - item_func specifies what function should generate the element name for\n      items in a list. \n      Default is 'item'\n    - cdata specifies whether string values should be wrapped in CDATA sections.\n      Default is False\n    \"\"\"\n    LOG.info('Inside dicttoxml(): type(obj) is: \"%s\", obj=\"%s\"' % (type(obj).__name__, unicode_me(obj)))\n    output = []\n    addline = output.append\n    if root == True:\n        addline('<?xml version=\"1.0\" encoding=\"UTF-8\" ?>')\n        addline('<%s>%s</%s>' % (\n        custom_root, \n        convert(obj, ids, attr_type, item_func, cdata, parent=custom_root), \n        custom_root,\n        )\n    )\n    else:\n        addline(convert(obj, ids, attr_type, item_func, cdata, parent=''))\n    return ''.join(output).encode('utf-8')", "response": "Converts a python object into XML."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef getCallSourceLines(callFrame, icNames, icMethod):\n    code = callFrame.f_code\n\n    # inspect.getblock(), which is called internally by inspect.getsource(),\n    # only returns the first line of <code> when <code> represents a top-level\n    # module, not the entire module's source, as needed here. The\n    #\n    #   if ismodule(object):\n    #       return lines, 0\n    #\n    # check in inspect.py doesn't account for code objects of modules, only\n    # actual module objects themselves.\n    #\n    # A workaround is to call findsource() directly on code objects of modules,\n    # which bypasses getblock().\n    #\n    # Also, the errors raised differ between Python2 and Python3 . In Python2,\n    # inspect.findsource() and inspect.getsource() raise IOErrors. In Python3,\n    # inspect.findsource() and inspect.getsource() raise OSErrors.\n    try:\n        if code.co_name == '<module>':  # Module -> use workaround above.\n            parentBlockStartLine = 1\n            lines = inspect.findsource(code)[0]  # Raises [IO/OS]Error.\n            parentBlockSource = ''.join(lines)\n        else:  # Not a module -> use inspect.getsource() normally.\n            parentBlockStartLine = code.co_firstlineno\n            parentBlockSource = inspect.getsource(code)  # Raises [IO/OS]Error.\n    except (IOError, OSError) as err:\n        if 'source code' in err.args[0]:\n            raise NoSourceAvailableError()\n        else:\n            raise\n\n    lineno = inspect.getframeinfo(callFrame)[1]\n    linenoRelativeToParent = lineno - parentBlockStartLine + 1\n\n    # There could be multiple ic() calls on the same line(s), like\n    #\n    #   ic(1); ic(2); ic(3,\n    #       4,\n    #       5); ic(6)\n    #\n    # so include all of them. Which invocation is the appropriate one will be\n    # determined later via bytecode offset calculations.\n    #\n    # TODO(grun): Support invocations of ic() where ic() is an attribute chain\n    # in the AST. For example, support\n    #\n    #   import icecream\n    #   icecream.ic()\n    #\n    # and\n    #\n    #   class Foo:\n    #       blah = ic\n    #   Foo.blah()\n    #\n    parentBlockSource = textwrap.dedent(parentBlockSource)\n    potentialCalls = [\n        node for node in ast.walk(ast.parse(parentBlockSource))\n        if isAstNodeIceCreamCall(node, icNames, icMethod) and\n        linenoRelativeToParent in getAllLineNumbersOfAstNode(node)]\n\n    if not potentialCalls:\n        # TODO(grun): Add note that to NoSourceAvailableError that this\n        # situation can occur when the underlying source changed during\n        # execution.\n        raise NoSourceAvailableError()\n\n    endLine = lineno - parentBlockStartLine + 1\n    startLine = min(call.lineno for call in potentialCalls)\n    lines = parentBlockSource.splitlines()[startLine - 1: endLine]\n\n    # inspect's lineno attribute doesn't point to the closing right parenthesis\n    # if the closing right parenthesis is on its own line without any\n    # arguments. E.g.\n    #\n    #  ic(1,\n    #     2  <--- inspect's reported lineno.\n    #     )  <--- Should be the reported lineno.\n    #\n    # Detect this situation and add the missing right parenthesis.\n    if isCallStrMissingClosingRightParenthesis('\\n'.join(lines).strip()):\n        lines.append(')')\n\n    source = stripCommentsAndNewlines('\\n'.join(lines)).strip()\n\n    absoluteStartLineNum = parentBlockStartLine + startLine - 1\n    startLineOffset = calculateLineOffsets(code)[absoluteStartLineNum]\n\n    return source, absoluteStartLineNum, startLineOffset", "response": "Returns the source code of a call frame."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef splitExpressionsOntoSeparateLines(source):\n    indices = [expr.col_offset for expr in ast.parse(source).body]\n    lines = [s.strip() for s in splitStringAtIndices(source, indices)]\n    oneExpressionPerLine = joinContinuedLines(lines)\n\n    return oneExpressionPerLine", "response": "Split every expression onto its own line so that any preceding and or trailing expressions are properly separated from ic."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsplitting the source string into lines that are needed to be extracted.", "response": "def splitCallsOntoSeparateLines(icNames, icMethod, source):\n    \"\"\"\n    To determine the bytecode offsets of ic() calls with dis.findlinestarts(),\n    every ic() invocation needs to start its own line. That is, this\n\n      foo(ic(1), ic(2))\n\n    needs to be turned into\n\n      foo(\n      ic(1),\n      ic(2))\n\n    Then the bytecode offsets of ic(1) and ic(2) can be determined with\n    dis.findlinestarts().\n\n    This split is necessary for ic() calls inside other expressions,\n    like foo(ic(1)), to be extracted correctly.\n    \"\"\"\n    callIndices = [\n        node.func.col_offset if hasattr(node, 'func') else node.col_offset\n        for node in ast.walk(ast.parse(source))\n        if isAstNodeIceCreamCall(node, icNames, icMethod)]\n    lines = splitStringAtIndices(source, callIndices)\n    sourceWithNewlinesBeforeInvocations = joinContinuedLines(lines)\n\n    return sourceWithNewlinesBeforeInvocations"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse the argument string from an AST and returns a list of strings.", "response": "def extractArgumentsFromCallStr(callStr):\n    \"\"\"\n    Parse the argument string via an AST instead of the overly simple\n    callStr.split(','). The latter incorrectly splits any string parameters\n    that contain commas therein, like ic(1, 'a,b', 2).\n    \"\"\"\n    def isTuple(ele):\n        return classname(ele) == 'Tuple'\n\n    paramsStr = callStr.split('(', 1)[-1].rsplit(')', 1)[0].strip()\n\n    root = ast.parse(paramsStr).body[0].value\n    eles = root.elts if isTuple(root) else [root]\n\n    # The ast module parses 'a, b' and '(a, b)' identically. Thus, ast.parse()\n    # alone can't tell the difference between\n    #\n    #   ic(a, b)\n    #\n    # and\n    #\n    #   ic((a, b))\n    #\n    # Detect this situation and preserve whole tuples, e.g. '(a, b)', passed to\n    # ic() by creating a new, temporary tuple around the original tuple and\n    # parsing that.\n    if paramsStr[0] == '(' and paramsStr[-1] == ')' and len(eles) > 1:\n        newTupleStr = '(' + paramsStr + \", 'ignored')\"\n        argStrs = extractArgumentsFromCallStr(newTupleStr)[:-1]\n        return argStrs\n\n    indices = [\n        max(0, e.col_offset - 1) if isTuple(e) else e.col_offset for e in eles]\n    argStrs = [s.strip(' ,') for s in splitStringAtIndices(paramsStr, indices)]\n\n    return argStrs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef init_websocket(self, event_handler, websocket_cls=Websocket):\n\t\tself.websocket = websocket_cls(self.options, self.client.token)\n\t\tloop = asyncio.get_event_loop()\n\t\tloop.run_until_complete(self.websocket.connect(event_handler))\n\t\treturn loop", "response": "This method will initialize the websocket connection to mattermost server."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlog the user in.", "response": "def login(self):\n\t\t\"\"\"\n\t\tLogs the user in.\n\n\t\tThe log in information is saved in the client\n\t\t\t- userid\n\t\t\t- username\n\t\t\t- cookies\n\n\t\t:return: The raw response from the request\n\t\t\"\"\"\n\t\tif self.options['token']:\n\t\t\tself.client.token = self.options['token']\n\t\t\tresult = self.users.get_user('me')\n\t\telse:\n\t\t\tresponse = self.users.login_user({\n\t\t\t\t'login_id': self.options['login_id'],\n\t\t\t\t'password': self.options['password'],\n\t\t\t\t'token': self.options['mfa_token']\n\t\t\t})\n\t\t\tif response.status_code == 200:\n\t\t\t\tself.client.token = response.headers['Token']\n\t\t\t\tself.client.cookies = response.cookies\n\t\t\ttry:\n\t\t\t\tresult = response.json()\n\t\t\texcept ValueError:\n\t\t\t\tlog.debug('Could not convert response to json, returning raw response')\n\t\t\t\tresult = response\n\n\t\tlog.debug(result)\n\n\t\tif 'id' in result:\n\t\t\tself.client.userid = result['id']\n\t\tif 'username' in result:\n\t\t\tself.client.username = result['username']\n\n\t\treturn result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nlogging the user out.", "response": "def logout(self):\n\t\t\"\"\"\n\t\tLog the user out.\n\n\t\t:return: The JSON response from the server\n\t\t\"\"\"\n\t\tresult = self.users.logout_user()\n\t\tself.client.token = ''\n\t\tself.client.userid = ''\n\t\tself.client.username = ''\n\t\tself.client.cookies = None\n\t\treturn result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef connect(self, event_handler):\n\t\tcontext = ssl.create_default_context(purpose=ssl.Purpose.CLIENT_AUTH)\n\t\tif not self.options['verify']:\n\t\t\tcontext.verify_mode = ssl.CERT_NONE\n\n\t\tscheme = 'wss://'\n\t\tif self.options['scheme'] != 'https':\n\t\t\tscheme = 'ws://'\n\t\t\tcontext = None\n\n\t\turl = '{scheme:s}{url:s}:{port:s}{basepath:s}/websocket'.format(\n\t\t\tscheme=scheme,\n\t\t\turl=self.options['url'],\n\t\t\tport=str(self.options['port']),\n\t\t\tbasepath=self.options['basepath']\n\t\t)\n\n\t\twebsocket = yield from websockets.connect(\n\t\t\turl,\n\t\t\tssl=context,\n\t\t)\n\n\t\tyield from self._authenticate_websocket(websocket, event_handler)\n\t\tyield from self._start_loop(websocket, event_handler)", "response": "Connect to the websocket and authenticate it."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _start_loop(self, websocket, event_handler):\n\t\tlog.debug('Starting websocket loop')\n\t\twhile True:\n\t\t\ttry:\n\t\t\t\tyield from asyncio.wait_for(\n\t\t\t\t\tself._wait_for_message(websocket, event_handler),\n\t\t\t\t\ttimeout=self.options['timeout']\n\t\t\t\t)\n\t\t\texcept asyncio.TimeoutError:\n\t\t\t\tyield from websocket.pong()\n\t\t\t\tlog.debug(\"Sending heartbeat...\")\n\t\t\t\tcontinue", "response": "Start the websocket loop."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsending a authentication challenge over a websocket. This is not needed when we just send the cookie we got on login when connecting to the websocket.", "response": "def _authenticate_websocket(self, websocket, event_handler):\n\t\t\"\"\"\n\t\tSends a authentication challenge over a websocket.\n\t\tThis is not needed when we just send the cookie we got on login\n\t\twhen connecting to the websocket.\n\t\t\"\"\"\n\t\tlog.debug('Authenticating websocket')\n\t\tjson_data = json.dumps({\n\t\t\t\"seq\": 1,\n\t\t\t\"action\": \"authentication_challenge\",\n\t\t\t\"data\": {\n\t\t\t\t\"token\": self._token\n\t\t\t}\n\t\t}).encode('utf8')\n\t\tyield from websocket.send(json_data)\n\t\twhile True:\n\t\t\tmessage = yield from websocket.recv()\n\t\t\tstatus = json.loads(message)\n\t\t\tlog.debug(status)\n\t\t\t# We want to pass the events to the event_handler already\n\t\t\t# because the hello event could arrive before the authentication ok response\n\t\t\tyield from event_handler(message)\n\t\t\tif ('status' in status and status['status'] == 'OK') and \\\n\t\t\t\t\t('seq_reply' in status and status['seq_reply'] == 1):\n\t\t\t\tlog.info('Websocket authentification OK')\n\t\t\t\treturn True\n\t\t\telif 'seq_reply' in status and status['seq_reply'] == 1:\n\t\t\t\tlog.error('Websocket authentification failed')"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nruns a notebook cell and updates the output of that cell in - place.", "response": "def run_cell(self, cell):\n        \"\"\"Run a notebook cell and update the output of that cell in-place.\"\"\"\n        logging.info('Running cell:\\n%s\\n', cell.input)\n        self.kc.execute(cell.input)\n        reply = self.kc.get_shell_msg()\n        status = reply['content']['status']\n        traceback_text = ''\n        if status == 'error':\n            traceback_text = 'Cell raised uncaught exception: \\n' + \\\n                '\\n'.join(reply['content']['traceback'])\n            logging.info(traceback_text)\n        else:\n            logging.info('Cell returned')\n\n        outs = list()\n        while True:\n            try:\n                msg = self.kc.get_iopub_msg(timeout=1)\n                if msg['msg_type'] == 'status':\n                    if msg['content']['execution_state'] == 'idle':\n                        break\n            except Empty:\n                # execution state should return to idle\n                # before the queue becomes empty,\n                # if it doesn't, something bad has happened\n                raise\n\n            content = msg['content']\n            msg_type = msg['msg_type']\n\n            # IPython 3.0.0-dev writes pyerr/pyout in the notebook format\n            # but uses error/execute_result in the message spec. This does the\n            # translation needed for tests to pass with IPython 3.0.0-dev\n            notebook3_format_conversions = {\n                'error': 'pyerr',\n                'execute_result': 'pyout'\n            }\n            msg_type = notebook3_format_conversions.get(msg_type, msg_type)\n\n            out = NotebookNode(output_type=msg_type)\n\n            if 'execution_count' in content:\n                cell['prompt_number'] = content['execution_count']\n                out.prompt_number = content['execution_count']\n\n            if msg_type in ('status', 'pyin', 'execute_input'):\n                continue\n            elif msg_type == 'stream':\n                out.stream = content['name']\n                # in msgspec 5, this is name, text\n                # in msgspec 4, this is name, data\n                if 'text' in content:\n                    out.text = content['text']\n                else:\n                    out.text = content['data']\n            elif msg_type in ('display_data', 'pyout'):\n                for mime, data in content['data'].items():\n                    try:\n                        attr = self.MIME_MAP[mime]\n                    except KeyError:\n                        raise NotImplementedError(\n                            'unhandled mime type: %s' % mime\n                        )\n\n                    # In notebook version <= 3 JSON data is stored as a string\n                    # Evaluation of IPython2's JSON gives strings directly\n                    # Therefore do not encode for IPython versions prior to 3\n                    json_encode = (\n                            IPython.version_info[0] >= 3 and\n                            mime == \"application/json\")\n\n                    data_out = data if not json_encode else json.dumps(data)\n                    setattr(out, attr, data_out)\n            elif msg_type == 'pyerr':\n                out.ename = content['ename']\n                out.evalue = content['evalue']\n                out.traceback = content['traceback']\n            elif msg_type == 'clear_output':\n                outs = list()\n                continue\n            else:\n                raise NotImplementedError(\n                    'unhandled iopub message: %s' % msg_type\n                )\n            outs.append(out)\n        cell['outputs'] = outs\n\n        if status == 'error':\n            raise NotebookError(traceback_text)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\niterate over the notebook cells containing code.", "response": "def iter_code_cells(self):\n        \"\"\"Iterate over the notebook cells containing code.\"\"\"\n        for ws in self.nb.worksheets:\n            for cell in ws.cells:\n                if cell.cell_type == 'code':\n                    yield cell"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef run_notebook(self, skip_exceptions=False, progress_callback=None):\n        for i, cell in enumerate(self.iter_code_cells()):\n            try:\n                self.run_cell(cell)\n            except NotebookError:\n                if not skip_exceptions:\n                    raise\n            if progress_callback:\n                progress_callback(i)", "response": "Runs all the notebook cells in order and update the outputs in - place."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef config(filename):\n    Config = collections.namedtuple('Config', [\n        'git',\n        'lock_file',\n        'version',\n        'name',\n        'src',\n        'dst',\n        'files',\n        'post_commands',\n    ])\n\n    return [Config(**d) for d in _get_config_generator(filename)]", "response": "Construct a list of Config objects and return a list."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_files_config(src_dir, files_list):\n    FilesConfig = collections.namedtuple('FilesConfig',\n                                         ['src', 'dst', 'post_commands'])\n\n    return [\n        FilesConfig(**d) for d in _get_files_generator(src_dir, files_list)\n    ]", "response": "Construct a list of FileConfig objects and return a list."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_config_generator(filename):\n    for d in _get_config(filename):\n        repo = d['git']\n        parsedrepo = giturlparse.parse(repo)\n        name = '{}.{}'.format(parsedrepo.owner, parsedrepo.name)\n        src_dir = os.path.join(_get_clone_dir(), name)\n        files = d.get('files')\n        post_commands = d.get('post_commands', [])\n        dst_dir = None\n        if not files:\n            dst_dir = _get_dst_dir(d['dst'])\n        yield {\n            'git': repo,\n            'lock_file': _get_lock_file(name),\n            'version': d['version'],\n            'name': name,\n            'src': src_dir,\n            'dst': dst_dir,\n            'files': _get_files_config(src_dir, files),\n            'post_commands': post_commands,\n        }", "response": "A generator which populates and return a dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses the provided YAML file and return a dict.", "response": "def _get_config(filename):\n    \"\"\"\n    Parse the provided YAML file and return a dict.\n\n    :parse filename: A string containing the path to YAML file.\n    :return: dict\n    \"\"\"\n    i = interpolation.Interpolator(interpolation.TemplateWithDefaults,\n                                   os.environ)\n\n    with open(filename, 'r') as stream:\n        try:\n            interpolated_config = i.interpolate(stream.read())\n            return yaml.safe_load(interpolated_config)\n        except yaml.parser.ParserError as e:\n            msg = 'Error parsing gilt config: {0}'.format(e)\n            raise ParseError(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_dst_dir(dst_dir):\n    wd = os.getcwd()\n    _makedirs(dst_dir)\n\n    return os.path.join(wd, dst_dir)", "response": "Returns a base directory that will be used to store the a\n    in the file system."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _makedirs(path):\n    dirname, _ = os.path.split(path)\n    try:\n        os.makedirs(dirname)\n    except OSError as exc:\n        if exc.errno == errno.EEXIST:\n            pass\n        else:\n            raise", "response": "Create a base directory of the provided path and return None."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef main(ctx, config, debug):  # pragma: no cover\n    ctx.obj = {}\n    ctx.obj['args'] = {}\n    ctx.obj['args']['debug'] = debug\n    ctx.obj['args']['config'] = config", "response": "gilt - A GIT layering tool."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef overlay(ctx):  # pragma: no cover\n    args = ctx.obj.get('args')\n    filename = args.get('config')\n    debug = args.get('debug')\n    _setup(filename)\n\n    for c in config.config(filename):\n        with fasteners.InterProcessLock(c.lock_file):\n            util.print_info('{}:'.format(c.name))\n            if not os.path.exists(c.src):\n                git.clone(c.name, c.git, c.src, debug=debug)\n            if c.dst:\n                git.extract(c.src, c.dst, c.version, debug=debug)\n                post_commands = {c.dst: c.post_commands}\n            else:\n                git.overlay(c.src, c.files, c.version, debug=debug)\n                post_commands = {\n                    conf.dst: conf.post_commands\n                    for conf in c.files\n                }\n            # Run post commands if any.\n            for dst, commands in post_commands.items():\n                for command in commands:\n                    msg = '  - running `{}` in {}'.format(command, dst)\n                    util.print_info(msg)\n                    cmd = util.build_sh_cmd(command, cwd=dst)\n                    util.run_command(cmd, debug=debug)", "response": "Overlays all the files in the current directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef clone(name, repository, destination, debug=False):\n    msg = '  - cloning {} to {}'.format(name, destination)\n    util.print_info(msg)\n    cmd = sh.git.bake('clone', repository, destination)\n    util.run_command(cmd, debug=debug)", "response": "Clone the specified repository into a temporary directory and return None."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef extract(repository, destination, version, debug=False):\n    with util.saved_cwd():\n        if os.path.isdir(destination):\n            shutil.rmtree(destination)\n\n        os.chdir(repository)\n        _get_version(version, debug)\n        cmd = sh.git.bake(\n            'checkout-index', force=True, all=True, prefix=destination)\n        util.run_command(cmd, debug=debug)\n        msg = '  - extracting ({}) {} to {}'.format(version, repository,\n                                                    destination)\n        util.print_info(msg)", "response": "Extract the specified repository and version into the given directory and return None."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\noverlay the specified files into the given repository and version into the given directory and returns None.", "response": "def overlay(repository, files, version, debug=False):\n    \"\"\"\n    Overlay files from the specified repository/version into the given\n    directory and return None.\n\n    :param repository: A string containing the path to the repository to be\n     extracted.\n    :param files: A list of `FileConfig` objects.\n    :param version: A string containing the branch/tag/sha to be exported.\n    :param debug: An optional bool to toggle debug output.\n    :return: None\n    \"\"\"\n    with util.saved_cwd():\n        os.chdir(repository)\n        _get_version(version, debug)\n\n        for fc in files:\n            if '*' in fc.src:\n                for filename in glob.glob(fc.src):\n                    util.copy(filename, fc.dst)\n                    msg = '  - copied ({}) {} to {}'.format(\n                        version, filename, fc.dst)\n                    util.print_info(msg)\n            else:\n                if os.path.isdir(fc.dst) and os.path.isdir(fc.src):\n                    shutil.rmtree(fc.dst)\n                util.copy(fc.src, fc.dst)\n                msg = '  - copied ({}) {} to {}'.format(\n                    version, fc.src, fc.dst)\n                util.print_info(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nhandle switching to the specified version and return None.", "response": "def _get_version(version, debug=False):\n    \"\"\"\n    Handle switching to the specified version and return None.\n\n    1. Fetch the origin.\n    2. Checkout the specified version.\n    3. Clean the repository before we begin.\n    4. Pull the origin when a branch; _not_ a commit id.\n\n    :param version: A string containing the branch/tag/sha to be exported.\n    :param debug: An optional bool to toggle debug output.\n    :return: None\n    \"\"\"\n    if not any(\n        (_has_branch(version, debug), _has_tag(version, debug), _has_commit(\n            version, debug))):\n        cmd = sh.git.bake('fetch')\n        util.run_command(cmd, debug=debug)\n    cmd = sh.git.bake('checkout', version)\n    util.run_command(cmd, debug=debug)\n    cmd = sh.git.bake('clean', '-d', '-x', '-f')\n    util.run_command(cmd, debug=debug)\n    if _has_branch(version, debug):\n        cmd = sh.git.bake('pull', rebase=True, ff_only=True)\n        util.run_command(cmd, debug=debug)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndetermine if a version is a local git commit sha or not.", "response": "def _has_commit(version, debug=False):\n    \"\"\"\n    Determine a version is a local git commit sha or not.\n\n    :param version: A string containing the branch/tag/sha to be determined.\n    :param debug: An optional bool to toggle debug output.\n    :return: bool\n    \"\"\"\n    if _has_tag(version, debug) or _has_branch(version, debug):\n        return False\n    cmd = sh.git.bake('cat-file', '-e', version)\n    try:\n        util.run_command(cmd, debug=debug)\n        return True\n    except sh.ErrorReturnCode:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndetermining a version is a local git tag name or not.", "response": "def _has_tag(version, debug=False):\n    \"\"\"\n    Determine a version is a local git tag name or not.\n\n    :param version: A string containing the branch/tag/sha to be determined.\n    :param debug: An optional bool to toggle debug output.\n    :return: bool\n    \"\"\"\n    cmd = sh.git.bake('show-ref', '--verify', '--quiet',\n                      \"refs/tags/{}\".format(version))\n    try:\n        util.run_command(cmd, debug=debug)\n        return True\n    except sh.ErrorReturnCode:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nexecute the given command and return None.", "response": "def run_command(cmd, debug=False):\n    \"\"\"\n    Execute the given command and return None.\n\n    :param cmd: A `sh.Command` object to execute.\n    :param debug: An optional bool to toggle debug output.\n    :return: None\n    \"\"\"\n    if debug:\n        msg = '  PWD: {}'.format(os.getcwd())\n        print_warn(msg)\n        msg = '  COMMAND: {}'.format(cmd)\n        print_warn(msg)\n    cmd()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef build_sh_cmd(cmd, cwd=None):\n    args = cmd.split()\n    return getattr(sh, args[0]).bake(_cwd=cwd, *args[1:])", "response": "Build a sh. Command from a string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef copy(src, dst):\n    try:\n        shutil.copytree(src, dst)\n    except OSError as exc:\n        if exc.errno == errno.ENOTDIR:\n            shutil.copy(src, dst)\n        else:\n            raise", "response": "Copy a file or directory to another directory."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning all the tables in this table as a DataFrame. This is a simple query that returns all the tables in this table.", "response": "def all(self):\n        \"\"\"\n        Returns entire table as a DataFrame. This is executing:\n            SELECT\n                *\n            FROM\n                <name_of_the_table>\n\n        Examples\n        --------\n        >>> from db import DemoDB\n        >>> db = DemoDB()\n        >>> len(db.tables.Track.all())\n        3503\n        >>> df = db.tables.Track.all()\n        \"\"\"\n\n        q = self._query_templates['table']['all'].format(schema=self.schema,\n                                                         table=self.name)\n        return pd.read_sql(q, self._con)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns all unique values in a DataFrame.", "response": "def unique(self, *args):\n        \"\"\"\n        Returns all unique values as a DataFrame. This is executing:\n            SELECT\n                DISTINCT\n                    <name_of_the_column_1>\n                    , <name_of_the_column_2>\n                    , <name_of_the_column_3>\n                    ...\n            FROM\n                <name_of_the_table>\n\n        Parameters\n        ----------\n        *args: columns as strings\n\n        Examples\n        --------\n        >>> from db import DemoDB\n        >>> db = DemoDB()\n        >>> db.tables.Track.unique(\"GenreId\")\n            GenreId\n        0         1\n        1         2\n        2         3\n        3         4\n        4         5\n        5         6\n        6         7\n        7         8\n        8         9\n        9        10\n        10       11\n        11       12\n        12       13\n        13       14\n        14       15\n        15       16\n        16       17\n        17       18\n        18       19\n        19       20\n        20       21\n        21       22\n        22       23\n        23       24\n        24       25\n        >>> len(db.tables.Track.unique(\"GenreId\", \"MediaTypeId\"))\n        38\n        \"\"\"\n        q = self._query_templates['table']['unique'].format(columns=self._format_columns(args), schema=self.schema,\n                                                            table=self.name)\n\n        return pd.read_sql(q, self._con)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_dict(self):\n        return {'schema': self.schema, 'name': self.name, 'columns': [col.to_dict() for col in self._columns],\n                'foreign_keys': self.foreign_keys.to_dict(), 'ref_keys': self.ref_keys.to_dict()}", "response": "Serialize representation of the table for local caching."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef list_profiles():\n    profiles = {}\n    user = os.path.expanduser(\"~\")\n    for f in os.listdir(user):\n        if f.startswith(\".db.py_\"):\n            profile = load_from_json(os.path.join(user, f))\n            tables = profile.pop('tables', None)\n            if tables:\n                profile['metadata'] = True\n            else:\n                profile['metadata'] = False\n            profiles[f[7:]] = profile\n    return profiles", "response": "Lists all of the database profiles available by the user"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef remove_profile(name, s3=False):\n    user = os.path.expanduser(\"~\")\n    if s3:\n        f = os.path.join(user, S3_PROFILE_ID + name)\n    else:\n        f = os.path.join(user, DBPY_PROFILE_ID + name)\n    try:\n        try:\n            open(f)\n        except:\n            raise Exception(\"Profile '{0}' does not exist. Could not find file {1}\".format(name, f))\n        os.remove(f)\n    except Exception as e:\n        raise Exception(\"Could not remove profile {0}! Excpetion: {1}\".format(name, e))", "response": "Removes a profile from your config file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef DemoDB(keys_per_column=None, **kwargs):\n    _ROOT = os.path.abspath(os.path.dirname(__file__))\n    chinook = os.path.join(_ROOT, 'data', 'chinook.sqlite')\n    return DB(filename=chinook, dbtype='sqlite', keys_per_column=keys_per_column, **kwargs)", "response": "Provides an instance of DB that hooks up to the Chinook DB\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef tables(self):\n        if len(self._tables) == 0:\n            self.refresh_schema(self._exclude_system_tables, self._use_cache)\n        return self._tables", "response": "A lazy loaded reference to the table metadata for the DB."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load_credentials(self, profile=\"default\"):\n        f = profile_path(DBPY_PROFILE_ID, profile)\n        if f:\n            creds = load_from_json(f)\n            self.username = creds.get('username')\n            self.password = creds.get('password')\n            self.hostname = creds.get('hostname')\n            self.port = creds.get('port')\n            self.filename = creds.get('filename')\n            self.dbname = creds.get('dbname')\n            self.dbtype = creds.get('dbtype')\n            self.schemas = creds.get('schemas')\n            self.limit = creds.get('limit')\n            self.keys_per_column = creds.get('keys_per_column')\n        else:\n            raise Exception(\"Credentials not configured!\")", "response": "Loads credentials for a given profile."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef save_credentials(self, profile=\"default\"):\n        f = profile_path(DBPY_PROFILE_ID, profile)\n        dump_to_json(f, self.credentials)", "response": "Save your database credentials so you don t have to save them in script."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef save_metadata(self, profile=\"default\"):\n        if len(self.tables) > 0:\n            f = profile_path(DBPY_PROFILE_ID, profile)\n            dump_to_json(f, self.to_dict())", "response": "Save the database credentials plus the database properties to your db. py profile."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef find_table(self, search):\n        tables = []\n        for table in self.tables:\n            if glob.fnmatch.fnmatch(table.name, search):\n                tables.append(table)\n        return TableSet(tables)", "response": "Returns a TableSet containing all tables that match the search pattern."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_column(self, search, data_type=None):\n        if isinstance(data_type, str):\n            data_type = [data_type]\n        cols = []\n        for table in self.tables:\n            for col in vars(table):\n                if glob.fnmatch.fnmatch(col, search):\n                    if data_type and isinstance(getattr(table, col), Column) and getattr(table, col).type not in data_type:\n                        continue\n                    if isinstance(getattr(table, col), Column):\n                        cols.append(getattr(table, col))\n        return ColumnSet(cols)", "response": "This method searches through the database and returns all the columns that match the search pattern."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nexecutes a query and return the record for the current time.", "response": "def query(self, q, data=None, union=True, limit=None):\n        \"\"\"\n        Query your database with a raw string.\n\n        Parameters\n        ----------\n        q: str\n            Query string to execute\n        data: list, dict\n            Optional argument for handlebars-queries. Data will be passed to the\n            template and rendered using handlebars.\n        union: bool\n            Whether or not \"UNION ALL\" handlebars templates. This will return\n            any handlebars queries as a single data frame.\n        limit: int\n            Number of records to return\n\n        Examples\n        --------\n        >>> from db import DemoDB\n        >>> db = DemoDB()\n\n        db.query(\"select * from Track\").head(2)\n           TrackId                                     Name  AlbumId  MediaTypeId  \\\\\\r\n        0        1  For Those About To Rock (We Salute You)        1            1\n        1        2                        Balls to the Wall        2            2\n        <BLANKLINE>\n           GenreId                                   Composer  Milliseconds     Bytes  \\\\\\r\n        0        1  Angus Young, Malcolm Young, Brian Johnson        343719  11170334\n        1        1                                       None        342562   5510424\n        <BLANKLINE>\n           UnitPrice\n        0       0.99\n        1       0.99\n\n        db.query(\"select * from Track\", limit=10)\n           TrackId                                     Name  AlbumId  MediaTypeId  \\\n        0        1  For Those About To Rock (We Salute You)        1            1\n        1        2                        Balls to the Wall        2            2\n        2        3                          Fast As a Shark        3            2\n        3        4                        Restless and Wild        3            2\n        4        5                     Princess of the Dawn        3            2\n        5        6                    Put The Finger On You        1            1\n        6        7                          Let's Get It Up        1            1\n        7        8                         Inject The Venom        1            1\n        8        9                               Snowballed        1            1\n        9       10                               Evil Walks        1            1\n\n           GenreId                                           Composer  Milliseconds  \\\n        0        1          Angus Young, Malcolm Young, Brian Johnson        343719\n        1        1                                               None        342562\n        2        1  F. Baltes, S. Kaufman, U. Dirkscneider & W. Ho...        230619\n        3        1  F. Baltes, R.A. Smith-Diesel, S. Kaufman, U. D...        252051\n        4        1                         Deaffy & R.A. Smith-Diesel        375418\n        5        1          Angus Young, Malcolm Young, Brian Johnson        205662\n        6        1          Angus Young, Malcolm Young, Brian Johnson        233926\n        7        1          Angus Young, Malcolm Young, Brian Johnson        210834\n        8        1          Angus Young, Malcolm Young, Brian Johnson        203102\n        9        1          Angus Young, Malcolm Young, Brian Johnson        263497\n\n              Bytes  UnitPrice\n        0  11170334       0.99\n        1   5510424       0.99\n        2   3990994       0.99\n        3   4331779       0.99\n        4   6290521       0.99\n        5   6713451       0.99\n        6   7636561       0.99\n        7   6852860       0.99\n        8   6599424       0.99\n        9   8611245       0.99\n        >>> q = '''\n        ... SELECT\n        ...   a.Title,\n        ...   t.Name,\n        ...   t.UnitPrice\n        ... FROM\n        ...   Album a\n        ... INNER JOIN\n        ...   Track t\n        ...     on a.AlbumId = t.AlbumId;\n        ... '''\n        >>> len(db.query(q))\n        3503\n\n        db.query(q, limit=10)\n                                           Title  \\\n        0  For Those About To Rock We Salute You\n        1                      Balls to the Wall\n        2                      Restless and Wild\n        3                      Restless and Wild\n        4                      Restless and Wild\n        5  For Those About To Rock We Salute You\n        6  For Those About To Rock We Salute You\n        7  For Those About To Rock We Salute You\n        8  For Those About To Rock We Salute You\n        9  For Those About To Rock We Salute You\n\n                                              Name  UnitPrice\n        0  For Those About To Rock (We Salute You)       0.99\n        1                        Balls to the Wall       0.99\n        2                          Fast As a Shark       0.99\n        3                        Restless and Wild       0.99\n        4                     Princess of the Dawn       0.99\n        5                    Put The Finger On You       0.99\n        6                          Let's Get It Up       0.99\n        7                         Inject The Venom       0.99\n        8                               Snowballed       0.99\n        9                               Evil Walks       0.99\n\n        >>> template = '''\n        ...    SELECT\n        ...    '{{ name }}' as table_name,\n        ...    COUNT(*) as cnt\n        ... FROM\n        ...     {{ name }}\n        ... GROUP BY\n        ...     table_name\n        ... '''\n        >>> data = [\n        ...    {\"name\": \"Album\"},\n        ...    {\"name\": \"Artist\"},\n        ...    {\"name\": \"Track\"}\n        ... ]\n        >>>\n\n        db.query(q, data=data)\n          table_name   cnt\n        0      Album   347\n        1     Artist   275\n        2      Track  3503\n\n        >>> q = '''\n        ... SELECT\n        ... {{#cols}}\n        ...    {{#if @last}}\n        ...        {{ . }}\n        ...    {{else}}\n        ...        {{ . }} ,\n        ...    {{/if}}\n        ... {{/cols}}\n        ... FROM\n        ...    Album;\n        ... '''\n        >>> data = {\"cols\": [\"AlbumId\", \"Title\", \"ArtistId\"]}\n        >>> len(db.query(q, data=data, union=False))\n        347\n\n        db.query(q, data=data, union=False)\n           AlbumId                                  Title  ArtistId\n        0        1  For Those About To Rock We Salute You         1\n        1        2                      Balls to the Wall         2\n        2        3                      Restless and Wild         2\n        3        4                      Let There Be Rock         1\n        4        5                               Big Ones         3\n\n    \"\"\"\n        if data:\n            q = self._apply_handlebars(q, data, union)\n        if limit:\n            q = self._assign_limit(q, limit)\n        return pd.read_sql(q, self.con)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef query_from_file(self, filename, data=None, union=True, limit=None):\n        with open(filename) as fp:\n            q = fp.read()\n\n        return self.query(q, data=data, union=union, limit=limit)", "response": "Query your database from a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _create_sqlite_metatable(self):\n        sys.stderr.write(\"Indexing schema. This will take a second...\")\n        rows_to_insert = []\n        tables = [row[0] for row in self.cur.execute(\"select name from sqlite_master where type='table';\")]\n        for table in tables:\n            for row in self.cur.execute(\"pragma table_info('{0}')\".format(table)):\n                rows_to_insert.append((table, row[1], row[2]))\n        # find for table and column names\n        self.cur.execute(\"drop table if exists tmp_dbpy_schema;\")\n        self.cur.execute(\"create temp table tmp_dbpy_schema(table_name varchar, column_name varchar, data_type varchar);\")\n        for row in rows_to_insert:\n            self.cur.execute(\"insert into tmp_dbpy_schema(table_name, column_name, data_type) values('{0}', '{1}', '{2}');\".format(*row))\n        self.cur.execute(\"SELECT name, sql  FROM sqlite_master where sql like '%REFERENCES%';\")\n\n        # find for foreign keys\n        self.cur.execute(\"drop table if exists tmp_dbpy_foreign_keys;\")\n        self.cur.execute(\"create temp table tmp_dbpy_foreign_keys(table_name varchar, column_name varchar, foreign_table varchar, foreign_column varchar);\")\n        foreign_keys = []\n        self.cur.execute(\"SELECT name, sql  FROM sqlite_master ;\")\n        for (table_name, sql) in self.cur:\n            rgx = \"FOREIGN KEY \\(\\[(.*)\\]\\) REFERENCES \\[(.*)\\] \\(\\[(.*)\\]\\)\"\n            if sql is None:\n                continue\n            for (column_name, foreign_table, foreign_key) in re.findall(rgx, sql):\n                foreign_keys.append((table_name, column_name, foreign_table, foreign_key))\n        for row in foreign_keys:\n            sql_insert = \"insert into tmp_dbpy_foreign_keys(table_name, column_name, foreign_table, foreign_column) values('{0}', '{1}', '{2}', '{3}');\"\n            self.cur.execute(sql_insert.format(*row))\n\n        self.con.commit()\n        sys.stderr.write(\"finished!\\n\")", "response": "Create the sqlite metatable tables."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrefreshing the database s schema and returns a TableSet containing all tables and their related keys.", "response": "def refresh_schema(self, exclude_system_tables=True, use_cache=False):\n        \"\"\"\n        Pulls your database's schema again and looks for any new tables and\n        columns.\n        \"\"\"\n\n        col_meta, table_meta = self._get_db_metadata(exclude_system_tables, use_cache)\n        tables = self._gen_tables_from_col_tuples(col_meta)\n\n        # Three modes for refreshing schema\n        # 1. load directly from cache\n        # 2. use a single query for getting all key relationships\n        # 3. use the naive approach\n        if use_cache:\n            # generate our Tables, and load them into a TableSet\n            self._tables = TableSet([Table(self.con, self._query_templates, table_meta[t]['schema'], t, tables[t],\n                                           keys_per_column=self.keys_per_column,\n                                           foreign_keys=table_meta[t]['foreign_keys']['columns'],\n                                           ref_keys=table_meta[t]['ref_keys']['columns'])\n                                     for t in sorted(tables.keys())])\n\n        # optimize the foreign/ref key query by doing it one time, database-wide, if query is available\n        elif not use_cache and isinstance(self._query_templates.get('system', {}).get('foreign_keys_for_db', None), str):\n\n            self.cur.execute(self._query_templates['system']['foreign_keys_for_db'])\n            table_db_foreign_keys = defaultdict(list)\n            for rel in self.cur:\n                # second value in relationship tuple is the table name\n                table_db_foreign_keys[rel[1]].append(rel)\n\n            self.cur.execute(self._query_templates['system']['ref_keys_for_db'])\n            table_db_ref_keys = defaultdict(list)\n            for rel in self.cur:\n                # second value in relationship tuple is the table name\n                table_db_ref_keys[rel[1]].append(rel)\n\n            # generate our Tables, and load them into a TableSet\n            self._tables = TableSet([Table(self.con, self._query_templates, tables[t][0].schema, t, tables[t],\n                                           keys_per_column=self.keys_per_column,\n                                           foreign_keys=table_db_foreign_keys[t], ref_keys=table_db_ref_keys[t])\n                                     for t in sorted(tables.keys())])\n        elif not use_cache:\n            self._tables = TableSet([Table(self.con, self._query_templates, tables[t][0].schema, t, tables[t],\n                                           keys_per_column=self.keys_per_column) for t in sorted(tables.keys())])\n\n        sys.stderr.write(\"done!\\n\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nuploading a dataframe to redshift via s3.", "response": "def to_redshift(self, name, df, drop_if_exists=False, chunk_size=10000,\n                    AWS_ACCESS_KEY=None, AWS_SECRET_KEY=None, s3=None,\n                    print_sql=False, bucket_location=None, s3_bucket=None):\n        \"\"\"\n        Upload a dataframe to redshift via s3.\n\n        Parameters\n        ----------\n        name: str\n            name for your shiny new table\n        df: DataFrame\n            data frame you want to save to the db\n        drop_if_exists: bool (False)\n            whether you'd like to drop the table if it already exists\n        chunk_size: int (10000)\n            Number of DataFrame chunks to upload and COPY from S3. Upload speed\n            is *much* faster if chunks = multiple-of-slices. Ex: DW1.XL nodes\n            have 2 slices per node, so if running 2 nodes you will want\n            chunk_size=4, 8, etc\n        AWS_ACCESS_KEY: str\n            your aws access key. if this is None, the function will try\n            and grab AWS_ACCESS_KEY from your environment variables\n        AWS_SECRET_KEY: str\n            your aws secrety key. if this is None, the function will try\n            and grab AWS_SECRET_KEY from your environment variables\n        s3: S3\n            alternative to using keys, you can use an S3 object\n        print_sql: bool (False)\n            option for printing sql statement that will be executed\n        bucket_location: boto.s3.connection.Location\n            a specific AWS location in which to create the temporary transfer s3\n            bucket. This should match your redshift cluster's region.\n\n        Examples\n        --------\n        \"\"\"\n        if self.dbtype!=\"redshift\":\n            raise Exception(\"Sorry, feature only available for redshift.\")\n        try:\n            from boto.s3.connection import S3Connection\n            from boto.s3.key import Key\n            from boto.s3.connection import Location\n\n            # if boto is present, set the bucket_location to default.\n            # we can't do this in the function definition because we're\n            # lazily importing boto only if necessary here.\n            if bucket_location is None:\n                bucket_location = Location.DEFAULT\n\n        except ImportError:\n            raise Exception(\"Couldn't find boto library. Please ensure it is installed\")\n\n        if s3 is not None:\n            AWS_ACCESS_KEY = s3.access_key\n            AWS_SECRET_KEY = s3.secret_key\n        if AWS_ACCESS_KEY is None:\n            AWS_ACCESS_KEY = os.environ.get('AWS_ACCESS_KEY')\n        if AWS_SECRET_KEY is None:\n            AWS_SECRET_KEY = os.environ.get('AWS_SECRET_KEY')\n        if AWS_ACCESS_KEY is None:\n            raise Exception(\"Must specify AWS_ACCESS_KEY as either function argument or as an environment variable `AWS_ACCESS_KEY`\")\n        if AWS_SECRET_KEY is None:\n            raise Exception(\"Must specify AWS_SECRET_KEY as either function argument or as an environment variable `AWS_SECRET_KEY`\")\n\n        conn = S3Connection(AWS_ACCESS_KEY, AWS_SECRET_KEY)\n        #this way users with permission on specific buckets can use this feature\n        bucket_name = \"dbpy-{0}\".format(uuid.uuid4())\n        if s3_bucket:\n            bucket = conn.get_bucket(s3_bucket)\n            bucket_name = s3_bucket\n        else:\n            bucket = conn.create_bucket(bucket_name, location=bucket_location)\n        # we're going to chunk the file into pieces. according to amazon, this is\n        # much faster when it comes time to run the \\COPY statment.\n        #\n        # see http://docs.aws.amazon.com/redshift/latest/dg/t_splitting-data-files.html\n        sys.stderr.write(\"Transfering {0} to s3 in chunks\".format(name))\n        len_df = len(df)\n        chunks = range(0, len_df, chunk_size)\n\n        def upload_chunk(i):\n            conn = S3Connection(AWS_ACCESS_KEY, AWS_SECRET_KEY)\n            chunk = df[i:(i+chunk_size)]\n            k = Key(bucket)\n            k.key = 'data-%d-%d.csv.gz' % (i, i + chunk_size)\n            k.set_metadata('parent', 'db.py')\n            out = StringIO()\n            with gzip.GzipFile(fileobj=out, mode=\"w\") as f:\n                  f.write(chunk.to_csv(index=False, encoding='utf-8'))\n            k.set_contents_from_string(out.getvalue())\n            sys.stderr.write(\".\")\n            return i\n\n        threads = []\n        for i in chunks:\n            t = threading.Thread(target=upload_chunk, args=(i, ))\n            t.start()\n            threads.append(t)\n\n        # join all threads\n        for t in threads:\n            t.join()\n        sys.stderr.write(\"done\\n\")\n\n        if drop_if_exists:\n            sql = \"DROP TABLE IF EXISTS {0};\".format(name)\n            if print_sql:\n                sys.stderr.write(sql + \"\\n\")\n            self._try_command(sql)\n\n        # generate schema from pandas and then adapt for redshift\n        sql = pd.io.sql.get_schema(df, name)\n        # defaults to using SQLite format. need to convert it to Postgres\n        sql = sql.replace(\"[\", \"\").replace(\"]\", \"\")\n        # we'll create the table ONLY if it doens't exist\n        sql = sql.replace(\"CREATE TABLE\", \"CREATE TABLE IF NOT EXISTS\")\n        if print_sql:\n            sys.stderr.write(sql + \"\\n\")\n        self._try_command(sql)\n        self.con.commit()\n\n        # perform the \\COPY here. the s3 argument is a prefix, so it'll pick up\n        # all of the data*.gz files we've created\n        sys.stderr.write(\"Copying data from s3 to redshfit...\")\n        sql = \"\"\"\n        copy {name} from 's3://{bucket_name}/data'\n        credentials 'aws_access_key_id={AWS_ACCESS_KEY};aws_secret_access_key={AWS_SECRET_KEY}'\n        CSV IGNOREHEADER as 1 GZIP;\n        \"\"\".format(name=name, bucket_name=bucket_name,\n                   AWS_ACCESS_KEY=AWS_ACCESS_KEY, AWS_SECRET_KEY=AWS_SECRET_KEY)\n        if print_sql:\n            sys.stderr.write(sql + \"\\n\")\n        self._try_command(sql)\n        self.con.commit()\n        sys.stderr.write(\"done!\\n\")\n        # tear down the bucket\n        sys.stderr.write(\"Tearing down bucket...\")\n        for key in bucket.list():\n            key.delete()\n        if not s3_bucket:\n            conn.delete_bucket(bucket_name)\n        sys.stderr.write(\"done!\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_dict(self):\n        db_dict = self.credentials\n        db_dict.update(self.tables.to_dict())\n        return db_dict", "response": "Dict representation of the database as credentials plus tables dict representation."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef profile_path(profile_id, profile):\n    user = os.path.expanduser(\"~\")\n    return os.path.join(user, profile_id + profile)", "response": "Create full path to given provide for the current user."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_from_json(file_path):\n    if os.path.exists(file_path):\n        raw_data = open(file_path, 'rb').read()\n        return json.loads(base64.decodestring(raw_data).decode('utf-8'))", "response": "Load the stored data from json and return as a dict"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef save_credentials(self, profile):\n        filename = profile_path(S3_PROFILE_ID, profile)\n        creds = {\n            \"access_key\": self.access_key,\n            \"secret_key\": self.secret_key\n        }\n        dump_to_json(filename, creds)", "response": "Save credentials to a dotfile so you can open them grab them later."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_credentials(self, profile):\n        f = profile_path(S3_PROFILE_ID, profile)\n        if os.path.exists(f):\n            creds = load_profile(f)\n            if 'access_key' not in creds:\n                raise Exception(\"`access_key` not found in s3 profile '{0}'\".format(profile))\n            self.access_key = creds['access_key']\n            if 'access_key' not in creds:\n                raise Exception(\"`secret_key` not found in s3 profile '{0}'\".format(profile))\n            self.secret_key = creds['secret_key']", "response": "Loads credentials for a given profile."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the first n values of your column as a DataFrame. This is a simple query that returns the first n values of your column as a DataFrame. This is a simple query that returns the first n values of your column.", "response": "def head(self, n=6):\n        \"\"\"\n        Returns first n values of your column as a DataFrame. This is executing:\n            SELECT\n                <name_of_the_column>\n            FROM\n                <name_of_the_table>\n            LIMIT <n>\n\n        Parameters\n        ----------\n        n: int\n            number of rows to return\n\n        Examples\n        --------\n        >>> from db import DemoDB\n        >>> db = DemoDB()\n        >>> db.tables.Customer.City.head()\n        0    Sao Jose dos Campos\n        1              Stuttgart\n        2               Montreal\n        3                   Oslo\n        4                 Prague\n        5                 Prague\n        Name: City, dtype: object\n        >>> db.tables.Customer.City.head(2)\n        0    Sao Jose dos Campos\n        1              Stuttgart\n        Name: City, dtype: object\n        \"\"\"\n        q = self._query_templates['column']['head'].format(column=self.name, schema=self.schema,\n                                                           table=self.table, n=n)\n        return pd.read_sql(q, self._con)[self.name]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nserialize representation of the column for local caching.", "response": "def to_dict(self):\n        \"\"\"\n        Serialize representation of the column for local caching.\n        \"\"\"\n        return {'schema': self.schema, 'table': self.table, 'name': self.name, 'type': self.type}"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef rate_limit(self, rate_limit):\n        self._rate_limit = bool(rate_limit)\n        self._rate_limit_last_call = None\n        self.clear_memoized()", "response": "Turn on or off rate limiting"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef timeout(self, timeout):\n\n        if timeout is None:\n            self._timeout = None  # no timeout\n            return\n        self._timeout = float(timeout)", "response": "Set the timeout in seconds or fractions of a second"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef language(self, lang):\n        lang = lang.lower()\n        if self._lang == lang:\n            return\n\n        url = self._api_url\n        tmp = url.replace(\"/{0}.\".format(self._lang), \"/{0}.\".format(lang))\n\n        self._api_url = tmp\n        self._lang = lang\n        self.clear_memoized()", "response": "Sets the language to use ; attempts to change the API URL"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef refresh_interval(self, refresh_interval):\n        if isinstance(refresh_interval, int) and refresh_interval > 0:\n            self._refresh_interval = refresh_interval\n        else:\n            self._refresh_interval = None", "response": "Set the new refresh interval"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef login(self, username, password, strict=True):\n        # get login token\n        params = {\n            \"action\": \"query\",\n            \"meta\": \"tokens\",\n            \"type\": \"login\",\n            \"format\": \"json\",\n        }\n        token_res = self._get_response(params)\n        if \"query\" in token_res and \"tokens\" in token_res[\"query\"]:\n            token = token_res[\"query\"][\"tokens\"][\"logintoken\"]\n\n        params = {\n            \"action\": \"login\",\n            \"lgname\": username,\n            \"lgpassword\": password,\n            \"lgtoken\": token,\n            \"format\": \"json\",\n        }\n\n        res = self._post_response(params)\n        if res[\"login\"][\"result\"] == \"Success\":\n            self._is_logged_in = True\n            return True\n        self._is_logged_in = False\n        reason = res[\"login\"][\"reason\"]\n        if strict:\n            msg = \"MediaWiki login failure: {}\".format(reason)\n            raise MediaWikiLoginError(msg)\n        return False", "response": "Login as specified user with the specified password."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_api_url(self, api_url=\"https://{lang}.wikipedia.org/w/api.php\", lang=\"en\"):\n        old_api_url = self._api_url\n        old_lang = self._lang\n        self._lang = lang.lower()\n        self._api_url = api_url.format(lang=self._lang)\n        try:\n            self._get_site_info()\n            self.__supported_languages = None  # reset this\n        except MediaWikiException:\n            # reset api url and lang in the event that the exception was caught\n            self._api_url = old_api_url\n            self._lang = old_lang\n            raise MediaWikiAPIURLError(api_url)\n        self.clear_memoized()", "response": "Sets the API URL and language of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nresetting the session information", "response": "def _reset_session(self):\n        \"\"\" Set session information \"\"\"\n        headers = {\"User-Agent\": self._user_agent}\n        self._session = requests.Session()\n        self._session.headers.update(headers)\n        self._is_logged_in = False"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef supported_languages(self):\n        if self.__supported_languages is None:\n            res = self.wiki_request({\"meta\": \"siteinfo\", \"siprop\": \"languages\"})\n            tmp = res[\"query\"][\"languages\"]\n            supported = {lang[\"code\"]: lang[\"*\"] for lang in tmp}\n            self.__supported_languages = supported\n        return self.__supported_languages", "response": "Returns a dict of all supported language prefixes on the MediaWiki site."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef random(self, pages=1):\n        if pages is None or pages < 1:\n            raise ValueError(\"Number of pages must be greater than 0\")\n\n        query_params = {\"list\": \"random\", \"rnnamespace\": 0, \"rnlimit\": pages}\n\n        request = self.wiki_request(query_params)\n        titles = [page[\"title\"] for page in request[\"query\"][\"random\"]]\n\n        if len(titles) == 1:\n            return titles[0]\n        return titles", "response": "Request a random page title or a list of random titles"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef search(self, query, results=10, suggestion=False):\n\n        self._check_query(query, \"Query must be specified\")\n\n        search_params = {\n            \"list\": \"search\",\n            \"srprop\": \"\",\n            \"srlimit\": results,\n            \"srsearch\": query,\n        }\n        if suggestion:\n            search_params[\"srinfo\"] = \"suggestion\"\n\n        raw_results = self.wiki_request(search_params)\n\n        self._check_error_response(raw_results, query)\n\n        search_results = [d[\"title\"] for d in raw_results[\"query\"][\"search\"]]\n\n        if suggestion:\n            sug = None\n            if raw_results[\"query\"].get(\"searchinfo\"):\n                sug = raw_results[\"query\"][\"searchinfo\"][\"suggestion\"]\n            return search_results, sug\n        return search_results", "response": "Search for similar titles in the page title tree."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef suggest(self, query):\n        res, suggest = self.search(query, results=1, suggestion=True)\n        try:\n            title = suggest or res[0]\n        except IndexError:  # page doesn't exist\n            title = None\n        return title", "response": "Gather suggestions based on the provided query and return the first suggestion found if no suggestions found"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsearches for pages that relate to the specified geocoords or near .", "response": "def geosearch(\n        self,\n        latitude=None,\n        longitude=None,\n        radius=1000,\n        title=None,\n        auto_suggest=True,\n        results=10,\n    ):\n        \"\"\" Search for pages that relate to the provided geocoords or near\n            the page\n\n            Args:\n                latitude (Decimal or None): Latitude geocoord; must be \\\n                                            coercable to decimal\n                longitude (Decimal or None): Longitude geocoord; must be \\\n                                             coercable to decimal\n                radius (int): Radius around page or geocoords to pull back; \\\n                              in meters\n                title (str): Page title to use as a geocoordinate; this has \\\n                             precedence over lat/long\n                auto_suggest (bool): Auto-suggest the page title\n                results (int): Number of pages within the radius to return\n            Returns:\n                list: A listing of page titles\n            Raises:\n                ValueError: If either the passed latitutde or longitude are \\\n                            not coercable to a Decimal \"\"\"\n\n        def test_lat_long(val):\n            \"\"\" handle testing lat and long \"\"\"\n            if not isinstance(val, Decimal):\n                error = (\n                    \"Latitude and Longitude must be specified either as \"\n                    \"a Decimal or in formats that can be coerced into \"\n                    \"a Decimal.\"\n                )\n                try:\n                    return Decimal(val)\n                except (DecimalException, TypeError):\n                    raise ValueError(error)\n            return val\n\n        # end local function\n\n        params = {\"list\": \"geosearch\", \"gsradius\": radius, \"gslimit\": results}\n        if title is not None:\n            if auto_suggest:\n                title = self.suggest(title)\n            params[\"gspage\"] = title\n        else:\n            lat = test_lat_long(latitude)\n            lon = test_lat_long(longitude)\n            params[\"gscoord\"] = \"{0}|{1}\".format(lat, lon)\n\n        raw_results = self.wiki_request(params)\n\n        self._check_error_response(raw_results, title)\n\n        return [d[\"title\"] for d in raw_results[\"query\"][\"geosearch\"]]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nexecutes a MediaWiki opensearch request and return a list of results that are stored in a tuple.", "response": "def opensearch(self, query, results=10, redirect=True):\n        \"\"\" Execute a MediaWiki opensearch request, similar to search box\n            suggestions and conforming to the OpenSearch specification\n\n            Args:\n                query (str): Title to search for\n                results (int): Number of pages within the radius to return\n                redirect (bool): If **False** return the redirect itself, \\\n                                 otherwise resolve redirects\n            Returns:\n                List: List of results that are stored in a tuple \\\n                      (Title, Summary, URL) \"\"\"\n\n        self._check_query(query, \"Query must be specified\")\n\n        query_params = {\n            \"action\": \"opensearch\",\n            \"search\": query,\n            \"limit\": (100 if results > 100 else results),\n            \"redirects\": (\"resolve\" if redirect else \"return\"),\n            \"warningsaserror\": True,\n            \"namespace\": \"\",\n        }\n\n        results = self.wiki_request(query_params)\n\n        self._check_error_response(results, query)\n\n        res = list()\n        for i, item in enumerate(results[1]):\n            res.append((item, results[2][i], results[3][i]))\n\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nperforming a prefix search using the provided prefix string.", "response": "def prefixsearch(self, prefix, results=10):\n        \"\"\" Perform a prefix search using the provided prefix string\n\n            Args:\n                prefix (str): Prefix string to use for search\n                results (int): Number of pages with the prefix to return\n            Returns:\n                list: List of page titles\n            Note:\n                **Per the documentation:** \"The purpose of this module is \\\n                similar to action=opensearch: to take user input and provide \\\n                the best-matching titles. Depending on the search engine \\\n                backend, this might include typo correction, redirect \\\n                avoidance, or other heuristics.\" \"\"\"\n\n        self._check_query(prefix, \"Prefix must be specified\")\n\n        query_params = {\n            \"list\": \"prefixsearch\",\n            \"pssearch\": prefix,\n            \"pslimit\": (\"max\" if results > 500 else results),\n            \"psnamespace\": 0,\n            \"psoffset\": 0,  # parameterize to skip to later in the list?\n        }\n\n        raw_results = self.wiki_request(query_params)\n\n        self._check_error_response(raw_results, prefix)\n\n        return [rec[\"title\"] for rec in raw_results[\"query\"][\"prefixsearch\"]]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the summary for the title in question", "response": "def summary(self, title, sentences=0, chars=0, auto_suggest=True, redirect=True):\n        \"\"\" Get the summary for the title in question\n\n            Args:\n                title (str): Page title to summarize\n                sentences (int): Number of sentences to return in summary\n                chars (int): Number of characters to return in summary\n                auto_suggest (bool): Run auto-suggest on title before \\\n                                     summarizing\n                redirect (bool): Use page redirect on title before summarizing\n            Returns:\n                str: The summarized results of the page\n            Note:\n                Precedence for parameters: sentences then chars; if both are \\\n                0 then the entire first section is returned \"\"\"\n        page_info = self.page(title, auto_suggest=auto_suggest, redirect=redirect)\n        return page_info.summarize(sentences, chars)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef categorymembers(self, category, results=10, subcategories=True):\n        self._check_query(category, \"Category must be specified\")\n\n        max_pull = 5000\n        search_params = {\n            \"list\": \"categorymembers\",\n            \"cmprop\": \"ids|title|type\",\n            \"cmtype\": (\"page|subcat\" if subcategories else \"page\"),\n            \"cmlimit\": (results if results is not None else max_pull),\n            \"cmtitle\": \"{0}:{1}\".format(self.category_prefix, category),\n        }\n        pages = list()\n        subcats = list()\n        returned_results = 0\n        finished = False\n        last_cont = dict()\n        while not finished:\n            params = search_params.copy()\n            params.update(last_cont)\n            raw_res = self.wiki_request(params)\n\n            self._check_error_response(raw_res, category)\n\n            current_pull = len(raw_res[\"query\"][\"categorymembers\"])\n            for rec in raw_res[\"query\"][\"categorymembers\"]:\n                if rec[\"type\"] == \"page\":\n                    pages.append(rec[\"title\"])\n                elif rec[\"type\"] == \"subcat\":\n                    tmp = rec[\"title\"]\n                    if tmp.startswith(self.category_prefix):\n                        tmp = tmp[len(self.category_prefix) + 1 :]\n                    subcats.append(tmp)\n\n            cont = raw_res.get(\"query-continue\", False)\n            if cont and \"categorymembers\" in cont:\n                cont = cont[\"categorymembers\"]\n            else:\n                cont = raw_res.get(\"continue\", False)\n\n            if cont is False or last_cont == cont:\n                break\n\n            returned_results += current_pull\n            if results is None or (results - returned_results > 0):\n                last_cont = cont\n            else:\n                finished = True\n\n            if results is not None and results - returned_results < max_pull:\n                search_params[\"cmlimit\"] = results - returned_results\n        # end while loop\n\n        if subcategories:\n            return pages, subcats\n        return pages", "response": "Get informaton about a category."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef categorytree(self, category, depth=5):\n\n        def __cat_tree_rec(cat, depth, tree, level, categories, links):\n            \"\"\" recursive function to build out the tree \"\"\"\n            tree[cat] = dict()\n            tree[cat][\"depth\"] = level\n            tree[cat][\"sub-categories\"] = dict()\n            tree[cat][\"links\"] = list()\n            tree[cat][\"parent-categories\"] = list()\n            parent_cats = list()\n\n            if cat not in categories:\n                tries = 0\n                while True:\n                    if tries > 10:\n                        raise MediaWikiCategoryTreeError(cat)\n                    try:\n                        pag = self.page(\"{0}:{1}\".format(self.category_prefix, cat))\n                        categories[cat] = pag\n                        parent_cats = categories[cat].categories\n                        links[cat] = self.categorymembers(\n                            cat, results=None, subcategories=True\n                        )\n                        break\n                    except PageError:\n                        raise PageError(\"{0}:{1}\".format(self.category_prefix, cat))\n                    except KeyboardInterrupt:\n                        raise\n                    except Exception:\n                        tries = tries + 1\n                        time.sleep(1)\n            else:\n                parent_cats = categories[cat].categories\n\n            tree[cat][\"parent-categories\"].extend(parent_cats)\n            tree[cat][\"links\"].extend(links[cat][0])\n\n            if depth and level >= depth:\n                for ctg in links[cat][1]:\n                    tree[cat][\"sub-categories\"][ctg] = None\n            else:\n                for ctg in links[cat][1]:\n                    __cat_tree_rec(\n                        ctg,\n                        depth,\n                        tree[cat][\"sub-categories\"],\n                        level + 1,\n                        categories,\n                        links,\n                    )\n\n        # ###################################\n        # ### Actual Function Code        ###\n        # ###################################\n\n        # make it simple to use both a list or a single category term\n        if not isinstance(category, list):\n            cats = [category]\n        else:\n            cats = category\n\n        # parameter verification\n        if len(cats) == 1 and (cats[0] is None or cats[0] == \"\"):\n            msg = (\n                \"CategoryTree: Parameter 'category' must either \"\n                \"be a list of one or more categories or a string; \"\n                \"provided: '{}'\".format(category)\n            )\n            raise ValueError(msg)\n\n        if depth is not None and depth < 1:\n            msg = (\n                \"CategoryTree: Parameter 'depth' must be either None \"\n                \"(for the full tree) or be greater than 0\"\n            )\n            raise ValueError(msg)\n\n        results = dict()\n        categories = dict()\n        links = dict()\n\n        for cat in cats:\n            if cat is None or cat == \"\":\n                continue\n            __cat_tree_rec(cat, depth, results, 0, categories, links)\n        return results", "response": "Generate the Category Tree for the given category and depth"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a MediaWiki page object based on the provided title or pageid.", "response": "def page(\n        self, title=None, pageid=None, auto_suggest=True, redirect=True, preload=False\n    ):\n        \"\"\" Get MediaWiki page based on the provided title or pageid\n\n            Args:\n                title (str): Page title\n                pageid (int): MediaWiki page identifier\n                auto-suggest (bool): **True:** Allow page title auto-suggest\n                redirect (bool): **True:** Follow page redirects\n                preload (bool): **True:** Load most page properties\n            Raises:\n                ValueError: when title is blank or None and no pageid is \\\n                            provided\n            Raises:\n                :py:func:`mediawiki.exceptions.PageError`: if page does \\\n                not exist\n            Note:\n                Title takes precedence over pageid if both are provided \"\"\"\n        if (title is None or title.strip() == \"\") and pageid is None:\n            raise ValueError(\"Either a title or a pageid must be specified\")\n        elif title:\n            if auto_suggest:\n                temp_title = self.suggest(title)\n                if temp_title is None:  # page doesn't exist\n                    raise PageError(title=title)\n                else:\n                    title = temp_title\n            return MediaWikiPage(self, title, redirect=redirect, preload=preload)\n        else:  # must be pageid\n            return MediaWikiPage(self, pageid=pageid, preload=preload)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef wiki_request(self, params):\n\n        params[\"format\"] = \"json\"\n        if \"action\" not in params:\n            params[\"action\"] = \"query\"\n\n        limit = self._rate_limit\n        last_call = self._rate_limit_last_call\n        if limit and last_call and last_call + self._min_wait > datetime.now():\n            # call time to quick for rate limited api requests, wait\n            wait_time = (last_call + self._min_wait) - datetime.now()\n            time.sleep(int(wait_time.total_seconds()))\n\n        req = self._get_response(params)\n\n        if self._rate_limit:\n            self._rate_limit_last_call = datetime.now()\n\n        return req", "response": "Make a request to the MediaWiki API using the given search\n            parameters"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses out the Wikimedia site information including API Version and Extensions", "response": "def _get_site_info(self):\n        \"\"\" Parse out the Wikimedia site information including\n        API Version and Extensions \"\"\"\n        response = self.wiki_request(\n            {\"meta\": \"siteinfo\", \"siprop\": \"extensions|general\"}\n        )\n\n        # parse what we need out here!\n        query = response.get(\"query\", None)\n        if query is None or query.get(\"general\", None) is None:\n            raise MediaWikiException(\"Missing query in response\")\n\n        gen = query.get(\"general\", None)\n\n        api_version = gen[\"generator\"].split(\" \")[1].split(\"-\")[0]\n\n        major_minor = api_version.split(\".\")\n        for i, item in enumerate(major_minor):\n            major_minor[i] = int(item)\n        self._api_version = tuple(major_minor)\n        self._api_version_str = \".\".join([str(x) for x in self._api_version])\n\n        # parse the base url out\n        tmp = gen.get(\"server\", \"\")\n        if tmp == \"\":\n            raise MediaWikiException(\"Unable to parse base url\")\n        if tmp.startswith(\"http://\") or tmp.startswith(\"https://\"):\n            self._base_url = tmp\n        elif gen[\"base\"].startswith(\"https:\"):\n            self._base_url = \"https:{}\".format(tmp)\n        else:\n            self._base_url = \"http:{}\".format(tmp)\n\n        self._extensions = [ext[\"name\"] for ext in query[\"extensions\"]]\n        self._extensions = sorted(list(set(self._extensions)))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck for default error messages and throw correct exception", "response": "def _check_error_response(response, query):\n        \"\"\" check for default error messages and throw correct exception \"\"\"\n        if \"error\" in response:\n            http_error = [\"HTTP request timed out.\", \"Pool queue is full\"]\n            geo_error = [\n                \"Page coordinates unknown.\",\n                \"One of the parameters gscoord, gspage, gsbbox is required\",\n                \"Invalid coordinate provided\",\n            ]\n            err = response[\"error\"][\"info\"]\n            if err in http_error:\n                raise HTTPTimeoutError(query)\n            elif err in geo_error:\n                raise MediaWikiGeoCoordError(err)\n            else:\n                raise MediaWikiException(err)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_response(self, params):\n        return self._session.get(\n            self._api_url, params=params, timeout=self._timeout\n        ).json(encoding=\"utf8\")", "response": "wrap the call to the requests package"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _post_response(self, params):\n        return self._session.post(\n            self._api_url, data=params, timeout=self._timeout\n        ).json(encoding=\"utf8\")", "response": "wrap a post call to the requests package"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndetermine all positional and named arguments as a dict", "response": "def parse_all_arguments(func):\n    \"\"\" determine all positional and named arguments as a dict \"\"\"\n    args = dict()\n    if sys.version_info < (3, 0):\n        func_args = inspect.getargspec(func)\n        if func_args.defaults is not None:\n            val = len(func_args.defaults)\n            for i, itm in enumerate(func_args.args[-val:]):\n                args[itm] = func_args.defaults[i]\n    else:\n        func_args = inspect.signature(func)\n        for itm in list(func_args.parameters)[1:]:\n            param = func_args.parameters[itm]\n            if param.default is not param.empty:\n                args[param.name] = param.default\n    return args"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nhandling python 3 unicode and python 2. 7 byte strings", "response": "def str_or_unicode(text):\n    \"\"\" handle python 3 unicode and python 2.7 byte strings \"\"\"\n    encoding = sys.stdout.encoding\n    if sys.version_info > (3, 0):\n        return text.encode(encoding).decode(encoding)\n    return text.encode(encoding)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncombining the pulling of the content revision and parent ID", "response": "def _pull_content_revision_parent(self):\n        \"\"\" combine the pulling of these three properties \"\"\"\n\n        if self._revision_id is None:\n            query_params = {\n                \"prop\": \"extracts|revisions\",\n                \"explaintext\": \"\",\n                \"rvprop\": \"ids\",\n            }\n            query_params.update(self.__title_query_param())\n            request = self.mediawiki.wiki_request(query_params)\n            page_info = request[\"query\"][\"pages\"][self.pageid]\n            self._content = page_info[\"extract\"]\n            self._revision_id = page_info[\"revisions\"][0][\"revid\"]\n            self._parent_id = page_info[\"revisions\"][0][\"parentid\"]\n        return self._content, self._revision_id, self._parent_id"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef html(self):\n        if self._html is False:\n            self._html = None\n            query_params = {\n                \"prop\": \"revisions\",\n                \"rvprop\": \"content\",\n                \"rvlimit\": 1,\n                \"rvparse\": \"\",\n                \"titles\": self.title,\n            }\n            request = self.mediawiki.wiki_request(query_params)\n            page = request[\"query\"][\"pages\"][self.pageid]\n            self._html = page[\"revisions\"][0][\"*\"]\n        return self._html", "response": "str - > HTML representation of the page"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlist Images on the page", "response": "def images(self):\n        \"\"\" list: Images on the page\n\n            Note:\n                Not settable \"\"\"\n        if self._images is None:\n            self._images = list()\n            params = {\n                \"generator\": \"images\",\n                \"gimlimit\": \"max\",\n                \"prop\": \"imageinfo\",  # this will be replaced by fileinfo\n                \"iiprop\": \"url\",\n            }\n            for page in self._continued_query(params):\n                if \"imageinfo\" in page and \"url\" in page[\"imageinfo\"][0]:\n                    self._images.append(page[\"imageinfo\"][0][\"url\"])\n            self._images = sorted(self._images)\n        return self._images"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef logos(self):\n        if self._logos is None:\n            self._logos = list()\n            soup = BeautifulSoup(self.html, \"html.parser\")\n            info = soup.find(\"table\", {\"class\": \"infobox\"})\n            if info is not None:\n                children = info.findAll(\"\", {\"class\": \"image\"})\n                for child in children:\n                    self._logos.append(\"https:\" + child.img[\"src\"])\n        return self._logos", "response": "List of URLs of all logos in the infobox."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses hatnotes from the HTML", "response": "def hatnotes(self):\n        \"\"\" list: Parse hatnotes from the HTML\n\n            Note:\n                Not settable\n            Note:\n                Side effect is to also pull the html which can be slow\n            Note:\n                This is a parsing operation and not part of the standard API\"\"\"\n        if self._hatnotes is None:\n            self._hatnotes = list()\n            soup = BeautifulSoup(self.html, \"html.parser\")\n            notes = soup.findAll(\"\", {\"class\": \"hatnote\"})\n            if notes is not None:\n                for note in notes:\n                    tmp = list()\n                    for child in note.children:\n                        if hasattr(child, \"text\"):\n                            tmp.append(child.text)\n                        else:\n                            tmp.append(child)\n                    self._hatnotes.append(\"\".join(tmp))\n        return self._hatnotes"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nlist of external links or references listed anywhere on the MediaWiki page Note : This method is only available if the page has not been settable.", "response": "def references(self):\n        \"\"\" list: External links, or references, listed anywhere on the \\\n                  MediaWiki page\n            Note:\n                Not settable\n            Note\n                May include external links within page that are not \\\n                technically cited anywhere \"\"\"\n        if self._references is None:\n            self._references = list()\n            self.__pull_combined_properties()\n        return self._references"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef categories(self):\n        if self._categories is None:\n            self._categories = list()\n            self.__pull_combined_properties()\n        return self._categories", "response": "list of non - hidden categories on the page"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the GeoCoordinates of the place referenced ; results in \\ lat / long tuple or None if no geocoordinates present", "response": "def coordinates(self):\n        \"\"\" Tuple: GeoCoordinates of the place referenced; results in \\\n            lat/long tuple or None if no geocoordinates present\n\n            Note:\n                Not settable\n            Note:\n                Requires the GeoData extension to be installed \"\"\"\n        if self._coordinates is False:\n            self._coordinates = None\n            self.__pull_combined_properties()\n        return self._coordinates"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef links(self):\n        if self._links is None:\n            self._links = list()\n            self.__pull_combined_properties()\n        return self._links", "response": "list of all MediaWiki page links on the page\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef redirects(self):\n        if self._redirects is None:\n            self._redirects = list()\n            self.__pull_combined_properties()\n        return self._redirects", "response": "list of all redirects to this page"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nlist Pages that link to this page", "response": "def backlinks(self):\n        \"\"\" list: Pages that link to this page\n\n            Note:\n                Not settable \"\"\"\n        if self._backlinks is None:\n            self._backlinks = list()\n            params = {\n                \"action\": \"query\",\n                \"list\": \"backlinks\",\n                \"bltitle\": self.title,\n                \"bllimit\": \"max\",\n                \"blfilterredir\": \"nonredirects\",\n                \"blnamespace\": 0,\n            }\n            tmp = [link[\"title\"] for link in self._continued_query(params, \"backlinks\")]\n            self._backlinks = sorted(tmp)\n        return self._backlinks"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsummarizes the page either by number of sentences chars or first page.", "response": "def summarize(self, sentences=0, chars=0):\n        \"\"\" Summarize page either by number of sentences, chars, or first\n            section (**default**)\n\n            Args:\n                sentences (int): Number of sentences to use in summary \\\n                                 (first `x` sentences)\n                chars (int): Number of characters to use in summary \\\n                             (first `x` characters)\n            Returns:\n                str: The summary of the MediaWiki page\n            Note:\n                Precedence for parameters: sentences then chars; if both are \\\n                0 then the entire first section is returned \"\"\"\n        query_params = {\"prop\": \"extracts\", \"explaintext\": \"\", \"titles\": self.title}\n        if sentences:\n            query_params[\"exsentences\"] = 10 if sentences > 10 else sentences\n        elif chars:\n            query_params[\"exchars\"] = 1 if chars < 1 else chars\n        else:\n            query_params[\"exintro\"] = \"\"\n\n        request = self.mediawiki.wiki_request(query_params)\n        summary = request[\"query\"][\"pages\"][self.pageid][\"extract\"]\n        return summary"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_section_links(self, section_title):\n        soup = BeautifulSoup(self.html, \"html.parser\")\n        headlines = soup.find_all(\"span\", {\"class\": \"mw-headline\"})\n        tmp_soup = BeautifulSoup(section_title, \"html.parser\")\n        tmp_sec_title = tmp_soup.get_text().lower()\n        id_tag = None\n        for headline in headlines:\n            tmp_id = headline.text\n            if tmp_id.lower() == tmp_sec_title:\n                id_tag = headline.get(\"id\")\n                break\n\n        if id_tag is not None:\n            return self._parse_section_links(id_tag)\n        return None", "response": "Parse all links within a section"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef __load(self, redirect=True, preload=False):\n        query_params = {\n            \"prop\": \"info|pageprops\",\n            \"inprop\": \"url\",\n            \"ppprop\": \"disambiguation\",\n            \"redirects\": \"\",\n        }\n        query_params.update(self.__title_query_param())\n\n        request = self.mediawiki.wiki_request(query_params)\n\n        query = request[\"query\"]\n        pageid = list(query[\"pages\"].keys())[0]\n        page = query[\"pages\"][pageid]\n\n        # determine result of the request\n        # missing is present if the page is missing\n        if \"missing\" in page:\n            self._raise_page_error()\n        # redirects is present in query if page is a redirect\n        elif \"redirects\" in query:\n            self._handle_redirect(redirect, preload, query, page)\n        # if pageprops is returned, it must be a disambiguation error\n        elif \"pageprops\" in page:\n            self._raise_disambiguation_error(page, pageid)\n        else:\n            self.pageid = pageid\n            self.title = page[\"title\"]\n            self.url = page[\"fullurl\"]", "response": "load the basic page information"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _raise_page_error(self):\n        if hasattr(self, \"title\"):\n            raise PageError(title=self.title)\n        else:\n            raise PageError(pageid=self.pageid)", "response": "raise the correct type of page error"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _raise_disambiguation_error(self, page, pageid):\n        query_params = {\n            \"prop\": \"revisions\",\n            \"rvprop\": \"content\",\n            \"rvparse\": \"\",\n            \"rvlimit\": 1,\n        }\n        query_params.update(self.__title_query_param())\n        request = self.mediawiki.wiki_request(query_params)\n        html = request[\"query\"][\"pages\"][pageid][\"revisions\"][0][\"*\"]\n\n        lis = BeautifulSoup(html, \"html.parser\").find_all(\"li\")\n        filtered_lis = [\n            li for li in lis if \"tocsection\" not in \"\".join(li.get(\"class\", list()))\n        ]\n        may_refer_to = [li.a.get_text() for li in filtered_lis if li.a]\n\n        disambiguation = list()\n        for lis_item in filtered_lis:\n            item = lis_item.find_all(\"a\")\n            one_disambiguation = dict()\n            one_disambiguation[\"description\"] = lis_item.text\n            if item and hasattr(item[0], \"title\"):\n                one_disambiguation[\"title\"] = item[0][\"title\"]\n            else:\n                # these are non-linked records so double up the text\n                one_disambiguation[\"title\"] = lis_item.text\n            disambiguation.append(one_disambiguation)\n        raise DisambiguationError(\n            getattr(self, \"title\", page[\"title\"]),\n            may_refer_to,\n            page[\"fullurl\"],\n            disambiguation,\n        )", "response": "parse and raise a disambiguation error"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nhandle redirect from page to page", "response": "def _handle_redirect(self, redirect, preload, query, page):\n        \"\"\" handle redirect \"\"\"\n        if redirect:\n            redirects = query[\"redirects\"][0]\n\n            if \"normalized\" in query:\n                normalized = query[\"normalized\"][0]\n                if normalized[\"from\"] != self.title:\n                    raise MediaWikiException(ODD_ERROR_MESSAGE)\n                from_title = normalized[\"to\"]\n            else:\n                if not getattr(self, \"title\", None):\n                    self.title = redirects[\"from\"]\n                    delattr(self, \"pageid\")\n                from_title = self.title\n            if redirects[\"from\"] != from_title:\n                raise MediaWikiException(ODD_ERROR_MESSAGE)\n\n            # change the title and reload the whole object\n            self.__init__(\n                self.mediawiki,\n                title=redirects[\"to\"],\n                redirect=redirect,\n                preload=preload,\n            )\n        else:\n            raise RedirectError(getattr(self, \"title\", page[\"title\"]))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nyields the pages that were continued by the query", "response": "def _continued_query(self, query_params, key=\"pages\"):\n        \"\"\" Based on\n            https://www.mediawiki.org/wiki/API:Query#Continuing_queries \"\"\"\n        query_params.update(self.__title_query_param())\n\n        last_cont = dict()\n        prop = query_params.get(\"prop\")\n\n        while True:\n            params = query_params.copy()\n            params.update(last_cont)\n\n            request = self.mediawiki.wiki_request(params)\n\n            if \"query\" not in request:\n                break\n\n            pages = request[\"query\"][key]\n            if \"generator\" in query_params:\n                for datum in pages.values():\n                    yield datum\n            elif isinstance(pages, list):\n                for datum in list(enumerate(pages)):\n                    yield datum[1]\n            else:\n                for datum in pages[self.pageid].get(prop, list()):\n                    yield datum\n\n            if \"continue\" not in request or request[\"continue\"] == last_cont:\n                break\n\n            last_cont = request[\"continue\"]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses the links in the unordered list", "response": "def _parse_section_links(self, id_tag):\n        \"\"\" given a section id, parse the links in the unordered list \"\"\"\n        soup = BeautifulSoup(self.html, \"html.parser\")\n        info = soup.find(\"span\", {\"id\": id_tag})\n        all_links = list()\n\n        if info is None:\n            return all_links\n\n        for node in soup.find(id=id_tag).parent.next_siblings:\n            if not isinstance(node, Tag):\n                continue\n            elif node.get(\"role\", \"\") == \"navigation\":\n                continue\n            elif \"infobox\" in node.get(\"class\", []):\n                continue\n\n            # this is actually the child node's class...\n            is_headline = node.find(\"span\", {\"class\": \"mw-headline\"})\n            if is_headline is not None:\n                break\n            elif node.name == \"a\":\n                all_links.append(self.__parse_link_info(node))\n            else:\n                for link in node.findAll(\"a\"):\n                    all_links.append(self.__parse_link_info(link))\n        return all_links"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses the link info tag for the link", "response": "def __parse_link_info(self, link):\n        \"\"\" parse the <a> tag for the link \"\"\"\n        href = link.get(\"href\", \"\")\n        txt = link.string or href\n        is_rel = is_relative_url(href)\n        if is_rel is True:\n            tmp = \"{0}{1}\".format(self.mediawiki.base_url, href)\n        elif is_rel is None:\n            tmp = \"{0}{1}\".format(self.url, href)\n        else:\n            tmp = href\n        return txt, tmp"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _parse_sections(self):\n\n        def _list_to_dict(_dict, path, sec):\n            tmp = _dict\n            for elm in path[:-1]:\n                tmp = tmp[elm]\n            tmp[sec] = OrderedDict()\n\n        self._sections = list()\n        section_regexp = r\"\\n==* .* ==*\\n\"  # '== {STUFF_NOT_\\n} =='\n        found_obj = re.findall(section_regexp, self.content)\n\n        res = OrderedDict()\n        path = list()\n        last_depth = 0\n        for obj in found_obj:\n            depth = obj.count(\"=\") / 2  # this gets us to the single side...\n            depth -= 2  # now, we can calculate depth\n\n            sec = obj.lstrip(\"\\n= \").rstrip(\" =\\n\")\n            if depth == 0:\n                last_depth = 0\n                path = [sec]\n                res[sec] = OrderedDict()\n            elif depth > last_depth:\n                last_depth = depth\n                path.append(sec)\n                _list_to_dict(res, path, sec)\n            elif depth < last_depth:\n                # path.pop()\n                while last_depth > depth:\n                    path.pop()\n                    last_depth -= 1\n                path.pop()\n                path.append(sec)\n                _list_to_dict(res, path, sec)\n                last_depth = depth\n            else:\n                path.pop()\n                path.append(sec)\n                _list_to_dict(res, path, sec)\n                last_depth = depth\n            self._sections.append(sec)\n\n        self._table_of_contents = res", "response": "parse sections and TOC"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\npulling combined properties from the request and returns a dict of the combined properties.", "response": "def __pull_combined_properties(self):\n        \"\"\" something here... \"\"\"\n\n        query_params = {\n            \"titles\": self.title,\n            \"prop\": \"extracts|redirects|links|coordinates|categories|extlinks\",\n            \"continue\": dict(),\n            # summary\n            \"explaintext\": \"\",\n            \"exintro\": \"\",  # full first section for the summary!\n            # redirects\n            \"rdprop\": \"title\",\n            \"rdlimit\": \"max\",\n            # links\n            \"plnamespace\": 0,\n            \"pllimit\": \"max\",\n            # coordinates\n            \"colimit\": \"max\",\n            # categories\n            \"cllimit\": \"max\",\n            \"clshow\": \"!hidden\",\n            # references\n            \"ellimit\": \"max\",\n        }\n\n        last_cont = dict()\n        results = dict()\n        idx = 0\n        while True:\n            params = query_params.copy()\n            params.update(last_cont)\n\n            request = self.mediawiki.wiki_request(params)\n            idx += 1\n\n            # print(idx)\n            # quick exit\n            if \"query\" not in request:\n                # print(request)\n                break\n\n            keys = [\n                \"extracts\",\n                \"redirects\",\n                \"links\",\n                \"coordinates\",\n                \"categories\",\n                \"extlinks\",\n            ]\n            new_cont = request.get(\"continue\")\n            request = request[\"query\"][\"pages\"][self.pageid]\n            if not results:\n                results = request\n            else:\n                for key in keys:\n                    if key in request and request.get(key) is not None:\n                        val = request.get(key)\n                        tmp = results.get(key)\n                        if isinstance(tmp, (list, tuple)):\n                            results[key] = results.get(key, list) + val\n            if new_cont is None or new_cont == last_cont:\n                break\n\n            last_cont = new_cont\n\n        # redirects\n        tmp = [link[\"title\"] for link in results.get(\"redirects\", list())]\n        self._redirects = sorted(tmp)\n\n        # summary\n        self._summary = results[\"extract\"]\n\n        # links\n        tmp = [link[\"title\"] for link in results.get(\"links\", list())]\n        self._links = sorted(tmp)\n\n        # categories\n        def _get_cat(val):\n            \"\"\" parse the category correctly \"\"\"\n            tmp = val[\"title\"]\n            if tmp.startswith(self.mediawiki.category_prefix):\n                return tmp[len(self.mediawiki.category_prefix) + 1 :]\n            return tmp\n\n        tmp = [_get_cat(link) for link in results.get(\"categories\", list())]\n        self._categories = sorted(tmp)\n\n        # coordinates\n        if \"coordinates\" in results:\n            self._coordinates = (\n                Decimal(results[\"coordinates\"][0][\"lat\"]),\n                Decimal(results[\"coordinates\"][0][\"lon\"]),\n            )\n\n        # references\n        tmp = [link[\"*\"] for link in results.get(\"extlinks\", list())]\n        self._references = sorted(tmp)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn True or False if the system is armed in any way", "response": "def is_armed(self):\n        \"\"\"Return True or False if the system is armed in any way\"\"\"\n        alarm_code = self.get_armed_status()\n\n        if alarm_code == YALE_STATE_ARM_FULL:\n            return True\n\n        if alarm_code == YALE_STATE_ARM_PARTIAL:\n            return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_api_name(self, func):\n        words = func.__name__.split('_')\n        words = [w.capitalize() for w in words]\n        return ' '.join(words)", "response": "e. g. Convert Do Work to Do Work"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef linkify(props):\n    match = props['match']\n    protocol = match.group(1)\n    url = match.group(2)\n    href = protocol + url\n\n    if props['block']['type'] == BLOCK_TYPES.CODE:\n        return href\n\n    link_props = {\n        'href': href,\n    }\n\n    if href.startswith('www'):\n        link_props['href'] = 'http://' + href\n\n    return DOM.create_element('a', link_props, href)", "response": "Wrap plain URLs with link tags."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating an element of the given type.", "response": "def create_element(cls, type_=None, props=None, *children):\n        \"\"\"\n        Signature inspired by React.createElement.\n        createElement(\n          string/Component type,\n          [dict props],\n          [children ...]\n        )\n        https://facebook.github.io/react/docs/top-level-api.html#react.createelement\n        \"\"\"\n        # Create an empty document fragment.\n        if not type_:\n            return cls.dom.create_tag('fragment')\n\n        if props is None:\n            props = {}\n\n        # If the first element of children is a list, we use it as the list.\n        if len(children) and isinstance(children[0], (list, tuple)):\n            children = children[0]\n\n        # The children prop is the first child if there is only one.\n        props['children'] = children[0] if len(children) == 1 else children\n\n        if callable(type_):\n            # Function component, via def or lambda.\n            elt = type_(props)\n        else:\n            # Raw tag, as a string.\n            attributes = {}\n\n            # Never render those attributes on a raw tag.\n            props.pop('children', None)\n            props.pop('block', None)\n            props.pop('blocks', None)\n            props.pop('entity', None)\n            props.pop('inline_style_range', None)\n\n            # Convert style object to style string, like the DOM would do.\n            if 'style' in props and isinstance(props['style'], dict):\n                rules = ['{0}: {1};'.format(DOM.camel_to_dash(s), props['style'][s]) for s in props['style'].keys()]\n                props['style'] = ''.join(sorted(rules))\n\n            # Convert props to HTML attributes.\n            for key in props:\n                if props[key] is False:\n                    props[key] = 'false'\n\n                if props[key] is True:\n                    props[key] = 'true'\n\n                if props[key] is not None:\n                    attributes[key] = unicode(props[key])\n\n            elt = cls.dom.create_tag(type_, attributes)\n\n            # Append the children inside the element.\n            for child in children:\n                if child not in (None, ''):\n                    cls.append_child(elt, child)\n\n        # If elt is \"empty\", create a fragment anyway to add children.\n        if elt in (None, ''):\n            elt = cls.dom.create_tag('fragment')\n\n        return elt"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef start_stop(name, start, stop, data=''):\n        return [\n            Command('start_%s' % name, start, data),\n            Command('stop_%s' % name, stop, data),\n        ]", "response": "Builds a pair of start and stop commands with the same data."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef from_ranges(ranges, name, data_key, start_key='offset', length_key='length'):\n        commands = []\n        for r in ranges:\n            data = r[data_key]\n            start = r[start_key]\n            stop = start + r[length_key]\n            commands.extend(Command.start_stop(name, start, stop, data))\n        return commands", "response": "Creates a list of commands from a list of ranges. Each command is converted to two start_stop commands."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef for_kind(kind_map, type_, fallback_key):\n        if type_ not in kind_map:\n            if fallback_key not in kind_map:\n                raise ConfigException('\"%s\" is not in the config and has no fallback' % type_)\n\n            config = kind_map[fallback_key]\n        else:\n            config = kind_map[type_]\n\n        if isinstance(config, dict):\n            if 'element' not in config:\n                raise ConfigException('\"%s\" does not define an element' % type_)\n\n            opts = Options(type_, **config)\n        else:\n            opts = Options(type_, config)\n\n        return opts", "response": "Create an Options object from any mapping."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrendering the content of a given piece of content state.", "response": "def render(self, content_state=None):\n        \"\"\"\n        Starts the export process on a given piece of content state.\n        \"\"\"\n        if content_state is None:\n            content_state = {}\n\n        blocks = content_state.get('blocks', [])\n        wrapper_state = WrapperState(self.block_map, blocks)\n        document = DOM.create_element()\n        entity_map = content_state.get('entityMap', {})\n        min_depth = 0\n\n        for block in blocks:\n            depth = block['depth']\n            elt = self.render_block(block, entity_map, wrapper_state)\n\n            if depth > min_depth:\n                min_depth = depth\n\n            # At level 0, append the element to the document.\n            if depth == 0:\n                DOM.append_child(document, elt)\n\n        # If there is no block at depth 0, we need to add the wrapper that contains the whole tree to the document.\n        if min_depth > 0 and wrapper_state.stack.length() != 0:\n            DOM.append_child(document, wrapper_state.stack.tail().elt)\n\n        return DOM.render(document)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef build_command_groups(self, block):\n        text = block['text']\n\n        commands = sorted(self.build_commands(block))\n        grouped = groupby(commands, Command.key)\n        listed = list(groupby(commands, Command.key))\n        sliced = []\n\n        i = 0\n        for start_index, commands in grouped:\n            if i < len(listed) - 1:\n                stop_index = listed[i + 1][0]\n                sliced.append((text[start_index:stop_index], list(commands)))\n            else:\n                sliced.append((text[start_index:start_index], list(commands)))\n            i += 1\n\n        return sliced", "response": "Creates command groups for the block modification commands."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbuild all of the manipulation commands for a given block.", "response": "def build_commands(self, block):\n        \"\"\"\n        Build all of the manipulation commands for a given block.\n        - One pair to set the text.\n        - Multiple pairs for styles.\n        - Multiple pairs for entities.\n        \"\"\"\n        text_commands = Command.start_stop('text', 0, len(block['text']))\n        style_commands = self.build_style_commands(block)\n        entity_commands = self.build_entity_commands(block)\n\n        return text_commands + style_commands + entity_commands"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run(\n    commands,\n    shell=None,\n    prompt_template=\"default\",\n    speed=1,\n    quiet=False,\n    test_mode=False,\n    commentecho=False,\n):\n    \"\"\"Main function for \"magic-running\" a list of commands.\"\"\"\n    if not quiet:\n        secho(\"We'll do it live!\", fg=\"red\", bold=True)\n        secho(\n            \"STARTING SESSION: Press Ctrl-C at any time to exit.\",\n            fg=\"yellow\",\n            bold=True,\n        )\n        click.pause()\n\n    click.clear()\n    state = SessionState(\n        shell=shell,\n        prompt_template=prompt_template,\n        speed=speed,\n        test_mode=test_mode,\n        commentecho=commentecho,\n    )\n\n    i = 0\n    while i < len(commands):\n        command = commands[i].strip()\n        i += 1\n        if not command:\n            continue\n        is_comment = command.startswith(\"#\")\n        if not is_comment:\n            command_as_list = shlex.split(ensure_utf8(command))\n        else:\n            command_as_list = None\n        shell_match = SHELL_RE.match(command)\n        if is_comment:\n            # Parse comment magic\n            match = OPTION_RE.match(command)\n            if match:\n                option, arg = match.group(\"option\"), match.group(\"arg\")\n                func = OPTION_MAP[option]\n                func(state, arg)\n            elif state.commentecho():\n                comment = command.lstrip(\"#\")\n                secho(comment, fg=\"yellow\", bold=True)\n            continue\n        # Handle 'export' and 'alias' commands by storing them in SessionState\n        elif command_as_list and command_as_list[0] in [\"alias\", \"export\"]:\n            magictype(\n                command, prompt_template=state[\"prompt_template\"], speed=state[\"speed\"]\n            )\n            # Store the raw commands instead of using add_envvar and add_alias\n            # to avoid having to parse the command ourselves\n            state.add_command(command)\n        # Handle ```python and ```ipython by running \"player\" consoles\n        elif shell_match:\n            shell_name = shell_match.groups()[0].strip()\n            py_commands = []\n            more = True\n            while more:  # slurp up all the python code\n                try:\n                    py_command = commands[i].rstrip()\n                except IndexError:\n                    raise SessionError(\n                        \"Unmatched {0} code block in \"\n                        \"session file.\".format(shell_name)\n                    )\n                i += 1\n                if py_command.startswith(\"```\"):\n                    i += 1\n                    more = False\n                else:\n                    py_commands.append(py_command)\n            # Run the player console\n            magictype(\n                shell_name,\n                prompt_template=state[\"prompt_template\"],\n                speed=state[\"speed\"],\n            )\n\n            if shell_name == \"ipython\":\n                try:\n                    from doitlive.ipython_consoles import start_ipython_player\n                except ImportError:\n                    raise RuntimeError(\n                        \"```ipython blocks require IPython to be installed\"\n                    )\n                # dedent all the commands to account for IPython's autoindentation\n                ipy_commands = [textwrap.dedent(cmd) for cmd in py_commands]\n                start_ipython_player(ipy_commands, speed=state[\"speed\"])\n            else:\n                start_python_player(py_commands, speed=state[\"speed\"])\n        else:\n            # goto_stealthmode determines when to switch to stealthmode\n            goto_stealthmode = magicrun(command, **state)\n            # stealthmode allows user to type live commands outside of automated script\n            i -= stealthmode(state, goto_stealthmode)\n    echo_prompt(state[\"prompt_template\"])\n    wait_for(RETURNS)\n    if not quiet:\n        secho(\"FINISHED SESSION\", fg=\"yellow\", bold=True)", "response": "Main function for magic - running a list of commands."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef completion():\n    shell = env.get(\"SHELL\", None)\n    if env.get(\"SHELL\", None):\n        echo(\n            click_completion.get_code(\n                shell=shell.split(os.sep)[-1], prog_name=\"doitlive\"\n            )\n        )\n    else:\n        echo(\n            \"Please ensure that the {SHELL} environment \"\n            \"variable is set.\".format(SHELL=style(\"SHELL\", bold=True))\n        )\n        sys.exit(1)", "response": "Output completion for the current user."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef play(quiet, session_file, shell, speed, prompt, commentecho):\n    run(\n        session_file.readlines(),\n        shell=shell,\n        speed=speed,\n        quiet=quiet,\n        test_mode=TESTING,\n        prompt_template=prompt,\n        commentecho=commentecho,\n    )", "response": "Play a session file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef demo(quiet, shell, speed, prompt, commentecho):\n    run(\n        DEMO,\n        shell=shell,\n        speed=speed,\n        test_mode=TESTING,\n        prompt_template=prompt,\n        quiet=quiet,\n        commentecho=commentecho,\n    )", "response": "Run a demo session."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef record(session_file, shell, prompt, alias, envvar):\n    if os.path.exists(session_file):\n        click.confirm(\n            'File \"{0}\" already exists. Overwrite?'.format(session_file),\n            abort=True,\n            default=False,\n        )\n\n    secho(\"We'll do it live!\", fg=\"red\", bold=True)\n    filename = click.format_filename(session_file)\n    secho(\"RECORDING SESSION: {}\".format(filename), fg=\"yellow\", bold=True)\n\n    print_recorder_instructions()\n\n    click.pause()\n    click.clear()\n    cwd = os.getcwd()  # Save cwd\n\n    # Run the recorder\n    commands = run_recorder(shell, prompt, aliases=alias, envvars=envvar)\n\n    os.chdir(cwd)  # Reset cwd\n\n    secho(\"FINISHED RECORDING SESSION\", fg=\"yellow\", bold=True)\n    secho(\"Writing to {0}...\".format(filename), fg=\"cyan\")\n    with open(session_file, \"w\", encoding=\"utf-8\") as fp:\n        fp.write(HEADER_TEMPLATE.format(shell=shell, prompt=prompt))\n        write_directives(fp, \"alias\", alias)\n        write_directives(fp, \"env\", envvar)\n        fp.write(\"\\n\")\n        fp.write(\"\".join(commands))\n        fp.write(\"\\n\")\n\n    play_cmd = style(\"doitlive play {}\".format(filename), bold=True)\n    echo(\"Done. Run {} to play back your session.\".format(play_cmd))", "response": "Record a session file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\npatch click echo function.", "response": "def echo(\n    message=None, file=None, nl=True, err=False, color=None, carriage_return=False\n):\n    \"\"\"\n    Patched click echo function.\n    \"\"\"\n    message = message or \"\"\n    if carriage_return and nl:\n        click_echo(message + \"\\r\\n\", file, False, err, color)\n    elif carriage_return and not nl:\n        click_echo(message + \"\\r\", file, False, err, color)\n    else:\n        click_echo(message, file, nl, err, color)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nechoes each character in text as keyboard characters are pressed at a time.", "response": "def magictype(text, prompt_template=\"default\", speed=1):\n    \"\"\"Echo each character in ``text`` as keyboard characters are pressed.\n    Characters are echo'd ``speed`` characters at a time.\n    \"\"\"\n    echo_prompt(prompt_template)\n    cursor_position = 0\n    return_to_regular_type = False\n    with raw_mode():\n        while True:\n            char = text[cursor_position : cursor_position + speed]\n            in_char = getchar()\n            if in_char in {ESC, CTRLC}:\n                echo(carriage_return=True)\n                raise click.Abort()\n            elif in_char == TAB:\n                return_to_regular_type = True\n                break\n            elif in_char == BACKSPACE:\n                if cursor_position > 0:\n                    echo(\"\\b \\b\", nl=False)\n                    cursor_position -= 1\n            elif in_char in RETURNS:\n                # Only return at end of command\n                if cursor_position >= len(text):\n                    echo(\"\\r\", nl=True)\n                    break\n            elif in_char == CTRLZ and hasattr(signal, \"SIGTSTP\"):\n                # Background process\n                os.kill(0, signal.SIGTSTP)\n                # When doitlive is back in foreground, clear the terminal\n                # and resume where we left off\n                click.clear()\n                echo_prompt(prompt_template)\n                echo(text[:cursor_position], nl=False)\n            else:\n                if cursor_position < len(text):\n                    echo(char, nl=False)\n                    increment = min([speed, len(text) - cursor_position])\n                    cursor_position += increment\n    return return_to_regular_type"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef regulartype(prompt_template=\"default\"):\n    echo_prompt(prompt_template)\n    command_string = \"\"\n    cursor_position = 0\n    with raw_mode():\n        while True:\n            in_char = getchar()\n            if in_char in {ESC, CTRLC}:\n                echo(carriage_return=True)\n                raise click.Abort()\n            elif in_char == TAB:\n                echo(\"\\r\", nl=True)\n                return in_char\n            elif in_char == BACKSPACE:\n                if cursor_position > 0:\n                    echo(\"\\b \\b\", nl=False)\n                    command_string = command_string[:-1]\n                    cursor_position -= 1\n            elif in_char in RETURNS:\n                echo(\"\\r\", nl=True)\n                return command_string\n            elif in_char == CTRLZ and hasattr(signal, \"SIGTSTP\"):\n                # Background process\n                os.kill(0, signal.SIGTSTP)\n                # When doitlive is back in foreground, clear the terminal\n                # and resume where we left off\n                click.clear()\n                echo_prompt(prompt_template)\n            else:\n                echo(in_char, nl=False)\n                command_string += in_char\n                cursor_position += 1", "response": "Echo each character typed by the user and return the command string."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef regularrun(\n    shell,\n    prompt_template=\"default\",\n    aliases=None,\n    envvars=None,\n    extra_commands=None,\n    speed=1,\n    test_mode=False,\n    commentecho=False,\n):\n    \"\"\"Allow user to run their own live commands until CTRL-Z is pressed again.\n    \"\"\"\n    loop_again = True\n    command_string = regulartype(prompt_template)\n    if command_string == TAB:\n        loop_again = False\n        return loop_again\n    run_command(\n        command_string,\n        shell,\n        aliases=aliases,\n        envvars=envvars,\n        extra_commands=extra_commands,\n        test_mode=test_mode,\n    )\n    return loop_again", "response": "Allow user to run live commands until CTRL - Z is pressed again."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nruns a command in a single shell context.", "response": "def magicrun(\n    text,\n    shell,\n    prompt_template=\"default\",\n    aliases=None,\n    envvars=None,\n    extra_commands=None,\n    speed=1,\n    test_mode=False,\n    commentecho=False,\n):\n    \"\"\"Echo out each character in ``text`` as keyboard characters are pressed,\n    wait for a RETURN keypress, then run the ``text`` in a shell context.\n    \"\"\"\n    goto_regulartype = magictype(text, prompt_template, speed)\n    if goto_regulartype:\n        return goto_regulartype\n    run_command(\n        text,\n        shell,\n        aliases=aliases,\n        envvars=envvars,\n        extra_commands=extra_commands,\n        test_mode=test_mode,\n    )\n    return goto_regulartype"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef run_commands(self):\n        more = 0\n        prompt = sys.ps1\n        for command in self.commands:\n            try:\n                prompt = sys.ps2 if more else sys.ps1\n                try:\n                    magictype(command, prompt_template=prompt, speed=self.speed)\n                except EOFError:\n                    self.write(\"\\n\")\n                    break\n                else:\n                    if command.strip() == \"exit()\":\n                        return\n                    more = self.push(command)\n            except KeyboardInterrupt:\n                self.write(\"\\nKeyboardInterrupt\\n\")\n                self.resetbuffer()\n                more = 0\n                sys.exit(1)\n        echo_prompt(prompt)\n        wait_for(RETURNS)", "response": "Automatically type and execute all commands."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef interact(self, banner=None):\n        try:\n            sys.ps1\n        except AttributeError:\n            sys.ps1 = \">>>\"\n        try:\n            sys.ps2\n        except AttributeError:\n            sys.ps2 = \"... \"\n        cprt = (\n            'Type \"help\", \"copyright\", \"credits\" or \"license\" for ' \"more information.\"\n        )\n        if banner is None:\n            self.write(\"Python %s on %s\\n%s\\n\" % (sys.version, sys.platform, cprt))\n        else:\n            self.write(\"%s\\n\" % str(banner))\n        self.run_commands()", "response": "Run an interactive session."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nstarts a new magic IPython shell.", "response": "def start_ipython_player(commands, speed=1):\n    \"\"\"Starts a new magic IPython shell.\"\"\"\n    PlayerTerminalIPythonApp.commands = commands\n    PlayerTerminalIPythonApp.speed = speed\n    PlayerTerminalIPythonApp.launch_instance()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nhandles the magictyping when a key is pressed", "response": "def on_feed_key(self, key_press):\n        \"\"\"Handles the magictyping when a key is pressed\"\"\"\n        if key_press.key in {Keys.Escape, Keys.ControlC}:\n            echo(carriage_return=True)\n            raise Abort()\n        if key_press.key == Keys.Backspace:\n            if self.current_command_pos > 0:\n                self.current_command_pos -= 1\n            return key_press\n        ret = None\n        if key_press.key != Keys.CPRResponse:\n            if self.current_command_pos < len(self.current_command):\n                current_key = self.current_command_key\n                ret = KeyPress(current_key)\n                increment = min(\n                    [self.speed, len(self.current_command) - self.current_command_pos]\n                )\n                self.current_command_pos += increment\n            else:\n                # Command is finished, wait for Enter\n                if key_press.key != Keys.Enter:\n                    return None\n                self.current_command_index += 1\n                self.current_command_pos = 0\n                ret = key_press\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ninitializing the InteractiveShell instance", "response": "def init_shell(self):\n        \"\"\"initialize the InteractiveShell instance\"\"\"\n        self.shell = PlayerTerminalInteractiveShell.instance(\n            commands=self.commands,\n            speed=self.speed,\n            parent=self,\n            display_banner=False,\n            profile_dir=self.profile_dir,\n            ipython_dir=self.ipython_dir,\n            user_ns=self.user_ns,\n        )\n        self.shell.configurables.append(self)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nenabling terminal raw mode during the context manager.", "response": "def raw_mode():\n    \"\"\"\n    Enables terminal raw mode during the context.\n\n    Note: Currently noop for Windows systems.\n\n    Usage: ::\n\n        with raw_mode():\n            do_some_stuff()\n    \"\"\"\n    if WIN:\n        # No implementation for windows yet.\n        yield  # needed for the empty context manager to work\n    else:\n        #  imports are placed here because this will fail under Windows\n        import tty\n        import termios\n\n        if not isatty(sys.stdin):\n            f = open(\"/dev/tty\")\n            fd = f.fileno()\n        else:\n            fd = sys.stdin.fileno()\n            f = None\n        try:\n            old_settings = termios.tcgetattr(fd)\n            tty.setraw(fd)\n        except termios.error:\n            pass\n        try:\n            yield\n        finally:\n            # this block sets the terminal to sane mode again,\n            # also in case an exception occured in the context manager\n            try:\n                termios.tcsetattr(fd, termios.TCSADRAIN, old_settings)\n                # sys.stdout.flush()  # not needed I think.\n                if f is not None:\n                    f.close()\n            except termios.error:\n                pass"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting a number to a string using the given alphabet.", "response": "def int_to_string(number, alphabet, padding=None):\n    \"\"\"\n    Convert a number to a string, using the given alphabet.\n    The output has the most significant digit first.\n    \"\"\"\n    output = \"\"\n    alpha_len = len(alphabet)\n    while number:\n        number, digit = divmod(number, alpha_len)\n        output += alphabet[digit]\n    if padding:\n        remainder = max(padding - len(output), 0)\n        output = output + alphabet[0] * remainder\n    return output[::-1]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef string_to_int(string, alphabet):\n    number = 0\n    alpha_len = len(alphabet)\n    for char in string:\n        number = number * alpha_len + alphabet.index(char)\n    return number", "response": "Convert a string to a number using the given alphabet."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nencodes a UUID into a string according to the alphabet and length.", "response": "def encode(self, uuid, pad_length=None):\n        \"\"\"\n        Encode a UUID into a string (LSB first) according to the alphabet\n\n        If leftmost (MSB) bits are 0, the string might be shorter.\n        \"\"\"\n        if pad_length is None:\n            pad_length = self._length\n        return int_to_string(uuid.int, self._alphabet, padding=pad_length)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndecoding a string according to the current alphabet into a UUID object", "response": "def decode(self, string, legacy=False):\n        \"\"\"\n        Decode a string according to the current alphabet into a UUID\n        Raises ValueError when encountering illegal characters\n        or a too-long string.\n\n        If string too short, fills leftmost (MSB) bits with 0.\n\n        Pass `legacy=True` if your UUID was encoded with a ShortUUID version\n        prior to 0.6.0.\n        \"\"\"\n        if legacy:\n            string = string[::-1]\n        return _uu.UUID(int=string_to_int(string, self._alphabet))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate and return a cryptographically - secure short random string", "response": "def random(self, length=None):\n        \"\"\"\n        Generate and return a cryptographically-secure short random string\n        of the specified length.\n        \"\"\"\n        if length is None:\n            length = self._length\n\n        random_num = int(binascii.b2a_hex(os.urandom(length)), 16)\n        return int_to_string(random_num, self._alphabet, padding=length)[:length]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_alphabet(self, alphabet):\n\n        # Turn the alphabet into a set and sort it to prevent duplicates\n        # and ensure reproducibility.\n        new_alphabet = list(sorted(set(alphabet)))\n        if len(new_alphabet) > 1:\n            self._alphabet = new_alphabet\n            self._alpha_len = len(self._alphabet)\n        else:\n            raise ValueError(\"Alphabet with more than \" \"one unique symbols required.\")", "response": "Set the alphabet to be used for new UUIDs."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef encoded_length(self, num_bytes=16):\n        factor = math.log(256) / math.log(self._alpha_len)\n        return int(math.ceil(factor * num_bytes))", "response": "Returns the length of the shortened UUID."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nassembles a list of expressions into a LLVM module.", "response": "def asm_module(exprs, dst_reg, sym_to_reg, triple_or_target=None):\n    '''\n    Generate an LLVM module for a list of expressions\n\n    Arguments:\n      * See :meth:`arybo.lib.exprs_asm.asm_binary` for a description of the list of arguments\n\n    Output:\n      * An LLVM module with one function named \"__arybo\", containing the\n        translated expression.\n\n    See :meth:`arybo.lib.exprs_asm.asm_binary` for an usage example.\n    '''\n\n    if not llvmlite_available:\n        raise RuntimeError(\"llvmlite module unavailable! can't assemble...\")\n\n    target = llvm_get_target(triple_or_target)\n\n    M = ll.Module()\n    fntype = ll.FunctionType(ll.VoidType(), [])\n    func = ll.Function(M, fntype, name='__arybo')\n    func.attributes.add(\"naked\")\n    func.attributes.add(\"nounwind\")\n    BB = func.append_basic_block()\n\n    IRB = ll.IRBuilder()\n    IRB.position_at_end(BB)\n\n    sym_to_value = {sym: IRB.load_reg(IntType(reg[1]), reg[0], reg[0]) for sym,reg in six.iteritems(sym_to_reg)}\n\n    ret = to_llvm_ir(exprs, sym_to_value, IRB)\n    IRB.store_reg(ret, IntType(dst_reg[1]), dst_reg[0])\n    # See https://llvm.org/bugs/show_bug.cgi?id=15806\n    IRB.unreachable()\n\n    return M"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nassembles a list of expressions into a binary stream of the same architecture as the given target.", "response": "def asm_binary(exprs, dst_reg, sym_to_reg, triple_or_target=None):\n    '''\n    Compile and assemble an expression for a given architecture.\n\n    Arguments:\n      * *exprs*: list of expressions to convert. This can represent a graph of\n        expressions.\n      * *dst_reg*: final register on which to store the result of the last\n        expression. This is represented by a tuple (\"reg_name\", reg_size_bits).\n        Example: (\"rax\", 64)\n      * *sym_to_reg*: a dictionnary that maps Arybo variable name to registers\n        (described as tuple, see *dst_reg*). Example: {\"x\": (\"rdi\",64), \"y\": (\"rsi\", 64)}\n      * *triple_or_target*: LLVM architecture triple to use. Use by default the\n        host architecture. Example: \"x86_64-unknown-unknown\"\n\n    Output:\n      * binary stream of the assembled expression for the given target\n\n    Here is an example that will compile and assemble \"x+y\" for x86_64::\n\n        from arybo.lib import MBA \n        from arybo.lib import mba_exprs\n        from arybo.lib.exprs_asm import asm_binary\n        mba = MBA(64)\n        x = mba.var(\"x\")\n        y = mba.var(\"y\")\n        e = mba_exprs.ExprBV(x) + mba_exprs.ExprBV(y)\n        code = asm_binary([e], (\"rax\", 64), {\"x\": (\"rdi\", 64), \"y\": (\"rsi\", 64)}, \"x86_64-unknown-unknown\")\n        print(code.hex())\n\n    which outputs ``488d0437`` (which is equivalent to ``lea rax,[rdi+rsi*1]``).\n    '''\n    if not llvmlite_available:\n        raise RuntimeError(\"llvmlite module unavailable! can't assemble...\")\n\n    target = llvm_get_target(triple_or_target)\n    M = asm_module(exprs, dst_reg, sym_to_reg, target)\n\n    # Use LLVM to compile the '__arybo' function. As the function is naked and\n    # is the only, we just got to dump the .text section to get the binary\n    # assembly.\n    # No need for keystone or whatever hype stuff. llvmlite does the job.\n\n    M = llvm.parse_assembly(str(M))\n    M.verify()\n    target_machine = target.create_target_machine()\n    obj_bin = target_machine.emit_object(M)\n    obj = llvm.ObjectFileRef.from_data(obj_bin)\n    for s in obj.sections():\n        if s.is_text():\n            return s.data()\n    raise RuntimeError(\"unable to get the assembled binary!\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns true if o is in e", "response": "def expr_contains(e, o):\n    ''' Returns true if o is in e '''\n    if o == e:\n        return True\n    if e.has_args():\n        for a in e.args():\n            if expr_contains(a, o):\n                return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nzero - extend the variable to n bits.", "response": "def zext(self, n):\n        ''' Zero-extend the variable to n bits.\n        \n        n bits must be stricly larger than the actual number of bits, or a\n        ValueError is thrown\n        '''\n\n        if n <= self.nbits:\n            raise ValueError(\"n must be > %d bits\" % self.nbits)\n\n        mba_ret = self.__new_mba(n)\n        ret = mba_ret.from_cst(0)\n        for i in range(self.nbits):\n            ret.vec[i] = self.vec[i]\n        return mba_ret.from_vec(ret)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sext(self, n):\n        ''' Sign-extend the variable to n bits.\n        \n        n bits must be stricly larger than the actual number of bits, or a\n        ValueError is thrown\n        '''\n\n        if n <= self.nbits:\n            raise ValueError(\"n must be > %d bits\" % self.nbits)\n\n        mba_ret = self.__new_mba(n)\n        ret = mba_ret.from_cst(0)\n        for i in range(self.nbits):\n            ret.vec[i] = self.vec[i]\n        last_bit = self.vec[self.nbits-1]\n        for i in range(self.nbits,n):\n            ret.vec[i] = last_bit\n        return mba_ret.from_vec(ret)", "response": "Sign - extend the variable to n bits."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef evaluate(self, values):\n        ''' Evaluates the expression to an integer\n\n        values is a dictionnary that associates n-bit variables to integer\n        values. Every symbolic variables used in the expression must be\n        represented. \n\n        For instance, let x and y 4-bit variables, and e = x+y:\n\n        >>> mba = MBA(4)\n        >>> x = mba.var('x')\n        >>> y = mba.var('y')\n        >>> e = x+y\n\n        To evaluate e with x=4 and y=5, we can do:\n\n        >>> e.eval({x: 4, y: 5})\n        9\n\n        If a variable is missing from values, an exception will occur. (x\n        or y in the example above)\n        '''\n        ret = self.mba.evaluate(self.vec, values)\n        if isinstance(ret, six.integer_types):\n            return ret\n        return self.from_vec(self.mba, ret)", "response": "Evaluates the expression to an integer sequence of n - bit variables and returns the result."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes the vectorial decomposition of the expression according to the given symbols.", "response": "def vectorial_decomp(self, symbols):\n        ''' Compute the vectorial decomposition of the expression according to the given symbols.\n\n        symbols is a list that represents the input of the resulting\n        application. They are considerated as a flatten vector of bits.\n\n        Args:\n            symbols: TODO\n\n        Returns:\n            An :class:`pytanque.App` object\n\n        Example:\n            >>> mba = MBA(4)\n            >>> x = mba.var('x')\n            >>> y = mba.var('y')\n            >>> e = x^y^6\n            >>> e.vectorial_decomp([x,y])\n            App NL = Vec([\n            0,\n            0,\n            0,\n            0\n            ])\n            AffApp matrix = Mat([\n                [1, 0, 0, 0, 1, 0, 0, 0]\n                [0, 1, 0, 0, 0, 1, 0, 0]\n                [0, 0, 1, 0, 0, 0, 1, 0]\n                [0, 0, 0, 1, 0, 0, 0, 1]\n                ])\n            AffApp cst = Vec([\n                0,\n                1,\n                1,\n                0\n                ])\n        '''\n        try:\n            symbols = [s.vec for s in symbols]\n            N = sum(map(lambda s: len(s), symbols))\n            symbols_ = Vector(N)\n            i = 0\n            for v in symbols:\n                for s in v:\n                    symbols_[i] = s\n                    i += 1\n            symbols = symbols_\n        except TypeError: pass\n        return self.mba.vectorial_decomp(symbols, self.vec)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef var(self, name):\n        ''' Get an n-bit named symbolic variable\n\n        Returns:\n            An :class:`MBAVariable` object representing a symbolic variable\n\n        Example:\n            >>> mba.var('x')\n            Vec([\n            x0,\n            x1,\n            x2,\n            x3\n            ])\n        '''\n\n        ret = self.from_vec(self.var_symbols(name)) \n        ret.name = name\n        return ret", "response": "Get an n - bit named symbolic variable."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef permut2expr(self, P):\n        ''' Convert a substitution table into an arybo application\n\n        Args:\n            P: list of integers. The list must not contain more than 2**nbits elements.\n\n        Returns:\n            A tuple containing an :class:`MBAVariable` object with the result\n            and the symbolic input variable used in this object. A typical use\n            case is to feed these into vectorial_decomp.\n\n        Example:\n            >>> mba = MBA(4)\n            >>> P = [i^7 for i in range(16)]\n            >>> E,X = mba.permut2expr(P)\n            >>> E.vectorial_decomp([X])\n            App NL = Vec([\n            0,\n            0,\n            0,\n            0\n            ])\n            AffApp matrix = Mat([\n            [1, 0, 0, 0]\n            [0, 1, 0, 0]\n            [0, 0, 1, 0]\n            [0, 0, 0, 1]\n            ])\n            AffApp cst = Vec([\n            1,\n            1,\n            1,\n            0\n            ])\n        '''\n        if len(P) > (1<<self.nbits):\n            raise ValueError(\"P must not contain more than %d elements\" % (1<<self.nbits))\n        X = self.var('X')\n        ret = super(MBA, self).permut2expr(P, X.vec)\n        return self.from_vec(ret), X", "response": "Convert a substitution table into an arybo application of the same type."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting a subset of Triton s AST into Arybo s representation.", "response": "def tritonast2arybo(e, use_exprs=True, use_esf=False, context=None):\n    ''' Convert a subset of Triton's AST into Arybo's representation\n\n    Args:\n        e: Triton AST\n        use_esf: use ESFs when creating the final expression\n        context: dictionnary that associates Triton expression ID to arybo expressions\n\n    Returns:\n        An :class:`arybo.lib.MBAVariable` object\n    '''\n\n    children_ = e.getChildren()\n    children = (tritonast2arybo(c,use_exprs,use_esf,context) for c in children_)\n    reversed_children = (tritonast2arybo(c,use_exprs,use_esf,context) for c in reversed(children_))\n\n    Ty = e.getType()\n    if Ty == TAstN.ZX:\n        n = next(children)\n        v = next(children)\n        n += v.nbits\n        if n == v.nbits:\n            return v\n        return v.zext(n)\n    if Ty == TAstN.SX:\n        n = next(children)\n        v = next(children)\n        n += v.nbits\n        if n == v.nbits:\n            return v\n        return v.sext(n)\n    if Ty == TAstN.INTEGER:\n        return e.getInteger()\n    if Ty == TAstN.BV:\n        cst = next(children)\n        nbits = next(children)\n        if use_exprs:\n            return EX.ExprCst(cst, nbits)\n        else:\n            return _get_mba(nbits,use_esf).from_cst(cst)\n    if Ty == TAstN.EXTRACT:\n        last = next(children)\n        first = next(children)\n        v = next(children)\n        return v[first:last+1]\n    if Ty == TAstN.CONCAT:\n        if use_exprs:\n            return EX.ExprConcat(*list(reversed_children))\n        else:\n            return flatten(reversed_children)\n    if Ty == TAstN.VARIABLE:\n        name = e.getSymbolicVariable().getName()\n        ret = _get_mba(e.getBitvectorSize(),use_esf).var(name)\n        if use_exprs:\n            ret = EX.ExprBV(ret)\n        return ret\n    if Ty == TAstN.REFERENCE:\n        if context is None:\n            raise ValueError(\"reference node without context can't be resolved\")\n        id_ = e.getSymbolicExpression().getId()\n        ret = context.get(id_, None)\n        if ret is None:\n            raise ValueError(\"expression id %d not found in context\" % id_)\n        return ret\n    if Ty == TAstN.LET:\n        # Alias\n        # djo: \"c'est pas utilise osef\"\n        raise ValueError(\"unsupported LET operation\")\n\n    # Logical/arithmetic shifts\n    shifts = {\n        TAstN.BVASHR: lambda a,b: a.ashr(b),\n        TAstN.BVLSHR: lambda a,b: a.lshr(b),\n        TAstN.BVSHL:  operator.lshift,\n        TAstN.BVROL:  lambda x,n: x.rol(n),\n        TAstN.BVROR:  lambda x,n: x.ror(n)\n    }\n    shift = shifts.get(Ty, None)\n    if not shift is None:\n        v = next(children)\n        n = next(children)\n        return shift(v,n)\n\n    # Unary op\n    unops = {\n        TAstN.BVNOT: lambda x: ~x,\n        TAstN.LNOT:  lambda x: ~x,\n        TAstN.BVNEG: operator.neg\n    }\n    unop = unops.get(Ty, None)\n    if not unop is None:\n        return unop(next(children))\n\n    binops = {\n        TAstN.BVADD:  operator.add,\n        TAstN.BVSUB:  operator.sub,\n        TAstN.BVAND:  operator.and_,\n        TAstN.BVOR:   operator.or_,\n        TAstN.BVXOR:  operator.xor,\n        TAstN.BVMUL:  operator.mul,\n        TAstN.BVNAND: lambda x,y: ~(x&y),\n        TAstN.BVNOR:  lambda x,y: ~(x|y),\n        TAstN.BVXNOR: lambda x,y: ~(x^y),\n        TAstN.BVUDIV: lambda x,y: x.udiv(y),\n        TAstN.BVSDIV: lambda x,y: x.sdiv(y),\n        TAstN.BVUREM: lambda x,y: x.urem(y),\n        TAstN.BVSREM: lambda x,y: x.srem(y),\n        TAstN.LAND:   operator.and_,\n        TAstN.LOR:    operator.or_\n    }\n    binop = binops.get(Ty, None)\n    if not binop is None:\n        return reduce(binop, children)\n\n    # Logical op\n    lops = {\n        TAstN.EQUAL:    lambda x,y: EX.ExprCmpEq(x,y),\n        TAstN.DISTINCT: lambda x,y: EX.ExprCmpNeq(x,y),\n        TAstN.BVUGE:    lambda x,y: EX.ExprCmpGte(x,y,False),\n        TAstN.BVUGT:    lambda x,y: EX.ExprCmpGt(x,y,False),\n        TAstN.BVULE:    lambda x,y: EX.ExprCmpLte(x,y,False),\n        TAstN.BVULT:    lambda x,y: EX.ExprCmpLt(x,y,False),\n        TAstN.BVSGE:    lambda x,y: EX.ExprCmpGte(x,y,True),\n        TAstN.BVSGT:    lambda x,y: EX.ExprCmpGt(x,y,True),\n        TAstN.BVSLE:    lambda x,y: EX.ExprCmpLte(x,y,True),\n        TAstN.BVSLT:    lambda x,y: EX.ExprCmpLt(x,y,True)\n    }\n    lop = lops.get(Ty, None)\n    if not lop is None:\n        return reduce(lop, children)\n\n    # Conditional\n    if Ty != TAstN.ITE:\n        raise ValueError(\"unsupported node type %s\" % str(Ty))\n    return EX.ExprCond(next(children), next(children), next(children))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef response_hook(self, r, **kwargs):\n        if r.status_code == 401:\n            # Handle server auth.\n            www_authenticate = r.headers.get('www-authenticate', '').lower()\n            auth_type = _auth_type_from_header(www_authenticate)\n\n            if auth_type is not None:\n                return self.retry_using_http_NTLM_auth(\n                    'www-authenticate',\n                    'Authorization',\n                    r,\n                    auth_type,\n                    kwargs\n                )\n        elif r.status_code == 407:\n            # If we didn't have server auth, do proxy auth.\n            proxy_authenticate = r.headers.get(\n                'proxy-authenticate', ''\n            ).lower()\n            auth_type = _auth_type_from_header(proxy_authenticate)\n            if auth_type is not None:\n                return self.retry_using_http_NTLM_auth(\n                    'proxy-authenticate',\n                    'Proxy-authorization',\n                    r,\n                    auth_type,\n                    kwargs\n                )\n\n        return r", "response": "The actual hook handler."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the server certificate at the request_url and return it as a hash.", "response": "def _get_server_cert(self, response):\n        \"\"\"\n        Get the certificate at the request_url and return it as a hash. Will get the raw socket from the\n        original response from the server. This socket is then checked if it is an SSL socket and then used to\n        get the hash of the certificate. The certificate hash is then used with NTLMv2 authentication for\n        Channel Binding Tokens support. If the raw object is not a urllib3 HTTPReponse (default with requests)\n        then no certificate will be returned.\n\n        :param response: The original 401 response from the server\n        :return: The hash of the DER encoded certificate at the request_url or None if not a HTTPS endpoint\n        \"\"\"\n        if self.send_cbt:\n            certificate_hash = None\n            raw_response = response.raw\n\n            if isinstance(raw_response, HTTPResponse):\n                if sys.version_info > (3, 0):\n                    socket = raw_response._fp.fp.raw._sock\n                else:\n                    socket = raw_response._fp.fp._sock\n\n                try:\n                    server_certificate = socket.getpeercert(True)\n                except AttributeError:\n                    pass\n                else:\n                    certificate_hash = _get_certificate_hash(server_certificate)\n            else:\n                warnings.warn(\n                    \"Requests is running with a non urllib3 backend, cannot retrieve server certificate for CBT\",\n                    NoCertificateRetrievedWarning)\n\n            return certificate_hash\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dummy(DF, cols=None):\n\tdummies = (get_dummies(DF[col]) for col in \n\t\t(DF.columns if cols is None else cols))\n\treturn concat(dummies, axis=1, keys=DF.columns)", "response": "Dummy code select columns of a DataFrame."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the row factor scores of the rank - reduced representation.", "response": "def fs_r(self, percent=0.9, N=None):\n\t\t\"\"\"Get the row factor scores (dimensionality-reduced representation),\n\t\tchoosing how many factors to retain, directly or based on the explained\n\t\tvariance.\n\n\t\t'percent': The minimum variance that the retained factors are required\n\t\t\t\t\t\t\t\tto explain (default: 90% = 0.9)\n\t\t'N': The number of factors to retain. Overrides 'percent'.\n\t\t\t\tIf the rank is less than N, N is ignored.\n\t\t\"\"\"\n\t\tif not 0 <= percent <= 1:\n\t\t\traise ValueError(\"Percent should be a real number between 0 and 1.\")\n\t\tif N:\n\t\t\tif not isinstance(N, (int, int64)) or N <= 0:\n\t\t\t\traise ValueError(\"N should be a positive integer.\")\n\t\t\tN = min(N, self.rank)\n\t\tself.k = 1 + flatnonzero(cumsum(self.L) >= sum(self.L)*percent)[0]\n\t\t#  S = zeros((self._numitems, self.k))\n\t\t# the sign of the square root can be either way; singular value vs. eigenvalue\n\t\t# fill_diagonal(S, -sqrt(self.E) if self.cor else self.s)\n\t\tnum2ret = N if N else self.k\n\t\ts = -sqrt(self.L) if self.cor else self.s\n\t\tS = diagsvd(s[:num2ret], self._numitems, num2ret)\n\t\tself.F = self.D_r.dot(self.P).dot(S)\n\t\treturn self.F"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the column factor scores of the given rank.", "response": "def fs_c(self, percent=0.9, N=None):\n\t\t\"\"\"Get the column factor scores (dimensionality-reduced representation),\n\t\tchoosing how many factors to retain, directly or based on the explained\n\t\tvariance.\n\n\t\t'percent': The minimum variance that the retained factors are required\n\t\t\t\t\t\t\t\tto explain (default: 90% = 0.9)\n\t\t'N': The number of factors to retain. Overrides 'percent'.\n\t\t\t\tIf the rank is less than N, N is ignored.\n\t\t\"\"\"\n\t\tif not 0 <= percent <= 1:\n\t\t\traise ValueError(\"Percent should be a real number between 0 and 1.\")\n\t\tif N:\n\t\t\tif not isinstance(N, (int, int64)) or N <= 0:\n\t\t\t\traise ValueError(\"N should be a positive integer.\")\n\t\t\tN = min(N, self.rank)  # maybe we should notify the user?\n\t\t\t# S = zeros((self._numitems, N))\n\t\t# else:\n\t\tself.k = 1 + flatnonzero(cumsum(self.L) >= sum(self.L)*percent)[0]\n\t\t#  S = zeros((self._numitems, self.k))\n\t\t# the sign of the square root can be either way; singular value vs. eigenvalue\n\t\t# fill_diagonal(S, -sqrt(self.E) if self.cor else self.s)\n\t\tnum2ret = N if N else self.k\n\t\ts = -sqrt(self.L) if self.cor else self.s\n\t\tS = diagsvd(s[:num2ret], len(self.Q), num2ret)\n\t\tself.G = _mul(self.D_c, self.Q.T, S)  # important! note the transpose on Q\n\t\treturn self.G"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the squared cosines for each row.", "response": "def cos_r(self, N=None):  # percent=0.9\n\t\t\"\"\"Return the squared cosines for each row.\"\"\"\n\n\t\tif not hasattr(self, 'F') or self.F.shape[1] < self.rank:\n\t\t\tself.fs_r(N=self.rank)  # generate F\n\t\tself.dr = norm(self.F, axis=1)**2\n\t\t# cheaper than diag(self.F.dot(self.F.T))?\n\n\t\treturn apply_along_axis(lambda _: _/self.dr, 0, self.F[:, :N]**2)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the squared cosines for each column.", "response": "def cos_c(self, N=None):  # percent=0.9,\n\t\t\"\"\"Return the squared cosines for each column.\"\"\"\n\n\t\tif not hasattr(self, 'G') or self.G.shape[1] < self.rank:\n\t\t\tself.fs_c(N=self.rank)  # generate\n\t\tself.dc = norm(self.G, axis=1)**2\n\t\t# cheaper than diag(self.G.dot(self.G.T))?\n\n\t\treturn apply_along_axis(lambda _: _/self.dc, 0, self.G[:, :N]**2)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the contribution of each row.", "response": "def cont_r(self, percent=0.9, N=None):\n\t\t\"\"\"Return the contribution of each row.\"\"\"\n\n\t\tif not hasattr(self, 'F'):\n\t\t\tself.fs_r(N=self.rank)  # generate F\n\t\treturn apply_along_axis(lambda _: _/self.L[:N], 1,\n\t\t\t   apply_along_axis(lambda _: _*self.r, 0, self.F[:, :N]**2))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cont_c(self, percent=0.9, N=None):  # bug? check axis number 0 vs 1 here\n\n\t\tif not hasattr(self, 'G'):\n\t\t\tself.fs_c(N=self.rank)  # generate G\n\t\treturn apply_along_axis(lambda _: _/self.L[:N], 1,\n\t\t\t   apply_along_axis(lambda _: _*self.c, 0, self.G[:, :N]**2))", "response": "Return the contribution of each column."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef expl_var(self, greenacre=True, N=None):\n\t\tif greenacre:\n\t\t\tgreenacre_inertia = (self.K / (self.K - 1.) * (sum(self.s**4)\n\t\t\t\t\t\t\t\t - (self.J - self.K) / self.K**2.))\n\t\t\treturn (self._benzecri() / greenacre_inertia)[:N]\n\t\telse:\n\t\t\tE = self._benzecri() if self.cor else self.s**2\n\t\t\treturn (E / sum(E))[:N]", "response": "proportion of explained inertia ( variance ) for each factor. N = None if no factor is specified"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfinding the supplementary row factor scores.", "response": "def fs_r_sup(self, DF, N=None):\n\t\t\"\"\"Find the supplementary row factor scores.\n\n\t\tncols: The number of singular vectors to retain.\n\t\tIf both are passed, cols is given preference.\n\t\t\"\"\"\n\t\tif not hasattr(self, 'G'):\n\t\t\tself.fs_c(N=self.rank)  # generate G\n\n\t\tif N and (not isinstance(N, int) or N <= 0):\n\t\t\traise ValueError(\"ncols should be a positive integer.\")\n\t\ts = -sqrt(self.E) if self.cor else self.s\n\t\tN = min(N, self.rank) if N else self.rank\n\t\tS_inv = diagsvd(-1/s[:N], len(self.G.T), N)\n\t\t# S = diagsvd(s[:N], len(self.tau), N)\n\t\treturn _mul(DF.div(DF.sum(axis=1), axis=0), self.G, S_inv)[:, :N]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfinding the supplementary column factor scores.", "response": "def fs_c_sup(self, DF, N=None):\n\t\t\"\"\"Find the supplementary column factor scores.\n\n\t\tncols: The number of singular vectors to retain.\n\t\tIf both are passed, cols is given preference.\n\t\t\"\"\"\n\t\tif not hasattr(self, 'F'):\n\t\t\tself.fs_r(N=self.rank)  # generate F\n\n\t\tif N and (not isinstance(N, int) or N <= 0):\n\t\t\traise ValueError(\"ncols should be a positive integer.\")\n\t\ts = -sqrt(self.E) if self.cor else self.s\n\t\tN = min(N, self.rank) if N else self.rank\n\t\tS_inv = diagsvd(-1/s[:N], len(self.F.T), N)\n\t\t# S = diagsvd(s[:N], len(self.tau), N)\n\t\treturn _mul((DF/DF.sum()).T, self.F, S_inv)[:, :N]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef data_recognise(self, data=None):\n        data = data or self.data\n        data_lower = data.lower()\n        if data_lower.startswith(u\"http://\") or data_lower.startswith(u\"https://\"):\n            return u'url'\n        elif data_lower.startswith(u\"mailto:\"):\n            return u'email'\n        elif data_lower.startswith(u\"matmsg:to:\"):\n            return u'emailmessage'\n        elif data_lower.startswith(u\"tel:\"):\n            return u'telephone'\n        elif data_lower.startswith(u\"smsto:\"):\n            return u'sms'\n        elif data_lower.startswith(u\"mmsto:\"):\n            return u'mms'\n        elif data_lower.startswith(u\"geo:\"):\n            return u'geo'\n        elif data_lower.startswith(u\"mebkm:title:\"):\n            return u'bookmark'\n        elif data_lower.startswith(u\"mecard:\"):\n            return u'phonebook'\n        else:\n            return u'text'", "response": "Returns an unicode string indicating the data type of the data paramater"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef data_to_string(self):\n        # FIX-ME: if we don't add the BOM_UTF8 char, QtQR doesn't decode\n        # correctly; but if we add it, mobile apps don't.-\n        # Apparently is a zbar bug.\n        if self.data_type == 'text':\n            return BOM_UTF8 + self.__class__.data_encode[self.data_type](self.data).encode('utf-8')\n        else:\n            return self.__class__.data_encode[self.data_type](self.data).encode('utf-8')", "response": "Returns a UTF8 string with the QR Code s data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef legend_scaler(legend_values, max_labels=10.0):\n    if len(legend_values) < max_labels:\n        legend_ticks = legend_values\n    else:\n        spacer = int(math.ceil(len(legend_values)/max_labels))\n        legend_ticks = []\n        for i in legend_values[::spacer]:\n            legend_ticks += [i]\n            legend_ticks += ['']*(spacer-1)\n    return legend_ticks", "response": "Scales the legend values so that there isn t a collision\n   ."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef linear_gradient(hexList, nColors):\n    def _scale(start, finish, length, i):\n        \"\"\"\n        Return the value correct value of a number that is in between start\n        and finish, for use in a loop of length *length*.\n\n        \"\"\"\n        base = 16\n\n        fraction = float(i) / (length - 1)\n        raynge = int(finish, base) - int(start, base)\n        thex = hex(int(int(start, base) + fraction * raynge)).split('x')[-1]\n        if len(thex) != 2:\n            thex = '0' + thex\n        return thex\n\n    allColors = []\n    # Separate (R, G, B) pairs.\n    for start, end in zip(hexList[:-1], hexList[1:]):\n        # Linearly intepolate between pair of hex ###### values and\n        # add to list.\n        nInterpolate = 765\n        for index in range(nInterpolate):\n            r = _scale(start[1:3], end[1:3], nInterpolate, index)\n            g = _scale(start[3:5], end[3:5], nInterpolate, index)\n            b = _scale(start[5:7], end[5:7], nInterpolate, index)\n            allColors.append(''.join(['#', r, g, b]))\n\n    # Pick only nColors colors from the total list.\n    result = []\n    for counter in range(nColors):\n        fraction = float(counter) / (nColors - 1)\n        index = int(fraction * (len(allColors) - 1))\n        result.append(allColors[index])\n    return result", "response": "This function will take a list of hexcode values and will return a list of length nColors where the colors are linearly interpolated between the two colors."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef color_brewer(color_code, n=6):\n    maximum_n = 253\n    minimum_n = 3\n\n    # Raise an error if the n requested is greater than the maximum.\n    if n > maximum_n:\n        raise ValueError('The maximum number of colors in a'\n                         ' ColorBrewer sequential color series is 253')\n    if n < minimum_n:\n        raise ValueError('The minimum number of colors in a'\n                         ' ColorBrewer sequential color series is 3')\n\n    if not isinstance(color_code, str):\n        raise ValueError('color should be a string, not a {}.'\n                         .format(type(color_code)))\n    if color_code[-2:] == '_r':\n        base_code = color_code[:-2]\n        core_color_code = base_code + '_' + str(n).zfill(2)\n        color_reverse = True\n    else:\n        base_code = color_code\n        core_color_code = base_code + '_' + str(n).zfill(2)\n        color_reverse = False\n\n    with open(os.path.join(rootpath, '_schemes.json')) as f:\n        schemes = json.loads(f.read())\n\n    with open(os.path.join(rootpath, '_cnames.json')) as f:\n        scheme_info = json.loads(f.read())\n\n    with open(os.path.join(rootpath, 'scheme_base_codes.json')) as f:\n        core_schemes = json.loads(f.read())['codes']\n\n    if base_code not in core_schemes:\n        raise ValueError(base_code + ' is not a valid ColorBrewer code')\n\n    try:\n        schemes[core_color_code]\n        explicit_scheme = True\n    except KeyError:\n        explicit_scheme = False\n\n    # Only if n is greater than the scheme length do we interpolate values.\n    if not explicit_scheme:\n        # Check to make sure that it is not a qualitative scheme.\n        if scheme_info[base_code] == 'Qualitative':\n            matching_quals = []\n            for key in schemes:\n                if base_code + '_' in key:\n                    matching_quals.append(int(key.split('_')[1]))\n\n            raise ValueError('Expanded color support is not available'\n                             ' for Qualitative schemes; restrict the'\n                             ' number of colors for the ' + base_code +\n                             ' code to between ' + str(min(matching_quals)) +\n                             ' and ' + str(max(matching_quals))\n                             )\n        else:\n            if not color_reverse:\n                color_scheme = linear_gradient(schemes.get(core_color_code), n)\n            else:\n                color_scheme = linear_gradient(schemes.get(core_color_code)[::-1], n)\n    else:\n        if not color_reverse:\n            color_scheme = schemes.get(core_color_code, None)\n        else:\n            color_scheme = schemes.get(core_color_code, None)[::-1]\n    return color_scheme", "response": "Generate a colorbrewer color scheme of length len type scheme."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef split_six(series=None):\n    if pd is None:\n        raise ImportError('The Pandas package is required'\n                          ' for this functionality')\n    if np is None:\n        raise ImportError('The NumPy package is required'\n                          ' for this functionality')\n\n    def base(x):\n        if x > 0:\n            base = pow(10, math.floor(math.log10(x)))\n            return round(x/base)*base\n        else:\n            return 0\n\n    quants = [0, 50, 75, 85, 90]\n    # Some weirdness in series quantiles a la 0.13.\n    arr = series.values\n    return [base(np.percentile(arr, x)) for x in quants]", "response": "Returns a list of integers that are in a single order - of - magnitude domain."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntakes a string file or array - like object and transforms it into a URL.", "response": "def image_to_url(image, colormap=None, origin='upper'):\n    \"\"\"Infers the type of an image argument and transforms it into a URL.\n\n    Parameters\n    ----------\n    image: string, file or array-like object\n        * If string, it will be written directly in the output file.\n        * If file, it's content will be converted as embedded in the\n          output file.\n        * If array-like, it will be converted to PNG base64 string and\n          embedded in the output.\n    origin : ['upper' | 'lower'], optional, default 'upper'\n        Place the [0, 0] index of the array in the upper left or\n        lower left corner of the axes.\n    colormap : callable, used only for `mono` image.\n        Function of the form [x -> (r,g,b)] or [x -> (r,g,b,a)]\n        for transforming a mono image into RGB.\n        It must output iterables of length 3 or 4, with values between\n        0. and 1.  Hint : you can use colormaps from `matplotlib.cm`.\n    \"\"\"\n    if hasattr(image, 'read'):\n        # We got an image file.\n        if hasattr(image, 'name'):\n            # We try to get the image format from the file name.\n            fileformat = image.name.lower().split('.')[-1]\n        else:\n            fileformat = 'png'\n        url = 'data:image/{};base64,{}'.format(\n            fileformat, base64.b64encode(image.read()).decode('utf-8'))\n    elif (not (isinstance(image, text_type) or\n               isinstance(image, binary_type))) and hasattr(image, '__iter__'):\n        # We got an array-like object.\n        png = write_png(image, origin=origin, colormap=colormap)\n        url = 'data:image/png;base64,' + base64.b64encode(png).decode('utf-8')\n    else:\n        # We got an URL.\n        url = json.loads(json.dumps(image))\n\n    return url.replace('\\n', ' ')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef write_png(data, origin='upper', colormap=None):\n    if np is None:\n        raise ImportError('The NumPy package is required'\n                          ' for this functionality')\n\n    if colormap is None:\n        def colormap(x):\n            return (x, x, x, 1)\n\n    array = np.atleast_3d(data)\n    height, width, nblayers = array.shape\n\n    if nblayers not in [1, 3, 4]:\n        raise ValueError('Data must be NxM (mono), '\n                         'NxMx3 (RGB), or NxMx4 (RGBA)')\n    assert array.shape == (height, width, nblayers)\n\n    if nblayers == 1:\n        array = np.array(list(map(colormap, array.ravel())))\n        nblayers = array.shape[1]\n        if nblayers not in [3, 4]:\n            raise ValueError('colormap must provide colors of'\n                             'length 3 (RGB) or 4 (RGBA)')\n        array = array.reshape((height, width, nblayers))\n    assert array.shape == (height, width, nblayers)\n\n    if nblayers == 3:\n        array = np.concatenate((array, np.ones((height, width, 1))), axis=2)\n        nblayers = 4\n    assert array.shape == (height, width, nblayers)\n    assert nblayers == 4\n\n    # Normalize to uint8 if it isn't already.\n    if array.dtype != 'uint8':\n        array = array * 255./array.max(axis=(0, 1)).reshape((1, 1, 4))\n        array = array.astype('uint8')\n\n    # Eventually flip the image.\n    if origin == 'lower':\n        array = array[::-1, :, :]\n\n    # Transform the array to bytes.\n    raw_data = b''.join([b'\\x00' + array[i, :, :].tobytes()\n                         for i in range(height)])\n\n    def png_pack(png_tag, data):\n        chunk_head = png_tag + data\n        return (struct.pack('!I', len(data)) +\n                chunk_head +\n                struct.pack('!I', 0xFFFFFFFF & zlib.crc32(chunk_head)))\n\n    return b''.join([\n        b'\\x89PNG\\r\\n\\x1a\\n',\n        png_pack(b'IHDR', struct.pack('!2I5B', width, height, 8, 6, 0, 0, 0)),\n        png_pack(b'IDAT', zlib.compress(raw_data, 9)),\n        png_pack(b'IEND', b'')])", "response": "Write a PNG file into a base64 - encoded byte string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\niterate over a list representing a feature and returns a list of points", "response": "def iter_points(x):\n    \"\"\"Iterates over a list representing a feature, and returns a list of points,\n    whatever the shape of the array (Point, MultiPolyline, etc).\n    \"\"\"\n    if isinstance(x, (list, tuple)):\n        if len(x):\n            if isinstance(x[0], (list, tuple)):\n                out = []\n                for y in x:\n                    out += iter_points(y)\n                return out\n            else:\n                return [x]\n        else:\n            return []\n    else:\n        raise ValueError('List/tuple type expected. Got {!r}.'.format(x))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrender the HTML representation of the element.", "response": "def render(self, **kwargs):\n        \"\"\"Renders the HTML representation of the element.\"\"\"\n        self.color_domain = [self.vmin + (self.vmax-self.vmin) * k/499. for\n                             k in range(500)]\n        self.color_range = [self.__call__(x) for x in self.color_domain]\n        self.tick_labels = legend_scaler(self.index)\n\n        super(ColorMap, self).render(**kwargs)\n\n        figure = self.get_root()\n        assert isinstance(figure, Figure), ('You cannot render this Element '\n                                            'if it is not in a Figure.')\n\n        figure.header.add_child(JavascriptLink(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\"), name='d3')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nproviding the color corresponding to value x in the form of a tuple R G B A.", "response": "def rgba_bytes_tuple(self, x):\n        \"\"\"Provides the color corresponding to value `x` in the\n        form of a tuple (R,G,B,A) with int values between 0 and 255.\n        \"\"\"\n        return tuple(int(u*255.9999) for u in self.rgba_floats_tuple(x))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprovide the color corresponding to value x in the form of a tuple ( R G B A.", "response": "def rgba_floats_tuple(self, x):\n        \"\"\"Provides the color corresponding to value `x` in the\n        form of a tuple (R,G,B,A) with float values between 0. and 1.\n        \"\"\"\n        if x <= self.index[0]:\n            return self.colors[0]\n        if x >= self.index[-1]:\n            return self.colors[-1]\n\n        i = len([u for u in self.index if u < x])  # 0 < i < n.\n        if self.index[i-1] < self.index[i]:\n            p = (x - self.index[i-1])*1./(self.index[i]-self.index[i-1])\n        elif self.index[i-1] == self.index[i]:\n            p = 1.\n        else:\n            raise ValueError('Thresholds are not sorted.')\n\n        return tuple((1.-p) * self.colors[i-1][j] + p*self.colors[i][j] for j\n                     in range(4))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsplitting the LinearColormap into a StepColormap.", "response": "def to_step(self, n=None, index=None, data=None, method=None,\n                quantiles=None, round_method=None):\n        \"\"\"Splits the LinearColormap into a StepColormap.\n\n        Parameters\n        ----------\n        n : int, default None\n            The number of expected colors in the ouput StepColormap.\n            This will be ignored if `index` is provided.\n        index : list of floats, default None\n            The values corresponding to each color bounds.\n            It has to be sorted.\n            If None, a regular grid between `vmin` and `vmax` is created.\n        data : list of floats, default None\n            A sample of data to adapt the color map to.\n        method : str, default 'linear'\n            The method used to create data-based colormap.\n            It can be 'linear' for linear scale, 'log' for logarithmic,\n            or 'quant' for data's quantile-based scale.\n        quantiles : list of floats, default None\n            Alternatively, you can provide explicitely the quantiles you\n            want to use in the scale.\n        round_method : str, default None\n            The method used to round thresholds.\n            * If 'int', all values will be rounded to the nearest integer.\n            * If 'log10', all values will be rounded to the nearest\n            order-of-magnitude integer. For example, 2100 is rounded to\n            2000, 2790 to 3000.\n\n        Returns\n        -------\n        A StepColormap with `n=len(index)-1` colors.\n\n        Examples:\n        >> lc.to_step(n=12)\n        >> lc.to_step(index=[0, 2, 4, 6, 8, 10])\n        >> lc.to_step(data=some_list, n=12)\n        >> lc.to_step(data=some_list, n=12, method='linear')\n        >> lc.to_step(data=some_list, n=12, method='log')\n        >> lc.to_step(data=some_list, n=12, method='quantiles')\n        >> lc.to_step(data=some_list, quantiles=[0, 0.3, 0.7, 1])\n        >> lc.to_step(data=some_list, quantiles=[0, 0.3, 0.7, 1],\n        ...           round_method='log10')\n\n        \"\"\"\n        msg = 'You must specify either `index` or `n`'\n        if index is None:\n            if data is None:\n                if n is None:\n                    raise ValueError(msg)\n                else:\n                    index = [self.vmin + (self.vmax-self.vmin)*i*1./n for\n                             i in range(1+n)]\n                    scaled_cm = self\n            else:\n                max_ = max(data)\n                min_ = min(data)\n                scaled_cm = self.scale(vmin=min_, vmax=max_)\n                method = ('quantiles' if quantiles is not None\n                          else method if method is not None\n                          else 'linear'\n                          )\n                if method.lower().startswith('lin'):\n                    if n is None:\n                        raise ValueError(msg)\n                    index = [min_ + i*(max_-min_)*1./n for i in range(1+n)]\n                elif method.lower().startswith('log'):\n                    if n is None:\n                        raise ValueError(msg)\n                    if min_ <= 0:\n                        msg = ('Log-scale works only with strictly '\n                               'positive values.')\n                        raise ValueError(msg)\n                    index = [math.exp(\n                        math.log(min_) + i * (math.log(max_) -\n                                              math.log(min_)) * 1./n\n                    ) for i in range(1+n)]\n                elif method.lower().startswith('quant'):\n                    if quantiles is None:\n                        if n is None:\n                            msg = ('You must specify either `index`, `n` or'\n                                   '`quantiles`.')\n                            raise ValueError(msg)\n                        else:\n                            quantiles = [i*1./n for i in range(1+n)]\n                    p = len(data)-1\n                    s = sorted(data)\n                    index = [s[int(q*p)] * (1.-(q*p) % 1) +\n                             s[min(int(q*p) + 1, p)] * ((q*p) % 1) for\n                             q in quantiles]\n                else:\n                    raise ValueError('Unknown method {}'.format(method))\n        else:\n            scaled_cm = self.scale(vmin=min(index), vmax=max(index))\n\n        n = len(index)-1\n\n        if round_method == 'int':\n            index = [round(x) for x in index]\n\n        if round_method == 'log10':\n            index = [_base(x) for x in index]\n\n        colors = [scaled_cm.rgba_floats_tuple(index[i] * (1.-i/(n-1.)) +\n                                              index[i+1] * i/(n-1.)) for\n                  i in range(n)]\n\n        return StepColormap(colors, index=index, vmin=index[0], vmax=index[-1])"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntransforms the colorscale so that the minimal and maximal values are fit the given parameters.", "response": "def scale(self, vmin=0., vmax=1.):\n        \"\"\"Transforms the colorscale so that the minimal and maximal values\n        fit the given parameters.\n        \"\"\"\n        return LinearColormap(\n            self.colors,\n            index=[vmin + (vmax-vmin)*(x-self.vmin)*1./(self.vmax-self.vmin) for x in self.index],  # noqa\n            vmin=vmin,\n            vmax=vmax,\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_linear(self, index=None):\n        if index is None:\n            n = len(self.index)-1\n            index = [self.index[i]*(1.-i/(n-1.))+self.index[i+1]*i/(n-1.) for\n                     i in range(n)]\n\n        colors = [self.rgba_floats_tuple(x) for x in index]\n        return LinearColormap(colors, index=index,\n                              vmin=self.vmin, vmax=self.vmax)", "response": "Transforms the StepColormap into a LinearColormap."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntransforming the colorscale so that the minimal and maximal values are fit.", "response": "def scale(self, vmin=0., vmax=1.):\n        \"\"\"Transforms the colorscale so that the minimal and maximal values\n        fit the given parameters.\n        \"\"\"\n        return StepColormap(\n            self.colors,\n            index=[vmin + (vmax-vmin)*(x-self.vmin)*1./(self.vmax-self.vmin) for x in self.index],  # noqa\n            vmin=vmin,\n            vmax=vmax,\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing the bounds of the object and all its children.", "response": "def get_bounds(self):\n        \"\"\"Computes the bounds of the object and all it's children\n        in the form [[lat_min, lon_min], [lat_max, lon_max]].\n        \"\"\"\n        bounds = self._get_self_bounds()\n\n        for child in self._children.values():\n            child_bounds = child.get_bounds()\n            bounds = [\n                [\n                    none_min(bounds[0][0], child_bounds[0][0]),\n                    none_min(bounds[0][1], child_bounds[0][1]),\n                    ],\n                [\n                    none_max(bounds[1][0], child_bounds[1][0]),\n                    none_max(bounds[1][1], child_bounds[1][1]),\n                    ],\n                ]\n        return bounds"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd element to a parent.", "response": "def add_to(self, parent, name=None, index=None):\n        \"\"\"Add element to a parent.\"\"\"\n        parent.add_child(self, name=name, index=index)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_dict(self, depth=-1, ordered=True, **kwargs):\n        if ordered:\n            dict_fun = OrderedDict\n        else:\n            dict_fun = dict\n        out = dict_fun()\n        out['name'] = self._name\n        out['id'] = self._id\n        if depth != 0:\n            out['children'] = dict_fun([(name, child.to_dict(depth=depth-1))\n                                        for name, child in self._children.items()])  # noqa\n        return out", "response": "Returns a dict representation of the object."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a JSON representation of the object.", "response": "def to_json(self, depth=-1, **kwargs):\n        \"\"\"Returns a JSON representation of the object.\"\"\"\n        return json.dumps(self.to_dict(depth=depth, ordered=True), **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef render(self, **kwargs):\n        return self._template.render(this=self, kwargs=kwargs)", "response": "Renders the HTML representation of the element."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsaves an Element into a file.", "response": "def save(self, outfile, close_file=True, **kwargs):\n        \"\"\"Saves an Element into a file.\n\n        Parameters\n        ----------\n        outfile : str or file object\n            The file (or filename) where you want to output the html.\n        close_file : bool, default True\n            Whether the file has to be closed after write.\n        \"\"\"\n        if isinstance(outfile, text_type) or isinstance(outfile, binary_type):\n            fid = open(outfile, 'wb')\n        else:\n            fid = outfile\n\n        root = self.get_root()\n        html = root.render(**kwargs)\n        fid.write(html.encode('utf8'))\n        if close_file:\n            fid.close()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_code(self):\n        if self.code is None:\n            self.code = urlopen(self.url).read()\n        return self.code", "response": "Opens the link and returns the response s content."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_dict(self, depth=-1, **kwargs):\n        out = super(Link, self).to_dict(depth=-1, **kwargs)\n        out['url'] = self.url\n        return out", "response": "Returns a dict representation of the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_dict(self, depth=-1, **kwargs):\n        out = super(Figure, self).to_dict(depth=depth, **kwargs)\n        out['header'] = self.header.to_dict(depth=depth-1, **kwargs)\n        out['html'] = self.html.to_dict(depth=depth-1, **kwargs)\n        out['script'] = self.script.to_dict(depth=depth-1, **kwargs)\n        return out", "response": "Returns a dict representation of the object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef render(self, **kwargs):\n        for name, child in self._children.items():\n            child.render(**kwargs)\n        return self._template.render(this=self, kwargs=kwargs)", "response": "Renders the HTML representation of the element."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _repr_html_(self, **kwargs):\n        html = self.render(**kwargs)\n        html = \"data:text/html;charset=utf-8;base64,\" + base64.b64encode(html.encode('utf8')).decode('utf8')  # noqa\n\n        if self.height is None:\n            iframe = (\n            '<div style=\"width:{width};\">'\n            '<div style=\"position:relative;width:100%;height:0;padding-bottom:{ratio};\">'  # noqa\n            '<iframe src=\"{html}\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;'  # noqa\n            'border:none !important;\" '\n            'allowfullscreen webkitallowfullscreen mozallowfullscreen>'\n            '</iframe>'\n            '</div></div>').format\n            iframe = iframe(html=html,\n                            width=self.width,\n                            ratio=self.ratio)\n        else:\n            iframe = ('<iframe src=\"{html}\" width=\"{width}\" height=\"{height}\"'\n                      'style=\"border:none !important;\" '\n                      '\"allowfullscreen\" \"webkitallowfullscreen\" \"mozallowfullscreen\">'  # noqa\n                      '</iframe>').format\n            iframe = iframe(html=html, width=self.width, height=self.height)\n        return iframe", "response": "Displays the Figure in a Jupyter notebook."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a div child subplot in a matplotlib. figure. add_subplot style.", "response": "def add_subplot(self, x, y, n, margin=0.05):\n        \"\"\"Creates a div child subplot in a matplotlib.figure.add_subplot style.\n\n        Parameters\n        ----------\n        x : int\n            The number of rows in the grid.\n        y : int\n            The number of columns in the grid.\n        n : int\n            The cell number in the grid, counted from 1 to x*y.\n\n        Example:\n        >>> fig.add_subplot(3,2,5)\n        # Create a div in the 5th cell of a 3rows x 2columns\n        grid(bottom-left corner).\n        \"\"\"\n        width = 1./y\n        height = 1./x\n        left = ((n-1) % y)*width\n        top = ((n-1)//y)*height\n\n        left = left+width*margin\n        top = top+height*margin\n        width = width*(1-2.*margin)\n        height = height*(1-2.*margin)\n\n        div = Div(position='absolute',\n                  width='{}%'.format(100.*width),\n                  height='{}%'.format(100.*height),\n                  left='{}%'.format(100.*left),\n                  top='{}%'.format(100.*top),\n                  )\n        self.add_child(div)\n        return div"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrender the HTML representation of the element.", "response": "def render(self, **kwargs):\n        \"\"\"Renders the HTML representation of the element.\"\"\"\n        figure = self.get_root()\n        assert isinstance(figure, Figure), ('You cannot render this Element '\n                                            'if it is not in a Figure.')\n\n        header = self._template.module.__dict__.get('header', None)\n        if header is not None:\n            figure.header.add_child(Element(header(self, kwargs)),\n                                    name=self.get_name())\n\n        html = self._template.module.__dict__.get('html', None)\n        if html is not None:\n            figure.html.add_child(Element(html(self, kwargs)),\n                                  name=self.get_name())\n\n        script = self._template.module.__dict__.get('script', None)\n        if script is not None:\n            figure.script.add_child(Element(script(self, kwargs)),\n                                    name=self.get_name())\n\n        for name, element in self._children.items():\n            element.render(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nupdating the progress bar and percentage indicator.", "response": "def update(self, iterations=1, item_id=None, force_flush=False):\n        \"\"\"\n        Updates the progress bar / percentage indicator.\n\n        Parameters\n        ----------\n        iterations : int (default: 1)\n            default argument can be changed to integer values\n            >=1 in order to update the progress indicators more than once\n            per iteration.\n        item_id : str (default: None)\n            Print an item_id sring behind the progress bar\n        force_flush : bool (default: False)\n            If True, flushes the progress indicator to the output screen\n            in each iteration.\n\n        \"\"\"\n        self.item_id = item_id\n        self.cnt += iterations\n        self._print(force_flush=force_flush)\n        self._finish()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _check_stream(self):\n        if self.stream:\n\n            try:\n                supported = ('PYCHARM_HOSTED' in os.environ or\n                             os.isatty(sys.stdout.fileno()))\n\n            # a fix for IPython notebook \"IOStream has no fileno.\"\n            except(UnsupportedOperation):\n                supported = True\n\n            else:\n                if self.stream is not None and hasattr(self.stream, 'write'):\n                    self._stream_out = self.stream.write\n                    self._stream_flush = self.stream.flush\n\n            if supported:\n                if self.stream == 1:\n                    self._stream_out = sys.stdout.write\n                    self._stream_flush = sys.stdout.flush\n                elif self.stream == 2:\n                    self._stream_out = sys.stderr.write\n                    self._stream_flush = sys.stderr.flush\n            else:\n                if self.stream is not None and hasattr(self.stream, 'write'):\n                    self._stream_out = self.stream.write\n                    self._stream_flush = self.stream.flush\n                else:\n                    print('Warning: No valid output stream.')", "response": "Determines which output stream to use based on the user input."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn elapsed time at update.", "response": "def _elapsed(self):\n        \"\"\" Returns elapsed time at update. \"\"\"\n        self.last_time = time.time()\n        return self.last_time - self.start"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating estimated time left until completion.", "response": "def _calc_eta(self):\n        \"\"\" Calculates estimated time left until completion. \"\"\"\n        elapsed = self._elapsed()\n        if self.cnt == 0 or elapsed < 0.001:\n            return None\n        rate = float(self.cnt) / elapsed\n        self.eta = (float(self.max_iter) - float(self.cnt)) / rate"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _finish(self):\n        if self.active and self.cnt >= self.max_iter:\n            self.total_time = self._elapsed()\n            self.end = time.time()\n            self.last_progress -= 1  # to force a refreshed _print()\n            self._print()\n            if self.track:\n                self._stream_out('\\nTotal time elapsed: ' +\n                                 self._get_time(self.total_time))\n            self._stream_out('\\n')\n            self.active = False", "response": "Determines if the maximum number of iterations reached."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _print_title(self):\n        if self.title:\n            self._stream_out('{}\\n'.format(self.title))\n            self._stream_flush()", "response": "Prints tracking title at initialization."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _cache_eta(self):\n        self._calc_eta()\n        self._cached_output += ' | ETA: ' + self._get_time(self.eta)", "response": "Print the estimated time left."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nshrink bar if number of iterations is less than the max_iter", "response": "def _adjust_width(self):\n        \"\"\"Shrinks bar if number of iterations is less than the bar width\"\"\"\n        if self.bar_width > self.max_iter:\n            self.bar_width = int(self.max_iter)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _print(self, force_flush=False):\n        self._stream_flush()\n        next_perc = self._calc_percent()\n        if self.update_interval:\n            do_update = time.time() - self.last_time >= self.update_interval\n        elif force_flush:\n            do_update = True\n        else:\n            do_update = next_perc > self.last_progress\n\n        if do_update and self.active:\n            self.last_progress = next_perc\n            self._cache_percent_indicator(self.last_progress)\n            if self.track:\n                self._cached_output += ' Time elapsed: ' + \\\n                                       self._get_time(self._elapsed())\n                self._cache_eta()\n            if self.item_id:\n                self._cache_item_id()\n            self._stream_out('\\r%s' % self._cached_output)\n            self._stream_flush()\n            self._cached_output = ''", "response": "Prints formatted percentage and tracked time to the screen."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the next line in the file updating the offset.", "response": "def next(self):\n        \"\"\"\n        Return the next line in the file, updating the offset.\n        \"\"\"\n        try:\n            line = self._get_next_line()\n        except StopIteration:\n            # we've reached the end of the file; if we're processing the\n            # rotated log file or the file has been renamed, we can continue with the actual file; otherwise\n            # update the offset file\n            if self._is_new_file():\n                self._rotated_logfile = None\n                self._fh.close()\n                self._offset = 0\n                # open up current logfile and continue\n                try:\n                    line = self._get_next_line()\n                except StopIteration:  # oops, empty file\n                    self._update_offset_file()\n                    raise\n            else:\n                self._update_offset_file()\n                raise\n\n        if self.paranoid:\n            self._update_offset_file()\n        elif self.every_n and self.every_n <= self._since_update:\n            self._update_offset_file()\n\n        return line"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef read(self):\n        lines = self.readlines()\n        if lines:\n            try:\n                return ''.join(lines)\n            except TypeError:\n                return ''.join(force_text(line) for line in lines)\n        else:\n            return None", "response": "Read in all unread lines and return them as a single string."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a filehandle to the file being tailed with the position set to the current offset.", "response": "def _filehandle(self):\n        \"\"\"\n        Return a filehandle to the file being tailed, with the position set\n        to the current offset.\n        \"\"\"\n        if not self._fh or self._is_closed():\n            filename = self._rotated_logfile or self.filename\n            if filename.endswith('.gz'):\n                self._fh = gzip.open(filename, 'r')\n            else:\n                self._fh = open(filename, \"r\", 1)\n            if self.read_from_end and not exists(self._offset_file):\n                self._fh.seek(0, os.SEEK_END)\n            else:\n                self._fh.seek(self._offset)\n\n        return self._fh"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _update_offset_file(self):\n        if self.on_update:\n            self.on_update()\n        offset = self._filehandle().tell()\n        inode = stat(self.filename).st_ino\n        fh = open(self._offset_file, \"w\")\n        fh.write(\"%s\\n%s\\n\" % (inode, offset))\n        fh.close()\n        self._since_update = 0", "response": "Update the offset file with the current inode and offset."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _determine_rotated_logfile(self):\n        rotated_filename = self._check_rotated_filename_candidates()\n        if rotated_filename and exists(rotated_filename):\n            if stat(rotated_filename).st_ino == self._offset_file_inode:\n                return rotated_filename\n\n            # if the inode hasn't changed, then the file shrank; this is expected with copytruncate,\n            # otherwise print a warning\n            if stat(self.filename).st_ino == self._offset_file_inode:\n                if self.copytruncate:\n                    return rotated_filename\n                else:\n                    sys.stderr.write(\n                        \"[pygtail] [WARN] file size of %s shrank, and copytruncate support is \"\n                        \"disabled (expected at least %d bytes, was %d bytes).\\n\" %\n                        (self.filename, self._offset, stat(self.filename).st_size))\n\n        return None", "response": "Determine the filename of the current logfile and return it if it is rotated."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck for various rotated logfile filename patterns and return the first match we find.", "response": "def _check_rotated_filename_candidates(self):\n        \"\"\"\n        Check for various rotated logfile filename patterns and return the first\n        match we find.\n        \"\"\"\n        # savelog(8)\n        candidate = \"%s.0\" % self.filename\n        if (exists(candidate) and exists(\"%s.1.gz\" % self.filename) and\n            (stat(candidate).st_mtime > stat(\"%s.1.gz\" % self.filename).st_mtime)):\n            return candidate\n\n        # logrotate(8)\n        # with delaycompress\n        candidate = \"%s.1\" % self.filename\n        if exists(candidate):\n            return candidate\n\n        # without delaycompress\n        candidate = \"%s.1.gz\" % self.filename\n        if exists(candidate):\n            return candidate\n\n        rotated_filename_patterns = [\n            # logrotate dateext rotation scheme - `dateformat -%Y%m%d` + with `delaycompress`\n            \"%s-[0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9]\",\n            # logrotate dateext rotation scheme - `dateformat -%Y%m%d` + without `delaycompress`\n            \"%s-[0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9].gz\",\n            # logrotate dateext rotation scheme - `dateformat -%Y%m%d-%s` + with `delaycompress`\n            \"%s-[0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9]-[0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9]\",\n            # logrotate dateext rotation scheme - `dateformat -%Y%m%d-%s` + without `delaycompress`\n            \"%s-[0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9]-[0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9].gz\",\n            # for TimedRotatingFileHandler\n            \"%s.[0-9][0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9]\",\n        ]\n        if self.log_patterns:\n            rotated_filename_patterns.extend(self.log_patterns)\n\n        # break into directory and filename components to support cases where the\n        # the file is prepended as part of rotation\n        file_dir, rel_filename = os.path.split(self.filename)\n        for rotated_filename_pattern in rotated_filename_patterns:\n            candidates = glob.glob(os.path.join(file_dir, rotated_filename_pattern % rel_filename))\n            if candidates:\n                candidates.sort()\n                return candidates[-1]  # return most recent\n\n        # no match\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a session with automatic retries on 5xx errors.", "response": "def create_s3_session():\n    \"\"\"\n    Creates a session with automatic retries on 5xx errors.\n    \"\"\"\n    sess = requests.Session()\n    retries = Retry(total=3,\n                    backoff_factor=.5,\n                    status_forcelist=[500, 502, 503, 504])\n    sess.mount('https://', HTTPAdapter(max_retries=retries))\n    return sess"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nloading a module from sys. modules.", "response": "def load_module(self, fullname):\n        \"\"\"\n        Returns an empty module.\n        \"\"\"\n        mod = sys.modules.setdefault(fullname, imp.new_module(fullname))\n        mod.__file__ = self._path\n        mod.__loader__ = self\n        mod.__path__ = []\n        mod.__package__ = fullname\n        return mod"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_module(self, fullname):\n        mod = sys.modules.get(fullname)\n        if mod is not None:\n            return mod\n\n        # We're creating an object rather than a module. It's a hack, but it's approved by Guido:\n        # https://mail.python.org/pipermail/python-ideas/2012-May/014969.html\n\n        mod = _from_core_node(self._store, self._root)\n        sys.modules[fullname] = mod\n        return mod", "response": "Loads a module from the store."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef find_module(self, fullname, path=None):\n        if not fullname.startswith(self._module_name + '.'):\n            # Not a quilt submodule.\n            return None\n\n        submodule = fullname[len(self._module_name) + 1:]\n        parts = submodule.split('.')\n\n        # Pop the team prefix if this is a team import.\n        if self._teams:\n            team = parts.pop(0)\n        else:\n            team = None\n\n        # Handle full paths first.\n        if len(parts) == 2:\n            store, pkg = PackageStore.find_package(team, parts[0], parts[1])\n            if pkg is not None:\n                return PackageLoader(store, pkg)\n            else:\n                return None\n\n        # Return fake loaders for partial paths.\n        for store_dir in PackageStore.find_store_dirs():\n            store = PackageStore(store_dir)\n\n            if len(parts) == 0:\n                assert self._teams\n                path = store.team_path(team)\n            elif len(parts) == 1:\n                path = store.user_path(team, parts[0])\n\n            if os.path.isdir(path):\n                return FakeLoader(path)\n\n        # Nothing is found.\n        return None", "response": "Find the module in the package store and return a loader for the module."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _have_pyspark():\n    if _have_pyspark.flag is None:\n        try:\n            if PackageStore.get_parquet_lib() is ParquetLib.SPARK:\n                import pyspark  # pylint:disable=W0612\n                _have_pyspark.flag = True\n            else:\n                _have_pyspark.flag = False\n        except ImportError:\n            _have_pyspark.flag = False\n    return _have_pyspark.flag", "response": "Check if we re running Pyspark"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _path_hash(path, transform, kwargs):\n    sortedargs = [\"%s:%r:%s\" % (key, value, type(value))\n                  for key, value in sorted(iteritems(kwargs))]\n    srcinfo = \"{path}:{transform}:{{{kwargs}}}\".format(path=os.path.abspath(path),\n                                                   transform=transform,\n                                                   kwargs=\",\".join(sortedargs))\n    return digest_string(srcinfo)", "response": "Generate a hash of a source file path + transform + args\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate node data by globbing a directory for a pattern", "response": "def _gen_glob_data(dir, pattern, child_table):\n    \"\"\"Generates node data by globbing a directory for a pattern\"\"\"\n    dir = pathlib.Path(dir)\n    matched = False\n    used_names = set()  # Used by to_nodename to prevent duplicate names\n    # sorted so that renames (if any) are consistently ordered\n    for filepath in sorted(dir.glob(pattern)):\n        if filepath.is_dir():\n            continue\n        else:\n            matched = True\n\n        # create node info\n        node_table = {} if child_table is None else child_table.copy()\n        filepath = filepath.relative_to(dir)\n        node_table[RESERVED['file']] = str(filepath)\n        node_name = to_nodename(filepath.stem, invalid=used_names)\n        used_names.add(node_name)\n        print(\"Matched with {!r}: {!r} from {!r}\".format(pattern, node_name, str(filepath)))\n\n        yield node_name, node_table\n\n    if not matched:\n        print(\"Warning: {!r} matched no files.\".format(pattern))\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nbuilds a node from a YAML file.", "response": "def _build_node(build_dir, pkg_store, pkg_root, node_path, node, checks_contents=None,\n                dry_run=False, env='default', ancestor_args={}):\n    \"\"\"\n    Parameters\n    ----------\n    ancestor_args : dict\n      any transform inherited from an ancestor\n      plus any inherited handler kwargs\n      Users can thus define kwargs that affect entire subtrees\n      (e.g. transform: csv for 500 .txt files)\n      and overriding of ancestor or peer values.\n      Child transform or kwargs override ancestor k:v pairs.\n    \"\"\"\n    assert isinstance(pkg_store, PackageStore)\n    assert isinstance(pkg_root, RootNode)\n    if _is_internal_node(node):\n        if not dry_run:\n            pkg_store.add_to_package_group(pkg_root, node_path, None)\n\n        # Make a consumable copy.  This is to cover a quirk introduced by accepting nodes named\n        # like RESERVED keys -- if a RESERVED key is actually matched, it should be removed from\n        # the node, or it gets treated like a subnode (or like a node with invalid content)\n        node = node.copy()\n\n        # NOTE: YAML parsing does not guarantee key order\n        # fetch local transform and kwargs values; we do it using ifs\n        # to prevent `key: None` from polluting the update\n        local_args = _get_local_args(node, [RESERVED['transform'], RESERVED['kwargs']])\n        group_args = ancestor_args.copy()\n        group_args.update(local_args)\n        _consume(node, local_args)\n\n        # if it's not a reserved word it's a group that we can descend\n        groups = {k: v for k, v in iteritems(node) if _is_valid_group(v)}\n        _consume(node, groups)\n\n        if node:\n            # Unused keys -- either keyword typos or node names with invalid values.\n            #   For now, until build.yml schemas, pointing out one should do.\n            key, value = node.popitem()\n            raise BuildException(\"Invalid syntax: expected node data for {!r}, got {!r}\".format(key, value))\n        for child_name, child_table in groups.items():\n            if glob.has_magic(child_name):\n                # child_name is a glob string, use it to generate multiple child nodes\n                for gchild_name, gchild_table in _gen_glob_data(build_dir, child_name, child_table):\n                    _build_node(build_dir,\n                                pkg_store,\n                                pkg_root,\n                                node_path + [gchild_name],\n                                gchild_table,\n                                checks_contents=checks_contents,\n                                dry_run=dry_run,\n                                env=env,\n                                ancestor_args=group_args)\n            else:\n                if not isinstance(child_name, str) or not is_nodename(child_name):\n                    raise StoreException(\"Invalid node name: %r\" % child_name)\n                _build_node(build_dir,\n                            pkg_store,\n                            pkg_root,\n                            node_path + [child_name],\n                            child_table,\n                            checks_contents=checks_contents,\n                            dry_run=dry_run,\n                            env=env,\n                            ancestor_args=group_args)\n    else:  # leaf node\n        # prevent overwriting existing node names\n        if find_in_package(pkg_root, node_path):\n            message = \"Naming conflict: {!r} added to package more than once\"\n            raise BuildException(message.format('/'.join(node_path)))\n        # handle group leaf nodes (empty groups)\n        if not node:\n            if not dry_run:\n                pkg_store.add_to_package_group(pkg_root, node_path, None)\n            return\n\n        include_package = node.get(RESERVED['package'])\n        rel_path = node.get(RESERVED['file'])\n        if rel_path and include_package:\n            message = \"A node must define only one of {0} or {1}\"\n            raise BuildException(message.format(RESERVED['file'], RESERVED['package']))\n        elif include_package: # package composition\n            team, user, pkgname, subpath = parse_package(include_package, allow_subpath=True)\n            store, existing_pkg = PackageStore.find_package(team, user, pkgname)\n            if existing_pkg is None:\n                raise BuildException(\"Package not found: %s\" % include_package)\n\n            if subpath:\n                node = find_in_package(existing_pkg, subpath)\n                if node is None:\n                    msg = \"Package {team}:{owner}/{pkg} has no subpackage: {subpath}\"\n                    raise BuildException(msg.format(team=team,\n                                                    owner=user,\n                                                    pkg=pkgname,\n                                                    subpath=subpath))\n            else:\n                node = GroupNode(existing_pkg.children)\n            pkg_store.add_to_package_package_tree(pkg_root, node_path, node)\n        elif rel_path: # handle nodes built from input files\n            path = os.path.join(build_dir, rel_path)\n\n            rel_meta_path = node.get(RESERVED['meta'])\n            if rel_meta_path:\n                with open(os.path.join(build_dir, rel_meta_path)) as fd:\n                    try:\n                        metadata = json.load(fd)\n                    except ValueError as ex:\n                        raise BuildException(\"Failed to parse %r as JSON: %s\" % (rel_meta_path, ex))\n                    if SYSTEM_METADATA in metadata:\n                        raise BuildException(\"Invalid metadata in %r: not allowed to use key %r\" %\n                                             (rel_meta_path, SYSTEM_METADATA))\n            else:\n                metadata = None\n\n            # get either the locally defined transform and target or inherit from an ancestor\n            transform = node.get(RESERVED['transform']) or ancestor_args.get(RESERVED['transform'])\n\n            ID = 'id' # pylint:disable=C0103\n            PARQUET = 'parquet' # pylint:disable=C0103\n            if transform:\n                transform = transform.lower()\n                if transform in PANDAS_PARSERS:\n                    target = TargetType.PANDAS # pylint:disable=R0204\n                elif transform == PARQUET:\n                    target = TargetType.PANDAS # pylint:disable=R0204\n                elif transform == ID:\n                    target = TargetType.FILE # pylint:disable=R0204\n                else:\n                    raise BuildException(\"Unknown transform '%s' for %s\" %\n                                         (transform, rel_path))\n            else:\n                # Guess transform and target based on file extension if not provided\n                _, ext = splitext_no_dot(rel_path)\n\n                if ext in PANDAS_PARSERS:\n                    transform = ext\n                    target = TargetType.PANDAS\n                elif ext == PARQUET:\n                    transform = ext\n                    target = TargetType.PANDAS\n                else:\n                    transform = ID\n                    target = TargetType.FILE\n                print(\"Inferring 'transform: %s' for %s\" % (transform, rel_path))\n\n\n            # TODO: parse/check environments:\n            # environments = node.get(RESERVED['environments'])\n            checks = node.get(RESERVED['checks'])\n            if transform == ID:\n                #TODO move this to a separate function\n                if checks:\n                    with open(path, 'r') as fd:\n                        data = fd.read()\n                        _run_checks(data, checks, checks_contents, node_path, rel_path, target, env=env)\n                if not dry_run:\n                    print(\"Registering %s...\" % path)\n                    pkg_store.add_to_package_file(pkg_root,\n                                                  path,\n                                                  node_path,\n                                                  target,\n                                                  rel_path,\n                                                  transform,\n                                                  metadata)\n            elif transform == PARQUET:\n                if checks:\n                    from pyarrow.parquet import ParquetDataset\n                    dataset = ParquetDataset(path)\n                    table = dataset.read(nthreads=4)\n                    dataframe = table.to_pandas()\n                    _run_checks(dataframe, checks, checks_contents, node_path, rel_path, target, env=env)\n                if not dry_run:\n                    print(\"Registering %s...\" % path)\n                    pkg_store.add_to_package_file(pkg_root,\n                                                  path,\n                                                  node_path,\n                                                  target,\n                                                  rel_path,\n                                                  transform,\n                                                  metadata)\n            else:\n                # copy so we don't modify shared ancestor_args\n                handler_args = dict(ancestor_args.get(RESERVED['kwargs'], {}))\n                # local kwargs win the update\n                handler_args.update(node.get(RESERVED['kwargs'], {}))\n                # Check Cache\n                store = PackageStore()\n                path_hash = _path_hash(path, transform, handler_args)\n                source_hash = digest_file(path)\n\n                cachedobjs = []\n                if os.path.exists(store.cache_path(path_hash)):\n                    with open(store.cache_path(path_hash), 'r') as entry:\n                        cache_entry = json.load(entry)\n                        if cache_entry['source_hash'] == source_hash:\n                            cachedobjs = cache_entry['obj_hashes']\n                            assert isinstance(cachedobjs, list)\n\n                # TODO: check for changes in checks else use cache\n                # below is a heavy-handed fix but it's OK for check builds to be slow\n                if (not checks and\n                    cachedobjs and\n                    all(os.path.exists(store.object_path(obj)) for obj in cachedobjs)):\n                    # Use existing objects instead of rebuilding\n                    pkg_store.add_to_package_cached_df(pkg_root,\n                                                       cachedobjs,\n                                                       node_path,\n                                                       target,\n                                                       rel_path,\n                                                       transform,\n                                                       metadata)\n                else:\n                    # read source file into DataFrame\n                    print(\"Serializing %s...\" % path)\n                    if _have_pyspark():\n                        dataframe = _file_to_spark_data_frame(transform, path, handler_args)\n                    else:\n                        dataframe = _file_to_data_frame(transform, path, handler_args)\n\n                    if checks:\n                        # TODO: test that design works for internal nodes... e.g. iterating\n                        # over the children and getting/checking the data, err msgs, etc.\n                        _run_checks(dataframe, checks, checks_contents, node_path, rel_path, target, env=env)\n\n                    # serialize DataFrame to file(s)\n                    if not dry_run:\n                        print(\"Saving as binary dataframe...\")\n                        obj_hashes = pkg_store.add_to_package_df(pkg_root,\n                                                                 dataframe,\n                                                                 node_path,\n                                                                 target,\n                                                                 rel_path,\n                                                                 transform,\n                                                                 metadata)\n\n                        # Add to cache\n                        cache_entry = dict(\n                            source_hash=source_hash,\n                            obj_hashes=obj_hashes\n                            )\n                        with open(store.cache_path(path_hash), 'w') as entry:\n                            json.dump(cache_entry, entry)\n        else: # rel_path and package are both None\n            message = \"Leaf nodes must define either a %s or %s key\"\n            raise BuildException(message % (RESERVED['file'], RESERVED['package']))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncopies the dict d remove keywords", "response": "def _remove_keywords(d):\n    \"\"\"\n    copy the dict, filter_keywords\n\n    Parameters\n    ----------\n    d : dict\n    \"\"\"\n    return { k:v for k, v in iteritems(d) if k not in RESERVED }"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef build_package(team, username, package, subpath, yaml_path,\n                  checks_path=None, dry_run=False, env='default'):\n    \"\"\"\n    Builds a package from a given Yaml file and installs it locally.\n\n    Returns the name of the package.\n    \"\"\"\n    def find(key, value):\n        \"\"\"\n        find matching nodes recursively;\n        only descend iterables that aren't strings\n        \"\"\"\n        if isinstance(value, Iterable) and not isinstance(value, string_types):\n            for k, v in iteritems(value):\n                if k == key:\n                    yield v\n                elif isinstance(v, dict):\n                    for result in find(key, v):\n                        yield result\n                elif isinstance(v, list):\n                    for item in v:\n                        for result in find(key, item):\n                            yield result\n\n    build_data = load_yaml(yaml_path)\n    # default to 'checks.yml' if build.yml contents: contains checks, but\n    # there's no inlined checks: defined by build.yml\n    if (checks_path is None and list(find('checks', build_data['contents'])) and\n        'checks' not in build_data):\n        checks_path = 'checks.yml'\n        checks_contents = load_yaml(checks_path, optional=True)\n    elif checks_path is not None:\n        checks_contents = load_yaml(checks_path)\n    else:\n        checks_contents = None\n    build_package_from_contents(team, username, package, subpath, os.path.dirname(yaml_path), build_data,\n                                checks_contents=checks_contents, dry_run=dry_run, env=env)", "response": "Builds a package from a given Yaml file and installs it locally."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef splitext_no_dot(filename):\n    name, ext = os.path.splitext(filename)\n    ext = ext.lower()\n    return name, ext.strip('.')", "response": "Wrap os. path. splitext to return the name and extension without the."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating a build file based on the contents of a directory tree.", "response": "def generate_contents(startpath, outfilename=DEFAULT_BUILDFILE):\n    \"\"\"\n    Generate a build file (yaml) based on the contents of a\n    directory tree.\n    \"\"\"\n    def _ignored_name(name):\n        return (\n            name.startswith('.') or\n            name == PACKAGE_DIR_NAME or\n            name.endswith('~') or\n            name == outfilename\n        )\n\n    def _generate_contents(dir_path):\n        safename_duplicates = defaultdict(list)\n        for name in os.listdir(dir_path):\n            if _ignored_name(name):\n                continue\n\n            path = os.path.join(dir_path, name)\n\n            if os.path.isdir(path):\n                nodename = name\n                ext = None\n            elif os.path.isfile(path):\n                nodename, ext = splitext_no_dot(name)\n            else:\n                continue\n\n            safename = to_identifier(nodename)\n            safename_duplicates[safename].append((name, nodename, ext))\n\n        safename_to_name = {}\n        for safename, duplicates in iteritems(safename_duplicates):\n            for name, nodename, ext in duplicates:\n                if len(duplicates) > 1 and ext:\n                    new_safename = to_identifier(name)  # Name with ext\n                else:\n                    new_safename = safename\n                existing_name = safename_to_name.get(new_safename)\n                if existing_name is not None:\n                    message = \"Duplicate node names in directory %r.\"\n                    message += \" %r was renamed to %r, which overlaps with %r\"\n                    raise BuildException(\n                        message % (dir_path, name, new_safename, existing_name)\n                    )\n                safename_to_name[new_safename] = name\n\n        contents = {}\n        for safename, name in iteritems(safename_to_name):\n            path = os.path.join(dir_path, name)\n\n            if os.path.isdir(path):\n                data = _generate_contents(path)\n            else:\n                rel_path = os.path.relpath(path, startpath)\n                data = dict(file=rel_path)\n\n            contents[safename] = data\n\n        return contents\n\n    return dict(\n        contents=_generate_contents(startpath)\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef generate_build_file(startpath, outfilename=DEFAULT_BUILDFILE):\n    buildfilepath = os.path.join(startpath, outfilename)\n    if os.path.exists(buildfilepath):\n        raise BuildException(\"Build file %s already exists.\" % buildfilepath)\n\n    contents = generate_contents(startpath, outfilename)\n\n    with open(buildfilepath, 'w') as outfile:\n        yaml.dump(contents, outfile, default_flow_style=False)\n    return buildfilepath", "response": "Generate a build file based on the contents of a\n    directory tree."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nplot an image from a Quilt fragment path.", "response": "def plot(figsize=None, formats=None, limit=100, titlelen=10, **kwargs):\n    \"\"\"Display an image [in a Jupyter Notebook] from a Quilt fragment path.\n    Intended for use with `%matplotlib inline`.\n\n    Convenience method that loops over supblots that call\n    `plt.imshow(image.imread(FRAG_PATH))`.\n\n    Keyword arguments\n    * figsize=None # None means auto, else provide (HEIGHT_INCHES, WIDTH_INCHES)\n    * formats=None # List of extensions as strings ['jpg', 'png', ...]\n    * limit=100 # maximum number of images to display\n    * titlelen=10 # max number of characters in subplot title\n    * **kwargs - all remaining kwargs are passed to plt.subplots;\n      see https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplots.html\n    \"\"\"\n    # pylint: disable=protected-access\n    def _plot(node, paths):\n        lower_formats = set((x.lower() for x in formats)) if formats is not None else None\n        def node_filter(frag, meta):\n            filepath = meta.get('_system', {}).get('filepath', None)\n            # don't try to read DataFrames as images\n            if isinstance(frag, string_types) and filepath:\n                _, ext = splitext_no_dot(filepath)\n                if lower_formats is None or ext.lower() in lower_formats:\n                    return True\n            return False\n        # assume DataNode has one path; doesn't work with multi-fragment images\n        display = [('', paths[0], node._meta)]\n        # for GroupNodes, display all DataNode children\n        if isinstance(node, GroupNode):\n            datanodes = [(x, y) for (x, y) in node._items() if isinstance(y, DataNode)]\n            display = [(x, y._data(), y._meta) for (x, y) in datanodes]\n            # sort by name so iteration is reproducible (and unit tests pass)\n            display = sorted(display, key=lambda rec: rec[0])\n            display = [x for x in display if node_filter(x[1], x[2])]\n            if len(display) > limit:\n                print('Displaying {} of {} images{}'.format(limit, len(display), ELLIPSIS))\n                display = display[:limit]\n        # display can be empty e.g. if no DataNode children\n        if not display:\n            print('No images to display.')\n            return\n        # cast to int to avoid downstream complaints of\n        # 'float' object cannot be interpreted as an index\n        floatlen = float(len(display)) # prevent integer division in 2.7\n        cols = min(int(floor(sqrt(floatlen))), 8)\n        rows = int(ceil(floatlen/cols))\n\n        plt.tight_layout()\n        plt.subplots(\n            rows,\n            cols,\n            figsize=(cols*2, rows*2) if not figsize else figsize,\n            **kwargs)\n\n        for i in range(rows*cols):\n            axes = plt.subplot(rows, cols, i + 1) # subplots start at 1, not 0\n            axes.axis('off')\n            if i < len(display):\n                (name, frag, meta) = display[i]\n                plt.title(name[:titlelen] + ELLIPSIS if len(name) > titlelen else name)\n                filepath = meta.get('_system', {}).get('filepath', None)\n                _, ext = splitext_no_dot(filepath)\n                try:\n                    bits = mpimg.imread(frag, format=ext)\n                    plt.imshow(bits)\n                # Mac throws OSError, Linux IOError if file not recognizable\n                except (IOError, OSError) as err:\n                    print('{}: {}'.format(name, str(err)))\n                    continue \n    return _plot"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsends email to owner of package regarding new comment", "response": "def send_comment_email(email, package_owner, package_name, commenter):\n    \"\"\"Send email to owner of package regarding new comment\"\"\"\n    link = '{CATALOG_URL}/package/{owner}/{pkg}/comments'.format(\n        CATALOG_URL=CATALOG_URL, owner=package_owner, pkg=package_name)\n    subject = \"New comment on {package_owner}/{package_name}\".format(\n        package_owner=package_owner, package_name=package_name)\n    html = render_template('comment_email.html', commenter=commenter, link=link)\n    body = render_template('comment_email.txt', commenter=commenter, link=link)\n    send_email(recipients=[email], sender=DEFAULT_SENDER, subject=subject,\n               html=html, body=body)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef hash_contents(contents):\n    assert isinstance(contents, GroupNode)\n\n    result = hashlib.sha256()\n\n    def _hash_int(value):\n        result.update(struct.pack(\">L\", value))\n\n    def _hash_str(string):\n        assert isinstance(string, string_types)\n        _hash_int(len(string))\n        result.update(string.encode())\n\n    def _hash_object(obj):\n        _hash_str(obj.json_type)\n        if isinstance(obj, (TableNode, FileNode)):\n            hashes = obj.hashes\n            _hash_int(len(hashes))\n            for hval in hashes:\n                _hash_str(hval)\n        elif isinstance(obj, GroupNode):\n            children = obj.children\n            _hash_int(len(children))\n            for key, child in sorted(iteritems(children)):\n                _hash_str(key)\n                _hash_object(child)\n        else:\n            assert False, \"Unexpected object: %r\" % obj\n\n        # Backward compatibility: only hash metadata_hash if it's present.\n        if obj.metadata_hash is not None:\n            _hash_str(obj.metadata_hash)\n\n    _hash_object(contents)\n\n    return result.hexdigest()", "response": "Creates a hash of key names and hashes in a package dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef find_object_hashes(root, meta_only=False):\n    stack = [root]\n    while stack:\n        obj = stack.pop()\n        if not meta_only and isinstance(obj, (TableNode, FileNode)):\n            for objhash in obj.hashes:\n                yield objhash\n        stack.extend(itervalues(obj.get_children()))\n        if obj.metadata_hash is not None:\n            yield obj.metadata_hash", "response": "Iterator that returns hashes of all of the file and table nodes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dataset(\n        node_parser,\n        include=lambda x: True,\n        input_transform=None,\n        target_transform=None):\n    \"\"\"Convert immediate children of a GroupNode into a torch.data.Dataset\n    Keyword arguments\n    * node_parser=callable that converts a DataNode to a Dataset item\n    * include=lambda x: True\n      lambda(quilt.nodes.GroupNode) => {True, False}\n      intended to filter nodes based on metadata\n    * input_transform=None; optional callable that takes the item as its argument\n    * output_transform=None; optional callable that takes the item as its argument;\n      implementation may make its own copy of item to avoid side effects\n\n      Dataset.__getitem__ returns the following tuple\n      item = node_parser(node)\n      (input_transform(item), output_transform(item))\n      Or, if no _transform functions are provided:\n      (item, item)\n    \"\"\"\n    def _dataset(node, paths): # pylint: disable=unused-argument\n        return DatasetFromGroupNode(\n            node,\n            node_parser=node_parser,\n            include=include,\n            input_transform=input_transform,\n            target_transform=target_transform)\n\n    return _dataset", "response": "Convert immediate children of a GroupNode into a torch. data. Dataset"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef digest_file(fname):\n    #chunk size in bytes\n    size = 4096\n    hval = hashlib.new(HASH_TYPE)\n    with open(fname, 'rb') as fd:\n        for chunk in iter(lambda: fd.read(size), b''):\n            hval.update(chunk)\n    return hval.hexdigest()", "response": "Digests a file using SHA - 2"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\napply driver - specific hacks to our node objects", "response": "def apply_driver_hacks(self, app, info, options):\n        \"\"\"\n        Set custom SQLAlchemy engine options:\n        - Teach it to encode and decode our node objects\n        - Enable pre-ping (i.e., test the DB connection before trying to use it)\n        \"\"\"\n        options.update(dict(\n            json_serializer=lambda data: json.dumps(data, default=encode_node),\n            json_deserializer=lambda data: json.loads(data, object_hook=decode_node),\n            pool_pre_ping=True,\n        ))\n        super(QuiltSQLAlchemy, self).apply_driver_hacks(app, info, options)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef send(self, endpoint, json_message):\n        _send_event_task.spool(endpoint=endpoint, json_message=json_message)", "response": "Sends a message to the specified endpoint."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsplitting the input several times, returning intermediate results at each level: - delimited by underscores - letter/number boundaries - word segments E.g., tokenize('landuse_austin_tx_24dates') -> ['landuse', 'land', 'use', 'austin', 'tx', '24dates', '24', 'dates'] (Don't need a token for the original string because to_tsvector splits on underscores.)", "response": "def tokenize(string):\n    \"\"\"\n    Split the input several times, returning intermediate results at each level:\n    - delimited by underscores\n    - letter/number boundaries\n    - word segments\n    E.g., tokenize('landuse_austin_tx_24dates') ->\n        ['landuse', 'land', 'use', 'austin', 'tx', '24dates', '24', 'dates']\n\n    (Don't need a token for the original string because to_tsvector splits on underscores.)\n    \"\"\"\n    if not wordsegment.BIGRAMS:\n        # Should only happen in dev.\n        wordsegment.load()\n\n    lvl1_parts = string.split('_')\n    for lvl1 in lvl1_parts:\n        lvl2_parts = ALPHA_NUM_RE.findall(lvl1)\n        if len(lvl2_parts) > 1:\n            yield lvl1\n        for lvl2 in lvl2_parts:\n            lvl3_parts = wordsegment.segment(lvl2)\n            if len(lvl3_parts) > 1:\n                yield lvl2\n            yield from lvl3_parts"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbuild and run parser", "response": "def main(args=None):\n    \"\"\"Build and run parser\n\n    :param args: cli args from tests\n    \"\"\"\n    parser = argument_parser()\n    args = parser.parse_args(args)\n\n    # If 'func' isn't present, something is misconfigured above or no (positional) arg was given.\n    if not hasattr(args, 'func'):\n        args = parser.parse_args(['help'])  # show help\n\n    # Convert argparse.Namespace into dict and clean it up.\n    # We can then pass it directly to the helper function.\n    kwargs = vars(args)\n\n    # handle the '--dev' option\n    if kwargs.pop('dev') or os.environ.get('QUILT_DEV_MODE', '').strip().lower() == 'true':\n        # Enables CLI ctrl-c tracebacks, and whatever anyone else uses it for\n        quilt._DEV_MODE = True\n    else:\n        # Disables CLI ctrl-c tracebacks, etc.\n        quilt._DEV_MODE = False\n\n    func = kwargs.pop('func')\n\n    try:\n        func(**kwargs)\n        return 0\n    except QuiltException as ex:\n        print(ex.message, file=sys.stderr)\n        return 1\n    except requests.exceptions.ConnectionError as ex:\n        print(\"Failed to connect: %s\" % ex, file=sys.stderr)\n        return 1"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses the extended package syntax and returns a tuple of package hash version and tag.", "response": "def parse_package_extended(identifier):\n    \"\"\"\n    Parses the extended package syntax and returns a tuple of (package, hash, version, tag).\n    \"\"\"\n    match = EXTENDED_PACKAGE_RE.match(identifier)\n    if match is None:\n        raise ValueError\n\n    full_name, pkg_hash, version, tag = match.groups()\n    team, user, name, subpath = parse_package(full_name, allow_subpath=True)\n\n    # namedtuple return value\n    return PackageInfo(full_name, team, user, name, subpath, pkg_hash, version, tag)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading a file into a string", "response": "def file_to_str(fname):\n    \"\"\"\n    Read a file into a string\n    PRE: fname is a small file (to avoid hogging memory and its discontents)\n    \"\"\"\n    data = None\n    # rU = read with Universal line terminator\n    with open(fname, 'rU') as fd:\n        data = fd.read()\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncompresses a string. Same as gzip. compress in Python 3.", "response": "def gzip_compress(data):\n    \"\"\"\n    Compress a string. Same as gzip.compress in Python3.\n    \"\"\"\n    buf = BytesIO()\n    with gzip.GzipFile(fileobj=buf, mode='wb') as fd:\n        fd.write(data)\n    return buf.getvalue()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sub_dirs(path, invisible=False):\n    dirs = [x for x in os.listdir(path) if os.path.isdir(os.path.join(path, x))]\n    if not invisible:\n        dirs = [x for x in dirs if not x.startswith('.')]\n\n    return dirs", "response": "Get a list of all child directories in a directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sub_files(path, invisible=False):\n    files = [x for x in os.listdir(path) if os.path.isfile(os.path.join(path, x))]\n    if not invisible:\n        files = [x for x in files if not x.startswith('.')]\n\n    return files", "response": "Get a list of all child files in a directory."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if string can be a valid python identifier", "response": "def is_identifier(string):\n    \"\"\"Check if string could be a valid python identifier\n\n    :param string: string to be tested\n    :returns: True if string can be a python identifier, False otherwise\n    :rtype: bool\n    \"\"\"\n    matched = PYTHON_IDENTIFIER_RE.match(string)\n    return bool(matched) and not keyword.iskeyword(string)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert a string to a python identifier.", "response": "def to_identifier(string):\n    \"\"\"Makes a python identifier (perhaps an ugly one) out of any string.\n\n    This isn't an isomorphic change, the original name can't be recovered\n    from the change in all cases, so it must be stored separately.\n\n    Examples:\n    >>> to_identifier('Alice\\'s Restaurant') -> 'Alice_s_Restaurant'\n    >>> to_identifier('#if') -> 'if' -> QuiltException\n    >>> to_identifier('9foo') -> 'n9foo'\n\n    :param string: string to convert\n    :returns: `string`, converted to python identifier if needed\n    :rtype: string\n    \"\"\"\n    # Not really useful to expose as a CONSTANT, and python will compile and cache\n    result = re.sub(r'[^0-9a-zA-Z_]', '_', string)\n\n    # compatibility with older behavior and tests, doesn't hurt anyways -- \"_\" is a\n    # pretty useless name to translate to.  With this, it'll raise an exception.\n    result = result.strip('_')\n\n    if result and result[0].isdigit():\n        result = \"n\" + result\n\n    if not is_identifier(result):\n        raise QuiltException(\"Unable to generate Python identifier from name: {!r}\".format(string))\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_nodename(string, invalid=None, raise_exc=False):\n    string = to_identifier(string)\n\n    #TODO: Remove this stanza once keywords are permissible in nodenames\n    if keyword.iskeyword(string):\n        string += '_'   # there are no keywords ending in \"_\"\n\n    # Done if no deduplication needed\n    if invalid is None:\n        return string\n\n    # Deduplicate\n    if string in invalid and raise_exc:\n        raise QuiltException(\"Conflicting node name after string conversion: {!r}\".format(string))\n\n    result = string\n    counter = 1\n    while result in invalid:\n        # first conflicted name will be \"somenode_2\"\n        # The result is \"somenode\", \"somenode_2\", \"somenode_3\"..\n        counter += 1\n        result = \"{}_{}\".format(string, counter)\n\n    return result", "response": "Converts a string to a Quilt Node name."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a hard or soft link of path at linkpath at linkpath.", "response": "def fs_link(path, linkpath, linktype='soft'):\n    \"\"\"Create a hard or soft link of `path` at `linkpath`\n\n    Works on Linux/OSX/Windows (Vista+).\n\n    :param src: File or directory to be linked\n    :param dest: Path of link to create\n    :param linktype: 'soft' or 'hard'\n    \"\"\"\n    global WIN_SOFTLINK\n    global WIN_HARDLINK\n    WIN_NO_ERROR = 22\n\n    assert linktype in ('soft', 'hard')\n\n    path, linkpath = pathlib.Path(path), pathlib.Path(linkpath)\n\n    # Checks\n    if not path.exists():    # particularly important on Windows to prevent false success\n        raise QuiltException(\"Path to link to does not exist: {}\".format(path))\n    if linkpath.exists():\n        raise QuiltException(\"Link path already exists: {}\".format(linkpath))\n\n    # Windows\n    if os.name == 'nt':\n        # clear out any pre-existing, un-checked errors\n        ctypes.WinError()\n            \n        # Check Windows version (reasonably) supports symlinks\n        if not sys.getwindowsversion()[0] >= 6:\n            raise QuiltException(\"Unsupported operation: This version of Windows does not support linking.\")\n\n        # Acquire the windows CreateXLinkW() function\n        if linktype == 'soft':\n            if WIN_SOFTLINK is None:\n                WIN_SOFTLINK = ctypes.windll.kernel32.CreateSymbolicLinkW\n                WIN_SOFTLINK.restype = ctypes.c_bool\n            create_link = lambda l, p: WIN_SOFTLINK(str(l), str(p), p.is_dir())\n        elif linktype == 'hard':\n            if WIN_HARDLINK is None:\n                WIN_HARDLINK = ctypes.windll.kernel32.CreateHardLinkW\n                WIN_HARDLINK.restype = ctypes.c_bool\n            create_link = WIN_HARDLINK\n\n        # Call and check results\n        create_link(linkpath, path)\n        # Check WinError, because the return value for CreateSymbolicLinkW's type is suspect due to a\n        # (possible) bug: https://stackoverflow.com/questions/33010440/createsymboliclink-on-windows-10\n        # We have user results with similar effects (success reported, but not actual)\n        error = ctypes.WinError()\n        if error.winerror:\n            raise QuiltException(\"Linking failed: \" + str(error), original_error=error)\n        # Handle the case wehere linking failed and windows gave no error:\n        if not linkpath.exists() and linkpath.is_symlink():\n            raise QuiltException(\"Linking failed: Expected symlink at: {}\".format(linkpath))\n    # Linux, OSX\n    else:\n        try:\n            if linktype == 'soft':\n                linkpath.symlink_to(path)\n            elif linktype == 'hard':\n                os.link(str(path), str(linkpath))\n        except OSError as error:\n            raise QuiltException(\"Linking failed: \" + str(error), original_error=error)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef find_in_package(corenode, path):\n    if not isinstance(corenode, core.Node):\n        raise QuiltException(\"Corenode must be a valid Quilt package node. %r\" % corenode)\n\n    # Check for empty path\n    if not path:\n        return None\n\n    try:\n        node = corenode\n        count = 0\n        for part in path:\n            if not is_nodename(part):\n                raise TypeError(\"Invalid node name: {!r}\".format(part))\n            node = node.children[part]\n            count += 1\n        return node\n    except KeyError:\n        return None\n    except AttributeError:\n        traversed = '/'.join(path[:count])\n        raise TypeError(\"Not a GroupNode: Node at {!r}\".format(traversed))", "response": "Find a node in a Quilt package tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads bytes and update the progress bar.", "response": "def read(self, size=-1):\n        \"\"\"Read bytes and update the progress bar.\"\"\"\n        buf = self._fd.read(size)\n        self._progress_cb(len(buf))\n        return buf"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind a library to read and write Parquet files Find and choose a library to write Parquet files based on installed options.", "response": "def get_parquet_lib(cls):\n        \"\"\"\n        Find/choose a library to read and write Parquet files\n        based on installed options.\n        \"\"\"\n        if cls.__parquet_lib is None:\n            parq_env = os.environ.get('QUILT_PARQUET_LIBRARY', ParquetLib.ARROW.value)\n            cls.__parquet_lib = ParquetLib(parq_env)\n        return cls.__parquet_lib"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates the directories needed for the store.", "response": "def create_dirs(self):\n        \"\"\"\n        Creates the store directory and its subdirectories.\n        \"\"\"\n        if not os.path.isdir(self._path):\n            os.makedirs(self._path)\n        for dir_name in [self.OBJ_DIR, self.TMP_OBJ_DIR, self.PKG_DIR, self.CACHE_DIR]:\n            path = os.path.join(self._path, dir_name)\n            if not os.path.isdir(path):\n                os.mkdir(path)\n        if not os.path.exists(self._version_path()):\n            self._write_format_version()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the primary package directory and any additional ones from QUILT_PACKAGE_DIRS.", "response": "def find_store_dirs(cls):\n        \"\"\"\n        Returns the primary package directory and any additional ones from QUILT_PACKAGE_DIRS.\n        \"\"\"\n        store_dirs = [default_store_location()]\n        extra_dirs_str = os.getenv('QUILT_PACKAGE_DIRS')\n        if extra_dirs_str:\n            store_dirs.extend(extra_dirs_str.split(':'))\n        return store_dirs"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef find_package(cls, team, user, package, pkghash=None, store_dir=None):\n        cls.check_name(team, user, package)\n        dirs = cls.find_store_dirs()\n        for store_dir in dirs:\n            store = PackageStore(store_dir)\n            pkg = store.get_package(team, user, package, pkghash=pkghash)\n            if pkg is not None:\n                return store, pkg\n        return None, None", "response": "Finds an existing package in one of the store directories."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets a package from this store.", "response": "def get_package(self, team, user, package, pkghash=None):\n        \"\"\"\n        Gets a package from this store.\n        \"\"\"\n        self.check_name(team, user, package)\n        path = self.package_path(team, user, package)\n        if not os.path.isdir(path):\n            return None\n\n        if pkghash is None:\n            latest_tag = os.path.join(path, self.TAGS_DIR, self.LATEST)\n            if not os.path.exists(latest_tag):\n                return None\n\n            with open (latest_tag, 'r') as tagfile:\n                pkghash = tagfile.read()\n\n        assert pkghash is not None\n        contents_path = os.path.join(path, self.CONTENTS_DIR, pkghash)\n        if not os.path.isfile(contents_path):\n            return None\n\n        with open(contents_path, 'r') as contents_file:\n            try:\n                return json.load(contents_file, object_hook=decode_node)\n            except AssertionError as err:\n                if str(err).startswith(\"Bad package format\"):\n                    name = \"{}{}/{}, {}\".format(\n                        team + ':' if team else '',\n                        user,\n                        package,\n                        pkghash\n                        )\n                    raise StoreException(\"Error in {}: {}\".format(name, str(err)))\n                else:\n                    raise"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ninstalls a new package in the default package store.", "response": "def install_package(self, team, user, package, contents):\n        \"\"\"\n        Creates a new package in the default package store\n        and allocates a per-user directory if needed.\n        \"\"\"\n        self.check_name(team, user, package)\n        assert contents is not None\n\n        self.create_dirs()\n        path = self.package_path(team, user, package)\n\n        # Delete any existing data.\n        try:\n            os.remove(path)\n        except OSError:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_package_node(self, team, user, package, dry_run=False):\n        contents = RootNode(dict())\n        if dry_run:\n            return contents\n\n        self.check_name(team, user, package)\n        assert contents is not None\n        self.create_dirs()\n\n        # Delete any existing data.\n        path = self.package_path(team, user, package)\n        try:\n            os.remove(path)\n        except OSError:\n            pass\n\n        return contents", "response": "Creates a new package and initializes its contents."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef remove_package(self, team, user, package):\n        self.check_name(team, user, package)\n\n        path = self.package_path(team, user, package)\n        remove_objs = set()\n        # TODO: do we really want to delete invisible dirs?\n        if os.path.isdir(path):\n            # Collect objects from all instances for potential cleanup\n            contents_path = os.path.join(path, PackageStore.CONTENTS_DIR)\n            for instance in os.listdir(contents_path):\n                pkg = self.get_package(team, user, package, pkghash=instance)\n                remove_objs.update(find_object_hashes(pkg))\n            # Remove package manifests\n            rmtree(path)\n\n        return self.prune(remove_objs)", "response": "Removes a package from this store."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef iterpackages(self):\n        pkgdir = os.path.join(self._path, self.PKG_DIR)\n        if not os.path.isdir(pkgdir):\n            return\n        for team in sub_dirs(pkgdir):\n            for user in sub_dirs(self.team_path(team)):\n                for pkg in sub_dirs(self.user_path(team, user)):\n                    pkgpath = self.package_path(team, user, pkg)\n                    for hsh in sub_files(os.path.join(pkgpath, PackageStore.CONTENTS_DIR)):\n                        yield self.get_package(team, user, pkg, pkghash=hsh)", "response": "Iterate over all the packages in the PackageStore."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlist all packages in this store.", "response": "def ls_packages(self):\n        \"\"\"\n        List packages in this store.\n        \"\"\"\n        packages = []\n        pkgdir = os.path.join(self._path, self.PKG_DIR)\n        if not os.path.isdir(pkgdir):\n            return []\n        for team in sub_dirs(pkgdir):\n            for user in sub_dirs(self.team_path(team)):\n                for pkg in sub_dirs(self.user_path(team, user)):\n                    pkgpath = self.package_path(team, user, pkg)\n                    pkgmap = {h : [] for h in sub_files(os.path.join(pkgpath, PackageStore.CONTENTS_DIR))}\n                    for tag in sub_files(os.path.join(pkgpath, PackageStore.TAGS_DIR)):\n                        with open(os.path.join(pkgpath, PackageStore.TAGS_DIR, tag), 'r') as tagfile:\n                            pkghash = tagfile.read()\n                            pkgmap[pkghash].append(tag)\n                    for pkghash, tags in pkgmap.items():\n                        # add teams here if any other than DEFAULT_TEAM should be hidden.\n                        team_token = '' if team in (DEFAULT_TEAM,) else team + ':'\n                        fullpkg = \"{team}{owner}/{pkg}\".format(team=team_token, owner=user, pkg=pkg)\n                        # Add an empty string tag for untagged hashes\n                        displaytags = tags if tags else [\"\"]\n                        # Display a separate full line per tag like Docker\n                        for tag in displaytags:\n                            packages.append((fullpkg, str(tag), pkghash))\n\n        return packages"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef team_path(self, team=None):\n        if team is None:\n            team = DEFAULT_TEAM\n        return os.path.join(self._path, self.PKG_DIR, team)", "response": "Returns the path to the team s users package repositories."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef user_path(self, team, user):\n        return os.path.join(self.team_path(team), user)", "response": "Returns the path to the user s package repositories."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef package_path(self, team, user, package):\n        return os.path.join(self.user_path(team, user), package)", "response": "Returns the path to a package repository."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef object_path(self, objhash):\n        return os.path.join(self._path, self.OBJ_DIR, objhash)", "response": "Returns the path to an object file based on its hash."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the path to a temporary object before we know its hash.", "response": "def temporary_object_path(self, name):\n        \"\"\"\n        Returns the path to a temporary object, before we know its hash.\n        \"\"\"\n        return os.path.join(self._path, self.TMP_OBJ_DIR, name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cache_path(self, name):\n        return os.path.join(self._path, self.CACHE_DIR, name)", "response": "Returns the path to a temporary object before we know its hash."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef prune(self, objs=None):\n        if objs is None:\n            objdir = os.path.join(self._path, self.OBJ_DIR)\n            objs = os.listdir(objdir)\n        remove_objs = set(objs)\n\n        for pkg in self.iterpackages():\n            remove_objs.difference_update(find_object_hashes(pkg))\n\n        for obj in remove_objs:\n            path = self.object_path(obj)\n            if os.path.exists(path):\n                os.chmod(path, S_IWUSR)\n                os.remove(path)\n        return remove_objs", "response": "Remove objects not referenced by any packages."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_dataframe(self, hash_list):\n        self._check_hashes(hash_list)\n        parqlib = self.get_parquet_lib()\n        if parqlib is ParquetLib.SPARK:\n            return self._read_parquet_spark(hash_list)\n        elif parqlib is ParquetLib.ARROW:\n            try:\n                return self._read_parquet_arrow(hash_list)\n            except ValueError as err:\n                raise StoreException(str(err))\n        else:\n            assert False, \"Unimplemented Parquet Library %s\" % parqlib", "response": "Creates a DataFrame from a set of objects identified by hashes."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef save_dataframe(self, dataframe):\n        storepath = self.temporary_object_path(str(uuid.uuid4()))\n\n        # switch parquet lib\n        parqlib = self.get_parquet_lib()\n        if isinstance(dataframe, pd.DataFrame):\n            #parqlib is ParquetLib.ARROW: # other parquet libs are deprecated, remove?\n            import pyarrow as pa\n            from pyarrow import parquet\n            table = pa.Table.from_pandas(dataframe)\n            parquet.write_table(table, storepath)\n        elif parqlib is ParquetLib.SPARK:\n            from pyspark import sql as sparksql\n            assert isinstance(dataframe, sparksql.DataFrame)\n            dataframe.write.parquet(storepath)\n        else:\n            assert False, \"Unimplemented ParquetLib %s\" % parqlib\n\n        # Move serialized DataFrame to object store\n        if os.path.isdir(storepath): # Pyspark\n            hashes = []\n            files = [ofile for ofile in os.listdir(storepath) if ofile.endswith(\".parquet\")]\n            for obj in files:\n                path = os.path.join(storepath, obj)\n                objhash = digest_file(path)\n                self._move_to_store(path, objhash)\n                hashes.append(objhash)\n            rmtree(storepath)\n        else:\n            filehash = digest_file(storepath)\n            self._move_to_store(storepath, filehash)\n            hashes = [filehash]\n\n        return hashes", "response": "Save a DataFrame to the store."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nloads a numpy array.", "response": "def load_numpy(self, hash_list):\n        \"\"\"\n        Loads a numpy array.\n        \"\"\"\n        assert len(hash_list) == 1\n        self._check_hashes(hash_list)\n        with open(self.object_path(hash_list[0]), 'rb') as fd:\n            return np.load(fd, allow_pickle=False)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the path of the file that corresponds to the given hash list.", "response": "def get_file(self, hash_list):\n        \"\"\"\n        Returns the path of the file - but verifies that the hash is actually present.\n        \"\"\"\n        assert len(hash_list) == 1\n        self._check_hashes(hash_list)\n        return self.object_path(hash_list[0])"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsaves a file to the store.", "response": "def save_file(self, srcfile):\n        \"\"\"\n        Save a (raw) file to the store.\n        \"\"\"\n        filehash = digest_file(srcfile)\n        if not os.path.exists(self.object_path(filehash)):\n            # Copy the file to a temporary location first, then move, to make sure we don't end up with\n            # truncated contents if the build gets interrupted.\n            tmppath = self.temporary_object_path(filehash)\n            copyfile(srcfile, tmppath)\n            self._move_to_store(tmppath, filehash)\n\n        return filehash"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsaves the metadata to the store.", "response": "def save_metadata(self, metadata):\n        \"\"\"\n        Save metadata to the store.\n        \"\"\"\n        if metadata in (None, {}):\n            return None\n\n        if SYSTEM_METADATA in metadata:\n            raise StoreException(\"Not allowed to store %r in metadata\" % SYSTEM_METADATA)\n\n        path = self.temporary_object_path(str(uuid.uuid4()))\n\n        with open(path, 'w') as fd:\n            try:\n                # IMPORTANT: JSON format affects the hash of the package.\n                # In particular, it cannot contain line breaks because of Windows (LF vs CRLF).\n                # To be safe, we use the most compact encoding.\n                json.dump(metadata, fd, sort_keys=True, separators=(',', ':'))\n            except (TypeError, ValueError):\n                raise StoreException(\"Metadata is not serializable\")\n\n        metahash = digest_file(path)\n        self._move_to_store(path, metahash)\n        return metahash"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsaves the contents of the root node to a file in the local version cache.", "response": "def save_package_contents(self, root, team, owner, pkgname):\n        \"\"\"\n        Saves the in-memory contents to a file in the local\n        package repository.\n        \"\"\"\n        assert isinstance(root, RootNode)\n        instance_hash = hash_contents(root)\n        pkg_path = self.package_path(team, owner, pkgname)\n        if not os.path.isdir(pkg_path):\n            os.makedirs(pkg_path)\n            os.mkdir(os.path.join(pkg_path, self.CONTENTS_DIR))\n            os.mkdir(os.path.join(pkg_path, self.TAGS_DIR))\n            os.mkdir(os.path.join(pkg_path, self.VERSIONS_DIR))\n\n        dest = os.path.join(pkg_path, self.CONTENTS_DIR, instance_hash)\n        with open(dest, 'w') as contents_file:\n            json.dump(root, contents_file, default=encode_node, indent=2, sort_keys=True)\n\n        tag_dir = os.path.join(pkg_path, self.TAGS_DIR)\n        if not os.path.isdir(tag_dir):\n            os.mkdir(tag_dir)\n\n        latest_tag = os.path.join(pkg_path, self.TAGS_DIR, self.LATEST)\n        with open (latest_tag, 'w') as tagfile:\n            tagfile.write(\"{hsh}\".format(hsh=instance_hash))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmoving the object at srcpath to destpath.", "response": "def _move_to_store(self, srcpath, objhash):\n        \"\"\"\n        Make the object read-only and move it to the store.\n        \"\"\"\n        destpath = self.object_path(objhash)\n        if os.path.exists(destpath):\n            # Windows: delete any existing object at the destination.\n            os.chmod(destpath, S_IWUSR)\n            os.remove(destpath)\n        os.chmod(srcpath, S_IRUSR | S_IRGRP | S_IROTH)  # Make read-only\n        move(srcpath, destpath)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding an object to the contents of the package.", "response": "def _add_to_package_contents(self, pkgroot, node_path, hashes, target,\n                                 source_path, transform, user_meta_hash):\n        \"\"\"\n        Adds an object (name-hash mapping) or group to package contents.\n        \"\"\"\n        assert isinstance(node_path, list)\n        assert user_meta_hash is None or isinstance(user_meta_hash, str)\n\n        contents = pkgroot\n\n        if not node_path:\n            # Allow setting metadata on the root node, but that's it.\n            assert target is TargetType.GROUP\n            contents.metadata_hash = user_meta_hash\n            return\n\n        ptr = contents\n        for node in node_path[:-1]:\n            ptr = ptr.children[node]\n\n        if target is TargetType.GROUP:\n            node = GroupNode(\n                children=dict(),\n                metadata_hash=user_meta_hash\n            )\n        else:\n            node = FileNode( # pylint:disable=R0204\n                hashes=hashes,\n                metadata=dict(\n                    q_ext=transform,\n                    q_path=source_path,\n                    q_target=target.value\n                ),\n                metadata_hash=user_meta_hash\n            )\n\n        ptr.children[node_path[-1]] = node"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_to_package_numpy(self, root, ndarray, node_path, target, source_path, transform, custom_meta):\n        filehash = self.save_numpy(ndarray)\n        metahash = self.save_metadata(custom_meta)\n        self._add_to_package_contents(root, node_path, [filehash], target, source_path, transform, metahash)", "response": "Add a Numpy array to the package."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a file to the package.", "response": "def add_to_package_file(self, root, srcfile, node_path, target, source_path, transform, custom_meta):\n        \"\"\"\n        Save a (raw) file to the store.\n        \"\"\"\n        filehash = self.save_file(srcfile)\n        metahash = self.save_metadata(custom_meta)\n        self._add_to_package_contents(root, node_path, [filehash], target, source_path, transform, metahash)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_to_package_package_tree(self, root, node_path, pkgnode):\n        if node_path:\n            ptr = root\n            for node in node_path[:-1]:\n                ptr = ptr.children.setdefault(node, GroupNode(dict()))\n            ptr.children[node_path[-1]] = pkgnode\n        else:\n            if root.children:\n                raise PackageException(\"Attempting to overwrite root node of a non-empty package.\")\n            root.children = pkgnode.children.copy()", "response": "Adds a package or sub - package tree from an existing package to this package s\n        contents."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninstalls an interrupt handler for the clean .", "response": "def _install_interrupt_handler():\n    \"\"\"Suppress KeyboardInterrupt traceback display in specific situations\n\n    If not running in dev mode, and if executed from the command line, then\n    we raise SystemExit instead of KeyboardInterrupt.  This provides a clean\n    exit.\n\n    :returns: None if no action is taken, original interrupt handler otherwise\n    \"\"\"\n    # These would clutter the quilt.x namespace, so they're imported here instead.\n    import os\n    import sys\n    import signal\n    import pkg_resources\n    from .tools import const\n\n    # Check to see what entry points / scripts are configred to run quilt from the CLI\n    # By doing this, we have these benefits:\n    #   * Avoid closing someone's Jupyter/iPython/bPython session when they hit ctrl-c\n    #   * Avoid calling exit() when being used as an external lib\n    #   * Provide exceptions when running in Jupyter/iPython/bPython\n    #   * Provide exceptions when running in unexpected circumstances\n    quilt = pkg_resources.get_distribution('quilt')\n    executable = os.path.basename(sys.argv[0])\n    entry_points = quilt.get_entry_map().get('console_scripts', [])\n\n    # When python is run with '-c', this was executed via 'python -c \"<some python code>\"'\n    if executable == '-c':\n        # This is awkward and somewhat hackish, but we have to ensure that this is *us*\n        # executing via 'python -c'\n        if len(sys.argv) > 1 and sys.argv[1] == 'quilt testing':\n            # it's us.  Let's pretend '-c' is an entry point.\n            entry_points['-c'] = 'blah'\n            sys.argv.pop(1)\n    if executable not in entry_points:\n        return\n\n    # We're running as a console script.\n    # If not in dev mode, use SystemExit instead of raising KeyboardInterrupt\n    def handle_interrupt(signum, stack):\n        # Check for dev mode\n        if _DEV_MODE is None:\n            # Args and environment have not been parsed, and no _DEV_MODE state has been set.\n            dev_mode = True if len(sys.argv) > 1 and sys.argv[1] == '--dev' else False\n            dev_mode = True if os.environ.get('QUILT_DEV_MODE', '').strip().lower() == 'true' else dev_mode\n        else:  # Use forced dev-mode if _DEV_MODE is set\n            dev_mode = _DEV_MODE\n\n        # In order to display the full traceback, we lose control of the exit code here.\n        # Dev mode ctrl-c exit just produces the generic exit error code 1\n        if dev_mode:\n            raise KeyboardInterrupt()\n        # Normal exit\n        # avoid annoying prompt displacement when hitting ctrl-c\n        print()\n        exit(const.EXIT_KB_INTERRUPT)\n\n    return signal.signal(signal.SIGINT, handle_interrupt)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef save(package, data, params={}, is_public=False):\n    for key, value in params.items():\n        if isinstance(value, np.ndarray):\n            value = value.astype(float)\n            params[key] = value.tolist()\n    build_from_node(package, nodes.DataNode(None, None, data, params))\n    push('{}'.format(package), is_public=is_public)", "response": "Save data to Quilt registry at user / package"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _data(self, asa=None):\n        if asa is not None:\n            if self._store is None or self._hashes is None:\n                msg = (\n                    \"Can only use asa functions with built dataframes.\"\n                    \" Build this package and try again.\"\n                )\n                raise ValueError(msg)\n            return asa(self, [self._store.object_path(obj) for obj in self._hashes])\n        else:\n            if self.__cached_data is None:\n                # TODO(dima): Temporary code.\n                if self._target() == TargetType.PANDAS:\n                    self.__cached_data = self._store.load_dataframe(self._hashes)\n                elif self._target() == TargetType.NUMPY:\n                    self.__cached_data = self._store.load_numpy(self._hashes)\n                else:\n                    self.__cached_data = self._store.get_file(self._hashes)\n            return self.__cached_data", "response": "Returns the contents of the node as a dataframe or a file path."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a list of all the data keys that are referenced by this object.", "response": "def _data_keys(self):\n        \"\"\"\n        every child key referencing a dataframe\n        \"\"\"\n        return [name for name, child in iteritems(self._children) if not isinstance(child, GroupNode)]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of all child keys referencing a group that is not a dataframe", "response": "def _group_keys(self):\n        \"\"\"\n        every child key referencing a group that is not a dataframe\n        \"\"\"\n        return [name for name, child in iteritems(self._children) if isinstance(child, GroupNode)]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the data for this group.", "response": "def _data(self, asa=None):\n        \"\"\"\n        Merges all child dataframes. Only works for dataframes stored on disk - not in memory.\n        \"\"\"\n        hash_list = []\n        stack = [self]\n        alldfs = True\n        store = None\n        while stack:\n            node = stack.pop()\n            if isinstance(node, GroupNode):\n                stack.extend(child for _, child in sorted(node._items(), reverse=True))\n            else:\n                if node._target() != TargetType.PANDAS:\n                    alldfs = False\n                if node._store is None or node._hashes is None:\n                    msg = \"Can only merge built dataframes. Build this package and try again.\"\n                    raise NotImplementedError(msg)\n                node_store = node._store\n                if store is None:\n                    store = node_store\n                if node_store != store:\n                    raise NotImplementedError(\"Can only merge dataframes from the same store\")\n                hash_list += node._hashes\n\n        if asa is None:\n            if not hash_list:\n                return None\n            if not alldfs:\n                raise ValueError(\"Group contains non-dataframe nodes\")\n            return store.load_dataframe(hash_list)\n        else:\n            if hash_list:\n                assert store is not None\n                return asa(self, [store.object_path(obj) for obj in hash_list])\n            else:\n                return asa(self, [])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _set(self, path, value, build_dir=''):\n        assert isinstance(path, list) and len(path) > 0\n\n        if isinstance(value, pd.DataFrame):\n            metadata = {SYSTEM_METADATA: {'target': TargetType.PANDAS.value}}\n        elif isinstance(value, np.ndarray):\n            metadata = {SYSTEM_METADATA: {'target': TargetType.NUMPY.value}}\n        elif isinstance(value, string_types + (bytes,)):\n            # bytes -> string for consistency when retrieving metadata\n            value = value.decode() if isinstance(value, bytes) else value\n            if os.path.isabs(value):\n                raise ValueError(\"Invalid path: expected a relative path, but received {!r}\".format(value))\n            # Security: filepath does not and should not retain the build_dir's location!\n            metadata = {SYSTEM_METADATA: {'filepath': value, 'transform': 'id'}}\n            if build_dir:\n                value = os.path.join(build_dir, value)\n        else:\n            accepted_types = tuple(set((pd.DataFrame, np.ndarray, bytes) + string_types))\n            raise TypeError(\"Bad value type: Expected instance of any type {!r}, but received type {!r}\"\n                            .format(accepted_types, type(value)), repr(value)[0:100])\n\n        for key in path:\n            if not is_nodename(key):\n                raise ValueError(\"Invalid name for node: {}\".format(key))\n\n        node = self\n        for key in path[:-1]:\n            child = node._get(key)\n            if not isinstance(child, GroupNode):\n                child = GroupNode({})\n                node[key] = child\n\n            node = child\n\n        key = path[-1]\n        node[key] = DataNode(None, None, value, metadata)", "response": "Create and set a node by path."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nhandle an API exception and returns a JSON response.", "response": "def handle_api_exception(error):\n    \"\"\"\n    Converts an API exception into an error response.\n    \"\"\"\n    _mp_track(\n        type=\"exception\",\n        status_code=error.status_code,\n        message=error.message,\n    )\n\n    response = jsonify(dict(\n        message=error.message\n    ))\n    response.status_code = error.status_code\n    return response"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_package(auth, owner, package_name):\n    package = (\n        Package.query\n        .filter_by(owner=owner, name=package_name)\n        .join(Package.access)\n        .filter(_access_filter(auth))\n        .one_or_none()\n    )\n    if package is None:\n        raise PackageNotFoundException(owner, package_name, auth.is_logged_in)\n    return package", "response": "Helper for looking up a package and checking permissions."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if the current user is allowed to create private packages.", "response": "def _private_packages_allowed():\n    \"\"\"\n    Checks if the current user is allowed to create private packages.\n\n    In the public cloud, the user needs to be on a paid plan.\n    There are no restrictions in other deployments.\n    \"\"\"\n    if not HAVE_PAYMENTS or TEAM_ID:\n        return True\n\n    customer = _get_or_create_customer()\n    plan = _get_customer_plan(customer)\n    return plan != PaymentPlan.FREE"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread the credentials updates the access token if necessary and returns it.", "response": "def _create_auth(team, timeout=None):\n    \"\"\"\n    Reads the credentials, updates the access token if necessary, and returns it.\n    \"\"\"\n    url = get_registry_url(team)\n    contents = _load_auth()\n    auth = contents.get(url)\n\n    if auth is not None:\n        # If the access token expires within a minute, update it.\n        if auth['expires_at'] < time.time() + 60:\n            try:\n                auth = _update_auth(team, auth['refresh_token'], timeout)\n            except CommandException as ex:\n                raise CommandException(\n                    \"Failed to update the access token (%s). Run `quilt login%s` again.\" %\n                    (ex, ' ' + team if team else '')\n                )\n            contents[url] = auth\n            _save_auth(contents)\n\n    return auth"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _create_session(team, auth):\n    session = requests.Session()\n    session.hooks.update(dict(\n        response=partial(_handle_response, team)\n    ))\n    session.headers.update({\n        \"Content-Type\": \"application/json\",\n        \"Accept\": \"application/json\",\n        \"User-Agent\": \"quilt-cli/%s (%s %s) %s/%s\" % (\n            VERSION, platform.system(), platform.release(),\n            platform.python_implementation(), platform.python_version()\n        )\n    })\n    if auth is not None:\n        session.headers[\"Authorization\"] = \"Bearer %s\" % auth['access_token']\n\n    return session", "response": "Create a session object to be used for push install etc."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_session(team, timeout=None):\n    global _sessions            # pylint:disable=C0103\n    session = _sessions.get(team)\n    if session is None:\n        auth = _create_auth(team, timeout)\n        _sessions[team] = session = _create_session(team, auth)\n\n    assert session is not None\n\n    return session", "response": "Creates a session or returns an existing session."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _check_team_login(team):\n    contents = _load_auth()\n\n    for auth in itervalues(contents):\n        existing_team = auth.get('team')\n        if team and team != existing_team:\n            raise CommandException(\n                \"Can't log in as team %r; log out first.\" % team\n            )\n        elif not team and existing_team:\n            raise CommandException(\n                \"Can't log in as a public user; log out from team %r first.\" % existing_team\n            )", "response": "Check if team login is allowed."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks that the team registry actually exists.", "response": "def _check_team_exists(team):\n    \"\"\"\n    Check that the team registry actually exists.\n    \"\"\"\n    if team is None:\n        return\n\n    hostname = urlparse(get_registry_url(team)).hostname\n    try:\n        socket.gethostbyname(hostname)\n    except IOError:\n        try:\n            # Do we have internet?\n            socket.gethostbyname('quiltdata.com')\n        except IOError:\n            message = \"Can't find quiltdata.com. Check your internet connection.\"\n        else:\n            message = \"Unable to connect to registry. Is the team name %r correct?\" % team\n        raise CommandException(message)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nauthenticates Launches a web browser and asks the user for a token.", "response": "def login(team=None):\n    \"\"\"\n    Authenticate.\n\n    Launches a web browser and asks the user for a token.\n    \"\"\"\n    _check_team_id(team)\n    _check_team_exists(team)\n    _check_team_login(team)\n\n    login_url = \"%s/login\" % get_registry_url(team)\n\n    print(\"Launching a web browser...\")\n    print(\"If that didn't work, please visit the following URL: %s\" % login_url)\n\n    _open_url(login_url)\n\n    print()\n    refresh_token = input(\"Enter the code from the webpage: \")\n\n    login_with_token(refresh_token, team)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef login_with_token(refresh_token, team=None):\n    # Get an access token and a new refresh token.\n    _check_team_id(team)\n    auth = _update_auth(team, refresh_token)\n\n    url = get_registry_url(team)\n    contents = _load_auth()\n    contents[url] = auth\n    _save_auth(contents)\n\n    _clear_session(team)", "response": "Authenticate using an existing token and a new refresh token."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef generate(directory, outfilename=DEFAULT_BUILDFILE):\n    try:\n        buildfilepath = generate_build_file(directory, outfilename=outfilename)\n    except BuildException as builderror:\n        raise CommandException(str(builderror))\n\n    print(\"Generated build-file %s.\" % (buildfilepath))", "response": "Generate a build - file for quilt build from a directory of\n    source files."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check(path=None, env='default'):\n    # TODO: add files=<list of files> to check only a subset...\n    # also useful for 'quilt build' to exclude certain files?\n    # (if not, then require dry_run=True if files!=None/all)\n    build(\"dry_run/dry_run\", path=path, dry_run=True, env=env)", "response": "Execute the checks for a given build. yml file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef build(package, path=None, dry_run=False, env='default', force=False, build_file=False):\n    # TODO: rename 'path' param to 'target'?\n    team, _, _, subpath = parse_package(package, allow_subpath=True)\n    _check_team_id(team)\n    logged_in_team = _find_logged_in_team()\n    if logged_in_team is not None and team is None and force is False:\n        answer = input(\"You're logged in as a team member, but you aren't specifying \"\n                       \"a team for the package you're currently building. Maybe you meant:\\n\"\n                       \"quilt build {team}:{package}\\n\"\n                       \"Are you sure you want to continue? (y/N) \".format(\n                                team=logged_in_team, package=package))\n        if answer.lower() != 'y':\n            return\n\n    # Backward compatibility: if there's no subpath, we're building a top-level package,\n    # so treat `path` as a build file, not as a data node.\n    if not subpath:\n        build_file = True\n\n    package_hash = hashlib.md5(package.encode('utf-8')).hexdigest()\n    try:\n        _build_internal(package, path, dry_run, env, build_file)\n    except Exception as ex:\n        _log(team, type='build', package=package_hash, dry_run=dry_run, env=env, error=str(ex))\n        raise\n    _log(team, type='build', package=package_hash, dry_run=dry_run, env=env)", "response": "Compile a Quilt data package from a build file or an existing package node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbuild a Quilt data package from an existing package node.", "response": "def build_from_node(package, node):\n    \"\"\"\n    Compile a Quilt data package from an existing package node.\n    \"\"\"\n    team, owner, pkg, subpath = parse_package(package, allow_subpath=True)\n    _check_team_id(team)\n    store = PackageStore()\n\n    pkg_root = get_or_create_package(store, team, owner, pkg, subpath)\n\n    if not subpath and not isinstance(node, nodes.GroupNode):\n        raise CommandException(\"Top-level node must be a group\")\n\n    def _process_node(node, path):\n        if not isinstance(node._meta, dict):\n            raise CommandException(\n                \"Error in %s: value must be a dictionary\" % '.'.join(path + ['_meta'])\n            )\n        meta = dict(node._meta)\n        system_meta = meta.pop(SYSTEM_METADATA, {})\n        if not isinstance(system_meta, dict):\n            raise CommandException(\n                \"Error in %s: %s overwritten. %s is a reserved metadata key. Try a different key.\" %\n                ('.'.join(path + ['_meta']), SYSTEM_METADATA, SYSTEM_METADATA)\n            )\n        if isinstance(node, nodes.GroupNode):\n            store.add_to_package_group(pkg_root, path, meta)\n            for key, child in node._items():\n                _process_node(child, path + [key])\n        elif isinstance(node, nodes.DataNode):\n            # TODO: Reuse existing fragments if we have them.\n            data = node._data()\n            filepath = system_meta.get('filepath')\n            transform = system_meta.get('transform')\n            if isinstance(data, pd.DataFrame):\n                store.add_to_package_df(pkg_root, data, path, TargetType.PANDAS, filepath, transform, meta)\n            elif isinstance(data, np.ndarray):\n                store.add_to_package_numpy(pkg_root, data, path, TargetType.NUMPY, filepath, transform, meta)\n            elif isinstance(data, string_types):\n                store.add_to_package_file(pkg_root, data, path, TargetType.FILE, filepath, transform, meta)\n            else:\n                assert False, \"Unexpected data type: %r\" % data\n        else:\n            assert False, \"Unexpected node type: %r\" % node\n\n    try:\n        _process_node(node, subpath)\n    except StoreException as ex:\n        raise CommandException(\"Failed to build the package: %s\" % ex)\n\n    store.save_package_contents(pkg_root, team, owner, pkg)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef build_from_path(package, path, dry_run=False, env='default', outfilename=DEFAULT_BUILDFILE):\n    team, owner, pkg, subpath = parse_package(package, allow_subpath=True)\n\n    if not os.path.exists(path):\n        raise CommandException(\"%s does not exist.\" % path)\n\n    try:\n        if os.path.isdir(path):\n            buildpath = os.path.join(path, outfilename)\n            if os.path.exists(buildpath):\n                raise CommandException(\n                    \"Build file already exists. Run `quilt build %r` instead.\" % buildpath\n                )\n\n            contents = generate_contents(path, outfilename)\n            build_package_from_contents(team, owner, pkg, subpath, path, contents, dry_run=dry_run, env=env)\n        else:\n            build_package(team, owner, pkg, subpath, path, dry_run=dry_run, env=env)\n\n        if not dry_run:\n            print(\"Built %s successfully.\" % package)\n    except BuildException as ex:\n        raise CommandException(\"Failed to build the package: %s\" % ex)", "response": "Compile a Quilt data package from a build file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef log(package):\n    team, owner, pkg = parse_package(package)\n    session = _get_session(team)\n\n    response = session.get(\n        \"{url}/api/log/{owner}/{pkg}/\".format(\n            url=get_registry_url(team),\n            owner=owner,\n            pkg=pkg\n        )\n    )\n\n    table = [(\"Hash\", \"Pushed\", \"Author\", \"Tags\", \"Versions\")]\n    for entry in reversed(response.json()['logs']):\n        ugly = datetime.fromtimestamp(entry['created'])\n        nice = ugly.strftime(\"%Y-%m-%d %H:%M:%S\")\n        table.append((entry['hash'], nice, entry['author'],\n            str(entry.get('tags', [])), str(entry.get('versions', []))))\n    _print_table(table)", "response": "List all of the changes to a package on the server."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\npush a Quilt data package to the server", "response": "def push(package, is_public=False, is_team=False, reupload=False, hash=None):\n    \"\"\"\n    Push a Quilt data package to the server\n    \"\"\"\n    team, owner, pkg, subpath = parse_package(package, allow_subpath=True)\n    _check_team_id(team)\n    session = _get_session(team)\n\n    store, pkgroot = PackageStore.find_package(team, owner, pkg, pkghash=hash)\n    if pkgroot is None:\n        raise CommandException(\"Package {package} not found.\".format(package=package))\n\n    pkghash = hash_contents(pkgroot)\n    if hash is not None:\n        assert pkghash == hash\n    contents = pkgroot\n\n    for component in subpath:\n        try:\n            contents = contents.children[component]\n        except (AttributeError, KeyError):\n            raise CommandException(\"Invalid subpath: %r\" % component)\n\n    def _push_package(dry_run=False, sizes=dict()):\n        data = json.dumps(dict(\n            dry_run=dry_run,\n            is_public=is_public,\n            is_team=is_team,\n            contents=contents,\n            description=\"\",  # TODO\n            sizes=sizes\n        ), default=encode_node)\n\n        compressed_data = gzip_compress(data.encode('utf-8'))\n\n        if subpath:\n            return session.post(\n                \"{url}/api/package_update/{owner}/{pkg}/{subpath}\".format(\n                    url=get_registry_url(team),\n                    owner=owner,\n                    pkg=pkg,\n                    subpath='/'.join(subpath)\n                ),\n                data=compressed_data,\n                headers={\n                    'Content-Encoding': 'gzip'\n                }\n            )\n        else:\n            return session.put(\n                \"{url}/api/package/{owner}/{pkg}/{hash}\".format(\n                    url=get_registry_url(team),\n                    owner=owner,\n                    pkg=pkg,\n                    hash=pkghash\n                ),\n                data=compressed_data,\n                headers={\n                    'Content-Encoding': 'gzip'\n                }\n            )\n\n    print(\"Fetching upload URLs from the registry...\")\n    resp = _push_package(dry_run=True)\n    obj_urls = resp.json()['upload_urls']\n\n    assert set(obj_urls) == set(find_object_hashes(contents))\n\n    obj_sizes = {\n        obj_hash: os.path.getsize(store.object_path(obj_hash)) for obj_hash in obj_urls\n    }\n\n    success = upload_fragments(store, obj_urls, obj_sizes, reupload=reupload)\n    if not success:\n        raise CommandException(\"Failed to upload fragments\")\n\n    print(\"Uploading package metadata...\")\n    resp = _push_package(sizes=obj_sizes)\n    package_url = resp.json()['package_url']\n\n    if not subpath:\n        # Update the latest tag.\n        print(\"Updating the 'latest' tag...\")\n        session.put(\n            \"{url}/api/tag/{owner}/{pkg}/{tag}\".format(\n                url=get_registry_url(team),\n                owner=owner,\n                pkg=pkg,\n                tag=LATEST_TAG\n            ),\n            data=json.dumps(dict(\n                hash=pkghash\n            ))\n        )\n\n    print(\"Push complete. %s is live:\\n%s\" % (package, package_url))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef version_list(package):\n    team, owner, pkg = parse_package(package)\n    session = _get_session(team)\n\n    response = session.get(\n        \"{url}/api/version/{owner}/{pkg}/\".format(\n            url=get_registry_url(team),\n            owner=owner,\n            pkg=pkg\n        )\n    )\n\n    for version in response.json()['versions']:\n        print(\"%s: %s\" % (version['version'], version['hash']))", "response": "List the versions of a package."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds a new version for a given package hash.", "response": "def version_add(package, version, pkghash, force=False):\n    \"\"\"\n    Add a new version for a given package hash.\n\n    Version format needs to follow PEP 440.\n    Versions are permanent - once created, they cannot be modified or deleted.\n    \"\"\"\n    team, owner, pkg = parse_package(package)\n    session = _get_session(team)\n\n    try:\n        Version(version)\n    except ValueError:\n        url = \"https://www.python.org/dev/peps/pep-0440/#examples-of-compliant-version-schemes\"\n        raise CommandException(\n            \"Invalid version format; see %s\" % url\n        )\n\n    if not force:\n        answer = input(\"Versions cannot be modified or deleted; are you sure? (y/n) \")\n        if answer.lower() != 'y':\n            return\n\n    session.put(\n        \"{url}/api/version/{owner}/{pkg}/{version}\".format(\n            url=get_registry_url(team),\n            owner=owner,\n            pkg=pkg,\n            version=version\n        ),\n        data=json.dumps(dict(\n            hash=_match_hash(package, pkghash)\n        ))\n    )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef tag_list(package):\n    team, owner, pkg = parse_package(package)\n    session = _get_session(team)\n\n    response = session.get(\n        \"{url}/api/tag/{owner}/{pkg}/\".format(\n            url=get_registry_url(team),\n            owner=owner,\n            pkg=pkg\n        )\n    )\n\n    for tag in response.json()['tags']:\n        print(\"%s: %s\" % (tag['tag'], tag['hash']))", "response": "List the tags of a package."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef tag_add(package, tag, pkghash):\n    team, owner, pkg = parse_package(package)\n    session = _get_session(team)\n\n    session.put(\n        \"{url}/api/tag/{owner}/{pkg}/{tag}\".format(\n            url=get_registry_url(team),\n            owner=owner,\n            pkg=pkg,\n            tag=tag\n        ),\n        data=json.dumps(dict(\n            hash=_match_hash(package, pkghash)\n        ))\n    )", "response": "Add a new tag for a given package hash."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndownloads multiple Quilt data packages via quilt. xml requirements file.", "response": "def install_via_requirements(requirements_str, force=False):\n    \"\"\"\n    Download multiple Quilt data packages via quilt.xml requirements file.\n    \"\"\"\n    if requirements_str[0] == '@':\n        path = requirements_str[1:]\n        if os.path.isfile(path):\n            yaml_data = load_yaml(path)\n            if 'packages' not in yaml_data.keys():\n                raise CommandException('Error in {filename}: missing \"packages\" node'.format(filename=path))\n        else:\n            raise CommandException(\"Requirements file not found: {filename}\".format(filename=path))\n    else:\n        yaml_data = yaml.safe_load(requirements_str)\n    for pkginfo in yaml_data['packages']:\n        info = parse_package_extended(pkginfo)\n        install(info.full_name, info.hash, info.version, info.tag, force=force)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninstalling a Quilt data package from the server and install locally.", "response": "def install(package, hash=None, version=None, tag=None, force=False, meta_only=False):\n    \"\"\"\n    Download a Quilt data package from the server and install locally.\n\n    At most one of `hash`, `version`, or `tag` can be given. If none are\n    given, `tag` defaults to \"latest\".\n\n    `package` may be a node tree - in which case, its fragments get downloaded.\n    No other parameters are allowed.\n    \"\"\"\n    if isinstance(package, nodes.Node):\n        if not (hash is version is tag is None and force is meta_only is False):\n            raise ValueError(\"Parameters not allowed when installing a node\")\n        _materialize(package)\n        return\n\n    if hash is version is tag is None:\n        tag = LATEST_TAG\n\n    # @filename ==> read from file\n    # newline = multiple lines ==> multiple requirements\n    package = package.strip()\n    if len(package) == 0:\n        raise CommandException(\"package name is empty.\")\n\n    if package[0] == '@' or '\\n' in package:\n        return install_via_requirements(package, force=force)\n\n    assert [hash, version, tag].count(None) == 2\n\n    team, owner, pkg, subpath = parse_package(package, allow_subpath=True)\n    _check_team_id(team)\n    session = _get_session(team)\n    store = PackageStore()\n    existing_pkg = store.get_package(team, owner, pkg)\n\n    print(\"Downloading package metadata...\")\n\n    try:\n        if version is not None:\n            response = session.get(\n                \"{url}/api/version/{owner}/{pkg}/{version}\".format(\n                    url=get_registry_url(team),\n                    owner=owner,\n                    pkg=pkg,\n                    version=version\n                )\n            )\n            pkghash = response.json()['hash']\n        elif tag is not None:\n            response = session.get(\n                \"{url}/api/tag/{owner}/{pkg}/{tag}\".format(\n                    url=get_registry_url(team),\n                    owner=owner,\n                    pkg=pkg,\n                    tag=tag\n                )\n            )\n            pkghash = response.json()['hash']\n        else:\n            pkghash = _match_hash(package, hash)\n    except HTTPResponseException as e:\n        logged_in_team = _find_logged_in_team()\n        if (team is None and logged_in_team is not None\n                and e.response.status_code == requests.codes.not_found):\n            raise CommandException(\"Package {owner}/{pkg} does not exist. \"\n                                   \"Maybe you meant {team}:{owner}/{pkg}?\".format(\n                                   owner=owner, pkg=pkg, team=logged_in_team))\n        else:\n            raise\n\n    assert pkghash is not None\n\n    response = session.get(\n        \"{url}/api/package/{owner}/{pkg}/{hash}\".format(\n            url=get_registry_url(team),\n            owner=owner,\n            pkg=pkg,\n            hash=pkghash\n        ),\n        params=dict(\n            subpath='/'.join(subpath),\n            meta_only='true' if meta_only else ''\n        )\n    )\n    assert response.ok # other responses handled by _handle_response\n\n    if existing_pkg is not None and not force:\n        print(\"{package} already installed.\".format(package=package))\n        overwrite = input(\"Overwrite? (y/n) \")\n        if overwrite.lower() != 'y':\n            return\n\n    dataset = response.json(object_hook=decode_node)\n    contents = dataset['contents']\n\n    # Verify contents hash\n    if pkghash != hash_contents(contents):\n        raise CommandException(\"Mismatched hash. Try again.\")\n\n    # TODO: Shouldn't need this? At least don't need the contents\n    store.install_package(team, owner, pkg, contents)\n\n    obj_urls = dataset['urls']\n    obj_sizes = dataset['sizes']\n\n    # Skip the objects we already have\n    for obj_hash in list(obj_urls):\n        if os.path.exists(store.object_path(obj_hash)):\n            del obj_urls[obj_hash]\n            del obj_sizes[obj_hash]\n\n    if obj_urls:\n        success = download_fragments(store, obj_urls, obj_sizes)\n        if not success:\n            raise CommandException(\"Failed to download fragments\")\n    else:\n        print(\"Fragments already downloaded\")\n\n    store.save_package_contents(contents, team, owner, pkg)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprint list of users who can access a package.", "response": "def access_list(package):\n    \"\"\"\n    Print list of users who can access a package.\n    \"\"\"\n    team, owner, pkg = parse_package(package)\n    session = _get_session(team)\n\n    lookup_url = \"{url}/api/access/{owner}/{pkg}/\".format(url=get_registry_url(team), owner=owner, pkg=pkg)\n    response = session.get(lookup_url)\n\n    data = response.json()\n    users = data['users']\n\n    print('\\n'.join(users))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd access to a node.", "response": "def access_add(package, user):\n    \"\"\"\n    Add access\n    \"\"\"\n    team, owner, pkg = parse_package(package)\n    session = _get_session(team)\n\n    session.put(\"%s/api/access/%s/%s/%s\" % (get_registry_url(team), owner, pkg, user))\n    print(u'Access added for %s' % user)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nremoves access for a user from a package", "response": "def access_remove(package, user):\n    \"\"\"\n    Remove access\n    \"\"\"\n    team, owner, pkg = parse_package(package)\n    session = _get_session(team)\n\n    session.delete(\"%s/api/access/%s/%s/%s\" % (get_registry_url(team), owner, pkg, user))\n    print(u'Access removed for %s' % user)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndeleting a package from the server.", "response": "def delete(package):\n    \"\"\"\n    Delete a package from the server.\n\n    Irreversibly deletes the package along with its history, tags, versions, etc.\n    \"\"\"\n    team, owner, pkg = parse_package(package)\n\n    answer = input(\n        \"Are you sure you want to delete this package and its entire history? \"\n        \"Type '%s' to confirm: \" % package\n    )\n\n    if answer != package:\n        print(\"Not deleting.\")\n        return 1\n\n    session = _get_session(team)\n\n    session.delete(\"%s/api/package/%s/%s/\" % (get_registry_url(team), owner, pkg))\n    print(\"Deleted.\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ls():                       # pylint:disable=C0103\n    for pkg_dir in PackageStore.find_store_dirs():\n        print(\"%s\" % pkg_dir)\n        packages = PackageStore(pkg_dir).ls_packages()\n        for package, tag, pkghash in sorted(packages):\n            print(\"{0:30} {1:20} {2}\".format(package, tag, pkghash))", "response": "List all installed Quilt data packages"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef rm(package, force=False):\n    team, owner, pkg = parse_package(package)\n\n    if not force:\n        confirmed = input(\"Remove {0}? (y/n) \".format(package))\n        if confirmed.lower() != 'y':\n            return\n\n    store = PackageStore()\n    deleted = store.remove_package(team, owner, pkg)\n    for obj in deleted:\n        print(\"Removed: {0}\".format(obj))", "response": "Remove a package from the local store."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load(pkginfo, hash=None):\n    node, pkgroot, info = _load(pkginfo, hash)\n    for subnode_name in info.subpath:\n        node = node[subnode_name]\n\n    return node", "response": "load a node from a package info"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nexports the specified node or children to files.", "response": "def export(package, output_path='.', force=False, symlinks=False):\n    \"\"\"Export package file data.\n\n    Exports specified node (or its children) to files.\n\n    `symlinks`\n    **Warning** This is an advanced feature, use at your own risk.\n                You must have a thorough understanding of permissions,\n                and it is your responsibility to ensure that the linked\n                files are not modified.  If they do become modified, it\n                may corrupt your package store, and you may lose data,\n                or you may use or distribute incorrect data.\n    If `symlinks` is `True`:\n      * Nodes that point to binary data will be symlinked instead of copied.\n      * All other specified nodes (columnar data) will be exported as normal.\n\n    :param package: package/subpackage name, e.g., user/foo or user/foo/bar\n    :param output_path: distination folder\n    :param symlinks: Use at your own risk.  See full description above.\n    :param force: if True, overwrite existing files\n    \"\"\"\n    # TODO: (future) Support other tags/versions (via load(), probably)\n    # TODO: (future) This would be *drastically* simplified by a 1:1 source-file to node-name correlation\n    # TODO: (future) This would be significantly simplified if node objects with useful accessors existed\n    #       (nodes.Node and subclasses are being phased out, per Kevin)\n\n    if symlinks is True:\n        from quilt import _DEV_MODE\n        if not _DEV_MODE:\n            response = input(\"Warning: Exporting using symlinks to the package store.\\n\"\n                \"\\tThis is an advanced feature.\\n\"\n                \"\\tThe package store must not be written to directly, or it may be corrupted.\\n\"\n                \"\\tManaging permissions and usage are your responsibility.\\n\\n\"\n                \"Are you sure you want to continue? (yes/No) \")\n            if response != 'yes':\n                raise CommandException(\"No action taken: 'yes' not given\")\n    ## Helpers\n    # Perhaps better as Node.iteritems()\n    def iteritems(node, base_path=None, recursive=False):\n        if base_path is None:\n            base_path = pathlib.PureWindowsPath()\n        assert isinstance(node, nodes.GroupNode)\n\n        for name, child_node in node._items():\n            child_path = base_path / name\n            yield child_path, child_node\n            if recursive and isinstance(child_node, nodes.GroupNode):\n                for subpath, subnode in iteritems(child_node, child_path, recursive):\n                    yield subpath, subnode\n\n    # Perhaps better as Node.export_path\n    def get_export_path(node, node_path):\n        # If filepath is not present, generate fake path based on node parentage.\n        filepath = node._meta[SYSTEM_METADATA]['filepath']\n        if filepath:\n            dest = pathlib.PureWindowsPath(filepath)  # PureWindowsPath handles all win/lin/osx separators\n        else:\n            assert isinstance(node_path, pathlib.PureWindowsPath)\n            assert not node_path.anchor\n            print(\"Warning:  Missing export path in metadata.  Using node path: {}\"\n                  .format('/'.join(node_path.parts)))\n            dest = node_path\n\n        # When exporting dataframes, excel files are to be converted to csv.\n        # check also occurs in export_node(), but done here prevents filename conflicts\n        if node._target() == TargetType.PANDAS:\n            if dest.suffix != '.csv':\n                # avoid name collisions from files with same name but different source,\n                # as we shift all to being csv for export.\n                # foo.tsv -> foo_tsv.csv\n                # foo.xls -> foo_xls.csv\n                # ..etc.\n                dest = dest.with_name(dest.stem + dest.suffix.replace('.', '_')).with_suffix('.csv')\n        # if filepath isn't absolute\n        if not dest.anchor:\n            return pathlib.Path(*dest.parts)  # return a native path\n\n        # filepath is absolute, convert to relative.\n        dest = pathlib.Path(*dest.parts[1:])  # Issue warning as native path, and return it\n        print(\"Warning:  Converted export path to relative path: {}\".format(str(dest)))\n        return dest\n\n    def iter_filename_map(node, base_path):\n        \"\"\"Yields (<node>, <export path>) pairs for given `node`.\n\n        If `node` is a group node, yield pairs for children of `node`.\n        Otherwise, yield the path to export to for that node.\n\n        :returns: Iterator of (<node>, <export path>) pairs\n        \"\"\"\n        # Handle singular node export\n        if not isinstance(node, nodes.GroupNode):\n            yield (node, get_export_path(node, base_path))\n            return\n\n        for node_path, found_node in iteritems(node, base_path=base_path, recursive=True):\n            if not isinstance(found_node, nodes.GroupNode):\n                yield (found_node, get_export_path(found_node, node_path))\n\n    # perhaps better as Node.export()\n    def export_node(node, dest, use_symlinks=False):\n        if not dest.parent.exists():\n            dest.parent.mkdir(parents=True, exist_ok=True)\n        if node._target() == TargetType.FILE:\n            if use_symlinks is True:\n                fs_link(node(), dest)\n            else:\n                copyfile(node(), str(dest))\n        elif node._target() == TargetType.PANDAS:\n            df = node()\n            # 100 decimal places of pi will allow you to draw a circle the size of the known\n            # universe, and only vary by approximately the width of a proton.\n            # ..so, hopefully 78 decimal places (256 bits) is ok for float precision in CSV exports.\n            # If not, and someone complains, we can up it or add a parameter.\n            df.to_csv(str(dest), index=False, float_format='%r')\n        else:\n            assert False\n\n    def resolve_dirpath(dirpath):\n        \"\"\"Checks the dirpath and ensures it exists and is writable\n        :returns: absolute, resolved dirpath\n        \"\"\"\n        # ensure output path is writable.  I'd just check stat, but this is fully portable.\n        try:\n            dirpath.mkdir(exist_ok=True)  # could be '.'\n            with tempfile.TemporaryFile(dir=str(dirpath), prefix=\"quilt-export-write-test-\", suffix='.tmp'):\n                pass\n        except OSError as error:\n            raise CommandException(\"Invalid export path: not writable: \" + str(error))\n        return output_path.resolve()    # this gets the true absolute path, but requires the path exists.\n\n    def finalize(outpath, exports):\n        \"\"\"Finalize exports\n\n        This performs the following tasks:\n            * Ensure destination doesn't exist\n            * Remove absolute anchor in dest, if any\n            * Prefix destination with target dir\n\n        :returns: (<source Path / dest Path pairs list>, <set of zero-byte files>)\n        \"\"\"\n        # We return list instead of yielding, so that all prep logic is done before write is attempted.\n        final_export_map = []\n\n        for node, dest in exports:\n            dest = pathlib.PureWindowsPath(dest)\n            dest = outpath.joinpath(*dest.parts[1:]) if dest.anchor else outpath / dest\n\n            if dest.parent != outpath and dest.parent.exists() and not force:\n                raise CommandException(\"Invalid export path: subdir already exists: {!r}\"\n                                       .format(str(dest.parent)))\n            if dest.exists() and not force:\n                raise CommandException(\"Invalid export path: file already exists: {!r}\".format(str(dest)))\n\n            final_export_map.append((node, dest))\n        return final_export_map\n\n    def check_for_conflicts(export_list):\n        \"\"\"Checks for conflicting exports in the final export map of (src, dest) pairs\n\n        Export conflicts can be introduced in various ways -- for example:\n            * export-time mapping -- user maps two files to the same name\n            * coded builds -- user creates two files with the same path\n            * re-rooting absolute paths -- user entered absolute paths, which are re-rooted to the export dir\n            * build-time duplication -- user enters the same file path twice under different nodes\n\n        This checks for these conflicts and raises an error if they have occurred.\n\n        :raises: CommandException\n        \"\"\"\n        results = {}\n        conflicts = set()\n\n        # Export conflicts..\n        for src, dest in export_list:\n            if dest in conflicts:\n                continue    # already a known conflict\n            if dest not in results:\n                results[dest] = src\n                continue    # not a conflict..\n            if src._target() == TargetType.FILE and src() == results[dest]():\n                continue    # not a conflict (same src filename, same dest)..\n            # ..add other conditions that prevent this from being a conflict here..\n\n            # dest is a conflict.\n            conflicts.add(dest)\n\n        if conflicts:\n            conflict_strings = (os.linesep + '\\t').join(str(c) for c in conflicts)\n            conflict_error = CommandException(\n                \"Invalid export: Identical filename(s) with conflicting contents cannot be exported:\\n\\t\"\n                + conflict_strings\n                )\n            conflict_error.file_conflicts = conflicts\n            raise conflict_error\n\n        # Check for filenames that conflict with folder names\n        exports = set(results)\n        dirs = set()\n        for dest in results:\n            dirs.update(dest.parents)\n        file_dir_conflicts = exports & dirs\n\n        if file_dir_conflicts:\n            conflict_strings = (os.linesep + '\\t').join(str(c) for c in file_dir_conflicts)\n            conflict_error = CommandException(\n                \"Invalid Export: Filename(s) conflict with folder name(s):\\n\\t\" + conflict_strings\n                )\n            conflict_error.dir_file_conflicts = file_dir_conflicts\n            raise conflict_error\n        # TODO: return abbreviated list of exports based on found non-conflicting duplicates\n\n    ## Export Logic\n    output_path = pathlib.Path(output_path)\n    node, _, info = _load(package)\n\n    if info.subpath:\n        subpath = pathlib.PureWindowsPath(*info.subpath)\n        for name in info.subpath:\n            node = node._get(name)\n    else:\n        subpath = pathlib.PureWindowsPath()\n\n    resolved_output = resolve_dirpath(output_path)  # resolve/create output path\n    exports = iter_filename_map(node, subpath)      # Create src / dest map iterator\n    exports = finalize(resolved_output, exports)    # Fix absolutes, check dest nonexistent, prefix dest dir\n    check_for_conflicts(exports)                    # Prevent various potential dir/naming conflicts\n\n    # Skip it if there's nothing to do\n    if not exports:\n        # Technically successful, but with nothing to do.\n        # package may have no file nodes, or user may have filtered out all applicable targets.\n        # -- should we consider it an error and raise?\n        print(\"No files to export.\")\n        return\n\n    # All prep done, let's export..\n    try:\n        fmt = \"Exporting file {n_fmt} of {total_fmt} [{elapsed}]\"\n        sys.stdout.flush()   # flush prior text before making progress bar\n\n        # bar_format is not respected unless both ncols and total are set.\n        exports_bar = tqdm(exports, desc=\"Exporting: \", ncols=1, total=len(exports), bar_format=fmt)\n        # tqdm is threaded, and its display may not specify the exact file currently being exported.\n        with exports_bar:\n            for node, dest in exports_bar:\n                # Escape { and } in filenames for .format called by tqdm\n                fname = str(dest.relative_to(resolved_output)).replace('{', \"{{\").replace('}', '}}')\n                exports_bar.bar_format = fmt + \": \" + fname\n                exports_bar.update(0)\n                export_node(node, dest, use_symlinks=symlinks)\n    except OSError as error:\n        commandex = CommandException(\"Unexpected error during export: \" + str(error))\n        commandex.original_error = error\n        raise commandex"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ccor(alt, r, h1, zh):\n    e = (alt - zh) / h1\n    if(e>70.0):\n        return 1.0 # exp(0) # pragma: no cover\n    elif (e < -70.0):\n        return exp(r)\n    ex = exp(e)\n    e = r / (1.0 + ex)\n    return exp(e)", "response": "calculate the centrality of the current state of the current state"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ccor2(alt, r, h1, zh, h2):\n    '''\n/*        CHEMISTRY/DISSOCIATION CORRECTION FOR MSIS MODELS\n *         ALT - altitude\n *         R - target ratio\n *         H1 - transition scale length\n *         ZH - altitude of 1/2 R\n *         H2 - transition scale length #2 ?\n */\n '''\n    e1 = (alt - zh) / h1;\n    e2 = (alt - zh) / h2;\n    if ((e1 > 70.0) or (e2 > 70)): # pragma: no cover\n        return 1.0 # exp(0)\n    if ((e1 < -70) and (e2 < -70)): # pragma: no cover\n        return exp(r)\n    ex1 = exp(e1);\n    ex2 = exp(e2);\n    ccor2v = r / (1.0 + 0.5 * (ex1 + ex2));\n    return exp(ccor2v);", "response": "calculate the CCO2 from the given altitude r h1 zh h2"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef splini(xa, ya, y2a, n, x, y):\n    '''\n/*      INTEGRATE CUBIC SPLINE FUNCTION FROM XA(1) TO X\n *       XA,YA: ARRAYS OF TABULATED FUNCTION IN ASCENDING ORDER BY X\n *       Y2A: ARRAY OF SECOND DERIVATIVES\n *       N: SIZE OF ARRAYS XA,YA,Y2A\n *       X: ABSCISSA ENDPOINT FOR INTEGRATION\n *       Y: OUTPUT VALUE\n */\n '''\n    yi = 0\n    klo = 0\n    khi = 1\n    while((x>xa[klo]) and (khi<n)):\n        xx = x\n        if(khi < (n-1)):\n            if (x < xa[khi]):\n                xx = x\n            else:\n                xx = xa[khi]\n        h = xa[khi] - xa[klo]\n        a = (xa[khi] - xx)/h\n        b = (xx - xa[klo])/h\n        a2 = a*a\n        b2 = b*b\n        yi += ((1.0 - a2) * ya[klo] / 2.0 + b2 * ya[khi] / 2.0 + ((-(1.0+a2*a2)/4.0 + a2/2.0) * y2a[klo] + (b2*b2/4.0 - b2/2.0) * y2a[khi]) * h * h / 6.0) * h;\n        klo += 1;\n        khi += 1;\n    y[0] = yi\n    return", "response": "This function is used to generate a new array of tables from a given array of tables and a second array of tables and a second array of tables."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef spline(x, y, n, yp1, ypn, y2):\n    '''\n/*       CALCULATE 2ND DERIVATIVES OF CUBIC SPLINE INTERP FUNCTION\n *       ADAPTED FROM NUMERICAL RECIPES BY PRESS ET AL\n *       X,Y: ARRAYS OF TABULATED FUNCTION IN ASCENDING ORDER BY X\n *       N: SIZE OF ARRAYS X,Y\n *       YP1,YPN: SPECIFIED DERIVATIVES AT X[0] AND X[N-1]; VALUES\n *                >= 1E30 SIGNAL SIGNAL SECOND DERIVATIVE ZERO\n *       Y2: OUTPUT ARRAY OF SECOND DERIVATIVES\n */\n '''\n    u = [0.0]*n #I think this is the same as malloc\n\n    #no need for the out of memory\n\n    if (yp1 > 0.99E30): # pragma: no cover\n        y2[0] = 0\n        u[0] = 0\n    else:\n        y2[0]=-0.5\n        u[0]=(3.0/(x[1]-x[0]))*((y[1]-y[0])/(x[1]-x[0])-yp1)\n\n    for i in range(1, n-1):\n        sig = (x[i]-x[i-1])/(x[i+1] - x[i-1])\n        p = sig * y2[i-1] + 2.0\n        y2[i] = (sig - 1.0) / p\n        u[i] = (6.0 * ((y[i+1] - y[i])/(x[i+1] - x[i]) -(y[i] - y[i-1]) / (x[i] - x[i-1]))/(x[i+1] - x[i-1]) - sig * u[i-1])/p;\n\n    if (ypn>0.99E30): # pragma: no cover\n        qn = 0;\n        un = 0\n    else:\n        qn = 0.5\n        un = (3.0 / (x[n-1] - x[n-2])) * (ypn - (y[n-1] - y[n-2])/(x[n-1] - x[n-2]));\n\n    y2[n-1] = (un - qn * u[n-2]) / (qn * y2[n-2] + 1.0);\n\n    #it uses a for loop here, but its not something python can do (I dont think)\n    k = n-2\n    while(k >= 0):\n        y2[k] = y2[k] * y2[k+1] + u[k]\n        k -= 1\n    #This for loop might work\n    #for k in range(n-2, -1, -1):\n    #    y2[k] = y2[k] * y2[k+1] + u[k]\n    #no need to free u here\n    return", "response": "This function calculates the spline of the two arrays x and y."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef densm(alt, d0, xm, tz, mn3, zn3, tn3, tgn3, mn2, zn2, tn2, tgn2):\n    '''\n/*      Calculate Temperature and Density Profiles for lower atmos.  */\n'''\n    xs = [0.0]*10\n    ys = [0.0]*10\n    y2out = [0.0]*10\n    rgas = 831.4\n    #rgas = 831.44621    #maybe make this a global constant?\n    densm_tmp=d0\n    if (alt>zn2[0]): # pragma: no cover\n        if(xm==0.0):\n            return tz[0]\n        else:\n            return d0\n\n    #/* STRATOSPHERE/MESOSPHERE TEMPERATURE */\n    if (alt>zn2[mn2-1]):\n        z=alt\n    else:\n        z=zn2[mn2-1];\n    mn=mn2;\n    z1=zn2[0];\n    z2=zn2[mn-1];\n    t1=tn2[0];\n    t2=tn2[mn-1];\n    zg = zeta(z, z1);\n    zgdif = zeta(z2, z1);\n\n    #/* set up spline nodes */\n    for k in range(mn):\n        xs[k]=zeta(zn2[k],z1)/zgdif;\n        ys[k]=1.0 / tn2[k];\n    yd1=-tgn2[0] / (t1*t1) * zgdif;\n    yd2=-tgn2[1] / (t2*t2) * zgdif * (pow(((re[0]+z2)/(re[0]+z1)),2.0));\n\n    #/* calculate spline coefficients */\n    spline (xs, ys, mn, yd1, yd2, y2out);   #No need to change this\n    x = zg/zgdif;\n    y = [0.0]\n    splint (xs, ys, y2out, mn, x, y);\n\n    #/* temperature at altitude */\n    tz[0] = 1.0 / y[0];\n    if (xm!=0.0):\n        #/* calaculate stratosphere / mesospehere density */\n        glb = gsurf[0] / (pow((1.0 + z1/re[0]),2.0));\n        gamm = xm * glb * zgdif / rgas;\n\n        #/* Integrate temperature profile */\n        yi = [0.0]\n        splini(xs, ys, y2out, mn, x, yi);\n        expl=gamm*yi[0];\n        if (expl>50.0): # pragma: no cover\n            expl=50.0\n\n        #/* Density at altitude */\n        densm_tmp = densm_tmp * (t1 / tz[0]) * exp(-expl);\n\n    if (alt>zn3[0]):\n        if (xm==0.0):\n            return tz[0]\n        else:\n            return densm_tmp\n\n    #/* troposhere / stratosphere temperature */\n    z = alt;\n    mn = mn3;\n    z1=zn3[0];\n    z2=zn3[mn-1];\n    t1=tn3[0];\n    t2=tn3[mn-1];\n    zg=zeta(z,z1);\n    zgdif=zeta(z2,z1);\n\n\n\n    #/* set up spline nodes */\n    for k in range(mn):\n        xs[k] = zeta(zn3[k],z1) / zgdif;\n        ys[k] = 1.0 / tn3[k];\n    \n    yd1=-tgn3[0] / (t1*t1) * zgdif;\n    yd2=-tgn3[1] / (t2*t2) * zgdif * (pow(((re[0]+z2)/(re[0]+z1)),2.0));\n\n    #/* calculate spline coefficients */\n    spline (xs, ys, mn, yd1, yd2, y2out);\n    x = zg/zgdif;\n    y = [0.0]\n    splint (xs, ys, y2out, mn, x, y);\n\n    #/* temperature at altitude */\n    tz[0] = 1.0 / y[0];\n    if (xm!=0.0):\n        #/* calaculate tropospheric / stratosphere density */\n        glb = gsurf[0] / (pow((1.0 + z1/re[0]),2.0));\n        gamm = xm * glb * zgdif / rgas;\n\n        #/* Integrate temperature profile */\n        yi = [0.0]\n        splini(xs, ys, y2out, mn, x, yi);\n        expl=gamm*yi[0];\n        if (expl>50.0): # pragma: no cover\n            expl=50.0;\n\n        #/* Density at altitude */\n        densm_tmp = densm_tmp * (t1 / tz[0]) * exp(-expl);\n    \n    if (xm==0.0):\n        return tz[0];\n    else:\n        return densm_tmp;", "response": "Calculate the Densm coefficient for a given altitude and temperature."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef densu(alt, dlb, tinf, tlb, xm, alpha, tz, zlb, s2, mn1, zn1, tn1, tgn1):\n    '''\n/*      Calculate Temperature and Density Profiles for MSIS models\n *      New lower thermo polynomial\n */\n tz, zn1, tn1, and tgn1 are simulated pointers\n '''\n    rgas = 831.4\n    #rgas = 831.44621    #maybe make this a global constant?\n    densu_temp = 1.0\n\n    xs = [0.0]*5\n    ys = [0.0]*5\n    y2out = [0.0]*5\n\n    #/* joining altitudes of Bates and spline */\n    za=zn1[0];\n    if (alt>za):\n        z=alt;\n    else:\n        z=za;\n\n    #/* geopotential altitude difference from ZLB */\n    zg2 = zeta(z, zlb);\n    \n    #/* Bates temperature */\n    tt = tinf - (tinf - tlb) * exp(-s2*zg2);\n    ta = tt;\n    tz[0] = tt\n    densu_temp = tz[0]\n\n    if (alt<za):\n        #/* calculate temperature below ZA\n        # * temperature gradient at ZA from Bates profile */\n        dta = (tinf - ta) * s2 * pow(((re[0]+zlb)/(re[0]+za)),2.0);\n        tgn1[0]=dta;\n        tn1[0]=ta;\n        if (alt>zn1[mn1-1]):\n            z=alt;\n        else:\n            z=zn1[mn1-1];\n        mn=mn1;\n        z1=zn1[0];\n        z2=zn1[mn-1];\n        t1=tn1[0];\n        t2=tn1[mn-1];\n        #/* geopotental difference from z1 */\n        zg = zeta (z, z1);\n        zgdif = zeta(z2, z1);\n        #/* set up spline nodes */\n        for k in range(mn):\n            xs[k] = zeta(zn1[k], z1) / zgdif;\n            ys[k] = 1.0 / tn1[k];\n        \n        #/* end node derivatives */\n        yd1 = -tgn1[0] / (t1*t1) * zgdif;\n        yd2 = -tgn1[1] / (t2*t2) * zgdif * pow(((re[0]+z2)/(re[0]+z1)),2.0);\n        #/* calculate spline coefficients */\n        spline (xs, ys, mn, yd1, yd2, y2out);\n        x = zg / zgdif;\n        y = [0.0]\n        splint (xs, ys, y2out, mn, x, y);\n        #/* temperature at altitude */\n        tz[0] = 1.0 / y[0];\n        densu_temp = tz[0];\n\n    if (xm==0):\n        return densu_temp;\n\n    #/* calculate density above za */\n    glb = gsurf[0] / pow((1.0 + zlb/re[0]),2.0);\n    gamma = xm * glb / (s2 * rgas * tinf);\n    expl = exp(-s2 * gamma * zg2);\n    if (expl>50.0): # pragma: no cover\n        expl=50.0;\n    if (tt<=0): # pragma: no cover\n        expl=50.0;\n\n    #/* density at altitude */\n    densa = dlb * pow((tlb/tt),((1.0+alpha+gamma))) * expl;\n    densu_temp=densa;\n    if (alt>=za):\n        return densu_temp;\n\n    #/* calculate density below za */\n    glb = gsurf[0] / pow((1.0 + z1/re[0]),2.0);\n    gamm = xm * glb * zgdif / rgas;\n\n    #/* integrate spline temperatures */\n    yi = [0]\n    splini (xs, ys, y2out, mn, x, yi);\n    expl = gamm * yi[0];\n    if (expl>50.0): # pragma: no cover\n        expl=50.0;\n    if (tz[0]<=0): # pragma: no cover\n        expl=50.0;\n\n    #/* density at altitude */\n    densu_temp = densu_temp * pow ((t1 / tz[0]),(1.0 + alpha)) * exp(-expl);\n    return densu_temp;", "response": "This function calculates the temperature and density of a single MSIS model."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfunction to compute the upper thermosphere globe", "response": "def globe7(p, Input, flags):\n    '''\n/*       CALCULATE G(L) FUNCTION \n *       Upper Thermosphere Parameters */\n'''\n    t = [0]*15  #modified this, there was a for loop that did this\n    sw9 = 1\n    sr = 7.2722E-5;\n    dgtr = 1.74533E-2;\n    dr = 1.72142E-2;\n    hr = 0.2618;\n\n    tloc = Input.lst\n    #for j in range(14):\n    #    t[j] = 0\n    if(flags.sw[9] > 0):\n        sw9 = 1\n    elif(flags.sw[9] < 0): # pragma: no cover\n        sw9 = -1\n    xlong = Input.g_long\n\n    #/* calculate legendre polynomials */\n    c = sin(Input.g_lat * dgtr);\n    s = cos(Input.g_lat * dgtr);\n    c2 = c*c;\n    c4 = c2*c2;\n    s2 = s*s;\n\n    plg[0][1] = c;\n    plg[0][2] = 0.5*(3.0*c2 -1.0);\n    plg[0][3] = 0.5*(5.0*c*c2-3.0*c);\n    plg[0][4] = (35.0*c4 - 30.0*c2 + 3.0)/8.0;\n    plg[0][5] = (63.0*c2*c2*c - 70.0*c2*c + 15.0*c)/8.0;\n    plg[0][6] = (11.0*c*plg[0][5] - 5.0*plg[0][4])/6.0;\n#/*      plg[0][7] = (13.0*c*plg[0][6] - 6.0*plg[0][5])/7.0; */\n    plg[1][1] = s;\n    plg[1][2] = 3.0*c*s;\n    plg[1][3] = 1.5*(5.0*c2-1.0)*s;\n    plg[1][4] = 2.5*(7.0*c2*c-3.0*c)*s;\n    plg[1][5] = 1.875*(21.0*c4 - 14.0*c2 +1.0)*s;\n    plg[1][6] = (11.0*c*plg[1][5]-6.0*plg[1][4])/5.0;\n#/*      plg[1][7] = (13.0*c*plg[1][6]-7.0*plg[1][5])/6.0; */\n#/*      plg[1][8] = (15.0*c*plg[1][7]-8.0*plg[1][6])/7.0; */\n    plg[2][2] = 3.0*s2;\n    plg[2][3] = 15.0*s2*c;\n    plg[2][4] = 7.5*(7.0*c2 -1.0)*s2;\n    plg[2][5] = 3.0*c*plg[2][4]-2.0*plg[2][3];\n    plg[2][6] =(11.0*c*plg[2][5]-7.0*plg[2][4])/4.0;\n    plg[2][7] =(13.0*c*plg[2][6]-8.0*plg[2][5])/5.0;\n    plg[3][3] = 15.0*s2*s;\n    plg[3][4] = 105.0*s2*s*c; \n    plg[3][5] =(9.0*c*plg[3][4]-7.*plg[3][3])/2.0;\n    plg[3][6] =(11.0*c*plg[3][5]-8.*plg[3][4])/3.0;\n\n    if( not (((flags.sw[7]==0) and (flags.sw[8]==0)) and (flags.sw[14] == 0))):\n        global stloc\n        stloc = sin(hr*tloc);\n        global ctloc\n        ctloc = cos(hr*tloc);\n        global s2tloc\n        s2tloc = sin(2.0*hr*tloc);\n        global c2tloc\n        c2tloc = cos(2.0*hr*tloc);\n        global s3tloc\n        s3tloc = sin(3.0*hr*tloc);\n        global c3tloc\n        c3tloc = cos(3.0*hr*tloc);\n\n    cd32 = cos(dr*(Input.doy-p[31]));\n    cd18 = cos(2.0*dr*(Input.doy-p[17]));\n    cd14 = cos(dr*(Input.doy-p[13]));\n    cd39 = cos(2.0*dr*(Input.doy-p[38]));\n    p32=p[31];\n    p18=p[17];\n    p14=p[13];\n    p39=p[38];\n\n    #/* F10.7 EFFECT */\n    df = Input.f107 - Input.f107A;\n    global dfa\n    dfa = Input.f107A - 150.0;\n    t[0] =  p[19]*df*(1.0+p[59]*dfa) + p[20]*df*df + p[21]*dfa + p[29]*pow(dfa,2.0);\n    f1 = 1.0 + (p[47]*dfa +p[19]*df+p[20]*df*df)*flags.swc[1];\n    f2 = 1.0 + (p[49]*dfa+p[19]*df+p[20]*df*df)*flags.swc[1];\n\n    #/*  TIME INDEPENDENT */\n    t[1] = (p[1]*plg[0][2]+ p[2]*plg[0][4]+p[22]*plg[0][6]) + \\\n          (p[14]*plg[0][2])*dfa*flags.swc[1] +p[26]*plg[0][1];\n\n    #/*  SYMMETRICAL ANNUAL */\n    t[2] = p[18]*cd32;\n\n    #/*  SYMMETRICAL SEMIANNUAL */\n    t[3] = (p[15]+p[16]*plg[0][2])*cd18;\n\n    #/*  ASYMMETRICAL ANNUAL */\n    t[4] =  f1*(p[9]*plg[0][1]+p[10]*plg[0][3])*cd14;\n\n    #/*  ASYMMETRICAL SEMIANNUAL */\n    t[5] =    p[37]*plg[0][1]*cd39;\n\n    #/* DIURNAL */\n    if (flags.sw[7]):\n        t71 = (p[11]*plg[1][2])*cd14*flags.swc[5];\n        t72 = (p[12]*plg[1][2])*cd14*flags.swc[5];\n        t[6] = f2*((p[3]*plg[1][1] + p[4]*plg[1][3] + p[27]*plg[1][5] + t71) * \\\n                   ctloc + (p[6]*plg[1][1] + p[7]*plg[1][3] + p[28]*plg[1][5] \\\n                            + t72)*stloc);\n\n\n    #/* SEMIDIURNAL */\n    if (flags.sw[8]):\n        t81 = (p[23]*plg[2][3]+p[35]*plg[2][5])*cd14*flags.swc[5];\n        t82 = (p[33]*plg[2][3]+p[36]*plg[2][5])*cd14*flags.swc[5];\n        t[7] = f2*((p[5]*plg[2][2]+ p[41]*plg[2][4] + t81)*c2tloc +(p[8]*plg[2][2] + p[42]*plg[2][4] + t82)*s2tloc);\n    \n\n    #/* TERDIURNAL */\n    if (flags.sw[14]):\n        t[13] = f2 * ((p[39]*plg[3][3]+(p[93]*plg[3][4]+p[46]*plg[3][6])*cd14*flags.swc[5])* s3tloc +(p[40]*plg[3][3]+(p[94]*plg[3][4]+p[48]*plg[3][6])*cd14*flags.swc[5])* c3tloc);\n\n\n    #/* magnetic activity based on daily ap */\n    if (flags.sw[9]==-1):\n        ap = Input.ap_a;\n        if (p[51]!=0):\n            exp1 = exp(-10800.0*sqrt(p[51]*p[51])/(1.0+p[138]*(45.0-sqrt(Input.g_lat*Input.g_lat))));\n            if (exp1>0.99999): # pragma: no cover\n                    exp1=0.99999;\n            if (p[24]<1.0E-4): # pragma: no cover\n                    p[24]=1.0E-4;\n            apt[0]=sg0(exp1,p,ap.a);\n            #/* apt[1]=sg2(exp1,p,ap->a);\n            #   apt[2]=sg0(exp2,p,ap->a);\n            #   apt[3]=sg2(exp2,p,ap->a);\n            #*/\n            if (flags.sw[9]):\n                t[8] = apt[0]*(p[50]+p[96]*plg[0][2]+p[54]*plg[0][4]+ \\\n                        (p[125]*plg[0][1]+p[126]*plg[0][3]+p[127]*plg[0][5])*cd14*flags.swc[5]+ \\\n                        (p[128]*plg[1][1]+p[129]*plg[1][3]+p[130]*plg[1][5])*flags.swc[7]* \\\n                                       cos(hr*(tloc-p[131])));\n                    \n            \n    else:\n        apd=Input.ap-4.0;\n        p44=p[43];\n        p45=p[44];\n        if (p44<0): # pragma: no cover\n            p44 = 1.0E-5;\n        global apdf\n        apdf = apd + (p45-1.0)*(apd + (exp(-p44 * apd) - 1.0)/p44);\n        if (flags.sw[9]):\n            t[8]=apdf*(p[32]+p[45]*plg[0][2]+p[34]*plg[0][4]+ \\\n             (p[100]*plg[0][1]+p[101]*plg[0][3]+p[102]*plg[0][5])*cd14*flags.swc[5]+\n             (p[121]*plg[1][1]+p[122]*plg[1][3]+p[123]*plg[1][5])*flags.swc[7]*\n                cos(hr*(tloc-p[124])));\n            \n    \n\n    if ((flags.sw[10]) and (Input.g_long>-1000.0)):\n\n            #/* longitudinal */\n            if (flags.sw[11]):\n                    t[10] = (1.0 + p[80]*dfa*flags.swc[1])* \\\n                     ((p[64]*plg[1][2]+p[65]*plg[1][4]+p[66]*plg[1][6]\\\n                      +p[103]*plg[1][1]+p[104]*plg[1][3]+p[105]*plg[1][5]\\\n                      +flags.swc[5]*(p[109]*plg[1][1]+p[110]*plg[1][3]+p[111]*plg[1][5])*cd14)* \\\n                          cos(dgtr*Input.g_long) \\\n                      +(p[90]*plg[1][2]+p[91]*plg[1][4]+p[92]*plg[1][6]\\\n                      +p[106]*plg[1][1]+p[107]*plg[1][3]+p[108]*plg[1][5]\\\n                      +flags.swc[5]*(p[112]*plg[1][1]+p[113]*plg[1][3]+p[114]*plg[1][5])*cd14)* \\\n                      sin(dgtr*Input.g_long));\n            \n\n            #/* ut and mixed ut, longitude */\n            if (flags.sw[12]):\n                    t[11]=(1.0+p[95]*plg[0][1])*(1.0+p[81]*dfa*flags.swc[1])*\\\n                            (1.0+p[119]*plg[0][1]*flags.swc[5]*cd14)*\\\n                            ((p[68]*plg[0][1]+p[69]*plg[0][3]+p[70]*plg[0][5])*\\\n                            cos(sr*(Input.sec-p[71])));\n                    t[11]+=flags.swc[11]*\\\n                            (p[76]*plg[2][3]+p[77]*plg[2][5]+p[78]*plg[2][7])*\\\n                            cos(sr*(Input.sec-p[79])+2.0*dgtr*Input.g_long)*(1.0+p[137]*dfa*flags.swc[1]);\n            \n\n            #/* ut, longitude magnetic activity */\n            if (flags.sw[13]):\n                if (flags.sw[9]==-1):\n                    if (p[51]):\n                            t[12]=apt[0]*flags.swc[11]*(1.+p[132]*plg[0][1])*\\\n                                    ((p[52]*plg[1][2]+p[98]*plg[1][4]+p[67]*plg[1][6])*\\\n                                     cos(dgtr*(Input.g_long-p[97])))\\\n                                    +apt[0]*flags.swc[11]*flags.swc[5]*\\\n                                    (p[133]*plg[1][1]+p[134]*plg[1][3]+p[135]*plg[1][5])*\\\n                                    cd14*cos(dgtr*(Input.g_long-p[136])) \\\n                                    +apt[0]*flags.swc[12]* \\\n                                    (p[55]*plg[0][1]+p[56]*plg[0][3]+p[57]*plg[0][5])*\\\n                                    cos(sr*(Input.sec-p[58]));\n                        \n                else:\n                    t[12] = apdf*flags.swc[11]*(1.0+p[120]*plg[0][1])*\\\n                            ((p[60]*plg[1][2]+p[61]*plg[1][4]+p[62]*plg[1][6])*\\\n                            cos(dgtr*(Input.g_long-p[63])))\\\n                            +apdf*flags.swc[11]*flags.swc[5]* \\\n                            (p[115]*plg[1][1]+p[116]*plg[1][3]+p[117]*plg[1][5])* \\\n                            cd14*cos(dgtr*(Input.g_long-p[118])) \\\n                            + apdf*flags.swc[12]* \\\n                            (p[83]*plg[0][1]+p[84]*plg[0][3]+p[85]*plg[0][5])* \\\n                            cos(sr*(Input.sec-p[75]));\n                    \t\t\t\n            \n    \n\n    #/* parms not used: 82, 89, 99, 139-149 */\n    tinf = p[30];\n    for i in range(14):\n        tinf = tinf + abs(flags.sw[i+1])*t[i];\n    return tinf;"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef gtd7(Input, flags, output):\n    '''The standard model subroutine (GTD7) always computes the \n    \u2018\u2018thermospheric\u2019\u2019 mass density by explicitly summing the masses of\n    the species in equilibrium at the thermospheric temperature T(z).\n    '''\n    mn3 = 5\n    zn3 = [32.5,20.0,15.0,10.0,0.0]\n    mn2 = 4\n    zn2 = [72.5,55.0,45.0,32.5]\n    zmix = 62.5\n    soutput = nrlmsise_output()\n\n    tselec(flags);\n\n    #/* Latitude variation of gravity (none for sw[2]=0) */\n    xlat=Input.g_lat;\n    if (flags.sw[2]==0): # pragma: no cover\n        xlat=45.0;\n    glatf(xlat, gsurf, re);\n\n    xmm = pdm[2][4];\n\n    #/* THERMOSPHERE / MESOSPHERE (above zn2[0]) */\n    if (Input.alt>zn2[0]):\n        altt=Input.alt;\n    else:\n        altt=zn2[0];\n\n    tmp=Input.alt;\n    Input.alt=altt;\n\n    gts7(Input, flags, soutput);\n    altt=Input.alt;\n    Input.alt=tmp;\n    if (flags.sw[0]): # pragma: no cover  #/* metric adjustment */\n        dm28m= dm28*1.0E6;\n    else:\n        dm28m = dm28;\n    output.t[0]=soutput.t[0];\n    output.t[1]=soutput.t[1];\n    if (Input.alt>=zn2[0]): \n        for i in range(9):\n            output.d[i]=soutput.d[i];\n        return\n\n\n#/*       LOWER MESOSPHERE/UPPER STRATOSPHERE (between zn3[0] and zn2[0])\n#*         Temperature at nodes and gradients at end nodes\n#*         Inverse temperature a linear function of spherical harmonics\n#*/\n    meso_tgn2[0]=meso_tgn1[1];\n    meso_tn2[0]=meso_tn1[4];\n    meso_tn2[1]=pma[0][0]*pavgm[0]/(1.0-flags.sw[20]*glob7s(pma[0], Input, flags));\n    meso_tn2[2]=pma[1][0]*pavgm[1]/(1.0-flags.sw[20]*glob7s(pma[1], Input, flags));\n    meso_tn2[3]=pma[2][0]*pavgm[2]/(1.0-flags.sw[20]*flags.sw[22]*glob7s(pma[2], Input, flags));\n    meso_tgn2[1]=pavgm[8]*pma[9][0]*(1.0+flags.sw[20]*flags.sw[22]*glob7s(pma[9], Input, flags))*meso_tn2[3]*meso_tn2[3]/(pow((pma[2][0]*pavgm[2]),2.0));\n    meso_tn3[0]=meso_tn2[3];\n\n    if (Input.alt<zn3[0]):\n#/*       LOWER STRATOSPHERE AND TROPOSPHERE (below zn3[0])\n#*         Temperature at nodes and gradients at end nodes\n#*         Inverse temperature a linear function of spherical harmonics\n#*/\n        meso_tgn3[0]=meso_tgn2[1];\n        meso_tn3[1]=pma[3][0]*pavgm[3]/(1.0-flags.sw[22]*glob7s(pma[3], Input, flags));\n        meso_tn3[2]=pma[4][0]*pavgm[4]/(1.0-flags.sw[22]*glob7s(pma[4], Input, flags));\n        meso_tn3[3]=pma[5][0]*pavgm[5]/(1.0-flags.sw[22]*glob7s(pma[5], Input, flags));\n        meso_tn3[4]=pma[6][0]*pavgm[6]/(1.0-flags.sw[22]*glob7s(pma[6], Input, flags));\n        meso_tgn3[1]=pma[7][0]*pavgm[7]*(1.0+flags.sw[22]*glob7s(pma[7], Input, flags)) *meso_tn3[4]*meso_tn3[4]/(pow((pma[6][0]*pavgm[6]),2.0));\n    \n\n    #/* LINEAR TRANSITION TO FULL MIXING BELOW zn2[0] */\n\n    dmc=0;\n    if (Input.alt>zmix):\n        dmc = 1.0 - (zn2[0]-Input.alt)/(zn2[0] - zmix);\n    dz28=soutput.d[2];\n    \n    #/**** N2 density ****/\n    dmr=soutput.d[2] / dm28m - 1.0;\n    tz = [0.0]\n    output.d[2]=densm(Input.alt,dm28m,xmm, tz, mn3, zn3, meso_tn3, meso_tgn3, mn2, zn2, meso_tn2, meso_tgn2);\n    output.d[2]=output.d[2] * (1.0 + dmr*dmc);\n\n    #/**** HE density ****/\n    dmr = soutput.d[0] / (dz28 * pdm[0][1]) - 1.0;\n    output.d[0] = output.d[2] * pdm[0][1] * (1.0 + dmr*dmc);\n\n    #/**** O density ****/\n    output.d[1] = 0;\n    output.d[8] = 0;\n\n    #/**** O2 density ****/\n    dmr = soutput.d[3] / (dz28 * pdm[3][1]) - 1.0;\n    output.d[3] = output.d[2] * pdm[3][1] * (1.0 + dmr*dmc);\n\n    #/**** AR density ***/\n    dmr = soutput.d[4] / (dz28 * pdm[4][1]) - 1.0;\n    output.d[4] = output.d[2] * pdm[4][1] * (1.0 + dmr*dmc);\n\n    #/**** Hydrogen density ****/\n    output.d[6] = 0;\n\n    #/**** Atomic nitrogen density ****/\n    output.d[7] = 0;\n\n    #/**** Total mass density */\n    output.d[5] = 1.66E-24 * (4.0 * output.d[0] + 16.0 * output.d[1] + 28.0 * output.d[2] + 32.0 * output.d[3] + 40.0 * output.d[4] + output.d[6] + 14.0 * output.d[7]);\n\n    if (flags.sw[0]): # pragma: no cover\n        output.d[5]=output.d[5]/1000;\n\n    #/**** temperature at altitude ****/\n    global dd\n    dd = densm(Input.alt, 1.0, 0, tz, mn3, zn3, meso_tn3, meso_tgn3, mn2, zn2, meso_tn2, meso_tgn2);\n    output.t[1]=tz[0];\n    return", "response": "This function computes the standard model subroutine for the GTD7."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef gts7(Input, flags, output):\n    '''\n/*     Thermospheric portion of NRLMSISE-00\n *     See GTD7 for more extensive comments\n *     alt > 72.5 km! \n */\n '''\n    zn1 = [120.0, 110.0, 100.0, 90.0, 72.5]\n    mn1 = 5\n    dgtr=1.74533E-2;\n    dr=1.72142E-2;\n    alpha = [-0.38, 0.0, 0.0, 0.0, 0.17, 0.0, -0.38, 0.0, 0.0]\n    altl = [200.0, 300.0, 160.0, 250.0, 240.0, 450.0, 320.0, 450.0]\n    za = pdl[1][15];\n    zn1[0] = za;\n\n    for j in range(9):\n        output.d[j]=0;\n\n    #/* TINF VARIATIONS NOT IMPORTANT BELOW ZA OR ZN1(1) */\n    if (Input.alt>zn1[0]):\n        tinf = ptm[0]*pt[0] * \\\n                    (1.0+flags.sw[16]*globe7(pt,Input,flags));\n    else:\n        tinf = ptm[0]*pt[0];\n    output.t[0]=tinf;\n\n    #/*  GRADIENT VARIATIONS NOT IMPORTANT BELOW ZN1(5) */\n    if (Input.alt>zn1[4]):\n        g0 = ptm[3]*ps[0] * \\\n            (1.0+flags.sw[19]*globe7(ps,Input,flags));\n    else:\n        g0 = ptm[3]*ps[0];\n    tlb = ptm[1] * (1.0 + flags.sw[17]*globe7(pd[3],Input,flags))*pd[3][0];\n    s = g0 / (tinf - tlb);\n\n#/*      Lower thermosphere temp variations not significant for\n# *       density above 300 km */\n    if (Input.alt<300.0):\n        meso_tn1[1]=ptm[6]*ptl[0][0]/(1.0-flags.sw[18]*glob7s(ptl[0], Input, flags));\n        meso_tn1[2]=ptm[2]*ptl[1][0]/(1.0-flags.sw[18]*glob7s(ptl[1], Input, flags));\n        meso_tn1[3]=ptm[7]*ptl[2][0]/(1.0-flags.sw[18]*glob7s(ptl[2], Input, flags));\n        meso_tn1[4]=ptm[4]*ptl[3][0]/(1.0-flags.sw[18]*flags.sw[20]*glob7s(ptl[3], Input, flags));\n        meso_tgn1[1]=ptm[8]*pma[8][0]*(1.0+flags.sw[18]*flags.sw[20]*glob7s(pma[8], Input, flags))*meso_tn1[4]*meso_tn1[4]/(pow((ptm[4]*ptl[3][0]),2.0));\n    else:\n        meso_tn1[1]=ptm[6]*ptl[0][0];\n        meso_tn1[2]=ptm[2]*ptl[1][0];\n        meso_tn1[3]=ptm[7]*ptl[2][0];\n        meso_tn1[4]=ptm[4]*ptl[3][0];\n        meso_tgn1[1]=ptm[8]*pma[8][0]*meso_tn1[4]*meso_tn1[4]/(pow((ptm[4]*ptl[3][0]),2.0));\n\t\n\n    z0 = zn1[3];\n    t0 = meso_tn1[3];\n    tr12 = 1.0;\n\n    #/* N2 variation factor at Zlb */\n    g28=flags.sw[21]*globe7(pd[2], Input, flags);\n\n    #/* VARIATION OF TURBOPAUSE HEIGHT */\n    zhf=pdl[1][24]*(1.0+flags.sw[5]*pdl[0][24]*sin(dgtr*Input.g_lat)*cos(dr*(Input.doy-pt[13])));\n    output.t[0]=tinf;\n    xmm = pdm[2][4];\n    z = Input.alt;\n\n\n    #/**** N2 DENSITY ****/\n\n    #/* Diffusive density at Zlb */\n    db28 = pdm[2][0]*exp(g28)*pd[2][0];\n    #/* Diffusive density at Alt */\n    RandomVariable = [output.t[1]]\n    output.d[2]=densu(z,db28,tinf,tlb,28.0,alpha[2],RandomVariable,ptm[5],s,mn1,zn1,meso_tn1,meso_tgn1);\n    output.t[1] = RandomVariable[0]\n    dd=output.d[2];\n    #/* Turbopause */\n    zh28=pdm[2][2]*zhf;\n    zhm28=pdm[2][3]*pdl[1][5]; \n    xmd=28.0-xmm;\n    #/* Mixed density at Zlb */\n    tz = [0]\n    b28=densu(zh28,db28,tinf,tlb,xmd,(alpha[2]-1.0),tz,ptm[5],s,mn1, zn1,meso_tn1,meso_tgn1);\n    if ((flags.sw[15]) and (z<=altl[2])):\n        #/*  Mixed density at Alt */\n        global dm28\n        dm28=densu(z,b28,tinf,tlb,xmm,alpha[2],tz,ptm[5],s,mn1,zn1,meso_tn1,meso_tgn1);\n        #/*  Net density at Alt */\n        output.d[2]=dnet(output.d[2],dm28,zhm28,xmm,28.0);\n    \n\n\n    #/**** HE DENSITY ****/\n\n    #/*   Density variation factor at Zlb */\n    g4 = flags.sw[21]*globe7(pd[0], Input, flags);\n    #/*  Diffusive density at Zlb */\n    db04 = pdm[0][0]*exp(g4)*pd[0][0];\n    #/*  Diffusive density at Alt */\n    RandomVariable = [output.t[1]]\n    output.d[0]=densu(z,db04,tinf,tlb, 4.,alpha[0],RandomVariable,ptm[5],s,mn1,zn1,meso_tn1,meso_tgn1);\n    output.t[1] = RandomVariable[0]\n    dd=output.d[0];\n    if ((flags.sw[15]) and (z<altl[0])):\n        #/*  Turbopause */\n        zh04=pdm[0][2];\n        #/*  Mixed density at Zlb */\n        RandomVariable = [output.t[1]]\n        b04=densu(zh04,db04,tinf,tlb,4.-xmm,alpha[0]-1.,RandomVariable,ptm[5],s,mn1,zn1,meso_tn1,meso_tgn1);\n        output.t[1] = RandomVariable[0]\n        #/*  Mixed density at Alt */\n        RandomVariable = [output.t[1]]\n        global dm04\n        dm04=densu(z,b04,tinf,tlb,xmm,0.,RandomVariable,ptm[5],s,mn1,zn1,meso_tn1,meso_tgn1);\n        output.t[1] = RandomVariable[0]\n        zhm04=zhm28;\n        #/*  Net density at Alt */\n        output.d[0]=dnet(output.d[0],dm04,zhm04,xmm,4.);\n        #/*  Correction to specified mixing ratio at ground */\n        rl=log(b28*pdm[0][1]/b04);\n        zc04=pdm[0][4]*pdl[1][0];\n        hc04=pdm[0][5]*pdl[1][1];\n        #/*  Net density corrected at Alt */\n        output.d[0]=output.d[0]*ccor(z,rl,hc04,zc04);\n    \n\n\n    #/**** O DENSITY ****/\n\n    #/*  Density variation factor at Zlb */\n    g16= flags.sw[21]*globe7(pd[1],Input,flags);\n    #/*  Diffusive density at Zlb */\n    db16 =  pdm[1][0]*exp(g16)*pd[1][0];\n    #/*   Diffusive density at Alt */\n    RandomVariable = [output.t[1]]\n    output.d[1]=densu(z,db16,tinf,tlb, 16.,alpha[1],RandomVariable,ptm[5],s,mn1, zn1,meso_tn1,meso_tgn1);\n    output.t[1] = RandomVariable[0]\n    dd=output.d[1];\n    if ((flags.sw[15]) and (z<=altl[1])):\n        #/*   Turbopause */\n        zh16=pdm[1][2];\n        #/*  Mixed density at Zlb */\n        RandomVariable = [output.t[1]]\n        b16=densu(zh16,db16,tinf,tlb,16.0-xmm,(alpha[1]-1.0), RandomVariable,ptm[5],s,mn1,zn1,meso_tn1,meso_tgn1);\n        output.t[1] = RandomVariable[0]\n        #/*  Mixed density at Alt */\n        RandomVariable = [output.t[1]]\n        global dm16\n        dm16=densu(z,b16,tinf,tlb,xmm,0.,RandomVariable,ptm[5],s,mn1,zn1,meso_tn1,meso_tgn1);\n        output.t[1] = RandomVariable[0]\n        zhm16=zhm28;\n        #/*  Net density at Alt */\n        output.d[1]=dnet(output.d[1],dm16,zhm16,xmm,16.);\n        rl=pdm[1][1]*pdl[1][16]*(1.0+flags.sw[1]*pdl[0][23]*(Input.f107A-150.0));\n        hc16=pdm[1][5]*pdl[1][3];\n        zc16=pdm[1][4]*pdl[1][2];\n        hc216=pdm[1][5]*pdl[1][4];\n        output.d[1]=output.d[1]*ccor2(z,rl,hc16,zc16,hc216);\n        #/*   Chemistry correction */\n        hcc16=pdm[1][7]*pdl[1][13];\n        zcc16=pdm[1][6]*pdl[1][12];\n        rc16=pdm[1][3]*pdl[1][14];\n        #/*  Net density corrected at Alt */\n        output.d[1]=output.d[1]*ccor(z,rc16,hcc16,zcc16);\n    \n\n\n    #/**** O2 DENSITY ****/\n\n    #/*   Density variation factor at Zlb */\n    g32= flags.sw[21]*globe7(pd[4], Input, flags);\n    #/*  Diffusive density at Zlb */\n    db32 = pdm[3][0]*exp(g32)*pd[4][0];\n    #/*   Diffusive density at Alt */\n    RandomVariable = [output.t[1]]\n    output.d[3]=densu(z,db32,tinf,tlb, 32.,alpha[3],RandomVariable,ptm[5],s,mn1, zn1,meso_tn1,meso_tgn1);\n    output.t[1] = RandomVariable[0]\n    dd=output.d[3];\n    if (flags.sw[15]):\n        if (z<=altl[3]):\n            #/*   Turbopause */\n            zh32=pdm[3][2];\n            #/*  Mixed density at Zlb */\n            RandomVariable = [output.t[1]]\n            b32=densu(zh32,db32,tinf,tlb,32.-xmm,alpha[3]-1., RandomVariable,ptm[5],s,mn1,zn1,meso_tn1,meso_tgn1);\n            output.t[1] = RandomVariable[0]\n            #/*  Mixed density at Alt */\n            RandomVariable = [output.t[1]]\n            global dm32\n            dm32=densu(z,b32,tinf,tlb,xmm,0.,RandomVariable,ptm[5],s,mn1,zn1,meso_tn1,meso_tgn1);\n            output.t[1] = RandomVariable[0]\n            zhm32=zhm28;\n            #/*  Net density at Alt */\n            output.d[3]=dnet(output.d[3],dm32,zhm32,xmm,32.);\n            #/*   Correction to specified mixing ratio at ground */\n            rl=log(b28*pdm[3][1]/b32);\n            hc32=pdm[3][5]*pdl[1][7];\n            zc32=pdm[3][4]*pdl[1][6];\n            output.d[3]=output.d[3]*ccor(z,rl,hc32,zc32);\n        \n        #/*  Correction for general departure from diffusive equilibrium above Zlb */\n        hcc32=pdm[3][7]*pdl[1][22];\n        hcc232=pdm[3][7]*pdl[0][22];\n        zcc32=pdm[3][6]*pdl[1][21];\n        rc32=pdm[3][3]*pdl[1][23]*(1.+flags.sw[1]*pdl[0][23]*(Input.f107A-150.));\n        #/*  Net density corrected at Alt */\n        output.d[3]=output.d[3]*ccor2(z,rc32,hcc32,zcc32,hcc232);\n    \n\n\n    #/**** AR DENSITY ****/\n\n    #/*   Density variation factor at Zlb */\n    g40= flags.sw[21]*globe7(pd[5],Input,flags);\n    #/*  Diffusive density at Zlb */\n    db40 = pdm[4][0]*exp(g40)*pd[5][0];\n    #/*   Diffusive density at Alt */\n    RandomVariable = [output.t[1]]\n    output.d[4]=densu(z,db40,tinf,tlb, 40.,alpha[4],RandomVariable,ptm[5],s,mn1,zn1,meso_tn1,meso_tgn1);\n    output.t[1] = RandomVariable[0]\n    dd=output.d[4];\n    if ((flags.sw[15]) and (z<=altl[4])):\n        #/*   Turbopause */\n        zh40=pdm[4][2];\n        #/*  Mixed density at Zlb */\n        RandomVariable = [output.t[1]]\n        b40=densu(zh40,db40,tinf,tlb,40.-xmm,alpha[4]-1.,RandomVariable,ptm[5],s,mn1,zn1,meso_tn1,meso_tgn1);\n        output.t[1] = RandomVariable[0]\n        #/*  Mixed density at Alt */\n        RandomVariable = [output.t[1]]\n        global dm40\n        dm40=densu(z,b40,tinf,tlb,xmm,0.,RandomVariable,ptm[5],s,mn1,zn1,meso_tn1,meso_tgn1);\n        output.t[1] = RandomVariable[0]\n        zhm40=zhm28;\n        #/*  Net density at Alt */\n        output.d[4]=dnet(output.d[4],dm40,zhm40,xmm,40.);\n        #/*   Correction to specified mixing ratio at ground */\n        rl=log(b28*pdm[4][1]/b40);\n        hc40=pdm[4][5]*pdl[1][9];\n        zc40=pdm[4][4]*pdl[1][8];\n        #/*  Net density corrected at Alt */\n        output.d[4]=output.d[4]*ccor(z,rl,hc40,zc40);\n      \n\n\n    #/**** HYDROGEN DENSITY ****/\n\n    #/*   Density variation factor at Zlb */\n    g1 = flags.sw[21]*globe7(pd[6], Input, flags);\n    #/*  Diffusive density at Zlb */\n    db01 = pdm[5][0]*exp(g1)*pd[6][0];\n    #/*   Diffusive density at Alt */\n    RandomVariable = [output.t[1]]\n    output.d[6]=densu(z,db01,tinf,tlb,1.,alpha[6],RandomVariable,ptm[5],s,mn1,zn1,meso_tn1,meso_tgn1);\n    output.t[1] = RandomVariable[0]\n    dd=output.d[6];\n    if ((flags.sw[15]) and (z<=altl[6])):\n        #/*   Turbopause */\n        zh01=pdm[5][2];\n        #/*  Mixed density at Zlb */\n        RandomVariable = [output.t[1]]\n        b01=densu(zh01,db01,tinf,tlb,1.-xmm,alpha[6]-1., RandomVariable,ptm[5],s,mn1,zn1,meso_tn1,meso_tgn1);\n        output.t[1] = RandomVariable[0]\n        #/*  Mixed density at Alt */\n        RandomVariable = [output.t[1]]\n        global dm01\n        dm01=densu(z,b01,tinf,tlb,xmm,0.,RandomVariable,ptm[5],s,mn1,zn1,meso_tn1,meso_tgn1);\n        output.t[1] = RandomVariable[0]\n        zhm01=zhm28;\n        #/*  Net density at Alt */\n        output.d[6]=dnet(output.d[6],dm01,zhm01,xmm,1.);\n        #/*   Correction to specified mixing ratio at ground */\n        rl=log(b28*pdm[5][1]*sqrt(pdl[1][17]*pdl[1][17])/b01);\n        hc01=pdm[5][5]*pdl[1][11];\n        zc01=pdm[5][4]*pdl[1][10];\n        output.d[6]=output.d[6]*ccor(z,rl,hc01,zc01);\n        #/*   Chemistry correction */\n        hcc01=pdm[5][7]*pdl[1][19];\n        zcc01=pdm[5][6]*pdl[1][18];\n        rc01=pdm[5][3]*pdl[1][20];\n        #/*  Net density corrected at Alt */\n        output.d[6]=output.d[6]*ccor(z,rc01,hcc01,zcc01);\n\n\n\n    #/**** ATOMIC NITROGEN DENSITY ****/\n\n    #/*   Density variation factor at Zlb */\n    g14 = flags.sw[21]*globe7(pd[7],Input,flags);\n    #/*  Diffusive density at Zlb */\n    db14 = pdm[6][0]*exp(g14)*pd[7][0];\n    #/*   Diffusive density at Alt */\n    RandomVariable = [output.t[1]]\n    output.d[7]=densu(z,db14,tinf,tlb,14.,alpha[7],RandomVariable,ptm[5],s,mn1,zn1,meso_tn1,meso_tgn1);\n    output.t[1] = RandomVariable[0]\n    dd=output.d[7];\n    if ((flags.sw[15]) and (z<=altl[7])): \n        #/*   Turbopause */\n        zh14=pdm[6][2];\n        #/*  Mixed density at Zlb */\n        RandomVariable = [output.t[1]]\n        b14=densu(zh14,db14,tinf,tlb,14.-xmm,alpha[7]-1., RandomVariable,ptm[5],s,mn1,zn1,meso_tn1,meso_tgn1);\n        output.t[1] = RandomVariable[0]\n        #/*  Mixed density at Alt */\n        RandomVariable = [output.t[1]]\n        global dm14\n        dm14=densu(z,b14,tinf,tlb,xmm,0.,RandomVariable,ptm[5],s,mn1,zn1,meso_tn1,meso_tgn1);\n        output.t[1] = RandomVariable[0]\n        zhm14=zhm28;\n        #/*  Net density at Alt */\n        output.d[7]=dnet(output.d[7],dm14,zhm14,xmm,14.);\n        #/*   Correction to specified mixing ratio at ground */\n        rl=log(b28*pdm[6][1]*sqrt(pdl[0][2]*pdl[0][2])/b14);\n        hc14=pdm[6][5]*pdl[0][1];\n        zc14=pdm[6][4]*pdl[0][0];\n        output.d[7]=output.d[7]*ccor(z,rl,hc14,zc14);\n        #/*   Chemistry correction */\n        hcc14=pdm[6][7]*pdl[0][4];\n        zcc14=pdm[6][6]*pdl[0][3];\n        rc14=pdm[6][3]*pdl[0][5];\n        #/*  Net density corrected at Alt */\n        output.d[7]=output.d[7]*ccor(z,rc14,hcc14,zcc14);\n    \n\n\n    #/**** Anomalous OXYGEN DENSITY ****/\n\n    g16h = flags.sw[21]*globe7(pd[8],Input,flags);\n    db16h = pdm[7][0]*exp(g16h)*pd[8][0];\n    tho = pdm[7][9]*pdl[0][6];\n    RandomVariable = [output.t[1]]\n    dd=densu(z,db16h,tho,tho,16.,alpha[8],RandomVariable,ptm[5],s,mn1, zn1,meso_tn1,meso_tgn1);\n    output.t[1] = RandomVariable[0]\n    zsht=pdm[7][5];\n    zmho=pdm[7][4];\n    zsho=scalh(zmho,16.0,tho);\n    output.d[8]=dd*exp(-zsht/zsho*(exp(-(z-zmho)/zsht)-1.));\n\n\n    #/* total mass density */\n    output.d[5] = 1.66E-24*(4.0*output.d[0]+16.0*output.d[1]+28.0*output.d[2]+32.0*output.d[3]+40.0*output.d[4]+ output.d[6]+14.0*output.d[7]);\n    db48=1.66E-24*(4.0*db04+16.0*db16+28.0*db28+32.0*db32+40.0*db40+db01+14.0*db14);\n\n\n\n    #/* temperature */\n    z = sqrt(Input.alt*Input.alt);\n    RandomVariable = [output.t[1]]\n    ddum = densu(z,1.0, tinf, tlb, 0.0, 0.0, RandomVariable, ptm[5], s, mn1, zn1, meso_tn1, meso_tgn1);\n    output.t[1] = RandomVariable[0]\n    if (flags.sw[0]): # pragma: no cover\n        for i in range(9):\n            output.d[i]=output.d[i]*1.0E6;\n        output.d[5]=output.d[5]/1000;\n    return", "response": "This function computes the Thermospheric portion of NRLMSISE - 00 and returns the corresponding times for the current thread."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nseems sto work fine. >> from sympy import * >> from sympy.abc import * >> Matrix([[a, b], [c, d]]).det() a*d - b*c >> Matrix([[a, b, c], [d, e, f], [g, h, i]]).det() a*e*i - a*f*h - b*d*i + b*f*g + c*d*h - c*e*g A few terms can be slightly factored out of the 3x dim. >> Matrix([[a, b, c, d], [e, f, g, h], [i, j, k, l], [m, n, o, p]]).det() a*f*k*p - a*f*l*o - a*g*j*p + a*g*l*n + a*h*j*o - a*h*k*n - b*e*k*p + b*e*l*o + b*g*i*p - b*g*l*m - b*h*i*o + b*h*k*m + c*e*j*p - c*e*l*n - c*f*i*p + c*f*l*m + c*h*i*n - c*h*j*m - d*e*j*o + d*e*k*n + d*f*i*o - d*f*k*m - d*g*i*n + d*g*j*m 72 mult vs ~48 in cse'd version' Commented out - takes a few seconds >> #Matrix([[a, b, c, d, e], [f, g, h, i, j], [k, l, m, n, o], [p, q, r, s, t], [u, v, w, x, y]]).det() 260 multiplies with cse; 480 without it.", "response": "def det(matrix):\n    '''Seem sto work fine.\n    \n    >> from sympy import *\n    >> from sympy.abc import *\n    >> Matrix([[a, b], [c, d]]).det()\n    a*d - b*c\n    >> Matrix([[a, b, c], [d, e, f], [g, h, i]]).det()\n    a*e*i - a*f*h - b*d*i + b*f*g + c*d*h - c*e*g\n    \n    A few terms can be slightly factored out of the 3x dim.\n    \n    >> Matrix([[a, b, c, d], [e, f, g, h], [i, j, k, l], [m, n, o, p]]).det()\n    a*f*k*p - a*f*l*o - a*g*j*p + a*g*l*n + a*h*j*o - a*h*k*n - b*e*k*p + b*e*l*o + b*g*i*p - b*g*l*m - b*h*i*o + b*h*k*m + c*e*j*p - c*e*l*n - c*f*i*p + c*f*l*m + c*h*i*n - c*h*j*m - d*e*j*o + d*e*k*n + d*f*i*o - d*f*k*m - d*g*i*n + d*g*j*m\n\n    72 mult vs ~48 in cse'd version'\n    \n    Commented out - takes a few seconds\n    >> #Matrix([[a, b, c, d, e], [f, g, h, i, j], [k, l, m, n, o], [p, q, r, s, t], [u, v, w, x, y]]).det()\n    \n    260 multiplies with cse; 480 without it.\n    '''\n    size = len(matrix)\n    if size == 1:\n        return matrix[0]\n    elif size == 2:\n        (a, b), (c, d) = matrix\n        return a*d - c*b\n    elif size == 3:\n        (a, b, c), (d, e, f), (g, h, i) = matrix\n        return a*(e*i - h*f) - d*(b*i - h*c) + g*(b*f - e*c)\n    elif size == 4:\n        (a, b, c, d), (e, f, g, h), (i, j, k, l), (m, n, o, p) = matrix\n        return (a*f*k*p - a*f*l*o - a*g*j*p + a*g*l*n + a*h*j*o - a*h*k*n \n                - b*e*k*p + b*e*l*o + b*g*i*p - b*g*l*m - b*h*i*o + b*h*k*m \n                + c*e*j*p - c*e*l*n - c*f*i*p + c*f*l*m + c*h*i*n - c*h*j*m \n                - d*e*j*o + d*e*k*n + d*f*i*o - d*f*k*m - d*g*i*n + d*g*j*m)\n    elif size == 5:\n        (a, b, c, d, e), (f, g, h, i, j), (k, l, m, n, o), (p, q, r, s, t), (u, v, w, x, y) = matrix\n        x0 = s*y\n        x1 = a*g*m\n        x2 = t*w\n        x3 = a*g*n\n        x4 = r*x\n        x5 = a*g*o\n        x6 = t*x\n        x7 = a*h*l\n        x8 = q*y\n        x9 = a*h*n\n        x10 = s*v\n        x11 = a*h*o\n        x12 = r*y\n        x13 = a*i*l\n        x14 = t*v\n        x15 = a*i*m\n        x16 = q*w\n        x17 = a*i*o\n        x18 = s*w\n        x19 = a*j*l\n        x20 = q*x\n        x21 = a*j*m\n        x22 = r*v\n        x23 = a*j*n\n        x24 = b*f*m\n        x25 = b*f*n\n        x26 = b*f*o\n        x27 = b*h*k\n        x28 = t*u\n        x29 = b*h*n\n        x30 = p*x\n        x31 = b*h*o\n        x32 = b*i*k\n        x33 = p*y\n        x34 = b*i*m\n        x35 = r*u\n        x36 = b*i*o\n        x37 = b*j*k\n        x38 = s*u\n        x39 = b*j*m\n        x40 = p*w\n        x41 = b*j*n\n        x42 = c*f*l\n        x43 = c*f*n\n        x44 = c*f*o\n        x45 = c*g*k\n        x46 = c*g*n\n        x47 = c*g*o\n        x48 = c*i*k\n        x49 = c*i*l\n        x50 = p*v\n        x51 = c*i*o\n        x52 = c*j*k\n        x53 = c*j*l\n        x54 = q*u\n        x55 = c*j*n\n        x56 = d*f*l\n        x57 = d*f*m\n        x58 = d*f*o\n        x59 = d*g*k\n        x60 = d*g*m\n        x61 = d*g*o\n        x62 = d*h*k\n        x63 = d*h*l\n        x64 = d*h*o\n        x65 = d*j*k\n        x66 = d*j*l\n        x67 = d*j*m\n        x68 = e*f*l\n        x69 = e*f*m\n        x70 = e*f*n\n        x71 = e*g*k\n        x72 = e*g*m\n        x73 = e*g*n\n        x74 = e*h*k\n        x75 = e*h*l\n        x76 = e*h*n\n        x77 = e*i*k\n        x78 = e*i*l\n        x79 = e*i*m        \n        return (x0*x1 - x0*x24 + x0*x27 + x0*x42 - x0*x45 - x0*x7 - x1*x6 \n                + x10*x11 - x10*x21 - x10*x44 + x10*x52 + x10*x69 - x10*x74\n                - x11*x20 + x12*x13 + x12*x25 - x12*x3 - x12*x32 - x12*x56\n                + x12*x59 - x13*x2 + x14*x15 + x14*x43 - x14*x48 - x14*x57\n                + x14*x62 - x14*x9 - x15*x8 + x16*x17 - x16*x23 - x16*x58 \n                + x16*x65 + x16*x70 - x16*x77 - x17*x22 + x18*x19 + x18*x26\n                - x18*x37 - x18*x5 - x18*x68 + x18*x71 - x19*x4 - x2*x25\n                + x2*x3 + x2*x32 + x2*x56 - x2*x59 + x20*x21 + x20*x44 \n                - x20*x52 - x20*x69 + x20*x74 + x22*x23 + x22*x58 - x22*x65\n                - x22*x70 + x22*x77 + x24*x6 - x26*x4 - x27*x6 + x28*x29 \n                - x28*x34 - x28*x46 + x28*x49 + x28*x60 - x28*x63 - x29*x33\n                + x30*x31 - x30*x39 - x30*x47 + x30*x53 + x30*x72 - x30*x75\n                - x31*x38 + x33*x34 + x33*x46 - x33*x49 - x33*x60 + x33*x63 \n                + x35*x36 - x35*x41 - x35*x61 + x35*x66 + x35*x73 - x35*x78 \n                - x36*x40 + x37*x4 + x38*x39 + x38*x47 - x38*x53 - x38*x72 \n                + x38*x75 + x4*x5 + x4*x68 - x4*x71 + x40*x41 + x40*x61\n                - x40*x66 - x40*x73 + x40*x78 - x42*x6 - x43*x8 + x45*x6 \n                + x48*x8 + x50*x51 - x50*x55 - x50*x64 + x50*x67 + x50*x76 \n                - x50*x79 - x51*x54 + x54*x55 + x54*x64 - x54*x67 - x54*x76\n                + x54*x79 + x57*x8 + x6*x7 - x62*x8 + x8*x9)\n    else:\n        # TODO algorithm?\n        import numpy as np\n        return float(np.linalg.det(matrix))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef inv(matrix):\n    '''\n    5 has way too many multiplies.\n    >> from sympy import *\n    >> from sympy.abc import *\n    >> Matrix([a]).inv()\n    Matrix([[1/a]])\n    \n    >> cse(Matrix([[a, b], [c, d]]).inv())\n    Matrix([\n    [1/a + b*c/(a**2*(d - b*c/a)), -b/(a*(d - b*c/a))],\n    [          -c/(a*(d - b*c/a)),      1/(d - b*c/a)]])\n    \n    >> m_3 = Matrix([[a, b, c], [d, e, f], [g, h, i]])\n    >> #cse(m_3.inv())\n\n    >> m_4 = Matrix([[a, b, c, d], [e, f, g, h], [i, j, k, l], [m, n, o, p]])\n    >> cse(m_4.inv())\n    \n    # Note: for 3, 4 - forgot to generate code using optimizations='basic'\n    '''\n    size = len(matrix)\n    if size == 1:\n        return [1.0/matrix[0]]\n    elif size == 2:\n        (a, b), (c, d) = matrix\n        x0 = 1.0/a\n        x1 = b*x0\n        x2 = 1.0/(d - c*x1)\n        x3 = c*x2\n        return [[x0 + b*x3*x0*x0, -x1*x2],\n                [-x0*x3, x2]]\n    elif size == 3:\n        (a, b, c), (d, e, f), (g, h, i) = matrix\n        x0 = 1./a\n        x1 = b*d\n        x2 = e - x0*x1\n        x3 = 1./x2\n        x4 = b*g\n        x5 = h - x0*x4\n        x6 = x0*x3\n        x7 = d*x6\n        x8 = -g*x0 + x5*x7\n        x9 = c*d\n        x10 = f - x0*x9\n        x11 = b*x6\n        x12 = c*x0 - x10*x11\n        x13 = a*e\n        x14 = -x1 + x13\n        x15 = 1./(-a*f*h - c*e*g + f*x4 + h*x9 - i*x1 + i*x13)\n        x16 = x14*x15\n        x17 = x12*x16\n        x18 = x14*x15*x3\n        x19 = x18*x5\n        x20 = x10*x18\n        return [[x0 - x17*x8 + x1*x3*x0*x0, -x11 + x12*x19, -x17],\n                [-x20*x8 - x7, x10*x16*x5*x2**-2 + x3, -x20],\n                [ x16*x8, -x19, x16]]\n    elif size == 4:\n        (a, b, c, d), (e, f, g, h), (i, j, k, l), (m, n, o, p) = matrix\n        x0 = 1./a\n        x1 = b*e\n        x2 = f - x0*x1\n        x3 = 1./x2\n        x4 = i*x0\n        x5 = -b*x4 + j\n        x6 = x0*x3\n        x7 = e*x6\n        x8 = -x4 + x5*x7\n        x9 = c*x0\n        x10 = -e*x9 + g\n        x11 = b*x6\n        x12 = -x10*x11 + x9\n        x13 = a*f\n        x14 = -x1 + x13\n        x15 = k*x13\n        x16 = b*g*i\n        x17 = c*e*j\n        x18 = a*g*j\n        x19 = k*x1\n        x20 = c*f*i\n        x21 = x15 + x16 + x17 - x18 - x19 - x20\n        x22 = 1/x21\n        x23 = x14*x22\n        x24 = x12*x23\n        x25 = m*x0\n        x26 = -b*x25 + n\n        x27 = x26*x3\n        x28 = -m*x9 + o - x10*x27\n        x29 = x23*x8\n        x30 = -x25 + x26*x7 - x28*x29\n        x31 = d*x0\n        x32 = -e*x31 + h\n        x33 = x3*x32\n        x34 = -i*x31 + l - x33*x5\n        x35 = -x11*x32 - x24*x34 + x31\n        x36 = a*n\n        x37 = g*l\n        x38 = h*o\n        x39 = l*o\n        x40 = b*m\n        x41 = h*k\n        x42 = c*l\n        x43 = f*m\n        x44 = c*h\n        x45 = i*n\n        x46 = d*k\n        x47 = e*n\n        x48 = d*o\n        x49 = d*g\n        x50 = j*m\n        x51 = 1.0/(a*j*x38 - b*i*x38 - e*j*x48 + f*i*x48 + p*x15 \n                 + p*x16 + p*x17 - p*x18 - p*x19 - p*x20 + x1*x39 \n                 - x13*x39 + x36*x37 - x36*x41 - x37*x40 + x40*x41\n                 + x42*x43 - x42*x47 - x43*x46 + x44*x45 - x44*x50 \n                 - x45*x49 + x46*x47 + x49*x50)\n        x52 = x21*x51\n        x53 = x35*x52\n        x54 = x14*x22*x3\n        x55 = x5*x54\n        x56 = -x27 + x28*x55\n        x57 = x52*x56\n        x58 = x14*x51\n        x59 = x28*x58\n        x60 = x10*x54\n        x61 = x33 - x34*x60\n        x62 = x52*x61\n        x63 = x34*x58\n        return [[x0 - x24*x8 - x30*x53 + x1*x3*x0*x0, -x11 + x12*x55 - x35*x57, -x24 + x35*x59, -x53],\n             [-x30*x62 - x60*x8 - x7, x10*x23*x5*x2**-2 + x3 - x56*x62, x59*x61 - x60, -x62],\n             [x29 - x30*x63, -x55 - x56*x63, x14**2*x22*x28*x34*x51 + x23, -x63],\n             [x30*x52, x57, -x59, x52]]\n    else:\n        # TODO algorithm?\n        import numpy as np\n        return np.linalg.inv(matrix)", "response": "This function is used to generate the inverse of the matrix."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef Peclet_heat(V, L, rho=None, Cp=None, k=None, alpha=None):\n    r'''Calculates heat transfer Peclet number or `Pe` for a specified velocity\n    `V`, characteristic length `L`, and specified properties for the given\n    fluid.\n\n    .. math::\n        Pe = \\frac{VL\\rho C_p}{k} = \\frac{LV}{\\alpha}\n\n    Inputs either of any of the following sets:\n\n    * V, L, density `rho`, heat capacity `Cp`, and thermal conductivity `k`\n    * V, L, and thermal diffusivity `alpha`\n\n    Parameters\n    ----------\n    V : float\n        Velocity [m/s]\n    L : float\n        Characteristic length [m]\n    rho : float, optional\n        Density, [kg/m^3]\n    Cp : float, optional\n        Heat capacity, [J/kg/K]\n    k : float, optional\n        Thermal conductivity, [W/m/K]\n    alpha : float, optional\n        Thermal diffusivity, [m^2/s]\n\n    Returns\n    -------\n    Pe : float\n        Peclet number (heat) []\n\n    Notes\n    -----\n    .. math::\n        Pe = \\frac{\\text{Bulk heat transfer}}{\\text{Conduction heat transfer}}\n\n    An error is raised if none of the required input sets are provided.\n\n    Examples\n    --------\n    >>> Peclet_heat(1.5, 2, 1000., 4000., 0.6)\n    20000000.0\n    >>> Peclet_heat(1.5, 2, alpha=1E-7)\n    30000000.0\n\n    References\n    ----------\n    .. [1] Green, Don, and Robert Perry. Perry's Chemical Engineers' Handbook,\n       Eighth Edition. McGraw-Hill Professional, 2007.\n    .. [2] Cengel, Yunus, and John Cimbala. Fluid Mechanics: Fundamentals and\n       Applications. Boston: McGraw Hill Higher Education, 2006.\n    '''\n    if rho and Cp and k:\n        alpha =  k/(rho*Cp)\n    elif not alpha:\n        raise Exception('Either heat capacity and thermal conductivity and\\\n        density, or thermal diffusivity is needed')\n    return V*L/alpha", "response": "r Returns a base class that calculates the heat transfer Peclet number or Pe for a given velocity V L density rho Cp and thermal conductivity k and alpha."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef Graetz_heat(V, D, x, rho=None, Cp=None, k=None, alpha=None):\n    r'''Calculates Graetz number or `Gz` for a specified velocity\n    `V`, diameter `D`, axial distance `x`, and specified properties for the\n    given fluid.\n\n    .. math::\n        Gz = \\frac{VD^2\\cdot C_p \\rho}{x\\cdot k} = \\frac{VD^2}{x \\alpha}\n\n    Inputs either of any of the following sets:\n\n    * V, D, x, density `rho`, heat capacity `Cp`, and thermal conductivity `k`\n    * V, D, x, and thermal diffusivity `alpha`\n\n    Parameters\n    ----------\n    V : float\n        Velocity, [m/s]\n    D : float\n        Diameter [m]\n    x : float\n        Axial distance [m]\n    rho : float, optional\n        Density, [kg/m^3]\n    Cp : float, optional\n        Heat capacity, [J/kg/K]\n    k : float, optional\n        Thermal conductivity, [W/m/K]\n    alpha : float, optional\n        Thermal diffusivity, [m^2/s]\n\n    Returns\n    -------\n    Gz : float\n        Graetz number []\n\n    Notes\n    -----\n    .. math::\n        Gz = \\frac{\\text{Time for radial heat diffusion in a fluid by conduction}}\n        {\\text{Time taken by fluid to reach distance x}}\n\n    .. math::\n        Gz = \\frac{D}{x}RePr\n\n    An error is raised if none of the required input sets are provided.\n\n    Examples\n    --------\n    >>> Graetz_heat(1.5, 0.25, 5, 800., 2200., 0.6)\n    55000.0\n    >>> Graetz_heat(1.5, 0.25, 5, alpha=1E-7)\n    187500.0\n\n    References\n    ----------\n    .. [1] Bergman, Theodore L., Adrienne S. Lavine, Frank P. Incropera, and\n       David P. DeWitt. Introduction to Heat Transfer. 6E. Hoboken, NJ:\n       Wiley, 2011.\n    '''\n    if rho and Cp and k:\n        alpha =  k/(rho*Cp)\n    elif not alpha:\n        raise Exception('Either heat capacity and thermal conductivity and\\\n        density, or thermal diffusivity is needed')\n    return V*D**2/(x*alpha)", "response": "r Returns a Graetz number or Gz for a specified velocity V diameter D axial distance x and properties of the specified properties."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef Schmidt(D, mu=None, nu=None, rho=None):\n    r'''Calculates Schmidt number or `Sc` for a fluid with the given\n    parameters.\n\n    .. math::\n        Sc = \\frac{\\mu}{D\\rho} = \\frac{\\nu}{D}\n\n    Inputs can be any of the following sets:\n\n    * Diffusivity, dynamic viscosity, and density\n    * Diffusivity and kinematic viscosity\n\n    Parameters\n    ----------\n    D : float\n        Diffusivity of a species, [m^2/s]\n    mu : float, optional\n        Dynamic viscosity, [Pa*s]\n    nu : float, optional\n        Kinematic viscosity, [m^2/s]\n    rho : float, optional\n        Density, [kg/m^3]\n\n    Returns\n    -------\n    Sc : float\n        Schmidt number []\n\n    Notes\n    -----\n    .. math::\n        Sc =\\frac{\\text{kinematic viscosity}}{\\text{molecular diffusivity}}\n        = \\frac{\\text{viscous diffusivity}}{\\text{species diffusivity}}\n\n    An error is raised if none of the required input sets are provided.\n\n    Examples\n    --------\n    >>> Schmidt(D=2E-6, mu=4.61E-6, rho=800)\n    0.00288125\n    >>> Schmidt(D=1E-9, nu=6E-7)\n    599.9999999999999\n\n    References\n    ----------\n    .. [1] Green, Don, and Robert Perry. Perry's Chemical Engineers' Handbook,\n       Eighth Edition. McGraw-Hill Professional, 2007.\n    .. [2] Cengel, Yunus, and John Cimbala. Fluid Mechanics: Fundamentals and\n       Applications. Boston: McGraw Hill Higher Education, 2006.\n    '''\n    if rho and mu:\n        return mu/(rho*D)\n    elif nu:\n        return nu/D\n    else:\n        raise Exception('Insufficient information provided for Schmidt number calculation')", "response": "r Returns the Schmidt number or Sc for a fluid with the given parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef Confinement(D, rhol, rhog, sigma, g=g):\n    r'''Calculates Confinement number or `Co` for a fluid in a channel of \n    diameter `D` with liquid and gas densities `rhol` and `rhog` and surface\n    tension `sigma`, under the influence of gravitational force `g`.\n\n    .. math::\n        \\text{Co}=\\frac{\\left[\\frac{\\sigma}{g(\\rho_l-\\rho_g)}\\right]^{0.5}}{D}\n\n    Parameters\n    ----------\n    D : float\n        Diameter of channel, [m]\n    rhol : float\n        Density of liquid phase, [kg/m^3]\n    rhog : float\n        Density of gas phase, [kg/m^3]\n    sigma : float\n        Surface tension between liquid-gas phase, [N/m]\n    g : float, optional\n        Acceleration due to gravity, [m/s^2]\n\n    Returns\n    -------\n    Co : float\n        Confinement number [-]\n\n    Notes\n    -----\n    Used in two-phase pressure drop and heat transfer correlations. First used\n    in [1]_ according to [3]_.\n\n    .. math::\n        \\text{Co} = \\frac{\\frac{\\text{surface tension force}}\n        {\\text{buoyancy force}}}{\\text{Channel area}}\n\n    Examples\n    --------\n    >>> Confinement(0.001, 1077, 76.5, 4.27E-3)\n    0.6596978265315191\n\n    References\n    ----------\n    .. [1] Cornwell, Keith, and Peter A. Kew. \"Boiling in Small Parallel \n       Channels.\" In Energy Efficiency in Process Technology, edited by Dr P. \n       A. Pilavachi, 624-638. Springer Netherlands, 1993. \n       doi:10.1007/978-94-011-1454-7_56.\n    .. [2] Kandlikar, Satish G. Heat Transfer and Fluid Flow in Minichannels \n       and Microchannels. Elsevier, 2006.\n    .. [3] Tran, T. N, M. -C Chyu, M. W Wambsganss, and D. M France. Two-Phase \n       Pressure Drop of Refrigerants during Flow Boiling in Small Channels: An \n       Experimental Investigation and Correlation Development.\" International \n       Journal of Multiphase Flow 26, no. 11 (November 1, 2000): 1739-54. \n       doi:10.1016/S0301-9322(99)00119-6.\n    '''\n    return (sigma/(g*(rhol-rhog)))**0.5/D", "response": "r Returns a sequence of co - code numbers for a fluid in a channel of \n    diameter D rhol and rhog and surface tension sigma under the influence of gravitational force g."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef Morton(rhol, rhog, mul, sigma, g=g):\n    r'''Calculates Morton number or `Mo` for a liquid and vapor with the\n    specified properties, under the influence of gravitational force `g`.\n\n    .. math::\n        Mo = \\frac{g \\mu_l^4(\\rho_l - \\rho_g)}{\\rho_l^2 \\sigma^3}\n        \n    Parameters\n    ----------\n    rhol : float\n        Density of liquid phase, [kg/m^3]\n    rhog : float\n        Density of gas phase, [kg/m^3]\n    mul : float\n        Viscosity of liquid phase, [Pa*s]\n    sigma : float\n        Surface tension between liquid-gas phase, [N/m]\n    g : float, optional\n        Acceleration due to gravity, [m/s^2]\n\n    Returns\n    -------\n    Mo : float\n        Morton number, [-]\n\n    Notes\n    -----\n    Used in modeling bubbles in liquid.\n\n    Examples\n    --------\n    >>> Morton(1077.0, 76.5, 4.27E-3, 0.023)\n    2.311183104430743e-07\n\n    References\n    ----------\n    .. [1] Kunes, Josef. Dimensionless Physical Quantities in Science and \n       Engineering. Elsevier, 2012.\n    .. [2] Yan, Xiaokang, Kaixin Zheng, Yan Jia, Zhenyong Miao, Lijun Wang,\n       Yijun Cao, and Jiongtian Liu. \u201cDrag Coefficient Prediction of a Single \n       Bubble Rising in Liquids.\u201d Industrial & Engineering Chemistry Research, \n       April 2, 2018. https://doi.org/10.1021/acs.iecr.7b04743.\n    '''\n    mul2 = mul*mul\n    return g*mul2*mul2*(rhol - rhog)/(rhol*rhol*sigma*sigma*sigma)", "response": "Calculates the Morton number or Mo for a single entry in the liquid and vapor."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef Froude(V, L, g=g, squared=False):\n    r'''Calculates Froude number `Fr` for velocity `V` and geometric length\n    `L`. If desired, gravity can be specified as well. Normally the function\n    returns the result of the equation below; Froude number is also often\n    said to be defined as the square of the equation below.\n\n    .. math::\n        Fr = \\frac{V}{\\sqrt{gL}}\n\n    Parameters\n    ----------\n    V : float\n        Velocity of the particle or fluid, [m/s]\n    L : float\n        Characteristic length, no typical definition [m]\n    g : float, optional\n        Acceleration due to gravity, [m/s^2]\n    squared : bool, optional\n        Whether to return the squared form of Froude number\n\n    Returns\n    -------\n    Fr : float\n        Froude number, [-]\n\n    Notes\n    -----\n    Many alternate definitions including density ratios have been used.\n\n    .. math::\n        Fr = \\frac{\\text{Inertial Force}}{\\text{Gravity Force}}\n\n    Examples\n    --------\n    >>> Froude(1.83, L=2., g=1.63)\n    1.0135432593877318\n    >>> Froude(1.83, L=2., squared=True)\n    0.17074638128208924\n\n    References\n    ----------\n    .. [1] Green, Don, and Robert Perry. Perry's Chemical Engineers' Handbook,\n       Eighth Edition. McGraw-Hill Professional, 2007.\n    .. [2] Cengel, Yunus, and John Cimbala. Fluid Mechanics: Fundamentals and\n       Applications. Boston: McGraw Hill Higher Education, 2006.\n    '''\n    Fr = V/(L*g)**0.5\n    if squared:\n        Fr *= Fr\n    return Fr", "response": "r Returns the Froude number Fr for a given velocity V and geometric length L."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the Stokes Number for a given characteristic velocity Dp particle diameter rhop and fluid viscosity mu.", "response": "def Stokes_number(V, Dp, D, rhop, mu):\n    r'''Calculates Stokes Number for a given characteristic velocity `V`, \n    particle diameter `Dp`, characteristic diameter `D`, particle density \n    `rhop`, and fluid viscosity `mu`.\n\n    .. math::\n        \\text{Stk} = \\frac{\\rho_p V D_p^2}{18\\mu_f D}\n\n    Parameters\n    ----------\n    V : float\n        Characteristic velocity (often superficial), [m/s]\n    Dp : float\n        Particle diameter, [m]\n    D : float\n        Characteristic diameter (ex demister wire diameter or cyclone \n        diameter), [m]\n    rhop : float\n        Particle density, [kg/m^3]\n    mu : float\n        Fluid viscosity, [Pa*s]\n    \n    Returns\n    -------\n    Stk : float\n        Stokes numer, [-]\n\n    Notes\n    -----\n    Used in droplet impaction or collection studies.\n\n    Examples\n    --------\n    >>> Stokes_number(V=0.9, Dp=1E-5, D=1E-3, rhop=1000, mu=1E-5)\n    0.5\n\n    References\n    ----------\n    .. [1] Rhodes, Martin J. Introduction to Particle Technology. Wiley, 2013.\n    .. [2] Al-Dughaither, Abdullah S., Ahmed A. Ibrahim, and Waheed A. \n       Al-Masry. \"Investigating Droplet Separation Efficiency in Wire-Mesh Mist\n       Eliminators in Bubble Column.\" Journal of Saudi Chemical Society 14, no.\n       4 (October 1, 2010): 331-39. https://doi.org/10.1016/j.jscs.2010.04.001.\n    '''\n    return rhop*V*(Dp*Dp)/(18.0*mu*D)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef Suratman(L, rho, mu, sigma):\n    r'''Calculates Suratman number, `Su`, for a fluid with the given\n    characteristic length, density, viscosity, and surface tension.\n\n    .. math::\n        \\text{Su} = \\frac{\\rho\\sigma L}{\\mu^2}\n\n    Parameters\n    ----------\n    L : float\n        Characteristic length [m]\n    rho : float\n        Density of fluid, [kg/m^3]\n    mu : float\n        Viscosity of fluid, [Pa*s]\n    sigma : float\n        Surface tension, [N/m]\n\n    Returns\n    -------\n    Su : float\n        Suratman number []\n\n    Notes\n    -----\n    Also known as Laplace number. Used in two-phase flow, especially the \n    bubbly-slug regime. No confusion regarding the definition of this group \n    has been observed.\n\n    .. math::\n        \\text{Su} = \\frac{\\text{Re}^2}{\\text{We}} =\\frac{\\text{Inertia}\\cdot\n        \\text{Surface tension} }{\\text{(viscous forces)}^2}\n\n    The oldest reference to this group found by the author is in 1963, from \n    [2]_.\n\n    Examples\n    --------\n    >>> Suratman(1E-4, 1000., 1E-3, 1E-1)\n    10000.0\n\n    References\n    ----------\n    .. [1] Sen, Nilava. \"Suratman Number in Bubble-to-Slug Flow Pattern \n       Transition under Microgravity.\" Acta Astronautica 65, no. 3-4 (August \n       2009): 423-28. doi:10.1016/j.actaastro.2009.02.013.\n    .. [2] Catchpole, John P., and George. Fulford. \"DIMENSIONLESS GROUPS.\" \n       Industrial & Engineering Chemistry 58, no. 3 (March 1, 1966): 46-60. \n       doi:10.1021/ie50675a012.\n    '''\n    return rho*sigma*L/(mu*mu)", "response": "r A simple function that calculates the Suratman number for a given fluid with the given characteristic length density viscosity and surface tension."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef P_isothermal_critical_flow(P, fd, D, L):\n    r'''Calculates critical flow pressure `Pcf` for a fluid flowing\n    isothermally and suffering pressure drop caused by a pipe's friction factor.\n\n    .. math::\n        P_2 = P_{1} e^{\\frac{1}{2 D} \\left(D \\left(\\operatorname{LambertW}\n        {\\left (- e^{\\frac{1}{D} \\left(- D - L f_d\\right)} \\right )} + 1\\right)\n        + L f_d\\right)}\n\n    Parameters\n    ----------\n    P : float\n        Inlet pressure [Pa]\n    fd : float\n        Darcy friction factor for flow in pipe [-]\n    D : float\n        Diameter of pipe, [m]\n    L : float\n        Length of pipe, [m]\n\n    Returns\n    -------\n    Pcf : float\n        Critical flow pressure of a compressible gas flowing from `P1` to `Pcf`\n        in a tube of length L and friction factor `fd` [Pa]\n\n    Notes\n    -----\n    Assumes isothermal flow. Developed based on the `isothermal_gas` model,\n    using SymPy.\n\n    The isothermal gas model is solved for maximum mass flow rate; any pressure\n    drop under it is impossible due to the formation of a shock wave.\n\n    Examples\n    --------\n    >>> P_isothermal_critical_flow(P=1E6, fd=0.00185, L=1000., D=0.5)\n    389699.7317645518\n\n    References\n    ----------\n    .. [1] Wilkes, James O. Fluid Mechanics for Chemical Engineers with\n       Microfluidics and CFD. 2 edition. Upper Saddle River, NJ: Prentice Hall,\n       2005.\n    '''\n    # Correct branch of lambertw found by trial and error\n    lambert_term = float(lambertw(-exp((-D - L*fd)/D), -1).real)\n    return P*exp((D*(lambert_term + 1.0) + L*fd)/(2.0*D))", "response": "Calculates critical flow pressure Pcf for a fluid flowing from P1 to P2."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef P_upstream_isothermal_critical_flow(P, fd, D, L):\n    '''Not part of the public API. Reverses `P_isothermal_critical_flow`.\n\n    Examples\n    --------\n    >>> P_upstream_isothermal_critical_flow(P=389699.7317645518, fd=0.00185,\n    ... L=1000., D=0.5)\n    1000000.0000000001\n    '''\n    lambertw_term = float(lambertw(-exp(-(fd*L+D)/D), -1).real)\n    return exp(-0.5*(D*lambertw_term+fd*L+D)/D)*P", "response": "Not part of the public API. Reverses P_isothermal_critical_flow."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef P_stagnation(P, T, Tst, k):\n    r'''Calculates stagnation flow pressure `Pst` for a fluid with the\n    given isentropic coefficient and specified stagnation temperature and\n    normal temperature. Normally used with converging/diverging nozzles.\n\n    .. math::\n        \\frac{P_0}{P}=\\left(\\frac{T_0}{T}\\right)^{\\frac{k}{k-1}}\n\n    Parameters\n    ----------\n    P : float\n        Normal pressure of a fluid [Pa]\n    T : float\n        Normal temperature of a fluid [K]\n    Tst : float\n        Stagnation temperature of a fluid moving at a certain velocity [K]\n    k : float\n        Isentropic coefficient []\n\n    Returns\n    -------\n    Pst : float\n        Stagnation pressure of a fluid moving at a certain velocity [Pa]\n\n    Notes\n    -----\n    Assumes isentropic flow.\n\n    Examples\n    --------\n    Example 12-1 in [1]_.\n\n    >>> P_stagnation(54050., 255.7, 286.8, 1.4)\n    80772.80495900588\n\n    References\n    ----------\n    .. [1] Cengel, Yunus, and John Cimbala. Fluid Mechanics: Fundamentals and\n       Applications. Boston: McGraw Hill Higher Education, 2006.\n    '''\n    return P*(Tst/T)**(k/(k - 1.0))", "response": "r Calculates the stagnation flow pressure P for a fluid with the given isentropic coefficient T and Tst. Normally used with converging and diverging nozzles."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef T_stagnation(T, P, Pst, k):\n    r'''Calculates stagnation flow temperature `Tst` for a fluid with the\n    given isentropic coefficient and specified stagnation pressure and\n    normal pressure. Normally used with converging/diverging nozzles.\n\n    .. math::\n        T=T_0\\left(\\frac{P}{P_0}\\right)^{\\frac{k-1}{k}}\n\n    Parameters\n    ----------\n    T : float\n        Normal temperature of a fluid [K]\n    P : float\n        Normal pressure of a fluid [Pa]\n    Pst : float\n        Stagnation pressure of a fluid moving at a certain velocity [Pa]\n    k : float\n        Isentropic coefficient []\n\n    Returns\n    -------\n    Tst : float\n        Stagnation temperature of a fluid moving at a certain velocity [K]\n\n    Notes\n    -----\n    Assumes isentropic flow.\n\n    Examples\n    --------\n    Example 12-1 in [1]_.\n\n    >>> T_stagnation(286.8, 54050, 54050*8, 1.4)\n    519.5230938217768\n\n    References\n    ----------\n    .. [1] Cengel, Yunus, and John Cimbala. Fluid Mechanics: Fundamentals and\n       Applications. Boston: McGraw Hill Higher Education, 2006.\n    '''\n    return T*(Pst/P)**((k - 1.0)/k)", "response": "r Returns the stagnation flow temperature T for a fluid with the given isentropic coefficient P Pst and k."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef Oliphant(SG, Tavg, L=None, D=None, P1=None, P2=None, Q=None, Ts=288.7,\n             Ps=101325., Zavg=1, E=0.92):\n    r'''Calculation function for dealing with flow of a compressible gas in a\n    pipeline with the Oliphant formula. Can calculate any of the following,\n    given all other inputs:\n\n    * Flow rate\n    * Upstream pressure\n    * Downstream pressure\n    * Diameter of pipe (numerical solution)\n    * Length of pipe\n\n    This model is a more complete conversion to metric of the Imperial version\n    presented in [1]_.\n\n    .. math::\n        Q = 84.5872\\left(D^{2.5} + 0.20915D^3\\right)\\frac{T_s}{P_s}\\left(\\frac\n        {P_1^2 - P_2^2}{L\\cdot {SG} \\cdot T_{avg}}\\right)^{0.5}\n\n    Parameters\n    ----------\n    SG : float\n        Specific gravity of fluid with respect to air at the reference\n        temperature and pressure `Ts` and `Ps`, [-]\n    Tavg : float\n        Average temperature of the fluid in the pipeline, [K]\n    L : float, optional\n        Length of pipe, [m]\n    D : float, optional\n        Diameter of pipe, [m]\n    P1 : float, optional\n        Inlet pressure to pipe, [Pa]\n    P2 : float, optional\n        Outlet pressure from pipe, [Pa]\n    Q : float, optional\n        Flow rate of gas through pipe, [m^3/s]\n    Ts : float, optional\n        Reference temperature for the specific gravity of the gas, [K]\n    Ps : float, optional\n        Reference pressure for the specific gravity of the gas, [Pa]\n    Zavg : float, optional\n        Average compressibility factor for gas, [-]\n    E : float, optional\n        Pipeline efficiency, a correction factor between 0 and 1\n\n    Returns\n    -------\n    Q, P1, P2, D, or L : float\n        The missing input which was solved for [base SI]\n\n    Notes\n    -----\n    Recommended in [1]_ for use between vacuum and 100 psi.\n\n    The model is simplified by grouping constants here; however, it is presented\n    in the imperial unit set inches (diameter), miles (length), psi, Rankine,\n    and MMSCFD in [1]_:\n\n    .. math::\n        Q = 42(24)\\left(D^{2.5} + \\frac{D^3}{30}\\right)\\left(\\frac{14.4}{P_s}\n        \\right)\\left(\\frac{T_s}{520}\\right)\\left[\\left(\\frac{0.6}{SG}\\right)\n        \\left(\\frac{520}{T_{avg}}\\right)\\left(\\frac{P_1^2 - P_2^2}{L}\\right)\n        \\right]^{0.5}\n\n    Examples\n    --------\n    >>> Oliphant(D=0.340, P1=90E5, P2=20E5, L=160E3, SG=0.693, Tavg=277.15)\n    28.851535408143057\n\n    References\n    ----------\n    .. [1] GPSA. GPSA Engineering Data Book. 13th edition. Gas Processors\n       Suppliers Association, Tulsa, OK, 2012.\n    .. [2] F. N. Oliphant, \"Production of Natural Gas,\" Report. USGS, 1902.\n    '''\n    # c1 = 42*24*Q*foot**3/day*(mile)**0.5*9/5.*(5/9.)**0.5*psi*(1/psi)*14.4/520.*0.6**0.5*520**0.5/inch**2.5\n    c1 = 84.587176139918568651410168968141078948974609375000\n    c2 = 0.2091519350460528670065940559652517549694 # 1/(30.*0.0254**0.5)\n    if Q is None and (None not in [L, D, P1, P2]):\n        return c1*(D**2.5 + c2*D**3)*Ts/Ps*((P1**2-P2**2)/(L*SG*Tavg))**0.5\n    elif D is None and (None not in [L, Q, P1, P2]):\n        to_solve = lambda D : Q - Oliphant(SG=SG, Tavg=Tavg, L=L, D=D, P1=P1, P2=P2, Ts=Ts, Ps=Ps, Zavg=Zavg, E=E)\n        return newton(to_solve, 0.5)\n    elif P1 is None and (None not in [L, Q, D, P2]):\n        return (L*Ps**2*Q**2*SG*Tavg/(Ts**2*c1**2*(D**3*c2 + D**2.5)**2) + P2**2)**0.5\n    elif P2 is None and (None not in [L, Q, D, P1]):\n        return (-L*Ps**2*Q**2*SG*Tavg/(Ts**2*c1**2*(D**3*c2 + D**2.5)**2) + P1**2)**0.5\n    elif L is None and (None not in [P2, Q, D, P1]):\n        return Ts**2*c1**2*(P1**2 - P2**2)*(D**3*c2 + D**2.5)**2/(Ps**2*Q**2*SG*Tavg)\n    else:\n        raise Exception('This function solves for either flow, upstream \\\npressure, downstream pressure, diameter, or length; all other inputs \\\nmust be provided.')", "response": "r This function calculates the flow of a compressible gas in a base - level a\n   ."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef Fritzsche(SG, Tavg, L=None, D=None, P1=None, P2=None, Q=None, Ts=288.7, \n              Ps=101325., Zavg=1, E=1):\n    r'''Calculation function for dealing with flow of a compressible gas in a\n    pipeline with the Fritzsche formula. Can calculate any of the following,\n    given all other inputs:\n\n    * Flow rate\n    * Upstream pressure\n    * Downstream pressure\n    * Diameter of pipe\n    * Length of pipe\n\n    A variety of different constants and expressions have been presented\n    for the Fritzsche formula. Here, the form as in [1]_\n    is used but with all inputs in base SI units.\n\n    .. math::\n        Q = 93.500 \\frac{T_s}{P_s}\\left(\\frac{P_1^2 - P_2^2}\n        {L\\cdot {SG}^{0.8587} \\cdot T_{avg}}\\right)^{0.538}D^{2.69}\n\n    Parameters\n    ----------\n    SG : float\n        Specific gravity of fluid with respect to air at the reference\n        temperature and pressure `Ts` and `Ps`, [-]\n    Tavg : float\n        Average temperature of the fluid in the pipeline, [K]\n    L : float, optional\n        Length of pipe, [m]\n    D : float, optional\n        Diameter of pipe, [m]\n    P1 : float, optional\n        Inlet pressure to pipe, [Pa]\n    P2 : float, optional\n        Outlet pressure from pipe, [Pa]\n    Q : float, optional\n        Flow rate of gas through pipe, [m^3/s]\n    Ts : float, optional\n        Reference temperature for the specific gravity of the gas, [K]\n    Ps : float, optional\n        Reference pressure for the specific gravity of the gas, [Pa]\n    Zavg : float, optional\n        Average compressibility factor for gas, [-]\n    E : float, optional\n        Pipeline efficiency, a correction factor between 0 and 1\n\n    Returns\n    -------\n    Q, P1, P2, D, or L : float\n        The missing input which was solved for [base SI]\n\n    Notes\n    -----\n    This model is also presented in [1]_ with a leading constant of 2.827,\n    the same exponents as used here, units of mm  (diameter), kPa, km (length),\n    and flow in m^3/hour.\n\n    This model is shown in base SI units in [2]_, and with a leading constant\n    of 94.2565, a diameter power of 2.6911, main group power of 0.5382\n    and a specific gravity power of 0.858. The difference is very small.\n\n    Examples\n    --------\n    >>> Fritzsche(D=0.340, P1=90E5, P2=20E5, L=160E3, SG=0.693, Tavg=277.15)\n    39.421535157535565\n\n    References\n    ----------\n    .. [1] Menon, E. Shashi. Gas Pipeline Hydraulics. 1st edition. Boca Raton,\n       FL: CRC Press, 2005.\n    .. [2] Coelho, Paulo M., and Carlos Pinho. \"Considerations about Equations\n       for Steady State Flow in Natural Gas Pipelines.\" Journal of the\n       Brazilian Society of Mechanical Sciences and Engineering 29, no. 3\n       (September 2007): 262-73. doi:10.1590/S1678-58782007000300005.\n    '''\n    # Rational('2.827E-3')/(3600*24)*(1000)**Rational('2.69')*(1000)**Rational('0.538')*1000/(1000**2)**Rational('0.538')\n    c5 = 93.50009798751128188757518688244137811221 # 14135*10**(57/125)/432\n    c2 = 0.8587\n    c3 = 0.538\n    c4 = 2.69\n    if Q is None and (None not in [L, D, P1, P2]):\n        return c5*E*(Ts/Ps)*((P1**2 - P2**2)/(SG**c2*Tavg*L*Zavg))**c3*D**c4\n    elif D is None and (None not in [L, Q, P1, P2]):\n        return (Ps*Q*(SG**(-c2)*(P1**2 - P2**2)/(L*Tavg*Zavg))**(-c3)/(E*Ts*c5))**(1./c4)\n    elif P1 is None and (None not in [L, Q, D, P2]):\n        return (L*SG**c2*Tavg*Zavg*(D**(-c4)*Ps*Q/(E*Ts*c5))**(1./c3) + P2**2)**0.5\n    elif P2 is None and (None not in [L, Q, D, P1]):\n        return (-L*SG**c2*Tavg*Zavg*(D**(-c4)*Ps*Q/(E*Ts*c5))**(1./c3) + P1**2)**0.5\n    elif L is None and (None not in [P2, Q, D, P1]):\n        return SG**(-c2)*(D**(-c4)*Ps*Q/(E*Ts*c5))**(-1./c3)*(P1**2 - P2**2)/(Tavg*Zavg)\n    else:\n        raise Exception('This function solves for either flow, upstream pressure, downstream pressure, diameter, or length; all other inputs must be provided.')", "response": "r This function calculates the flow of a compressible gas in a base SI unit with the Fritzsche formula."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef Kp_helical_ribbon_Rieger(D, h, nb, pitch, width, T):\n    r'''Calculates product of power number and Reynolds number for a\n    specified geometry for a heilical ribbon mixer in the laminar regime.\n    One of several correlations listed in [1]_, it used more data than other\n    listed correlations and was recommended.\n\n    .. math::\n        K_p = 82.8\\frac{h}{D}\\left(\\frac{c}{D}\\right)^{-0.38} \\left(\\frac{p}{D}\\right)^{-0.35}\n        \\left(\\frac{w}{D}\\right)^{0.20} n_b^{0.78}\n\n    Parameters\n    ----------\n    D : float\n        Impeller diameter [m]\n    h : float\n        Ribbon mixer height, [m]\n    nb : float:\n        Number of blades, [-]\n    pitch : float\n         Height of one turn around a helix [m]\n    width : float\n         Width of one blade [m]\n    T : float\n        Tank diameter, [m]\n\n    Returns\n    -------\n    Kp : float\n        Product of Power number and Reynolds number for laminar regime []\n\n    Notes\n    -----\n    Example is from example 9-6 in [1]_. Confirmed.\n\n    Examples\n    --------\n    >>> Kp_helical_ribbon_Rieger(D=1.9, h=1.9, nb=2, pitch=1.9, width=.19, T=2)\n    357.39749163259256\n\n    References\n    ----------\n    .. [1] Paul, Edward L, Victor A Atiemo-Obeng, and Suzanne M Kresta.\n       Handbook of Industrial Mixing: Science and Practice.\n       Hoboken, N.J.: Wiley-Interscience, 2004.\n    .. [2] Rieger, F., V. Novak, and D. Havelkov (1988). The influence of the\n       geometrical shape on the power requirements of ribbon impellers,\n       Int. Chem. Eng., 28, 376-383.\n    '''\n    c = 0.5*(T - D)\n    return 82.8*h/D*(c/D)**-.38*(pitch/D)**-0.35*(width/D)**0.2*nb**0.78", "response": "r Calculates the product of power number and Reynolds number for a heilical ribbon mixer."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef size_tee(Q1, Q2, D, D2, n=1, pipe_diameters=5):\n    r'''Calculates CoV of an optimal or specified tee for mixing at a tee\n    according to [1]_. Assumes turbulent flow.\n    The smaller stream in injected into the main pipe, which continues\n    straight.\n    COV calculation is according to [2]_.\n\n    .. math::\n        TODO\n\n    Parameters\n    ----------\n    Q1 : float\n        Volumetric flow rate of larger stream [m^3/s]\n    Q2 : float\n        Volumetric flow rate of smaller stream [m^3/s]\n    D : float\n        Diameter of pipe after tee [m]\n    D2 : float\n        Diameter of mixing inlet, optional (optimally calculated if not\n        specified) [m]\n    n : float\n        Number of jets, 1 to 4 []\n    pipe_diameters : float\n        Number of diameters along tail pipe for CoV calculation, 0 to 5 []\n\n    Returns\n    -------\n    CoV : float\n        Standard deviation of dimensionless concentration [-]\n\n    Notes\n    -----\n    Not specified if this works for liquid also, though probably not.\n    Example is from example Example 9-6 in [1]_. Low precision used in example.\n\n    Examples\n    --------\n    >>> size_tee(Q1=11.7, Q2=2.74, D=0.762, D2=None, n=1, pipe_diameters=5)\n    0.2940930233038544\n\n    References\n    ----------\n    .. [1] Paul, Edward L, Victor A Atiemo-Obeng, and Suzanne M Kresta.\n       Handbook of Industrial Mixing: Science and Practice.\n       Hoboken, N.J.: Wiley-Interscience, 2004.\n    .. [2] Giorges, Aklilu T. G., Larry J. Forney, and Xiaodong Wang.\n       \"Numerical Study of Multi-Jet Mixing.\" Chemical Engineering Research and\n       Design, Fluid Flow, 79, no. 5 (July 2001): 515-22.\n       doi:10.1205/02638760152424280.\n    '''\n    V1 = Q1/(pi/4*D**2)\n    Cv = Q2/(Q1 + Q2)\n    COV0 = ((1-Cv)/Cv)**0.5\n    if not D2:\n        D2 = (Q2/Q1)**(2/3.)*D\n    V2 = Q2/(pi/4*D2**2)\n    B = n**2*(D2/D)**2*(V2/V1)**2\n    if not n == 1 and not n == 2 and not n == 3 and not n ==4:\n        raise Exception('Only 1 or 4 side streams investigated')\n    if n == 1:\n        if B < 0.7:\n            E = 1.33\n        else:\n            E = 1/33. + 0.95*log(B/0.7)\n    elif n == 2:\n        if B < 0.8:\n            E = 1.44\n        else:\n            E = 1.44 + 0.95*log(B/0.8)**1.5\n    elif n == 3:\n        if B < 0.8:\n            E = 1.75\n        else:\n            E = 1.75 + 0.95*log(B/0.8)**1.8\n    else:\n        if B < 2:\n            E = 1.97\n        else:\n            E = 1.97 + 0.95*log(B/2.)**2\n    COV = (0.32/B**0.86*(pipe_diameters)**-E )**0.5\n    return COV", "response": "r Returns a new tee that is the size of the tee at a tee."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef COV_motionless_mixer(Ki, Q1, Q2, pipe_diameters):\n    r'''Calculates CoV of a motionless mixer with a regression parameter in\n    [1]_ and originally in [2]_.\n\n    .. math::\n        \\frac{CoV}{CoV_0} = K_i^{L/D}\n\n    Parameters\n    ----------\n    Ki : float\n        Correlation parameter specific to a mixer's design, [-]\n    Q1 : float\n        Volumetric flow rate of larger stream [m^3/s]\n    Q2 : float\n        Volumetric flow rate of smaller stream [m^3/s]\n    pipe_diameters : float\n        Number of diameters along tail pipe for CoV calculation, 0 to 5 []\n\n    Returns\n    -------\n    CoV : float\n        Standard deviation of dimensionless concentration [-]\n\n    Notes\n    -----\n    Example 7-8.3.2 in [1]_, solved backwards.\n\n    Examples\n    --------\n    >>> COV_motionless_mixer(Ki=.33, Q1=11.7, Q2=2.74, pipe_diameters=4.74/.762)\n    0.0020900028665727685\n\n    References\n    ----------\n    .. [1] Paul, Edward L, Victor A Atiemo-Obeng, and Suzanne M Kresta.\n       Handbook of Industrial Mixing: Science and Practice.\n       Hoboken, N.J.: Wiley-Interscience, 2004.\n    .. [2] Streiff, F. A., S. Jaffer, and G. Schneider (1999). Design and\n       application of motionless mixer technology, Proc. ISMIP3, Osaka,\n       pp. 107-114.\n    '''\n    Cv = Q2/(Q1 + Q2)\n    COV0 = ((1-Cv)/Cv)**0.5\n    COVr = Ki**(pipe_diameters)\n    COV = COV0*COVr\n    return COV", "response": "rCalculates the cov of a motionless mixer with a regression parameter in Ki and originally in K2."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncalculate the Darcy friction factor for a given Reynolds number and relative roughness.", "response": "def Clamond(Re, eD, fast=False):\n    r'''Calculates Darcy friction factor using a solution accurate to almost\n    machine precision. Recommended very strongly. For details of the algorithm,\n    see [1]_. \n    \n    Parameters\n    ----------\n    Re : float\n        Reynolds number, [-]\n    eD : float\n        Relative roughness, [-]\n    fast : bool, optional\n        If true, performs only one iteration, which gives roughly half the\n        number of decimals of accuracy, [-]\n\n    Returns\n    -------\n    fd : float\n        Darcy friction factor [-]\n\n    Notes\n    -----\n    This is a highly optimized function, 4 times faster than the solution using\n    the LambertW function, and faster than many other approximations which are \n    much less accurate.\n\n    The code used here is only slightly modified than that in [1]_, for further\n    performance improvements. \n    \n    For 10 < Re < 1E12, and 0 < eD < 0.01, this equation has been confirmed\n    numerically to provide a solution to the Colebrook equation accurate to an \n    rtol of 1E-9 or better - the same level of accuracy as the analytical \n    solution to the Colebrook equation due to floating point precision.\n    \n    Comparing this to the numerical solution of the Colebrook equation,\n    identical values are given accurate to an rtol of 1E-9 for 10 < Re < 1E100, \n    and 0 < eD < 1 and beyond.\n    \n    However, for values of Re under 10, different answers from the `Colebrook`\n    equation appear and then quickly a ValueError is raised. \n    \n    Examples\n    --------\n    >>> Clamond(1E5, 1E-4)\n    0.01851386607747165\n\n    References\n    ----------\n    .. [1] Clamond, Didier. \"Efficient Resolution of the Colebrook Equation.\" \n       Industrial & Engineering Chemistry Research 48, no. 7 (April 1, 2009): \n       3665-71. doi:10.1021/ie801626g.  \n       http://math.unice.fr/%7Edidierc/DidPublis/ICR_2009.pdf\n    '''\n    X1 = eD*Re*0.1239681863354175460160858261654858382699 # (log(10)/18.574).evalf(40)\n    X2 = log(Re) - 0.7793974884556819406441139701653776731705 # log(log(10)/5.02).evalf(40)\n    F = X2 - 0.2\n    X1F = X1 + F\n    X1F1 = 1. + X1F\n    \n    E = (log(X1F) - 0.2)/(X1F1)\n    F = F - (X1F1 + 0.5*E)*E*(X1F)/(X1F1 + E*(1. + E*0.3333333333333333))\n    \n    if not fast:\n        X1F = X1 + F\n        X1F1 = 1. + X1F\n        E = (log(X1F) + F - X2)/(X1F1)\n        F = F - (X1F1 + 0.5*E)*E*(X1F)/(X1F1 + E*(1. + E*0.3333333333333333))\n\n    return 1.325474527619599502640416597148504422899/(F*F)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef Wood_1966(Re, eD):\n    r'''Calculates Darcy friction factor using the method in Wood (1966) [2]_\n    as shown in [1]_.\n\n    .. math::\n        f_d = 0.094(\\frac{\\epsilon}{D})^{0.225} + 0.53(\\frac{\\epsilon}{D})\n        + 88(\\frac{\\epsilon}{D})^{0.4}Re^{-A_1}\n\n    .. math::\n        A_1 = 1.62(\\frac{\\epsilon}{D})^{0.134}\n\n    Parameters\n    ----------\n    Re : float\n        Reynolds number, [-]\n    eD : float\n        Relative roughness, [-]\n\n    Returns\n    -------\n    fd : float\n        Darcy friction factor [-]\n\n    Notes\n    -----\n    Range is 4E3 <= Re <= 5E7;  1E-5 <= eD <= 4E-2.\n\n    Examples\n    --------\n    >>> Wood_1966(1E5, 1E-4)\n    0.021587570560090762\n\n    References\n    ----------\n    .. [1] Winning, H. and T. Coole. \"Explicit Friction Factor Accuracy and\n       Computational Efficiency for Turbulent Flow in Pipes.\" Flow, Turbulence\n       and Combustion 90, no. 1 (January 1, 2013): 1-27.\n       doi:10.1007/s10494-012-9419-7\n    .. [2] \tWood, D.J.: An Explicit Friction Factor Relationship, vol. 60.\n       Civil Engineering American Society of Civil Engineers (1966)\n    '''\n    A1 = 1.62*eD**0.134\n    return 0.094*eD**0.225 + 0.53*eD +88*eD**0.4*Re**-A1", "response": "Calculates the Darcy friction factor using the method in Wood ( 1966 )."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef Churchill_1977(Re, eD):\n    r'''Calculates Darcy friction factor using the method in Churchill and\n    (1977) [2]_ as shown in [1]_.\n\n    .. math::\n        f_f = 2\\left[(\\frac{8}{Re})^{12} + (A_2 + A_3)^{-1.5}\\right]^{1/12}\n\n    .. math::\n        A_2 = \\left\\{2.457\\ln\\left[(\\frac{7}{Re})^{0.9}\n        + 0.27\\frac{\\epsilon}{D}\\right]\\right\\}^{16}\n\n    .. math::\n        A_3 = \\left( \\frac{37530}{Re}\\right)^{16}\n\n    Parameters\n    ----------\n    Re : float\n        Reynolds number, [-]\n    eD : float\n        Relative roughness, [-]\n\n    Returns\n    -------\n    fd : float\n        Darcy friction factor [-]\n\n    Notes\n    -----\n    No range of validity specified for this equation.\n\n    Examples\n    --------\n    >>> Churchill_1977(1E5, 1E-4)\n    0.018462624566280075\n\n    References\n    ----------\n    .. [1] Winning, H. and T. Coole. \"Explicit Friction Factor Accuracy and\n       Computational Efficiency for Turbulent Flow in Pipes.\" Flow, Turbulence\n       and Combustion 90, no. 1 (January 1, 2013): 1-27.\n       doi:10.1007/s10494-012-9419-7\n    .. [2]\tChurchill, S.W.: Friction factor equation spans all fluid flow\n       regimes. Chem. Eng. J. 91, 91-92 (1977)\n    '''\n    A3 = (37530/Re)**16\n    A2 = (2.457*log((7./Re)**0.9 + 0.27*eD))**16\n    ff = 2*((8/Re)**12 + 1/(A2+A3)**1.5)**(1/12.)\n    return 4*ff", "response": "Calculates the Darcy friction factor using the method in Churchill and\n    ( 1977 )."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef Chen_1979(Re, eD):\n    r'''Calculates Darcy friction factor using the method in Chen (1979) [2]_\n    as shown in [1]_.\n\n    .. math::\n        \\frac{1}{\\sqrt{f_f}} = -4\\log\\left[\\frac{\\epsilon}{3.7065D}\n        -\\frac{5.0452}{Re}\\log A_4\\right]\n\n    .. math::\n        A_4 = \\frac{(\\epsilon/D)^{1.1098}}{2.8257}\n        + \\left(\\frac{7.149}{Re}\\right)^{0.8981}\n\n    Parameters\n    ----------\n    Re : float\n        Reynolds number, [-]\n    eD : float\n        Relative roughness, [-]\n\n    Returns\n    -------\n    fd : float\n        Darcy friction factor [-]\n\n    Notes\n    -----\n    Range is 4E3 <= Re <= 4E8;  1E-7 <= eD <= 5E-2.\n\n    Examples\n    --------\n    >>> Chen_1979(1E5, 1E-4)\n    0.018552817507472126\n\n    References\n    ----------\n    .. [1] Winning, H. and T. Coole. \"Explicit Friction Factor Accuracy and\n       Computational Efficiency for Turbulent Flow in Pipes.\" Flow, Turbulence\n       and Combustion 90, no. 1 (January 1, 2013): 1-27.\n       doi:10.1007/s10494-012-9419-7\n    .. [2] \tChen, Ning Hsing. \"An Explicit Equation for Friction Factor in\n       Pipe.\" Industrial & Engineering Chemistry Fundamentals 18, no. 3\n       (August 1, 1979): 296-97. doi:10.1021/i160071a019.\n    '''\n    A4 = eD**1.1098/2.8257 + (7.149/Re)**0.8981\n    ff = (-4*log10(eD/3.7065 - 5.0452/Re*log10(A4)))**-2\n    return 4*ff", "response": "Calculates the Darcy friction factor using the method in Chen 1979."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef Shacham_1980(Re, eD):\n    r'''Calculates Darcy friction factor using the method in Shacham (1980) [2]_\n    as shown in [1]_.\n\n    .. math::\n        \\frac{1}{\\sqrt{f_f}} = -4\\log\\left[\\frac{\\epsilon}{3.7D} -\n        \\frac{5.02}{Re} \\log\\left(\\frac{\\epsilon}{3.7D}\n        + \\frac{14.5}{Re}\\right)\\right]\n\n    Parameters\n    ----------\n    Re : float\n        Reynolds number, [-]\n    eD : float\n        Relative roughness, [-]\n\n    Returns\n    -------\n    fd : float\n        Darcy friction factor [-]\n\n    Notes\n    -----\n    Range is 4E3 <= Re <= 4E8\n\n    Examples\n    --------\n    >>> Shacham_1980(1E5, 1E-4)\n    0.01860641215097828\n\n    References\n    ----------\n    .. [1] Winning, H. and T. Coole. \"Explicit Friction Factor Accuracy and\n       Computational Efficiency for Turbulent Flow in Pipes.\" Flow, Turbulence\n       and Combustion 90, no. 1 (January 1, 2013): 1-27.\n       doi:10.1007/s10494-012-9419-7\n    .. [2] Shacham, M. \"Comments on: 'An Explicit Equation for Friction\n       Factor in Pipe.'\" Industrial & Engineering Chemistry Fundamentals 19,\n       no. 2 (May 1, 1980): 228-228. doi:10.1021/i160074a019.\n    '''\n    ff = (-4*log10(eD/3.7 - 5.02/Re*log10(eD/3.7 + 14.5/Re)))**-2\n    return 4*ff", "response": "Calculates the Darcy friction factor using the method in Shacham 1980."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef Zigrang_Sylvester_1(Re, eD):\n    r'''Calculates Darcy friction factor using the method in\n     Zigrang and Sylvester (1982) [2]_ as shown in [1]_.\n\n    .. math::\n        \\frac{1}{\\sqrt{f_f}} = -4\\log\\left[\\frac{\\epsilon}{3.7D}\n        - \\frac{5.02}{Re}\\log A_5\\right]\n\n        A_5 = \\frac{\\epsilon}{3.7D} + \\frac{13}{Re}\n\n    Parameters\n    ----------\n    Re : float\n        Reynolds number, [-]\n    eD : float\n        Relative roughness, [-]\n\n    Returns\n    -------\n    fd : float\n        Darcy friction factor [-]\n\n    Notes\n    -----\n    Range is 4E3 <= Re <= 1E8;  4E-5 <= eD <= 5E-2.\n\n    Examples\n    --------\n    >>> Zigrang_Sylvester_1(1E5, 1E-4)\n    0.018646892425980794\n\n    References\n    ----------\n    .. [1] Winning, H. and T. Coole. \"Explicit Friction Factor Accuracy and\n       Computational Efficiency for Turbulent Flow in Pipes.\" Flow, Turbulence\n       and Combustion 90, no. 1 (January 1, 2013): 1-27.\n       doi:10.1007/s10494-012-9419-7\n    .. [2] \tZigrang, D. J., and N. D. Sylvester.\"Explicit Approximations to the\n       Solution of Colebrook's Friction Factor Equation.\" AIChE Journal 28,\n       no. 3 (May 1, 1982): 514-15. doi:10.1002/aic.690280323.\n    '''\n    A5 = eD/3.7 + 13/Re\n    ff = (-4*log10(eD/3.7 - 5.02/Re*log10(A5)))**-2\n    return 4*ff", "response": "A function that calculates the Darcy friction factor using the method in\n     Zigrang and Sylvester."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef Zigrang_Sylvester_2(Re, eD):\n    r'''Calculates Darcy friction factor using the second method in\n     Zigrang and Sylvester (1982) [2]_ as shown in [1]_.\n\n    .. math::\n        \\frac{1}{\\sqrt{f_f}} = -4\\log\\left[\\frac{\\epsilon}{3.7D}\n        - \\frac{5.02}{Re}\\log A_6\\right]\n\n    .. math::\n        A_6 = \\frac{\\epsilon}{3.7D} - \\frac{5.02}{Re}\\log A_5\n\n    .. math::\n        A_5 = \\frac{\\epsilon}{3.7D} + \\frac{13}{Re}\n\n    Parameters\n    ----------\n    Re : float\n        Reynolds number, [-]\n    eD : float\n        Relative roughness, [-]\n\n    Returns\n    -------\n    fd : float\n        Darcy friction factor [-]\n\n    Notes\n    -----\n    Range is 4E3 <= Re <= 1E8;  4E-5 <= eD <= 5E-2\n\n    Examples\n    --------\n    >>> Zigrang_Sylvester_2(1E5, 1E-4)\n    0.01850021312358548\n\n    References\n    ----------\n    .. [1] Winning, H. and T. Coole. \"Explicit Friction Factor Accuracy and\n       Computational Efficiency for Turbulent Flow in Pipes.\" Flow, Turbulence\n       and Combustion 90, no. 1 (January 1, 2013): 1-27.\n       doi:10.1007/s10494-012-9419-7\n    .. [2] \tZigrang, D. J., and N. D. Sylvester.\"Explicit Approximations to the\n       Solution of Colebrook's Friction Factor Equation.\" AIChE Journal 28,\n       no. 3 (May 1, 1982): 514-15. doi:10.1002/aic.690280323.\n    '''\n    A5 = eD/3.7 + 13/Re\n    A6 = eD/3.7 - 5.02/Re*log10(A5)\n    ff = (-4*log10(eD/3.7 - 5.02/Re*log10(A6)))**-2\n    return 4*ff", "response": "A second method in\n     Zigrang and Sylvester."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef Serghides_1(Re, eD):\n    r'''Calculates Darcy friction factor using the method in Serghides (1984)\n    [2]_ as shown in [1]_.\n\n    .. math::\n        f=\\left[A-\\frac{(B-A)^2}{C-2B+A}\\right]^{-2}\n\n    .. math::\n        A=-2\\log_{10}\\left[\\frac{\\epsilon/D}{3.7}+\\frac{12}{Re}\\right]\n\n    .. math::\n        B=-2\\log_{10}\\left[\\frac{\\epsilon/D}{3.7}+\\frac{2.51A}{Re}\\right]\n\n    .. math::\n        C=-2\\log_{10}\\left[\\frac{\\epsilon/D}{3.7}+\\frac{2.51B}{Re}\\right]\n\n    Parameters\n    ----------\n    Re : float\n        Reynolds number, [-]\n    eD : float\n        Relative roughness, [-]\n\n    Returns\n    -------\n    fd : float\n        Darcy friction factor [-]\n\n    Notes\n    -----\n    No range of validity specified for this equation.\n\n    Examples\n    --------\n    >>> Serghides_1(1E5, 1E-4)\n    0.01851358983180063\n\n    References\n    ----------\n    .. [1] Winning, H. and T. Coole. \"Explicit Friction Factor Accuracy and\n       Computational Efficiency for Turbulent Flow in Pipes.\" Flow, Turbulence\n       and Combustion 90, no. 1 (January 1, 2013): 1-27.\n       doi:10.1007/s10494-012-9419-7\n    .. [2] Serghides T.K (1984).\"Estimate friction factor accurately\"\n       Chemical Engineering, Vol. 91(5), pp. 63-64.\n    '''\n    A = -2*log10(eD/3.7 + 12/Re)\n    B = -2*log10(eD/3.7 + 2.51*A/Re)\n    C = -2*log10(eD/3.7 + 2.51*B/Re)\n    return (A - (B-A)**2/(C-2*B + A))**-2", "response": "Return a base of Serghides for a given Reynolds number and relative roughness."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a base of Serghides for a given Reynolds number and relative roughness.", "response": "def Serghides_2(Re, eD):\n    r'''Calculates Darcy friction factor using the method in Serghides (1984)\n    [2]_ as shown in [1]_.\n\n    .. math::\n        f_d = \\left[ 4.781 - \\frac{(A - 4.781)^2}\n        {B-2A+4.781}\\right]^{-2}\n\n    .. math::\n        A=-2\\log_{10}\\left[\\frac{\\epsilon/D}{3.7}+\\frac{12}{Re}\\right]\n\n    .. math::\n        B=-2\\log_{10}\\left[\\frac{\\epsilon/D}{3.7}+\\frac{2.51A}{Re}\\right]\n\n\n    Parameters\n    ----------\n    Re : float\n        Reynolds number, [-]\n    eD : float\n        Relative roughness, [-]\n\n    Returns\n    -------\n    fd : float\n        Darcy friction factor [-]\n\n    Notes\n    -----\n    No range of validity specified for this equation.\n\n    Examples\n    --------\n    >>> Serghides_2(1E5, 1E-4)\n    0.018486377560664482\n\n    References\n    ----------\n    .. [1] Winning, H. and T. Coole. \"Explicit Friction Factor Accuracy and\n       Computational Efficiency for Turbulent Flow in Pipes.\" Flow, Turbulence\n       and Combustion 90, no. 1 (January 1, 2013): 1-27.\n       doi:10.1007/s10494-012-9419-7\n    .. [2]\tSerghides T.K (1984).\"Estimate friction factor accurately\"\n       Chemical Engineering, Vol. 91(5), pp. 63-64.\n    '''\n    A = -2*log10(eD/3.7 + 12/Re)\n    B = -2*log10(eD/3.7 + 2.51*A/Re)\n    return (4.781 - (A - 4.781)**2/(B - 2*A + 4.781))**-2"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef Romeo_2002(Re, eD):\n    r'''Calculates Darcy friction factor using the method in Romeo (2002)\n    [2]_ as shown in [1]_.\n\n    .. math::\n        \\frac{1}{\\sqrt{f_d}} = -2\\log\\left\\{\\frac{\\epsilon}{3.7065D}\\times\n        \\frac{5.0272}{Re}\\times\\log\\left[\\frac{\\epsilon}{3.827D} -\n        \\frac{4.567}{Re}\\times\\log\\left(\\frac{\\epsilon}{7.7918D}^{0.9924} +\n        \\left(\\frac{5.3326}{208.815+Re}\\right)^{0.9345}\\right)\\right]\\right\\}\n\n    Parameters\n    ----------\n    Re : float\n        Reynolds number, [-]\n    eD : float\n        Relative roughness, [-]\n\n    Returns\n    -------\n    fd : float\n        Darcy friction factor [-]\n\n    Notes\n    -----\n    Range is 3E3 <= Re <= 1.5E8;  0 <= eD <= 5E-2\n\n    Examples\n    --------\n    >>> Romeo_2002(1E5, 1E-4)\n    0.018530291219676177\n\n    References\n    ----------\n    .. [1] Winning, H. and T. Coole. \"Explicit Friction Factor Accuracy and\n       Computational Efficiency for Turbulent Flow in Pipes.\" Flow, Turbulence\n       and Combustion 90, no. 1 (January 1, 2013): 1-27.\n       doi:10.1007/s10494-012-9419-7\n    .. [2] Romeo, Eva, Carlos Royo, and Antonio Monzon.\"Improved Explicit\n       Equations for Estimation of the Friction Factor in Rough and Smooth\n       Pipes.\" Chemical Engineering Journal 86, no. 3 (April 28, 2002): 369-74.\n       doi:10.1016/S1385-8947(01)00254-6.\n    '''\n    fd = (-2*log10(eD/3.7065-5.0272/Re*log10(eD/3.827-4.567/Re*log10((eD/7.7918)**0.9924+(5.3326/(208.815+Re))**0.9345))))**-2\n    return fd", "response": "Romeo 2002 - > 2002 Friction factor calculation."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef Sonnad_Goudar_2006(Re, eD):\n    r'''Calculates Darcy friction factor using the method in Sonnad and Goudar\n    (2006) [2]_ as shown in [1]_.\n\n    .. math::\n        \\frac{1}{\\sqrt{f_d}} = 0.8686\\ln\\left(\\frac{0.4587Re}{S^{S/(S+1)}}\\right)\n\n    .. math::\n        S = 0.1240\\times\\frac{\\epsilon}{D}\\times Re + \\ln(0.4587Re)\n\n    Parameters\n    ----------\n    Re : float\n        Reynolds number, [-]\n    eD : float\n        Relative roughness, [-]\n\n    Returns\n    -------\n    fd : float\n        Darcy friction factor [-]\n\n    Notes\n    -----\n    Range is 4E3 <= Re <= 1E8;  1E-6 <= eD <= 5E-2\n\n    Examples\n    --------\n    >>> Sonnad_Goudar_2006(1E5, 1E-4)\n    0.0185971269898162\n\n    References\n    ----------\n    .. [1] Winning, H. and T. Coole. \"Explicit Friction Factor Accuracy and\n       Computational Efficiency for Turbulent Flow in Pipes.\" Flow, Turbulence\n       and Combustion 90, no. 1 (January 1, 2013): 1-27.\n       doi:10.1007/s10494-012-9419-7\n    .. [2] \tTravis, Quentin B., and Larry W. Mays.\"Relationship between\n       Hazen-William and Colebrook-White Roughness Values.\" Journal of\n       Hydraulic Engineering 133, no. 11 (November 2007): 1270-73.\n       doi:10.1061/(ASCE)0733-9429(2007)133:11(1270).\n    '''\n    S = 0.124*eD*Re + log(0.4587*Re)\n    return (.8686*log(.4587*Re/S**(S/(S+1))))**-2", "response": "Return a base class for a given base class for a given Reynolds number and relative roughness."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef Rao_Kumar_2007(Re, eD):\n    r'''Calculates Darcy friction factor using the method in Rao and Kumar\n    (2007) [2]_ as shown in [1]_.\n\n    .. math::\n        \\frac{1}{\\sqrt{f_d}} = 2\\log\\left(\\frac{(2\\frac{\\epsilon}{D})^{-1}}\n        {\\left(\\frac{0.444 + 0.135Re}{Re}\\right)\\beta}\\right)\n\n    .. math::\n        \\beta = 1 - 0.55\\exp(-0.33\\ln\\left[\\frac{Re}{6.5}\\right]^2)\n\n    Parameters\n    ----------\n    Re : float\n        Reynolds number, [-]\n    eD : float\n        Relative roughness, [-]\n\n    Returns\n    -------\n    fd : float\n        Darcy friction factor [-]\n\n    Notes\n    -----\n    No range of validity specified for this equation.\n    This equation is fit to original experimental friction factor data.\n    Accordingly, this equation should not be used unless appropriate\n    consideration is given.\n\n    Examples\n    --------\n    >>> Rao_Kumar_2007(1E5, 1E-4)\n    0.01197759334600925\n\n    References\n    ----------\n    .. [1] Winning, H. and T. Coole. \"Explicit Friction Factor Accuracy and\n       Computational Efficiency for Turbulent Flow in Pipes.\" Flow, Turbulence\n       and Combustion 90, no. 1 (January 1, 2013): 1-27.\n       doi:10.1007/s10494-012-9419-7\n    .. [2] Rao, A.R., Kumar, B.: Friction factor for turbulent pipe flow.\n       Division of Mechanical Sciences, Civil Engineering Indian Institute of\n       Science Bangalore, India ID Code 9587 (2007)\n    '''\n    beta = 1 - 0.55*exp(-0.33*(log(Re/6.5))**2)\n    return (2*log10((2*eD)**-1/beta/((0.444+0.135*Re)/Re)))**-2", "response": "Calculates the Darcy friction factor using the method in Rao and Kumar."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef Buzzelli_2008(Re, eD):\n    r'''Calculates Darcy friction factor using the method in Buzzelli (2008)\n    [2]_ as shown in [1]_.\n\n    .. math::\n        \\frac{1}{\\sqrt{f_d}} = B_1 - \\left[\\frac{B_1 +2\\log(\\frac{B_2}{Re})}\n        {1 + \\frac{2.18}{B_2}}\\right]\n\n    .. math::\n        B_1 = \\frac{0.774\\ln(Re)-1.41}{1+1.32\\sqrt{\\frac{\\epsilon}{D}}}\n\n    .. math::\n        B_2 = \\frac{\\epsilon}{3.7D}Re+2.51\\times B_1\n\n    Parameters\n    ----------\n    Re : float\n        Reynolds number, [-]\n    eD : float\n        Relative roughness, [-]\n\n    Returns\n    -------\n    fd : float\n        Darcy friction factor [-]\n\n    Notes\n    -----\n    No range of validity specified for this equation.\n\n    Examples\n    --------\n    >>> Buzzelli_2008(1E5, 1E-4)\n    0.018513948401365277\n\n    References\n    ----------\n    .. [1] Winning, H. and T. Coole. \"Explicit Friction Factor Accuracy and\n       Computational Efficiency for Turbulent Flow in Pipes.\" Flow, Turbulence\n       and Combustion 90, no. 1 (January 1, 2013): 1-27.\n       doi:10.1007/s10494-012-9419-7\n    .. [2] \tBuzzelli, D.: Calculating friction in one step.\n       Mach. Des. 80, 54-55 (2008)\n    '''\n    B1 = (.774*log(Re)-1.41)/(1+1.32*eD**0.5)\n    B2 = eD/3.7*Re + 2.51*B1\n    return (B1- (B1+2*log10(B2/Re))/(1+2.18/B2))**-2", "response": "A function that calculates the Darcy friction factor using the method in Buzzelli 2008."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates the Darcy friction factor using the method in Brkic2011_1.", "response": "def Brkic_2011_1(Re, eD):\n    r'''Calculates Darcy friction factor using the method in Brkic\n    (2011) [2]_ as shown in [1]_.\n\n    .. math::\n        f_d = [-2\\log(10^{-0.4343\\beta} + \\frac{\\epsilon}{3.71D})]^{-2}\n\n    .. math::\n        \\beta = \\ln \\frac{Re}{1.816\\ln\\left(\\frac{1.1Re}{\\ln(1+1.1Re)}\\right)}\n\n    Parameters\n    ----------\n    Re : float\n        Reynolds number, [-]\n    eD : float\n        Relative roughness, [-]\n\n    Returns\n    -------\n    fd : float\n        Darcy friction factor [-]\n\n    Notes\n    -----\n    No range of validity specified for this equation.\n\n    Examples\n    --------\n    >>> Brkic_2011_1(1E5, 1E-4)\n    0.01812455874141297\n\n    References\n    ----------\n    .. [1] Winning, H. and T. Coole. \"Explicit Friction Factor Accuracy and\n       Computational Efficiency for Turbulent Flow in Pipes.\" Flow, Turbulence\n       and Combustion 90, no. 1 (January 1, 2013): 1-27.\n       doi:10.1007/s10494-012-9419-7\n    .. [2] \tBrkic, Dejan.\"Review of Explicit Approximations to the Colebrook\n       Relation for Flow Friction.\" Journal of Petroleum Science and\n       Engineering 77, no. 1 (April 2011): 34-48.\n       doi:10.1016/j.petrol.2011.02.006.\n    '''\n    beta = log(Re/(1.816*log(1.1*Re/log(1+1.1*Re))))\n    return (-2*log10(10**(-0.4343*beta)+eD/3.71))**-2"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef Brkic_2011_2(Re, eD):\n    r'''Calculates Darcy friction factor using the method in Brkic\n    (2011) [2]_ as shown in [1]_.\n\n    .. math::\n        f_d = [-2\\log(\\frac{2.18\\beta}{Re}+ \\frac{\\epsilon}{3.71D})]^{-2}\n\n    .. math::\n        \\beta = \\ln \\frac{Re}{1.816\\ln\\left(\\frac{1.1Re}{\\ln(1+1.1Re)}\\right)}\n\n    Parameters\n    ----------\n    Re : float\n        Reynolds number, [-]\n    eD : float\n        Relative roughness, [-]\n\n    Returns\n    -------\n    fd : float\n        Darcy friction factor [-]\n\n    Notes\n    -----\n    No range of validity specified for this equation.\n\n    Examples\n    --------\n    >>> Brkic_2011_2(1E5, 1E-4)\n    0.018619745410688716\n\n    References\n    ----------\n    .. [1] Winning, H. and T. Coole. \"Explicit Friction Factor Accuracy and\n       Computational Efficiency for Turbulent Flow in Pipes.\" Flow, Turbulence\n       and Combustion 90, no. 1 (January 1, 2013): 1-27.\n       doi:10.1007/s10494-012-9419-7\n    .. [2] \tBrkic, Dejan.\"Review of Explicit Approximations to the Colebrook\n       Relation for Flow Friction.\" Journal of Petroleum Science and\n       Engineering 77, no. 1 (April 2011): 34-48.\n       doi:10.1016/j.petrol.2011.02.006.\n    '''\n    beta = log(Re/(1.816*log(1.1*Re/log(1+1.1*Re))))\n    return (-2*log10(2.18*beta/Re + eD/3.71))**-2", "response": "Calculates the Darcy friction factor using the method in Brkic2011_2011_2."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef helical_laminar_fd_Schmidt(Re, Di, Dc):\n    r'''Calculates Darcy friction factor for a fluid flowing inside a curved \n    pipe such as a helical coil under laminar conditions, using the method of \n    Schmidt [1]_ as shown in [2]_ and [3]_.\n    \n    .. math::\n        f_{curved} = f_{\\text{straight,laminar}} \\left[1 + 0.14\\left(\\frac{D_i}\n        {D_c}\\right)^{0.97}Re^{\\left[1 - 0.644\\left(\\frac{D_i}{D_c}\n        \\right)^{0.312}\\right]}\\right]\n        \n    Parameters\n    ----------\n    Re : float\n        Reynolds number with `D=Di`, [-]\n    Di : float\n        Inner diameter of the coil, [m]\n    Dc : float\n        Diameter of the helix/coil measured from the center of the tube on one\n        side to the center of the tube on the other side, [m]\n\n    Returns\n    -------\n    fd : float\n        Darcy friction factor for a curved pipe [-]\n\n    Notes\n    -----\n    The range of validity of this equation is specified only for Re,\n    :math:`100 < Re < Re_{critical}`.\n    \n    The form of the equation is such that as the curvature becomes negligible,\n    straight tube result is obtained.                     \n\n    Examples\n    --------\n    >>> helical_laminar_fd_Schmidt(250, .02, .1)\n    0.47460725672835236\n\n    References\n    ----------\n    .. [1] Schmidt, Eckehard F. \"W\u00e4rme\u00fcbergang Und Druckverlust in \n       Rohrschlangen.\" Chemie Ingenieur Technik 39, no. 13 (July 10, 1967): \n       781-89. doi:10.1002/cite.330391302.\n    .. [2] El-Genk, Mohamed S., and Timothy M. Schriener. \"A Review and \n       Correlations for Convection Heat Transfer and Pressure Losses in \n       Toroidal and Helically Coiled Tubes.\" Heat Transfer Engineering 0, no. 0\n       (June 7, 2016): 1-28. doi:10.1080/01457632.2016.1194693.\n    .. [3] Pimenta, T. A., and J. B. L. M. Campos. \"Friction Losses of \n       Newtonian and Non-Newtonian Fluids Flowing in Laminar Regime in a \n       Helical Coil.\" Experimental Thermal and Fluid Science 36 (January 2012):\n       194-204. doi:10.1016/j.expthermflusci.2011.09.013.\n    '''\n    fd = friction_laminar(Re)\n    D_ratio = Di/Dc\n    return fd*(1. + 0.14*D_ratio**0.97*Re**(1. - 0.644*D_ratio**0.312))", "response": "Calculates the Darcy friction factor for a curved pipe with a given diameter and inner diameter."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate the Darcy friction factor for a curved pipe with a given diameter and inner diameter.", "response": "def helical_turbulent_fd_Mori_Nakayama(Re, Di, Dc):\n    r'''Calculates Darcy friction factor for a fluid flowing inside a curved \n    pipe such as a helical coil under turbulent conditions, using the method of \n    Mori and Nakayama [1]_, also shown in [2]_ and [3]_.\n        \n    .. math::\n        f_{curv} = 0.3\\left(\\frac{D_i}{D_c}\\right)^{0.5}\n        \\left[Re\\left(\\frac{D_i}{D_c}\\right)^2\\right]^{-0.2}\\left[1 \n        + 0.112\\left[Re\\left(\\frac{D_i}{D_c}\\right)^2\\right]^{-0.2}\\right]\n\n    Parameters\n    ----------\n    Re : float\n        Reynolds number with `D=Di`, [-]\n    Di : float\n        Inner diameter of the coil, [m]\n    Dc : float\n        Diameter of the helix/coil measured from the center of the tube on one\n        side to the center of the tube on the other side, [m]\n\n    Returns\n    -------\n    fd : float\n        Darcy friction factor for a curved pipe [-]\n\n    Notes\n    -----    \n    Valid from the transition to turbulent flow up to \n    :math:`Re=6.5\\times 10^{5}\\sqrt{D_i/D_c}`. Does not use a straight pipe \n    correlation, and so will not converge on the\n    straight pipe result at very low curvature.\n\n    Examples\n    --------\n    >>> helical_turbulent_fd_Mori_Nakayama(1E4, 0.01, .2)\n    0.037311802071379796\n\n    References\n    ----------\n    .. [1] Mori, Yasuo, and Wataru Nakayama. \"Study of Forced Convective Heat \n       Transfer in Curved Pipes (2nd Report, Turbulent Region).\" International \n       Journal of Heat and Mass Transfer 10, no. 1 (January 1, 1967): 37-59.\n       doi:10.1016/0017-9310(67)90182-2. \n    .. [2] El-Genk, Mohamed S., and Timothy M. Schriener. \"A Review and \n       Correlations for Convection Heat Transfer and Pressure Losses in \n       Toroidal and Helically Coiled Tubes.\" Heat Transfer Engineering 0, no. 0\n       (June 7, 2016): 1-28. doi:10.1080/01457632.2016.1194693.\n    .. [3] Ali, Shaukat. \"Pressure Drop Correlations for Flow through Regular\n       Helical Coil Tubes.\" Fluid Dynamics Research 28, no. 4 (April 2001): \n       295-310. doi:10.1016/S0169-5983(00)00034-4.\n    '''\n    term = (Re*(Di/Dc)**2)**-0.2\n    return 0.3*(Dc/Di)**-0.5*term*(1. + 0.112*term)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates the Darcy friction factor for a curved pipe with a helical turbulent condition.", "response": "def helical_turbulent_fd_Czop (Re, Di, Dc):\n    r'''Calculates Darcy friction factor for a fluid flowing inside a curved \n    pipe such as a helical coil under turbulent conditions, using the method of \n    Czop [1]_, also shown in [2]_.\n        \n    .. math::\n        f_{curv} = 0.096De^{-0.1517}\n\n    Parameters\n    ----------\n    Re : float\n        Reynolds number with `D=Di`, [-]\n    Di : float\n        Inner diameter of the coil, [m]\n    Dc : float\n        Diameter of the helix/coil measured from the center of the tube on one\n        side to the center of the tube on the other side, [m]\n\n    Returns\n    -------\n    fd : float\n        Darcy friction factor for a curved pipe [-]\n\n    Notes\n    -----    \n    Valid for :math:`2\\times10^4 < Re < 1.5\\times10^{5}`. Does not use a \n    straight pipe correlation, and so will not converge on the\n    straight pipe result at very low curvature.\n\n    Examples\n    --------\n    >>> helical_turbulent_fd_Czop(1E4, 0.01, .2)\n    0.02979575250574106\n\n    References\n    ----------\n    .. [1] Czop, V., D. Barbier, and S. Dong. \"Pressure Drop, Void Fraction and\n       Shear Stress Measurements in an Adiabatic Two-Phase Flow in a Coiled \n       Tube.\" Nuclear Engineering and Design 149, no. 1 (September 1, 1994): \n       323-33. doi:10.1016/0029-5493(94)90298-4.\n    .. [2] El-Genk, Mohamed S., and Timothy M. Schriener. \"A Review and \n       Correlations for Convection Heat Transfer and Pressure Losses in \n       Toroidal and Helically Coiled Tubes.\" Heat Transfer Engineering 0, no. 0\n       (June 7, 2016): 1-28. doi:10.1080/01457632.2016.1194693.\n    '''\n    De = Dean(Re=Re, Di=Di, D=Dc)\n    return 0.096*De**-0.1517"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the Darcy friction factor for a curved pipe of a helical coil.", "response": "def helical_turbulent_fd_Ju(Re, Di, Dc,roughness=0):\n    r'''Calculates Darcy friction factor for a fluid flowing inside a curved \n    pipe such as a helical coil under turbulent conditions, using the method of \n    Ju et al. [1]_, also shown in [2]_.\n        \n    .. math::\n        f_{curv} = f_{\\text{str,turb}}\\left[1 +0.11Re^{0.23}\\left(\\frac{D_i}\n        {D_c}\\right)^{0.14}\\right]\n        \n    Parameters\n    ----------\n    Re : float\n        Reynolds number with `D=Di`, [-]\n    Di : float\n        Inner diameter of the coil, [m]\n    Dc : float\n        Diameter of the helix/coil measured from the center of the tube on one\n        side to the center of the tube on the other side, [m]\n    roughness : float, optional\n        Roughness of pipe wall [m]        \n\n    Returns\n    -------\n    fd : float\n        Darcy friction factor for a curved pipe [-]\n\n    Notes\n    -----\n    Claimed to be valid for all turbulent conditions with :math:`De>11.6`.\n    At very low curvatures, converges on the straight pipe result.\n\n    Examples\n    --------\n    >>> helical_turbulent_fd_Ju(1E4, 0.01, .2)\n    0.04945959480770937\n\n    References\n    ----------\n    .. [1] Ju, Huaiming, Zhiyong Huang, Yuanhui Xu, Bing Duan, and Yu Yu. \n       \"Hydraulic Performance of Small Bending Radius Helical Coil-Pipe.\" \n       Journal of Nuclear Science and Technology 38, no. 10 (October 1, 2001): \n       826-31. doi:10.1080/18811248.2001.9715102.\n    .. [2] El-Genk, Mohamed S., and Timothy M. Schriener. \"A Review and \n       Correlations for Convection Heat Transfer and Pressure Losses in \n       Toroidal and Helically Coiled Tubes.\" Heat Transfer Engineering 0, no. 0\n       (June 7, 2016): 1-28. doi:10.1080/01457632.2016.1194693.\n    '''\n    fd = friction_factor(Re=Re, eD=roughness/Di)\n    return fd*(1. + 0.11*Re**0.23*(Di/Dc)**0.14)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncalculate the Darcy friction factor for a curved pipe with a given diameter Di and Dc.", "response": "def helical_turbulent_fd_Mandal_Nigam(Re, Di, Dc, roughness=0):\n    r'''Calculates Darcy friction factor for a fluid flowing inside a curved \n    pipe such as a helical coil under turbulent conditions, using the method of \n    Mandal and Nigam [1]_, also shown in [2]_.\n        \n    .. math::\n        f_{curv} = f_{\\text{str,turb}} [1 + 0.03{De}^{0.27}]\n        \n    Parameters\n    ----------\n    Re : float\n        Reynolds number with `D=Di`, [-]\n    Di : float\n        Inner diameter of the coil, [m]\n    Dc : float\n        Diameter of the helix/coil measured from the center of the tube on one\n        side to the center of the tube on the other side, [m]\n    roughness : float, optional\n        Roughness of pipe wall [m]        \n\n    Returns\n    -------\n    fd : float\n        Darcy friction factor for a curved pipe [-]\n\n    Notes\n    -----\n    Claimed to be valid for all turbulent conditions with \n    :math:`2500 < De < 15000`. At very low curvatures, converges on the \n    straight pipe result.\n\n    Examples\n    --------\n    >>> helical_turbulent_fd_Mandal_Nigam(1E4, 0.01, .2)\n    0.03831658117115902\n\n    References\n    ----------\n    .. [1] Mandal, Monisha Mridha, and K. D. P. Nigam. \"Experimental Study on \n       Pressure Drop and Heat Transfer of Turbulent Flow in Tube in Tube \n       Helical Heat Exchanger.\" Industrial & Engineering Chemistry Research 48,\n       no. 20 (October 21, 2009): 9318-24. doi:10.1021/ie9002393. \n    .. [2] El-Genk, Mohamed S., and Timothy M. Schriener. \"A Review and \n       Correlations for Convection Heat Transfer and Pressure Losses in \n       Toroidal and Helically Coiled Tubes.\" Heat Transfer Engineering 0, no. 0\n       (June 7, 2016): 1-28. doi:10.1080/01457632.2016.1194693.\n    '''\n    De = Dean(Re=Re, Di=Di, D=Dc)\n    fd = friction_factor(Re=Re, eD=roughness/Di)\n    return fd*(1. + 0.03*De**0.27)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef friction_factor_curved(Re, Di, Dc, roughness=0.0, Method=None, \n                           Rec_method='Schmidt', \n                           laminar_method='Schmidt laminar',\n                           turbulent_method='Schmidt turbulent', Darcy=True, \n                           AvailableMethods=False):\n    r'''Calculates friction factor fluid flowing in a curved pipe or helical\n    coil, supporting both laminar and turbulent regimes. Selects the \n    appropriate regime by default, and has default correlation choices.\n    Optionally, a specific correlation can be specified with the `Method` \n    keyword.\n    \n    The default correlations are those recommended in [1]_, and are believed to \n    be the best publicly available.\n    \n    Examples\n    --------\n    >>> friction_factor_curved(Re=1E5, Di=0.02, Dc=0.5)\n    0.022961996738387523\n\n    Parameters\n    ----------\n    Re : float\n        Reynolds number with `D=Di`, [-]\n    Di : float\n        Inner diameter of the tube making up the coil, [m]\n    Dc : float\n        Diameter of the helix/coil measured from the center of the tube on one\n        side to the center of the tube on the other side, [m]\n    roughness : float, optional\n        Roughness of pipe wall [m]        \n\n    Returns\n    -------\n    f : float\n        Friction factor, [-]\n    methods : list, only returned if AvailableMethods == True\n        List of methods in the regime the specified `Re` is in at the given\n        `Di` and `Dc`.\n\n    Other Parameters\n    ----------------\n    Method : string, optional\n        A string of the function name to use, overriding the default turbulent/\n        laminar selection.\n    Rec_method : str, optional\n        Critical Reynolds number transition criteria; one of ['Seth Stahel', \n        'Ito', 'Kubair Kuloor', 'Kutateladze Borishanskii', 'Schmidt', \n        'Srinivasan']; the default is 'Schmidt'.\n    laminar_method : str, optional\n        Friction factor correlation for the laminar regime; one of \n        ['White', 'Mori Nakayama laminar', 'Schmidt laminar']; the default is\n        'Schmidt laminar'.\n    turbulent_method : str, optional\n        Friction factor correlation for the turbulent regime; one of \n        ['Guo', 'Ju', 'Schmidt turbulent', 'Prasad', 'Mandel Nigam', \n        'Mori Nakayama turbulent', 'Czop']; the default is 'Schmidt turbulent'.\n    Darcy : bool, optional\n        If False, will return fanning friction factor, 1/4 of the Darcy value\n    AvailableMethods : bool, optional\n        If True, function will consider which methods claim to be valid for\n        the range of `Re` and `eD` given\n    \n    See Also\n    --------\n    fluids.geometry.HelicalCoil\n    helical_turbulent_fd_Schmidt\n    helical_turbulent_fd_Srinivasan\n    helical_turbulent_fd_Mandal_Nigam\n    helical_turbulent_fd_Ju\n    helical_turbulent_fd_Guo\n    helical_turbulent_fd_Czop\n    helical_turbulent_fd_Prasad\n    helical_turbulent_fd_Mori_Nakayama\n    helical_laminar_fd_Schmidt\n    helical_laminar_fd_Mori_Nakayama\n    helical_laminar_fd_White\n    helical_transition_Re_Schmidt\n    helical_transition_Re_Srinivasan\n    helical_transition_Re_Kutateladze_Borishanskii\n    helical_transition_Re_Kubair_Kuloor\n    helical_transition_Re_Ito\n    helical_transition_Re_Seth_Stahel\n\n    Notes\n    -----\n    The range of acccuracy of these correlations is much than that in a \n    straight pipe.    \n    \n    References\n    ----------\n    .. [1] Schlunder, Ernst U, and International Center for Heat and Mass\n       Transfer. Heat Exchanger Design Handbook. Washington:\n       Hemisphere Pub. Corp., 1983.\n    '''\n    if Rec_method in curved_friction_transition_methods:\n        Re_crit = curved_friction_transition_methods[Rec_method](Di, Dc)\n    else:\n        raise Exception('Invalid method specified for transition Reynolds number.')\n    \n    turbulent = False if Re < Re_crit else True\n    \n    def list_methods():\n        if turbulent:\n            return list(curved_friction_turbulent_methods.keys())\n        else:\n            return list(curved_friction_laminar_methods.keys())\n    if AvailableMethods:\n        return list_methods()\n    \n    if not Method:\n        Method = turbulent_method if turbulent else laminar_method\n    \n    if Method in curved_friction_laminar_methods:\n        f = curved_friction_laminar_methods[Method](Re, Di, Dc)\n    elif Method in curved_friction_turbulent_methods:\n        correlation, supports_roughness = curved_friction_turbulent_methods[Method]\n        if supports_roughness:\n            f = correlation(Re, Di, Dc, roughness)\n        else:\n            f = correlation(Re, Di, Dc)\n    else:\n        raise Exception('Invalid method for friction factor calculation')\n        \n    if not Darcy:\n        f *= 0.25\n    return f", "response": "r Returns a friction factor for a curved coil."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef friction_plate_Martin_1999(Re, plate_enlargement_factor):\n    r'''Calculates Darcy friction factor for single-phase flow in a \n    Chevron-style plate heat exchanger according to [1]_. \n    \n    .. math::\n        \\frac{1}{\\sqrt{f_f}} = \\frac{\\cos \\phi}{\\sqrt{0.045\\tan\\phi\n        + 0.09\\sin\\phi + f_0/\\cos(\\phi)}} + \\frac{1-\\cos\\phi}{\\sqrt{3.8f_1}}\n        \n    .. math::\n        f_0 = 16/Re \\text{ for } Re < 2000\n        \n    .. math::\n        f_0 = (1.56\\ln Re - 3)^{-2} \\text{ for } Re \\ge 2000\n        \n    .. math::\n        f_1 = \\frac{149}{Re} + 0.9625 \\text{ for } Re < 2000\n        \n    .. math::\n        f_1 = \\frac{9.75}{Re^{0.289}} \\text{ for } Re \\ge 2000\n        \n    Parameters\n    ----------\n    Re : float\n        Reynolds number with respect to the hydraulic diameter of the channels,\n        [-]\n    plate_enlargement_factor : float\n        The extra surface area multiplier as compared to a flat plate\n        caused the corrugations, [-]\n\n    Returns\n    -------\n    fd : float\n        Darcy friction factor [-]\n\n    Notes\n    -----\n    Based on experimental data from Re from 200 - 10000 and enhancement \n    factors calculated with chevron angles of 0 to 80 degrees. See \n    `PlateExchanger` for further clarification on the definitions.\n    \n    The length the friction factor gets multiplied by is not the flow path\n    length, but rather the straight path length from port to port as if there\n    were no chevrons.\n    \n    Note there is a discontinuity at Re = 2000 for the transition from\n    laminar to turbulent flow, although the literature suggests the transition\n    is actually smooth.\n    \n    This was first developed in [2]_ and only minor modifications by the \n    original author were made before its republication in [1]_. \n    This formula is also suggested in [3]_\n\n    Examples\n    --------\n    >>> friction_plate_Martin_1999(Re=20000, plate_enlargement_factor=1.15)\n    2.284018089834134\n\n    References\n    ----------\n    .. [1] Martin, Holger. \"Economic optimization of compact heat exchangers.\"\n       EF-Conference on Compact Heat Exchangers and Enhancement Technology for \n       the Process Industries, Banff, Canada, July 18-23, 1999, 1999. \n       https://publikationen.bibliothek.kit.edu/1000034866.\n    .. [2] Martin, Holger. \"A Theoretical Approach to Predict the Performance \n       of Chevron-Type Plate Heat Exchangers.\" Chemical Engineering and \n       Processing: Process Intensification 35, no. 4 (January 1, 1996): 301-10. \n       https://doi.org/10.1016/0255-2701(95)04129-X.\n    .. [3] Shah, Ramesh K., and Dusan P. Sekulic. Fundamentals of Heat \n       Exchanger Design. 1st edition. Hoboken, NJ: Wiley, 2002.\n    '''\n    phi = plate_enlargement_factor\n    \n    if Re < 2000.:\n        f0 = 16./Re\n        f1 = 149./Re + 0.9625\n    else:\n        f0 = (1.56*log(Re) - 3.0)**-2\n        f1 = 9.75*Re**-0.289\n        \n    rhs = cos(phi)*(0.045*tan(phi) + 0.09*sin(phi) + f0/cos(phi))**-0.5\n    rhs += (1. - cos(phi))*(3.8*f1)**-0.5\n    ff = rhs**-2.\n    return ff*4.0", "response": "Calculates the friction factor for a single - phase flow in a \n    based on the Martin 1999 paper."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef friction_plate_Martin_VDI(Re, plate_enlargement_factor):\n    r'''Calculates Darcy friction factor for single-phase flow in a \n    Chevron-style plate heat exchanger according to [1]_. \n    \n    .. math::\n        \\frac{1}{\\sqrt{f_d}} = \\frac{\\cos \\phi}{\\sqrt{0.28\\tan\\phi\n        + 0.36\\sin\\phi + f_0/\\cos(\\phi)}} + \\frac{1-\\cos\\phi}{\\sqrt{3.8f_1}}\n        \n    .. math::\n        f_0 = 64/Re \\text{ for } Re < 2000\n        \n    .. math::\n        f_0 = (1.56\\ln Re - 3)^{-2} \\text{ for } Re \\ge 2000\n        \n    .. math::\n        f_1 = \\frac{597}{Re} + 3.85 \\text{ for } Re < 2000\n        \n    .. math::\n        f_1 = \\frac{39}{Re^{0.289}} \\text{ for } Re \\ge 2000\n        \n    Parameters\n    ----------\n    Re : float\n        Reynolds number with respect to the hydraulic diameter of the channels,\n        [-]\n    plate_enlargement_factor : float\n        The extra surface area multiplier as compared to a flat plate\n        caused the corrugations, [-]\n\n    Returns\n    -------\n    fd : float\n        Darcy friction factor [-]\n\n    Notes\n    -----\n    Based on experimental data from Re from 200 - 10000 and enhancement \n    factors calculated with chevron angles of 0 to 80 degrees. See \n    `PlateExchanger` for further clarification on the definitions.\n    \n    The length the friction factor gets multiplied by is not the flow path\n    length, but rather the straight path length from port to port as if there\n    were no chevrons.\n    \n    Note there is a discontinuity at Re = 2000 for the transition from\n    laminar to turbulent flow, although the literature suggests the transition\n    is actually smooth.\n    \n    This is a revision of the Martin's earlier model, adjusted to predidct\n    higher friction factors.\n    \n    There are three parameters in this model, a, b and c; it is posisble\n    to adjust them to better fit a know exchanger's pressure drop.\n    \n    See Also\n    --------\n    friction_plate_Martin_1999\n\n    Examples\n    --------\n    >>> friction_plate_Martin_VDI(Re=20000, plate_enlargement_factor=1.15)\n    2.702534119024076\n\n    References\n    ----------\n    .. [1] Gesellschaft, V. D. I., ed. VDI Heat Atlas. 2nd edition.\n       Berlin; New York:: Springer, 2010.\n    '''\n    phi = plate_enlargement_factor\n    \n    if Re < 2000.:\n        f0 = 64./Re\n        f1 = 597./Re + 3.85\n    else:\n        f0 = (1.8*log10(Re) - 1.5)**-2\n        f1 = 39.*Re**-0.289\n        \n    a, b, c = 3.8, 0.28, 0.36\n        \n    rhs = cos(phi)*(b*tan(phi) + c*sin(phi) + f0/cos(phi))**-0.5\n    rhs += (1. - cos(phi))*(a*f1)**-0.5\n    return rhs**-2.0", "response": "Calculates the friction factor for a single - phase flow in a \n   ."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef friction_plate_Kumar(Re, chevron_angle):\n    r'''Calculates Darcy friction factor for single-phase flow in a \n    **well-designed** Chevron-style plate heat exchanger according to [1]_.\n    The data is believed to have been developed by APV International Limited,\n    since acquired by SPX Corporation. This uses a curve fit of that data\n    published in [2]_.\n    \n    .. math::\n        f_f = \\frac{C_2}{Re^p}\n        \n    C2 and p are coefficients looked up in a table, with varying ranges\n    of Re validity and chevron angle validity. See the source for their\n    exact values.\n        \n    Parameters\n    ----------\n    Re : float\n        Reynolds number with respect to the hydraulic diameter of the channels,\n        [-]\n    chevron_angle : float\n        Angle of the plate corrugations with respect to the vertical axis\n        (the direction of flow if the plates were straight), between 0 and\n        90. Many plate exchangers use two alternating patterns; use their\n        average angle for that situation [degrees]\n        \n    Returns\n    -------\n    fd : float\n        Darcy friction factor [-]\n\n    Notes\n    -----\n    Data on graph from Re=0.1 to Re=10000, with chevron angles 30 to 65 degrees.\n    See `PlateExchanger` for further clarification on the definitions.\n    \n    It is believed the constants used in this correlation were curve-fit to\n    the actual graph in [1]_ by the author of [2]_ as there is no \n    \n    The length the friction factor gets multiplied by is not the flow path\n    length, but rather the straight path length from port to port as if there\n    were no chevrons.\n    \n    As the coefficients change, there are numerous small discontinuities, \n    although the data on the graphs is continuous with sharp transitions\n    of the slope.\n    \n    The author of [1]_ states clearly this correlation is \"applicable only to \n    well designed Chevron PHEs\".\n    \n    Examples\n    --------\n    >>> friction_plate_Kumar(Re=2000, chevron_angle=30)\n    2.9760669055634517\n\n    References\n    ----------\n    .. [1] Kumar, H. \"The plate heat exchanger: construction and design.\" In \n       First U.K. National Conference on Heat Transfer: Held at the University \n       of Leeds, 3-5 July 1984, Institute of Chemical Engineering Symposium \n       Series, vol. 86, pp. 1275-1288. 1984.\n    .. [2] Ayub, Zahid H. \"Plate Heat Exchanger Literature Survey and New Heat\n       Transfer and Pressure Drop Correlations for Refrigerant Evaporators.\" \n       Heat Transfer Engineering 24, no. 5 (September 1, 2003): 3-16. \n       doi:10.1080/01457630304056.\n    '''\n    beta_list_len = len(Kumar_beta_list)\n    \n    for i in range(beta_list_len):\n        if chevron_angle <= Kumar_beta_list[i]:\n            C2_options, p_options, Re_ranges = Kumar_C2s[i], Kumar_Ps[i], Kumar_fd_Res[i]\n            break\n        elif i == beta_list_len-1:\n            C2_options, p_options, Re_ranges = Kumar_C2s[-1], Kumar_Ps[-1], Kumar_fd_Res[-1]\n                \n    Re_len = len(Re_ranges)\n    \n    for j in range(Re_len):\n        if Re <= Re_ranges[j]:\n            C2, p = C2_options[j], p_options[j]\n            break\n        elif j == Re_len-1:\n            C2, p = C2_options[-1], p_options[-1]\n        \n    # Originally in Fanning friction factor basis\n    return 4.0*C2*Re**-p", "response": "Calculates the friction factor for a single - phase flow in a given pattern."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates the friction factor for a single - phase flow in a given Chevron - style plate and chevron angle.", "response": "def friction_plate_Muley_Manglik(Re, chevron_angle, plate_enlargement_factor):\n    r'''Calculates Darcy friction factor for single-phase flow in a \n    Chevron-style plate heat exchanger according to [1]_, also shown and\n    recommended in [2]_.\n    \n    .. math::\n        f_f = [2.917 - 0.1277\\beta + 2.016\\times10^{-3} \\beta^2]\n        \\times[20.78 - 19.02\\phi + 18.93\\phi^2 - 5.341\\phi^3]\n        \\times Re^{-[0.2 + 0.0577\\sin[(\\pi \\beta/45)+2.1]]}\n        \n    Parameters\n    ----------\n    Re : float\n        Reynolds number with respect to the hydraulic diameter of the channels,\n        [-]\n    chevron_angle : float\n        Angle of the plate corrugations with respect to the vertical axis\n        (the direction of flow if the plates were straight), between 0 and\n        90. Many plate exchangers use two alternating patterns; use their\n        average angle for that situation [degrees]\n    plate_enlargement_factor : float\n        The extra surface area multiplier as compared to a flat plate\n        caused the corrugations, [-]\n\n    Returns\n    -------\n    fd : float\n        Darcy friction factor [-]\n\n    Notes\n    -----\n    Based on experimental data of plate enacement factors up to 1.5, and valid \n    for Re > 1000 and chevron angles from 30 to 60 degrees with sinusoidal \n    shape. See `PlateExchanger` for further clarification on the definitions.\n    \n    The length the friction factor gets multiplied by is not the flow path\n    length, but rather the straight path length from port to port as if there\n    were no chevrons.\n    \n    This is a continuous model with no discontinuities.\n    \n    Examples\n    --------\n    >>> friction_plate_Muley_Manglik(Re=2000, chevron_angle=45, plate_enlargement_factor=1.2)\n    1.0880870804075413\n\n    References\n    ----------\n    .. [1] Muley, A., and R. M. Manglik. \"Experimental Study of Turbulent Flow\n       Heat Transfer and Pressure Drop in a Plate Heat Exchanger With Chevron \n       Plates.\" Journal of Heat Transfer 121, no. 1 (February 1, 1999): 110-17.\n       doi:10.1115/1.2825923.\n    .. [2] Ayub, Zahid H. \"Plate Heat Exchanger Literature Survey and New Heat\n       Transfer and Pressure Drop Correlations for Refrigerant Evaporators.\" \n       Heat Transfer Engineering 24, no. 5 (September 1, 2003): 3-16. \n       doi:10.1080/01457630304056.\n    '''\n    beta, phi = chevron_angle, plate_enlargement_factor\n    # Beta is indeed chevron angle; with respect to angle of mvoement\n    # Still might be worth another check\n    t1 = (2.917 - 0.1277*beta + 2.016E-3*beta**2)\n    t2 = (5.474 - 19.02*phi + 18.93*phi**2 - 5.341*phi**3)\n    t3 = -(0.2 + 0.0577*sin(pi*beta/45. + 2.1))\n    # Equation returns fanning friction factor\n    return 4*t1*t2*Re**t3"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef roughness_Farshad(ID=None, D=None, coeffs=None):\n    r'''Calculates of retrieves the roughness of a pipe based on the work of\n    [1]_. This function will return an average value for pipes of a given\n    material, or if diameter is provided, will calculate one specifically for\n    the pipe inner diameter according to the following expression with \n    constants `A` and `B`:\n    \n    .. math::\n        \\epsilon = A\\cdot D^{B+1}\n    \n    Please not that `A` has units of inches, and `B` requires `D` to be in \n    inches as well.\n    \n    The list of supported materials is as follows:\n\n        * 'Plastic coated'\n        * 'Carbon steel, honed bare'\n        * 'Cr13, electropolished bare'\n        * 'Cement lining'\n        * 'Carbon steel, bare'\n        * 'Fiberglass lining'\n        * 'Cr13, bare'\n    \n    If `coeffs` and `D` are given, the custom coefficients for the equation as\n    given by the user will be used and `ID` is not required.\n\n    Parameters\n    ----------\n    ID : str, optional\n        Name of pipe material from above list\n    D : float, optional\n        Actual inner diameter of pipe, [m]\n    coeffs : tuple, optional\n        (A, B) Coefficients to use directly, instead of looking them up;\n        they are actually dimensional, in the forms (inch^-B, -) but only\n        coefficients with those dimensions are available [-]\n\n    Returns\n    -------\n    epsilon : float\n        Roughness of pipe [m]\n    \n    Notes\n    -----\n    The diameter-dependent form provides lower roughness values for larger\n    diameters.\n    \n    The measurements were based on DIN 4768/1 (1987), using both a \n    \"Dektak ST Surface Profiler\" and a \"Hommel Tester T1000\". Both instruments\n    were found to be in agreement. A series of flow tests, in which pressure \n    drop directly measured, were performed as well, with nitrogen gas as an \n    operating fluid. The accuracy of the data from these tests is claimed to be\n    within 1%.\n    \n    Using those results, the authors back-calculated what relative roughness \n    values would be necessary to produce the observed pressure drops. The \n    average difference between this back-calculated roughness and the measured\n    roughness was 6.75%.\n    \n    For microchannels, this model will predict roughness much larger than the\n    actual channel diameter.\n\n    Examples\n    --------\n    >>> roughness_Farshad('Cr13, bare', 0.05)\n    5.3141677781137006e-05\n\n    References\n    ----------\n    .. [1] Farshad, Fred F., and Herman H. Rieke. \"Surface Roughness Design \n       Values for Modern Pipes.\" SPE Drilling & Completion 21, no. 3 (September\n       1, 2006): 212-215. doi:10.2118/89040-PA.\n    '''\n    # Case 1, coeffs given; only run if ID is not given.\n    if ID is None and coeffs:\n        A, B = coeffs\n        return A*(D/inch)**(B+1)*inch\n    # Case 2, lookup parameters\n    try :\n        dat = _Farshad_roughness[ID]\n    except:\n        raise KeyError('ID was not in _Farshad_roughness.')\n    if D is None:\n        return dat[0]\n    else:\n        A, B = dat[1], dat[2]\n        return A*(D/inch)**(B+1)*inch", "response": "r Returns the average value for a given work of a given frequency hierarchy."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef one_phase_dP(m, rho, mu, D, roughness=0, L=1, Method=None):\n    r'''Calculates single-phase pressure drop. This is a wrapper\n    around other methods.\n\n    Parameters\n    ----------\n    m : float\n        Mass flow rate of fluid, [kg/s]\n    rho : float\n        Density of fluid, [kg/m^3]\n    mu : float\n        Viscosity of fluid, [Pa*s]\n    D : float\n        Diameter of pipe, [m]\n    roughness : float, optional\n        Roughness of pipe for use in calculating friction factor, [m]\n    L : float, optional\n        Length of pipe, [m]\n    Method : string, optional\n        A string of the function name to use\n\n    Returns\n    -------\n    dP : float\n        Pressure drop of the single-phase flow, [Pa]\n\n    Notes\n    -----\n\n    Examples\n    --------\n    >>> one_phase_dP(10.0, 1000, 1E-5, .1, L=1)\n    63.43447321097365\n    \n    References\n    ----------\n    .. [1] Crane Co. Flow of Fluids Through Valves, Fittings, and Pipe. Crane,\n       2009.\n    '''\n    D2 = D*D\n    V = m/(0.25*pi*D2*rho)\n    Re = Reynolds(V=V, rho=rho, mu=mu, D=D)\n    fd = friction_factor(Re=Re, eD=roughness/D, Method=Method)\n    dP = fd*L/D*(0.5*rho*V*V)\n    return dP", "response": "r Calculates the pressure drop of a single - phase fluid."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef one_phase_dP_acceleration(m, D, rho_o, rho_i):\n    r'''This function handles calculation of one-phase fluid pressure drop\n    due to acceleration for flow inside channels. This is a discrete \n    calculation, providing the total differential in pressure for a given  \n    length and should be called as part of a segment solver routine.\n    \n    .. math::\n        - \\left(\\frac{d P}{dz}\\right)_{acc} = G^2 \\frac{d}{dz} \\left[\\frac{\n        1}{\\rho_o} - \\frac{1}{\\rho_i} \\right]\n\n    Parameters\n    ----------\n    m : float\n        Mass flow rate of fluid, [kg/s]\n    D : float\n        Diameter of pipe, [m]\n    rho_o : float\n        Fluid density out, [kg/m^3]\n    rho_i : float\n        Fluid density int, [kg/m^3]\n        \n    Returns\n    -------\n    dP : float\n        Acceleration component of pressure drop for one-phase flow, [Pa]\n        \n    Notes\n    -----\n\n    Examples\n    --------\n    >>> one_phase_dP_acceleration(m=1, D=0.1, rho_o=827.1, rho_i=830)\n    0.06848289670840459\n    '''\n    G = 4.0*m/(pi*D*D)\n    return G*G*(1.0/rho_o - 1.0/rho_i)", "response": "r This function calculates the pressure drop for one - phase fluid pressure drop for a given pipe rate and flow density."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef one_phase_dP_gravitational(angle, rho, L=1.0, g=g):\n    r'''This function handles calculation of one-phase liquid-gas pressure drop\n    due to gravitation for flow inside channels. This is either a differential \n    calculation for a segment with an infinitesimal difference in elevation (if\n    `L`=1 or a discrete calculation.\n    \n    .. math::\n        -\\left(\\frac{dP}{dz} \\right)_{grav} =  \\rho g \\sin \\theta\n    \n    .. math::\n        -\\left(\\Delta P \\right)_{grav} =  L \\rho g \\sin \\theta\n    \n    Parameters\n    ----------\n    angle : float\n        The angle of the pipe with respect to the horizontal, [degrees]\n    rho : float\n        Fluid density, [kg/m^3]\n    L : float, optional\n        Length of pipe, [m]\n    g : float, optional\n        Acceleration due to gravity, [m/s^2]\n\n    Returns\n    -------\n    dP : float\n        Gravitational component of pressure drop for one-phase flow, [Pa/m] or\n        [Pa]\n        \n    Notes\n    -----\n        \n    Examples\n    --------    \n    >>> one_phase_dP_gravitational(angle=90, rho=2.6)\n    25.49729\n    >>> one_phase_dP_gravitational(angle=90, rho=2.6, L=4)\n    101.98916\n    '''\n    angle = radians(angle)\n    return L*g*sin(angle)*rho", "response": "r This function handles calculation of one - phase liquid - gas pressure drop at a given angle and pressure drop."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef transit_sunrise_sunset(dates, lat, lon, delta_t, numthreads):\n    isnumpy = isinstance(dates, ndarray)\n    if isnumpy:\n        condition = ((dates % 86400) != 0.0).any()\n    else:\n        condition = (dates % 86400) != 0.0\n    if condition:\n        raise ValueError('Input dates must be at 00:00 UTC')\n\n    utday = (dates // 86400) * 86400\n    ttday0 = utday - delta_t\n    ttdayn1 = ttday0 - 86400\n    ttdayp1 = ttday0 + 86400\n\n    # index 0 is v, 1 is alpha, 2 is delta\n    utday_res = solar_position(utday, 0, 0, 0, 0, 0, delta_t,\n                               0, numthreads, sst=True)\n    v = utday_res[0]\n\n    ttday0_res = solar_position(ttday0, 0, 0, 0, 0, 0, delta_t,\n                                0, numthreads, sst=True)\n    ttdayn1_res = solar_position(ttdayn1, 0, 0, 0, 0, 0, delta_t,\n                                 0, numthreads, sst=True)\n    ttdayp1_res = solar_position(ttdayp1, 0, 0, 0, 0, 0, delta_t,\n                                 0, numthreads, sst=True)\n    m0 = (ttday0_res[1] - lon - v) / 360\n    cos_arg = ((np.sin(np.radians(-0.8333)) - np.sin(np.radians(lat))\n               * np.sin(np.radians(ttday0_res[2]))) /\n               (np.cos(np.radians(lat)) * np.cos(np.radians(ttday0_res[2]))))\n    if isnumpy:\n        cos_arg[abs(cos_arg) > 1] = np.nan\n    else:\n        if abs(cos_arg) > 1:\n            cos_arg = np.nan\n    \n    H0 = np.degrees(np.arccos(cos_arg)) % 180\n\n    if isnumpy:\n        m = np.empty((3, len(utday)))\n    else:\n        m = np.empty((3, 1))\n    m[0] = m0 % 1\n    m[1] = (m[0] - H0 / 360)\n    m[2] = (m[0] + H0 / 360)\n\n    # need to account for fractions of day that may be the next or previous\n    # day in UTC\n    add_a_day = m[2] >= 1\n    sub_a_day = m[1] < 0\n    m[1] = m[1] % 1\n    m[2] = m[2] % 1\n    vs = v + 360.985647 * m\n    n = m + delta_t / 86400\n\n    a = ttday0_res[1] - ttdayn1_res[1]\n    \n    if isnumpy:\n        a[abs(a) > 2] = a[abs(a) > 2] % 1\n        ap = ttday0_res[2] - ttdayn1_res[2] \n        ap[abs(ap) > 2] = ap[abs(ap) > 2] % 1\n        b = ttdayp1_res[1] - ttday0_res[1]\n        b[abs(b) > 2] = b[abs(b) > 2] % 1\n        \n        bp = ttdayp1_res[2] - ttday0_res[2]\n        bp[abs(bp) > 2] = bp[abs(bp) > 2] % 1\n    else:\n        if abs(a) > 2:\n            a = a %1\n        ap = ttday0_res[2] - ttdayn1_res[2]\n        if (abs(ap) > 2):\n            ap = ap % 1\n        b = ttdayp1_res[1] - ttday0_res[1]\n        if (abs(b) > 2):\n            b = b % 1\n        bp = ttdayp1_res[2] - ttday0_res[2]\n        if abs(bp) > 2:\n            bp = bp % 1\n    \n    \n    c = b - a\n    cp = bp - ap\n\n    alpha_prime = ttday0_res[1] + (n * (a + b + c * n)) / 2\n    delta_prime = ttday0_res[2] + (n * (ap + bp + cp * n)) / 2\n    Hp = (vs + lon - alpha_prime) % 360\n    Hp[Hp >= 180] = Hp[Hp >= 180] - 360\n\n    h = np.degrees(np.arcsin(np.sin(np.radians(lat)) *\n                             np.sin(np.radians(delta_prime)) +\n                             np.cos(np.radians(lat)) *\n                             np.cos(np.radians(delta_prime))\n                             * np.cos(np.radians(Hp))))\n\n    T = (m[0] - Hp[0] / 360) * 86400\n    R = (m[1] + (h[1] + 0.8333) / (360 * np.cos(np.radians(delta_prime[1])) *\n                                   np.cos(np.radians(lat)) *\n                                   np.sin(np.radians(Hp[1])))) * 86400\n    S = (m[2] + (h[2] + 0.8333) / (360 * np.cos(np.radians(delta_prime[2])) *\n                                   np.cos(np.radians(lat)) *\n                                   np.sin(np.radians(Hp[2])))) * 86400\n\n    S[add_a_day] += 86400\n    R[sub_a_day] -= 86400\n\n    transit = T + utday\n    sunrise = R + utday\n    sunset = S + utday\n\n    return transit, sunrise, sunset", "response": "Calculate the sun transit sunrise sunset for a set of dates at a given location."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef orifice_expansibility_1989(D, Do, P1, P2, k):\n    r'''Calculates the expansibility factor for orifice plate calculations\n    based on the geometry of the plate, measured pressures of the orifice, and\n    the isentropic exponent of the fluid.\n    \n    .. math::\n        \\epsilon = 1- (0.41 + 0.35\\beta^4)\\Delta P/\\kappa/P_1\n        \n    Parameters\n    ----------\n    D : float\n        Upstream internal pipe diameter, [m]\n    Do : float\n        Diameter of orifice at flow conditions, [m]\n    P1 : float\n        Static pressure of fluid upstream of orifice at the cross-section of\n        the pressure tap, [Pa]\n    P2 : float\n        Static pressure of fluid downstream of orifice at the cross-section of\n        the pressure tap, [Pa]\n    k : float\n        Isentropic exponent of fluid, [-]\n\n    Returns\n    -------\n    expansibility : float\n        Expansibility factor (1 for incompressible fluids, less than 1 for\n        real fluids), [-]\n\n    Notes\n    -----\n    This formula was determined for the range of P2/P1 >= 0.75, and for fluids\n    of air, steam, and natural gas. However, there is no objection to using\n    it for other fluids.\n    \n    This is an older formula used to calculate expansibility factors for \n    orifice plates.\n    \n    In this standard, an expansibility factor formula transformation in terms  \n    of the pressure after the orifice is presented as well. This is the more\n    standard formulation in terms of the upstream conditions. The other formula\n    is below for reference only:\n    \n    .. math::\n        \\epsilon_2 = \\sqrt{1 + \\frac{\\Delta P}{P_2}} -  (0.41 + 0.35\\beta^4)\n        \\frac{\\Delta P}{\\kappa P_2 \\sqrt{1 + \\frac{\\Delta P}{P_2}}}\n        \n    [2]_ recommends this formulation for wedge meters as well.\n\n    Examples\n    --------\n    >>> orifice_expansibility_1989(D=0.0739, Do=0.0222, P1=1E5, P2=9.9E4, k=1.4)\n    0.9970510687411718\n\n    References\n    ----------\n    .. [1] American Society of Mechanical Engineers. MFC-3M-1989 Measurement \n       Of Fluid Flow In Pipes Using Orifice, Nozzle, And Venturi. ASME, 2005.\n    .. [2] Miller, Richard W. Flow Measurement Engineering Handbook. 3rd\n       edition. New York: McGraw-Hill Education, 1996.\n    '''\n    return 1.0 - (0.41 + 0.35*(Do/D)**4)*(P1 - P2)/(k*P1)", "response": "r Returns the expansibility factor for orifice plates in the 1989 - style system."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef C_Reader_Harris_Gallagher(D, Do, rho, mu, m, taps='corner'):\n    r'''Calculates the coefficient of discharge of the orifice based on the \n    geometry of the plate, measured pressures of the orifice, mass flow rate\n    through the orifice, and the density and viscosity of the fluid.\n    \n    .. math::\n        C = 0.5961 + 0.0261\\beta^2 - 0.216\\beta^8 + 0.000521\\left(\\frac{\n        10^6\\beta}{Re_D}\\right)^{0.7}\\\\\n        + (0.0188 + 0.0063A)\\beta^{3.5} \\left(\\frac{10^6}{Re_D}\\right)^{0.3} \\\\\n        +(0.043 + 0.080\\exp(-10L_1) -0.123\\exp(-7L_1))(1-0.11A)\\frac{\\beta^4}\n        {1-\\beta^4} \\\\\n        -  0.031(M_2' - 0.8M_2'^{1.1})\\beta^{1.3}\n        \n    .. math::\n        M_2' = \\frac{2L_2'}{1-\\beta}\n        \n    .. math::\n        A = \\left(\\frac{19000\\beta}{Re_{D}}\\right)^{0.8}\n        \n    .. math::\n        Re_D = \\frac{\\rho v D}{\\mu}\n        \n        \n    If D < 71.12 mm (2.8 in.) (Note this is a continuous addition; there is no\n    discontinuity):\n        \n    .. math::\n        C += 0.11(0.75-\\beta)\\left(2.8-\\frac{D}{0.0254}\\right)\n        \n    If the orifice has corner taps:\n        \n    .. math::\n        L_1 = L_2' = 0\n        \n    If the orifice has D and D/2 taps:\n        \n    .. math::\n        L_1 = 1\n        \n    .. math::\n        L_2' = 0.47\n    \n    If the orifice has Flange taps:\n    \n    .. math::\n        L_1 = L_2' = \\frac{0.0254}{D}\n        \n    Parameters\n    ----------\n    D : float\n        Upstream internal pipe diameter, [m]\n    Do : float\n        Diameter of orifice at flow conditions, [m]\n    rho : float\n        Density of fluid at `P1`, [kg/m^3]\n    mu : float\n        Viscosity of fluid at `P1`, [Pa*s]\n    m : float\n        Mass flow rate of fluid through the orifice, [kg/s]\n    taps : str\n        The orientation of the taps; one of 'corner', 'flange', 'D', or 'D/2',\n        [-]\n        \n    Returns\n    -------\n    C : float\n        Coefficient of discharge of the orifice, [-]\n\n    Notes\n    -----\n    The following limits apply to the orifice plate standard [1]_:\n        \n    The measured pressure difference for the orifice plate should be under\n    250 kPa.\n    \n    There are roughness limits as well; the roughness should be under 6 \n    micrometers, although there are many more conditions to that given in [1]_.\n        \n    For orifice plates with D and D/2 or corner pressure taps:\n        \n    * Orifice bore diameter muse be larger than 12.5 mm (0.5 inches)\n    * Pipe diameter between 50 mm and 1 m (2 to 40 inches)\n    * Beta between 0.1 and 0.75 inclusive\n    * Reynolds number larger than 5000 (for :math:`0.10 \\le \\beta \\le 0.56`)\n      or for :math:`\\beta \\ge 0.56, Re_D \\ge 16000\\beta^2`\n      \n    For orifice plates with flange pressure taps:\n\n    * Orifice bore diameter muse be larger than 12.5 mm (0.5 inches)\n    * Pipe diameter between 50 mm and 1 m (2 to 40 inches)\n    * Beta between 0.1 and 0.75 inclusive\n    * Reynolds number larger than 5000 and also larger than\n      :math:`170000\\beta^2 D`.\n      \n    This is also presented in Crane's TP410 (2009) publication, whereas the\n    1999 and 1982 editions showed only a graph for discharge coefficients.\n    \n    Examples\n    --------\n    >>> C_Reader_Harris_Gallagher(D=0.07391, Do=0.0222, rho=1.165, mu=1.85E-5, \n    ... m=0.12, taps='flange')\n    0.5990326277163659\n    \n    References\n    ----------\n    .. [1] American Society of Mechanical Engineers. Mfc-3M-2004 Measurement \n       Of Fluid Flow In Pipes Using Orifice, Nozzle, And Venturi. ASME, 2001.\n    .. [2] ISO 5167-2:2003 - Measurement of Fluid Flow by Means of Pressure \n       Differential Devices Inserted in Circular Cross-Section Conduits Running\n       Full -- Part 2: Orifice Plates.  \n    .. [3] Reader-Harris, M. J., \"The Equation for the Expansibility Factor for\n       Orifice Plates,\" Proceedings of FLOMEKO 1998, Lund, Sweden, 1998: \n       209-214.\n    .. [4] Reader-Harris, Michael. Orifice Plates and Venturi Tubes. Springer, \n       2015.\n    '''\n    A_pipe = 0.25*pi*D*D\n    v = m/(A_pipe*rho)\n    Re_D = rho*v*D/mu\n    Re_D_inv = 1.0/Re_D\n    \n    beta = Do/D\n    if taps == 'corner':\n        L1, L2_prime = 0.0, 0.0\n    elif taps == 'D' or taps == 'D/2':\n        L1 = 1.0\n        L2_prime = 0.47\n    elif taps == 'flange':\n        L1 = L2_prime = 0.0254/D\n    else:\n        raise Exception('Unsupported tap location')\n        \n    beta2 = beta*beta\n    beta4 = beta2*beta2\n    beta8 = beta4*beta4\n    \n    A = (19000.0*beta*Re_D_inv)**0.8\n    M2_prime = 2.0*L2_prime/(1.0 - beta)\n    \n    delta_C_upstream = ((0.043 + 0.080*exp(-1E1*L1) - 0.123*exp(-7.0*L1))\n            *(1.0 - 0.11*A)*beta4/(1.0 - beta4))\n    \n    # The max part is not in the ISO standard\n    t1 = log10(3700.*Re_D_inv)\n    if t1 < 0.0:\n        t1 = 0.0\n    delta_C_downstream = (-0.031*(M2_prime - 0.8*M2_prime**1.1)*beta**1.3\n                          *(1.0 + 8.0*t1))\n    \n    # C_inf is discharge coefficient with corner taps for infinite Re\n    # Cs, slope term, provides increase in discharge coefficient for lower\n    # Reynolds numbers.\n    x1 = (1E6*Re_D_inv)**0.3\n    x2 = 22.7 - 0.0047*Re_D\n    t2 = x1 if x1 > x2 else x2\n    # max term is not in the ISO standard\n    C_inf_C_s = (0.5961 + 0.0261*beta2 - 0.216*beta8 \n                 + 0.000521*(1E6*beta*Re_D_inv)**0.7\n                 + (0.0188 + 0.0063*A)*beta**3.5*(\n                 t2))\n    \n    C = (C_inf_C_s + delta_C_upstream + delta_C_downstream)\n    if D < 0.07112:\n        # Limit is 2.8 inches, .1 inches smaller than the internal diameter of\n        # a sched. 80 pipe.\n        # Suggested to be required not becausue of any effect of small\n        # diameters themselves, but because of edge radius differences.\n        # max term is given in [4]_ Reader-Harris, Michael book\n        # There is a check for t3 being negative and setting it to zero if so\n        # in some sources but that only occurs when t3 is exactly the limit\n        # (0.07112) so it is not needed\n        t3 = (2.8 - D/0.0254)\n        delta_C_diameter = 0.011*(0.75 - beta)*t3\n        C += delta_C_diameter\n    \n    return C", "response": "r A function to calculate the coefficient of discharge of the orifice based on the given fluid and density and viscosity of the fluid."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef discharge_coefficient_to_K(D, Do, C):\n    r'''Converts a discharge coefficient to a standard loss coefficient,\n    for use in computation of the actual pressure drop of an orifice or other\n    device.\n\n    .. math::\n        K = \\left[\\frac{\\sqrt{1-\\beta^4(1-C^2)}}{C\\beta^2} - 1\\right]^2\n        \n    Parameters\n    ----------\n    D : float\n        Upstream internal pipe diameter, [m]\n    Do : float\n        Diameter of orifice at flow conditions, [m]\n    C : float\n        Coefficient of discharge of the orifice, [-]\n\n    Returns\n    -------\n    K : float\n        Loss coefficient with respect to the velocity and density of the fluid\n        just upstream of the orifice, [-]\n\n    Notes\n    -----\n    If expansibility is used in the orifice calculation, the result will not\n    match with the specified pressure drop formula in [1]_; it can almost\n    be matched by dividing the calculated mass flow by the expansibility factor\n    and using that mass flow with the loss coefficient. \n    \n    Examples\n    --------\n    >>> discharge_coefficient_to_K(D=0.07366, Do=0.05, C=0.61512)\n    5.2314291729754\n    \n    References\n    ----------\n    .. [1] American Society of Mechanical Engineers. Mfc-3M-2004 Measurement \n       Of Fluid Flow In Pipes Using Orifice, Nozzle, And Venturi. ASME, 2001.\n    .. [2] ISO 5167-2:2003 - Measurement of Fluid Flow by Means of Pressure \n       Differential Devices Inserted in Circular Cross-Section Conduits Running\n       Full -- Part 2: Orifice Plates.\n    '''\n    beta = Do/D\n    beta2 = beta*beta\n    beta4 = beta2*beta2\n    return ((1.0 - beta4*(1.0 - C*C))**0.5/(C*beta2) - 1.0)**2", "response": "r Converts a discharge coefficient to a standard loss coefficient."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef nozzle_expansibility(D, Do, P1, P2, k, beta=None):\n    r'''Calculates the expansibility factor for a nozzle or venturi nozzle,\n    based on the geometry of the plate, measured pressures of the orifice, and\n    the isentropic exponent of the fluid.\n    \n    .. math::\n        \\epsilon = \\left\\{\\left(\\frac{\\kappa \\tau^{2/\\kappa}}{\\kappa-1}\\right)\n        \\left(\\frac{1 - \\beta^4}{1 - \\beta^4 \\tau^{2/\\kappa}}\\right)\n        \\left[\\frac{1 - \\tau^{(\\kappa-1)/\\kappa}}{1 - \\tau}\n        \\right] \\right\\}^{0.5}\n        \n    Parameters\n    ----------\n    D : float\n        Upstream internal pipe diameter, [m]\n    Do : float\n        Diameter of orifice of the venturi or nozzle, [m]\n    P1 : float\n        Static pressure of fluid upstream of orifice at the cross-section of\n        the pressure tap, [Pa]\n    P2 : float\n        Static pressure of fluid downstream of orifice at the cross-section of\n        the pressure tap, [Pa]\n    k : float\n        Isentropic exponent of fluid, [-]\n    beta : float, optional\n        Optional `beta` ratio, which is useful to specify for wedge meters or\n        flow meters which have a different beta ratio calculation, [-]\n\n    Returns\n    -------\n    expansibility : float\n        Expansibility factor (1 for incompressible fluids, less than 1 for\n        real fluids), [-]\n\n    Notes\n    -----\n    This formula was determined for the range of P2/P1 >= 0.75.\n\n    Examples\n    --------\n    >>> nozzle_expansibility(D=0.0739, Do=0.0222, P1=1E5, P2=9.9E4, k=1.4)\n    0.9945702344566746\n\n    References\n    ----------\n    .. [1] American Society of Mechanical Engineers. Mfc-3M-2004 Measurement \n       Of Fluid Flow In Pipes Using Orifice, Nozzle, And Venturi. ASME, 2001.\n    .. [2] ISO 5167-3:2003 - Measurement of Fluid Flow by Means of Pressure \n       Differential Devices Inserted in Circular Cross-Section Conduits Running\n       Full -- Part 3: Nozzles and Venturi Nozzles.\n    '''\n    if beta is None:\n        beta = Do/D\n    beta2 = beta*beta\n    beta4 = beta2*beta2\n    tau = P2/P1\n    term1 = k*tau**(2.0/k )/(k - 1.0)\n    term2 = (1.0 - beta4)/(1.0 - beta4*tau**(2.0/k))\n    try:\n        term3 = (1.0 - tau**((k - 1.0)/k))/(1.0 - tau)\n    except ZeroDivisionError:\n        '''Obtained with:\n            from sympy import *\n            tau, k = symbols('tau, k')\n            expr = (1 - tau**((k - 1)/k))/(1 - tau)\n            limit(expr, tau, 1)\n        '''\n        term3 = (k - 1.0)/k\n    return (term1*term2*term3)**0.5", "response": "r Returns the expansibility factor for a nozzle or venturi nozzle."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef C_ISA_1932_nozzle(D, Do, rho, mu, m):\n    r'''Calculates the coefficient of discharge of an ISA 1932 style nozzle \n    used for measuring flow rate of fluid, based on the geometry of the nozzle, \n    mass flow rate through the nozzle, and the density and viscosity of the\n    fluid.\n    \n    .. math::\n        C = 0.9900 - 0.2262\\beta^{4.1} - (0.00175\\beta^2 - 0.0033\\beta^{4.15})\n        \\left(\\frac{10^6}{Re_D}\\right)^{1.15}\n        \n    Parameters\n    ----------\n    D : float\n        Upstream internal pipe diameter, [m]\n    Do : float\n        Diameter of nozzle orifice at flow conditions, [m]\n    rho : float\n        Density of fluid at `P1`, [kg/m^3]\n    mu : float\n        Viscosity of fluid at `P1`, [Pa*s]\n    m : float\n        Mass flow rate of fluid through the nozzle, [kg/s]\n        \n    Returns\n    -------\n    C : float\n        Coefficient of discharge of the nozzle orifice, [-]\n\n    Notes\n    -----\n    \n    Examples\n    --------\n    >>> C_ISA_1932_nozzle(D=0.07391, Do=0.0422, rho=1.2, mu=1.8E-5, m=0.1)\n    0.9635849973250495\n    \n    References\n    ----------\n    .. [1] American Society of Mechanical Engineers. Mfc-3M-2004 Measurement \n       Of Fluid Flow In Pipes Using Orifice, Nozzle, And Venturi. ASME, 2001.\n    .. [2] ISO 5167-3:2003 - Measurement of Fluid Flow by Means of Pressure \n       Differential Devices Inserted in Circular Cross-Section Conduits Running\n       Full -- Part 3: Nozzles and Venturi Nozzles.\n    '''\n    A_pipe = pi/4.*D*D\n    v = m/(A_pipe*rho)\n    Re_D = rho*v*D/mu\n    beta = Do/D\n    C = (0.9900 - 0.2262*beta**4.1\n         - (0.00175*beta**2 - 0.0033*beta**4.15)*(1E6/Re_D)**1.15)\n    return C", "response": "A function that calculates the coefficient of discharge of an ISA 1932 style nozzle given the diameter of the nozzle orifice at flow conditions rho mu and m."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dP_venturi_tube(D, Do, P1, P2):\n    r'''Calculates the non-recoverable pressure drop of a venturi tube \n    differential pressure meter based on the pressure drop and the geometry of \n    the venturi meter.\n    \n    .. math::\n        \\epsilon =  \\frac{\\Delta\\bar w }{\\Delta P}\n        \n    The :math:`\\epsilon` value is looked up in a table of values as a function\n    of beta ratio and upstream pipe diameter (roughness impact).\n        \n    Parameters\n    ----------\n    D : float\n        Upstream internal pipe diameter, [m]\n    Do : float\n        Diameter of venturi tube at flow conditions, [m]\n    P1 : float\n        Static pressure of fluid upstream of venturi tube at the cross-section \n        of the pressure tap, [Pa]\n    P2 : float\n        Static pressure of fluid downstream of venturi tube at the\n         cross-section of the pressure tap, [Pa]\n\n    Returns\n    -------\n    dP : float\n        Non-recoverable pressure drop of the venturi tube, [Pa]\n\n    Notes\n    -----\n    The recoverable pressure drop should be recovered by 6 pipe diameters \n    downstream of the venturi tube.\n    \n    Note there is some information on the effect of Reynolds number as well\n    in [1]_ and [2]_, with a curve showing an increased pressure drop\n    from 1E5-6E5 to with a decreasing multiplier from 1.75 to 1; the multiplier\n    is 1 for higher Reynolds numbers. This is not currently included in this\n    implementation.\n    \n    Examples\n    --------\n    >>> dP_venturi_tube(D=0.07366, Do=0.05, P1=200000.0, P2=183000.0)\n    1788.5717754177406\n    \n    References\n    ----------\n    .. [1] American Society of Mechanical Engineers. Mfc-3M-2004 Measurement \n       Of Fluid Flow In Pipes Using Orifice, Nozzle, And Venturi. ASME, 2001.\n    .. [2] ISO 5167-4:2003 - Measurement of Fluid Flow by Means of Pressure \n       Differential Devices Inserted in Circular Cross-Section Conduits Running\n       Full -- Part 4: Venturi Tubes.\n    '''\n    # Effect of Re is not currently included\n    beta = Do/D\n    epsilon_D65 = interp(beta, venturi_tube_betas, venturi_tube_dP_high)\n    epsilon_D500 = interp(beta, venturi_tube_betas, venturi_tube_dP_low)\n    epsilon = interp(D, D_bound_venturi_tube, [epsilon_D65, epsilon_D500])\n    return epsilon*(P1 - P2)", "response": "r Returns the non - recoverable pressure drop of a venturi tube at the given diameter."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef cone_meter_expansibility_Stewart(D, Dc, P1, P2, k):\n    r'''Calculates the expansibility factor for a cone flow meter,\n    based on the geometry of the cone meter, measured pressures of the orifice, \n    and the isentropic exponent of the fluid. Developed in [1]_, also shown\n    in [2]_.\n    \n    .. math::\n        \\epsilon = 1 - (0.649 + 0.696\\beta^4) \\frac{\\Delta P}{\\kappa P_1}\n\n    Parameters\n    ----------\n    D : float\n        Upstream internal pipe diameter, [m]\n    Dc : float\n        Diameter of the largest end of the cone meter, [m]\n    P1 : float\n        Static pressure of fluid upstream of cone meter at the cross-section of\n        the pressure tap, [Pa]\n    P2 : float\n        Static pressure of fluid at the end of the center of the cone pressure \n        tap, [Pa]\n    k : float\n        Isentropic exponent of fluid, [-]\n\n    Returns\n    -------\n    expansibility : float\n        Expansibility factor (1 for incompressible fluids, less than 1 for\n        real fluids), [-]\n\n    Notes\n    -----\n    This formula was determined for the range of P2/P1 >= 0.75; the only gas\n    used to determine the formula is air.\n\n    Examples\n    --------\n    >>> cone_meter_expansibility_Stewart(D=1, Dc=0.9, P1=1E6, P2=8.5E5, k=1.2)\n    0.9157343\n\n    References\n    ----------\n    .. [1] Stewart, D. G., M. Reader-Harris, and NEL Dr RJW Peters. \"Derivation\n       of an Expansibility Factor for the V-Cone Meter.\" In Flow Measurement \n       International Conference, Peebles, Scotland, UK, 2001.\n    .. [2] ISO 5167-5:2016 - Measurement of Fluid Flow by Means of Pressure \n       Differential Devices Inserted in Circular Cross-Section Conduits Running\n       Full -- Part 5: Cone meters.\n    '''\n    dP = P1 - P2\n    beta = diameter_ratio_cone_meter(D, Dc)\n    return 1.0 - (0.649 + 0.696*beta**4)*dP/(k*P1)", "response": "r Returns the expansibility factor for a given cone flow meter."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef dP_cone_meter(D, Dc, P1, P2):\n    r'''Calculates the non-recoverable pressure drop of a cone meter\n    based on the measured pressures before and at the cone end, and the \n    geometry of the cone meter according to [1]_.\n    \n    .. math::\n        \\Delta \\bar \\omega = (1.09 - 0.813\\beta)\\Delta P\n        \n    Parameters\n    ----------\n    D : float\n        Upstream internal pipe diameter, [m]\n    Dc : float\n        Diameter of the largest end of the cone meter, [m]\n    P1 : float\n        Static pressure of fluid upstream of cone meter at the cross-section of\n        the pressure tap, [Pa]\n    P2 : float\n        Static pressure of fluid at the end of the center of the cone pressure \n        tap, [Pa]\n\n    Returns\n    -------\n    dP : float\n        Non-recoverable pressure drop of the orifice plate, [Pa]\n\n    Notes\n    -----\n    The recoverable pressure drop should be recovered by 6 pipe diameters \n    downstream of the cone meter.\n    \n    Examples\n    --------\n    >>> dP_cone_meter(1, .7, 1E6, 9.5E5)\n    25470.093437973323\n    \n    References\n    ----------\n    .. [1] ISO 5167-5:2016 - Measurement of Fluid Flow by Means of Pressure \n       Differential Devices Inserted in Circular Cross-Section Conduits Running\n       Full -- Part 5: Cone meters.\n    '''\n    dP = P1 - P2\n    beta = diameter_ratio_cone_meter(D, Dc)\n    return (1.09 - 0.813*beta)*dP", "response": "r Returns the non - recoverable pressure drop of a cone meter."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef diameter_ratio_wedge_meter(D, H):\n    r'''Calculates the diameter ratio `beta` used to characterize a wedge \n    flow meter as given in [1]_ and [2]_.\n    \n    .. math::\n        \\beta = \\left(\\frac{1}{\\pi}\\left\\{\\arccos\\left[1 - \\frac{2H}{D}\n        \\right] - 2 \\left[1 - \\frac{2H}{D}\n        \\right]\\left(\\frac{H}{D} - \\left[\\frac{H}{D}\\right]^2\n        \\right)^{0.5}\\right\\}\\right)^{0.5}\n        \n    Parameters\n    ----------\n    D : float\n        Upstream internal pipe diameter, [m]\n    H : float\n        Portion of the diameter of the clear segment of the pipe up to the \n        wedge blocking flow; the height of the pipe up to the wedge, [m]\n    \n    Returns\n    -------\n    beta : float\n        Wedge meter diameter ratio, [-]\n\n    Notes\n    -----\n    \n    Examples\n    --------\n    >>> diameter_ratio_wedge_meter(D=0.2027, H=0.0608)\n    0.5022531424646643\n    \n    References\n    ----------\n    .. [1] Hollingshead, Colter. \"Discharge Coefficient Performance of Venturi,\n       Standard Concentric Orifice Plate, V-Cone, and Wedge Flow Meters at \n       Small Reynolds Numbers.\" May 1, 2011. \n       https://digitalcommons.usu.edu/etd/869.\n    .. [2] IntraWedge WEDGE FLOW METER Type: IWM. January 2011.\n       http://www.intra-automation.com/download.php?file=pdf/products/technical_information/en/ti_iwm_en.pdf\n    '''\n    H_D = H/D\n    t0 = 1.0 - 2.0*H_D\n    t1 = acos(t0)\n    t2 = 2.0*(t0)\n    t3 = (H_D - H_D*H_D)**0.5\n    t4 = t1 - t2*t3\n    return (1./pi*t4)**0.5", "response": "r Calculates the diameter ratio beta used to characterize a wedge \n    flow meter."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef dP_wedge_meter(D, H, P1, P2):\n    r'''Calculates the non-recoverable pressure drop of a wedge meter\n    based on the measured pressures before and at the wedge meter, and the \n    geometry of the wedge meter according to [1]_.\n    \n    .. math::\n        \\Delta \\bar \\omega = (1.09 - 0.79\\beta)\\Delta P\n        \n    Parameters\n    ----------\n    D : float\n        Upstream internal pipe diameter, [m]\n    H : float\n        Portion of the diameter of the clear segment of the pipe up to the \n        wedge blocking flow; the height of the pipe up to the wedge, [m]\n    P1 : float\n        Static pressure of fluid upstream of wedge meter at the cross-section \n        of the pressure tap, [Pa]\n    P2 : float\n        Static pressure of fluid at the end of the wedge meter pressure tap, [\n        Pa]\n\n    Returns\n    -------\n    dP : float\n        Non-recoverable pressure drop of the wedge meter, [Pa]\n\n    Notes\n    -----\n    The recoverable pressure drop should be recovered by 5 pipe diameters \n    downstream of the wedge meter.\n    \n    Examples\n    --------\n    >>> dP_wedge_meter(1, .7, 1E6, 9.5E5)\n    20344.849697483587\n    \n    References\n    ----------\n    .. [1] ISO/DIS 5167-6 - Measurement of Fluid Flow by Means of Pressure \n       Differential Devices Inserted in Circular Cross-Section Conduits Running \n       Full -- Part 6: Wedge Meters.\n    '''\n    dP = P1 - P2\n    beta = diameter_ratio_wedge_meter(D, H)\n    return (1.09 - 0.79*beta)*dP", "response": "r Returns the non - recoverable pressure drop of a wedge meter."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef C_Reader_Harris_Gallagher_wet_venturi_tube(mg, ml, rhog, rhol, D, Do, H=1):\n    r'''Calculates the coefficient of discharge of the wet gas venturi tube \n    based on the  geometry of the tube, mass flow rates of liquid and vapor\n    through the tube, the density of the liquid and gas phases, and an \n    adjustable coefficient `H`.\n    \n    .. math::\n        C = 1 - 0.0463\\exp(-0.05Fr_{gas, th}) \\cdot \\min\\left(1, \n        \\sqrt{\\frac{X}{0.016}}\\right)\n        \n    .. math::\n        Fr_{gas, th} = \\frac{Fr_{\\text{gas, densionetric }}}{\\beta^{2.5}}\n        \n    .. math::\n        \\phi = \\sqrt{1 + C_{Ch} X + X^2}\n        \n    .. math::\n        C_{Ch} = \\left(\\frac{\\rho_l}{\\rho_{1,g}}\\right)^n + \n        \\left(\\frac{\\rho_{1, g}}{\\rho_{l}}\\right)^n\n        \n    .. math::\n        n = \\max\\left[0.583 - 0.18\\beta^2 - 0.578\\exp\\left(\\frac{-0.8 \n        Fr_{\\text{gas, densiometric}}}{H}\\right),0.392 - 0.18\\beta^2 \\right]\n            \n    .. math::\n        X = \\left(\\frac{m_l}{m_g}\\right) \\sqrt{\\frac{\\rho_{1,g}}{\\rho_l}}\n        \n    .. math::\n        {Fr_{\\text{gas, densiometric}}} = \\frac{v_{gas}}{\\sqrt{gD}}\n        \\sqrt{\\frac{\\rho_{1,g}}{\\rho_l - \\rho_{1,g}}}\n        =  \\frac{4m_g}{\\rho_{1,g} \\pi D^2 \\sqrt{gD}}\n        \\sqrt{\\frac{\\rho_{1,g}}{\\rho_l - \\rho_{1,g}}}\n        \n    Parameters\n    ----------\n    mg : float\n        Mass flow rate of gas through the venturi tube, [kg/s]\n    ml : float\n        Mass flow rate of liquid through the venturi tube, [kg/s]\n    rhog : float\n        Density of gas at `P1`, [kg/m^3]\n    rhol : float\n        Density of liquid at `P1`, [kg/m^3]\n    D : float\n        Upstream internal pipe diameter, [m]\n    Do : float\n        Diameter of venturi tube at flow conditions, [m]\n    H : float, optional\n        A surface-tension effect coefficient used to adjust for different \n        fluids, (1 for a hydrocarbon liquid, 1.35 for water, 0.79 for water in \n        steam) [-]\n\n    Returns\n    -------\n    C : float\n        Coefficient of discharge of the wet gas venturi tube flow meter\n        (includes flow rate of gas ONLY), [-]\n\n    Notes\n    -----\n    This model has more error than single phase differential pressure meters.\n    The model was first published in [1]_, and became ISO 11583 later.\n    \n    The limits of this correlation according to [2]_ are as follows:\n        \n    .. math::\n        0.4 \\le \\beta \\le 0.75\n        \n    .. math::\n        0 < X \\le 0.3\n        \n    .. math::\n        Fr_{gas, th} > 3\n        \n    .. math::\n        \\frac{\\rho_g}{\\rho_l} > 0.02\n        \n    .. math::\n        D \\ge 50 \\text{ mm}\n    \n    Examples\n    --------\n    >>> C_Reader_Harris_Gallagher_wet_venturi_tube(mg=5.31926, ml=5.31926/2, \n    ... rhog=50.0, rhol=800., D=.1, Do=.06, H=1)\n    0.9754210845876333\n    \n    References\n    ----------\n    .. [1] Reader-harris, Michael, and Tuv Nel. An Improved Model for \n       Venturi-Tube Over-Reading in Wet Gas, 2009. \n    .. [2] ISO/TR 11583:2012 Measurement of Wet Gas Flow by Means of Pressure \n       Differential Devices Inserted in Circular Cross-Section Conduits.\n    '''\n    V = 4*mg/(rhog*pi*D**2)\n    Frg =  Froude_densimetric(V, L=D, rho1=rhol, rho2=rhog, heavy=False)\n    beta = Do/D\n    beta2 = beta*beta\n    Fr_gas_th = Frg*beta**-2.5\n    \n    n = max(0.583 - 0.18*beta2 - 0.578*exp(-0.8*Frg/H), \n            0.392 - 0.18*beta2)\n    \n    C_Ch = (rhol/rhog)**n + (rhog/rhol)**n\n    X =  ml/mg*(rhog/rhol)**0.5\n    OF = (1.0 + C_Ch*X + X*X)**0.5\n    \n    C = 1.0 - 0.0463*exp(-0.05*Fr_gas_th)*min(1.0, (X/0.016)**0.5)\n    return C", "response": "r A function that calculates the coefficient of discharge of the wet gas venturi tube."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dP_Reader_Harris_Gallagher_wet_venturi_tube(D, Do, P1, P2, ml, mg, rhol, \n                                                rhog, H=1):\n    r'''Calculates the non-recoverable pressure drop of a wet gas venturi \n    nozzle based on the pressure drop and the geometry of the venturi nozzle,\n    the mass flow rates of liquid and gas through it, the densities of the\n    vapor and liquid phase, and an adjustable coefficient `H`.\n    \n    .. math::\n        Y = \\frac{\\Delta \\bar \\omega}{\\Delta P} - 0.0896 - 0.48\\beta^9\n        \n    .. math::\n        Y_{max} = 0.61\\exp\\left[-11\\frac{\\rho_{1,g}}{\\rho_l}\n        - 0.045 \\frac{Fr_{gas}}{H}\\right]\n        \n    .. math::\n        \\frac{Y}{Y_{max}} = 1 - \\exp\\left[-35 X^{0.75} \\exp\n        \\left( \\frac{-0.28Fr_{gas}}{H}\\right)\\right]\n            \n    .. math::\n        X = \\left(\\frac{m_l}{m_g}\\right) \\sqrt{\\frac{\\rho_{1,g}}{\\rho_l}}\n        \n    .. math::\n        {Fr_{\\text{gas, densiometric}}} = \\frac{v_{gas}}{\\sqrt{gD}}\n        \\sqrt{\\frac{\\rho_{1,g}}{\\rho_l - \\rho_{1,g}}}\n        =  \\frac{4m_g}{\\rho_{1,g} \\pi D^2 \\sqrt{gD}}\n        \\sqrt{\\frac{\\rho_{1,g}}{\\rho_l - \\rho_{1,g}}}\n        \n    Parameters\n    ----------\n    D : float\n        Upstream internal pipe diameter, [m]\n    Do : float\n        Diameter of venturi tube at flow conditions, [m]\n    P1 : float\n        Static pressure of fluid upstream of venturi tube at the cross-section \n        of the pressure tap, [Pa]\n    P2 : float\n        Static pressure of fluid downstream of venturi tube at the cross-\n        section of the pressure tap, [Pa]\n    ml : float\n        Mass flow rate of liquid through the venturi tube, [kg/s]\n    mg : float\n        Mass flow rate of gas through the venturi tube, [kg/s]\n    rhol : float\n        Density of liquid at `P1`, [kg/m^3]\n    rhog : float\n        Density of gas at `P1`, [kg/m^3]\n    H : float, optional\n        A surface-tension effect coefficient used to adjust for different \n        fluids, (1 for a hydrocarbon liquid, 1.35 for water, 0.79 for water in \n        steam) [-]\n\n    Returns\n    -------\n    C : float\n        Coefficient of discharge of the wet gas venturi tube flow meter\n        (includes flow rate of gas ONLY), [-]\n\n    Notes\n    -----\n    The model was first published in [1]_, and became ISO 11583 later.\n    \n    Examples\n    --------\n    >>> dP_Reader_Harris_Gallagher_wet_venturi_tube(D=.1, Do=.06, H=1, \n    ... P1=6E6, P2=6E6-5E4, ml=5.31926/2, mg=5.31926, rhog=50.0, rhol=800.,)\n    16957.43843129572\n    \n    References\n    ----------\n    .. [1] Reader-harris, Michael, and Tuv Nel. An Improved Model for \n       Venturi-Tube Over-Reading in Wet Gas, 2009. \n    .. [2] ISO/TR 11583:2012 Measurement of Wet Gas Flow by Means of Pressure \n       Differential Devices Inserted in Circular Cross-Section Conduits.\n    '''\n    dP = P1 - P2\n    beta = Do/D\n    X =  ml/mg*(rhog/rhol)**0.5\n\n    V = 4*mg/(rhog*pi*D**2)\n    Frg =  Froude_densimetric(V, L=D, rho1=rhol, rho2=rhog, heavy=False)\n\n    Y_ratio = 1.0 - exp(-35.0*X**0.75*exp(-0.28*Frg/H))\n    Y_max = 0.61*exp(-11.0*rhog/rhol - 0.045*Frg/H)\n    Y = Y_max*Y_ratio\n    rhs = -0.0896 - 0.48*beta**9\n    dw = dP*(Y - rhs)\n    return dw", "response": "r Calculates the non - recoverable pressure drop of a wet venturi nozzle at flow conditions and pressure drops of liquid and liquid phase through it."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef differential_pressure_meter_dP(D, D2, P1, P2, C=None, \n                                   meter_type=ISO_5167_ORIFICE):\n    r'''Calculates either the non-recoverable pressure drop of a differential\n    pressure flow meter based on the geometry of the meter, measured pressures \n    of the meter, and for most models the meter discharge coefficient.\n    \n    Parameters\n    ----------\n    D : float\n        Upstream internal pipe diameter, [m]\n    D2 : float\n        Diameter of orifice, or venturi meter orifice, or flow tube orifice,\n        or cone meter end diameter, or wedge meter fluid flow height, [m]\n    P1 : float\n        Static pressure of fluid upstream of differential pressure meter at the\n        cross-section of the pressure tap, [Pa]\n    P2 : float\n        Static pressure of fluid downstream of differential pressure meter or \n        at the prescribed location (varies by type of meter) [Pa]\n    C : float, optional\n        Coefficient of discharge (used only in orifice plates, and venturi\n        nozzles), [-]\n    meter_type : str, optional\n        One of ('ISO 5167 orifice', 'long radius nozzle', 'ISA 1932 nozzle', \n        'as cast convergent venturi tube', \n        'machined convergent venturi tube', \n        'rough welded convergent venturi tube', 'cone meter', 'cone meter'), \n        [-]\n        \n    Returns\n    -------\n    dP : float\n        Non-recoverable pressure drop of the differential pressure flow\n        meter, [Pa]\n\n    Notes\n    -----\n    See the appropriate functions for the documentation for the formulas and\n    references used in each method.\n    \n    Wedge meters, and venturi nozzles do not have standard formulas available\n    for pressure drop computation.\n        \n    Examples\n    --------\n    >>> differential_pressure_meter_dP(D=0.07366, D2=0.05, P1=200000.0, \n    ... P2=183000.0, meter_type='as cast convergent venturi tube')\n    1788.5717754177406\n    '''\n    if meter_type == ISO_5167_ORIFICE:\n        dP = dP_orifice(D=D, Do=D2, P1=P1, P2=P2, C=C)\n\n    elif meter_type == LONG_RADIUS_NOZZLE:\n        dP = dP_orifice(D=D, Do=D2, P1=P1, P2=P2, C=C)\n    elif meter_type == ISA_1932_NOZZLE:\n        dP = dP_orifice(D=D, Do=D2, P1=P1, P2=P2, C=C)\n    elif meter_type == VENTURI_NOZZLE:\n        raise Exception(NotImplemented)\n    \n    elif meter_type == AS_CAST_VENTURI_TUBE:\n        dP = dP_venturi_tube(D=D, Do=D2, P1=P1, P2=P2)\n    elif meter_type == MACHINED_CONVERGENT_VENTURI_TUBE:\n        dP = dP_venturi_tube(D=D, Do=D2, P1=P1, P2=P2)\n    elif meter_type == ROUGH_WELDED_CONVERGENT_VENTURI_TUBE:\n        dP = dP_venturi_tube(D=D, Do=D2, P1=P1, P2=P2)\n        \n    elif meter_type == CONE_METER:\n        dP = dP_cone_meter(D=D, Dc=D2, P1=P1, P2=P2)\n    elif meter_type == WEDGE_METER:\n        dP = dP_wedge_meter(D=D, H=D2, P1=P1, P2=P2)\n    return dP", "response": "r Returns the non -recoverable pressure drop of a differential flow meter."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef psd_spacing(d_min=None, d_max=None, pts=20, method='logarithmic'):\n    r'''Create a particle spacing mesh in one of several ways for use in\n    modeling discrete particle size distributions. The allowable meshes are\n    'linear', 'logarithmic', a geometric series specified by a Renard number\n    such as 'R10', or the meshes available in one of several sieve standards.\n    \n    Parameters\n    ----------\n    d_min : float, optional\n        The minimum diameter at which the mesh starts, [m]\n    d_max : float, optional\n        The maximum diameter at which the mesh ends, [m]\n    pts : int, optional\n        The number of points to return for the mesh (note this is not respected\n        by sieve meshes), [-]\n    method : str, optional\n        Either 'linear', 'logarithmic', a Renard number like 'R10' or 'R5' or\n        'R2.5', or one of the sieve standards 'ISO 3310-1 R40/3', \n        'ISO 3310-1 R20', 'ISO 3310-1 R20/3', 'ISO 3310-1', 'ISO 3310-1 R10', \n        'ASTM E11', [-]\n\n    Returns\n    -------\n    ds : list[float]\n        The generated mesh diameters, [m]\n\n    Notes\n    -----\n    Note that when specifying a Renard series, only one of `d_min` or `d_max` can\n    be respected! Provide only one of those numbers. \n    \n    Note that when specifying a sieve standard the number of points is not\n    respected!\n\n    Examples\n    --------\n    >>> psd_spacing(d_min=5e-5, d_max=5e-4, method='ISO 3310-1 R20/3')\n    [6.3e-05, 9e-05, 0.000125, 0.00018, 0.00025, 0.000355, 0.0005]\n\n    References\n    ----------\n    .. [1] ASTM E11 - 17 - Standard Specification for Woven Wire Test Sieve \n       Cloth and Test Sieves.\n    .. [2] ISO 3310-1:2016 - Test Sieves -- Technical Requirements and Testing\n       -- Part 1: Test Sieves of Metal Wire Cloth.\n    '''\n    if method == 'logarithmic':\n        return logspace(log10(d_min), log10(d_max), pts)\n    elif method == 'linear':\n        return linspace(d_min, d_max, pts)\n    elif method[0] in ('R', 'r'):\n        ratio = 10**(1.0/float(method[1:]))\n        if d_min is not None and d_max is not None:\n            raise Exception('For geometric (Renard) series, only '\n                            'one of `d_min` and `d_max` should be provided')\n        if d_min is not None:\n            ds = [d_min]\n            for i in range(pts-1):\n                ds.append(ds[-1]*ratio)\n            return ds\n        elif d_max is not None:\n            ds = [d_max]\n            for i in range(pts-1):\n                ds.append(ds[-1]/ratio)\n            return list(reversed(ds))\n    elif method in sieve_spacing_options:\n        l = sieve_spacing_options[method]\n        ds = []\n        for sieve in l:\n           if  d_min <= sieve.opening <= d_max:\n               ds.append(sieve.opening)\n        return list(reversed(ds))\n    else:\n        raise Exception('Method not recognized')", "response": "r Creates a particle spacing mesh in one of several ways for use in a non - discrete particle size distributions."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pdf_lognormal_basis_integral(d, d_characteristic, s, n):\n    r'''Calculates the integral of the multiplication of d^n by the lognormal\n    pdf, given a particle diameter `d`, characteristic particle\n    diameter `d_characteristic`, distribution standard deviation `s`, and\n    exponent `n`.\n    \n    .. math::\n        \\int d^n\\cdot q(d)\\; dd = -\\frac{1}{2} \\exp\\left(\\frac{s^2 n^2}{2}\n        \\right)d^n \\left(\\frac{d}{d_{characteristic}}\\right)^{-n}\n        \\text{erf}\\left[\\frac{s^2 n - \\log(d/d_{characteristic})}\n        {\\sqrt{2} s} \\right]\n        \n    This is the crucial integral required for interconversion between different\n    bases such as number density (denoted :math:`q_0(d)`), length density \n    (:math:`q_1(d)`), surface area density (:math:`q_2(d)`), or volume density \n    (:math:`q_3(d)`).\n        \n    Parameters\n    ----------\n    d : float\n        Specified particle diameter, [m]\n    d_characteristic : float\n        Characteristic particle diameter; often D[3, 3] is used for this\n        purpose but not always, [m]\n    s : float\n        Distribution standard deviation, [-]    \n    n : int\n        Exponent of the multiplied n\n\n    Returns\n    -------\n    pdf_basis_integral : float\n        Integral of lognormal pdf multiplied by d^n, [-]\n\n    Notes\n    -----\n    This integral has been verified numerically. This integral is itself\n    integrated, so it is crucial to obtain an analytical form for at least\n    this integral.\n    \n    Note overflow or zero division issues may occur for very large values of \n    `s`, larger than 10. No mathematical limit was able to be obtained with \n    a CAS.\n\n    Examples\n    --------\n    >>> pdf_lognormal_basis_integral(d=1E-4, d_characteristic=1E-5, s=1.1, n=-2)\n    56228306549.26362\n    '''\n    try:\n        s2 = s*s\n        t0 = exp(s2*n*n*0.5)\n        d_ratio = d/d_characteristic\n        t1 = (d/(d_ratio))**n\n        t2 = erf((s2*n - log(d_ratio))/(2.**0.5*s))\n        return -0.5*t0*t1*t2\n    except (OverflowError, ZeroDivisionError, ValueError):\n        return pdf_lognormal_basis_integral(d=1E-80, d_characteristic=d_characteristic, s=s, n=n)", "response": "r Returns the integral of the multiplication of d^n by the lognormal pdf."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef pdf_Gates_Gaudin_Schuhman(d, d_characteristic, m):\n    r'''Calculates the probability density of a particle\n    distribution following the Gates, Gaudin and Schuhman (GGS) model given a \n    particle diameter `d`, characteristic (maximum) particle\n    diameter `d_characteristic`, and exponent `m`.\n    \n    .. math::\n        q(d) = \\frac{n}{d}\\left(\\frac{d}{d_{characteristic}}\\right)^m \n        \\text{ if } d < d_{characteristic} \\text{ else } 0\n        \n    Parameters\n    ----------\n    d : float\n        Specified particle diameter, [m]\n    d_characteristic : float\n        Characteristic particle diameter; in this model, it is the largest \n        particle size diameter in the distribution, [m]\n    m : float\n        Particle size distribution exponent, [-]    \n\n    Returns\n    -------\n    pdf : float\n        GGS probability density function, [-]\n\n    Notes\n    -----\n    The characteristic diameter can be in terns of number density (denoted \n    :math:`q_0(d)`), length density (:math:`q_1(d)`), surface area density\n    (:math:`q_2(d)`), or volume density (:math:`q_3(d)`). Volume density is\n    most often used. Interconversions among the distributions is possible but\n    tricky.\n\n    Examples\n    --------\n    >>> pdf_Gates_Gaudin_Schuhman(d=2E-4, d_characteristic=1E-3, m=2.3)\n    283.8355768512045\n\n    References\n    ----------\n    .. [1] Schuhmann, R., 1940. Principles of Comminution, I-Size Distribution \n       and Surface Calculations. American Institute of Mining, Metallurgical \n       and Petroleum Engineers Technical Publication 1189. Mining Technology, \n       volume 4, p. 1-11.\n    .. [2] Bayat, Hossein, Mostafa Rastgo, Moharram Mansouri Zadeh, and Harry \n       Vereecken. \"Particle Size Distribution Models, Their Characteristics and\n       Fitting Capability.\" Journal of Hydrology 529 (October 1, 2015): 872-89.\n    '''\n    if d <= d_characteristic:\n        return m/d*(d/d_characteristic)**m\n    else:\n        return 0.0", "response": "PDF for Gaudin and Schuhman model."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates the integral of the multiplication of d^n by the Gaudin and Schuhman model given a particle diameter d_characteristic and exponent m and n.", "response": "def pdf_Gates_Gaudin_Schuhman_basis_integral(d, d_characteristic, m, n):\n    r'''Calculates the integral of the multiplication of d^n by the Gates, \n    Gaudin and Schuhman (GGS) model given a particle diameter `d`,\n    characteristic (maximum) particle diameter `d_characteristic`, and exponent\n    `m`.\n    \n    .. math::\n        \\int d^n\\cdot q(d)\\; dd =\\frac{m}{m+n} d^n \\left(\\frac{d}\n        {d_{characteristic}}\\right)^m\n        \n    Parameters\n    ----------\n    d : float\n        Specified particle diameter, [m]\n    d_characteristic : float\n        Characteristic particle diameter; in this model, it is the largest \n        particle size diameter in the distribution, [m]\n    m : float\n        Particle size distribution exponent, [-]    \n    n : int\n        Exponent of the multiplied n, [-]\n\n    Returns\n    -------\n    pdf_basis_integral : float\n        Integral of Rosin Rammler pdf multiplied by d^n, [-]\n\n    Notes\n    -----\n    This integral does not have any numerical issues as `d` approaches 0.\n\n    Examples\n    --------\n    >>> pdf_Gates_Gaudin_Schuhman_basis_integral(d=2E-4, d_characteristic=1E-3, m=2.3, n=-3)\n    -10136984887.543015\n    '''\n    return m/(m+n)*d**n*(d/d_characteristic)**m"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pdf_Rosin_Rammler_basis_integral(d, k, m, n):\n    r'''Calculates the integral of the multiplication of d^n by the Rosin\n    Rammler (RR) pdf, given a particle diameter `d`, and the two parameters `k`\n    and `m`.\n    \n    .. math::\n        \\int d^n\\cdot q(d)\\; dd =-d^{m+n} k(d^mk)^{-\\frac{m+n}{m}}\\Gamma\n        \\left(\\frac{m+n}{m}\\right)\\text{gammaincc}\\left[\\left(\\frac{m+n}{m}\n        \\right), kd^m\\right]\n    \n    Parameters\n    ----------\n    d : float\n        Specified particle diameter, [m]\n    k : float\n        Parameter in the model, [(1/m)^m]\n    m : float\n        Parameter in the model, [-]\n    n : int\n        Exponent of the multiplied n, [-]\n\n    Returns\n    -------\n    pdf_basis_integral : float\n        Integral of Rosin Rammler pdf multiplied by d^n, [-]\n\n    Notes\n    -----\n    This integral was derived using a CAS, and verified numerically.\n    The `gammaincc` function is that from scipy.special, and `gamma` from the\n    same.\n    \n    For very high powers of `n` or `m` when the diameter is very low, \n    exceptions may occur.\n\n    Examples\n    --------\n    >>> \"{:g}\".format(pdf_Rosin_Rammler_basis_integral(5E-2, 200, 2, 3))\n    '-0.000452399'\n    '''\n    # Also not able to compute the limit for d approaching 0.\n    try:\n        a = (m + n)/m\n        x = d**m*k\n        t1 = float(gamma(a))*float(gammaincc(a, x))\n        return (-d**(m+n)*k*(d**m*k)**(-a))*t1\n    except (OverflowError, ZeroDivisionError) as e:\n        if d == 1E-40:\n            raise e\n        return pdf_Rosin_Rammler_basis_integral(1E-40, k, m, n)", "response": "r Calculates the integral of the multiplication of d^n by the Rosin Rammler pdf given a particle diameter d and the two parameters k and m and n."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef pdf(self, d, n=None):\n        r'''Computes the probability density function of a\n        continuous particle size distribution at a specified particle diameter,\n        an optionally in a specified basis. The evaluation function varies with\n        the distribution chosen. The interconversion between distribution \n        orders is performed using the following formula [1]_:\n            \n        .. math::\n            q_s(d) = \\frac{x^{(s-r)} q_r(d) dd}\n            { \\int_0^\\infty d^{(s-r)} q_r(d) dd}\n        \n        Parameters\n        ----------\n        d : float\n            Particle size diameter, [m]\n        n : int, optional\n            None (for the `order` specified when the distribution was created),\n            0 (number), 1 (length), 2 (area), 3 (volume/mass),\n            or any integer, [-]\n\n        Returns\n        -------\n        pdf : float\n            The probability density function at the specified diameter and\n            order, [-]\n            \n        Notes\n        -----\n        The pdf order conversions are typically available analytically after \n        some work. They have been verified numerically. See the various\n        functions with names ending with 'basis_integral' for the formulations.\n        The distributions normally do not have analytical limits for diameters\n        of 0 or infinity, but large values suffice to capture the area of the\n        integral.\n        \n        Examples\n        --------\n        >>> psd = PSDLognormal(s=0.5, d_characteristic=5E-6, order=3)\n        >>> psd.pdf(1e-5)\n        30522.765209509154\n        >>> psd.pdf(1e-5, n=3)\n        30522.765209509154\n        >>> psd.pdf(1e-5, n=0)\n        1238.661379483343\n        \n        References\n        ----------\n        .. [1] Masuda, Hiroaki, Ko Higashitani, and Hideto Yoshida. Powder \n           Technology: Fundamentals of Particles, Powder Beds, and Particle \n           Generation. CRC Press, 2006.\n        '''\n        ans = self._pdf(d=d)\n        if n is not None and n != self.order:\n            power = n - self.order\n            numerator = d**power*ans\n            denominator = self._pdf_basis_integral_definite(d_min=0.0, d_max=self.d_excessive, n=power)\n            ans = numerator/denominator\n        # Handle splines which might go below zero\n        ans = max(ans, 0.0)\n        if self.truncated:\n            if d < self.d_min or d > self.d_max:\n                return 0.0\n            ans = (ans)/(self._cdf_d_max - self._cdf_d_min)\n        return ans", "response": "r Returns the probability density function of continuous particle size distribution at a specified diameter and optional basis number n."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ds_discrete(self, d_min=None, d_max=None, pts=20, limit=1e-9, \n                    method='logarithmic'):\n        r'''Create a particle spacing mesh to perform calculations with, \n        according to one of several ways. The allowable meshes are\n        'linear', 'logarithmic', a geometric series specified by a Renard \n        number such as 'R10', or the meshes available in one of several sieve \n        standards.\n        \n        Parameters\n        ----------\n        d_min : float, optional\n            The minimum diameter at which the mesh starts, [m]\n        d_max : float, optional\n            The maximum diameter at which the mesh ends, [m]\n        pts : int, optional\n            The number of points to return for the mesh (note this is not \n            respected by sieve meshes), [-]\n        limit : float\n            If `d_min` or `d_max` is not specified, it will be calculated as the\n            `dn` at which this limit or 1-limit exists (this is ignored for \n            Renard numbers), [-]\n        method : str, optional\n            Either 'linear', 'logarithmic', a Renard number like 'R10' or 'R5' \n            or'R2.5', or one of the sieve standards 'ISO 3310-1 R40/3', \n            'ISO 3310-1 R20', 'ISO 3310-1 R20/3', 'ISO 3310-1', \n            'ISO 3310-1 R10', 'ASTM E11', [-]\n    \n        Returns\n        -------\n        ds : list[float]\n            The generated mesh diameters, [m]\n    \n        Notes\n        -----\n        Note that when specifying a Renard series, only one of `d_min` or `d_max` can\n        be respected! Provide only one of those numbers. \n        \n        Note that when specifying a sieve standard the number of points is not\n        respected!\n    \n        References\n        ----------\n        .. [1] ASTM E11 - 17 - Standard Specification for Woven Wire Test Sieve \n           Cloth and Test Sieves.\n        .. [2] ISO 3310-1:2016 - Test Sieves -- Technical Requirements and Testing\n           -- Part 1: Test Sieves of Metal Wire Cloth.\n        '''\n        if method[0] not in ('R', 'r'):\n            if d_min is None:\n                d_min = self.dn(limit)\n            if d_max is None:\n                d_max = self.dn(1.0 - limit)\n        return psd_spacing(d_min=d_min, d_max=d_max, pts=pts, method=method)", "response": "r Returns a list of diameters that are distributed with the given meshes."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cdf_discrete(self, ds, n=None):\n        r'''Computes the cumulative distribution functions for a list of \n        specified particle diameters.\n                                        \n        Parameters\n        ----------\n        ds : list[float]\n            Particle size diameters, [m]\n        n : int, optional\n            None (for the `order` specified when the distribution was created),\n            0 (number), 1 (length), 2 (area), 3 (volume/mass),\n            or any integer, [-]\n\n        Returns\n        -------\n        cdfs : float\n            The cumulative distribution functions at the specified diameters \n            and order, [-]\n        \n        Examples\n        --------\n        >>> psd = PSDLognormal(s=0.5, d_characteristic=5E-6, order=3)\n        >>> psd.cdf_discrete([1e-6, 1e-5, 1e-4, 1e-3])\n        [0.0006434710129138987, 0.9171714809983015, 0.9999999989602018, 1.0]\n        '''\n        return [self.cdf(d, n=n) for d in ds]", "response": "r Returns the cumulative distribution functions for a list of \n        specified particle diameters."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef mean_size(self, p, q):\n        '''        \n        >>> psd = PSDLognormal(s=0.5, d_characteristic=5E-6)\n        >>> psd.mean_size(3, 2)\n        4.412484512922977e-06\n        \n        Note that for the case where p == q, a different set of formulas are\n        required - which do not have analytical results for many distributions.\n        Therefore, a close numerical approximation is used instead, to\n        perturb the values of p and q so they are 1E-9 away from each other.\n        This leads only to slight errors, as in the example below where the\n        correct answer is 5E-6.\n        \n        >>> psd.mean_size(3, 3)\n        4.9999999304923345e-06\n        '''\n        if p == q:\n            p -= 1e-9\n            q += 1e-9\n        pow1 = q - self.order \n        \n        denominator = self._pdf_basis_integral_definite(d_min=self.d_minimum, d_max=self.d_excessive, n=pow1)\n        root_power = p - q\n        pow3 = p - self.order\n        numerator = self._pdf_basis_integral_definite(d_min=self.d_minimum, d_max=self.d_excessive, n=pow3)\n        return (numerator/denominator)**(1.0/(root_power))", "response": "Calculates the mean size of the entry in the lognormal distribution."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef mean_size_ISO(self, k, r):\n        '''        \n        >>> psd = PSDLognormal(s=0.5, d_characteristic=5E-6)\n        >>> psd.mean_size_ISO(1, 2)\n        4.412484512922977e-06\n        '''\n        p = k + r\n        q = r\n        return self.mean_size(p=p, q=q)", "response": "Return the mean size of the lognormal distribution for the given key and resource."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef plot_pdf(self, n=(0, 1, 2, 3), d_min=None, d_max=None, pts=500,\n                 normalized=False, method='linear'):  # pragma: no cover\n        r'''Plot the probability density function of the particle size \n        distribution. The plotted range can be specified using `d_min` and \n        `d_max`, or estimated automatically. One or more order can be plotted,\n        by providing an iterable of ints as the value of `n` or just one int.\n                \n        Parameters\n        ----------\n        n : tuple(int) or int, optional\n            None (for the `order` specified when the distribution was created),\n            0 (number), 1 (length), 2 (area), 3 (volume/mass),\n            or any integer; as many as desired may be specified, [-]\n        d_min : float, optional\n            Lower particle size diameter, [m]\n        d_max : float, optional\n            Upper particle size diameter, [m]\n        pts : int, optional\n            The number of points for values to be calculated, [-]\n        normalized : bool, optional\n            Whether to display the actual probability density function, which\n            may have a huge magnitude - or to divide each point by the sum\n            of all the points. Doing this is a common practice, but the values\n            at each point are dependent on the number of points being plotted,\n            and the distribution of the points;\n            [-]\n        method : str, optional\n            Either 'linear', 'logarithmic', a Renard number like 'R10' or 'R5' \n            or'R2.5', or one of the sieve standards 'ISO 3310-1 R40/3', \n            'ISO 3310-1 R20', 'ISO 3310-1 R20/3', 'ISO 3310-1', \n            'ISO 3310-1 R10', 'ASTM E11', [-]\n        '''\n        try:\n            import matplotlib.pyplot as plt\n        except:  # pragma: no cover\n            raise Exception(NO_MATPLOTLIB_MSG)           \n        ds = self.ds_discrete(d_min=d_min, d_max=d_max, pts=pts, method=method)\n        try:\n            for ni in n:\n                fractions = [self.pdf(d, n=ni) for d in ds]\n                if normalized:\n                    fractions = normalize(fractions)\n                plt.semilogx(ds, fractions, label=_label_distribution_n(ni))\n        except Exception as e:\n            fractions = [self.pdf(d, n=n) for d in ds]\n            if normalized:\n                fractions = normalize(fractions)\n            plt.semilogx(ds, fractions, label=_label_distribution_n(n))\n        plt.ylabel('Probability density function, [-]')\n        plt.xlabel('Particle diameter, [m]')\n        plt.title('Probability density function of %s distribution with '\n                  'parameters %s' %(self.name, self.parameters))\n        plt.legend()\n        plt.show()\n        return fractions", "response": "Plot the probability density function of the particle size in a set of points."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef plot_cdf(self, n=(0, 1, 2, 3), d_min=None, d_max=None, pts=500,\n                 method='logarithmic'):   # pragma: no cover\n        r'''Plot the cumulative distribution function of the particle size \n        distribution. The plotted range can be specified using `d_min` and \n        `d_max`, or estimated automatically. One or more order can be plotted,\n        by providing an iterable of ints as the value of `n` or just one int.\n                \n        Parameters\n        ----------\n        n : tuple(int) or int, optional\n            None (for the `order` specified when the distribution was created),\n            0 (number), 1 (length), 2 (area), 3 (volume/mass),\n            or any integer; as many as desired may be specified, [-]\n        d_min : float, optional\n            Lower particle size diameter, [m]\n        d_max : float, optional\n            Upper particle size diameter, [m]\n        pts : int, optional\n            The number of points for values to be calculated, [-]\n        method : str, optional\n            Either 'linear', 'logarithmic', a Renard number like 'R10' or 'R5' \n            or'R2.5', or one of the sieve standards 'ISO 3310-1 R40/3', \n            'ISO 3310-1 R20', 'ISO 3310-1 R20/3', 'ISO 3310-1', \n            'ISO 3310-1 R10', 'ASTM E11', [-]\n        '''\n        try:\n            import matplotlib.pyplot as plt\n        except:  # pragma: no cover\n            raise Exception(NO_MATPLOTLIB_MSG)\n\n        ds = self.ds_discrete(d_min=d_min, d_max=d_max, pts=pts, method=method)\n        try:\n            for ni in n:\n                cdfs = self.cdf_discrete(ds=ds, n=ni)\n                plt.semilogx(ds, cdfs, label=_label_distribution_n(ni))\n        except:\n            cdfs = self.cdf_discrete(ds=ds, n=n)\n            plt.semilogx(ds, cdfs, label=_label_distribution_n(n))\n        if self.points:\n            plt.plot(self.ds, self.fraction_cdf, '+', label='Volume/Mass points')\n            \n            if hasattr(self, 'area_fractions'):\n                plt.plot(self.ds, cumsum(self.area_fractions), '+', label='Area points')\n            if hasattr(self, 'length_fractions'):\n                plt.plot(self.ds, cumsum(self.length_fractions), '+', label='Length points')\n            if hasattr(self, 'number_fractions'):\n                plt.plot(self.ds, cumsum(self.number_fractions), '+', label='Number points')\n                \n        plt.ylabel('Cumulative density function, [-]')\n        plt.xlabel('Particle diameter, [m]')\n        plt.title('Cumulative density function of %s distribution with '\n                  'parameters %s' %(self.name, self.parameters))\n        plt.legend()\n        plt.show()", "response": "r Draws the cumulative distribution function of the particle size in a set of points."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef Dis(self):\n        '''Representative diameters of each bin.\n        '''\n        return [self.di_power(i, power=1) for i in range(self.N)]", "response": "Representative diameters of each bin."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the mean size of a single object.", "response": "def mean_size(self, p, q):\n        '''        \n        >>> ds = 1E-6*np.array([240, 360, 450, 562.5, 703, 878, 1097, 1371, 1713, 2141, 2676, 3345, 4181, 5226, 6532])\n        >>> numbers = [65, 119, 232, 410, 629, 849, 990, 981, 825, 579, 297, 111, 21, 1]\n        >>> psd = ParticleSizeDistribution(ds=ds, fractions=numbers, order=0)\n        >>> psd.mean_size(3, 2)\n        0.002269321031745045\n        '''\n        if p != q:\n            # Note: D(p, q) = D(q, p); in ISO and proven experimentally\n            numerator = sum(self.di_power(i=i, power=p)*self.number_fractions[i] for i in range(self.N))\n            denominator = sum(self.di_power(i=i, power=q)*self.number_fractions[i] for i in range(self.N))\n            return (numerator/denominator)**(1.0/(p-q))\n        else:\n            numerator = sum(log(self.di_power(i=i, power=1))*self.di_power(i=i, power=p)*self.number_fractions[i] for i in range(self.N))\n            denominator = sum(self.di_power(i=i, power=q)*self.number_fractions[i] for i in range(self.N))\n            return exp(numerator/denominator)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef vssa(self):\n        r'''The volume-specific surface area of a particle size distribution.\n        Note this uses the diameters provided by the method `Dis`.\n        \n        .. math::\n            \\text{VSSA} = \\sum_i \\text{fraction}_i \\frac{SA_i}{V_i}\n        Returns\n        -------\n        VSSA : float\n            The volume-specific surface area of the distribution, [m^2/m^3] \n\n        References\n        ----------\n        .. [1] ISO 9276-2:2014 - Representation of Results of Particle Size \n           Analysis - Part 2: Calculation of Average Particle Sizes/Diameters  \n           and Moments from Particle Size Distributions.\n        '''\n        ds = self.Dis\n        Vs = [pi/6*di**3 for di in ds]\n        SAs = [pi*di**2 for di in ds]\n        SASs = [SA/V for SA, V in zip(SAs, Vs)]\n        VSSA = sum([fi*SASi for fi, SASi in zip(self.fractions, SASs)])\n        return VSSA", "response": "Return the volume - specific surface area of a particle size distribution."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef V_horiz_guppy(D, L, a, h, headonly=False):\n    r'''Calculates volume of a tank with guppy heads, according to [1]_.\n\n    .. math::\n        V_f = A_fL + \\frac{2aR^2}{3}\\cos^{-1}\\left(1 - \\frac{h}{R}\\right)\n        +\\frac{2a}{9R}\\sqrt{2Rh - h^2}(2h-3R)(h+R)\n\n    .. math::\n        Af = R^2\\cos^{-1}\\frac{R-h}{R} - (R-h)\\sqrt{2Rh - h^2}\n\n    Parameters\n    ----------\n    D : float\n        Diameter of the main cylindrical section, [m]\n    L : float\n        Length of the main cylindrical section, [m]\n    a : float\n        Distance the guppy head extends on one side, [m]\n    h : float\n        Height, as measured up to where the fluid ends, [m]\n    headonly : bool, optional\n        Function returns only the volume of a single head side if True\n\n    Returns\n    -------\n    V : float\n        Volume [m^3]\n\n    Examples\n    --------\n    Matching example from [1]_, with inputs in inches and volume in gallons.\n\n    >>> V_horiz_guppy(D=108., L=156., a=42., h=36)/231.\n    1931.7208029476762\n\n    References\n    ----------\n    .. [1] Jones, D. \"Calculating Tank Volume.\" Text. Accessed December 22, 2015.\n       http://www.webcalc.com.br/blog/Tank_Volume.PDF'''\n    R = 0.5*D\n    Af = R*R*acos((R-h)/R) - (R-h)*(2.*R*h - h*h)**0.5\n    Vf = 2.*a*R*R/3.*acos(1. - h/R) + 2.*a/9./R*(2*R*h - h**2)**0.5*(2*h - 3*R)*(h + R)\n    if headonly:\n        Vf = Vf/2.\n    else:\n        Vf += Af*L\n    return Vf", "response": "r Calculates volume of a tank with guppy heads."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef V_vertical_conical(D, a, h):\n    r'''Calculates volume of a vertical tank with a convex conical bottom,\n    according to [1]_. No provision for the top of the tank is made here.\n\n    .. math::\n        V_f = \\frac{\\pi}{4}\\left(\\frac{Dh}{a}\\right)^2\\left(\\frac{h}{3}\\right),\\; h < a\n\n    .. math::\n        V_f = \\frac{\\pi D^2}{4}\\left(h - \\frac{2a}{3}\\right),\\; h\\ge a\n\n    Parameters\n    ----------\n    D : float\n        Diameter of the main cylindrical section, [m]\n    a : float\n        Distance the cone head extends under the main cylinder, [m]\n    h : float\n        Height, as measured up to where the fluid ends, [m]\n\n    Returns\n    -------\n    V : float\n        Volume [m^3]\n\n    Examples\n    --------\n    Matching example from [1]_, with inputs in inches and volume in gallons.\n\n    >>> V_vertical_conical(132., 33., 24)/231.\n    250.67461381371024\n\n    References\n    ----------\n    .. [1] Jones, D. \"Calculating Tank Volume.\" Text. Accessed December 22, 2015.\n       http://www.webcalc.com.br/blog/Tank_Volume.PDF'''\n    if h < a:\n        Vf = pi/4*(D*h/a)**2*(h/3.)\n    else:\n        Vf = pi*D**2/4*(h - 2*a/3.)\n    return Vf", "response": "r Calculates the volume of a vertical conical tank with a convex conical bottom."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef V_vertical_ellipsoidal(D, a, h):\n    r'''Calculates volume of a vertical tank with a convex ellipsoidal bottom,\n    according to [1]_. No provision for the top of the tank is made here.\n\n    .. math::\n        V_f = \\frac{\\pi}{4}\\left(\\frac{Dh}{a}\\right)^2 \\left(a - \\frac{h}{3}\\right),\\; h < a\n\n    .. math::\n        V_f = \\frac{\\pi D^2}{4}\\left(h - \\frac{a}{3}\\right),\\; h \\ge a\n\n    Parameters\n    ----------\n    D : float\n        Diameter of the main cylindrical section, [m]\n    a : float\n        Distance the ellipsoid head extends under the main cylinder, [m]\n    h : float\n        Height, as measured up to where the fluid ends, [m]\n\n    Returns\n    -------\n    V : float\n        Volume [m^3]\n\n    Examples\n    --------\n    Matching example from [1]_, with inputs in inches and volume in gallons.\n\n    >>> V_vertical_ellipsoidal(132., 33., 24)/231.\n    783.3581681678445\n\n    References\n    ----------\n    .. [1] Jones, D. \"Calculating Tank Volume.\" Text. Accessed December 22, 2015.\n       http://www.webcalc.com.br/blog/Tank_Volume.PDF'''\n    if h < a:\n        Vf = pi/4*(D*h/a)**2*(a - h/3.)\n    else:\n        Vf = pi*D**2/4*(h - a/3.)\n    return Vf", "response": "r Calculates the volume of a vertical tank with a convex ellipsoidal bottom."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef V_vertical_spherical(D, a, h):\n    r'''Calculates volume of a vertical tank with a convex spherical bottom,\n    according to [1]_. No provision for the top of the tank is made here.\n\n    .. math::\n        V_f = \\frac{\\pi h^2}{4}\\left(2a + \\frac{D^2}{2a} - \\frac{4h}{3}\\right),\\; h < a\n\n    .. math::\n        V_f = \\frac{\\pi}{4}\\left(\\frac{2a^3}{3} - \\frac{aD^2}{2} + hD^2\\right),\\; h\\ge a\n\n    Parameters\n    ----------\n    D : float\n        Diameter of the main cylindrical section, [m]\n    a : float\n        Distance the spherical head extends under the main cylinder, [m]\n    h : float\n        Height, as measured up to where the fluid ends, [m]\n\n    Returns\n    -------\n    V : float\n        Volume [m^3]\n\n    Examples\n    --------\n    Matching example from [1]_, with inputs in inches and volume in gallons.\n\n    >>> V_vertical_spherical(132., 33., 24)/231.\n    583.6018352850442\n\n    References\n    ----------\n    .. [1] Jones, D. \"Calculating Tank Volume.\" Text. Accessed December 22, 2015.\n       http://www.webcalc.com.br/blog/Tank_Volume.PDF'''\n    if h < a:\n        Vf = pi*h**2/4*(2*a + D**2/2/a - 4*h/3)\n    else:\n        Vf = pi/4*(2*a**3/3 - a*D**2/2 + h*D**2)\n    return Vf", "response": "r Calculates the volume of a vertical tank with a convex spherical bottom."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef V_vertical_torispherical(D, f, k, h):\n    r'''Calculates volume of a vertical tank with a convex torispherical bottom,\n    according to [1]_. No provision for the top of the tank is made here.\n\n    .. math::\n        V_f = \\frac{\\pi h^2}{4}\\left(2a_1 + \\frac{D_1^2}{2a_1}\n        - \\frac{4h}{3}\\right),\\; 0 \\le h \\le a_1\n\n    .. math::\n        V_f = \\frac{\\pi}{4}\\left(\\frac{2a_1^3}{3} + \\frac{a_1D_1^2}{2}\\right)\n        +\\pi u\\left[\\left(\\frac{D}{2}-kD\\right)^2 +s\\right]\n        + \\frac{\\pi tu^2}{2} - \\frac{\\pi u^3}{3} + \\pi D(1-2k)\\left[\n        \\frac{2u-t}{4}\\sqrt{s+tu-u^2} + \\frac{t\\sqrt{s}}{4}\n        + \\frac{k^2D^2}{2}\\left(\\cos^{-1}\\frac{t-2u}{2kD}-\\alpha\\right)\\right]\n        ,\\; a_1 < h \\le a_1 + a_2\n\n    .. math::\n        V_f = \\frac{\\pi}{4}\\left(\\frac{2a_1^3}{3} + \\frac{a_1D_1^2}{2}\\right)\n        +\\frac{\\pi t}{2}\\left[\\left(\\frac{D}{2}-kD\\right)^2 +s\\right]\n        +\\frac{\\pi  t^3}{12} + \\pi D(1-2k)\\left[\\frac{t\\sqrt{s}}{4}\n        + \\frac{k^2D^2}{2}\\sin^{-1}(\\cos\\alpha)\\right]\n        + \\frac{\\pi D^2}{4}[h-(a_1+a_2)] ,\\;  a_1 + a_2 < h\n\n    .. math::\n        \\alpha = \\sin^{-1}\\frac{1-2k}{2(f-k)}\n\n    .. math::\n        a_1 = fD(1-\\cos\\alpha)\n\n    .. math::\n        a_2 = kD\\cos\\alpha\n\n    .. math::\n        D_1 = 2fD\\sin\\alpha\n\n    .. math::\n        s = (kD\\sin\\alpha)^2\n\n    .. math::\n        t = 2a_2\n\n    .. math::\n        u = h - fD(1-\\cos\\alpha)\n\n    Parameters\n    ----------\n    D : float\n        Diameter of the main cylindrical section, [m]\n    f : float\n        Dish-radius parameter; fD  = dish radius [1/m]\n    k : float\n        knuckle-radius parameter ; kD = knuckle radius [1/m]\n    h : float\n        Height, as measured up to where the fluid ends, [m]\n\n    Returns\n    -------\n    V : float\n        Volume [m^3]\n\n    Examples\n    --------\n    Matching example from [1]_, with inputs in inches and volume in gallons.\n\n    >>> V_vertical_torispherical(D=132., f=1.0, k=0.06, h=24)/231.\n    904.0688283793511\n\n    References\n    ----------\n    .. [1] Jones, D. \"Calculating Tank Volume.\" Text. Accessed December 22, 2015.\n       http://www.webcalc.com.br/blog/Tank_Volume.PDF'''\n    alpha = asin((1-2*k)/(2*(f-k)))\n    a1 = f*D*(1 - cos(alpha))\n    a2 = k*D*cos(alpha)\n    D1 = 2*f*D*sin(alpha)\n    s = (k*D*sin(alpha))**2\n    t = 2*a2\n    u = h - f*D*(1 - cos(alpha))\n\n    if 0 <= h <= a1:\n        Vf = pi*h**2/4*(2*a1 + D1**2/2/a1 - 4*h/3)\n    elif a1 < h <= a1 + a2:\n        Vf = (pi/4*(2*a1**3/3 + a1*D1**2/2.) + pi*u*((D/2. - k*D)**2 + s)\n        + pi*t*u**2/2. - pi*u**3/3. + pi*D*(1 - 2*k)*((2*u-t)/4.*(s + t*u\n        - u**2)**0.5 + t*s**0.5/4. + k**2*D**2/2*(acos((t-2*u)/(2*k*D))-alpha)))\n    else:\n        Vf = pi/4*(2*a1**3/3. + a1*D1**2/2.) + pi*t/2.*((D/2 - k*D)**2\n        + s) + pi*t**3/12. + pi*D*(1 - 2*k)*(t*s**0.5/4\n        + k**2*D**2/2*asin(cos(alpha))) + pi*D**2/4*(h - (a1 + a2))\n    return Vf", "response": "r Calculates the volume of a vertical torispherical tank with a convex torispherical bottom."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef V_vertical_conical_concave(D, a, h):\n    r'''Calculates volume of a vertical tank with a concave conical bottom,\n    according to [1]_. No provision for the top of the tank is made here.\n\n    .. math::\n        V = \\frac{\\pi D^2}{12} \\left(3h + a - \\frac{(a+h)^3}{a^2}\\right)\n        ,\\;\\; 0 \\le h < |a|\n\n    .. math::\n        V = \\frac{\\pi D^2}{12} (3h + a ),\\;\\; h \\ge |a|\n\n    Parameters\n    ----------\n    D : float\n        Diameter of the main cylindrical section, [m]\n    a : float\n        Negative distance the cone head extends inside the main cylinder, [m]\n    h : float\n        Height, as measured up to where the fluid ends, [m]\n\n    Returns\n    -------\n    V : float\n        Volume [m^3]\n\n    Examples\n    --------\n    Matching example from [1]_, with inputs in inches and volume in gallons.\n\n    >>> V_vertical_conical_concave(D=113., a=-33, h=15)/231\n    251.15825565795188\n\n    References\n    ----------\n    .. [1] Jones, D. \"Compute Fluid Volumes in Vertical Tanks.\" Chemical\n       Processing. December 18, 2003.\n       http://www.chemicalprocessing.com/articles/2003/193/\n    '''\n    if h < abs(a):\n        Vf = pi*D**2/12.*(3*h + a - (a+h)**3/a**2)\n    else:\n        Vf = pi*D**2/12.*(3*h + a)\n    return Vf", "response": "r Calculates volume of a vertical conical concave conical tank with a concave conical bottom."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing Fluid Volumes in Vertical Tanks.", "response": "def V_vertical_spherical_concave(D, a, h):\n    r'''Calculates volume of a vertical tank with a concave spherical bottom,\n    according to [1]_. No provision for the top of the tank is made here.\n\n    .. math::\n        V = \\frac{\\pi}{12}\\left[3D^2h + \\frac{a}{2}(3D^2 + 4a^2) + (a+h)^3\n        \\left(4 - \\frac{3D^2 + 12a^2}{2a(a+h)}\\right)\\right],\\;\\; 0 \\le h < |a|\n\n    .. math::\n        V = \\frac{\\pi}{12}\\left[3D^2h + \\frac{a}{2}(3D^2 + 4a^2) \\right]\n        ,\\;\\;  h \\ge |a|\n\n    Parameters\n    ----------\n    D : float\n        Diameter of the main cylindrical section, [m]\n    a : float\n        Negative distance the spherical head extends inside the main cylinder, [m]\n    h : float\n        Height, as measured up to where the fluid ends, [m]\n\n    Returns\n    -------\n    V : float\n        Volume [m^3]\n\n    Examples\n    --------\n    Matching example from [1]_, with inputs in inches and volume in gallons.\n\n    >>> V_vertical_spherical_concave(D=113., a=-33, h=15)/231\n    112.81405437348528\n\n    References\n    ----------\n    .. [1] Jones, D. \"Compute Fluid Volumes in Vertical Tanks.\" Chemical\n       Processing. December 18, 2003.\n       http://www.chemicalprocessing.com/articles/2003/193/\n    '''\n    if h < abs(a):\n        Vf = pi/12*(3*D**2*h + a/2.*(3*D**2 + 4*a**2) + (a+h)**3*(4 - (3*D**2+12*a**2)/(2.*a*(a+h))))\n    else:\n        Vf = pi/12*(3*D**2*h + a/2.*(3*D**2 + 4*a**2))\n    return Vf"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef V_vertical_torispherical_concave(D, f, k, h):\n    r'''Calculates volume of a vertical tank with a concave torispherical bottom,\n    according to [1]_. No provision for the top of the tank is made here.\n\n    .. math::\n        V = \\frac{\\pi D^2 h}{4} - v_1(h=a_1+a_2) + v_1(h=a_1 + a_2 -h),\\; 0 \\le h < a_2\n\n    .. math::\n        V = \\frac{\\pi D^2 h}{4} - v_1(h=a_1+a_2) + v_2(h=a_1 + a_2 -h),\\; a_2 \\le h < a_1 + a_2\n\n    .. math::\n        V = \\frac{\\pi D^2 h}{4} - v_1(h=a_1+a_2) + 0,\\; h \\ge a_1 + a_2\n\n    .. math::\n        v_1 = \\frac{\\pi}{4}\\left(\\frac{2a_1^3}{3} + \\frac{a_1D_1^2}{2}\\right)\n        +\\pi u\\left[\\left(\\frac{D}{2}-kD\\right)^2 +s\\right]\n        + \\frac{\\pi tu^2}{2} - \\frac{\\pi u^3}{3} + \\pi D(1-2k)\\left[\n        \\frac{2u-t}{4}\\sqrt{s+tu-u^2} + \\frac{t\\sqrt{s}}{4}\n        + \\frac{k^2D^2}{2}\\left(\\cos^{-1}\\frac{t-2u}{2kD}-\\alpha\\right)\\right]\n\n    .. math::\n        v_2 = \\frac{\\pi h^2}{4}\\left(2a_1 + \\frac{D_1^2}{2a_1} - \\frac{4h}{3}\\right)\n\n    .. math::\n        \\alpha = \\sin^{-1}\\frac{1-2k}{2(f-k)}\n\n    .. math::\n        a_1 = fD(1-\\cos\\alpha)\n\n    .. math::\n        a_2 = kD\\cos\\alpha\n\n    .. math::\n        D_1 = 2fD\\sin\\alpha\n\n    .. math::\n        s = (kD\\sin\\alpha)^2\n\n    .. math::\n        t = 2a_2\n\n    .. math::\n        u = h - fD(1-\\cos\\alpha)\n\n    Parameters\n    ----------\n    D : float\n        Diameter of the main cylindrical section, [m]\n    f : float\n        Dish-radius parameter; fD  = dish radius [1/m]\n    k : float\n        knuckle-radius parameter ; kD = knuckle radius [1/m]\n    h : float\n        Height, as measured up to where the fluid ends, [m]\n\n    Returns\n    -------\n    V : float\n        Volume [m^3]\n\n    Examples\n    --------\n    Matching example from [1]_, with inputs in inches and volume in gallons.\n\n    >>> V_vertical_torispherical_concave(D=113., f=0.71, k=0.081, h=15)/231\n    103.88569287163769\n\n    References\n    ----------\n    .. [1] Jones, D. \"Compute Fluid Volumes in Vertical Tanks.\" Chemical\n       Processing. December 18, 2003.\n       http://www.chemicalprocessing.com/articles/2003/193/\n    '''\n    alpha = asin((1-2*k)/(2.*(f-k)))\n    a1 = f*D*(1-cos(alpha))\n    a2 = k*D*cos(alpha)\n    D1 = 2*f*D*sin(alpha)\n    s = (k*D*sin(alpha))**2\n    t = 2*a2\n    def V1(h):\n        u = h-f*D*(1-cos(alpha))\n        v1 = pi/4*(2*a1**3/3. + a1*D1**2/2.) + pi*u*((D/2.-k*D)**2 +s)\n        v1 += pi*t*u**2/2. - pi*u**3/3.\n        v1 += pi*D*(1-2*k)*((2*u-t)/4.*(s+t*u-u**2)**0.5 + t*s**0.5/4.\n        + k**2*D**2/2.*(acos((t-2*u)/(2*k*D)) -alpha))\n        return v1\n    def V2(h):\n        v2 = pi*h**2/4.*(2*a1 + D1**2/(2.*a1) - 4*h/3.)\n        return v2\n    if 0 <= h < a2:\n        Vf = pi*D**2*h/4 - V1(a1+a2) + V1(a1+a2-h)\n    elif a2 <= h < a1 + a2:\n        Vf = pi*D**2*h/4 - V1(a1+a2) + V2(a1+a2-h)\n    else:\n        Vf = pi*D**2*h/4 - V1(a1+a2)\n    return Vf", "response": "r Calculates the volume of a vertical tank with a concave torispherical bottom."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates the surface area of a guppy head according to [ 1 ] _.", "response": "def SA_guppy_head(D, a):\n    r'''Calculates the surface area of a guppy head according to [1]_.\n    Some work was involved in combining formulas for the ellipse of the head,\n    and the conic section on the sides.\n\n    .. math::\n        SA = \\frac{\\pi  D}{4}\\sqrt{D^2 + a^2} + \\frac{\\pi D}{2}a\n\n    Parameters\n    ----------\n    D : float\n        Diameter of the main cylindrical section, [m]\n    a : float\n        Distance the conical head extends, [m]\n\n    Returns\n    -------\n    SA : float\n        Surface area [m^2]\n\n    Examples\n    --------\n    >>> SA_guppy_head(2, 1)\n    6.654000019110157\n\n    References\n    ----------\n    .. [1] Weisstein, Eric W. \"Cone.\" Text. Accessed March 14, 2016.\n       http://mathworld.wolfram.com/Cone.html.'''\n    return pi*D/4*(a**2 + D**2)**0.5 + pi*D/2*a"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef SA_torispheroidal(D, fd, fk):\n    r'''Calculates surface area of a torispherical head according to [1]_.\n    Somewhat involved. Equations are adapted to be used for a full head.\n\n    .. math::\n        SA = S_1 + S_2\n\n    .. math::\n        S_1 = 2\\pi D^2 f_d \\alpha\n\n    .. math::\n        S_2 = 2\\pi D^2 f_k\\left(\\alpha - \\alpha_1 + (0.5 - f_k)\\left(\\sin^{-1}\n        \\left(\\frac{\\alpha-\\alpha_2}{f_k}\\right) - \\sin^{-1}\\left(\\frac{\n        \\alpha_1-\\alpha_2}{f_k}\\right)\\right)\\right)\n\n    .. math::\n        \\alpha_1 = f_d\\left(1 - \\sqrt{1 - \\left(\\frac{0.5 - f_k}{f_d-f_k}\n        \\right)^2}\\right)\n\n    .. math::\n        \\alpha_2 = f_d - \\sqrt{f_d^2 - 2f_d f_k + f_k - 0.25}\n\n    .. math::\n        \\alpha = \\frac{a}{D_i}\n\n    Parameters\n    ----------\n    D : float\n        Diameter of the main cylindrical section, [m]\n    fd : float\n        Dish-radius parameter = f; fD  = dish radius [1/m]\n    fk : float\n        knuckle-radius parameter = k; kD = knuckle radius [1/m]\n\n    Returns\n    -------\n    SA : float\n        Surface area [m^2]\n\n    Examples\n    --------\n    Example from [1]_.\n\n    >>> SA_torispheroidal(D=2.54, fd=1.039370079, fk=0.062362205)\n    6.00394283477063\n\n    References\n    ----------\n    .. [1] Honeywell. \"Calculate Surface Areas and Cross-sectional Areas in\n       Vessels with Dished Heads\". https://www.honeywellprocess.com/library/marketing/whitepapers/WP-VesselsWithDishedHeads-UniSimDesign.pdf\n       Whitepaper. 2014.\n    '''\n    alpha_1 = fd*(1 - (1 - ((0.5-fk)/(fd-fk))**2)**0.5)\n    alpha_2 = fd - (fd**2 - 2*fd*fk + fk - 0.25)**0.5\n    alpha = alpha_1 # Up to top of dome\n    S1 = 2*pi*D**2*fd*alpha_1\n    alpha = alpha_2 # up to top of torus\n    S2_sub = asin((alpha-alpha_2)/fk) - asin((alpha_1-alpha_2)/fk)\n    S2 = 2*pi*D**2*fk*(alpha - alpha_1 + (0.5-fk)*S2_sub)\n    return S1 + S2", "response": "A simple formula for calculating the surface area of a torispherical head according to [ 1. 0 )."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef V_from_h(h, D, L, horizontal=True, sideA=None, sideB=None, sideA_a=0,\n             sideB_a=0, sideA_f=None, sideA_k=None, sideB_f=None, sideB_k=None):\n    r'''Calculates partially full volume of a vertical or horizontal tank with\n    different head types according to [1]_.\n\n    Parameters\n    ----------\n    h : float\n        Height of the liquid in the tank, [m]\n    D : float\n        Diameter of the cylindrical section of the tank, [m]\n    L : float\n        Length of the main cylindrical section of the tank, [m]\n    horizontal : bool, optional\n        Whether or not the tank is a horizontal or vertical tank\n    sideA : string, optional\n        The left (or bottom for vertical) head of the tank's type; one of\n        [None, 'conical', 'ellipsoidal', 'torispherical', 'guppy', 'spherical'].\n    sideB : string, optional\n        The right (or top for vertical) head of the tank's type; one of\n        [None, 'conical', 'ellipsoidal', 'torispherical', 'guppy', 'spherical'].\n    sideA_a : float, optional\n        The distance the head as specified by sideA extends down or to the left\n        from the main cylindrical section, [m]\n    sideB_a : float, optional\n        The distance the head as specified by sideB extends up or to the right\n        from the main cylindrical section, [m]\n    sideA_f : float, optional\n        Dish-radius parameter for side A; fD  = dish radius [1/m]\n    sideA_k : float, optional\n        knuckle-radius parameter for side A; kD = knuckle radius [1/m]\n    sideB_f : float, optional\n        Dish-radius parameter for side B; fD  = dish radius [1/m]\n    sideB_k : float, optional\n        knuckle-radius parameter for side B; kD = knuckle radius [1/m]\n\n    Returns\n    -------\n    V : float\n        Volume up to h [m^3]\n\n    Examples\n    --------\n    >>> V_from_h(h=7, D=1.5, L=5., horizontal=False, sideA='conical',\n    ... sideB='conical', sideA_a=2., sideB_a=1.)\n    10.013826583317465\n\n    References\n    ----------\n    .. [1] Jones, D. \"Compute Fluid Volumes in Vertical Tanks.\" Chemical\n       Processing. December 18, 2003.\n       http://www.chemicalprocessing.com/articles/2003/193/\n    '''\n    if sideA not in [None, 'conical', 'ellipsoidal', 'torispherical', 'spherical', 'guppy']:\n        raise Exception('Unspoorted head type for side A')\n    if sideB not in [None, 'conical', 'ellipsoidal', 'torispherical', 'spherical', 'guppy']:\n        raise Exception('Unspoorted head type for side B')\n    R = D/2.\n    V = 0\n    if horizontal:\n        # Conical case\n        if sideA == 'conical':\n            V += V_horiz_conical(D, L, sideA_a, h, headonly=True)\n        if sideB == 'conical':\n            V += V_horiz_conical(D, L, sideB_a, h, headonly=True)\n        # Elliosoidal case\n        if sideA == 'ellipsoidal':\n            V += V_horiz_ellipsoidal(D, L, sideA_a, h, headonly=True)\n        if sideB == 'ellipsoidal':\n            V += V_horiz_ellipsoidal(D, L, sideB_a, h, headonly=True)\n        # Guppy case\n        if sideA == 'guppy':\n            V += V_horiz_guppy(D, L, sideA_a, h, headonly=True)\n        if sideB == 'guppy':\n            V += V_horiz_guppy(D, L, sideB_a, h, headonly=True)\n        # Spherical case\n        if sideA == 'spherical':\n            V += V_horiz_spherical(D, L, sideA_a, h, headonly=True)\n        if sideB == 'spherical':\n            V += V_horiz_spherical(D, L, sideB_a, h, headonly=True)\n        # Torispherical case\n        if sideA == 'torispherical':\n            V += V_horiz_torispherical(D, L, sideA_f, sideA_k, h, headonly=True)\n        if sideB == 'torispherical':\n            V += V_horiz_torispherical(D, L, sideB_f, sideB_k, h, headonly=True)\n        if h > D: # Must be before Af, which will raise a domain error\n            raise Exception('Input height is above top of tank')\n        Af = R**2*acos((R-h)/R) - (R-h)*(2*R*h - h**2)**0.5\n        V += L*Af\n    else:\n        # Bottom head\n        if sideA in ['conical', 'ellipsoidal', 'torispherical', 'spherical']:\n            if sideA == 'conical':\n                V += V_vertical_conical(D, sideA_a, h=min(sideA_a, h))\n            if sideA == 'ellipsoidal':\n                V += V_vertical_ellipsoidal(D, sideA_a, h=min(sideA_a, h))\n            if sideA == 'spherical':\n                V += V_vertical_spherical(D, sideA_a, h=min(sideA_a, h))\n            if sideA == 'torispherical':\n                V += V_vertical_torispherical(D, sideA_f, sideA_k, h=min(sideA_a, h))\n        # Cylindrical section\n        if h >= sideA_a + L:\n            V += pi/4*D**2*L # All middle\n        elif h > sideA_a:\n            V += pi/4*D**2*(h - sideA_a) # Partial middle\n        # Top head\n        if h > sideA_a + L:\n            h2 = sideB_a - (h - sideA_a - L)\n            if sideB == 'conical':\n                V += V_vertical_conical(D, sideB_a, h=sideB_a)\n                V -= V_vertical_conical(D, sideB_a, h=h2)\n            if sideB == 'ellipsoidal':\n                V += V_vertical_ellipsoidal(D, sideB_a, h=sideB_a)\n                V -= V_vertical_ellipsoidal(D, sideB_a, h=h2)\n            if sideB == 'spherical':\n                V += V_vertical_spherical(D, sideB_a, h=sideB_a)\n                V -= V_vertical_spherical(D, sideB_a, h=h2)\n            if sideB == 'torispherical':\n                V += V_vertical_torispherical(D, sideB_f, sideB_k, h=sideB_a)\n                V -= V_vertical_torispherical(D, sideB_f, sideB_k, h=h2)\n        if h > L + sideA_a + sideB_a:\n            raise Exception('Input height is above top of tank')\n    return V", "response": "r Calculates the partially full volume of a vertical or horizontal tank with the given head types."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef A_hollow_cylinder(Di, Do, L):\n    r'''Returns the surface area of a hollow cylinder.\n\n    .. math::\n        A = \\pi D_o L + \\pi D_i L  + 2\\cdot \\frac{\\pi D_o^2}{4}\n        - 2\\cdot \\frac{\\pi D_i^2}{4}\n\n    Parameters\n    ----------\n    Di : float\n        Diameter of the hollow in the cylinder, [m]\n    Do : float\n        Diameter of the exterior of the cylinder, [m]\n    L : float\n        Length of the cylinder, [m]\n\n    Returns\n    -------\n    A : float\n        Surface area [m^2]\n\n    Examples\n    --------\n    >>> A_hollow_cylinder(0.005, 0.01, 0.1)\n    0.004830198704894308\n    '''\n    side_o = pi*Do*L\n    side_i = pi*Di*L\n    cap_circle = pi*Do**2/4*2\n    cap_removed = pi*Di**2/4*2\n    return side_o + side_i + cap_circle - cap_removed", "response": "r Returns the surface area of a hollow cylinder."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef A_multiple_hole_cylinder(Do, L, holes):\n    r'''Returns the surface area of a cylinder with multiple holes.\n    Calculation will naively return a negative value or other impossible\n    result if the number of cylinders added is physically impossible.\n    Holes may be of different shapes, but must be perpendicular to the\n    axis of the cylinder.\n\n    .. math::\n        A = \\pi D_o L + 2\\cdot \\frac{\\pi D_o^2}{4} +\n        \\sum_{i}^n \\left( \\pi D_i L  - 2\\cdot \\frac{\\pi D_i^2}{4}\\right)\n\n    Parameters\n    ----------\n    Do : float\n        Diameter of the exterior of the cylinder, [m]\n    L : float\n        Length of the cylinder, [m]\n    holes : list\n        List of tuples containing (diameter, count) pairs of descriptions for\n        each of the holes sizes.\n\n    Returns\n    -------\n    A : float\n        Surface area [m^2]\n\n    Examples\n    --------\n    >>> A_multiple_hole_cylinder(0.01, 0.1, [(0.005, 1)])\n    0.004830198704894308\n    '''\n    side_o = pi*Do*L\n    cap_circle = pi*Do**2/4*2\n    A = cap_circle + side_o\n    for Di, n in holes:\n        side_i = pi*Di*L\n        cap_removed = pi*Di**2/4*2\n        A = A + side_i*n - cap_removed*n\n    return A", "response": "r Returns the surface area of a cylinder with multiple holes."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_misc(self):\n        '''Set more parameters, after the tank is better defined than in the\n        __init__ function.\n\n        Notes\n        -----\n        Two of D, L, and L_over_D must be known when this function runs.\n        The other one is set from the other two first thing in this function.\n        a_ratio parameters are used to calculate a values for the heads here,\n        if applicable.\n        Radius is calculated here.\n        Maximum tank height is calculated here.\n        V_total is calculated here.\n        '''\n        if self.D and self.L:\n            # If L and D are known, get L_over_D\n            self.L_over_D = self.L/self.D\n        elif self.D and self.L_over_D:\n            # Otherwise, if L_over_D and D are provided, get L\n            self.L = self.D*self.L_over_D\n        elif self.L and self.L_over_D:\n            # Otherwise, if L_over_D and L are provided, get D\n            self.D = self.L/self.L_over_D\n\n        # Calculate diameter\n        self.R = self.D/2.\n\n        # If a_ratio is provided for either heads, use it.\n        if self.sideA and self.D:\n            if not self.sideA_a and self.sideA in ['conical', 'ellipsoidal', 'guppy', 'spherical']:\n                self.sideA_a = self.D*self.sideA_a_ratio\n        if self.sideB and self.D:\n            if not self.sideB_a and self.sideB in ['conical', 'ellipsoidal', 'guppy', 'spherical']:\n                self.sideB_a = self.D*self.sideB_a_ratio\n\n        # Calculate a for torispherical heads\n        if self.sideA == 'torispherical' and self.sideA_f and self.sideA_k:\n            self.sideA_a = a_torispherical(self.D, self.sideA_f, self.sideA_k)\n        if self.sideB == 'torispherical' and self.sideB_f and self.sideB_k:\n            self.sideB_a = a_torispherical(self.D, self.sideB_f, self.sideB_k)\n\n        # Calculate maximum tank height, h_max\n        if self.horizontal:\n            self.h_max = self.D\n        else:\n            self.h_max = self.L\n            if self.sideA_a:\n                self.h_max += self.sideA_a\n            if self.sideB_a:\n                self.h_max += self.sideB_a\n\n        # Set maximum height\n        self.V_total = self.V_from_h(self.h_max)\n\n        # Set surface areas\n        self.A, (self.A_sideA, self.A_sideB, self.A_lateral) = SA_tank(\n        D=self.D, L=self.L, sideA=self.sideA, sideB=self.sideB, sideA_a=self.sideA_a,\n        sideB_a=self.sideB_a, sideA_f=self.sideA_f, sideA_k=self.sideA_k,\n        sideB_f=self.sideB_f, sideB_k=self.sideB_k,\n             full_output=True)", "response": "Set some parameters after the tank is better defined than in the base class. This function is used to set the basic parameters of the tank."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef V_from_h(self, h, method='full'):\n        r'''Method to calculate the volume of liquid in a fully defined tank\n        given a specified height `h`. `h` must be under the maximum height.\n        If the method is 'chebyshev', and the coefficients have not yet been \n        calculated, they are created by calling `set_chebyshev_approximators`.\n\n        Parameters\n        ----------\n        h : float\n            Height specified, [m]\n        method : str\n            One of 'full' (calculated rigorously) or 'chebyshev'\n\n        Returns\n        -------\n        V : float\n            Volume of liquid in the tank up to the specified height, [m^3]\n            \n        Notes\n        -----\n        '''\n        if method == 'full':\n            return V_from_h(h, self.D, self.L, self.horizontal, self.sideA, \n                            self.sideB, self.sideA_a, self.sideB_a, \n                            self.sideA_f, self.sideA_k, self.sideB_f, \n                            self.sideB_k)\n        elif method == 'chebyshev':\n            if not self.chebyshev:\n                self.set_chebyshev_approximators()\n            return self.V_from_h_cheb(h)\n        else:\n            raise Exception(\"Allowable methods are 'full' or 'chebyshev'.\")", "response": "Method to calculate the volume of liquid in a fully defined tank at a specified height."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_table(self, n=100, dx=None):\n        r'''Method to set an interpolation table of liquids levels versus\n        volumes in the tank, for a fully defined tank. Normally run by the\n        h_from_V method, this may be run prior to its use with a custom\n        specification. Either the number of points on the table, or the\n        vertical distance between steps may be specified.\n\n        Parameters\n        ----------\n        n : float, optional\n            Number of points in the interpolation table, [-]\n        dx : float, optional\n            Vertical distance between steps in the interpolation table, [m]\n        '''\n        if dx:\n            self.heights = np.linspace(0, self.h_max, int(self.h_max/dx)+1)\n        else:\n            self.heights = np.linspace(0, self.h_max, n)\n        self.volumes = [self.V_from_h(h) for h in self.heights]\n        from scipy.interpolate import UnivariateSpline\n        self.interp_h_from_V = UnivariateSpline(self.volumes, self.heights, ext=3, s=0.0)\n        self.table = True", "response": "r Method to set an interpolation table of liquids versus volumes in a fully defined tank."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_chebyshev_approximators(self, deg_forward=50, deg_backwards=200):        \n        r'''Method to derive and set coefficients for chebyshev polynomial \n        function approximation of the height-volume and volume-height\n        relationship. \n        \n        A single set of chebyshev coefficients is used for the entire height-\n        volume and volume-height relationships respectively. \n        \n        The forward relationship, `V_from_h`, requires\n        far fewer coefficients in its fit than the reverse to obtain the same\n        relative accuracy. \n        \n        Optionally, deg_forward or deg_backwards can be set to None to try to \n        automatically fit the series to machine precision.\n        \n        Parameters\n        ----------\n        deg_forward : int, optional\n            The degree of the chebyshev polynomial to be created for the\n            `V_from_h` curve, [-]\n        deg_backwards : int, optional\n            The degree of the chebyshev polynomial to be created for the\n            `h_from_V` curve, [-]\n        '''\n        from fluids.optional.pychebfun import Chebfun\n        to_fit = lambda h: self.V_from_h(h, 'full')\n        \n        # These high-degree polynomials cannot safety be evaluated using Horner's methods     \n        # chebval is 2.5x as slow but 100% required; around 40 coefficients results are junk\n        self.c_forward = Chebfun.from_function(np.vectorize(to_fit), \n                                               [0.0, self.h_max], N=deg_forward).coefficients().tolist()\n\n        self.V_from_h_cheb = lambda x : chebval((2.0*x-self.h_max)/(self.h_max), self.c_forward)\n        \n        to_fit = lambda h: self.h_from_V(h, 'brenth')\n        self.c_backward = Chebfun.from_function(np.vectorize(to_fit), [0.0, self.V_total], N=deg_backwards).coefficients().tolist()\n        self.h_from_V_cheb = lambda x : chebval((2.0*x-self.V_total)/(self.V_total), self.c_backward)\n        self.chebyshev = True", "response": "r Method to derive and set coefficients for chebyshev polynomials for the height - volume and volume - height relationships respectively."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _V_solver_error(self, Vtarget, D, L, horizontal, sideA, sideB, sideA_a,\n                       sideB_a, sideA_f, sideA_k, sideB_f, sideB_k,\n                       sideA_a_ratio, sideB_a_ratio):\n        '''Function which uses only the variables given, and the TANK\n        class itself, to determine how far from the desired volume, Vtarget,\n        the volume produced by the specified parameters in a new TANK instance\n        is. Should only be used by solve_tank_for_V method.\n        '''\n        a = TANK(D=float(D), L=float(L), horizontal=horizontal, sideA=sideA, sideB=sideB,\n                 sideA_a=sideA_a, sideB_a=sideB_a, sideA_f=sideA_f,\n                 sideA_k=sideA_k, sideB_f=sideB_f, sideB_k=sideB_k,\n                 sideA_a_ratio=sideA_a_ratio, sideB_a_ratio=sideB_a_ratio)\n        error = abs(Vtarget - a.V_total)\n        return error", "response": "Function which uses only the variables given and the TANK object to determine the error in the desired volume Vtarget"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef plate_enlargement_factor_analytical(amplitude, wavelength):\n        r'''Calculates the enhancement factor of the sinusoidal waves of the\n        plate heat exchanger. This is the multiplier for the flat plate area\n        to obtain the actual area available for heat transfer. Obtained from\n        the following integral:\n    \n        .. math::\n            \\phi = \\frac{\\text{Effective area}}{\\text{Projected area}} \n            = \\frac{\\int_0^\\lambda\\sqrt{1 + \\left(\\frac{\\gamma\\pi}{2}\\right)^2\n            \\cos^2\\left(\\frac{2\\pi}{\\lambda}x\\right)}dx}{\\lambda}\n            \n    .. math::\n            \\gamma = \\frac{4a}{\\lambda}\n            \n        The solution to the integral is:\n            \n        .. math::\n            \\phi = \\frac{2E\\left(\\frac{-4a^2\\pi^2}{\\lambda^2}\\right)}{\\pi}\n            \n        where E is the complete elliptic integral of the second kind, \n        calculated with SciPy.\n    \n        Parameters\n        ----------\n        amplitude : float\n            Half the height of the wave of the ridges, [m]\n        wavelength : float\n            Distance between the bottoms of two of the ridges (sometimes called \n            pitch), [m]\n    \n        Returns\n        -------\n        plate_enlargement_factor : float\n            The extra surface area multiplier as compared to a flat plate\n            caused the corrugations, [-]\n    \n        Notes\n        -----\n        This is the exact analytical integral, obtained via Mathematica, Maple,\n        and quite a bit of trial and error. It is confirmed via numerical \n        integration. The expression normally given is an\n        approximation as follows:\n            \n        .. math::\n            \\phi = \\frac{1}{6}\\left(1+\\sqrt{1+A^2} + 4\\sqrt{1+A^2/2}\\right)\n            \n            A = \\frac{2\\pi a}{\\lambda}\n            \n        Most plate heat exchangers approximate a sinusoidal geometry only.\n    \n        Examples\n        --------\n        >>> PlateExchanger.plate_enlargement_factor_analytical(amplitude=5E-4, wavelength=3.7E-3)\n        1.1611862034509677\n        '''\n        b = 2.*amplitude\n        return 2.*float(ellipe(-b*b*pi*pi/(wavelength*wavelength)))/pi", "response": "Calculates the enhancement factor of the sinusoidal waves of the plate."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef diameter(self, H):\n        r'''Calculates cooling tower diameter at a specified height, using\n        the formulas for either hyperbola, depending on the height specified.\n        \n        .. math::\n            D = D_{throat}\\frac{\\sqrt{H^2 + b^2}}{b}\n            \n        The value of `H` and `b` used in the above equation is as follows:\n            \n            * `H_throat` - H  and `b_lower` if under the throat\n            * `H` - `H_throat` and `b_upper`, if above the throat\n                        \n        Parameters\n        ----------\n        H : float\n            Height at which to calculate the cooling tower diameter, [m]\n    \n        Returns\n        -------\n        D : float\n            Diameter of the cooling tower at the specified height, [m]\n        '''\n        # Compute the diameter at H\n        if H <= self.H_throat:\n            # Height relative to throat height\n            H = self.H_throat - H\n            b = self.b_lower\n        else:\n            H = H - self.H_throat\n            b = self.b_upper\n        R = self.D_throat*(H*H + b*b)**0.5/(2.0*b)\n        return R*2.0", "response": "r Calculates the diameter of the cooling tower at a specified height using the formulas for either hyperbola or throat."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None):\n    '''Port of numpy's linspace to pure python. Does not support dtype, and \n    returns lists of floats.\n    '''\n    num = int(num)\n    start = start * 1.\n    stop = stop * 1.\n\n    if num <= 0:\n        return []\n    if endpoint:\n        if num == 1:\n            return [start]\n        step = (stop-start)/float((num-1))\n        if num == 1:\n            step = nan\n        \n        y = [start]\n        for _ in range(num-2):\n            y.append(y[-1] + step)\n        y.append(stop)\n    else:\n        step = (stop-start)/float(num)\n        if num == 1:\n            step = nan\n        y = [start]\n        for _ in range(num-1):\n            y.append(y[-1] + step)\n    if retstep:\n        return y, step\n    else:\n        return y", "response": "Port of numpy s linspace to pure python. Does not support dtype and \n returns lists of floats."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a list of the n - dimensional elements of a set of resources", "response": "def polyder(c, m=1, scl=1, axis=0):\n    '''not quite a copy of numpy's version because this was faster to implement.\n    '''\n    c = list(c)\n    cnt = int(m)\n\n    if cnt == 0:\n        return c\n\n    n = len(c)\n    if cnt >= n:\n        c = c[:1]*0\n    else:\n        for i in range(cnt):\n            n = n - 1\n            c *= scl\n            der = [0.0 for _ in range(n)]\n            for j in range(n, 0, -1):\n                der[j - 1] = j*c[j]\n            c = der\n    return c"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef horner_log(coeffs, log_coeff, x):\n    '''Technically possible to save one addition of the last term of \n    coeffs is removed but benchmarks said nothing was saved'''\n    tot = 0.0\n    for c in coeffs:\n        tot = tot*x + c\n    return tot + log_coeff*log(x)", "response": "Horner log of a set of coefficients."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef isclose(a, b, rel_tol=1e-9, abs_tol=0.0):\n    '''Pure python and therefore slow version of the standard library isclose.\n    Works on older versions of python though! Hasn't been unit tested, but has\n    been tested.\n    \n    manual unit testing:\n        \n    from math import isclose as isclose2\n    from random import uniform\n    for i in range(10000000):\n        a = uniform(-1, 1)\n        b = uniform(-1, 1)\n        rel_tol = uniform(0, 1)\n        abs_tol = uniform(0, .001)\n        ans1 = isclose(a, b, rel_tol, abs_tol)\n        ans2 = isclose2(a, b, rel_tol=rel_tol, abs_tol=abs_tol)\n        try:\n            assert ans1 == ans2\n        except:\n            print(a, b, rel_tol, abs_tol)\n    \n    '''\n    if (rel_tol < 0.0 or abs_tol < 0.0 ):\n        raise ValueError('Negative tolerances')\n        \n    if ((a.real == b.real) and (a.imag == b.imag)):\n        return True\n    \n    if (isinf(a.real) or isinf(a.imag) or\n        isinf(b.real) or isinf(b.imag)):\n        return False\n\n    diff = abs(a - b)\n    return (((diff <= rel_tol*abs(b)) or\n             (diff <= rel_tol*abs(a))) or (diff <= abs_tol))", "response": "Pure python and therefore slow version of the standard library isclose."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef interp(x, dx, dy, left=None, right=None):\n    '''One-dimensional linear interpolation routine inspired/\n    reimplemented from NumPy for extra speed for scalar values\n    (and also numpy).\n    \n    Returns the one-dimensional piecewise linear interpolant to a function\n    with a given value at discrete data-points.\n    \n    Parameters\n    ----------\n    x : float\n        X-coordinate of the interpolated values, [-]\n    dx : list[float]\n        X-coordinates of the data points, must be increasing, [-]\n    dy : list[float]\n        Y-coordinates of the data points; same length as `dx`, [-]\n    left : float, optional\n        Value to return for `x < dx[0]`, default is `dy[0]`, [-]\n    right : float, optional\n        Value to return for `x > dx[-1]`, default is `dy[-1]`, [-]\n\n    Returns\n    -------\n    y : float\n        The interpolated value, [-]\n    \n    Notes\n    -----\n    This function is \"unsafe\" in that it assumes the x-coordinates\n    are increasing. It also does not check for nan's, that `dx` and `dy`\n    are the same length, and that `x` is scalar.\n    \n    Performance is 40-50% of that of NumPy under CPython.\n    \n    Examples\n    --------\n    >>> interp(2.5, [1, 2, 3], [3, 2, 0])\n    1.0\n    '''\n    lendx = len(dx)\n    j = binary_search(x, dx, lendx)\n    if (j == -1):\n        if left is not None:\n            return left\n        else:\n            return dy[0]\n    elif (j == lendx - 1):\n        return dy[j]\n    elif (j == lendx):\n        if right is not None:\n            return right\n        else:\n            return dy[-1]\n    else:\n        return (dy[j + 1] - dy[j])/(dx[j + 1] - dx[j])*(x - dx[j]) + dy[j]", "response": "One - dimensional linear interpolation routine inspired by NumPy for extra speed for scalar values."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef implementation_optimize_tck(tck):\n    '''Converts 1-d or 2-d splines calculated with SciPy's `splrep` or\n    `bisplrep` to a format for fastest computation - lists in PyPy, and numpy\n    arrays otherwise.\n    \n    Only implemented for 3 and 5 length `tck`s.\n    '''\n    if IS_PYPY:\n        return tck\n    else:\n        if len(tck) == 3:\n            tck[0] = np.array(tck[0])\n            tck[1] = np.array(tck[1])\n        elif len(tck) == 5:\n            tck[0] = np.array(tck[0])\n            tck[1] = np.array(tck[1])\n            tck[2] = np.array(tck[2])\n        else:\n            raise NotImplementedError\n    return tck", "response": "Converts 1 - d or 2 - d splines calculated with SciPy s splrep or\n    bisplrep and numpy\n    arrays otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef damping_maintain_sign(x, step, damping=1.0, factor=0.5):\n    '''Famping function which will maintain the sign of the variable being\n    manipulated. If the step puts it at the other sign, the distance between\n    `x` and `step` will be shortened by the multiple of `factor`; i.e. if \n    factor is `x`, the new value of `x` will be 0 exactly.\n    \n    The provided `damping` is applied as well.\n    \n    Parameters\n    ----------\n    x : float\n        Previous value in iteration, [-]\n    step : float\n        Change in `x`, [-]\n    damping : float, optional\n        The damping factor to be applied always, [-]\n    factor : float, optional\n        If the calculated step changes sign, this factor will be used instead\n        of the step, [-]\n\n    Returns\n    -------\n    x_new : float\n        The new value in the iteration, [-]\n    \n    Notes\n    -----\n    \n    Examples\n    --------\n    >>> damping_maintain_sign(100, -200, factor=.5)\n    50.0\n    '''\n    positive = x > 0.0\n    step_x = x + step\n    \n    if (positive and step_x < 0) or (not positive and step_x > 0.0):\n#        print('damping')\n        step = -factor*x\n    return x + step*damping", "response": "Famping function which will maintain the sign of the variable being\n    manipulated."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef py_bisect(f, a, b, args=(), xtol=_xtol, rtol=_rtol, maxiter=_iter,\n              ytol=None, full_output=False, disp=True):\n    '''Port of SciPy's C bisect routine.\n    '''\n    fa = f(a, *args)\n    fb = f(b, *args)\n    if fa*fb > 0.0:\n        raise ValueError(\"f(a) and f(b) must have different signs\") \n    elif fa == 0.0:\n        return a\n    elif fb == 0.0:\n        return b\n\n    dm = b - a\n    iterations = 0.0\n    \n    for i in range(maxiter):\n        dm *= 0.5\n        xm = a + dm\n        fm = f(xm, *args)\n        if fm*fa >= 0.0:\n            a = xm\n        abs_dm = fabs(dm)\n        \n        if fm == 0.0:\n            return xm\n        elif ytol is not None:\n            if (abs_dm < xtol + rtol*abs_dm) and abs(fm) < ytol:\n                return xm\n        elif (abs_dm < xtol + rtol*abs_dm):\n            return xm\n    raise UnconvergedError(\"Failed to converge after %d iterations\" %maxiter)", "response": "Port of SciPy s C bisect routine."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef py_newton(func, x0, fprime=None, args=(), tol=None, maxiter=_iter,\n              fprime2=None, low=None, high=None, damping=1.0, ytol=None,\n              xtol=1.48e-8, require_eval=False, damping_func=None,\n              bisection=False,\n              kwargs={}):\n    '''Newton's method designed to be mostly compatible with SciPy's \n    implementation, with a few features added and others now implemented.\n    \n    1) No tracking ofo how many iterations have progressed.\n    2) No ability to return a RootResults object\n    3) No warnings on some cases of bad input ( low tolerance, no iterations)\n    4) Ability to accept True for either fprime or fprime2, which means that\n       they are included in the return value of func\n    5) No handling for inf or nan!\n    \n    From scipy, with very minor modifications!\n    https://github.com/scipy/scipy/blob/v1.1.0/scipy/optimize/zeros.py#L66-L206\n    '''\n    if tol is not None:\n        xtol = tol\n    p0 = 1.0*x0\n#    p1 = p0 = 1.0*x0\n#    fval0 = None\n    if bisection:\n        a, b = None, None\n    if fprime is not None:\n        fprime2_included = fprime2 == True\n        fprime_included = fprime == True\n        \n        for iter in range(maxiter):\n            if fprime2_included:\n                fval, fder, fder2 = func(p0, *args, **kwargs)\n            elif fprime_included:\n                fval, fder = func(p0, *args, **kwargs)\n            elif fprime2 is not None:\n                fval = func(p0, *args, **kwargs)\n                fder = fprime(p0, *args, **kwargs)\n                fder2 = fprime2(p0, *args, **kwargs)\n            else:\n                fval = func(p0, *args, **kwargs)\n                fder = fprime(p0, *args, **kwargs)\n            \n            if fder == 0.0 or fval == 0.0:\n                return p0 # Cannot coninue or already finished\n            if bisection:\n                if fval < 0.0:\n                    a = p0\n#                    f_a = fval\n                else:\n                    b = p0\n#                    f_b = fval\n            \n            fder_inv = 1.0/fder\n            # Compute the next point\n            step = fval*fder_inv\n            if damping_func is not None:\n                if fprime2 is not None:\n                    step = step/(1.0 - 0.5*step*fder2*fder_inv)\n                p = damping_func(p0, -step, damping)\n#                variable, derivative, damping_factor\n                \n            elif fprime2 is None:\n                p = p0 - step*damping\n            else:\n                p = p0 - step/(1.0 - 0.5*step*fder2*fder_inv)*damping\n            \n            if bisection and a is not None and b is not None:\n                if not (a < p < b) or (b < p < a):\n#                if p < 0.0:\n#                    if p < a:\n#                    print('bisecting')\n                    p = 0.5*(a + b)\n#                else:\n#                    if p > b:\n#                        p = 0.5*(a + b)\n            \n                \n            if low is not None and p < low:\n                p = low\n            if high is not None and p > high:\n                p = high\n                \n            \n\n            \n            # p0 is last point (fval at that point), p is new \n                      \n            if ytol is not None and xtol is not None:\n                # Meet both tolerance - new value is under ytol, and old value\n                if abs(p - p0) < abs(xtol*p) and abs(fval) < ytol:\n                    if require_eval:\n                        return p0\n                    return p\n            elif xtol is not None:\n                if abs(p - p0) < abs(xtol*p):\n                    if require_eval:\n                        return p0\n                    return p\n            elif ytol is not None:\n                if abs(fval) < ytol:\n                    if require_eval:\n                        return p0\n                    return p\n            \n#            fval0, fval1 = fval, fval0\n#            p0, p1 = p, p0\n# need a history of fval also\n            p0 = p\n    else:\n        return secant(func, x0, args=args, maxiter=maxiter, low=low, high=high,\n                      damping=damping,\n                      xtol=xtol, ytol=ytol, kwargs=kwargs)\n\n    raise UnconvergedError(\"Failed to converge; maxiter (%d) reached, value=%f \" %(maxiter, p))", "response": "This is a wrapper for the Newton s method used to be mostly compatible with SciPy s \n    implementation."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef py_bisplev(x, y, tck, dx=0, dy=0):\n    '''Evaluate a bivariate B-spline or its derivatives.\n    For scalars, returns a float; for other inputs, mimics the formats of \n    SciPy's `bisplev`.\n    \n    Parameters\n    ----------\n    x : float or list[float]\n        x value (rank 1), [-]\n    y : float or list[float]\n        y value (rank 1), [-]\n    tck : tuple(list, list, list, int, int)\n        Tuple of knot locations, coefficients, and the degree of the spline,\n        [tx, ty, c, kx, ky], [-]\n    dx : int, optional\n        Order of partial derivative with respect to `x`, [-]\n    dy : int, optional\n        Order of partial derivative with respect to `y`, [-]\n    \n    Returns\n    -------\n    values : float or list[list[float]]\n        Calculated values from spline or their derivatives; according to the\n        same format as SciPy's `bisplev`, [-]\n    \n    Notes\n    -----\n    Use `bisplrep` to generate the `tck` representation; there is no Python\n    port of it.\n    '''\n    tx, ty, c, kx, ky = tck\n    if isinstance(x, (float, int)):\n        x = [x]\n    if isinstance(y, (float, int)):\n        y = [y]\n\n    z = [[cy_bispev(tx, ty, c, kx, ky, [xi], [yi])[0] for yi in y] for xi in x]\n    if len(x) == len(y) == 1:\n        return z[0][0]\n    return z", "response": "Evaluate a bivariate B - spline or its derivatives."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fpbspl(t, n, k, x, l, h, hh):\n    h[0] = 1.0\n    for j in range(1, k + 1):\n        hh[0:j] = h[0:j]\n        h[0] = 0.0\n        for i in range(j):\n            li = l+i\n            f = hh[i]/(t[li] - t[li - j])\n            h[i] = h[i] + f*(t[li] - x)\n            h[i + 1] = f*(x - t[li - j])", "response": "This function evaluates the non - zero b - splines of the degree k at t l < x < l."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cy_bispev(tx, ty, c, kx, ky, x, y):\n    '''Possible optimization: Do not evaluate derivatives, ever.\n    '''\n    nx = len(tx)\n    ny = len(ty)\n    mx = len(x)\n    my = len(y)\n\n    kx1 = kx + 1\n    ky1 = ky + 1\n\n    nkx1 = nx - kx1\n    nky1 = ny - ky1\n    \n    wx = [[0.0]*kx1]*mx\n    wy = [[0.0]*ky1]*my\n    lx = [0]*mx\n    ly = [0]*my\n\n    size_z = mx*my\n    \n    z = [0.0]*size_z\n    init_w(tx, kx, x, lx, wx)\n    init_w(ty, ky, y, ly, wy)\n    \n    for j in range(my):\n        for i in range(mx):\n            sp = 0.0\n            err = 0.0\n            for i1 in range(kx1):\n                for j1 in range(ky1):\n                    l2 = lx[i]*nky1 + ly[j] + i1*nky1 + j1\n                    a = c[l2]*wx[i][i1]*wy[j][j1] - err\n                    tmp = sp + a\n                    err = (tmp - sp) - a\n                    sp = tmp\n            z[j*mx + i] += sp\n    return z", "response": "This function computes the Bispev algorithm."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef round_edge_grill(alpha, l=None, Dh=None, fd=None):\n    r'''Returns the loss coefficient for a rounded square grill or square bar\n    screen or perforated plate with rounded edges of thickness l, as shown in\n    [1]_.\n\n    for Dh < l < 50D\n\n    .. math::\n        K = lookup(alpha)\n\n    else:\n\n    .. math::\n        K = lookup(alpha) + \\frac{fl}{\\alpha^2D}\n\n    Parameters\n    ----------\n    alpha : float\n        Fraction of grill open to flow [-]\n    l : float, optional\n        Thickness of the grill or plate [m]\n    Dh : float, optional\n        Hydraulic diameter of gap in grill, [m]\n    fd : float, optional\n        Darcy friction factor [-]\n\n    Returns\n    -------\n    K : float\n        Loss coefficient [-]\n\n    Notes\n    -----\n    If l, Dh, or fd is not provided, the first expression is used instead.\n    The alteration of the expression to include friction factor is there\n    if the grill is long enough to have considerable friction along the\n    surface of the grill.\n    alpha must be between 0.3 and 0.7.\n\n    Examples\n    --------\n    >>> round_edge_grill(.4)\n    1.0\n    >>> round_edge_grill(.4, l=.15, Dh=.002, fd=.0185)\n    2.3874999999999997\n\n    References\n    ----------\n    .. [1] Blevins, Robert D. Applied Fluid Dynamics Handbook. New York, N.Y.:\n       Van Nostrand Reinhold Co., 1984.\n    '''\n    t1 = float(splev(alpha, grills_rounded_tck))\n    if Dh and l and fd and l > 50*Dh:\n        return t1 + fd*l/Dh\n    else:\n        return t1", "response": "r Returns the loss coefficient for a rounded grill or square bar based on the given parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef control_valve_choke_P_g(xT, gamma, P1=None, P2=None):\n    r'''Calculates either the upstream or downstream pressure at which choked\n    flow though a gas control valve occurs, given either a set upstream or \n    downstream pressure. Implements an analytical solution of \n    the needed equations from the full function\n    :py:func:`~.size_control_valve_g`. A singularity arises as `xT` goes to 1\n    and `gamma` goes to 1.4.\n    \n    .. math::\n        P_1 = - \\frac{7 P_{2}}{5 \\gamma x_T - 7}\n        \n    .. math::\n        P_2 = \\frac{P_{1}}{7} \\left(- 5 \\gamma x_T + 7\\right)\n    \n    Parameters\n    ----------\n    xT : float, optional\n        Pressure difference ratio factor of a valve without fittings at choked\n        flow [-]\n    gamma : float\n        Specific heat capacity ratio [-]\n    P1 : float, optional\n        Absolute pressure upstream of the valve [Pa]\n    P2 : float, optional\n        Absolute pressure downstream of the valve [Pa]\n\n    Returns\n    -------\n    P_choke : float\n        Pressure at which a choke occurs in the gas valve [Pa]\n\n    Notes\n    -----\n    Extremely cheap to compute.\n    \n    Examples\n    --------\n    >>> control_valve_choke_P_g(1, 1.3, 1E5)\n    7142.857142857143\n    >>> control_valve_choke_P_g(1, 1.3, P2=7142.857142857143)\n    100000.0\n    '''\n    if P2 is None:\n        ans = P2 = P1*(-5.0*gamma*xT + 7.0)/7.0\n    elif P1 is None:\n        ans = P1 = -7.0*P2/(5.0*gamma*xT - 7.0)\n    else:\n        raise Exception('Either P1 or P2 needs to be specified')\n    return ans", "response": "r Calculates the upstream or downstream pressure at which a choke occurs in the set upstream or downstream pressure."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_choked_turbulent_g(x, Fgamma, xT=None, xTP=None):\n    r'''Calculates if a gas flow in IEC 60534 calculations is critical or\n    not, for use in IEC 60534 gas valve sizing calculations.\n    Either xT or xTP must be provided, depending on the calculation process.\n\n    .. math::\n        x \\ge F_\\gamma x_T\n\n    .. math::\n        x \\ge F_\\gamma x_{TP}\n\n    Parameters\n    ----------\n    x : float\n        Differential pressure over inlet pressure, [-]\n    Fgamma : float\n        Specific heat ratio factor [-]\n    xT : float, optional\n        Pressure difference ratio factor of a valve without fittings at choked\n        flow [-]\n    xTP : float\n        Pressure difference ratio factor of a valve with fittings at choked\n        flow [-]\n\n    Returns\n    -------\n    choked : bool\n        Whether or not the flow is choked [-]\n\n    Examples\n    --------\n    Example 3, compressible flow, non-choked with attached fittings:\n\n    >>> is_choked_turbulent_g(0.544, 0.929, 0.6)\n    False\n    >>> is_choked_turbulent_g(0.544, 0.929, xTP=0.625)\n    False\n\n    References\n    ----------\n    .. [1] IEC 60534-2-1 / ISA-75.01.01-2007\n    '''\n    if xT:\n        return x >= Fgamma*xT\n    elif xTP:\n        return x >= Fgamma*xTP\n    else:\n        raise Exception('Either xT or xTP is needed')", "response": "r Calculates if a flow in IEC 60534 gas valve sizing calculations is critical or critical or not."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef Reynolds_valve(nu, Q, D1, FL, Fd, C):\n    r'''Calculates Reynolds number of a control valve for a liquid or gas\n    flowing through it at a specified Q, for a specified D1, FL, Fd, C, and\n    with kinematic viscosity `nu` according to IEC 60534 calculations.\n\n    .. math::\n        Re_v = \\frac{N_4 F_d Q}{\\nu \\sqrt{C F_L}}\\left(\\frac{F_L^2 C^2}\n        {N_2D^4} +1\\right)^{1/4}\n\n    Parameters\n    ----------\n    nu : float\n        Kinematic viscosity, [m^2/s]\n    Q : float\n        Volumetric flow rate of the fluid [m^3/s]\n    D1 : float\n        Diameter of the pipe before the valve [m]\n    FL : float, optional\n        Liquid pressure recovery factor of a control valve without attached \n        fittings []\n    Fd : float\n        Valve style modifier [-]\n    C : float\n        Metric Kv valve flow coefficient (flow rate of water at a pressure drop  \n        of 1 bar) [m^3/hr]\n\n    Returns\n    -------\n    Rev : float\n        Valve reynolds number [-]\n\n    Examples\n    --------\n    >>> Reynolds_valve(3.26e-07, 360, 150.0, 0.9, 0.46, 165)\n    2966984.7525455453\n\n    References\n    ----------\n    .. [1] IEC 60534-2-1 / ISA-75.01.01-2007\n    '''\n    return N4*Fd*Q/nu/(C*FL)**0.5*(FL**2*C**2/(N2*D1**4) + 1)**0.25", "response": "r Returns the number of a control valve flowing through a liquid or gas base on a specified fluid."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef loss_coefficient_piping(d, D1=None, D2=None):\n    r'''Calculates the sum of loss coefficients from possible\n    inlet/outlet reducers/expanders around a control valve according to\n    IEC 60534 calculations.\n\n    .. math::\n        \\Sigma \\xi = \\xi_1 + \\xi_2 + \\xi_{B1} - \\xi_{B2}\n\n    .. math::\n        \\xi_1 = 0.5\\left[1 -\\left(\\frac{d}{D_1}\\right)^2\\right]^2\n\n    .. math::\n        \\xi_2 = 1.0\\left[1 -\\left(\\frac{d}{D_2}\\right)^2\\right]^2\n\n    .. math::\n        \\xi_{B1} = 1 - \\left(\\frac{d}{D_1}\\right)^4\n\n    .. math::\n        \\xi_{B2} = 1 - \\left(\\frac{d}{D_2}\\right)^4\n\n    Parameters\n    ----------\n    d : float\n        Diameter of the valve [m]\n    D1 : float\n        Diameter of the pipe before the valve [m]\n    D2 : float\n        Diameter of the pipe after the valve [m]\n\n    Returns\n    -------\n    loss : float\n        Sum of the four loss coefficients [-]\n\n    Examples\n    --------\n    In example 3, non-choked compressible flow with fittings:\n\n    >>> loss_coefficient_piping(0.05, 0.08, 0.1)\n    0.6580810546875\n\n    References\n    ----------\n    .. [1] IEC 60534-2-1 / ISA-75.01.01-2007\n    '''\n    loss = 0.\n    if D1:\n        loss += 1. - (d/D1)**4 # Inlet flow energy\n        loss += 0.5*(1. - (d/D1)**2)**2 # Inlet reducer\n    if D2:\n        loss += 1.0*(1. - (d/D2)**2)**2 # Outlet reducer (expander)\n        loss -= 1. - (d/D2)**4 # Outlet flow energy\n    return loss", "response": "r Calculates the sum of loss coefficients from a possible base on the pipe."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef Reynolds_factor(FL, C, d, Rev, full_trim=True):\n    r'''Calculates the Reynolds number factor `FR` for a valve with a Reynolds\n    number `Rev`, diameter `d`, flow coefficient `C`, liquid pressure recovery\n    factor `FL`, and with either full or reduced trim, all according to\n    IEC 60534 calculations.\n\n\n    If full trim:\n\n    .. math::\n        F_{R,1a} = 1 + \\left(\\frac{0.33F_L^{0.5}}{n_1^{0.25}}\\right)\\log_{10}\n        \\left(\\frac{Re_v}{10000}\\right)\n\n    .. math::\n        F_{R,2} = \\min(\\frac{0.026}{F_L}\\sqrt{n_1 Re_v},\\; 1)\n\n    .. math::\n        n_1 = \\frac{N_2}{\\left(\\frac{C}{d^2}\\right)^2}\n\n    .. math::\n        F_R = F_{R,2} \\text{ if Rev < 10 else } \\min(F_{R,1a}, F_{R,2})\n\n    Otherwise :\n\n    .. math::\n        F_{R,3a} = 1 + \\left(\\frac{0.33F_L^{0.5}}{n_2^{0.25}}\\right)\\log_{10}\n        \\left(\\frac{Re_v}{10000}\\right)\n\n    .. math::\n        F_{R,4} = \\frac{0.026}{F_L}\\sqrt{n_2 Re_v}\n\n    .. math::\n        n_2 = 1 + N_{32}\\left(\\frac{C}{d}\\right)^{2/3}\n\n    .. math::\n        F_R = F_{R,4} \\text{ if Rev < 10 else } \\min(F_{R,3a}, F_{R,4})\n\n    Parameters\n    ----------\n    FL : float\n        Liquid pressure recovery factor of a control valve without attached\n        fittings []\n    C : float\n        Metric Kv valve flow coefficient (flow rate of water at a pressure drop  \n        of 1 bar) [m^3/hr]\n    d : float\n        Diameter of the valve [m]\n    Rev : float\n        Valve reynolds number [-]\n    full_trim : bool\n        Whether or not the valve has full trim\n\n    Returns\n    -------\n    FR : float\n        Reynolds number factor for laminar or transitional flow []\n\n    Examples\n    --------\n    In Example 4, compressible flow with small flow trim sized for gas flow\n    (Cv in the problem was converted to Kv here to make FR match with N32, N2):\n\n    >>> Reynolds_factor(FL=0.98, C=0.015483, d=15., Rev=1202., full_trim=False)\n    0.7148753122302025\n\n\n    References\n    ----------\n    .. [1] IEC 60534-2-1 / ISA-75.01.01-2007\n    '''\n    if full_trim:\n        n1 = N2/(min(C/d**2, 0.04))**2 # C/d**2 must not exceed 0.04\n        FR_1a = 1 + (0.33*FL**0.5)/n1**0.25*log10(Rev/10000.)\n        FR_2 = 0.026/FL*(n1*Rev)**0.5\n        if Rev < 10:\n            FR = FR_2\n        else:\n            FR = min(FR_2, FR_1a)\n    else:\n        n2 = 1 + N32*(C/d**2)**(2/3.)\n        FR_3a = 1 + (0.33*FL**0.5)/n2**0.25*log10(Rev/10000.)\n        FR_4 = min(0.026/FL*(n2*Rev)**0.5, 1)\n        if Rev < 10:\n            FR = FR_4\n        else:\n            FR = min(FR_3a, FR_4)\n    return FR", "response": "r Returns the Reynolds number factor FR for a valve with a Liquid pressure recovery factor C and liquid pressure recovery factor Rev."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef size_control_valve_l(rho, Psat, Pc, mu, P1, P2, Q, D1=None, D2=None,\n                         d=None, FL=0.9, Fd=1, allow_choked=True, \n                         allow_laminar=True, full_output=False):\n    r'''Calculates flow coefficient of a control valve passing a liquid\n    according to IEC 60534. Uses a large number of inputs in SI units. Note the\n    return value is not standard SI. All parameters are required.\n    This sizing model does not officially apply to liquid mixtures, slurries,\n    non-Newtonian fluids, or liquid-solid conveyance systems. For details\n    of the calculations, consult [1]_.\n\n    Parameters\n    ----------\n    rho : float\n        Density of the liquid at the inlet [kg/m^3]\n    Psat : float\n        Saturation pressure of the fluid at inlet temperature [Pa]\n    Pc : float\n        Critical pressure of the fluid [Pa]\n    mu : float\n        Viscosity of the fluid [Pa*s]\n    P1 : float\n        Inlet pressure of the fluid before valves and reducers [Pa]\n    P2 : float\n        Outlet pressure of the fluid after valves and reducers [Pa]\n    Q : float\n        Volumetric flow rate of the fluid [m^3/s]\n    D1 : float, optional\n        Diameter of the pipe before the valve [m]\n    D2 : float, optional\n        Diameter of the pipe after the valve [m]\n    d : float, optional\n        Diameter of the valve [m]\n    FL : float, optional\n        Liquid pressure recovery factor of a control valve without attached \n        fittings (normally 0.8-0.9 at full open and decreasing as opened \n        further to below 0.5; use default very cautiously!) []\n    Fd : float, optional\n        Valve style modifier (0.1 to 1; varies tremendously depending on the\n        type of valve and position; do not use the default at all!) []\n    allow_choked : bool, optional\n        Overrides the automatic transition into the choked regime if this is\n        False and returns as if choked flow does not exist\n    allow_laminar : bool, optional\n        Overrides the automatic transition into the laminar regime if this is\n        False and returns as if laminar flow does not exist\n    full_output : bool, optional\n        If True, returns intermediate calculation values as\n        well as Kv in the form of a dictionary containing 'Kv', 'Rev', 'choked',\n        'FL', 'FLP', 'FR', 'FP', and 'laminar'. Some may be None if they are \n        not used in the calculation.\n\n    Returns\n    -------\n    Kv : float\n        Metric Kv valve flow coefficient (flow rate of water at a pressure drop  \n        of 1 bar) [m^3/hr]\n        \n    Notes\n    -----\n    It is possible to use this model without any diameters specified; in that\n    case, turbulent flow is assumed. Choked flow can still be modeled. This is\n    not recommended. All three diameters need to be None for this to work. \n    `FL` and `Fd` are not used by the models when the diameters are not \n    specified.\n\n    Examples\n    --------\n    From [1]_, matching example 1 for a globe, parabolic plug,\n    flow-to-open valve.\n\n    >>> size_control_valve_l(rho=965.4, Psat=70.1E3, Pc=22120E3, mu=3.1472E-4,\n    ... P1=680E3, P2=220E3, Q=0.1, D1=0.15, D2=0.15, d=0.15,\n    ... FL=0.9, Fd=0.46)\n    164.9954763704956\n\n    From [1]_, matching example 2 for a ball, segmented ball,\n    flow-to-open valve.\n\n    >>> size_control_valve_l(rho=965.4, Psat=70.1E3, Pc=22120E3, mu=3.1472E-4,\n    ... P1=680E3, P2=220E3, Q=0.1, D1=0.1, D2=0.1, d=0.1,\n    ... FL=0.6, Fd=0.98)\n    238.05817216710483\n    \n    References\n    ----------\n    .. [1] IEC 60534-2-1 / ISA-75.01.01-2007\n    '''\n    if full_output:\n        ans = {'FLP': None, 'FP': None, 'FR': None}\n    # Pa to kPa, according to constants in standard\n    P1, P2, Psat, Pc = P1/1000., P2/1000., Psat/1000., Pc/1000.\n    Q = Q*3600. # m^3/s to m^3/hr, according to constants in standard\n    nu = mu/rho # kinematic viscosity used in standard\n\n    dP = P1 - P2\n    FF = FF_critical_pressure_ratio_l(Psat=Psat, Pc=Pc)\n    choked = is_choked_turbulent_l(dP=dP, P1=P1, Psat=Psat, FF=FF, FL=FL)\n    if choked and allow_choked:\n        # Choked flow, equation 3\n        C = Q/N1/FL*(rho/rho0/(P1 - FF*Psat))**0.5\n    else:\n        # non-choked flow, eq 1\n        C = Q/N1*(rho/rho0/dP)**0.5\n    if D1 is None and D2 is None and d is None:\n        # Assume turbulent if no diameters are provided, no other calculations\n        Rev = 1e5\n    else:\n        # m to mm, according to constants in standard\n        D1, D2, d = D1*1000., D2*1000., d*1000.\n        Rev = Reynolds_valve(nu=nu, Q=Q, D1=D1, FL=FL, Fd=Fd, C=C)\n        # normal calculation path\n        if (Rev > 10000 or not allow_laminar) and (D1 != d or D2 != d):\n            # liquid, using Fp and FLP\n            FP = 1\n            Ci = C\n            def iterate_piping_turbulent(Ci):\n                loss = loss_coefficient_piping(d, D1, D2)\n                FP = (1 + loss/N2*(Ci/d**2)**2)**-0.5\n                loss_upstream = loss_coefficient_piping(d, D1)\n                FLP = FL*(1 + FL**2/N2*loss_upstream*(Ci/d**2)**2)**-0.5\n                choked = is_choked_turbulent_l(dP, P1, Psat, FF, FLP=FLP, FP=FP)\n                if choked:\n                    # Choked flow with piping, equation 4\n                    C = Q/N1/FLP*(rho/rho0/(P1-FF*Psat))**0.5\n                else:\n                    # Non-Choked flow with piping, equation 4\n                    C = Q/N1/FP*(rho/rho0/dP)**0.5\n                if Ci/C < 0.99:\n                    C = iterate_piping_turbulent(C)\n                    \n                if full_output:\n                    ans['FLP'] = FLP\n                    ans['FP'] = FP\n                return C\n    \n            C = iterate_piping_turbulent(Ci)\n        elif Rev <= 10000 and allow_laminar:\n            # Laminar\n            def iterate_piping_laminar(C):\n                Ci = 1.3*C\n                Rev = Reynolds_valve(nu=nu, Q=Q, D1=D1, FL=FL, Fd=Fd, C=Ci)                \n                if Ci/d**2 > 0.016*N18:\n                    FR = Reynolds_factor(FL=FL, C=Ci, d=d, Rev=Rev, full_trim=False)\n                else:\n                    FR = Reynolds_factor(FL=FL, C=Ci, d=d, Rev=Rev, full_trim=True)\n                if C/FR >= Ci:\n                    Ci = iterate_piping_laminar(Ci) # pragma: no cover\n                    \n                if full_output:\n                    ans['Rev'] = Rev\n                    ans['FR'] = FR\n                return Ci\n            C = iterate_piping_laminar(C)\n    if full_output:\n        ans['FF'] = FF\n        ans['choked'] = choked\n        ans['Kv'] = C\n        ans['laminar'] = Rev <= 10000\n        \n        # For the laminar case this is already set and needs to not be overwritten\n        if 'Rev' not in ans:\n            ans['Rev'] = Rev\n        return ans\n    else:\n        return C", "response": "r Returns a size of a control valve in liquid."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef size_control_valve_g(T, MW, mu, gamma, Z, P1, P2, Q, D1=None, D2=None, \n                         d=None, FL=0.9, Fd=1, xT=0.7, allow_choked=True, \n                         allow_laminar=True, full_output=False):\n    r'''Calculates flow coefficient of a control valve passing a gas\n    according to IEC 60534. Uses a large number of inputs in SI units. Note the\n    return value is not standard SI. All parameters are required. For details\n    of the calculations, consult [1]_. Note the inlet gas flow conditions.\n\n    Parameters\n    ----------\n    T : float\n        Temperature of the gas at the inlet [K]\n    MW : float\n        Molecular weight of the gas [g/mol]\n    mu : float\n        Viscosity of the fluid at inlet conditions [Pa*s]\n    gamma : float\n        Specific heat capacity ratio [-]\n    Z : float\n        Compressibility factor at inlet conditions, [-]\n    P1 : float\n        Inlet pressure of the gas before valves and reducers [Pa]\n    P2 : float\n        Outlet pressure of the gas after valves and reducers [Pa]\n    Q : float\n        Volumetric flow rate of the gas at *273.15 K* and 1 atm specifically\n        [m^3/s]\n    D1 : float, optional\n        Diameter of the pipe before the valve [m]\n    D2 : float, optional\n        Diameter of the pipe after the valve [m]\n    d : float, optional\n        Diameter of the valve [m]        \n    FL : float, optional\n        Liquid pressure recovery factor of a control valve without attached \n        fittings (normally 0.8-0.9 at full open and decreasing as opened \n        further to below 0.5; use default very cautiously!) []\n    Fd : float, optional\n        Valve style modifier (0.1 to 1; varies tremendously depending on the\n        type of valve and position; do not use the default at all!) []\n    xT : float, optional\n        Pressure difference ratio factor of a valve without fittings at choked\n        flow (increasing to 0.9 or higher as the valve is closed further and \n        decreasing to 0.1 or lower as the valve is opened further; use default\n        very cautiously!) [-]\n    allow_choked : bool, optional\n        Overrides the automatic transition into the choked regime if this is\n        False and returns as if choked flow does not exist\n    allow_laminar : bool, optional\n        Overrides the automatic transition into the laminar regime if this is\n        False and returns as if laminar flow does not exist\n    full_output : bool, optional\n        If True, returns intermediate calculation values as\n        well as Kv in the form of a dictionary containing 'Kv', 'Rev', 'choked',\n        'Y', 'FR', 'FP', 'xTP', and 'laminar'. Some may be None if they are \n        not used in the calculation.\n        \n    Returns\n    -------\n    Kv : float\n        Metric Kv valve flow coefficient (flow rate of water at a pressure drop  \n        of 1 bar) [m^3/hr]\n\n    Notes\n    -----\n    It is possible to use this model without any diameters specified; in that\n    case, turbulent flow is assumed. Choked flow can still be modeled. This is\n    not recommended. All three diameters need to be None for this to work.\n    `FL` and `Fd` are not used by the models when the diameters are not \n    specified, but `xT` definitely is used by the model.\n    \n    Examples\n    --------\n    From [1]_, matching example 3 for non-choked gas flow with attached\n    fittings  and a rotary, eccentric plug, flow-to-open control valve:\n\n    >>> size_control_valve_g(T=433., MW=44.01, mu=1.4665E-4, gamma=1.30,\n    ... Z=0.988, P1=680E3, P2=310E3, Q=38/36., D1=0.08, D2=0.1, d=0.05,\n    ... FL=0.85, Fd=0.42, xT=0.60)\n    72.58664545391052\n\n    From [1]_, roughly matching example 4 for a small flow trim sized tapered\n    needle plug valve. Difference is 3% and explained by the difference in\n    algorithms used.\n\n    >>> size_control_valve_g(T=320., MW=39.95, mu=5.625E-5, gamma=1.67, Z=1.0,\n    ... P1=2.8E5, P2=1.3E5, Q=0.46/3600., D1=0.015, D2=0.015, d=0.015, FL=0.98,\n    ... Fd=0.07, xT=0.8)\n    0.016498765335995726\n\n    References\n    ----------\n    .. [1] IEC 60534-2-1 / ISA-75.01.01-2007\n    '''\n    MAX_C_POSSIBLE = 1E40 # Quit iterations if C reaches this high\n    # Pa to kPa, according to constants in standard\n    P1, P2 = P1/1000., P2/1000.\n    Q = Q*3600. # m^3/s to m^3/hr, according to constants in standard\n    # Convert dynamic viscosity to kinematic viscosity\n    Vm = Z*R*T/(P1*1000)\n    rho = (Vm)**-1*MW/1000.\n    nu = mu/rho # kinematic viscosity used in standard\n\n    dP = P1 - P2\n    Fgamma = gamma/1.40\n    x = dP/P1\n    Y = max(1 - x/(3*Fgamma*xT), 2/3.)\n\n    choked = is_choked_turbulent_g(x, Fgamma, xT)\n    if choked and allow_choked:\n        # Choked, and flow coefficient from eq 14a\n        C = Q/(N9*P1*Y)*(MW*T*Z/xT/Fgamma)**0.5\n    else:\n        # Non-choked, and flow coefficient from eq 8a\n        C = Q/(N9*P1*Y)*(MW*T*Z/x)**0.5\n\n\n    if full_output:\n        ans = {'FP': None, 'xTP': None, 'FR': None, \n               'choked': choked, 'Y': Y}\n\n    if D1 is None and D2 is None and d is None:\n        # Assume turbulent if no diameters are provided, no other calculations\n        Rev = 1e5\n        if full_output:\n            ans['Rev'] = None\n    else:\n        # m to mm, according to constants in standard\n        D1, D2, d = D1*1000., D2*1000., d*1000. # Convert diameters to mm which is used in the standard\n        Rev = Reynolds_valve(nu=nu, Q=Q, D1=D1, FL=FL, Fd=Fd, C=C)\n        if full_output:\n            ans['Rev'] = Rev\n\n        if (Rev > 10000 or not allow_laminar) and (D1 != d or D2 != d):\n            # gas, using xTP and FLP\n            FP = 1.\n            MAX_ITER = 20\n            def iterate_piping_coef(Ci, iterations):\n                loss = loss_coefficient_piping(d, D1, D2)\n                FP = (1. + loss/N2*(Ci/d**2)**2)**-0.5\n                loss_upstream = loss_coefficient_piping(d, D1)\n                xTP = xT/FP**2/(1 + xT*loss_upstream/N5*(Ci/d**2)**2)\n                choked = is_choked_turbulent_g(x, Fgamma, xTP=xTP)\n                if choked:\n                    # Choked flow with piping, equation 17a\n                    C = Q/(N9*FP*P1*Y)*(MW*T*Z/xTP/Fgamma)**0.5\n                else:\n                    # Non-choked flow with piping, equation 11a\n                    C = Q/(N9*FP*P1*Y)*(MW*T*Z/x)**0.5\n                if Ci/C < 0.99 and iterations < MAX_ITER and Ci < MAX_C_POSSIBLE:\n                    C = iterate_piping_coef(C, iterations+1)\n                if full_output:\n                    ans['xTP'] = xTP\n                    ans['FP'] = FP\n                    ans['choked'] = choked\n                    if MAX_ITER == iterations or Ci >= MAX_C_POSSIBLE:\n                        ans['warning'] = 'Not converged in inner loop'\n                return C\n            C = iterate_piping_coef(C, 0)\n        elif Rev <= 10000 and allow_laminar:\n            # Laminar;\n            def iterate_piping_laminar(C):\n                Ci = 1.3*C\n                Rev = Reynolds_valve(nu=nu, Q=Q, D1=D1, FL=FL, Fd=Fd, C=Ci)\n                if Ci/d**2 > 0.016*N18:\n                    FR = Reynolds_factor(FL=FL, C=Ci, d=d, Rev=Rev, full_trim=False)\n                else:\n                    FR = Reynolds_factor(FL=FL, C=Ci, d=d, Rev=Rev, full_trim=True)\n                if C/FR >= Ci:\n                    Ci = iterate_piping_laminar(Ci)\n                if full_output:\n                    ans['FR'] = FR\n                    ans['Rev'] = Rev\n                return Ci\n            C = iterate_piping_laminar(C)\n    if full_output:\n        ans['Kv'] = C\n        ans['laminar'] = Rev <= 10000\n        return ans\n    else:\n        return C", "response": "r Returns the flow coefficient of a control valve in the inlet gas at the given temperature MW gamma and Z."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting from one flow coefficient scale to another.", "response": "def convert_flow_coefficient(flow_coefficient, old_scale, new_scale):\n    '''Convert from one flow coefficient scale to another; supports the `Kv`\n    `Cv`, and `Av` scales.\n    \n    Other scales are `Qn` and `Cg`, but clear definitions have yet to be\n    found.\n    \n    Parameters\n    ----------\n    flow_coefficient : float\n        Value of the flow coefficient to be converted, expressed in the \n        original scale.\n    old_scale : str\n        String specifying the original scale; one of 'Av', 'Cv', or 'Kv', [-]\n    new_scale : str\n        String specifying the new scale; one of 'Av', 'Cv', or 'Kv', [-]\n    \n    Returns\n    -------\n    converted_flow_coefficient : float \n        Flow coefficient converted to the specified scale.\n    \n    Notes\n    -----\n    `Qn` is a scale based on a flow of air in units of L/minute as air travels\n    through a valve and loses one bar of pressure (initially 7 bar absolute,\n    to 6 bar absolute). No consistent conversion factors have been found and \n    those from theory do not match what have been found. Some uses of `Qn` use\n    its flow rate as in normal (STP reference conditions) flow rate of air;\n    others use something like the 7 bar absolute condition.\n\n    Examples\n    --------\n    >>> convert_flow_coefficient(10, 'Kv', 'Av')\n    0.0002776532068951358\n    '''\n    # Convert from `old_scale` to Kv\n    if old_scale == 'Cv':\n        Kv = Cv_to_Kv(flow_coefficient)\n    elif old_scale == 'Kv':\n        Kv = flow_coefficient\n    elif old_scale == 'Av':\n        Cv = flow_coefficient/((rho0/psi)**0.5*gallon/minute)\n        Kv = Cv_to_Kv(Cv)\n    else:\n        raise NotImplementedError(\"%s scale is unsupported\" %old_scale)\n\n    if new_scale == 'Cv':\n        ans = Kv_to_Cv(Kv)\n    elif new_scale == 'Kv':\n        ans = Kv\n    elif new_scale == 'Av':\n        Cv = Kv_to_Cv(Kv)\n        ans = Cv*((rho0/psi)**0.5*gallon/minute)\n    else:\n        raise NotImplementedError(\"%s scale is unsupported\" %old_scale)\n\n    return ans"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef control_valve_noise_l_2015(m, P1, P2, Psat, rho, c, Kv, d, Di, FL, Fd,\n                               t_pipe, rho_pipe=7800.0, c_pipe=5000.0, \n                               rho_air=1.2, c_air=343.0, xFz=None, An=-4.6):\n    r'''Calculates the sound made by a liquid flowing through a control valve\n    according to the standard IEC 60534-8-4 (2015) [1]_.\n\n    Parameters\n    ----------\n    m : float\n        Mass flow rate of liquid through the control valve, [kg/s]\n    P1 : float\n        Inlet pressure of the fluid before valves and reducers [Pa]\n    P2 : float\n        Outlet pressure of the fluid after valves and reducers [Pa]\n    Psat : float\n        Saturation pressure of the fluid at inlet temperature [Pa]\n    rho : float\n        Density of the liquid at the inlet [kg/m^3]\n    c : float\n        Speed of sound of the liquid at the inlet conditions [m/s]\n    Kv : float\n        Metric Kv valve flow coefficient (flow rate of water at a pressure drop  \n        of 1 bar) [m^3/hr]\n    d : float\n        Diameter of the valve [m]\n    Di : float\n        Internal diameter of the pipe before and after the valve [m]\n    FL : float, optional\n        Liquid pressure recovery factor of a control valve without attached \n        fittings (normally 0.8-0.9 at full open and decreasing as opened \n        further to below 0.5) [-]\n    Fd : float, optional\n        Valve style modifier [-]\n    t_pipe : float\n        Wall thickness of the pipe after the valve, [m]\n    rho_pipe : float, optional\n        Density of the pipe wall material at flowing conditions, [kg/m^3]\n    c_pipe : float, optional\n        Speed of sound of the pipe wall material at flowing conditions, [m/s]\n    rho_air : float, optional\n        Density of the air surrounding the valve and pipe wall, [kg/m^3]\n    c_air : float, optional\n        Speed of sound of the air surrounding the valve and pipe wall, [m/s]\n    xFz : float, optional\n        If specified, this value `xFz` is used instead of estimated; the \n        calculation is sensitive to this value, [-]\n    An : float, optional\n        Valve correction factor for acoustic efficiency\n\n    Returns\n    -------\n    LpAe1m : float\n        A weighted sound pressure level 1 m from the pipe wall, 1 m distance\n        dowstream of the valve (at reference sound pressure level 2E-5), [dBA]\n        \n    Notes\n    -----\n    For formulas see [1]_. This takes on the order of 100 us to compute.\n    This model can also tell if noise is being produced in a valve just due to\n    turbulent flow, or cavitation. For values of `An`, see [1]_; it is\n    normally -4.6 for globel valves, -4.3 for butterfly valves, and -4.0 for\n    expanders.\n    \n    This model was checked against three examples in [1]_; they match to all\n    given decimals.\n    \n    A formula is given in [1]_ for multihole trim valves to estimate `xFz`\n    as well; this is not implemented here and `xFz` must be calculated by the\n    user separately. The formula is\n    \n    .. math::\n        x_{Fz} = \\left(4.5 + 1650\\frac{N_0d_H^2}{F_L}\\right)^{-1/2}\n        \n    Where `N0` is the number of open channels and `dH` is the multihole trim\n    hole diameter.\n\n    Examples\n    --------\n    >>> control_valve_noise_l_2015(m=40, P1=1E6, P2=6.5E5, Psat=2.32E3, \n    ... rho=997, c=1400, Kv=77.848, d=0.1, Di=0.1071, FL=0.92, Fd=0.42, \n    ... t_pipe=0.0036, rho_pipe=7800.0, c_pipe=5000.0,rho_air=1.293, \n    ... c_air=343.0, An=-4.6)\n    81.58200097996539\n\n    References\n    ----------\n    .. [1] IEC 60534-8-4\u202f: Industrial-Process Control Valves - Part 8-4: Noise \n       Considerations - Prediction of Noise Generated by Hydrodynamic Flow.\n       (2015)\n    '''\n    # Convert Kv to Cv as C\n    N34 = 1.17 # for Cv - conversion constant but not to many decimals\n    N14 = 0.0046\n    \n    C = Kv_to_Cv(Kv)\n    xF = (P1-P2)/(P1-Psat)\n    dPc = min(P1-P2, FL*FL*(P1 - Psat))\n    \n    if xFz is None:\n        xFz = 0.9*(1.0 + 3.0*Fd*(C/(N34*FL))**0.5)**-0.5\n    xFzp1 = xFz*(6E5/P1)**0.125\n    \n    Dj = N14*Fd*(C*FL)**0.5\n    \n    Uvc = 1.0/FL*(2.0*dPc/rho)**0.5\n    Wm = 0.5*m*Uvc*Uvc*FL*FL\n    cavitating = False if xF <= xFzp1 else True\n    \n    eta_turb = 10.0**An*Uvc/c\n    \n    if cavitating:\n    \teta_cav = 0.32*eta_turb*((P1 - P2)/(dPc*xFzp1))**0.5*exp(5.0*xFzp1)*((1.0 \n                 - xFzp1)/(1.0 - xF))**0.5*(xF/xFzp1)**5*(xF - xFzp1)**1.5\n    \tWa = (eta_turb+eta_cav)*Wm\n    else:\n    \tWa = eta_turb*Wm\n    \n    Lpi = 10.0*log10(3.2E9*Wa*rho*c/(Di*Di))\n    Stp = 0.036*FL*FL*C*Fd**0.75/(N34*xFzp1**1.5*d*d)*(1.0/(P1 - Psat))**0.57\n    f_p_turb = Stp*Uvc/Dj\n    \n    if cavitating:\n        f_p_cav = 6*f_p_turb*((1.0 - xF)/(1.0 - xFzp1))**2*(xFzp1/xF)**2.5\n        f_p_cav_inv = 1.0/f_p_cav\n        eta_denom = 1.0/(eta_turb + eta_cav)\n        t1 = eta_turb*eta_denom\n        t2 = eta_cav*eta_denom\n\n    fr = c_pipe/(pi*Di)    \n    fr_inv = 1.0/fr\n    TL_fr = -10.0 - 10.0*log10(c_pipe*rho_pipe*t_pipe/(c_air*rho_air*Di))\n    \n    t3 = - 10.0*log10((Di + 2.0*t_pipe + 2.0)/(Di + 2.0*t_pipe))\n\n#    F_cavs = []\n#    F_turbs = []\n#    LPis = []\n#    TL_fis = []\n#    L_pe1m_fis = []\n    LpAe1m_sum = 0.0\n    \n    f_p_turb_inv = 1.0/f_p_turb\n    \n    \n    \n    for fi, A in zip(fis_l_2015, A_weights_l_2015):\n        fi_inv = 1.0/fi\n        fi_turb_ratio = fi*f_p_turb_inv\n        F_turb = -8.0 - 10.0*log10(0.25*fi_turb_ratio*fi_turb_ratio*fi_turb_ratio\n                                   + fi_inv*f_p_turb) \n#        F_turbs.append(F_turb)\n        if cavitating:\n            fi_cav_ratio = (fi*f_p_cav_inv)**1.5\n            F_cav = -9.0 - 10.0*log10(0.25*fi_cav_ratio + 1.0/fi_cav_ratio)\n            LPif = (Lpi + 10.0*log10(t1*10.0**(0.1*F_turb)\n                    + t2*10.0**(0.1*F_cav)))\n        else:\n            LPif = Lpi + F_turb\n#        LPis.append(LPif)\n        TL_fi = TL_fr - 20.0*log10((fr*fi_inv) + (fi*fr_inv)**1.5)\n#        TL_fis.append(TL_fi)\n        \n        L_pe1m_fi = LPif + TL_fi + t3\n#        L_pe1m_fis.append(L_pe1m_fi)\n        \n        LpAe1m_sum += 10.0**(0.1*(L_pe1m_fi + A))\n    LpAe1m = 10.0*log10(LpAe1m_sum)\n    return LpAe1m", "response": "r Calculates the control valve noise for a liquid flowing through a control valve."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads a numpydoc function and compares the Parameters and Other Parameters with the input arguments of the actual function signature. Raises an exception if not correctly defined.", "response": "def check_args_order(func):\n    '''Reads a numpydoc function and compares the Parameters and\n    Other Parameters with the input arguments of the actual function signature.\n    Raises an exception if not correctly defined.\n    \n    >>> check_args_order(fluids.core.Reynolds)\n    '''\n    argspec = inspect.getargspec(func)\n    parsed_data = parse_numpydoc_variables_units(func)\n    # compare the parsed arguments with those actually defined\n    parsed_units = parsed_data['Parameters']['units']\n    parsed_parameters = parsed_data['Parameters']['vars']\n    if 'Other Parameters' in parsed_data:\n        parsed_parameters += parsed_data['Other Parameters']['vars']\n        parsed_units += parsed_data['Other Parameters']['units']\n    \n    if argspec.args != parsed_parameters: # pragma: no cover\n        raise Exception('Function %s signature is not the same as the documentation'\n                        ' signature = %s; documentation = %s' %(func.__name__, argspec.args, parsed_parameters))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwraps function for isothermal_gas", "response": "def isothermal_gas(rho, fd, P1=None, P2=None, L=None, D=None, m=None): # pragma: no cover\n    '''\n    >>> isothermal_gas(rho=11.3*u.kg/u.m**3, fd=0.00185*u.dimensionless, P1=1E6*u.Pa, P2=9E5*u.Pa, L=1000*u.m, D=0.5*u.m)\n    <Quantity(145.484757264, 'kilogram / second')>\n    '''\n    ans = wrapped_isothermal_gas(rho, fd, P1, P2, L, D, m)    \n    if m is None and (None not in [P1, P2, L, D]):\n        return ans*u.kg/u.s\n    elif L is None and (None not in [P1, P2, D, m]):\n        return ans*u.m\n    elif P1 is None and (None not in [L, P2, D, m]):\n        return ans*u.Pa\n    elif P2 is None and (None not in [L, P1, D, m]):\n        return ans*u.Pa\n    elif D is None and (None not in [P2, P1, L, m]):\n        return ans*u.m"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef cheb_range_simplifier(low, high, text=False):\n    '''\n    >>> low, high = 0.0023046250851646434, 4.7088985707840125\n    >>> cheb_range_simplifier(low, high, text=True)\n    'chebval(0.42493574399544564724*(x + -2.3556015979345885647), coeffs)'\n    '''\n    constant = 0.5*(-low-high)\n    factor = 2.0/(high-low)\n    if text:\n        return 'chebval(%.20g*(x + %.20g), coeffs)' %(factor, constant)\n    return constant, factor", "response": "Return a simplified Chinese range."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef cast_scalar(method):\n    @wraps(method)\n    def new_method(self, other):\n        if np.isscalar(other):\n            other = type(self)([other],self.domain())\n        return method(self, other)\n    return new_method", "response": "Cast scalars to constant interpolating objects\n           "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing DCT using FFT", "response": "def dct(data):\n    \"\"\"\n    Compute DCT using FFT\n    \"\"\"\n    N = len(data)//2\n    fftdata     = fftpack.fft(data, axis=0)[:N+1]\n    fftdata     /= N\n    fftdata[0]  /= 2.\n    fftdata[-1] /= 2.\n    if np.isrealobj(data):\n        data = np.real(fftdata)\n    else:\n        data = fftdata\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a Chebyshev polynomial approximation of the function f on the given domain.", "response": "def chebfun(f=None, domain=[-1,1], N=None, chebcoeff=None,):\n    \"\"\"\n    Create a Chebyshev polynomial approximation of the function $f$ on the interval :math:`[-1, 1]`.\n    :param callable f: Python, Numpy, or Sage function\n    :param int N: (default = None)  specify number of interpolating points\n    :param np.array chebcoeff: (default = np.array(0)) specify the coefficients\n    \"\"\"\n\n    # Chebyshev coefficients\n    if chebcoeff is not None:\n        return Chebfun.from_coeff(chebcoeff, domain)\n\n    # another instance\n    if isinstance(f, Polyfun):\n        return Chebfun.from_fun(f)\n\n    # callable\n    if hasattr(f, '__call__'):\n        return Chebfun.from_function(f, domain, N)\n\n    # from here on, assume that f is None, or iterable\n    if np.isscalar(f):\n        f = [f]\n\n    try:\n        iter(f) # interpolation values provided\n    except TypeError:\n        pass\n    else:\n        return Chebfun(f, domain)\n\n    raise TypeError('Impossible to initialise the object from an object of type {}'.format(type(f)))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef from_coeff(self, chebcoeff, domain=None, prune=True, vscale=1.):\n        coeffs = np.asarray(chebcoeff)\n        if prune:\n            N = self._cutoff(coeffs, vscale)\n            pruned_coeffs = coeffs[:N]\n        else:\n            pruned_coeffs = coeffs\n        values = self.polyval(pruned_coeffs)\n        return self(values, domain, vscale)", "response": "Initialise from a set of coefficients."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes the coefficients for a function f by dichotomy.", "response": "def dichotomy(self, f, kmin=2, kmax=12, raise_no_convergence=True,):\n        \"\"\"\n        Compute the coefficients for a function f by dichotomy.\n        kmin, kmax: log2 of number of interpolation points to try\n        raise_no_convergence: whether to raise an exception if the dichotomy does not converge\n        \"\"\"\n\n        for k in range(kmin, kmax):\n            N = pow(2, k)\n\n            sampled = self.sample_function(f, N)\n            coeffs = self.polyfit(sampled)\n\n            # 3) Check for negligible coefficients\n            #    If within bound: get negligible coeffs and bread\n            bnd = self._threshold(np.max(np.abs(coeffs)))\n\n            last = abs(coeffs[-2:])\n            if np.all(last <= bnd):\n                break\n        else:\n            if raise_no_convergence:\n                raise self.NoConvergence(last, bnd)\n        return coeffs"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef from_function(self, f, domain=None, N=None):\n        # rescale f to the unit domain\n        domain = self.get_default_domain(domain)\n        a,b = domain[0], domain[-1]\n        map_ui_ab = lambda t: 0.5*(b-a)*t + 0.5*(a+b)\n        args = {'f': lambda t: f(map_ui_ab(t))}\n        if N is not None: # N is provided\n            nextpow2 = int(np.log2(N))+1\n            args['kmin'] = nextpow2\n            args['kmax'] = nextpow2+1\n            args['raise_no_convergence'] = False\n        else:\n            args['raise_no_convergence'] = True\n\n        # Find out the right number of coefficients to keep\n        coeffs = self.dichotomy(**args)\n\n        return self.from_coeff(coeffs, domain)", "response": "Initialise from a function to sample."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _cutoff(self, coeffs, vscale):\n        bnd = self._threshold(vscale)\n        inds  = np.nonzero(abs(coeffs) >= bnd)\n        if len(inds[0]):\n            N = inds[0][-1]\n        else:\n            N = 0\n        return N+1", "response": "Compute cutoff index after which the coefficients are deemed negligible."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning True if the domains of two objects are the same.", "response": "def same_domain(self, fun2):\n        \"\"\"\n        Returns True if the domains of two objects are the same.\n        \"\"\"\n        return np.allclose(self.domain(), fun2.domain(), rtol=1e-14, atol=1e-14)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef restrict(self,subinterval):\n        if (subinterval[0] < self._domain[0]) or (subinterval[1] > self._domain[1]):\n            raise ValueError(\"Can only restrict to subinterval\")\n        return self.from_function(self, subinterval)", "response": "Returns a new instance of this class that matches self on subinterval."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a new array with n elements.", "response": "def basis(self, n):\n        \"\"\"\n        Chebyshev basis functions T_n.\n        \"\"\"\n        if n == 0:\n            return self(np.array([1.]))\n        vals = np.ones(n+1)\n        vals[1::2] = -1\n        return self(vals)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nevaluates the integral over the given interval using Clenshaw - Curtis quadrature.", "response": "def sum(self):\n        \"\"\"\n        Evaluate the integral over the given interval using\n        Clenshaw-Curtis quadrature.\n        \"\"\"\n        ak = self.coefficients()\n        ak2 = ak[::2]\n        n = len(ak2)\n        Tints = 2/(1-(2*np.arange(n))**2)\n        val = np.sum((Tints*ak2.T).T, axis=0)\n        a_, b_ = self.domain()\n        return 0.5*(b_-a_)*val"}
